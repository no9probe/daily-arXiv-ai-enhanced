{"id": "2506.19952", "pdf": "https://arxiv.org/pdf/2506.19952", "abs": "https://arxiv.org/abs/2506.19952", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.", "AI": {"tldr": "CycleDistill\u662f\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c11\u91cf\u793a\u4f8b\u8fdb\u884c\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7684\u5faa\u73af\u84b8\u998f\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u5e73\u884c\u8bed\u6599\u5e93\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u7cfb\u7edf\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u901a\u5e38\u7f3a\u4e4f\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u91cf\u793a\u4f8b\u4e0b\u7684\u673a\u5668\u7ffb\u8bd1\u8868\u73b0\u4e0d\u5982\u4e13\u7528\u7ffb\u8bd1\u7cfb\u7edf\u3002CycleDistill\u65e8\u5728\u901a\u8fc7\u5faa\u73af\u84b8\u998f\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u8bed\u8bed\u6599\u5e93\u548c\u5c11\u91cf\u793a\u4f8b\u751f\u6210\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u7cfb\u7edf\u3002", "method": "CycleDistill\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u5408\u6210\u5e73\u884c\u8bed\u6599\u5e93\uff08\u57fa\u4e8e\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u7ffb\u8bd1\uff09\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u5fae\u8c03\u6a21\u578b\uff0c\u5b9e\u73b0\u673a\u5668\u7ffb\u8bd1\u7684\u9010\u6b65\u63d0\u5347\u3002\u540c\u65f6\u7814\u7a76\u4e86\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u4f7f\u7528softmax\u6fc0\u6d3b\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u6539\u8fdb\u3002", "result": "\u5728\u4e09\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5b9e\u9a8c\u4e2d\uff0cCycleDistill\u4ec5\u4f9d\u8d56\u5355\u8bed\u8bed\u6599\u5e93\uff0c\u9996\u6b21\u8fed\u4ee3\u5373\u53ef\u5c06\u7ffb\u8bd1\u8d28\u91cf\u63d0\u534720-30 chrF\u70b9\uff0c\u4f18\u4e8e\u5c11\u6837\u672c\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "CycleDistill\u5c55\u793a\u4e86\u65e0\u9700\u5927\u89c4\u6a21\u5e73\u884c\u8bed\u6599\u5e93\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "CycleDistill\uff1a\u57fa\u4e8e\u5faa\u73af\u84b8\u998f\u7684\u5927\u8bed\u8a00\u6a21\u578b\u673a\u5668\u7ffb\u8bd1\u81ea\u4e3e\u65b9\u6cd5", "abstract_zh": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u8fdb\u884c\u5c11\u6837\u672c\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\uff0c\u4f46\u5176\u8868\u73b0\u901a\u5e38\u4e0d\u5982\u57fa\u4e8e\u5e73\u884c\u8bed\u6599\u5e93\u8bad\u7ec3\u7684\u4e13\u7528MT\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5e73\u884c\u8bed\u6599\u5e93\u5f80\u5f80\u7a00\u7f3a\u6216\u4e0d\u5b58\u5728\u3002\u672c\u6587\u63d0\u51faCycleDistill\uff0c\u4e00\u79cd\u5229\u7528LLMs\u548c\u5c11\u6837\u672c\u7ffb\u8bd1\u7684\u81ea\u4e3e\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cfMT\u7cfb\u7edf\u3002CycleDistill\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u5408\u6210\u5e73\u884c\u8bed\u6599\u5e93\uff08\u57fa\u4e8e\u96f6\u6837\u672c\u6216\u5c11\u6837\u672cMT\uff09\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u5fae\u8c03\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002CycleDistill\u4ec5\u97001\u81f34\u4e2a\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u65e0\u9700\u5176\u4ed6\u5e73\u884c\u8bed\u6599\u5e93\u3002\u5728\u9488\u5bf9\u4e09\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4ec5\u4f9d\u8d56\u5355\u8bed\u8bed\u6599\u5e93\uff0cCycleDistill\u9996\u6b21\u8fed\u4ee3\u5373\u53ef\u5c06\u7ffb\u8bd1\u8d28\u91cf\u63d0\u534720-30 chrF\u70b9\uff0c\u4f18\u4e8e\u5c11\u6837\u672c\u57fa\u7ebf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5229\u7528softmax\u6fc0\u6d3b\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u8f7b\u5fae\u6539\u8fdb\u3002"}}
{"id": "2506.19967", "pdf": "https://arxiv.org/pdf/2506.19967", "abs": "https://arxiv.org/abs/2506.19967", "authors": ["Travis Thompson", "Seung-Hwan Lim", "Paul Liu", "Ruoying He", "Dongkuan Xu"], "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive capabilities in\nlanguage understanding and generation, yet they continue to underperform on\nknowledge-intensive reasoning tasks due to limited access to structured context\nand multi-hop information. Retrieval-Augmented Generation (RAG) partially\nmitigates this by grounding generation in retrieved context, but conventional\nRAG and GraphRAG methods often fail to capture relational structure across\nnodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel\nframework that enhances LLM-based graph reasoning by applying inference-time\ncompute scaling. Our method combines sequential scaling with deep\nchain-of-thought graph traversal, and parallel scaling with majority voting\nover sampled trajectories within an interleaved reasoning-execution loop.\nExperiments on the GRBench benchmark demonstrate that our approach\nsignificantly improves multi-hop question answering performance, achieving\nsubstantial gains over both traditional GraphRAG and prior graph traversal\nbaselines. These findings suggest that inference-time scaling is a practical\nand architecture-agnostic solution for structured knowledge reasoning with LLMs", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInference-Scaled GraphRAG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u6269\u5c55\u63d0\u5347\u57fa\u4e8eLLM\u7684\u56fe\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u548c\u591a\u8df3\u4fe1\u606f\u7684\u8bbf\u95ee\u3002\u4f20\u7edf\u7684RAG\u548cGraphRAG\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8282\u70b9\u95f4\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684Inference-Scaled GraphRAG\u6846\u67b6\u7ed3\u5408\u4e86\u987a\u5e8f\u6269\u5c55\uff08\u901a\u8fc7\u6df1\u5ea6\u94fe\u5f0f\u601d\u7ef4\u56fe\u904d\u5386\uff09\u548c\u5e76\u884c\u6269\u5c55\uff08\u901a\u8fc7\u591a\u6570\u6295\u7968\u91c7\u6837\u8f68\u8ff9\uff09\uff0c\u5e76\u5728\u4ea4\u9519\u7684\u63a8\u7406-\u6267\u884c\u5faa\u73af\u4e2d\u5b9e\u73b0\u3002", "result": "\u5728GRBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edfGraphRAG\u548c\u5148\u524d\u7684\u56fe\u904d\u5386\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8eLLMs\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u3002", "paper_title_zh": "\u63a8\u7406\u6269\u5c55GraphRAG\uff1a\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u591a\u8df3\u95ee\u7b54", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u7531\u4e8e\u5bf9\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u548c\u591a\u8df3\u4fe1\u606f\u7684\u8bbf\u95ee\u6709\u9650\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4ecd\u4e0d\u7406\u60f3\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5c06\u751f\u6210\u57fa\u4e8e\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u90e8\u5206\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4f20\u7edf\u7684RAG\u548cGraphRAG\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8282\u70b9\u95f4\u7684\u5173\u7cfb\u7ed3\u6784\u3002\u6211\u4eec\u63d0\u51fa\u4e86Inference-Scaled GraphRAG\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5e94\u7528\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u6269\u5c55\u6765\u589e\u5f3a\u57fa\u4e8eLLM\u7684\u56fe\u63a8\u7406\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u987a\u5e8f\u6269\u5c55\uff08\u901a\u8fc7\u6df1\u5ea6\u94fe\u5f0f\u601d\u7ef4\u56fe\u904d\u5386\uff09\u548c\u5e76\u884c\u6269\u5c55\uff08\u901a\u8fc7\u591a\u6570\u6295\u7968\u91c7\u6837\u8f68\u8ff9\uff09\uff0c\u5e76\u5728\u4ea4\u9519\u7684\u63a8\u7406-\u6267\u884c\u5faa\u73af\u4e2d\u5b9e\u73b0\u3002\u5728GRBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edfGraphRAG\u548c\u5148\u524d\u7684\u56fe\u904d\u5386\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8eLLMs\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u3002"}}
{"id": "2506.19998", "pdf": "https://arxiv.org/pdf/2506.19998", "abs": "https://arxiv.org/abs/2506.19998", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "categories": ["cs.CL"], "comment": null, "summary": "REST APIs play important roles in enriching the action space of web agents,\nyet most API-based agents rely on curated and uniform toolsets that do not\nreflect the complexity of real-world APIs. Building tool-using agents for\narbitrary domains remains a major challenge, as it requires reading\nunstructured API documentation, testing APIs and inferring correct parameters.\nWe propose Doc2Agent, a scalable pipeline to build agents that can call\nPython-based tools generated from API documentation. Doc2Agent generates\nexecutable tools from API documentations and iteratively refines them using a\ncode agent. We evaluate our approach on real-world APIs, WebArena APIs, and\nresearch APIs, producing validated tools. We achieved a 55\\% relative\nperformance improvement with 90\\% lower cost compared to direct API calling on\nWebArena benchmark. A domain-specific agent built for glycomaterial science\nfurther demonstrates the pipeline's adaptability to complex, knowledge-rich\ntasks. Doc2Agent offers a generalizable solution for building tool agents from\nunstructured API documentation at scale.", "AI": {"tldr": "Doc2Agent\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4eceAPI\u6587\u6863\u751f\u6210\u5de5\u5177\u8c03\u7528\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684API\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u7edf\u4e00\u5de5\u5177\u96c6\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754cAPI\u7684\u590d\u6742\u6027\u3002\u6784\u5efa\u9002\u7528\u4e8e\u4efb\u610f\u9886\u57df\u7684\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4ece\u975e\u7ed3\u6784\u5316API\u6587\u6863\u4e2d\u63d0\u53d6\u4fe1\u606f\u5e76\u63a8\u65ad\u6b63\u786e\u53c2\u6570\u3002", "method": "Doc2Agent\u901a\u8fc7\u4eceAPI\u6587\u6863\u751f\u6210\u53ef\u6267\u884c\u5de5\u5177\uff0c\u5e76\u5229\u7528\u4ee3\u7801\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u8fd9\u4e9b\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u7684\u81ea\u52a8\u5316\u6784\u5efa\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cAPI\u3001WebArena API\u548c\u7814\u7a76API\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDoc2Agent\u6027\u80fd\u63d0\u5347\u4e8655%\uff0c\u6210\u672c\u964d\u4f4e\u4e8690%\u3002\u6b64\u5916\uff0c\u5728\u7cd6\u6750\u6599\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u8bc1\u660e\u4e86\u5176\u9002\u5e94\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "Doc2Agent\u4e3a\u4ece\u975e\u7ed3\u6784\u5316API\u6587\u6863\u5927\u89c4\u6a21\u6784\u5efa\u5de5\u5177\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "Doc2Agent\uff1a\u4eceAPI\u6587\u6863\u53ef\u6269\u5c55\u751f\u6210\u5de5\u5177\u8c03\u7528\u4ee3\u7406", "abstract_zh": "REST API\u5728\u4e30\u5bcc\u7f51\u7edc\u4ee3\u7406\u7684\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5927\u591a\u6570\u57fa\u4e8eAPI\u7684\u4ee3\u7406\u4f9d\u8d56\u7edf\u4e00\u5de5\u5177\u96c6\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754cAPI\u7684\u590d\u6742\u6027\u3002\u4e3a\u4efb\u610f\u9886\u57df\u6784\u5efa\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u4ecd\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u9605\u8bfb\u975e\u7ed3\u6784\u5316API\u6587\u6863\u3001\u6d4b\u8bd5API\u5e76\u63a8\u65ad\u6b63\u786e\u53c2\u6570\u3002\u6211\u4eec\u63d0\u51fa\u4e86Doc2Agent\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u8c03\u7528\u4eceAPI\u6587\u6863\u751f\u6210\u7684Python\u5de5\u5177\u7684\u4ee3\u7406\u3002Doc2Agent\u4eceAPI\u6587\u6863\u751f\u6210\u53ef\u6267\u884c\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u4ee3\u7801\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u8fd9\u4e9b\u5de5\u5177\u3002\u6211\u4eec\u5728\u771f\u5b9e\u4e16\u754cAPI\u3001WebArena API\u548c\u7814\u7a76API\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u5df2\u9a8c\u8bc1\u7684\u5de5\u5177\u3002\u4e0e\u76f4\u63a5\u8c03\u7528API\u76f8\u6bd4\uff0c\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u5b9e\u73b0\u4e8655%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u548c90%\u7684\u6210\u672c\u964d\u4f4e\u3002\u4e3a\u7cd6\u6750\u6599\u79d1\u5b66\u9886\u57df\u6784\u5efa\u7684\u7279\u5b9a\u9886\u57df\u4ee3\u7406\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u6d41\u6c34\u7ebf\u5bf9\u590d\u6742\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u9002\u5e94\u6027\u3002Doc2Agent\u4e3a\u4ece\u975e\u7ed3\u6784\u5316API\u6587\u6863\u5927\u89c4\u6a21\u6784\u5efa\u5de5\u5177\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19999", "pdf": "https://arxiv.org/pdf/2506.19999", "abs": "https://arxiv.org/abs/2506.19999", "authors": ["Francesco Ignazio Re", "Andreas Opedal", "Glib Manaiev", "Mario Giulianelli", "Ryan Cotterell"], "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior", "categories": ["cs.LG", "cs.CL", "q-bio.NC"], "comment": "ACL 2025", "summary": "Reading is a process that unfolds across space and time, alternating between\nfixations where a reader focuses on a specific point in space, and saccades\nwhere a reader rapidly shifts their focus to a new point. An ansatz of\npsycholinguistics is that modeling a reader's fixations and saccades yields\ninsight into their online sentence processing. However, standard approaches to\nsuch modeling rely on aggregated eye-tracking measurements and models that\nimpose strong assumptions, ignoring much of the spatio-temporal dynamics that\noccur during reading. In this paper, we propose a more general probabilistic\nmodel of reading behavior, based on a marked spatio-temporal point process,\nthat captures not only how long fixations last, but also where they land in\nspace and when they take place in time. The saccades are modeled using a Hawkes\nprocess, which captures how each fixation excites the probability of a new\nfixation occurring near it in time and space. The duration time of fixation\nevents is modeled as a function of fixation-specific predictors convolved\nacross time, thus capturing spillover effects. Empirically, our Hawkes process\nmodel exhibits a better fit to human saccades than baselines. With respect to\nfixation durations, we observe that incorporating contextual surprisal as a\npredictor results in only a marginal improvement in the model's predictive\naccuracy. This finding suggests that surprisal theory struggles to explain\nfine-grained eye movements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u70b9\u8fc7\u7a0b\u7684\u7cbe\u7ec6\u9605\u8bfb\u884c\u4e3a\u6a21\u578b\uff0c\u901a\u8fc7\u970d\u514b\u65af\u8fc7\u7a0b\u6355\u6349\u6ce8\u89c6\u548c\u626b\u89c6\u7684\u52a8\u6001\u7279\u6027\uff0c\u53d1\u73b0\u4f20\u7edf\u60ca\u8bb6\u7406\u8bba\u5bf9\u773c\u52a8\u89e3\u91ca\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u9605\u8bfb\u884c\u4e3a\u6a21\u578b\u591a\u57fa\u4e8e\u5f3a\u5047\u8bbe\u548c\u805a\u5408\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u66f4\u901a\u7528\u7684\u6982\u7387\u6a21\u578b\uff0c\u4ee5\u7cbe\u7ec6\u6355\u6349\u9605\u8bfb\u4e2d\u7684\u6ce8\u89c6\u548c\u626b\u89c6\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6807\u8bb0\u65f6\u7a7a\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u5176\u4e2d\u626b\u89c6\u901a\u8fc7\u970d\u514b\u65af\u8fc7\u7a0b\u5efa\u6a21\uff0c\u6355\u6349\u65f6\u7a7a\u90bb\u8fd1\u6027\uff1b\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u5219\u7ed3\u5408\u65f6\u95f4\u5377\u79ef\u7684\u9884\u6d4b\u56e0\u5b50\u5efa\u6a21\u3002", "result": "\u970d\u514b\u65af\u8fc7\u7a0b\u6a21\u578b\u5728\u626b\u89c6\u62df\u5408\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff1b\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u6a21\u578b\u4e2d\uff0c\u4e0a\u4e0b\u6587\u60ca\u8bb6\u56e0\u5b50\u7684\u52a0\u5165\u5bf9\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u672c\u6587\u6a21\u578b\u80fd\u66f4\u7cbe\u7ec6\u5730\u63cf\u8ff0\u9605\u8bfb\u884c\u4e3a\uff0c\u4f46\u60ca\u8bb6\u7406\u8bba\u5bf9\u773c\u52a8\u7684\u89e3\u91ca\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u4ed6\u56e0\u7d20\u3002", "paper_title_zh": "\u4e00\u79cd\u7528\u4e8e\u7cbe\u7ec6\u5efa\u6a21\u9605\u8bfb\u884c\u4e3a\u7684\u65f6\u7a7a\u70b9\u8fc7\u7a0b", "abstract_zh": "\u9605\u8bfb\u662f\u4e00\u4e2a\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e2d\u5c55\u5f00\u7684\u8fc7\u7a0b\uff0c\u4ea4\u66ff\u8868\u73b0\u4e3a\u6ce8\u89c6\uff08\u8bfb\u8005\u805a\u7126\u4e8e\u67d0\u4e00\u70b9\uff09\u548c\u626b\u89c6\uff08\u5feb\u901f\u8f6c\u79fb\u7126\u70b9\uff09\u3002\u5fc3\u7406\u8bed\u8a00\u5b66\u7684\u5047\u8bbe\u8ba4\u4e3a\uff0c\u5efa\u6a21\u6ce8\u89c6\u548c\u626b\u89c6\u80fd\u63ed\u793a\u8bfb\u8005\u7684\u5728\u7ebf\u53e5\u5b50\u5904\u7406\u673a\u5236\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u805a\u5408\u7684\u773c\u52a8\u6570\u636e\u53ca\u5f3a\u5047\u8bbe\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u9605\u8bfb\u4e2d\u7684\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u7684\u9605\u8bfb\u884c\u4e3a\u6982\u7387\u6a21\u578b\uff0c\u57fa\u4e8e\u6807\u8bb0\u65f6\u7a7a\u70b9\u8fc7\u7a0b\uff0c\u4e0d\u4ec5\u6355\u6349\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\uff0c\u8fd8\u8bb0\u5f55\u5176\u7a7a\u95f4\u4f4d\u7f6e\u548c\u65f6\u95f4\u70b9\u3002\u626b\u89c6\u901a\u8fc7\u970d\u514b\u65af\u8fc7\u7a0b\u5efa\u6a21\uff0c\u6355\u6349\u6bcf\u6b21\u6ce8\u89c6\u5bf9\u90bb\u8fd1\u65f6\u7a7a\u65b0\u6ce8\u89c6\u7684\u6fc0\u53d1\u6548\u5e94\uff1b\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u5219\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7684\u9884\u6d4b\u56e0\u5b50\u5efa\u6a21\uff0c\u4ee5\u6355\u6349\u6ea2\u51fa\u6548\u5e94\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u970d\u514b\u65af\u8fc7\u7a0b\u6a21\u578b\u5728\u626b\u89c6\u62df\u5408\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff1b\u800c\u5728\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u6a21\u578b\u4e2d\uff0c\u52a0\u5165\u4e0a\u4e0b\u6587\u60ca\u8bb6\u56e0\u5b50\u4ec5\u7565\u5fae\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8868\u660e\u60ca\u8bb6\u7406\u8bba\u96be\u4ee5\u89e3\u91ca\u7cbe\u7ec6\u773c\u52a8\u3002"}}
{"id": "2506.19939", "pdf": "https://arxiv.org/pdf/2506.19939", "abs": "https://arxiv.org/abs/2506.19939", "authors": ["Aryan Singh Dalal", "Sidharth Rai", "Rahul Singh", "Treman Singh Kaloya", "Rahul Harsha Cheppally", "Ajay Sharda"], "title": "Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement", "categories": ["cs.CV"], "comment": "Under publication process for COMPAG", "summary": "Application rate errors when using self-propelled agricultural sprayers for\nagricultural production remain a concern. Among other factors, spray boom\ninstability is one of the major contributors to application errors. Spray\nbooms' width of 38m, combined with 30 kph driving speeds, varying terrain, and\nmachine dynamics when maneuvering complex field boundaries, make controls of\nthese booms very complex. However, there is no quantitative knowledge on the\nextent of boom movement to systematically develop a solution that might include\nboom designs and responsive boom control systems. Therefore, this study was\nconducted to develop an automated computer vision system to quantify the boom\nmovement of various agricultural sprayers. A computer vision system was\ndeveloped to track a target on the edge of the sprayer boom in real time. YOLO\nV7, V8, and V11 neural network models were trained to track the boom's\nmovements in field operations to quantify effective displacement in the\nvertical and transverse directions. An inclinometer sensor was mounted on the\nboom to capture boom angles and validate the neural network model output. The\nresults showed that the model could detect the target with more than 90 percent\naccuracy, and distance estimates of the target on the boom were within 0.026 m\nof the inclinometer sensor data. This system can quantify the boom movement on\nthe current sprayer and potentially on any other sprayer with minor\nmodifications. The data can be used to make design improvements to make sprayer\nbooms more stable and achieve greater application accuracy.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u91cf\u5316\u519c\u4e1a\u55b7\u96fe\u5668\u55b7\u6746\u7684\u4f4d\u79fb\uff0c\u4ee5\u63d0\u9ad8\u55b7\u96fe\u7cbe\u5ea6\u3002\u901a\u8fc7YOLO\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9e\u65f6\u8ddf\u8e2a\u55b7\u6746\u76ee\u6807\uff0c\u7ed3\u5408\u503e\u89d2\u4f20\u611f\u5668\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u68c0\u6d4b\u7cbe\u5ea6\u8d85\u8fc790%\uff0c\u4f4d\u79fb\u4f30\u8ba1\u8bef\u5dee\u5c0f\u4e8e0.026\u7c73\u3002", "motivation": "\u519c\u4e1a\u55b7\u96fe\u5668\u5728\u4f5c\u4e1a\u4e2d\u55b7\u6746\u7684\u4e0d\u7a33\u5b9a\u6027\u662f\u5bfc\u81f4\u55b7\u96fe\u8bef\u5dee\u7684\u4e3b\u8981\u56e0\u7d20\u4e4b\u4e00\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u55b7\u6746\u4f4d\u79fb\u7684\u5b9a\u91cf\u7814\u7a76\uff0c\u96be\u4ee5\u7cfb\u7edf\u6539\u8fdb\u55b7\u6746\u8bbe\u8ba1\u548c\u63a7\u5236\u7cfb\u7edf\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u91cf\u5316\u55b7\u6746\u4f4d\u79fb\uff0c\u4e3a\u8bbe\u8ba1\u6539\u8fdb\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u5957\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u5229\u7528YOLO V7\u3001V8\u548cV11\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9e\u65f6\u8ddf\u8e2a\u55b7\u6746\u8fb9\u7f18\u7684\u76ee\u6807\u3002\u540c\u65f6\uff0c\u5728\u55b7\u6746\u4e0a\u5b89\u88c5\u503e\u89d2\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u51fa\u3002\u901a\u8fc7\u91cf\u5316\u55b7\u6746\u5728\u5782\u76f4\u548c\u6a2a\u5411\u65b9\u5411\u7684\u4f4d\u79fb\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5bf9\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\u8d85\u8fc790%\uff0c\u55b7\u6746\u4f4d\u79fb\u7684\u4f30\u8ba1\u503c\u4e0e\u503e\u89d2\u4f20\u611f\u5668\u6570\u636e\u7684\u8bef\u5dee\u57280.026\u7c73\u4ee5\u5185\u3002\u8be5\u7cfb\u7edf\u53ef\u9002\u7528\u4e8e\u5f53\u524d\u55b7\u96fe\u5668\u53ca\u5176\u4ed6\u55b7\u96fe\u5668\uff0c\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u91cf\u5316\u55b7\u6746\u4f4d\u79fb\uff0c\u4e3a\u55b7\u6746\u8bbe\u8ba1\u548c\u63a7\u5236\u7cfb\u7edf\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u9760\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u55b7\u96fe\u5668\u7684\u7a33\u5b9a\u6027\u548c\u55b7\u96fe\u7cbe\u5ea6\u3002", "paper_title_zh": "\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u519c\u4e1a\u55b7\u96fe\u5668\u55b7\u6746\u4f4d\u79fb\u81ea\u52a8\u5316\u91cf\u5316", "abstract_zh": "\u519c\u4e1a\u55b7\u96fe\u5668\u5728\u519c\u4e1a\u751f\u4ea7\u4e2d\u7684\u55b7\u96fe\u8bef\u5dee\u95ee\u9898\u4e00\u76f4\u5907\u53d7\u5173\u6ce8\uff0c\u5176\u4e2d\u55b7\u6746\u7684\u4e0d\u7a33\u5b9a\u6027\u662f\u4e3b\u8981\u56e0\u7d20\u4e4b\u4e00\u3002\u55b7\u6746\u5bbd\u5ea6\u8fbe38\u7c73\uff0c\u4f5c\u4e1a\u901f\u5ea630\u516c\u91cc/\u5c0f\u65f6\uff0c\u52a0\u4e4b\u590d\u6742\u5730\u5f62\u548c\u673a\u5668\u52a8\u6001\uff0c\u4f7f\u5f97\u55b7\u6746\u63a7\u5236\u6781\u4e3a\u590d\u6742\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u55b7\u6746\u4f4d\u79fb\u7684\u5b9a\u91cf\u7814\u7a76\uff0c\u96be\u4ee5\u7cfb\u7edf\u6539\u8fdb\u55b7\u6746\u8bbe\u8ba1\u548c\u63a7\u5236\u7cfb\u7edf\u3002\u4e3a\u6b64\uff0c\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u91cf\u5316\u55b7\u6746\u4f4d\u79fb\u3002\u901a\u8fc7\u8bad\u7ec3YOLO V7\u3001V8\u548cV11\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9e\u65f6\u8ddf\u8e2a\u55b7\u6746\u8fb9\u7f18\u7684\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u503e\u89d2\u4f20\u611f\u5668\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u68c0\u6d4b\u7cbe\u5ea6\u8d85\u8fc790%\uff0c\u55b7\u6746\u4f4d\u79fb\u4f30\u8ba1\u503c\u4e0e\u4f20\u611f\u5668\u6570\u636e\u7684\u8bef\u5dee\u57280.026\u7c73\u4ee5\u5185\u3002\u8be5\u7cfb\u7edf\u9002\u7528\u4e8e\u5f53\u524d\u55b7\u96fe\u5668\u53ca\u5176\u4ed6\u55b7\u96fe\u5668\uff0c\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u3002\u7814\u7a76\u6570\u636e\u53ef\u7528\u4e8e\u6539\u8fdb\u55b7\u6746\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u55b7\u96fe\u7cbe\u5ea6\u3002"}}
{"id": "2506.19923", "pdf": "https://arxiv.org/pdf/2506.19923", "abs": "https://arxiv.org/abs/2506.19923", "authors": ["Kaito Baba", "Chaoran Liu", "Shuhei Kurita", "Akiyoshi Sannai"], "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs", "categories": ["cs.AI", "cs.LG"], "comment": "22 pages, 2 figures", "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas to assist in\ndiscovering the overall proof strategy. It achieves an 86.1% success rate on\nthe MiniF2F benchmark, establishing a new state-of-the-art among methods using\nsmall language models (SLMs) with a much lower sample budget than previous\napproaches. We also present case studies illustrating how these generated\nlemmas contribute to solving challenging problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProver Agent\u7684\u65b0\u578bAI\u4ee3\u7406\uff0c\u7528\u4e8e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u52a9\u624bLean\uff0c\u901a\u8fc7\u534f\u8c03\u975e\u6b63\u5f0f\u63a8\u7406LLM\u3001\u5f62\u5f0f\u5316\u8bc1\u660e\u6a21\u578b\u53caLean\u7684\u53cd\u9988\uff0c\u5e76\u751f\u6210\u8f85\u52a9\u5f15\u7406\u4ee5\u5e2e\u52a9\u53d1\u73b0\u6574\u4f53\u8bc1\u660e\u7b56\u7565\u3002\u5728MiniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523086.1%\u7684\u6210\u529f\u7387\uff0c\u6210\u4e3a\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e14\u6837\u672c\u9884\u7b97\u66f4\u4f4e\u7684\u65b0\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u5408\u975e\u6b63\u5f0f\u63a8\u7406\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u65b9\u9762\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408LLMs\u4e0e\u5f62\u5f0f\u5316\u5de5\u5177Lean\uff0c\u63d0\u5347\u5b9a\u7406\u8bc1\u660e\u7684\u6210\u529f\u7387\u4e0e\u6548\u7387\u3002", "method": "Prover Agent\u7ed3\u5408\u4e86\u975e\u6b63\u5f0f\u63a8\u7406\u7684LLM\u3001\u5f62\u5f0f\u5316\u8bc1\u660e\u6a21\u578b\u53caLean\u7684\u53cd\u9988\u673a\u5236\uff0c\u540c\u65f6\u751f\u6210\u8f85\u52a9\u5f15\u7406\u4ee5\u4f18\u5316\u8bc1\u660e\u7b56\u7565\u3002\u901a\u8fc7\u534f\u8c03\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9a\u7406\u8bc1\u660e\u6d41\u7a0b\u3002", "result": "\u5728MiniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProver Agent\u53d6\u5f97\u4e8686.1%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4e14\u6837\u672c\u9884\u7b97\u66f4\u4f4e\u3002\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u7684\u5f15\u7406\u5bf9\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u8d21\u732e\u3002", "conclusion": "Prover Agent\u901a\u8fc7\u6574\u5408LLMs\u4e0e\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "paper_title_zh": "Prover Agent\uff1a\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u6846\u67b6", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86Prover Agent\uff0c\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u65b0\u578bAI\u4ee3\u7406\uff0c\u5b83\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u52a9\u624bLean\u76f8\u7ed3\u5408\u3002Prover Agent\u534f\u8c03\u4e86\u4e00\u4e2a\u975e\u6b63\u5f0f\u63a8\u7406\u7684LLM\u3001\u4e00\u4e2a\u5f62\u5f0f\u5316\u8bc1\u660e\u6a21\u578b\u4ee5\u53caLean\u7684\u53cd\u9988\uff0c\u540c\u65f6\u751f\u6210\u8f85\u52a9\u5f15\u7406\u4ee5\u5e2e\u52a9\u53d1\u73b0\u6574\u4f53\u8bc1\u660e\u7b56\u7565\u3002\u5728MiniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u53d6\u5f97\u4e8686.1%\u7684\u6210\u529f\u7387\uff0c\u6210\u4e3a\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e14\u6837\u672c\u9884\u7b97\u66f4\u4f4e\u7684\u65b0\u6700\u4f18\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8fd9\u4e9b\u751f\u6210\u7684\u5f15\u7406\u5982\u4f55\u5e2e\u52a9\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002"}}
{"id": "2506.20073", "pdf": "https://arxiv.org/pdf/2506.20073", "abs": "https://arxiv.org/abs/2506.20073", "authors": ["Kethmi Hirushini Hettige", "Jiahao Ji", "Cheng Long", "Shili Xiang", "Gao Cong", "Jingyuan Wang"], "title": "A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Spatio-temporal data mining plays a pivotal role in informed decision making\nacross diverse domains. However, existing models are often restricted to narrow\ntasks, lacking the capacity for multi-task inference and complex long-form\nreasoning that require generation of in-depth, explanatory outputs. These\nlimitations restrict their applicability to real-world, multi-faceted decision\nscenarios. In this work, we introduce STReason, a novel framework that\nintegrates the reasoning strengths of large language models (LLMs) with the\nanalytical capabilities of spatio-temporal models for multi-task inference and\nexecution. Without requiring task-specific finetuning, STReason leverages\nin-context learning to decompose complex natural language queries into modular,\ninterpretable programs, which are then systematically executed to generate both\nsolutions and detailed rationales. To facilitate rigorous evaluation, we\nconstruct a new benchmark dataset and propose a unified evaluation framework\nwith metrics specifically designed for long-form spatio-temporal reasoning.\nExperimental results show that STReason significantly outperforms advanced LLM\nbaselines across all metrics, particularly excelling in complex,\nreasoning-intensive spatio-temporal scenarios. Human evaluations further\nvalidate STReason's credibility and practical utility, demonstrating its\npotential to reduce expert workload and broaden the applicability to real-world\nspatio-temporal tasks. We believe STReason provides a promising direction for\ndeveloping more capable and generalizable spatio-temporal reasoning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTReason\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u65f6\u7a7a\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u63a8\u7406\u548c\u590d\u6742\u7684\u957f\u7bc7\u63a8\u7406\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5206\u89e3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e3a\u6a21\u5757\u5316\u7a0b\u5e8f\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u7a7a\u6570\u636e\u6316\u6398\u6a21\u578b\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\uff0c\u7f3a\u4e4f\u591a\u4efb\u52a1\u63a8\u7406\u548c\u590d\u6742\u957f\u7bc7\u63a8\u7406\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u591a\u573a\u666f\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408LLMs\u548c\u65f6\u7a7a\u6a21\u578b\u4f18\u52bf\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "STReason\u6846\u67b6\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u590d\u6742\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f\uff0c\u7cfb\u7edf\u6267\u884c\u540e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8be6\u7ec6\u89e3\u91ca\u3002\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u652f\u6301\u591a\u4efb\u52a1\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTReason\u5728\u6240\u6709\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684LLM\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u590d\u6742\u65f6\u7a7a\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "STReason\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u901a\u7528\u7684\u65f6\u7a7a\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u80fd\u591f\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u5e76\u6269\u5c55\u73b0\u5b9e\u65f6\u7a7a\u4efb\u52a1\u7684\u5e94\u7528\u8303\u56f4\u3002", "paper_title_zh": "\u6a21\u5757\u5316\u591a\u4efb\u52a1\u63a8\u7406\u6846\u67b6\uff1a\u65f6\u7a7a\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210", "abstract_zh": "\u65f6\u7a7a\u6570\u636e\u6316\u6398\u5728\u591a\u4e2a\u9886\u57df\u7684\u51b3\u7b56\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u901a\u5e38\u5c40\u9650\u4e8e\u72ed\u7a84\u7684\u4efb\u52a1\uff0c\u7f3a\u4e4f\u591a\u4efb\u52a1\u63a8\u7406\u548c\u590d\u6742\u957f\u7bc7\u63a8\u7406\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u751f\u6210\u6df1\u5165\u7684\u89e3\u91ca\u6027\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u591a\u573a\u666f\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u63d0\u51faSTReason\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u548c\u65f6\u7a7a\u6a21\u578b\u7684\u5206\u6790\u80fd\u529b\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u63a8\u7406\u548c\u6267\u884c\u3002STReason\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u590d\u6742\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f\uff0c\u7cfb\u7edf\u6267\u884c\u540e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8be6\u7ec6\u89e3\u91ca\u3002\u4e3a\u4fbf\u4e8e\u4e25\u683c\u8bc4\u4f30\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7528\u4e8e\u957f\u7bc7\u65f6\u7a7a\u63a8\u7406\u7684\u6307\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTReason\u5728\u6240\u6709\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684LLM\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u590d\u6742\u3001\u63a8\u7406\u5bc6\u96c6\u7684\u65f6\u7a7a\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86STReason\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u5c55\u793a\u4e86\u5176\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u548c\u6269\u5c55\u73b0\u5b9e\u65f6\u7a7a\u4efb\u52a1\u5e94\u7528\u8303\u56f4\u7684\u6f5c\u529b\u3002\u6211\u4eec\u8ba4\u4e3aSTReason\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u901a\u7528\u7684\u65f6\u7a7a\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2506.19955", "pdf": "https://arxiv.org/pdf/2506.19955", "abs": "https://arxiv.org/abs/2506.19955", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "title": "EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression", "categories": ["cs.CV"], "comment": null, "summary": "Density map estimation has become the mainstream paradigm in crowd counting.\nHowever, most existing methods overlook the extreme sparsity of ground-truth\ndensity maps. In real-world crowd scenes, the vast majority of spatial regions\n(often over 95%) contain no people, leading to heavily imbalanced count\ndistributions. Ignoring this imbalance can bias models toward overestimating\ndense regions and underperforming in sparse areas. Furthermore, most loss\nfunctions used in density estimation are majorly based on MSE and implicitly\nassume Gaussian distributions, which are ill-suited for modeling discrete,\nnon-negative count data. In this paper, we propose EBC-ZIP, a crowd counting\nframework that models the spatial distribution of counts using a Zero-Inflated\nPoisson (ZIP) regression formulation. Our approach replaces the traditional\nregression loss with the negative log-likelihood of the ZIP distribution,\nenabling better handling of zero-heavy distributions while preserving count\naccuracy. Built upon the recently proposed Enhanced Block Classification (EBC)\nframework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of\ntargets and ensuring training stability, while further improving performance\nthrough a more principled probabilistic loss. We also evaluate EBC-ZIP with\nbackbones of varying computational complexity to assess its scalability.\nExtensive experiments on four crowd counting benchmarks demonstrate that\nEBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEBC-ZIP\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u81a8\u80c0\u6cca\u677e\u56de\u5f52\u6539\u8fdb\u5757\u72b6\u4eba\u7fa4\u8ba1\u6570\uff0c\u89e3\u51b3\u5bc6\u5ea6\u56fe\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u5347\u7a00\u758f\u533a\u57df\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bc6\u5ea6\u56fe\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u7565\u771f\u5b9e\u573a\u666f\u4e2d\u4eba\u7fa4\u5206\u5e03\u7684\u6781\u7aef\u7a00\u758f\u6027\uff0895%\u4ee5\u4e0a\u533a\u57df\u65e0\u4eba\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u9ad8\u4f30\u5bc6\u96c6\u533a\u57df\uff0c\u4f4e\u4f30\u7a00\u758f\u533a\u57df\u3002\u6b64\u5916\uff0c\u4f20\u7edf\u635f\u5931\u51fd\u6570\u57fa\u4e8eMSE\u548c\u9ad8\u65af\u5206\u5e03\u5047\u8bbe\uff0c\u4e0d\u9002\u5408\u79bb\u6563\u975e\u8d1f\u8ba1\u6570\u6570\u636e\u3002", "method": "EBC-ZIP\u91c7\u7528\u96f6\u81a8\u80c0\u6cca\u677e\u56de\u5f52\uff08ZIP\uff09\u5efa\u6a21\u4eba\u7fa4\u7a7a\u95f4\u5206\u5e03\uff0c\u7528ZIP\u5206\u5e03\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u66ff\u4ee3\u4f20\u7edf\u56de\u5f52\u635f\u5931\uff0c\u7ed3\u5408\u589e\u5f3a\u5757\u5206\u7c7b\uff08EBC\uff09\u6846\u67b6\uff0c\u4fdd\u7559\u76ee\u6807\u79bb\u6563\u6027\u5e76\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEBC-ZIP\u8868\u73b0\u4f18\u4e8eEBC\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u5176\u5904\u7406\u96f6\u5bc6\u96c6\u5206\u5e03\u548c\u63d0\u5347\u8ba1\u6570\u51c6\u786e\u6027\u7684\u80fd\u529b\u3002", "conclusion": "EBC-ZIP\u901a\u8fc7ZIP\u56de\u5f52\u548cEBC\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u663e\u8457\u6539\u5584\u4e86\u4eba\u7fa4\u8ba1\u6570\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u5bc6\u5ea6\u56fe\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "EBC-ZIP\uff1a\u901a\u8fc7\u96f6\u81a8\u80c0\u6cca\u677e\u56de\u5f52\u6539\u8fdb\u5757\u72b6\u4eba\u7fa4\u8ba1\u6570", "abstract_zh": "\u5bc6\u5ea6\u56fe\u4f30\u8ba1\u5df2\u6210\u4e3a\u4eba\u7fa4\u8ba1\u6570\u7684\u4e3b\u6d41\u8303\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u771f\u5b9e\u5bc6\u5ea6\u56fe\u7684\u6781\u7aef\u7a00\u758f\u6027\u3002\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u7edd\u5927\u591a\u6570\u7a7a\u95f4\u533a\u57df\uff08\u901a\u5e38\u8d85\u8fc795%\uff09\u65e0\u4eba\uff0c\u5bfc\u81f4\u8ba1\u6570\u5206\u5e03\u4e25\u91cd\u4e0d\u5e73\u8861\u3002\u5ffd\u89c6\u8fd9\u79cd\u4e0d\u5e73\u8861\u4f1a\u4f7f\u6a21\u578b\u504f\u5411\u9ad8\u4f30\u5bc6\u96c6\u533a\u57df\uff0c\u4f4e\u4f30\u7a00\u758f\u533a\u57df\u3002\u6b64\u5916\uff0c\u5bc6\u5ea6\u4f30\u8ba1\u4e2d\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\u4e3b\u8981\u57fa\u4e8eMSE\uff0c\u9690\u542b\u5047\u8bbe\u9ad8\u65af\u5206\u5e03\uff0c\u4e0d\u9002\u5408\u5efa\u6a21\u79bb\u6563\u975e\u8d1f\u8ba1\u6570\u6570\u636e\u3002\u672c\u6587\u63d0\u51faEBC-ZIP\u6846\u67b6\uff0c\u91c7\u7528\u96f6\u81a8\u80c0\u6cca\u677e\u56de\u5f52\uff08ZIP\uff09\u5efa\u6a21\u8ba1\u6570\u7a7a\u95f4\u5206\u5e03\uff0c\u7528ZIP\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u66ff\u4ee3\u4f20\u7edf\u56de\u5f52\u635f\u5931\uff0c\u66f4\u597d\u5730\u5904\u7406\u96f6\u5bc6\u96c6\u5206\u5e03\u5e76\u4fdd\u6301\u8ba1\u6570\u51c6\u786e\u6027\u3002\u57fa\u4e8e\u589e\u5f3a\u5757\u5206\u7c7b\uff08EBC\uff09\u6846\u67b6\uff0cEBC-ZIP\u7ee7\u627f\u4e86EBC\u5728\u4fdd\u7559\u76ee\u6807\u79bb\u6563\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u66f4\u4e25\u8c28\u7684\u6982\u7387\u635f\u5931\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u4e0d\u540c\u8ba1\u7b97\u590d\u6742\u5ea6\u9aa8\u5e72\u7f51\u7edc\u7684EBC-ZIP\uff0c\u9a8c\u8bc1\u5176\u53ef\u6269\u5c55\u6027\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEBC-ZIP\u59cb\u7ec8\u4f18\u4e8eEBC\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2506.19977", "pdf": "https://arxiv.org/pdf/2506.19977", "abs": "https://arxiv.org/abs/2506.19977", "authors": ["Deng Pan", "Keerthiram Murugesan", "Nuno Moniz", "Nitesh Chawla"], "title": "Context Attribution with Multi-Armed Bandit Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Understanding which parts of the retrieved context contribute to a large\nlanguage model's generated answer is essential for building interpretable and\ntrustworthy generative QA systems. We propose a novel framework that formulates\ncontext attribution as a combinatorial multi-armed bandit (CMAB) problem. Each\ncontext segment is treated as a bandit arm, and we employ Combinatorial\nThompson Sampling (CTS) to efficiently explore the exponentially large space of\ncontext subsets under a limited query budget. Our method defines a reward\nfunction based on normalized token likelihoods, capturing how well a subset of\nsegments supports the original model response. Unlike traditional\nperturbation-based attribution methods such as SHAP, which sample subsets\nuniformly and incur high computational costs, our approach adaptively balances\nexploration and exploitation by leveraging posterior estimates of segment\nrelevance. This leads to substantially improved query efficiency while\nmaintaining high attribution fidelity. Extensive experiments on diverse\ndatasets and LLMs demonstrate that our method achieves competitive attribution\nquality with fewer model queries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u7684\u4e0a\u4e0b\u6587\u5f52\u56e0\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u6c64\u666e\u68ee\u91c7\u6837\uff08CTS\uff09\u9ad8\u6548\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b50\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u6548\u7387\u5e76\u4fdd\u6301\u4e86\u9ad8\u5f52\u56e0\u4fdd\u771f\u5ea6\u3002", "motivation": "\u7406\u89e3\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u4e2d\u54ea\u4e9b\u90e8\u5206\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u6709\u8d21\u732e\uff0c\u5bf9\u4e8e\u6784\u5efa\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u7684\u751f\u6210\u5f0f\u95ee\u7b54\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6270\u52a8\u5f52\u56e0\u65b9\u6cd5\uff08\u5982SHAP\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u5f52\u56e0\u95ee\u9898\u5efa\u6a21\u4e3a\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u95ee\u9898\uff0c\u6bcf\u4e2a\u4e0a\u4e0b\u6587\u7247\u6bb5\u89c6\u4e3a\u4e00\u4e2a\u8001\u864e\u673a\u81c2\uff0c\u91c7\u7528\u7ec4\u5408\u6c64\u666e\u68ee\u91c7\u6837\uff08CTS\uff09\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u9ad8\u6548\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b50\u96c6\u3002\u901a\u8fc7\u57fa\u4e8e\u5f52\u4e00\u5316\u6807\u8bb0\u4f3c\u7136\u7684\u5956\u52b1\u51fd\u6570\u8bc4\u4f30\u5b50\u96c6\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u652f\u6301\u7a0b\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ee5\u8f83\u5c11\u7684\u6a21\u578b\u67e5\u8be2\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5f52\u56e0\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u4e0a\u4e0b\u6587\u5f52\u56e0\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4e3a\u751f\u6210\u5f0f\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u7684\u4e0a\u4e0b\u6587\u5f52\u56e0", "abstract_zh": "\u7406\u89e3\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u4e2d\u54ea\u4e9b\u90e8\u5206\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u6709\u8d21\u732e\uff0c\u5bf9\u4e8e\u6784\u5efa\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u7684\u751f\u6210\u5f0f\u95ee\u7b54\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u5f52\u56e0\u95ee\u9898\u5efa\u6a21\u4e3a\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u95ee\u9898\u3002\u6bcf\u4e2a\u4e0a\u4e0b\u6587\u7247\u6bb5\u88ab\u89c6\u4e3a\u4e00\u4e2a\u8001\u864e\u673a\u81c2\uff0c\u5e76\u91c7\u7528\u7ec4\u5408\u6c64\u666e\u68ee\u91c7\u6837\uff08CTS\uff09\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u9ad8\u6548\u63a2\u7d22\u6307\u6570\u7ea7\u5927\u7684\u4e0a\u4e0b\u6587\u5b50\u96c6\u7a7a\u95f4\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u5f52\u4e00\u5316\u6807\u8bb0\u4f3c\u7136\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\uff0c\u6355\u6349\u5b50\u96c6\u5bf9\u539f\u59cb\u6a21\u578b\u54cd\u5e94\u7684\u652f\u6301\u7a0b\u5ea6\u3002\u4e0e\u4f20\u7edf\u57fa\u4e8e\u6270\u52a8\u7684\u5f52\u56e0\u65b9\u6cd5\uff08\u5982SHAP\uff09\u4e0d\u540c\uff0c\u540e\u8005\u5747\u5300\u91c7\u6837\u5b50\u96c6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u7247\u6bb5\u76f8\u5173\u6027\u7684\u540e\u9a8c\u4f30\u8ba1\u81ea\u9002\u5e94\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002\u8fd9\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5f52\u56e0\u4fdd\u771f\u5ea6\u3002\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ee5\u8f83\u5c11\u7684\u6a21\u578b\u67e5\u8be2\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5f52\u56e0\u8d28\u91cf\u3002"}}
{"id": "2506.20081", "pdf": "https://arxiv.org/pdf/2506.20081", "abs": "https://arxiv.org/abs/2506.20081", "authors": ["Dhruv Gupta", "Gayathri Ganesh Lakshmy", "Yiqing Xie"], "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Code Generation (RACG) is a critical technique for\nenhancing code generation by retrieving relevant information. In this work, we\nconduct an in-depth analysis of code retrieval by systematically masking\nspecific features while preserving code functionality. Our discoveries include:\n(1) although trained on code, current retrievers heavily rely on surface-level\ntextual features (e.g., docstrings, identifier names), and (2) they exhibit a\nstrong bias towards well-documented code, even if the documentation is\nirrelevant.Based on our discoveries, we propose SACL, a framework that enriches\ntextual information and reduces bias by augmenting code or structural knowledge\nwith semantic information. Extensive experiments show that SACL substantially\nimproves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /\nMBPP / SWE-Bench-Lite), which also leads to better code generation performance\n(e.g., by 4.88% Pass@1 on HumanEval).", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7801\u68c0\u7d22\u6a21\u578b\u5bf9\u8868\u9762\u6587\u672c\u7279\u5f81\uff08\u5982\u6587\u6863\u5b57\u7b26\u4e32\u548c\u6807\u8bc6\u7b26\u540d\u79f0\uff09\u7684\u4f9d\u8d56\u53ca\u5176\u5bf9\u6587\u6863\u5316\u4ee3\u7801\u7684\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSACL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u91cd\u65b0\u6392\u5e8f\u548c\u5b9a\u4f4d\u6765\u51cf\u5c11\u504f\u89c1\u5e76\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSACL\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u68c0\u7d22\u548c\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u68c0\u7d22\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u6587\u672c\u7279\u5f81\uff0c\u4e14\u5bf9\u6587\u6863\u5316\u4ee3\u7801\u5b58\u5728\u504f\u89c1\uff0c\u8fd9\u5f71\u54cd\u4e86\u68c0\u7d22\u548c\u751f\u6210\u7684\u51c6\u786e\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u504f\u89c1\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4ee3\u7801\u68c0\u7d22\u548c\u751f\u6210\u7684\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51faSACL\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u91cd\u65b0\u6392\u5e8f\u548c\u5b9a\u4f4d\uff0c\u51cf\u5c11\u5bf9\u8868\u9762\u6587\u672c\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u5e76\u964d\u4f4e\u5bf9\u6587\u6863\u5316\u4ee3\u7801\u7684\u504f\u89c1\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u7cfb\u7edf\u6027\u5730\u63a9\u76d6\u7279\u5b9a\u7279\u5f81\u4ee5\u5206\u6790\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u4ee3\u7801\u6216\u7ed3\u6784\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSACL\u5728HumanEval\u3001MBPP\u548cSWE-Bench-Lite\u6570\u636e\u96c6\u4e0a\u7684Recall@1\u5206\u522b\u63d0\u9ad8\u4e8612.8%\u30019.4%\u548c7.0%\uff0c\u540c\u65f6\u4ee3\u7801\u751f\u6210\u7684Pass@1\u4e5f\u63d0\u5347\u4e864.88%\u3002", "conclusion": "SACL\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u6709\u6548\u51cf\u5c11\u4e86\u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u6587\u672c\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u751f\u6210\u6027\u80fd\uff0c\u4e3a\u4ee3\u7801\u68c0\u7d22\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SACL\uff1a\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u91cd\u65b0\u6392\u5e8f\u548c\u5b9a\u4f4d\u7406\u89e3\u5e76\u51cf\u5c11\u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u6587\u672c\u504f\u89c1", "abstract_zh": "\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\uff08RACG\uff09\u662f\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u7684\u5173\u952e\u6280\u672f\u3002\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u63a9\u76d6\u7279\u5b9a\u7279\u5f81\u540c\u65f6\u4fdd\u7559\u4ee3\u7801\u529f\u80fd\uff0c\u5bf9\u4ee3\u7801\u68c0\u7d22\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\u5c3d\u7ba1\u57fa\u4e8e\u4ee3\u7801\u8bad\u7ec3\uff0c\u5f53\u524d\u68c0\u7d22\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u8868\u9762\u6587\u672c\u7279\u5f81\uff08\u5982\u6587\u6863\u5b57\u7b26\u4e32\u3001\u6807\u8bc6\u7b26\u540d\u79f0\uff09\uff1b\uff082\uff09\u5b83\u4eec\u5bf9\u6587\u6863\u5316\u4ee3\u7801\u8868\u73b0\u51fa\u5f3a\u70c8\u504f\u89c1\uff0c\u5373\u4f7f\u6587\u6863\u65e0\u5173\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51faSACL\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u4ee3\u7801\u6216\u7ed3\u6784\u77e5\u8bc6\u4ee5\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u5e76\u51cf\u5c11\u504f\u89c1\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSACL\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u68c0\u7d22\u6027\u80fd\uff08\u5982\u5728HumanEval/MBPP/SWE-Bench-Lite\u4e0a\u7684Recall@1\u5206\u522b\u63d0\u9ad8\u4e8612.8%/9.4%/7.0%\uff09\uff0c\u540c\u65f6\u4e5f\u6539\u5584\u4e86\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff08\u5982HumanEval\u4e0a\u7684Pass@1\u63d0\u9ad8\u4e864.88%\uff09\u3002"}}
{"id": "2506.20066", "pdf": "https://arxiv.org/pdf/2506.20066", "abs": "https://arxiv.org/abs/2506.20066", "authors": ["Hsiang-Wei Huang", "Wenhao Chai", "Kuang-Ming Chen", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "title": "ToSA: Token Merging with Spatial Awareness", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "Token merging has emerged as an effective strategy to accelerate Vision\nTransformers (ViT) by reducing computational costs. However, existing methods\nprimarily rely on the visual token's feature similarity for token merging,\noverlooking the potential of integrating spatial information, which can serve\nas a reliable criterion for token merging in the early layers of ViT, where the\nvisual tokens only possess weak visual information. In this paper, we propose\nToSA, a novel token merging method that combines both semantic and spatial\nawareness to guide the token merging process. ToSA leverages the depth image as\ninput to generate pseudo spatial tokens, which serve as auxiliary spatial\ninformation for the visual token merging process. With the introduced spatial\nawareness, ToSA achieves a more informed merging strategy that better preserves\ncritical scene structure. Experimental results demonstrate that ToSA\noutperforms previous token merging methods across multiple benchmarks on visual\nand embodied question answering while largely reducing the runtime of the ViT,\nmaking it an efficient solution for ViT acceleration. The code will be\navailable at: https://github.com/hsiangwei0903/ToSA", "AI": {"tldr": "ToSA\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u4f2a\u7a7a\u95f4\u4ee4\u724c\uff0c\u4f18\u5316\u89c6\u89c9Transformer\uff08ViT\uff09\u7684\u52a0\u901f\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u573a\u666f\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u4fe1\u606f\u5728ViT\u65e9\u671f\u5c42\u4e2d\u7684\u6f5c\u5728\u4f5c\u7528\u3002ToSA\u65e8\u5728\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u4fe1\u606f\uff0c\u63d0\u5347\u4ee4\u724c\u5408\u5e76\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "ToSA\u5229\u7528\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u4f2a\u7a7a\u95f4\u4ee4\u724c\uff0c\u4f5c\u4e3a\u89c6\u89c9\u4ee4\u724c\u5408\u5e76\u7684\u8f85\u52a9\u7a7a\u95f4\u4fe1\u606f\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u611f\u77e5\u6307\u5bfc\u5408\u5e76\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cToSA\u5728\u89c6\u89c9\u548c\u5177\u8eab\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11ViT\u8fd0\u884c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u573a\u666f\u7ed3\u6784\u3002", "conclusion": "ToSA\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684ViT\u52a0\u901f\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002", "paper_title_zh": "ToSA\uff1a\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u7684\u4ee4\u724c\u5408\u5e76", "abstract_zh": "\u4ee4\u724c\u5408\u5e76\u5df2\u6210\u4e3a\u52a0\u901f\u89c6\u89c9Transformer\uff08ViT\uff09\u7684\u6709\u6548\u7b56\u7565\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u4ee4\u724c\u7684\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u4fe1\u606f\u7684\u6f5c\u529b\u3002\u7a7a\u95f4\u4fe1\u606f\u53ef\u4f5c\u4e3aViT\u65e9\u671f\u5c42\u4e2d\u4ee4\u724c\u5408\u5e76\u7684\u53ef\u9760\u6807\u51c6\uff0c\u6b64\u65f6\u89c6\u89c9\u4ee4\u724c\u4ec5\u5305\u542b\u8f83\u5f31\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51faToSA\uff0c\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u65b0\u578b\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\u3002ToSA\u5229\u7528\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u4f2a\u7a7a\u95f4\u4ee4\u724c\uff0c\u4f5c\u4e3a\u89c6\u89c9\u4ee4\u724c\u5408\u5e76\u7684\u8f85\u52a9\u7a7a\u95f4\u4fe1\u606f\u3002\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u611f\u77e5\uff0cToSA\u5b9e\u73b0\u4e86\u66f4\u660e\u667a\u7684\u5408\u5e76\u7b56\u7565\uff0c\u66f4\u597d\u5730\u4fdd\u7559\u5173\u952e\u573a\u666f\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToSA\u5728\u89c6\u89c9\u548c\u5177\u8eab\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11ViT\u8fd0\u884c\u65f6\u95f4\uff0c\u6210\u4e3aViT\u52a0\u901f\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u5c06\u5728\u4ee5\u4e0b\u7f51\u5740\u63d0\u4f9b\uff1ahttps://github.com/hsiangwei0903/ToSA"}}
{"id": "2506.20008", "pdf": "https://arxiv.org/pdf/2506.20008", "abs": "https://arxiv.org/abs/2506.20008", "authors": ["Abdul Basit", "Minghao Shao", "Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "categories": ["cs.AI", "cs.PL", "cs.SE", "68T50, 81P68, 68T07, 68T20", "I.2.7; I.2.2"], "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QHackBench\uff0c\u4e00\u4e2a\u57fa\u4e8ePennyLane\u7684\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u666e\u901a\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6548\u679c\uff0c\u5e76\u5f15\u5165\u591a\u4ee3\u7406\u8bc4\u4f30\u6d41\u7a0b\u4f18\u5316\u7ed3\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0cRAG\u5728\u590d\u6742\u91cf\u5b50\u7b97\u6cd5\u4e2d\u8868\u73b0\u4e0e\u666e\u901a\u63d0\u793a\u76f8\u5f53\uff0c\u4e14\u591a\u4ee3\u7406\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5b9e\u9645\u91cf\u5b50\u7f16\u7a0b\u6311\u6218\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63a8\u52a8AI\u8f85\u52a9\u91cf\u5b50\u7f16\u7a0b\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u4eceQuantum Hackathon\uff08QHack\uff09\u7ade\u8d5b\u4e2d\u63d0\u53d6\u771f\u5b9e\u6311\u6218\uff0c\u6784\u5efa\u4e86QHackBench\u6570\u636e\u96c6\u3002\u901a\u8fc7\u666e\u901a\u63d0\u793a\u548cRAG\u4e24\u79cd\u65b9\u5f0f\u8bc4\u4f30\u6a21\u578b\u7684\u529f\u80fd\u6b63\u786e\u6027\u3001\u8bed\u6cd5\u6709\u6548\u6027\u548c\u6267\u884c\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u5f15\u5165\u591a\u4ee3\u7406\u8bc4\u4f30\u6d41\u7a0b\u8fed\u4ee3\u4f18\u5316\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRAG\u589e\u5f3a\u7684\u6a21\u578b\u5728\u590d\u6742\u91cf\u5b50\u7b97\u6cd5\u4e2d\u8868\u73b0\u4e0e\u666e\u901a\u63d0\u793a\u76f8\u5f53\u3002\u591a\u4ee3\u7406\u8bc4\u4f30\u6d41\u7a0b\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6267\u884c\u6210\u529f\u7387\uff0c\u5c24\u5176\u5728\u89e3\u51b3\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "QHackBench\u4e3aAI\u8f85\u52a9\u91cf\u5b50\u7f16\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\uff0cRAG\u548c\u591a\u4ee3\u7406\u6d41\u7a0b\u5c55\u793a\u4e86\u4f18\u5316\u6f5c\u529b\u3002\u7814\u7a76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "paper_title_zh": "QHackBench\uff1a\u57fa\u4e8ePennyLane Hackathon\u6311\u6218\u7684\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u5229\u7528Quantum Hackathon\uff08QHack\uff09\u7684\u5b9e\u9645\u6311\u6218\uff0c\u5bf9\u57fa\u4e8ePennyLane\u7684\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86QHackBench\uff0c\u4e00\u4e2a\u6e90\u81eaQHack\u7ade\u8d5b\u7684\u65b0\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u666e\u901a\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002\u6211\u4eec\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u6db5\u76d6\u4e86\u529f\u80fd\u6b63\u786e\u6027\u3001\u8bed\u6cd5\u6709\u6548\u6027\u548c\u4e0d\u540c\u96be\u5ea6\u6311\u6218\u7684\u6267\u884c\u6210\u529f\u7387\u3002\u7ed3\u679c\u8868\u660e\uff0cRAG\u589e\u5f3a\u7684\u6a21\u578b\u5728\u590d\u6742\u91cf\u5b50\u7b97\u6cd5\u4e2d\u8868\u73b0\u4e0e\u666e\u901a\u63d0\u793a\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u4ee3\u7406\u8bc4\u4f30\u6d41\u7a0b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u8fdb\u4e00\u6b65\u63d0\u5347\u6267\u884c\u6210\u529f\u7387\u3002\u4e3a\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5c06\u516c\u5f00QHackBench\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u63a8\u52a8AI\u8f85\u52a9\u91cf\u5b50\u7f16\u7a0b\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2506.20083", "pdf": "https://arxiv.org/pdf/2506.20083", "abs": "https://arxiv.org/abs/2506.20083", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder", "categories": ["cs.CL"], "comment": "In progress", "summary": "Integrating compositional and symbolic properties into current distributional\nsemantic spaces can enhance the interpretability, controllability,\ncompositionality, and generalisation capabilities of Transformer-based\nauto-regressive language models (LMs). In this survey, we offer a novel\nperspective on latent space geometry through the lens of compositional\nsemantics, a direction we refer to as \\textit{semantic representation\nlearning}. This direction enables a bridge between symbolic and distributional\nsemantics, helping to mitigate the gap between them. We review and compare\nthree mainstream autoencoder architectures-Variational AutoEncoder (VAE),\nVector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the\ndistinctive latent geometries they induce in relation to semantic structure and\ninterpretability.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff08\u5982VAE\u3001VQVAE\u548cSAE\uff09\u5728\u6f5c\u5728\u8bed\u4e49\u51e0\u4f55\u4e2d\u7ed3\u5408\u7ec4\u5408\u8bed\u4e49\u4e0e\u5206\u5e03\u8bed\u4e49\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u8868\u793a\u4e0a\u5b58\u5728\u7ec4\u5408\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6f5c\u5728\u8bed\u4e49\u51e0\u4f55\u7684\u7814\u7a76\uff0c\u5f25\u5408\u7b26\u53f7\u8bed\u4e49\u4e0e\u5206\u5e03\u8bed\u4e49\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e3b\u6d41\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff08VAE\u3001VQVAE\u548cSAE\uff09\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u6f5c\u5728\u8bed\u4e49\u51e0\u4f55\u4e2d\u5bf9\u8bed\u4e49\u7ed3\u6784\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u81ea\u7f16\u7801\u5668\u67b6\u6784\u5728\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u7684\u8bed\u4e49\u7279\u6027\uff0c\u4e3a\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "\u901a\u8fc7\u6f5c\u5728\u8bed\u4e49\u51e0\u4f55\u7684\u7814\u7a76\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ed3\u5408\u7ec4\u5408\u8bed\u4e49\u4e0e\u5206\u5e03\u8bed\u4e49\uff0c\u4ece\u800c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "\u7ed3\u5408\u7ec4\u5408\u8bed\u4e49\u4e0e\u5206\u5e03\u8bed\u4e49\uff1a\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u8bed\u4e49\u51e0\u4f55\u7efc\u8ff0", "abstract_zh": "\u5c06\u7ec4\u5408\u6027\u548c\u7b26\u53f7\u6027\u7279\u5f81\u878d\u5165\u5f53\u524d\u7684\u5206\u5e03\u8bed\u4e49\u7a7a\u95f4\uff0c\u53ef\u4ee5\u63d0\u5347\u57fa\u4e8eTransformer\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u63a7\u6027\u3001\u7ec4\u5408\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u7efc\u8ff0\u4ece\u7ec4\u5408\u8bed\u4e49\u7684\u89d2\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7684\u65b0\u89c6\u89d2\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u201c\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u201d\u3002\u8fd9\u4e00\u65b9\u5411\u80fd\u591f\u5f25\u5408\u7b26\u53f7\u8bed\u4e49\u4e0e\u5206\u5e03\u8bed\u4e49\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002\u6211\u4eec\u56de\u987e\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e3b\u6d41\u81ea\u7f16\u7801\u5668\u67b6\u6784\u2014\u2014\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u3001\u5411\u91cf\u91cf\u5316VAE\uff08VQVAE\uff09\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u8bed\u4e49\u7ed3\u6784\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u6240\u8bf1\u5bfc\u7684\u72ec\u7279\u6f5c\u5728\u51e0\u4f55\u7279\u6027\u3002"}}
{"id": "2506.20103", "pdf": "https://arxiv.org/pdf/2506.20103", "abs": "https://arxiv.org/abs/2506.20103", "authors": ["Jiahao Lin", "Weixuan Peng", "Bojia Zi", "Yifeng Gao", "Xianbiao Qi", "Xingjun Ma", "Yu-Gang Jiang"], "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos", "categories": ["cs.CV", "cs.AI", "I.4"], "comment": "7 page,4 figures,2 tables", "summary": "Recent advances in deep generative models have led to significant progress in\nvideo generation, yet the fidelity of AI-generated videos remains limited.\nSynthesized content often exhibits visual artifacts such as temporally\ninconsistent motion, physically implausible trajectories, unnatural object\ndeformations, and local blurring that undermine realism and user trust.\nAccurate detection and spatial localization of these artifacts are crucial for\nboth automated quality control and for guiding the development of improved\ngenerative models. However, the research community currently lacks a\ncomprehensive benchmark specifically designed for artifact localization in AI\ngenerated videos. Existing datasets either restrict themselves to video or\nframe level detection or lack the fine-grained spatial annotations necessary\nfor evaluating localization methods. To address this gap, we introduce\nBrokenVideos, a benchmark dataset of 3,254 AI-generated videos with\nmeticulously annotated, pixel-level masks highlighting regions of visual\ncorruption. Each annotation is validated through detailed human inspection to\nensure high quality ground truth. Our experiments show that training state of\nthe art artifact detection models and multi modal large language models (MLLMs)\non BrokenVideos significantly improves their ability to localize corrupted\nregions. Through extensive evaluation, we demonstrate that BrokenVideos\nestablishes a critical foundation for benchmarking and advancing research on\nartifact localization in generative video models. The dataset is available at:\nhttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BrokenVideos\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8eAI\u751f\u6210\u89c6\u9891\u4e2d\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u5b9a\u4f4d\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b3,254\u4e2a\u89c6\u9891\uff0c\u5e76\u63d0\u4f9b\u4e86\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u4f2a\u5f71\u533a\u57df\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u5728\u4f2a\u5f71\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u7684\u89c6\u9891\u5e38\u5305\u542b\u89c6\u89c9\u4f2a\u5f71\uff08\u5982\u65f6\u95f4\u4e0d\u4e00\u81f4\u7684\u8fd0\u52a8\u3001\u7269\u7406\u4e0d\u5408\u7406\u7684\u8f68\u8ff9\u7b49\uff09\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u4f2a\u5f71\u5b9a\u4f4d\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u8981\u4e48\u4ec5\u652f\u6301\u89c6\u9891\u6216\u5e27\u7ea7\u68c0\u6d4b\uff0c\u8981\u4e48\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u6807\u6ce8\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86BrokenVideos\u6570\u636e\u96c6\uff0c\u5305\u542b3,254\u4e2aAI\u751f\u6210\u89c6\u9891\uff0c\u5e76\u8fdb\u884c\u4e86\u50cf\u7d20\u7ea7\u7684\u4f2a\u5f71\u533a\u57df\u6807\u6ce8\uff0c\u6bcf\u9879\u6807\u6ce8\u5747\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u9ad8\u8d28\u91cf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bad\u7ec3\u4e86\u5148\u8fdb\u7684\u4f2a\u5f71\u68c0\u6d4b\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528BrokenVideos\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4f2a\u5f71\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u4f2a\u5f71\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "conclusion": "BrokenVideos\u586b\u8865\u4e86AI\u751f\u6210\u89c6\u9891\u4f2a\u5f71\u5b9a\u4f4d\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u6539\u8fdb\u3002", "paper_title_zh": "BrokenVideos\uff1aAI\u751f\u6210\u89c6\u9891\u4e2d\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u5b9a\u4f4d\u7684\u57fa\u51c6\u6570\u636e\u96c6", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46AI\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u4ecd\u6709\u9650\u5236\u3002\u5408\u6210\u5185\u5bb9\u5e38\u51fa\u73b0\u89c6\u89c9\u4f2a\u5f71\uff0c\u5982\u65f6\u95f4\u4e0d\u4e00\u81f4\u7684\u8fd0\u52a8\u3001\u7269\u7406\u4e0d\u5408\u7406\u7684\u8f68\u8ff9\u3001\u4e0d\u81ea\u7136\u7684\u7269\u4f53\u53d8\u5f62\u548c\u5c40\u90e8\u6a21\u7cca\uff0c\u8fd9\u4e9b\u4f2a\u5f71\u964d\u4f4e\u4e86\u771f\u5b9e\u6027\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\u3002\u51c6\u786e\u68c0\u6d4b\u548c\u7a7a\u95f4\u5b9a\u4f4d\u8fd9\u4e9b\u4f2a\u5f71\u5bf9\u4e8e\u81ea\u52a8\u5316\u8d28\u91cf\u63a7\u5236\u548c\u6539\u8fdb\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7814\u7a76\u793e\u533a\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8eAI\u751f\u6210\u89c6\u9891\u4f2a\u5f71\u5b9a\u4f4d\u7684\u5168\u9762\u57fa\u51c6\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u8981\u4e48\u4ec5\u652f\u6301\u89c6\u9891\u6216\u5e27\u7ea7\u68c0\u6d4b\uff0c\u8981\u4e48\u7f3a\u4e4f\u8bc4\u4f30\u5b9a\u4f4d\u65b9\u6cd5\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u6807\u6ce8\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86BrokenVideos\uff0c\u4e00\u4e2a\u5305\u542b3,254\u4e2aAI\u751f\u6210\u89c6\u9891\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u89c6\u9891\u5747\u7ecf\u8fc7\u7cbe\u5fc3\u6807\u6ce8\uff0c\u63d0\u4f9b\u4e86\u50cf\u7d20\u7ea7\u7684\u4f2a\u5f71\u533a\u57df\u63a9\u7801\u3002\u6bcf\u9879\u6807\u6ce8\u5747\u901a\u8fc7\u8be6\u7ec6\u7684\u4eba\u5de5\u68c0\u67e5\u4ee5\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u6807\u6ce8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728BrokenVideos\u4e0a\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u4f2a\u5f71\u68c0\u6d4b\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u4f2a\u5f71\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660eBrokenVideos\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u4f2a\u5f71\u5b9a\u4f4d\u7814\u7a76\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002\u6570\u636e\u96c6\u5730\u5740\uff1ahttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/\u3002"}}
{"id": "2506.20009", "pdf": "https://arxiv.org/pdf/2506.20009", "abs": "https://arxiv.org/abs/2506.20009", "authors": ["Konstantinos Vrettos", "Michail E. Klontzas"], "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "18 pages, 3 Figures", "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u5b9a\u5236\u5316\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u4efb\u52a1\uff0c\u5176\u6027\u80fd\u548c\u80fd\u6548\u4f18\u4e8e\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u548cDeepSeek\uff09\uff0c\u540c\u65f6\u51cf\u5c11\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u8d44\u6e90\u9700\u6c42\u548c\u9690\u79c1\u5b89\u5168\u95ee\u9898\u5f15\u53d1\u5173\u6ce8\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u73af\u4fdd\u7684\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u5b9a\u5236\u7684RAG\u6846\u67b6\uff0c\u76d1\u6d4b\u80fd\u6e90\u4f7f\u7528\u548c\u78b3\u6392\u653e\uff0c\u5e76\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982llama3.1:8b\u548cmedgemma-4b-it\uff09\u6784\u5efaRAG\u6a21\u578b\uff0c\u4e0e\u5546\u4e1a\u6a21\u578b\uff08\u5982DeepSeekV3-R1\u548co4-mini\uff09\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u5b9a\u5236RAG\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u80fd\u6548\u4e0a\u5747\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\u3002llama3.1:8B RAG\u51c6\u786e\u7387\u6700\u9ad8\uff0858.5%\uff09\uff0c\u80fd\u8017\u6700\u4f4e\uff08\u6bcf\u5343\u74e6\u65f60.52\u6027\u80fd\uff0c\u603b\u78b3\u6392\u653e473\u514b\uff09\uff0c\u6bd4o4-mini\u8282\u80fd172%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u672c\u5730\u5316RAG\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\uff0c\u4e14\u66f4\u73af\u4fdd\u3002\u6a21\u5757\u5316\u6846\u67b6\u7b26\u5408\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u63a8\u52a8\u53ef\u6301\u7eedAI\u53d1\u5c55\u3002", "paper_title_zh": "\u7cbe\u51c6\u4e14\u9ad8\u6548\uff1a\u672c\u5730\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u80cc\u666f\uff1a\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u73af\u5883\u548c\u4f26\u7406\u95ee\u9898\u7684\u5173\u6ce8\u3002\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u548cDeepSeek\uff09\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e14\u5b58\u5728\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u5b9a\u5236\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u4efb\u52a1\uff0c\u5e76\u76d1\u6d4b\u5176\u80fd\u6e90\u4f7f\u7528\u548c\u78b3\u6392\u653e\u3002\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982llama3.1:8b\u548cmedgemma-4b-it\uff09\u6784\u5efaRAG\u6a21\u578b\uff0c\u5e76\u4e0e\u5546\u4e1a\u6a21\u578b\uff08\u5982DeepSeekV3-R1\u548co4-mini\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002\u7ed3\u679c\uff1a\u5b9a\u5236RAG\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u80fd\u6548\u4e0a\u5747\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\u3002llama3.1:8B RAG\u51c6\u786e\u7387\u6700\u9ad8\uff0858.5%\uff09\uff0c\u80fd\u8017\u6700\u4f4e\uff08\u6bcf\u5343\u74e6\u65f60.52\u6027\u80fd\uff0c\u603b\u78b3\u6392\u653e473\u514b\uff09\uff0c\u6bd4o4-mini\u8282\u80fd172%\u3002\u7ed3\u8bba\uff1a\u7814\u7a76\u8868\u660e\uff0c\u672c\u5730\u5316RAG\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\uff0c\u4e14\u66f4\u73af\u4fdd\u3002\u6a21\u5757\u5316\u6846\u67b6\u7b26\u5408\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u63a8\u52a8\u53ef\u6301\u7eedAI\u53d1\u5c55\u3002"}}
{"id": "2506.20093", "pdf": "https://arxiv.org/pdf/2506.20093", "abs": "https://arxiv.org/abs/2506.20093", "authors": ["Yilin Wang", "Peixuan Lei", "Jie Song", "Yuzhe Hao", "Tao Chen", "Yuxuan Zhang", "Lei Jia", "Yuanxiang Li", "Zhongyu Wei"], "title": "ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset", "categories": ["cs.CL"], "comment": null, "summary": "Time-series data are critical in diverse applications, such as industrial\nmonitoring, medical diagnostics, and climate research. However, effectively\nintegrating these high-dimensional temporal signals with natural language for\ndynamic, interactive tasks remains a significant challenge. To address this, we\nintroduce the Time-Series Question Answering (Time-Series QA) task and release\nEngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset\ndesigned to capture complex interactions between time-series signals and\nnatural language. Building on this resource, we propose the Instruct Time\nTransformer (ITFormer), a novel framework that bridges time-series encoders\nwith frozen large language models (LLMs). ITFormer effectively extracts,\naligns, and fuses temporal and textual features, achieving a strong improvement\nin QA accuracy over strong baselines with fewer than 1\\% additional trainable\nparameters. By combining computational efficiency with robust cross-modal\nmodeling, our work establishes a adaptable paradigm for integrating temporal\ndata with natural language, paving the way for new research and applications in\nmulti-modal AI. More details about the project, including datasets and code,\nare available at: https://pandalin98.github.io/itformer_site/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6ITFormer\uff0c\u7528\u4e8e\u6865\u63a5\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u6570\u636e\u96c6EngineMT-QA\u3002ITFormer\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u4e0e\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u5de5\u4e1a\u76d1\u63a7\u3001\u533b\u7597\u8bca\u65ad\u548c\u6c14\u5019\u7814\u7a76\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u4e0e\u81ea\u7136\u8bed\u8a00\u6709\u6548\u7ed3\u5408\u4ee5\u652f\u6301\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Instruct Time Transformer (ITFormer)\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u3001\u5bf9\u9f50\u548c\u878d\u5408\u65f6\u95f4\u5e8f\u5217\u4e0e\u6587\u672c\u7279\u5f81\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u548c\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u5efa\u6a21\u3002", "result": "ITFormer\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4ec5\u9700\u5c11\u4e8e1%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u8303\u5f0f\uff0c\u4e3a\u591a\u6a21\u6001AI\u7684\u7814\u7a76\u548c\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "ITFormer\uff1a\u6865\u63a5\u65f6\u95f4\u5e8f\u5217\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u591a\u6a21\u6001\u95ee\u7b54\u6846\u67b6\u53ca\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u6570\u636e\u96c6", "abstract_zh": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u5de5\u4e1a\u76d1\u63a7\u3001\u533b\u7597\u8bca\u65ad\u548c\u6c14\u5019\u7814\u7a76\u7b49\u591a\u6837\u5316\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5982\u4f55\u5c06\u8fd9\u4e9b\u9ad8\u7ef4\u65f6\u95f4\u4fe1\u53f7\u4e0e\u81ea\u7136\u8bed\u8a00\u6709\u6548\u7ed3\u5408\u4ee5\u652f\u6301\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\uff08Time-Series QA\uff09\u4efb\u52a1\uff0c\u5e76\u53d1\u5e03\u4e86EngineMT-QA\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u4efb\u52a1\u3001\u65f6\u95f4-\u6587\u672c\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u4e0e\u81ea\u7136\u8bed\u8a00\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002\u57fa\u4e8e\u8fd9\u4e00\u8d44\u6e90\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Instruct Time Transformer (ITFormer)\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u4e0e\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76f8\u7ed3\u5408\u3002ITFormer\u6709\u6548\u63d0\u53d6\u3001\u5bf9\u9f50\u548c\u878d\u5408\u65f6\u95f4\u4e0e\u6587\u672c\u7279\u5f81\uff0c\u5728\u95ee\u7b54\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u4ec5\u9700\u5c11\u4e8e1%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u901a\u8fc7\u5c06\u8ba1\u7b97\u6548\u7387\u4e0e\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u65f6\u95f4\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9002\u5e94\u6027\u5f3a\u7684\u8303\u5f0f\uff0c\u4e3a\u591a\u6a21\u6001AI\u7684\u7814\u7a76\u548c\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002\u66f4\u591a\u9879\u76ee\u8be6\u60c5\uff0c\u5305\u62ec\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u8bf7\u8bbf\u95ee\uff1ahttps://pandalin98.github.io/itformer_site/"}}
{"id": "2506.20134", "pdf": "https://arxiv.org/pdf/2506.20134", "abs": "https://arxiv.org/abs/2506.20134", "authors": ["Ningwei Xie", "Zizi Tian", "Lei Yang", "Xiao-Ping Zhang", "Meng Guo", "Jie Li"], "title": "From 2D to 3D Cognition: A Brief Survey of General World Models", "categories": ["cs.CV"], "comment": null, "summary": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4ece2D\u611f\u77e5\u52303D\u8ba4\u77e5\u7684\u4e16\u754c\u6a21\u578b\u53d1\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e863D\u8868\u793a\u548c\u4e16\u754c\u77e5\u8bc6\u4e24\u5927\u6280\u672f\u9a71\u52a8\u529b\uff0c\u5e76\u63a2\u8ba8\u4e863D\u4e16\u754c\u5efa\u6a21\u7684\u4e09\u5927\u6838\u5fc3\u80fd\u529b\u53ca\u5176\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u53d1\u5c55\uff0c\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5b66\u4e60\u5916\u90e8\u4e16\u754c\u8868\u793a\u548c\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u8ba1\u7b97\u6846\u67b6\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4ece2D\u611f\u77e5\u52303D\u8ba4\u77e5\u7684\u8fc7\u6e21\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6982\u5ff5\u6846\u67b6\uff0c\u672c\u6587\u5bf9\u4ece2D\u611f\u77e5\u52303D\u8ba4\u77e5\u7684\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u4e86\u7ed3\u6784\u5316\u7efc\u8ff0\uff0c\u91cd\u70b9\u5206\u6790\u4e863D\u8868\u793a\u548c\u4e16\u754c\u77e5\u8bc6\u4e24\u5927\u6280\u672f\u9a71\u52a8\u529b\uff0c\u5e76\u63a2\u8ba8\u4e863D\u7269\u7406\u573a\u666f\u751f\u6210\u30013D\u7a7a\u95f4\u63a8\u7406\u548c3D\u7a7a\u95f4\u4ea4\u4e92\u4e09\u5927\u6838\u5fc3\u80fd\u529b\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e863D\u4e16\u754c\u5efa\u6a21\u7684\u6280\u672f\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u5177\u8eabAI\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u5b6a\u751f\u548c\u6e38\u620f/VR\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002\u540c\u65f6\uff0c\u6307\u51fa\u4e86\u6570\u636e\u3001\u5efa\u6a21\u548c\u90e8\u7f72\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a3D\u4e16\u754c\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u5f3a\u5927\u548c\u901a\u7528\u76843D\u4e16\u754c\u6a21\u578b\u7684\u5b9e\u73b0\u3002", "paper_title_zh": "\u4ece2D\u52303D\u8ba4\u77e5\uff1a\u901a\u7528\u4e16\u754c\u6a21\u578b\u7b80\u6790", "abstract_zh": "\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5b66\u4e60\u5916\u90e8\u4e16\u754c\u8868\u793a\u548c\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5728\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u53d1\u5c55\u4e2d\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u65e9\u671f\u7814\u7a76\u96c6\u4e2d\u4e8e2D\u89c6\u89c9\u611f\u77e5\u4e0e\u6a21\u62df\uff0c\u800c\u8fd1\u5e74\u6765\u76843D\u611f\u77e5\u751f\u6210\u4e16\u754c\u6a21\u578b\u5df2\u80fd\u5408\u6210\u51e0\u4f55\u4e00\u81f4\u4e14\u53ef\u4ea4\u4e92\u76843D\u73af\u5883\uff0c\u6807\u5fd7\u7740\u54113D\u7a7a\u95f4\u8ba4\u77e5\u7684\u8f6c\u53d8\u3002\u5c3d\u7ba1\u8fdb\u5c55\u8fc5\u901f\uff0c\u8be5\u9886\u57df\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u4ee5\u5206\u7c7b\u65b0\u5174\u6280\u672f\u5e76\u660e\u786e\u5176\u5728\u63a8\u52a83D\u8ba4\u77e5\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\u3002\u672c\u7efc\u8ff0\u901a\u8fc7\u5f15\u5165\u6982\u5ff5\u6846\u67b6\uff0c\u5bf9\u4ece2D\u611f\u77e5\u52303D\u8ba4\u77e5\u7684\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u4e86\u7ed3\u6784\u5316\u4e14\u524d\u77bb\u6027\u7684\u56de\u987e\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u6211\u4eec\u5f3a\u8c03\u4e863D\u8868\u793a\u548c\u4e16\u754c\u77e5\u8bc6\u4e24\u5927\u6280\u672f\u9a71\u52a8\u529b\u4f5c\u4e3a\u57fa\u7840\u652f\u67f1\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u5256\u6790\u4e86\u652f\u64913D\u4e16\u754c\u5efa\u6a21\u7684\u4e09\u5927\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff1a3D\u7269\u7406\u573a\u666f\u751f\u6210\u30013D\u7a7a\u95f4\u63a8\u7406\u548c3D\u7a7a\u95f4\u4ea4\u4e92\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u80fd\u529b\u5728\u5177\u8eabAI\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u5b6a\u751f\u548c\u6e38\u620f/VR\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002\u6700\u540e\uff0c\u6211\u4eec\u6307\u51fa\u4e86\u6570\u636e\u3001\u5efa\u6a21\u548c\u90e8\u7f72\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u63a8\u52a8\u66f4\u5f3a\u5927\u548c\u901a\u75283D\u4e16\u754c\u6a21\u578b\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2506.20018", "pdf": "https://arxiv.org/pdf/2506.20018", "abs": "https://arxiv.org/abs/2506.20018", "authors": ["Zechun Deng", "Ziwei Liu", "Ziqian Bi", "Junhao Song", "Chia Xin Liang", "Joe Yeong", "Junfeng Hao"], "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models", "categories": ["cs.AI", "cs.AR"], "comment": null, "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u4f4e\u5ef6\u8fdf\u53ef\u89e3\u91caAI\u6a21\u578b\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86AI\u9a71\u52a8\u7684\u51b3\u7b56\u5de5\u5177\u3001\u8fb9\u7f18\u7269\u8054\u7f51\u6280\u672f\u53ca\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u6a21\u578b\u538b\u7f29\u548c\u8fb9\u7f18\u8bbe\u5907\u4f18\u5316\u7684\u6280\u672f\u8fdb\u5c55\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4f4e\u5ef6\u8fdfAI\u6a21\u578b\u63d0\u5347\u51b3\u7b56\u6548\u7387\uff0c\u5e76\u63a8\u52a8AI\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u6280\u672f\u7684\u7ed3\u5408\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u73b0\u6709\u6280\u672f\uff0c\u5305\u62ecDeLLMa\u3001\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u548c\u8fb9\u7f18\u8bbe\u5907\u4f18\u5316\u7b56\u7565\uff0c\u5206\u6790\u5176\u5728\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u53d1\u7b56\u7565\u548c\u5e94\u7528\u9886\u57df\u7684\u5b9e\u7528\u89c6\u89d2\uff0c\u6307\u51fa\u9ad8\u6548\u7075\u6d3bAI\u652f\u6301\u7cfb\u7edf\u7684\u673a\u9047\uff0c\u5e76\u5c55\u793a\u4e86\u6280\u672f\u8fdb\u5c55\u5bf9\u51b3\u7b56\u652f\u6301\u7684\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u9886\u57df\u7684\u7a81\u7834\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86AI\u5728\u91cd\u5851\u5b9e\u65f6\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u5229\u7528\u4f4e\u5ef6\u8fdf\u53ef\u89e3\u91caAI\u6a21\u578b\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf", "abstract_zh": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u4f4e\u5ef6\u8fdfAI\u6a21\u578b\u7684\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86AI\u9a71\u52a8\u7684\u51b3\u7b56\u5de5\u5177\u3001\u8fb9\u7f18\u7269\u8054\u7f51\u6280\u672f\u53ca\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\u3002\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u8f85\u52a9\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u4e86DeLLMa\u3001\u6a21\u578b\u538b\u7f29\u6280\u672f\u548c\u8fb9\u7f18\u8bbe\u5907\u4f18\u5316\u7b49\u6280\u672f\u8fdb\u5c55\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8be6\u7ec6\u7efc\u8ff0\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u5f00\u53d1\u7b56\u7565\u548c\u5e94\u7528\u9886\u57df\u7684\u5b9e\u7528\u89c6\u89d2\uff0c\u4e3a\u9ad8\u6548\u7075\u6d3b\u7684AI\u652f\u6301\u7cfb\u7edf\u6307\u660e\u4e86\u65b9\u5411\u3002\u7ed3\u8bba\u4e3a\u8fd9\u4e00\u5feb\u901f\u53d8\u5316\u9886\u57df\u7684\u672a\u6765\u7a81\u7834\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7a81\u51fa\u4e86AI\u5bf9\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7684\u91cd\u5851\u6f5c\u529b\u3002"}}
{"id": "2506.20100", "pdf": "https://arxiv.org/pdf/2506.20100", "abs": "https://arxiv.org/abs/2506.20100", "authors": ["Vardhan Dongre", "Chi Gui", "Shubham Garg", "Hooshang Nayyeri", "Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Vikram S. Adve"], "title": "MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "66 pages, 32 figures, 23 tables", "summary": "We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning\nand decision-making in consultative interaction settings. Designed for the\nagriculture domain, MIRAGE captures the full complexity of expert consultations\nby combining natural user queries, expert-authored responses, and image-based\ncontext, offering a high-fidelity benchmark for evaluating models on grounded\nreasoning, clarification strategies, and long-form generation in a real-world,\nknowledge-intensive domain. Grounded in over 35,000 real user-expert\ninteractions and curated through a carefully designed multi-step pipeline,\nMIRAGE spans diverse crop health, pest diagnosis, and crop management\nscenarios. The benchmark includes more than 7,000 unique biological entities,\ncovering plant species, pests, and diseases, making it one of the most\ntaxonomically diverse benchmarks available for vision-language models, grounded\nin the real world. Unlike existing benchmarks that rely on well-specified user\ninputs and closed-set taxonomies, MIRAGE features underspecified, context-rich\nscenarios with open-world settings, requiring models to infer latent knowledge\ngaps, handle rare entities, and either proactively guide the interaction or\nrespond. Project Page: https://mirage-benchmark.github.io", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u519c\u4e1a\u4e13\u5bb6\u5bf9\u8bdd\u57fa\u51c6\uff0c\u7ed3\u5408\u7528\u6237\u67e5\u8be2\u3001\u4e13\u5bb6\u56de\u7b54\u548c\u56fe\u50cf\u4e0a\u4e0b\u6587\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u4f9d\u8d56\u660e\u786e\u8f93\u5165\u548c\u5c01\u95ed\u5206\u7c7b\uff0c\u800c\u519c\u4e1a\u9886\u57df\u7684\u4e13\u5bb6\u54a8\u8be2\u573a\u666f\u590d\u6742\u4e14\u5f00\u653e\uff0c\u9700\u8981\u6a21\u578b\u5177\u5907\u63a8\u7406\u548c\u4e3b\u52a8\u5f15\u5bfc\u80fd\u529b\u3002MIRAGE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e35,000+\u771f\u5b9e\u7528\u6237-\u4e13\u5bb6\u5bf9\u8bdd\uff0c\u901a\u8fc7\u591a\u6b65\u6d41\u7a0b\u6784\u5efaMIRAGE\u57fa\u51c6\uff0c\u6db5\u76d6\u4f5c\u7269\u5065\u5eb7\u3001\u75c5\u866b\u5bb3\u8bca\u65ad\u7b49\u573a\u666f\uff0c\u5305\u542b7,000+\u751f\u7269\u5b9e\u4f53\u3002", "result": "MIRAGE\u6210\u4e3a\u9996\u4e2a\u8986\u76d6\u5f00\u653e\u4e16\u754c\u3001\u591a\u6a21\u6001\u519c\u4e1a\u54a8\u8be2\u7684\u57fa\u51c6\uff0c\u652f\u6301\u6a21\u578b\u8bc4\u4f30\u63a8\u7406\u3001\u6f84\u6e05\u7b56\u7565\u548c\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u3002", "conclusion": "MIRAGE\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u5728\u519c\u4e1a\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u5f00\u653e\u573a\u666f\u4e0b\u7684\u4e13\u5bb6\u7ea7\u63a8\u7406\u7814\u7a76\u3002", "paper_title_zh": "MIRAGE\uff1a\u519c\u4e1a\u4e13\u5bb6\u5f15\u5bfc\u5bf9\u8bdd\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u641c\u7d22\u4e0e\u63a8\u7406\u57fa\u51c6", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86MIRAGE\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u4e13\u5bb6\u7ea7\u63a8\u7406\u548c\u51b3\u7b56\u7684\u65b0\u57fa\u51c6\uff0c\u4e13\u4e3a\u519c\u4e1a\u9886\u57df\u8bbe\u8ba1\u3002MIRAGE\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u7528\u6237\u67e5\u8be2\u3001\u4e13\u5bb6\u64b0\u5199\u7684\u56de\u7b54\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u4e0a\u4e0b\u6587\uff0c\u6355\u6349\u4e86\u4e13\u5bb6\u54a8\u8be2\u7684\u590d\u6742\u6027\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u4e2d\u7684\u63a8\u7406\u3001\u6f84\u6e05\u7b56\u7565\u548c\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u57fa\u51c6\u3002\u57fa\u4e8e35,000+\u771f\u5b9e\u7528\u6237-\u4e13\u5bb6\u5bf9\u8bdd\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u6b65\u6d41\u7a0b\u6784\u5efa\uff0cMIRAGE\u6db5\u76d6\u4e86\u4f5c\u7269\u5065\u5eb7\u3001\u75c5\u866b\u5bb3\u8bca\u65ad\u548c\u4f5c\u7269\u7ba1\u7406\u7b49\u591a\u79cd\u573a\u666f\u3002\u8be5\u57fa\u51c6\u5305\u542b7,000+\u72ec\u7279\u7684\u751f\u7269\u5b9e\u4f53\uff0c\u8986\u76d6\u690d\u7269\u79cd\u7c7b\u3001\u5bb3\u866b\u548c\u75be\u75c5\uff0c\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5206\u7c7b\u6700\u591a\u6837\u5316\u7684\u57fa\u51c6\u4e4b\u4e00\u3002\u4e0e\u73b0\u6709\u4f9d\u8d56\u660e\u786e\u8f93\u5165\u548c\u5c01\u95ed\u5206\u7c7b\u7684\u57fa\u51c6\u4e0d\u540c\uff0cMIRAGE\u4ee5\u5f00\u653e\u4e16\u754c\u4e3a\u80cc\u666f\uff0c\u8981\u6c42\u6a21\u578b\u63a8\u65ad\u6f5c\u5728\u77e5\u8bc6\u7f3a\u53e3\u3001\u5904\u7406\u7f55\u89c1\u5b9e\u4f53\uff0c\u5e76\u4e3b\u52a8\u5f15\u5bfc\u6216\u54cd\u5e94\u4ea4\u4e92\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://mirage-benchmark.github.io"}}
{"id": "2506.20151", "pdf": "https://arxiv.org/pdf/2506.20151", "abs": "https://arxiv.org/abs/2506.20151", "authors": ["Haipeng Fan", "Shiyuan Zhang", "Baohunesitu", "Zihang Guo", "Huaiwen Zhang"], "title": "EAR: Erasing Concepts from Unified Autoregressive Models", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, 1 tables", "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u6709\u6548\u64e6\u9664\u4e0d\u671f\u671b\u7684\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u901a\u8fc7WGA\u548cTLM\u7b56\u7565\uff0c\u4ee5\u53ca\u65b0\u57fa\u51c6ECGVF\uff0c\u5b9e\u9a8c\u8bc1\u660eEAR\u5728\u64e6\u9664\u6548\u679c\u548c\u6a21\u578b\u5b9e\u7528\u6027\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5982\u4f55\u5728\u4e0d\u5f71\u54cd\u6574\u4f53\u751f\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u79fb\u9664\u4e0d\u671f\u671b\u7684\u6982\u5ff5\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faEAR\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a97\u53e3\u68af\u5ea6\u7d2f\u79ef\uff08WGA\uff09\u7b56\u7565\u548c\u9608\u503c\u635f\u5931\u63a9\u7801\uff08TLM\uff09\u7b56\u7565\uff0c\u4f18\u5316\u6982\u5ff5\u64e6\u9664\u8fc7\u7a0b\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86ECGVF\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u677f\u548c\u89c6\u89c9\u5206\u7c7b\u5668\u751f\u6210\u5e76\u8fc7\u6ee4\u5927\u89c4\u6a21\u6982\u5ff5\u63d0\u793a\u5bf9\u3002", "result": "\u5728ECGVF\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEAR\u5728\u6982\u5ff5\u64e6\u9664\u6548\u679c\u548c\u6a21\u578b\u5b9e\u7528\u6027\u4fdd\u7559\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EAR\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u7b56\u7565\u548c\u57fa\u51c6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "EAR\uff1a\u4ece\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u64e6\u9664\u6982\u5ff5", "abstract_zh": "\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u7edf\u4e00\u4e14\u5f3a\u5927\u3002\u7136\u800c\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u6574\u4f53\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u4eceAR\u6a21\u578b\u4e2d\u79fb\u9664\u4e0d\u671f\u671b\u7684\u6982\u5ff5\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u64e6\u9664\u81ea\u56de\u5f52\u6a21\u578b\uff08EAR\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u5728AR\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u5b9e\u7528\u6027\u4fdd\u7559\u7684\u6982\u5ff5\u64e6\u9664\u7684\u5fae\u8c03\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5f15\u5165\u7a97\u53e3\u68af\u5ea6\u7d2f\u79ef\uff08WGA\uff09\u7b56\u7565\u4ee5\u5bf9\u9f50\u5757\u7ea7\u89e3\u7801\u4e0e\u64e6\u9664\u76ee\u6807\uff0c\u4ee5\u53ca\u9608\u503c\u635f\u5931\u63a9\u7801\uff08TLM\uff09\u7b56\u7565\u4ee5\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4fdd\u62a4\u4e0e\u76ee\u6807\u6982\u5ff5\u65e0\u5173\u7684\u5185\u5bb9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6\u2014\u2014\u64e6\u9664\u6982\u5ff5\u751f\u6210\u5668\u4e0e\u89c6\u89c9\u8fc7\u6ee4\u5668\uff08ECGVF\uff09\uff0c\u65e8\u5728\u4e3a\u8bc4\u4f30AR\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u64e6\u9664\u63d0\u4f9b\u66f4\u4e25\u683c\u548c\u5168\u9762\u7684\u57fa\u7840\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u5229\u7528\u7ed3\u6784\u5316\u6a21\u677f\u5728\u591a\u6837\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u9884\u751f\u6210\u5927\u89c4\u6a21\u7684\u76ee\u6807-\u66ff\u6362\u6982\u5ff5\u63d0\u793a\u5bf9\u3002\u968f\u540e\uff0c\u6211\u4eec\u4ece\u8fd9\u4e9b\u63d0\u793a\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u5206\u7c7b\u5668\u8fdb\u884c\u4e25\u683c\u8fc7\u6ee4\u4ee5\u786e\u4fdd\u6982\u5ff5\u7684\u4fdd\u771f\u5ea6\u548c\u5bf9\u9f50\u6027\u3002\u5728ECGVF\u57fa\u51c6\u4e0a\u5bf9AR\u6a21\u578bJanus-Pro\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEAR\u5728\u64e6\u9664\u6548\u679c\u548c\u6a21\u578b\u5b9e\u7528\u6027\u4fdd\u7559\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u4ee3\u7801\u53ef\u5728https://github.com/immc-lab/ear/\u83b7\u53d6\u3002"}}
{"id": "2506.20020", "pdf": "https://arxiv.org/pdf/2506.20020", "abs": "https://arxiv.org/abs/2506.20020", "authors": ["Saloni Dash", "Am\u00e9lie Reymond", "Emma S. Spiro", "Aylin Caliskan"], "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u8d4b\u4e88\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7279\u5b9a\u8eab\u4efd\u89d2\u8272\u540e\uff0c\u5176\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u52a8\u673a\u6027\u63a8\u7406\uff0c\u5bfc\u81f4\u5224\u65ad\u504f\u5dee\uff0c\u4e14\u96be\u4ee5\u901a\u8fc7\u5e38\u89c4\u65b9\u6cd5\u7ea0\u6b63\u3002", "motivation": "\u4eba\u7c7b\u63a8\u7406\u5e38\u56e0\u8eab\u4efd\u4fdd\u62a4\u7b49\u52a8\u673a\u800c\u4ea7\u751f\u504f\u89c1\uff0c\u5f71\u54cd\u7406\u6027\u51b3\u7b56\u3002LLMs\u662f\u5426\u4e5f\u4f1a\u56e0\u8eab\u4efd\u89d2\u8272\u800c\u9009\u62e9\u6027\u63a8\u7406\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u4e00\u95ee\u9898\u53ca\u5176\u793e\u4f1a\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e3a8\u79cdLLMs\uff08\u5f00\u6e90\u548c\u4e13\u6709\uff09\u5206\u914d\u4e864\u7c7b\u653f\u6cbb\u548c\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u51718\u79cd\u8eab\u4efd\u89d2\u8272\uff0c\u6d4b\u8bd5\u5176\u5728\u4e24\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u865a\u5047\u65b0\u95fb\u771f\u5b9e\u6027\u8fa8\u522b\u548c\u79d1\u5b66\u8bc1\u636e\u8bc4\u4f30\u3002", "result": "\u8eab\u4efd\u89d2\u8272\u4f7fLLMs\u7684\u771f\u5b9e\u6027\u8fa8\u522b\u80fd\u529b\u4e0b\u964d\u9ad8\u8fbe9%\uff0c\u653f\u6cbb\u8eab\u4efd\u89d2\u8272\u5728\u79d1\u5b66\u8bc1\u636e\u8bc4\u4f30\u4e2d\u8868\u73b0\u5c24\u4e3a\u660e\u663e\uff0c\u5f53\u4e8b\u5b9e\u4e0e\u5176\u8eab\u4efd\u4e00\u81f4\u65f6\u6b63\u786e\u7387\u63d0\u534790%\u3002\u5e38\u89c4\u53bb\u504f\u89c1\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u8eab\u4efd\u89d2\u8272\u8d4b\u4e88\u7684LLMs\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4e14\u96be\u4ee5\u901a\u8fc7\u5e38\u89c4\u65b9\u6cd5\u7ea0\u6b63\uff0c\u53ef\u80fd\u52a0\u5267\u8eab\u4efd\u4e00\u81f4\u6027\u63a8\u7406\u95ee\u9898\u3002", "paper_title_zh": "\u8d4b\u4e88\u8eab\u4efd\u89d2\u8272\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u7c7b\u4eba\u7684\u52a8\u673a\u6027\u63a8\u7406", "abstract_zh": "\u4eba\u7c7b\u63a8\u7406\u5e38\u56e0\u8eab\u4efd\u4fdd\u62a4\u7b49\u52a8\u673a\u800c\u4ea7\u751f\u504f\u89c1\uff0c\u5f71\u54cd\u7406\u6027\u51b3\u7b56\u548c\u793e\u4f1a\u8bae\u9898\u8ba8\u8bba\uff08\u5982\u6c14\u5019\u53d8\u5316\u6216\u75ab\u82d7\u5b89\u5168\uff09\uff0c\u751a\u81f3\u52a0\u5267\u653f\u6cbb\u6781\u5316\u3002\u6b64\u524d\u7814\u7a76\u663e\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e5f\u5b58\u5728\u7c7b\u4f3c\u8ba4\u77e5\u504f\u5dee\uff0c\u4f46\u5176\u9009\u62e9\u6027\u63a8\u7406\u7684\u7a0b\u5ea6\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u4e3a8\u79cdLLMs\u5206\u914d\u4e864\u7c7b\u653f\u6cbb\u548c\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u51718\u79cd\u8eab\u4efd\u89d2\u8272\uff0c\u6d4b\u8bd5\u5176\u5728\u865a\u5047\u65b0\u95fb\u771f\u5b9e\u6027\u8fa8\u522b\u548c\u79d1\u5b66\u8bc1\u636e\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8eab\u4efd\u89d2\u8272\u4f7fLLMs\u7684\u771f\u5b9e\u6027\u8fa8\u522b\u80fd\u529b\u4e0b\u964d\u9ad8\u8fbe9%\uff0c\u653f\u6cbb\u8eab\u4efd\u89d2\u8272\u5728\u79d1\u5b66\u8bc1\u636e\u8bc4\u4f30\u4e2d\u8868\u73b0\u5c24\u4e3a\u660e\u663e\uff08\u5f53\u4e8b\u5b9e\u4e0e\u5176\u8eab\u4efd\u4e00\u81f4\u65f6\u6b63\u786e\u7387\u63d0\u534790%\uff09\u3002\u5e38\u89c4\u53bb\u504f\u89c1\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002\u7814\u7a76\u8868\u660e\uff0c\u8eab\u4efd\u89d2\u8272\u8d4b\u4e88\u7684LLMs\u8868\u73b0\u51fa\u7c7b\u4eba\u7684\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4e14\u96be\u4ee5\u7ea0\u6b63\uff0c\u53ef\u80fd\u52a0\u5267\u8eab\u4efd\u4e00\u81f4\u6027\u63a8\u7406\u95ee\u9898\u3002"}}
{"id": "2506.20112", "pdf": "https://arxiv.org/pdf/2506.20112", "abs": "https://arxiv.org/abs/2506.20112", "authors": ["Songsoo Kim", "Seungtae Lee", "See Young Lee", "Joonho Kim", "Keechan Kan", "Dukyong Yoon"], "title": "A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection", "categories": ["cs.CL", "I.2.7"], "comment": "29 pages, 5 figures, 4 tables. Code available at\n  https://github.com/radssk/mp-rred", "summary": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u9519\u8bef\u68c0\u6d4b\u7684\u9633\u6027\u9884\u6d4b\u503c\uff08PPV\uff09\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c\u3002", "motivation": "\u7531\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u7684\u9519\u8bef\u53d1\u751f\u7387\u8f83\u4f4e\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6821\u5bf9\u65b9\u6cd5\u7684\u9633\u6027\u9884\u6d4b\u503c\uff08PPV\uff09\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u4e09\u9636\u6bb5LLM\u6846\u67b6\u662f\u5426\u80fd\u591f\u63d0\u5347PPV\u5e76\u51cf\u5c11\u64cd\u4f5c\u6210\u672c\u3002", "method": "\u7814\u7a76\u56de\u987e\u6027\u5206\u6790\u4e86\u6765\u81eaMIMIC-III\u6570\u636e\u5e93\u76841,000\u4efd\u653e\u5c04\u5b66\u62a5\u544a\uff08\u5305\u62ecX\u5149\u3001\u8d85\u58f0\u3001CT\u548cMRI\u5404250\u4efd\uff09\uff0c\u5e76\u4f7f\u7528CheXpert\u548cOpen-i\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\u3002\u6d4b\u8bd5\u4e86\u4e09\u79cdLLM\u6846\u67b6\uff1a\u5355\u63d0\u793a\u68c0\u6d4b\u5668\u3001\u63d0\u53d6\u5668\u52a0\u68c0\u6d4b\u5668\u3001\u4ee5\u53ca\u63d0\u53d6\u5668\u3001\u68c0\u6d4b\u5668\u548c\u5047\u9633\u6027\u9a8c\u8bc1\u5668\u7684\u7ec4\u5408\u3002\u901a\u8fc7PPV\u548c\u7edd\u5bf9\u771f\u9633\u6027\u7387\uff08aTPR\uff09\u8861\u91cf\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u63a8\u7406\u8d39\u7528\u548c\u8bc4\u5ba1\u5458\u85aa\u916c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4e09\u9636\u6bb5\u6846\u67b6\u7684PPV\u4ece0.063\uff08\u6846\u67b61\uff09\u663e\u8457\u63d0\u5347\u81f30.159\uff08\u6846\u67b63\uff09\uff0c\u64cd\u4f5c\u6210\u672c\u6bcf1,000\u4efd\u62a5\u544a\u4ece9.72\u7f8e\u5143\u964d\u81f35.58\u7f8e\u5143\uff0c\u51cf\u5c11\u4e8642.6%\u3002aTPR\u4fdd\u6301\u7a33\u5b9a\uff080.012-0.014\uff09\u3002\u5916\u90e8\u9a8c\u8bc1\u652f\u6301\u6846\u67b63\u7684\u4f18\u8d8aPPV\uff08CheXpert 0.133\uff0cOpen-i 0.105\uff09\u3002", "conclusion": "\u4e09\u9636\u6bb5LLM\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86PPV\u5e76\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3aAI\u8f85\u52a9\u653e\u5c04\u5b66\u62a5\u544a\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002", "paper_title_zh": "\u4e00\u79cd\u591a\u9636\u6bb5\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\u7528\u4e8e\u7cbe\u786e\u9ad8\u6548\u7684\u653e\u5c04\u5b66\u62a5\u544a\u9519\u8bef\u68c0\u6d4b", "abstract_zh": "\u80cc\u666f\uff1a\u7531\u4e8e\u9519\u8bef\u53d1\u751f\u7387\u4f4e\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u653e\u5c04\u5b66\u62a5\u544a\u6821\u5bf9\u65b9\u6cd5\u7684\u9633\u6027\u9884\u6d4b\u503c\uff08PPV\uff09\u6709\u9650\u3002\u76ee\u7684\uff1a\u8bc4\u4f30\u4e09\u9636\u6bb5LLM\u6846\u67b6\u662f\u5426\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u80fd\u63d0\u5347PPV\u5e76\u964d\u4f4e\u64cd\u4f5c\u6210\u672c\u3002\u6750\u6599\u4e0e\u65b9\u6cd5\uff1a\u56de\u987e\u6027\u5206\u6790\u4e86MIMIC-III\u6570\u636e\u5e93\u4e2d\u76841,000\u4efd\u8fde\u7eed\u653e\u5c04\u5b66\u62a5\u544a\uff08X\u5149\u3001\u8d85\u58f0\u3001CT\u548cMRI\u5404250\u4efd\uff09\uff0c\u5e76\u4f7f\u7528CheXpert\u548cOpen-i\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\u3002\u6d4b\u8bd5\u4e86\u4e09\u79cdLLM\u6846\u67b6\uff1a\uff081\uff09\u5355\u63d0\u793a\u68c0\u6d4b\u5668\uff1b\uff082\uff09\u63d0\u53d6\u5668\u52a0\u68c0\u6d4b\u5668\uff1b\uff083\uff09\u63d0\u53d6\u5668\u3001\u68c0\u6d4b\u5668\u548c\u5047\u9633\u6027\u9a8c\u8bc1\u5668\u3002\u901a\u8fc7PPV\u548c\u7edd\u5bf9\u771f\u9633\u6027\u7387\uff08aTPR\uff09\u8861\u91cf\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u63a8\u7406\u8d39\u7528\u548c\u8bc4\u5ba1\u5458\u85aa\u916c\u8ba1\u7b97\u6548\u7387\u3002\u7ed3\u679c\uff1a\u6846\u67b6PPV\u4ece0.063\uff08\u6846\u67b61\uff09\u663e\u8457\u63d0\u5347\u81f30.159\uff08\u6846\u67b63\uff09\uff0c\u64cd\u4f5c\u6210\u672c\u6bcf1,000\u4efd\u62a5\u544a\u4ece9.72\u7f8e\u5143\u964d\u81f35.58\u7f8e\u5143\u3002aTPR\u4fdd\u6301\u7a33\u5b9a\uff080.012-0.014\uff09\u3002\u5916\u90e8\u9a8c\u8bc1\u652f\u6301\u6846\u67b63\u7684\u4f18\u8d8aPPV\uff08CheXpert 0.133\uff0cOpen-i 0.105\uff09\u3002\u7ed3\u8bba\uff1a\u4e09\u9636\u6bb5LLM\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86PPV\u5e76\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3aAI\u8f85\u52a9\u653e\u5c04\u5b66\u62a5\u544a\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2506.20152", "pdf": "https://arxiv.org/pdf/2506.20152", "abs": "https://arxiv.org/abs/2506.20152", "authors": ["Deepak Ghimire", "Kilho Lee", "Seong-heum Kim"], "title": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u635f\u5931\u611f\u77e5\u7684\u81ea\u52a8\u7ed3\u6784\u5316\u526a\u679d\u6807\u51c6\u9009\u62e9\u65b9\u6cd5\uff08LAASP\uff09\uff0c\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u538b\u7f29\u548c\u52a0\u901f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u526a\u679d\u4e0e\u8bad\u7ec3\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u81ea\u52a8\u9009\u62e9\u526a\u679d\u6807\u51c6\u548c\u5c42\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u5206\u4e3a\u8bad\u7ec3\u3001\u526a\u679d\u548c\u5fae\u8c03\u4e09\u4e2a\u9636\u6bb5\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u526a\u679d\u4e0e\u8bad\u7ec3\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u81ea\u52a8\u9009\u62e9\u526a\u679d\u6807\u51c6\u548c\u5c42\uff0c\u4ee5\u63d0\u5347\u526a\u679d\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u526a\u679d\u4e0e\u8bad\u7ec3\u7ed3\u5408\u7684LAASP\u65b9\u6cd5\uff0c\u81ea\u52a8\u4ece\u9884\u8bbe\u7684\u526a\u679d\u6807\u51c6\u4e2d\u9009\u62e9\u9002\u5408\u7684\u526a\u679d\u6807\u51c6\u548c\u5c42\uff0c\u5e76\u901a\u8fc7\u7f51\u7edc\u635f\u5931\u6307\u5bfc\u9009\u62e9\u8fc7\u7a0b\u3002\u6bcf\u6b21\u526a\u679d\u540e\u77ed\u6682\u91cd\u8bad\u7ec3\u4ee5\u7f13\u89e3\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u81ea\u52a8\u786e\u5b9a\u5404\u5c42\u7684\u6700\u4f18\u526a\u679d\u7387\u3002", "result": "\u5728CIFAR-10\u548cImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAASP\u663e\u8457\u51cf\u5c11\u4e86FLOPs\uff08ResNet56\u548cResNet110\u51cf\u5c1152%\uff0cResNet50\u51cf\u5c1142%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff08ImageNet\u4e0a\u4ec5\u4e0b\u964d0.33%\uff09\u3002", "conclusion": "LAASP\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u526a\u679d\u6807\u51c6\u548c\u5c42\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u548c\u52a0\u901f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "paper_title_zh": "\u57fa\u4e8e\u635f\u5931\u611f\u77e5\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u7ed3\u6784\u5316\u526a\u679d\u6807\u51c6\u81ea\u52a8\u9009\u62e9", "abstract_zh": "\u7ed3\u6784\u5316\u526a\u679d\u662f\u4e00\u79cd\u6210\u719f\u7684\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u6280\u672f\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u635f\u5931\u611f\u77e5\u7684\u7ed3\u6784\u5316\u526a\u679d\u6807\u51c6\u81ea\u52a8\u9009\u62e9\u65b9\u6cd5\uff08LAASP\uff09\uff0c\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7626\u8eab\u548c\u52a0\u901f\u3002\u5927\u591a\u6570\u526a\u679d\u65b9\u6cd5\u91c7\u7528\u8bad\u7ec3\u3001\u526a\u679d\u548c\u5fae\u8c03\u4e09\u9636\u6bb5\u987a\u5e8f\u6d41\u7a0b\uff0c\u800c\u672c\u6587\u63d0\u51fa\u7684\u526a\u679d\u6280\u672f\u91c7\u7528\u526a\u679d\u4e0e\u8bad\u7ec3\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u7701\u53bb\u4e86\u7b2c\u4e00\u9636\u6bb5\uff0c\u5e76\u5c06\u7b2c\u4e8c\u548c\u7b2c\u4e09\u9636\u6bb5\u6574\u5408\u4e3a\u4e00\u4e2a\u5faa\u73af\u3002\u901a\u8fc7\u7f51\u7edc\u7684\u603b\u4f53\u635f\u5931\u6307\u5bfc\u4ece\u9884\u8bbe\u6807\u51c6\u6c60\u4e2d\u81ea\u52a8\u9009\u62e9\u57fa\u4e8e\u5e45\u5ea6\u6216\u76f8\u4f3c\u6027\u7684\u6ee4\u6ce2\u5668\u526a\u679d\u6807\u51c6\uff0c\u5e76\u5728\u6bcf\u6b21\u526a\u679d\u8fed\u4ee3\u4e2d\u9009\u62e9\u7279\u5b9a\u526a\u679d\u5c42\u3002\u4e3a\u7f13\u89e3\u526a\u679d\u5bfc\u81f4\u7684\u51c6\u786e\u7387\u9aa4\u964d\uff0c\u6bcf\u6b21\u51cf\u5c11\u9884\u5b9a\u4e49\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u6570\u91cf\u540e\uff0c\u7f51\u7edc\u4f1a\u77ed\u6682\u91cd\u8bad\u7ec3\u3002\u7f51\u7edc\u4e2d\u5404\u5c42\u7684\u6700\u4f18\u526a\u679d\u7387\u81ea\u52a8\u786e\u5b9a\uff0c\u65e0\u9700\u624b\u52a8\u5206\u914d\u56fa\u5b9a\u6216\u53ef\u53d8\u526a\u679d\u7387\u3002\u5728CIFAR-10\u548cImageNet\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9VGGNet\u548cResNet\u6a21\u578b\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7279\u522b\u662f\uff0cCIFAR-10\u6570\u636e\u96c6\u4e0a\u7684ResNet56\u548cResNet110\u6a21\u578b\u5728\u51cf\u5c1152% FLOPs\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86top-1\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0cImageNet\u6570\u636e\u96c6\u4e0a\u7684ResNet50\u6a21\u578b\u51cf\u5c11\u4e86\u8d85\u8fc742%\u7684FLOPs\uff0c\u800ctop-5\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.33%\u3002\u672c\u6587\u7684\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\uff1ahttps://github.com/ghimiredhikura/laasp\u3002"}}
{"id": "2506.20059", "pdf": "https://arxiv.org/pdf/2506.20059", "abs": "https://arxiv.org/abs/2506.20059", "authors": ["Weijieying Ren", "Tianxiang Zhao", "Lei Wang", "Tianchun Wang", "Vasant Honavar"], "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to remarkable\nprogresses in medical consultation. However, existing medical LLMs overlook the\nessential role of Electronic Health Records (EHR) and focus primarily on\ndiagnosis recommendation, limiting their clinical applicability. We propose\nDiaLLM, the first medical LLM that integrates heterogeneous EHR data into\nclinically grounded dialogues, enabling clinical test recommendation, result\ninterpretation, and diagnosis prediction to better align with real-world\nmedical practice. To construct clinically grounded dialogues from EHR, we\ndesign a Clinical Test Reference (CTR) strategy that maps each clinical code to\nits corresponding description and classifies test results as \"normal\" or\n\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for\nevidence acquisition and automated diagnosis. To handle the large action space,\nwe introduce a reject sampling strategy to reduce redundancy and improve\nexploration efficiency. Furthermore, a confirmation reward and a\nclass-sensitive diagnosis reward are designed to guide accurate diagnosis\nprediction. Extensive experimental results demonstrate that DiaLLM outperforms\nbaselines in clinical test recommendation and diagnosis prediction.", "AI": {"tldr": "DiaLLM\u662f\u9996\u4e2a\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e34\u5e8a\u6d4b\u8bd5\u53c2\u8003\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u3001\u7ed3\u679c\u89e3\u91ca\u548c\u8bca\u65ad\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u4f5c\u7528\uff0c\u4e14\u4ec5\u5173\u6ce8\u8bca\u65ad\u63a8\u8350\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u9002\u7528\u6027\u3002DiaLLM\u65e8\u5728\u901a\u8fc7\u6574\u5408EHR\u6570\u636e\uff0c\u63d0\u4f9b\u66f4\u7b26\u5408\u5b9e\u9645\u533b\u7597\u5b9e\u8df5\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002", "method": "DiaLLM\u91c7\u7528\u4e34\u5e8a\u6d4b\u8bd5\u53c2\u8003\uff08CTR\uff09\u7b56\u7565\u5c06\u4e34\u5e8a\u4ee3\u7801\u6620\u5c04\u4e3a\u63cf\u8ff0\u5e76\u5206\u7c7b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u540c\u65f6\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8bc1\u636e\u83b7\u53d6\u548c\u81ea\u52a8\u8bca\u65ad\u3002\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u8bbe\u8ba1\u4e86\u786e\u8ba4\u5956\u52b1\u548c\u7c7b\u522b\u654f\u611f\u8bca\u65ad\u5956\u52b1\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDiaLLM\u5728\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u548c\u8bca\u65ad\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "DiaLLM\u901a\u8fc7\u6574\u5408EHR\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e34\u5e8a\u9002\u7528\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u672a\u6765\u533b\u7597AI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "DiaLLMs\uff1a\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u4e34\u5e8a\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u548c\u8bca\u65ad\u9884\u6d4b", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u54a8\u8be2\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u533b\u7597LLMs\u5ffd\u89c6\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u8bca\u65ad\u63a8\u8350\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u9002\u7528\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86DiaLLM\uff0c\u8fd9\u662f\u9996\u4e2a\u5c06\u5f02\u6784EHR\u6570\u636e\u6574\u5408\u5230\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u7684\u533b\u7597LLM\uff0c\u652f\u6301\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u3001\u7ed3\u679c\u89e3\u91ca\u548c\u8bca\u65ad\u9884\u6d4b\uff0c\u4ee5\u66f4\u597d\u5730\u7b26\u5408\u5b9e\u9645\u533b\u7597\u5b9e\u8df5\u3002\u4e3a\u4eceEHR\u6784\u5efa\u4e34\u5e8a\u5bf9\u8bdd\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e34\u5e8a\u6d4b\u8bd5\u53c2\u8003\uff08CTR\uff09\u7b56\u7565\uff0c\u5c06\u6bcf\u4e2a\u4e34\u5e8a\u4ee3\u7801\u6620\u5c04\u4e3a\u5176\u5bf9\u5e94\u63cf\u8ff0\uff0c\u5e76\u5c06\u6d4b\u8bd5\u7ed3\u679c\u5206\u7c7b\u4e3a\u201c\u6b63\u5e38\u201d\u6216\u201c\u5f02\u5e38\u201d\u3002\u6b64\u5916\uff0cDiaLLM\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8bc1\u636e\u83b7\u53d6\u548c\u81ea\u52a8\u8bca\u65ad\u3002\u4e3a\u5904\u7406\u5927\u52a8\u4f5c\u7a7a\u95f4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u4ee5\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u786e\u8ba4\u5956\u52b1\u548c\u7c7b\u522b\u654f\u611f\u8bca\u65ad\u5956\u52b1\u4ee5\u6307\u5bfc\u51c6\u786e\u7684\u8bca\u65ad\u9884\u6d4b\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDiaLLM\u5728\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u548c\u8bca\u65ad\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2506.20119", "pdf": "https://arxiv.org/pdf/2506.20119", "abs": "https://arxiv.org/abs/2506.20119", "authors": ["Masaki Uto", "Yuma Ito"], "title": "Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EvalLAC'25: 2nd Workshop on Automatic Evaluation of\n  Learning and Assessment Content, held at AIED 2025, Palermo, Italy. This is\n  the camera-ready version submitted to CEUR Workshop Proceedings", "summary": "Evaluating the abilities of learners is a fundamental objective in the field\nof education. In particular, there is an increasing need to assess higher-order\nabilities such as expressive skills and logical thinking. Constructed-response\ntests such as short-answer and essay-based questions have become widely used as\na method to meet this demand. Although these tests are effective, they require\nsubstantial manual grading, making them both labor-intensive and costly. Item\nresponse theory (IRT) provides a promising solution by enabling the estimation\nof ability from incomplete score data, where human raters grade only a subset\nof answers provided by learners across multiple test items. However, the\naccuracy of ability estimation declines as the proportion of missing scores\nincreases. Although data augmentation techniques for imputing missing scores\nhave been explored in order to address this limitation, they often struggle\nwith inaccuracy for sparse or heterogeneous data. To overcome these challenges,\nthis study proposes a novel method for imputing missing scores by leveraging\nautomated scoring technologies for accurate IRT-based ability estimation. The\nproposed method achieves high accuracy in ability estimation while markedly\nreducing manual grading workload.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528AI\u8bc4\u5206\u6280\u672f\u586b\u8865\u7f3a\u5931\u5206\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\u4e2d\u80fd\u529b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u3002", "motivation": "\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\uff08\u5982\u7b80\u7b54\u548c\u8bba\u8ff0\u9898\uff09\u80fd\u6709\u6548\u8bc4\u4f30\u9ad8\u9636\u80fd\u529b\uff0c\u4f46\u4eba\u5de5\u8bc4\u5206\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u4f20\u7edfIRT\u65b9\u6cd5\u5728\u7f3a\u5931\u5206\u6570\u8f83\u591a\u65f6\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u73b0\u6709\u6570\u636e\u586b\u8865\u6280\u672f\u5bf9\u7a00\u758f\u6216\u5f02\u6784\u6570\u636e\u6548\u679c\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u8bc4\u5206\u6280\u672f\u586b\u8865\u7f3a\u5931\u5206\u6570\uff0c\u63d0\u5347\u80fd\u529b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u586b\u8865\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\u4e2d\u7684\u7f3a\u5931\u5206\u6570\uff0c\u4ece\u800c\u652f\u6301\u57fa\u4e8eIRT\u7684\u51c6\u786e\u80fd\u529b\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u5229\u7528AI\u8bc4\u5206\u5668\u751f\u6210\u7f3a\u5931\u5206\u6570\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u8bc4\u5206\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u80fd\u529b\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u8bc4\u5206\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u901a\u8fc7AI\u8bc4\u5206\u6280\u672f\u586b\u8865\u7f3a\u5931\u5206\u6570\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\u7684\u8bc4\u4f30\u6548\u7387\uff0c\u540c\u65f6\u51cf\u8f7b\u4eba\u5de5\u8d1f\u62c5\u3002", "paper_title_zh": "\u5229\u7528AI\u8bc4\u5206\u5668\u586b\u8865\u7f3a\u5931\u5206\u6570\u4ee5\u5b9e\u73b0\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7684\u80fd\u529b\u4f30\u8ba1", "abstract_zh": "\u8bc4\u4f30\u5b66\u4e60\u8005\u7684\u80fd\u529b\u662f\u6559\u80b2\u9886\u57df\u7684\u4e00\u9879\u57fa\u672c\u76ee\u6807\u3002\u7279\u522b\u662f\uff0c\u5bf9\u8868\u8fbe\u80fd\u529b\u4e0e\u903b\u8f91\u601d\u7ef4\u7b49\u9ad8\u9636\u80fd\u529b\u7684\u8bc4\u4f30\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\uff08\u5982\u7b80\u7b54\u548c\u8bba\u8ff0\u9898\uff09\u5df2\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u7684\u5e38\u7528\u65b9\u6cd5\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6d4b\u8bd5\u6709\u6548\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4eba\u5de5\u8bc4\u5206\uff0c\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4ece\u4e0d\u5b8c\u6574\u7684\u5206\u6570\u6570\u636e\u4e2d\u4f30\u8ba1\u80fd\u529b\uff0c\u5373\u4eba\u7c7b\u8bc4\u5206\u8005\u4ec5\u5bf9\u5b66\u4e60\u8005\u63d0\u4f9b\u7684\u90e8\u5206\u7b54\u6848\u8fdb\u884c\u8bc4\u5206\u3002\u7136\u800c\uff0c\u968f\u7740\u7f3a\u5931\u5206\u6570\u6bd4\u4f8b\u7684\u589e\u52a0\uff0c\u80fd\u529b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4f1a\u4e0b\u964d\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u586b\u8865\u7f3a\u5931\u5206\u6570\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u5bf9\u7a00\u758f\u6216\u5f02\u6784\u6570\u636e\u7684\u5904\u7406\u5f80\u5f80\u4e0d\u591f\u51c6\u786e\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u586b\u8865\u7f3a\u5931\u5206\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8eIRT\u7684\u51c6\u786e\u80fd\u529b\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u7684\u80fd\u529b\u4f30\u8ba1\u3002"}}
{"id": "2506.20155", "pdf": "https://arxiv.org/pdf/2506.20155", "abs": "https://arxiv.org/abs/2506.20155", "authors": ["Avadhoot Jadhav", "Ashutosh Srivastava", "Abhinav Java", "Silky Singh", "Tarun Ram Menta", "Surgan Jandial", "Balaji Krishnamurthy"], "title": "Towards Efficient Exemplar Based Image Editing with Multimodal VLMs", "categories": ["cs.CV"], "comment": "Accepted at ECCV 2024 (AI4VA Workshop)", "summary": "Text-to-Image Diffusion models have enabled a wide array of image editing\napplications. However, capturing all types of edits through text alone can be\nchallenging and cumbersome. The ambiguous nature of certain image edits is\nbetter expressed through an exemplar pair, i.e., a pair of images depicting an\nimage before and after an edit respectively. In this work, we tackle\nexemplar-based image editing -- the task of transferring an edit from an\nexemplar pair to a content image(s), by leveraging pretrained text-to-image\ndiffusion models and multimodal VLMs. Even though our end-to-end pipeline is\noptimization-free, our experiments demonstrate that it still outperforms\nbaselines on multiple types of edits while being ~4x faster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793a\u4f8b\u7684\u9ad8\u6548\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u4e14\u4f18\u4e8e\u57fa\u7ebf\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u901a\u8fc7\u6587\u5b57\u51c6\u786e\u8868\u8fbe\u67d0\u4e9b\u6a21\u7cca\u7684\u7f16\u8f91\u9700\u6c42\uff0c\u800c\u793a\u4f8b\u5bf9\uff08\u7f16\u8f91\u524d\u540e\u7684\u56fe\u50cf\u5bf9\uff09\u80fd\u66f4\u76f4\u89c2\u5730\u4f20\u8fbe\u7f16\u8f91\u610f\u56fe\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u57fa\u4e8e\u793a\u4f8b\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u63d0\u5347\u7f16\u8f91\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001VLMs\uff0c\u65e0\u9700\u4f18\u5316\u6b65\u9aa4\u5373\u53ef\u5c06\u793a\u4f8b\u5bf9\u4e2d\u7684\u7f16\u8f91\u6548\u679c\u8fc1\u79fb\u5230\u76ee\u6807\u56fe\u50cf\u4e0a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u63d0\u5347\u7ea64\u500d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001VLMs\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u793a\u4f8b\u56fe\u50cf\u7f16\u8f91\uff0c\u4e3a\u590d\u6742\u7f16\u8f91\u9700\u6c42\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u793a\u4f8b\u56fe\u50cf\u7f16\u8f91", "abstract_zh": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5df2\u652f\u6301\u5e7f\u6cdb\u7684\u56fe\u50cf\u7f16\u8f91\u5e94\u7528\u3002\u7136\u800c\uff0c\u4ec5\u901a\u8fc7\u6587\u5b57\u6355\u6349\u6240\u6709\u7c7b\u578b\u7684\u7f16\u8f91\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u4e14\u7e41\u7410\u3002\u67d0\u4e9b\u56fe\u50cf\u7f16\u8f91\u7684\u6a21\u7cca\u6027\u66f4\u9002\u5408\u901a\u8fc7\u793a\u4f8b\u5bf9\uff08\u5373\u7f16\u8f91\u524d\u540e\u7684\u56fe\u50cf\u5bf9\uff09\u8868\u8fbe\u3002\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u793a\u4f8b\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u2014\u2014\u5c06\u793a\u4f8b\u5bf9\u4e2d\u7684\u7f16\u8f91\u6548\u679c\u8fc1\u79fb\u5230\u5185\u5bb9\u56fe\u50cf\u4e0a\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001VLMs\u3002\u5c3d\u7ba1\u6211\u4eec\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\u65e0\u9700\u4f18\u5316\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u4e2d\u4ecd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u63d0\u5347\u7ea64\u500d\u3002"}}
{"id": "2506.20130", "pdf": "https://arxiv.org/pdf/2506.20130", "abs": "https://arxiv.org/abs/2506.20130", "authors": ["Adrien Bibal", "Steven N. Minton", "Deborah Khider", "Yolanda Gil"], "title": "AI Copilots for Reproducibility in Science: A Case Study", "categories": ["cs.AI"], "comment": null, "summary": "Open science initiatives seek to make research outputs more transparent,\naccessible, and reusable, but ensuring that published findings can be\nindependently reproduced remains a persistent challenge. This paper introduces\nOpenPub, an AI-powered platform that supports researchers, reviewers, and\nreaders through a suite of modular copilots focused on key open science tasks.\nIn this work, we present the Reproducibility Copilot, which analyzes\nmanuscripts, code, and supplementary materials to generate structured Jupyter\nNotebooks and recommendations aimed at facilitating computational, or \"rote\",\nreproducibility. We conducted feasibility tests using previously studied\nresearch papers with known reproducibility benchmarks. Results indicate that\nOpenPub can substantially reduce reproduction time - from over 30 hours to\nabout 1 hour - while achieving high coverage of figures, tables, and results\nsuitable for computational reproduction. The system systematically detects\nbarriers to reproducibility, including missing hyperparameters, undocumented\npreprocessing steps, and incomplete or inaccessible datasets. These findings\nsuggest that AI-driven tools can meaningfully reduce the burden of\nreproducibility efforts and contribute to more transparent and verifiable\nscientific communication. The modular copilot architecture also provides a\nfoundation for extending AI assistance to additional open science objectives\nbeyond reproducibility.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OpenPub\u5e73\u53f0\u53ca\u5176AI\u52a9\u624bReproducibility Copilot\uff0c\u65e8\u5728\u901a\u8fc7\u751f\u6210\u7ed3\u6784\u5316Jupyter Notebook\u548c\u5efa\u8bae\uff0c\u663e\u8457\u51cf\u5c11\u79d1\u5b66\u7814\u7a76\u7684\u590d\u73b0\u65f6\u95f4\uff0c\u4ece30\u5c0f\u65f6\u7f29\u77ed\u81f31\u5c0f\u65f6\uff0c\u5e76\u63d0\u9ad8\u590d\u73b0\u8986\u76d6\u7387\u3002", "motivation": "\u5f00\u653e\u79d1\u5b66\u5021\u8bae\u65e8\u5728\u63d0\u9ad8\u7814\u7a76\u7684\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4f46\u72ec\u7acb\u590d\u73b0\u5df2\u53d1\u8868\u6210\u679c\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u63d0\u51faAI\u9a71\u52a8\u7684OpenPub\u5e73\u53f0\uff0c\u4ee5\u89e3\u51b3\u590d\u73b0\u6027\u95ee\u9898\u3002", "method": "OpenPub\u5e73\u53f0\u901a\u8fc7Reproducibility Copilot\u5206\u6790\u8bba\u6587\u3001\u4ee3\u7801\u548c\u8865\u5145\u6750\u6599\uff0c\u751f\u6210\u7ed3\u6784\u5316Jupyter Notebook\u548c\u5efa\u8bae\uff0c\u4e13\u6ce8\u4e8e\u8ba1\u7b97\u590d\u73b0\u6027\u3002\u6d4b\u8bd5\u57fa\u4e8e\u5df2\u77e5\u590d\u73b0\u57fa\u51c6\u7684\u7814\u7a76\u8bba\u6587\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0cOpenPub\u5c06\u590d\u73b0\u65f6\u95f4\u4ece30\u591a\u5c0f\u65f6\u7f29\u77ed\u81f3\u7ea61\u5c0f\u65f6\uff0c\u5e76\u9ad8\u8986\u76d6\u7387\u5730\u590d\u73b0\u56fe\u8868\u548c\u7ed3\u679c\u3002\u7cfb\u7edf\u8fd8\u80fd\u68c0\u6d4b\u590d\u73b0\u969c\u788d\uff0c\u5982\u7f3a\u5931\u8d85\u53c2\u6570\u3001\u672a\u8bb0\u5f55\u7684\u9884\u5904\u7406\u6b65\u9aa4\u7b49\u3002", "conclusion": "AI\u5de5\u5177\u53ef\u663e\u8457\u51cf\u8f7b\u590d\u73b0\u8d1f\u62c5\uff0c\u4fc3\u8fdb\u900f\u660e\u548c\u53ef\u9a8c\u8bc1\u7684\u79d1\u5b66\u4ea4\u6d41\u3002OpenPub\u7684\u6a21\u5757\u5316\u67b6\u6784\u8fd8\u652f\u6301\u6269\u5c55\u81f3\u5176\u4ed6\u5f00\u653e\u79d1\u5b66\u76ee\u6807\u3002", "paper_title_zh": "\u79d1\u5b66\u590d\u73b0\u7684AI\u526f\u9a7e\u9a76\uff1a\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76", "abstract_zh": "\u5f00\u653e\u79d1\u5b66\u5021\u8bae\u65e8\u5728\u63d0\u9ad8\u7814\u7a76\u8f93\u51fa\u7684\u900f\u660e\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u4f46\u786e\u4fdd\u5df2\u53d1\u8868\u6210\u679c\u53ef\u72ec\u7acb\u590d\u73b0\u4ecd\u662f\u4e00\u9879\u6301\u7eed\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86OpenPub\uff0c\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u4e13\u6ce8\u4e8e\u5173\u952e\u5f00\u653e\u79d1\u5b66\u4efb\u52a1\u7684\u6a21\u5757\u5316\u526f\u9a7e\u9a76\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u3001\u5ba1\u7a3f\u4eba\u548c\u8bfb\u8005\u3002\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86Reproducibility Copilot\uff0c\u5b83\u5206\u6790\u8bba\u6587\u3001\u4ee3\u7801\u548c\u8865\u5145\u6750\u6599\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7684Jupyter Notebook\u548c\u5efa\u8bae\uff0c\u65e8\u5728\u4fc3\u8fdb\u8ba1\u7b97\u6216\u201c\u673a\u68b0\u201d\u590d\u73b0\u6027\u3002\u6211\u4eec\u4f7f\u7528\u5df2\u77e5\u590d\u73b0\u57fa\u51c6\u7684\u7814\u7a76\u8bba\u6587\u8fdb\u884c\u4e86\u53ef\u884c\u6027\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0cOpenPub\u53ef\u663e\u8457\u51cf\u5c11\u590d\u73b0\u65f6\u95f4\u2014\u2014\u4ece30\u591a\u5c0f\u65f6\u7f29\u77ed\u81f3\u7ea61\u5c0f\u65f6\u2014\u2014\u540c\u65f6\u5b9e\u73b0\u9002\u5408\u8ba1\u7b97\u590d\u73b0\u7684\u56fe\u8868\u548c\u7ed3\u679c\u7684\u9ad8\u8986\u76d6\u7387\u3002\u8be5\u7cfb\u7edf\u8fd8\u80fd\u7cfb\u7edf\u6027\u5730\u68c0\u6d4b\u590d\u73b0\u969c\u788d\uff0c\u5305\u62ec\u7f3a\u5931\u8d85\u53c2\u6570\u3001\u672a\u8bb0\u5f55\u7684\u9884\u5904\u7406\u6b65\u9aa4\u4ee5\u53ca\u4e0d\u5b8c\u6574\u6216\u4e0d\u53ef\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cAI\u9a71\u52a8\u5de5\u5177\u53ef\u663e\u8457\u51cf\u8f7b\u590d\u73b0\u8d1f\u62c5\uff0c\u4fc3\u8fdb\u66f4\u900f\u660e\u548c\u53ef\u9a8c\u8bc1\u7684\u79d1\u5b66\u4ea4\u6d41\u3002\u6a21\u5757\u5316\u526f\u9a7e\u9a76\u67b6\u6784\u8fd8\u4e3a\u6269\u5c55AI\u8f85\u52a9\u81f3\u590d\u73b0\u6027\u4ee5\u5916\u7684\u5176\u4ed6\u5f00\u653e\u79d1\u5b66\u76ee\u6807\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.20128", "pdf": "https://arxiv.org/pdf/2506.20128", "abs": "https://arxiv.org/abs/2506.20128", "authors": ["Aashiq Muhamed"], "title": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at LLM4Eval @ SIGIR 2025", "summary": "RAG systems enhance LLMs by incorporating external knowledge, which is\ncrucial for domains that demand factual accuracy and up-to-date information.\nHowever, evaluating the multifaceted quality of RAG outputs, spanning aspects\nsuch as contextual coherence, query relevance, factual correctness, and\ninformational completeness, poses significant challenges. Existing evaluation\nmethods often rely on simple lexical overlap metrics, which are inadequate for\ncapturing these nuances, or involve complex multi-stage pipelines with\nintermediate steps like claim extraction or require finetuning specialized\njudge models, hindering practical efficiency. To address these limitations, we\npropose CCRS (Contextual Coherence and Relevance Score), a novel suite of five\nmetrics that utilizes a single, powerful, pretrained LLM as a zero-shot,\nend-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance\n(QR), Information Density (ID), Answer Correctness (AC), and Information Recall\n(IR). We apply CCRS to evaluate six diverse RAG system configurations on the\nchallenging BioASQ dataset. Our analysis demonstrates that CCRS effectively\ndiscriminates between system performances, confirming, for instance, that the\nMistral-7B reader outperforms Llama variants. We provide a detailed analysis of\nCCRS metric properties, including score distributions, convergent/discriminant\nvalidity, tie rates, population statistics, and discriminative power. Compared\nto the complex RAGChecker framework, CCRS offers comparable or superior\ndiscriminative power for key aspects like recall and faithfulness, while being\nsignificantly more computationally efficient. CCRS thus provides a practical,\ncomprehensive, and efficient framework for evaluating and iteratively improving\nRAG systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCRS\u7684\u96f6\u6837\u672cLLM\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u3001\u95ee\u9898\u76f8\u5173\u6027\u3001\u4fe1\u606f\u5bc6\u5ea6\u3001\u7b54\u6848\u6b63\u786e\u6027\u548c\u4fe1\u606f\u53ec\u56de\u3002\u5b9e\u9a8c\u8868\u660eCCRS\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5224\u522b\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RAG\u7cfb\u7edf\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u591a\u4f9d\u8d56\u7b80\u5355\u7684\u8bcd\u6c47\u91cd\u53e0\u6307\u6807\u6216\u590d\u6742\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349RAG\u8f93\u51fa\u7684\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "CCRS\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u96f6\u6837\u672c\u7aef\u5230\u7aef\u8bc4\u4f30\u5668\uff0c\u63d0\u51fa\u4e94\u4e2a\u6307\u6807\uff1a\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff08CC\uff09\u3001\u95ee\u9898\u76f8\u5173\u6027\uff08QR\uff09\u3001\u4fe1\u606f\u5bc6\u5ea6\uff08ID\uff09\u3001\u7b54\u6848\u6b63\u786e\u6027\uff08AC\uff09\u548c\u4fe1\u606f\u53ec\u56de\uff08IR\uff09\u3002\u5728BioASQ\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u516d\u79cdRAG\u7cfb\u7edf\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCCRS\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540cRAG\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4f8b\u5982Mistral-7B\u4f18\u4e8eLlama\u53d8\u4f53\u3002CCRS\u5728\u5173\u952e\u6307\u6807\uff08\u5982\u53ec\u56de\u7387\u548c\u5fe0\u5b9e\u5ea6\uff09\u4e0a\u7684\u5224\u522b\u80fd\u529b\u4e0e\u590d\u6742\u6846\u67b6RAGChecker\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CCRS\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u5168\u9762\u4e14\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u8fed\u4ee3\u6539\u8fdb\u3002\u5176\u96f6\u6837\u672c\u8bbe\u8ba1\u548c\u591a\u7ef4\u5ea6\u6307\u6807\u4f7f\u5176\u6210\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u7406\u60f3\u66ff\u4ee3\u3002", "paper_title_zh": "CCRS\uff1a\u4e00\u79cd\u7528\u4e8e\u5168\u9762RAG\u8bc4\u4f30\u7684\u96f6\u6837\u672cLLM\u5373\u8bc4\u5224\u6846\u67b6", "abstract_zh": "RAG\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u589e\u5f3aLLM\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u6700\u65b0\u4fe1\u606f\u7684\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8bc4\u4f30RAG\u8f93\u51fa\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\uff08\u5982\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u3001\u67e5\u8be2\u76f8\u5173\u6027\u3001\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u4fe1\u606f\u5b8c\u6574\u6027\uff09\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u7b80\u5355\u7684\u8bcd\u6c47\u91cd\u53e0\u6307\u6807\uff0c\u65e0\u6cd5\u6355\u6349\u8fd9\u4e9b\u7ec6\u5fae\u5dee\u5f02\uff0c\u6216\u6d89\u53ca\u590d\u6742\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\uff08\u5982\u58f0\u660e\u63d0\u53d6\u6216\u5fae\u8c03\u4e13\u7528\u8bc4\u5224\u6a21\u578b\uff09\uff0c\u5f71\u54cd\u5b9e\u9645\u6548\u7387\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faCCRS\uff08\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u4e0e\u76f8\u5173\u6027\u8bc4\u5206\uff09\uff0c\u8fd9\u662f\u4e00\u5957\u5305\u542b\u4e94\u4e2a\u6307\u6807\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u4e00\u9884\u8bad\u7ec3LLM\u4f5c\u4e3a\u96f6\u6837\u672c\u7aef\u5230\u7aef\u8bc4\u5224\u5668\u3002CCRS\u8bc4\u4f30\uff1a\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff08CC\uff09\u3001\u95ee\u9898\u76f8\u5173\u6027\uff08QR\uff09\u3001\u4fe1\u606f\u5bc6\u5ea6\uff08ID\uff09\u3001\u7b54\u6848\u6b63\u786e\u6027\uff08AC\uff09\u548c\u4fe1\u606f\u53ec\u56de\uff08IR\uff09\u3002\u6211\u4eec\u5728\u5177\u6709\u6311\u6218\u6027\u7684BioASQ\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u516d\u79cdRAG\u7cfb\u7edf\u914d\u7f6e\u3002\u5206\u6790\u8868\u660e\uff0cCCRS\u80fd\u6709\u6548\u533a\u5206\u7cfb\u7edf\u6027\u80fd\uff0c\u4f8b\u5982Mistral-7B\u9605\u8bfb\u5668\u4f18\u4e8eLlama\u53d8\u4f53\u3002\u6211\u4eec\u8fd8\u8be6\u7ec6\u5206\u6790\u4e86CCRS\u6307\u6807\u7279\u6027\uff0c\u5305\u62ec\u5206\u6570\u5206\u5e03\u3001\u6536\u655b/\u5224\u522b\u6548\u5ea6\u3001\u5e73\u5c40\u7387\u3001\u603b\u4f53\u7edf\u8ba1\u548c\u5224\u522b\u80fd\u529b\u3002\u4e0e\u590d\u6742\u7684RAGChecker\u6846\u67b6\u76f8\u6bd4\uff0cCCRS\u5728\u5173\u952e\u65b9\u9762\uff08\u5982\u53ec\u56de\u7387\u548c\u5fe0\u5b9e\u5ea6\uff09\u7684\u5224\u522b\u80fd\u529b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002\u56e0\u6b64\uff0cCCRS\u4e3a\u8bc4\u4f30\u548c\u8fed\u4ee3\u6539\u8fdbRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u5168\u9762\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2506.20168", "pdf": "https://arxiv.org/pdf/2506.20168", "abs": "https://arxiv.org/abs/2506.20168", "authors": ["Zhentao He", "Can Zhang", "Ziheng Wu", "Zhenghao Chen", "Yufei Zhan", "Yifan Li", "Zhao Zhang", "Xian Wang", "Minghui Qiu"], "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u4e2d\u56e0\u89c6\u89c9\u9000\u5316\u5bfc\u81f4OCR\u5e7b\u89c9\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51faKIE-HVQA\u57fa\u51c6\u548cGRPO\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u89c6\u89c9\u9000\u5316\u573a\u666f\u4e0b\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u6216\u89c6\u89c9-\u6587\u672c\u63a8\u7406\u9519\u4f4d\uff0c\u5bfc\u81f4\u751f\u6210\u5e7b\u89c9\u5185\u5bb9\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faKIE-HVQA\u57fa\u51c6\u8bc4\u4f30OCR\u5e7b\u89c9\uff0c\u5e76\u8bbe\u8ba1GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u81ea\u6211\u611f\u77e5\u548c\u62d2\u7edd\u56de\u7b54\u673a\u5236\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c7B\u53c2\u6570\u6a21\u578b\u5728KIE-HVQA\u4e0a\u6bd4GPT-4o\u5e7b\u89c9\u51cf\u5c1122%\uff0c\u6807\u51c6\u4efb\u52a1\u6027\u80fd\u65e0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "GRPO\u6846\u67b6\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u9000\u5316\u4e0b\u7684\u5e7b\u89c9\u751f\u6210\uff0c\u517c\u5177\u6548\u679c\u4e0e\u9c81\u68d2\u6027\u3002", "paper_title_zh": "\u773c\u89c1\u4e3a\u5b9e\uff1f\u7f13\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684OCR\u5e7b\u89c9\u95ee\u9898", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u901a\u8fc7\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u63d0\u5347\u4e86\u6587\u6863\u7406\u89e3\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\uff0c\u8868\u73b0\u51fa\u8303\u5f0f\u4e0d\u5b8c\u6574\u6027\u3002\u5728\u6b64\u7c7b\u6761\u4ef6\u4e0b\uff0c\u5f53\u524d\u54cd\u5e94\u8303\u5f0f\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u611f\u77e5\u89c6\u89c9\u9000\u5316\u548c\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u6216\u89c6\u89c9-\u6587\u672c\u63a8\u7406\u9519\u4f4d\u3002\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u8bc6\u522b\u7684\u56f0\u96be\u5e38\u5e38\u5f15\u53d1\u5e7b\u89c9\u5185\u5bb9\u7684\u751f\u6210\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u7b54\u6848\u65f6\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u5c55\u793a\u548c\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u53ca\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86KIE-HVQA\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u9000\u5316\u6587\u6863\u7406\u89e3\u4e2dOCR\u5e7b\u89c9\u7684\u57fa\u51c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u8eab\u4efd\u8bc1\u548c\u53d1\u7968\u7b49\u6d4b\u8bd5\u6837\u672c\uff0c\u5e76\u6a21\u62df\u4e86\u73b0\u5b9e\u4e2d\u7684OCR\u53ef\u9760\u6027\u9000\u5316\u573a\u666f\u3002\u901a\u8fc7\u8fd9\u4e00\u8bbe\u7f6e\uff0c\u53ef\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u9000\u5316\u8f93\u5165\u4e0b\u533a\u5206\u53ef\u9760\u89c6\u89c9\u4fe1\u606f\u5e76\u636e\u6b64\u56de\u7b54\u7684\u80fd\u529b\uff0c\u4ece\u800c\u7a81\u663e\u907f\u514d\u5728\u4e0d\u786e\u5b9a\u6570\u636e\u4e0a\u751f\u6210\u5e7b\u89c9\u7684\u6311\u6218\u3002\u4e3a\u5b9e\u73b0\u89c6\u89c9\u53ef\u4fe1\u7684\u63a8\u7406\u5e76\u907f\u514d\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u57fa\u4e8eGRPO\u7684\u6846\u67b6\uff0c\u5176\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\u3002\u901a\u8fc7\u5c06\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u6211\u611f\u77e5\u548c\u62d2\u7edd\u56de\u7b54\u7684\u5206\u6790\u65b9\u6cd5\u7eb3\u5165\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6211\u4eec\u6210\u529f\u51cf\u5c11\u4e86\u6a21\u7cca\u533a\u57df\u7684\u5e7b\u89c9\u751f\u6210\u3002\u5728Qwen2.5-VL\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u76847B\u53c2\u6570\u6a21\u578b\u5728KIE-HVQA\u4e0a\u6bd4GPT-4o\u5b9e\u73b0\u4e8622%\u7684\u7edd\u5bf9\u5e7b\u89c9\u51cf\u5c11\uff0c\u4e14\u5728\u6807\u51c6\u4efb\u52a1\u4e2d\u6027\u80fd\u65e0\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u51fa\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.20249", "pdf": "https://arxiv.org/pdf/2506.20249", "abs": "https://arxiv.org/abs/2506.20249", "authors": ["Junyan Cheng", "Peter Clark", "Kyle Richardson"], "title": "Language Modeling by Language Models", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53LLM\u65b9\u6cd5Genesys\uff0c\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u8fc7\u7a0b\uff08\u4ece\u6784\u601d\u5230\u9a8c\u8bc1\uff09\u81ea\u52a8\u53d1\u73b0\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u3002\u91c7\u7528\u9057\u4f20\u7f16\u7a0b\u6846\u67b6\u548c\u89c4\u6a21\u6269\u5c55\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u751f\u6210\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u67b6\u6784\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528LLM\u6a21\u62df\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u53d1\u73b0\u8fc7\u7a0b\uff0c\u4ee5\u81ea\u52a8\u5316\u4f20\u7edf\u7814\u7a76\u4e2d\u4ece\u6784\u601d\u5230\u9a8c\u8bc1\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u4ece\u800c\u9ad8\u6548\u751f\u6210\u65b0\u9896\u4e14\u9ad8\u6027\u80fd\u7684\u6a21\u578b\u8bbe\u8ba1\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u591a\u667a\u80fd\u4f53LLM\u6a21\u62df\u7814\u7a76\u6d41\u7a0b\uff08\u63d0\u6848\u3001\u4ee3\u7801\u751f\u6210\u3001\u9884\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff09\uff1b2\uff09\u91c7\u7528\u89c4\u6a21\u6269\u5c55\u7b56\u7565\uff08\u4ece14M\u5230350M\u53c2\u6570\uff09\u548c\u9884\u7b97\u9650\u5236\uff1b3\uff09\u5f15\u5165\u9057\u4f20\u7f16\u7a0b\u6846\u67b6\uff0c\u4f18\u5316\u8bbe\u8ba1\u751f\u6210\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\uff1a1\uff09\u751f\u6210\u4e861,162\u4e2a\u65b0\u8bbe\u8ba1\uff0c\u5176\u4e2d1,062\u4e2a\u901a\u8fc7\u9884\u8bad\u7ec3\u9a8c\u8bc1\uff1b2\uff09\u6700\u4f73\u8bbe\u8ba1\u57286/9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eGPT2\u548cMamba2\u7b49\u73b0\u6709\u67b6\u6784\uff1b3\uff09\u9057\u4f20\u7f16\u7a0b\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u751f\u6210\u6210\u529f\u7387\uff08\u7ea686%\uff09\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0cGenesys\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u81ea\u52a8\u53d1\u73b0\u9ad8\u6027\u80fd\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u5176\u9057\u4f20\u7f16\u7a0b\u6846\u67b6\u548c\u89c4\u6a21\u6269\u5c55\u7b56\u7565\u4e3a\u81ea\u4e3b\u53d1\u73b0\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u5efa\u6a21", "abstract_zh": "\u6211\u4eec\u80fd\u5426\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u5efa\u6a21\u53d1\u73b0\u65b0\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u8fc7\u7a0b\uff1f\u53d7\u771f\u5b9e\u7814\u7a76\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53LLM\u65b9\u6cd5\uff0c\u6a21\u62df\u4ece\u6784\u601d\u548c\u6587\u732e\u68c0\u7d22\uff08\u63d0\u6848\u9636\u6bb5\uff09\u5230\u8bbe\u8ba1\u5b9e\u73b0\uff08\u4ee3\u7801\u751f\u6210\uff09\u3001\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u8bc4\u4f30\uff08\u9a8c\u8bc1\uff09\u7684\u4f20\u7edf\u7814\u7a76\u9636\u6bb5\u3002\u57fa\u4e8e\u89c4\u6a21\u6269\u5c55\u5b9a\u5f8b\uff0c\u6211\u4eec\u7684\u7cfb\u7edfGenesys\u91c7\u7528\u4e86\u4e00\u79cd\u201c\u89c4\u6a21\u9636\u68af\u201d\u65b9\u6cd5\uff1a\u65b0\u8bbe\u8ba1\u88ab\u63d0\u51fa\u3001\u5bf9\u6297\u6027\u8bc4\u5ba1\u3001\u5b9e\u73b0\uff0c\u5e76\u5728\u9010\u6e10\u589e\u5927\u7684\u6a21\u578b\u89c4\u6a21\uff0814M\u81f3350M\u53c2\u6570\uff09\u548c\u6709\u9650\u9884\u7b97\u4e0b\u9009\u62e9\u6027\u9a8c\u8bc1\u3002\u4e3a\u63d0\u9ad8\u53d1\u73b0\u6548\u7387\u548c\u53ef\u5206\u89e3\u6027\uff0cGenesys\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9057\u4f20\u7f16\u7a0b\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u5e38\u7528\u7684\u76f4\u63a5\u63d0\u793a\u751f\u6210\u5de5\u4f5c\u6d41\uff08\u4f8b\u5982\uff0c\u8bbe\u8ba1\u751f\u6210\u6210\u529f\u7387\u63d0\u5347\u7ea686\u4e2a\u767e\u5206\u70b9\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u74f6\u9888\uff09\u3002\u6211\u4eec\u62a5\u544a\u4e86\u6d89\u53ca1,162\u4e2a\u65b0\u8bbe\u8ba1\uff08\u5176\u4e2d1,062\u4e2a\u901a\u8fc7\u9884\u8bad\u7ec3\u5b8c\u5168\u9a8c\u8bc1\uff09\u7684\u5b9e\u9a8c\uff0c\u53d1\u73b0\u6700\u4f73\u8bbe\u8ba1\u57286/9\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5df2\u77e5\u67b6\u6784\uff08\u5982GPT2\u3001Mamba2\u7b49\uff09\u3002\u8fd9\u4e9b\u7ed3\u679c\u7ed3\u5408\u7cfb\u7edf\u7ea7\u6d88\u878d\u5b9e\u9a8c\u548c\u5f62\u5f0f\u5316\u7ed3\u679c\uff0c\u4e3a\u9ad8\u6548\u81ea\u4e3b\u53d1\u73b0\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.20160", "pdf": "https://arxiv.org/pdf/2506.20160", "abs": "https://arxiv.org/abs/2506.20160", "authors": ["Ruosen Li", "Ziming Luo", "Quan Zhang", "Ruochen Li", "Ben Zhou", "Ali Payani", "Xinya Du"], "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) achieve impressive reasoning capabilities by\ngenerating lengthy chain-of-thoughts, but this \"overthinking\" incurs high\nlatency and cost without commensurate accuracy gains. In this work, we\nintroduce AALC, a lightweight, accuracy-aware length reward integrated into\nreinforcement learning that dynamically balances correctness and brevity during\ntraining. By incorporating validation accuracy into the reward and employing a\nsmooth, dynamically scheduled length penalty, AALC delays length penalty until\ntarget performance is met. Through extensive experiments across standard and\nout-of-distribution math benchmarks, we show that our approach reduces response\nlength by over 50% while maintaining or even improving the original accuracy.\nFurthermore, qualitative analysis reveals that our method curbs redundant\nreasoning patterns such as excessive subgoal setting and verification, leading\nto structurally refined outputs rather than naive truncation. We also identify\nthat efficiency gains are accompanied by reduced interpretability: models\ntrained with AALC omit some narrative framing and explanatory context. These\nfindings highlight the potential of reward-based strategies to guide LRMs\ntoward more efficient, generalizable reasoning paths.", "AI": {"tldr": "AALC\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u9002\u5e94\u7cbe\u5ea6-\u957f\u5ea6\u63a7\u5236\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u54cd\u5e94\u957f\u5ea650%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u901a\u8fc7\u751f\u6210\u957f\u94fe\u63a8\u7406\u5b9e\u73b0\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5197\u957f\u7684\u63a8\u7406\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u4e14\u51c6\u786e\u6027\u63d0\u5347\u6709\u9650\u3002AALC\u65e8\u5728\u52a8\u6001\u5e73\u8861\u63a8\u7406\u7684\u6b63\u786e\u6027\u548c\u7b80\u6d01\u6027\u3002", "method": "AALC\u5c06\u8f7b\u91cf\u7ea7\u7684\u7cbe\u5ea6\u611f\u77e5\u957f\u5ea6\u5956\u52b1\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u7ed3\u5408\u9a8c\u8bc1\u7cbe\u5ea6\u548c\u52a8\u6001\u8c03\u5ea6\u7684\u957f\u5ea6\u60e9\u7f5a\uff0c\u5ef6\u8fdf\u957f\u5ea6\u60e9\u7f5a\u76f4\u81f3\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAALC\u5728\u6807\u51c6\u548c\u975e\u5206\u5e03\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u54cd\u5e94\u957f\u5ea650%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "AALC\u901a\u8fc7\u5956\u52b1\u7b56\u7565\u5f15\u5bfcLRMs\u751f\u6210\u66f4\u9ad8\u6548\u3001\u901a\u7528\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4f46\u53ef\u80fd\u727a\u7272\u90e8\u5206\u89e3\u91ca\u6027\u3002", "paper_title_zh": "AALC\uff1a\u901a\u8fc7\u81ea\u9002\u5e94\u7cbe\u5ea6-\u957f\u5ea6\u63a7\u5236\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u63a8\u7406", "abstract_zh": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u901a\u8fc7\u751f\u6210\u957f\u94fe\u63a8\u7406\u5c55\u73b0\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u4e14\u51c6\u786e\u6027\u63d0\u5347\u6709\u9650\u3002\u672c\u6587\u63d0\u51faAALC\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u7cbe\u5ea6\u611f\u77e5\u957f\u5ea6\u5956\u52b1\uff0c\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u52a8\u6001\u5e73\u8861\u8bad\u7ec3\u4e2d\u7684\u6b63\u786e\u6027\u548c\u7b80\u6d01\u6027\u3002\u901a\u8fc7\u5c06\u9a8c\u8bc1\u7cbe\u5ea6\u7eb3\u5165\u5956\u52b1\u5e76\u91c7\u7528\u52a8\u6001\u8c03\u5ea6\u7684\u957f\u5ea6\u60e9\u7f5a\uff0cAALC\u5ef6\u8fdf\u957f\u5ea6\u60e9\u7f5a\u76f4\u81f3\u76ee\u6807\u6027\u80fd\u8fbe\u6210\u3002\u5728\u6807\u51c6\u548c\u5206\u5e03\u5916\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAALC\u51cf\u5c11\u54cd\u5e94\u957f\u5ea650%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u539f\u59cb\u51c6\u786e\u6027\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6291\u5236\u4e86\u5197\u4f59\u63a8\u7406\u6a21\u5f0f\uff08\u5982\u8fc7\u591a\u7684\u5b50\u76ee\u6807\u8bbe\u5b9a\u548c\u9a8c\u8bc1\uff09\uff0c\u751f\u6210\u7ed3\u6784\u66f4\u7cbe\u70bc\u7684\u8f93\u51fa\u800c\u975e\u7b80\u5355\u622a\u65ad\u3002\u6b64\u5916\uff0c\u6548\u7387\u63d0\u5347\u4f34\u968f\u89e3\u91ca\u6027\u964d\u4f4e\uff1aAALC\u8bad\u7ec3\u7684\u6a21\u578b\u7701\u7565\u4e86\u4e00\u4e9b\u53d9\u8ff0\u6846\u67b6\u548c\u89e3\u91ca\u6027\u5185\u5bb9\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u5956\u52b1\u7b56\u7565\u5728\u5f15\u5bfcLRMs\u751f\u6210\u66f4\u9ad8\u6548\u3001\u901a\u7528\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20174", "pdf": "https://arxiv.org/pdf/2506.20174", "abs": "https://arxiv.org/abs/2506.20174", "authors": ["Man Duc Chuc"], "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models are rapidly transforming Earth Observation data mining by\nenabling generalizable and scalable solutions for key tasks such as scene\nclassification and semantic segmentation. While most efforts in the geospatial\ndomain have focused on developing large models trained from scratch using\nmassive Earth Observation datasets, an alternative strategy that remains\nunderexplored is the reuse and combination of existing pretrained models. In\nthis study, we investigate whether foundation models pretrained on remote\nsensing and general vision datasets can be effectively combined to improve\nperformance across a diverse set of key Earth Observation tasks. Using the\nGEO-Bench benchmark, we evaluate several prominent models, including Prithvi,\nHiera, and DOFA, on eleven datasets covering a range of spatial resolutions,\nsensor modalities, and task types. The results show that feature-level\nensembling of smaller pretrained models can match or exceed the performance of\nmuch larger models, while requiring less training time and computational\nresources. Moreover, the study highlights the potential of applying knowledge\ndistillation to transfer the strengths of ensembles into more compact models,\noffering a practical path for deploying foundation models in real-world Earth\nObservation applications.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7ec4\u5408\u9884\u8bad\u7ec3\u7684\u5c0f\u578b\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u77e5\u8bc6\u84b8\u998f\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u5c06\u96c6\u6210\u6a21\u578b\u4f18\u52bf\u8f6c\u79fb\u5230\u66f4\u7d27\u51d1\u6a21\u578b\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u5730\u7403\u89c2\u6d4b\u6570\u636e\u6316\u6398\u9886\u57df\u4e3b\u8981\u5173\u6ce8\u4ece\u5934\u8bad\u7ec3\u5927\u578b\u6a21\u578b\uff0c\u800c\u5ffd\u7565\u4e86\u5229\u7528\u548c\u7ec4\u5408\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u7ec4\u5408\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u5347\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u7814\u7a76\u4f7f\u7528GEO-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecPrithvi\u3001Hiera\u548cDOFA\u5728\u5185\u7684\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002\u901a\u8fc7\u7279\u5f81\u7ea7\u96c6\u6210\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u4e0e\u5927\u578b\u6a21\u578b\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u96c6\u6210\u6027\u80fd\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8a\u5927\u578b\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u77e5\u8bc6\u84b8\u998f\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5c06\u96c6\u6210\u6a21\u578b\u4f18\u52bf\u8f6c\u79fb\u5230\u7d27\u51d1\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7ec4\u5408\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5730\u7403\u89c2\u6d4b\u6570\u636e\u6316\u6398\u65b9\u6cd5\uff0c\u77e5\u8bc6\u84b8\u998f\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "paper_title_zh": "\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u7ec4\u5408\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u6cdb\u5316\u7684\u5730\u7403\u89c2\u6d4b\u6570\u636e\u6316\u6398", "abstract_zh": "\u57fa\u7840\u6a21\u578b\u6b63\u5728\u8fc5\u901f\u6539\u53d8\u5730\u7403\u89c2\u6d4b\u6570\u636e\u6316\u6398\uff0c\u4e3a\u573a\u666f\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u7b49\u5173\u952e\u4efb\u52a1\u63d0\u4f9b\u6cdb\u5316\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5c3d\u7ba1\u5730\u7406\u7a7a\u95f4\u9886\u57df\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5229\u7528\u5927\u89c4\u6a21\u5730\u7403\u89c2\u6d4b\u6570\u636e\u96c6\u4ece\u5934\u8bad\u7ec3\u5927\u578b\u6a21\u578b\uff0c\u4f46\u53e6\u4e00\u79cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u7b56\u7565\u662f\u91cd\u7528\u548c\u7ec4\u5408\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u9065\u611f\u6570\u636e\u548c\u901a\u7528\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u6709\u6548\u7ec4\u5408\uff0c\u4ee5\u63d0\u5347\u591a\u79cd\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u4f7f\u7528GEO-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5305\u62ecPrithvi\u3001Hiera\u548cDOFA\u5728\u5185\u7684\u591a\u4e2a\u6a21\u578b\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u4f20\u611f\u5668\u6a21\u6001\u548c\u4efb\u52a1\u7c7b\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7279\u5f81\u7ea7\u96c6\u6210\u6027\u80fd\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8a\u5927\u578b\u6a21\u578b\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u96c6\u6210\u6a21\u578b\u4f18\u52bf\u8f6c\u79fb\u5230\u66f4\u7d27\u51d1\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u5730\u7403\u89c2\u6d4b\u5e94\u7528\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.20274", "pdf": "https://arxiv.org/pdf/2506.20274", "abs": "https://arxiv.org/abs/2506.20274", "authors": ["Liya Wang", "David Yi", "Damien Jose", "John Passarelli", "James Gao", "Jordan Leventis", "Kang Li"], "title": "Enterprise Large Language Model Evaluation Benchmark", "categories": ["cs.AI"], "comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index", "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u768414\u4efb\u52a1\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u6807\u6ce8\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002\u901a\u8fc7\u8bc4\u4f30\u516d\u79cd\u9886\u5148\u6a21\u578b\uff0c\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5224\u65ad\u4efb\u52a1\u4e2d\u56e0\u8fc7\u5ea6\u601d\u8003\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\uff08\u5982MMLU\uff09\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4f01\u4e1a\u7279\u5b9a\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6307\u5bfc\u4f01\u4e1a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a14\u4efb\u52a1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86LLM-as-a-Labeler\u3001LLM-as-a-Judge\u548c\u7ea0\u6b63\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08CRAG\uff09\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b9,700\u4e2a\u6837\u672c\u7684\u7a33\u5065\u57fa\u51c6\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f00\u6e90\u6a21\u578b\uff08\u5982DeepSeek R1\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4e0e\u4e13\u6709\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u5728\u57fa\u4e8e\u5224\u65ad\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u8fc7\u5ea6\u601d\u8003\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u8bc4\u4f30\u7684\u84dd\u56fe\uff0c\u5e76\u63a8\u52a8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u5316\u90e8\u7f72\u3002", "paper_title_zh": "\u4f01\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63d0\u5347AI\u5de5\u5177\u751f\u4ea7\u529b\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\uff08\u5982\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3MMLU\uff09\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4f01\u4e1a\u7279\u5b9a\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u768414\u4efb\u52a1\u6846\u67b6\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30LLM\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u566a\u58f0\u6570\u636e\u548c\u6602\u8d35\u6807\u6ce8\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86LLM-as-a-Labeler\u3001LLM-as-a-Judge\u548c\u7ea0\u6b63\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08CRAG\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b9,700\u4e2a\u6837\u672c\u7684\u7a33\u5065\u57fa\u51c6\u3002\u5bf9\u516d\u79cd\u9886\u5148\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5f00\u6e90\u6a21\u578b\uff08\u5982DeepSeek R1\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u5728\u57fa\u4e8e\u5224\u65ad\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u8fc7\u5ea6\u601d\u8003\u3002\u6211\u4eec\u7684\u57fa\u51c6\u63ed\u793a\u4e86\u4f01\u4e1a\u6027\u80fd\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5e76\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u8bc4\u4f30\u7684\u84dd\u56fe\uff0c\u5e76\u63a8\u52a8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.20167", "pdf": "https://arxiv.org/pdf/2506.20167", "abs": "https://arxiv.org/abs/2506.20167", "authors": ["Fengze Li", "Yue Wang", "Yangle Liu", "Ming Huang", "Dou Hong", "Jieming Ma"], "title": "SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multivariate time series forecasting requires models to simultaneously\ncapture variable-wise structural dependencies and generalize across diverse\ntasks. While structural encoders are effective in modeling feature\ninteractions, they lack the capacity to support semantic-level reasoning or\ntask adaptation. Conversely, large language models (LLMs) possess strong\ngeneralization capabilities but remain incompatible with raw time series\ninputs. This gap limits the development of unified, transferable prediction\nsystems. Therefore, we introduce SEED, a structural encoder for\nembedding-driven decoding, which integrates four stages: a token-aware encoder\nfor patch extraction, a projection module that aligns patches with language\nmodel embeddings, a semantic reprogramming mechanism that maps patches to\ntask-aware prototypes, and a frozen language model for prediction. This modular\narchitecture decouples representation learning from inference, enabling\nefficient alignment between numerical patterns and semantic reasoning.\nEmpirical results demonstrate that the proposed method achieves consistent\nimprovements over strong baselines, and comparative studies on various datasets\nconfirm SEED's role in addressing the structural-semantic modeling gap.", "AI": {"tldr": "SEED\u662f\u4e00\u79cd\u7ed3\u6784\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5d4c\u5165\u9a71\u52a8\u7684\u89e3\u7801\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u4f9d\u8d56\u548c\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u7ed3\u6784-\u8bed\u4e49\u5efa\u6a21\u5dee\u8ddd\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9700\u8981\u6a21\u578b\u540c\u65f6\u6355\u6349\u53d8\u91cf\u95f4\u7684\u7ed3\u6784\u4f9d\u8d56\u5e76\u9002\u5e94\u591a\u6837\u4efb\u52a1\u3002\u73b0\u6709\u7ed3\u6784\u7f16\u7801\u5668\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5bfc\u81f4\u7edf\u4e00\u7684\u3001\u53ef\u8fc1\u79fb\u7684\u9884\u6d4b\u7cfb\u7edf\u53d1\u5c55\u53d7\u9650\u3002", "method": "SEED\u91c7\u7528\u56db\u9636\u6bb5\u6a21\u5757\u5316\u67b6\u6784\uff1a1\uff09\u57fa\u4e8e\u4ee4\u724c\u611f\u77e5\u7684\u7f16\u7801\u5668\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u7247\u6bb5\uff1b2\uff09\u6295\u5f71\u6a21\u5757\u5c06\u7247\u6bb5\u4e0e\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u5bf9\u9f50\uff1b3\uff09\u8bed\u4e49\u91cd\u7f16\u7a0b\u673a\u5236\u5c06\u7247\u6bb5\u6620\u5c04\u4e3a\u4efb\u52a1\u611f\u77e5\u539f\u578b\uff1b4\uff09\u51bb\u7ed3\u7684\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u89e3\u8026\u4e86\u8868\u793a\u5b66\u4e60\u4e0e\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u6570\u503c\u6a21\u5f0f\u4e0e\u8bed\u4e49\u63a8\u7406\u7684\u9ad8\u6548\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEED\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u89e3\u51b3\u7ed3\u6784-\u8bed\u4e49\u5efa\u6a21\u5dee\u8ddd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SEED\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u7f16\u7801\u5668\u548c\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u586b\u8865\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u7ed3\u6784-\u8bed\u4e49\u5efa\u6a21\u7a7a\u767d\uff0c\u4e3a\u7edf\u4e00\u3001\u53ef\u8fc1\u79fb\u7684\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "SEED\uff1a\u4e00\u79cd\u7528\u4e8e\u5d4c\u5165\u9a71\u52a8\u89e3\u7801\u7684\u7ed3\u6784\u7f16\u7801\u5668\uff0c\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b", "abstract_zh": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8981\u6c42\u6a21\u578b\u540c\u65f6\u6355\u6349\u53d8\u91cf\u95f4\u7684\u7ed3\u6784\u4f9d\u8d56\u5e76\u9002\u5e94\u591a\u6837\u4efb\u52a1\u3002\u7ed3\u6784\u7f16\u7801\u5668\u867d\u80fd\u6709\u6548\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u7ea7\u63a8\u7406\u6216\u4efb\u52a1\u9002\u5e94\u80fd\u529b\uff1b\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u5177\u5907\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5374\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u3002\u8fd9\u4e00\u5dee\u8ddd\u9650\u5236\u4e86\u7edf\u4e00\u3001\u53ef\u8fc1\u79fb\u9884\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faSEED\uff0c\u4e00\u79cd\u7528\u4e8e\u5d4c\u5165\u9a71\u52a8\u89e3\u7801\u7684\u7ed3\u6784\u7f16\u7801\u5668\uff0c\u5176\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a\u4ee4\u724c\u611f\u77e5\u7f16\u7801\u5668\u63d0\u53d6\u7247\u6bb5\u3001\u6295\u5f71\u6a21\u5757\u5bf9\u9f50\u7247\u6bb5\u4e0e\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u3001\u8bed\u4e49\u91cd\u7f16\u7a0b\u673a\u5236\u5c06\u7247\u6bb5\u6620\u5c04\u4e3a\u4efb\u52a1\u611f\u77e5\u539f\u578b\uff0c\u4ee5\u53ca\u51bb\u7ed3\u7684\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u9884\u6d4b\u3002\u8fd9\u79cd\u6a21\u5757\u5316\u67b6\u6784\u89e3\u8026\u4e86\u8868\u793a\u5b66\u4e60\u4e0e\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u6570\u503c\u6a21\u5f0f\u4e0e\u8bed\u4e49\u63a8\u7406\u7684\u9ad8\u6548\u5bf9\u9f50\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86SEED\u5728\u89e3\u51b3\u7ed3\u6784-\u8bed\u4e49\u5efa\u6a21\u5dee\u8ddd\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2506.20179", "pdf": "https://arxiv.org/pdf/2506.20179", "abs": "https://arxiv.org/abs/2506.20179", "authors": ["Enzhe Zhao", "Zhichang Guo", "Yao Li", "Fanghui Song", "Boying Wu"], "title": "Progressive Alignment Degradation Learning for Pansharpening", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 9 figures", "summary": "Deep learning-based pansharpening has been shown to effectively generate\nhigh-resolution multispectral (HRMS) images. To create supervised ground-truth\nHRMS images, synthetic data generated using the Wald protocol is commonly\nemployed. This protocol assumes that networks trained on artificial\nlow-resolution data will perform equally well on high-resolution data. However,\nwell-trained models typically exhibit a trade-off in performance between\nreduced-resolution and full-resolution datasets. In this paper, we delve into\nthe Wald protocol and find that its inaccurate approximation of real-world\ndegradation patterns limits the generalization of deep pansharpening models. To\naddress this issue, we propose the Progressive Alignment Degradation Module\n(PADM), which uses mutual iteration between two sub-networks, PAlignNet and\nPDegradeNet, to adaptively learn accurate degradation processes without relying\non predefined operators. Building on this, we introduce HFreqdiff, which embeds\nhigh-frequency details into a diffusion framework and incorporates CFB and BACM\nmodules for frequency-selective detail extraction and precise reverse process\nlearning. These innovations enable effective integration of high-resolution\npanchromatic and multispectral images, significantly enhancing spatial\nsharpness and quality. Experiments and ablation studies demonstrate the\nproposed method's superior performance compared to state-of-the-art techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5bf9\u9f50\u9000\u5316\u5b66\u4e60\u6a21\u5757\uff08PADM\uff09\u548cHFreqdiff\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u8272\u9510\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u591a\u5149\u8c31\u56fe\u50cf\u7684\u7a7a\u95f4\u6e05\u6670\u5ea6\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684Wald\u534f\u8bae\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5168\u8272\u9510\u5316\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPADM\u6a21\u5757\uff0c\u901a\u8fc7PAlignNet\u548cPDegradeNet\u7684\u76f8\u4e92\u8fed\u4ee3\u81ea\u9002\u5e94\u5b66\u4e60\u9000\u5316\u8fc7\u7a0b\uff1b\u5f15\u5165HFreqdiff\u6846\u67b6\uff0c\u7ed3\u5408CFB\u548cBACM\u6a21\u5757\u8fdb\u884c\u9ad8\u9891\u7ec6\u8282\u63d0\u53d6\u548c\u53cd\u5411\u8fc7\u7a0b\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a7a\u95f4\u6e05\u6670\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5168\u8272\u9510\u5316\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u591a\u5149\u8c31\u56fe\u50cf\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u6e10\u8fdb\u5bf9\u9f50\u9000\u5316\u5b66\u4e60\u7528\u4e8e\u5168\u8272\u9510\u5316", "abstract_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u8272\u9510\u5316\u65b9\u6cd5\u5df2\u8bc1\u660e\u80fd\u6709\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u591a\u5149\u8c31\uff08HRMS\uff09\u56fe\u50cf\u3002\u901a\u5e38\u4f7f\u7528Wald\u534f\u8bae\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u76d1\u7763\u5b66\u4e60\u7684\u771f\u5b9eHRMS\u56fe\u50cf\u3002\u8be5\u534f\u8bae\u5047\u8bbe\u5728\u4eba\u5de5\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u7f51\u7edc\u5728\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u4e0a\u8868\u73b0\u540c\u6837\u51fa\u8272\u3002\u7136\u800c\uff0c\u8bad\u7ec3\u826f\u597d\u7684\u6a21\u578b\u901a\u5e38\u5728\u964d\u5206\u8fa8\u7387\u548c\u5168\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u6743\u8861\u3002\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86Wald\u534f\u8bae\uff0c\u53d1\u73b0\u5176\u5bf9\u771f\u5b9e\u4e16\u754c\u9000\u5316\u6a21\u5f0f\u7684\u4e0d\u51c6\u786e\u8fd1\u4f3c\u9650\u5236\u4e86\u6df1\u5ea6\u5168\u8272\u9510\u5316\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6e10\u8fdb\u5bf9\u9f50\u9000\u5316\u6a21\u5757\uff08PADM\uff09\uff0c\u901a\u8fc7PAlignNet\u548cPDegradeNet\u4e24\u4e2a\u5b50\u7f51\u7edc\u7684\u76f8\u4e92\u8fed\u4ee3\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u51c6\u786e\u7684\u9000\u5316\u8fc7\u7a0b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b97\u5b50\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u4e86HFreqdiff\uff0c\u5c06\u9ad8\u9891\u7ec6\u8282\u5d4c\u5165\u6269\u6563\u6846\u67b6\uff0c\u5e76\u7ed3\u5408CFB\u548cBACM\u6a21\u5757\u8fdb\u884c\u9891\u7387\u9009\u62e9\u6027\u7ec6\u8282\u63d0\u53d6\u548c\u7cbe\u786e\u53cd\u5411\u8fc7\u7a0b\u5b66\u4e60\u3002\u8fd9\u4e9b\u521b\u65b0\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u5168\u8272\u548c\u591a\u5149\u8c31\u56fe\u50cf\u7684\u6709\u6548\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u6e05\u6670\u5ea6\u548c\u8d28\u91cf\u3002\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.20332", "pdf": "https://arxiv.org/pdf/2506.20332", "abs": "https://arxiv.org/abs/2506.20332", "authors": ["Jihao Gu", "Qihang Ai", "Yingyao Wang", "Pi Bu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Ziming Wang", "Yingxiu Zhao", "Ming-Liang Zhang", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards", "categories": ["cs.AI"], "comment": "14 pages, 12 figures", "summary": "Vision-language model-based mobile agents have gained the ability to not only\nunderstand complex instructions and mobile screenshots, but also optimize their\naction outputs via thinking and reasoning, benefiting from reinforcement\nlearning, such as Group Relative Policy Optimization (GRPO). However, existing\nresearch centers on offline reinforcement learning training or online\noptimization using action-level rewards, which limits the agent's dynamic\ninteraction with the environment. This often results in agents settling into\nlocal optima, thereby weakening their ability for exploration and error action\ncorrection. To address these challenges, we introduce an approach called\nMobile-R1, which employs interactive multi-turn reinforcement learning with\ntask-level rewards for mobile agents. Our training framework consists of three\nstages: initial format finetuning, single-step online training via action-level\nreward, followed by online training via task-level reward based on multi-turn\ntrajectories. This strategy is designed to enhance the exploration and error\ncorrection capabilities of Mobile-R1, leading to significant performance\nimprovements. Moreover, we have collected a dataset covering 28 Chinese\napplications with 24,521 high-quality manual annotations and established a new\nbenchmark with 500 trajectories. We will open source all resources, including\nthe dataset, benchmark, model weight, and codes:\nhttps://mobile-r1.github.io/Mobile-R1/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMobile-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u7ea7\u5956\u52b1\u7684\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u79fb\u52a8\u4ee3\u7406\u7684\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\u3002\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u521d\u59cb\u5fae\u8c03\u3001\u5355\u6b65\u5728\u7ebf\u8bad\u7ec3\u3001\u591a\u8f6e\u4efb\u52a1\u7ea7\u5956\u52b1\u8bad\u7ec3\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6216\u52a8\u4f5c\u7ea7\u5956\u52b1\u7684\u5728\u7ebf\u4f18\u5316\uff0c\u9650\u5236\u4e86\u79fb\u52a8\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u52a8\u6001\u4ea4\u4e92\u80fd\u529b\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4efb\u52a1\u7ea7\u5956\u52b1\u7684\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u4ee3\u7406\u7684\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\u3002", "method": "\u63d0\u51faMobile-R1\u65b9\u6cd5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u521d\u59cb\u683c\u5f0f\u5fae\u8c03\uff1b2\uff09\u57fa\u4e8e\u52a8\u4f5c\u7ea7\u5956\u52b1\u7684\u5355\u6b65\u5728\u7ebf\u8bad\u7ec3\uff1b3\uff09\u57fa\u4e8e\u591a\u8f6e\u8f68\u8ff9\u7684\u4efb\u52a1\u7ea7\u5956\u52b1\u5728\u7ebf\u8bad\u7ec3\u3002\u901a\u8fc7\u4efb\u52a1\u7ea7\u5956\u52b1\u589e\u5f3a\u4ee3\u7406\u7684\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\u3002", "result": "Mobile-R1\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b28\u4e2a\u4e2d\u6587\u5e94\u7528\u300124,521\u6761\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u53ca500\u6761\u8f68\u8ff9\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002\u6240\u6709\u8d44\u6e90\uff08\u6570\u636e\u96c6\u3001\u57fa\u51c6\u3001\u6a21\u578b\u6743\u91cd\u3001\u4ee3\u7801\uff09\u5747\u5df2\u5f00\u6e90\u3002", "conclusion": "Mobile-R1\u901a\u8fc7\u4efb\u52a1\u7ea7\u5956\u52b1\u7684\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79fb\u52a8\u4ee3\u7406\u7684\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u5f00\u6e90\u8d44\u6e90\u3002", "paper_title_zh": "Mobile-R1\uff1a\u57fa\u4e8e\u4efb\u52a1\u7ea7\u5956\u52b1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u79fb\u52a8\u4ee3\u7406\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5", "abstract_zh": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u79fb\u52a8\u4ee3\u7406\u4e0d\u4ec5\u80fd\u7406\u89e3\u590d\u6742\u6307\u4ee4\u548c\u79fb\u52a8\u5c4f\u5e55\u622a\u56fe\uff0c\u8fd8\u80fd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08\u5982\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316GRPO\uff09\u4f18\u5316\u5176\u52a8\u4f5c\u8f93\u51fa\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6216\u57fa\u4e8e\u52a8\u4f5c\u7ea7\u5956\u52b1\u7684\u5728\u7ebf\u4f18\u5316\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u52a8\u6001\u4ea4\u4e92\u80fd\u529b\uff0c\u5bfc\u81f4\u4ee3\u7406\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u524a\u5f31\u4e86\u63a2\u7d22\u4e0e\u9519\u8bef\u52a8\u4f5c\u7ea0\u6b63\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faMobile-R1\u65b9\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u4efb\u52a1\u7ea7\u5956\u52b1\u7684\u4ea4\u4e92\u5f0f\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u3002\u8bad\u7ec3\u6846\u67b6\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a\u521d\u59cb\u683c\u5f0f\u5fae\u8c03\u3001\u57fa\u4e8e\u52a8\u4f5c\u7ea7\u5956\u52b1\u7684\u5355\u6b65\u5728\u7ebf\u8bad\u7ec3\uff0c\u4ee5\u53ca\u57fa\u4e8e\u591a\u8f6e\u8f68\u8ff9\u7684\u4efb\u52a1\u7ea7\u5956\u52b1\u5728\u7ebf\u8bad\u7ec3\u3002\u8fd9\u4e00\u7b56\u7565\u65e8\u5728\u589e\u5f3aMobile-R1\u7684\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u6db5\u76d628\u4e2a\u4e2d\u6587\u5e94\u7528\u768424,521\u6761\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b500\u6761\u8f68\u8ff9\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002\u6240\u6709\u8d44\u6e90\uff08\u6570\u636e\u96c6\u3001\u57fa\u51c6\u3001\u6a21\u578b\u6743\u91cd\u3001\u4ee3\u7801\uff09\u5747\u5df2\u5f00\u6e90\uff1ahttps://mobile-r1.github.io/Mobile-R1/\u3002"}}
{"id": "2506.20178", "pdf": "https://arxiv.org/pdf/2506.20178", "abs": "https://arxiv.org/abs/2506.20178", "authors": ["Zhiyuan Wang", "Jinhao Duan", "Qingni Wang", "Xiaofeng Zhu", "Tianlong Chen", "Xiaoshuang Shi", "Kaidi Xu"], "title": "COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Uncertainty quantification (UQ) for foundation models is essential to\nidentify and mitigate potential hallucinations in automatically generated text.\nHowever, heuristic UQ approaches lack formal guarantees for key metrics such as\nthe false discovery rate (FDR) in selective prediction. Previous work adopts\nthe split conformal prediction (SCP) framework to ensure desired coverage of\nadmissible answers by constructing prediction sets, but these sets often\ncontain incorrect candidates, limiting their practical utility. To address\nthis, we propose COIN, an uncertainty-guarding selection framework that\ncalibrates statistically valid thresholds to filter a single generated answer\nper question under user-specified FDR constraints. COIN estimates the empirical\nerror rate on a calibration set and applies confidence interval methods such as\nClopper-Pearson to establish a high-probability upper bound on the true error\nrate (i.e., FDR). This enables the selection of the largest uncertainty\nthreshold that ensures FDR control on test data while significantly increasing\nsample retention. We demonstrate COIN's robustness in risk control, strong\ntest-time power in retaining admissible answers, and predictive efficiency\nunder limited calibration data across both general and multimodal text\ngeneration tasks. Furthermore, we show that employing alternative upper bound\nconstructions and UQ strategies can further boost COIN's power performance,\nwhich underscores its extensibility and adaptability to diverse application\nscenarios.", "AI": {"tldr": "COIN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u9009\u62e9\u6027\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u6821\u51c6\u9608\u503c\u5728\u7528\u6237\u6307\u5b9a\u7684FDR\u7ea6\u675f\u4e0b\u7b5b\u9009\u5355\u4e00\u7b54\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u4fdd\u7559\u7387\u548c\u9884\u6d4b\u6548\u7387\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u5bf9\u4e8e\u8bc6\u522b\u548c\u51cf\u5c11\u81ea\u52a8\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9009\u62e9\u6027\u9884\u6d4b\u4e2d\u5173\u952e\u6307\u6807\uff08\u5982FDR\uff09\u7684\u6b63\u5f0f\u4fdd\u8bc1\u3002", "method": "COIN\u901a\u8fc7\u6821\u51c6\u96c6\u4f30\u8ba1\u7ecf\u9a8c\u9519\u8bef\u7387\uff0c\u5e76\u5e94\u7528Clopper-Pearson\u7b49\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u5efa\u7acb\u771f\u5b9e\u9519\u8bef\u7387\u7684\u9ad8\u6982\u7387\u4e0a\u754c\uff08\u5373FDR\uff09\uff0c\u4ece\u800c\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0FDR\u63a7\u5236\u3002", "result": "COIN\u5728\u98ce\u9669\u63a7\u5236\u3001\u6d4b\u8bd5\u65f6\u4fdd\u7559\u5408\u683c\u7b54\u6848\u7684\u80fd\u529b\u4ee5\u53ca\u6709\u9650\u6821\u51c6\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u901a\u8fc7\u66ff\u4ee3\u4e0a\u754c\u6784\u9020\u548cUQ\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "COIN\u6846\u67b6\u5728\u4fdd\u8bc1FDR\u63a7\u5236\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u4fdd\u7559\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "paper_title_zh": "COIN\uff1a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u9009\u62e9\u6027\u95ee\u7b54\u6846\u67b6\uff0c\u4e3a\u57fa\u5ea7\u6a21\u578b\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u98ce\u9669\u4fdd\u8bc1", "abstract_zh": "\u57fa\u7840\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u5bf9\u4e8e\u8bc6\u522b\u548c\u51cf\u5c11\u81ea\u52a8\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9009\u62e9\u6027\u9884\u6d4b\u4e2d\u5173\u952e\u6307\u6807\uff08\u5982FDR\uff09\u7684\u6b63\u5f0f\u4fdd\u8bc1\u3002\u5148\u524d\u5de5\u4f5c\u91c7\u7528\u5206\u62c6\u5171\u5f62\u9884\u6d4b\uff08SCP\uff09\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u9884\u6d4b\u96c6\u6765\u786e\u4fdd\u5408\u683c\u7b54\u6848\u7684\u8986\u76d6\u7387\uff0c\u4f46\u8fd9\u4e9b\u96c6\u5408\u5e38\u5305\u542b\u9519\u8bef\u5019\u9009\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faCOIN\uff0c\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u6821\u51c6\u9608\u503c\u5728\u7528\u6237\u6307\u5b9a\u7684FDR\u7ea6\u675f\u4e0b\u7b5b\u9009\u5355\u4e00\u751f\u6210\u7b54\u6848\u3002COIN\u901a\u8fc7\u6821\u51c6\u96c6\u4f30\u8ba1\u7ecf\u9a8c\u9519\u8bef\u7387\uff0c\u5e76\u5e94\u7528Clopper-Pearson\u7b49\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u5efa\u7acb\u771f\u5b9e\u9519\u8bef\u7387\u7684\u9ad8\u6982\u7387\u4e0a\u754c\uff08\u5373FDR\uff09\uff0c\u4ece\u800c\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0FDR\u63a7\u5236\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6837\u672c\u4fdd\u7559\u7387\u3002\u6211\u4eec\u5c55\u793a\u4e86COIN\u5728\u98ce\u9669\u63a7\u5236\u3001\u6d4b\u8bd5\u65f6\u4fdd\u7559\u5408\u683c\u7b54\u6848\u7684\u80fd\u529b\u4ee5\u53ca\u6709\u9650\u6821\u51c6\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u6548\u7387\u65b9\u9762\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8868\u660e\u901a\u8fc7\u66ff\u4ee3\u4e0a\u754c\u6784\u9020\u548cUQ\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u5176\u5728\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.20214", "pdf": "https://arxiv.org/pdf/2506.20214", "abs": "https://arxiv.org/abs/2506.20214", "authors": ["Yanzhe Chen", "Huasong Zhong", "Yan Li", "Zhenheng Yang"], "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.MM"], "comment": "19 pages, 5 figures", "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.", "AI": {"tldr": "UniCode$^2$\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea7\u8054\u5927\u89c4\u6a21\u89c6\u89c9\u7801\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7801\u672c\u65b9\u6cd5\u5728\u5c0f\u8bcd\u6c47\u91cf\u6216\u5927\u89c4\u6a21\u6269\u5c55\u65f6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7801\u672c\u8bbe\u8ba1\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5c0f\u8bcd\u6c47\u91cf\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u800c\u5927\u89c4\u6a21\u6269\u5c55\u5219\u5bfc\u81f4\u4f4e\u6548\u7684\u4ee4\u724c\u5229\u7528\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u3002UniCode$^2$\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u89c6\u89c9\u4ee4\u724c\u5316\u3002", "method": "UniCode$^2$\u901a\u8fc7\u805a\u7c7b\u6570\u767e\u4e07SigLIP\u5e8f\u5217\u5d4c\u5165\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a50\u4e07\u6761\u76ee\u7684\u7801\u672c\uff0c\u4fdd\u6301\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u8bed\u4e49\u5bf9\u9f50\u3002\u91c7\u7528\u7ea7\u8054\u8bbe\u8ba1\uff1a\u51bb\u7ed3\u7801\u672c\u56fa\u5b9a\u5d4c\u5165\u7a7a\u95f4\uff0c\u53ef\u8bad\u7ec3\u7801\u672c\u4f18\u5316\u4efb\u52a1\u7279\u5b9a\u8bed\u4e49\uff0c\u4ece\u800c\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u9ad8\u5229\u7528\u7387\u3002", "result": "UniCode$^2$\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u727a\u7272\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u6216\u6a21\u5757\u6027\u7684\u524d\u63d0\u4e0b\u6269\u5c55\u89c6\u89c9\u4ee4\u724c\u7a7a\u95f4\u7684\u53ef\u884c\u6027\uff0c\u5e76\u652f\u6301\u9ad8\u8d28\u91cf\u89c6\u89c9\u5408\u6210\u3002", "conclusion": "UniCode$^2$\u901a\u8fc7\u7ea7\u8054\u5927\u89c4\u6a21\u7801\u672c\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u4ee4\u724c\u5316\u7684\u7a33\u5b9a\u6027\u4e0e\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "UniCode$^2$\uff1a\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u7ea7\u8054\u5927\u89c4\u6a21\u7801\u672c", "abstract_zh": "\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8054\u5408\u63a8\u8fdb\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u5176\u4e2d\u89c6\u89c9\u7801\u672c\u5c06\u56fe\u50cf\u79bb\u6563\u5316\u4e3a\u4ee4\u724c\u4ee5\u8fdb\u884c\u81ea\u56de\u5f52\u5efa\u6a21\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u7801\u672c\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u5c0f\u8bcd\u6c47\u91cf\uff08\u7ea61.6\u4e07\u6761\u76ee\uff09\uff0c\u8981\u4e48\u7b80\u5355\u5730\u6269\u5927\u89c4\u6a21\uff0c\u5bfc\u81f4\u4ee4\u724c\u5229\u7528\u7387\u4f4e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u6211\u4eec\u63d0\u51fa\u4e86UniCode$^2$\uff0c\u4e00\u79cd\u7ea7\u8054\u7801\u672c\u6846\u67b6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u7a33\u5b9a\u7684\u89c6\u89c9\u4ee4\u724c\u5316\u3002\u901a\u8fc7\u805a\u7c7b\u6570\u767e\u4e07SigLIP\u5e8f\u5217\u5d4c\u5165\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a50\u4e07\u6761\u76ee\u7684\u7801\u672c\uff0c\u65e2\u4fdd\u7559\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u53c8\u6269\u5c55\u4e86\u5bb9\u91cf\u3002\u7a33\u5b9a\u6027\u901a\u8fc7\u7ea7\u8054\u8bbe\u8ba1\u5f97\u5230\u4fdd\u8bc1\uff1a\u51bb\u7ed3\u7801\u672c\u56fa\u5b9a\u5d4c\u5165\u7a7a\u95f4\uff0c\u53ef\u8bad\u7ec3\u7801\u672c\u4f18\u5316\u4efb\u52a1\u7279\u5b9a\u8bed\u4e49\u3002\u8fd9\u79cd\u89e3\u8026\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u5229\u7528\u7387\u548c\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u89c6\u89c9\u4ee4\u724c\u4e0e\u6587\u672c\u8bed\u4e49\u7684\u5bf9\u9f50\u4f7f\u5176\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u6269\u6563\u89e3\u7801\u5668\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u89c6\u89c9\u5408\u6210\u4e14\u4ec5\u9700\u6700\u5c0f\u9002\u914d\u3002UniCode$^2$\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u727a\u7272\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u6216\u6a21\u5757\u6027\u7684\u524d\u63d0\u4e0b\u6269\u5c55\u89c6\u89c9\u4ee4\u724c\u7a7a\u95f4\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.20357", "pdf": "https://arxiv.org/pdf/2506.20357", "abs": "https://arxiv.org/abs/2506.20357", "authors": ["Sungwon Han", "Sungkyu Park", "Seungeon Lee"], "title": "Tabular Feature Discovery With Reasoning Type Exploration", "categories": ["cs.AI"], "comment": null, "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREFeat\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5229\u7528\u591a\u79cd\u63a8\u7406\u7c7b\u578b\u751f\u6210\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u5728\u8868\u683c\u6570\u636e\u7279\u5f81\u5de5\u7a0b\u4e2d\u751f\u6210\u7b80\u5355\u6216\u91cd\u590d\u7279\u5f81\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u572859\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u53d1\u73b0\u4e86\u66f4\u591a\u6837\u4e14\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7684\u7279\u5f81\u5de5\u7a0b\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u6b65\u9aa4\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5df2\u88ab\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u65b0\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u56e0LLM\u7684\u56fa\u6709\u504f\u5dee\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u6307\u5bfc\u800c\u751f\u6210\u8fc7\u4e8e\u7b80\u5355\u6216\u91cd\u590d\u7684\u7279\u5f81\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u591a\u79cd\u63a8\u7406\u7c7b\u578b\u548c\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\uff0c\u6539\u8fdbLLM\u9a71\u52a8\u7684\u7279\u5f81\u53d1\u73b0\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREFeat\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfcLLM\u5229\u7528\u591a\u79cd\u63a8\u7406\u7c7b\u578b\uff08\u5982\u903b\u8f91\u63a8\u7406\u3001\u7c7b\u6bd4\u63a8\u7406\u7b49\uff09\u6765\u751f\u6210\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5728\u7279\u5f81\u751f\u6210\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u6307\u5bfc\u548c\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\uff0c\u4ee5\u907f\u514d\u751f\u6210\u7b80\u5355\u6216\u91cd\u590d\u7684\u7279\u5f81\u3002", "result": "\u572859\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cREFeat\u65b9\u6cd5\u4e0d\u4ec5\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u8fd8\u80fd\u53d1\u73b0\u66f4\u591a\u6837\u4e14\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5728LLM\u9a71\u52a8\u7684\u7279\u5f81\u53d1\u73b0\u4e2d\u5f15\u5165\u4e30\u5bcc\u63a8\u7406\u8303\u5f0f\u548c\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684REFeat\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u63a8\u7406\u7c7b\u578b\u548c\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u8868\u683c\u6570\u636e\u7279\u5f81\u5de5\u7a0b\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u7684\u7279\u5f81\u53d1\u73b0\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "paper_title_zh": "\u57fa\u4e8e\u63a8\u7406\u7c7b\u578b\u63a2\u7d22\u7684\u8868\u683c\u7279\u5f81\u53d1\u73b0", "abstract_zh": "\u8868\u683c\u6570\u636e\u7684\u7279\u5f81\u5de5\u7a0b\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4e00\u6b65\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u7528\u4e8e\u901a\u8fc7\u5176\u5e7f\u6cdb\u7684\u77e5\u8bc6\u81ea\u52a8\u751f\u6210\u65b0\u7279\u5f81\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5e38\u56e0LLM\u9009\u62e9\u7684\u8f6c\u6362\u56fa\u6709\u504f\u5dee\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u6307\u5bfc\uff0c\u800c\u751f\u6210\u8fc7\u4e8e\u7b80\u5355\u6216\u91cd\u590d\u7684\u7279\u5f81\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREFeat\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u591a\u79cd\u63a8\u7406\u7c7b\u578b\u5f15\u5bfcLLM\u751f\u6210\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u3002\u572859\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u8fd8\u80fd\u53d1\u73b0\u66f4\u591a\u6837\u4e14\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u5728LLM\u9a71\u52a8\u7684\u8868\u683c\u6570\u636e\u7279\u5f81\u53d1\u73b0\u4e2d\u5f15\u5165\u4e30\u5bcc\u63a8\u7406\u8303\u5f0f\u548c\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u7684\u524d\u666f\u3002"}}
{"id": "2506.20199", "pdf": "https://arxiv.org/pdf/2506.20199", "abs": "https://arxiv.org/abs/2506.20199", "authors": ["Mengqi Wang", "Tiantian Feng", "Shrikanth Narayanan"], "title": "How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u68c0\u7d22\u9ad8\u8d28\u91cf\u793a\u4f8b\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u589e\u5f3a\u793a\u4f8b\u68c0\u7d22\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u4e3b\u89c2\u4efb\u52a1\u5982\u60c5\u611f\u8bc6\u522b\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4ecd\u5177\u6311\u6218\u6027\u3002\u53d7SLT 2024 GenSER\u6311\u6218\u8d5b\u542f\u53d1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u793a\u4f8b\u68c0\u7d22\u65b9\u6cd5\uff0c\u63d0\u5347\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u968f\u673a\u548c\u589e\u5f3a\u793a\u4f8b\u68c0\u7d22\u7684\u7b56\u7565\uff0c\u5e76\u5206\u6790\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5bf9\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u5728IEMOCAP\u3001MELD\u548cEmoryNLP\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u589e\u5f3a\u793a\u4f8b\u68c0\u7d22\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\uff0c\u8868\u660e\u68c0\u7d22\u5177\u6709\u8fde\u8d2f\u6027\u7684\u76ee\u6807\u793a\u4f8b\u5e76\u901a\u8fc7\u6539\u5199\u589e\u5f3a\u5176\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u793a\u4f8b\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u589e\u5f3a\u793a\u4f8b\u68c0\u7d22\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002", "paper_title_zh": "\u5982\u4f55\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u793a\u4f8b\u68c0\u7d22\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff1f", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5728\u4e3b\u89c2\u4efb\u52a1\u5982\u60c5\u611f\u8bc6\u522b\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4ecd\u5177\u6311\u6218\u6027\u3002\u53d7SLT 2024 GenSER\u6311\u6218\u8d5b\u542f\u53d1\uff0c\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u7684\u793a\u4f8b\u68c0\u7d22\u65b9\u6cd5\uff0c\u63d0\u5347\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08CER\uff09\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u57fa\u4e8e\u968f\u673a\u548c\u589e\u5f3a\u793a\u4f8b\u68c0\u7d22\u7684\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5bf9CER\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u5728IEMOCAP\u3001MELD\u548cEmoryNLP\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u793a\u4f8b\u68c0\u7d22\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\uff0c\u7a81\u663e\u4e86\u68c0\u7d22\u5177\u6709\u8fde\u8d2f\u6027\u7684\u76ee\u6807\u793a\u4f8b\u5e76\u901a\u8fc7\u6539\u5199\u589e\u5f3a\u5176\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.20222", "pdf": "https://arxiv.org/pdf/2506.20222", "abs": "https://arxiv.org/abs/2506.20222", "authors": ["Pujing Yang", "Guangyi Zhang", "Yunlong Cai", "Lei Yu", "Guanding Yu"], "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Event cameras asynchronously capture pixel-level intensity changes with\nextremely low latency. They are increasingly used in conjunction with RGB\ncameras for a wide range of vision-related applications. However, a major\nchallenge in these hybrid systems lies in the transmission of the large volume\nof triggered events and RGB images. To address this, we propose a transmission\nscheme that retains efficient reconstruction performance of both sources while\naccomplishing real-time deblurring in parallel. Conventional RGB cameras and\nevent cameras typically capture the same scene in different ways, often\nresulting in significant redundant information across their outputs. To address\nthis, we develop a joint event and image (E-I) transmission framework to\neliminate redundancy and thereby optimize channel bandwidth utilization. Our\napproach employs Bayesian modeling and the information bottleneck method to\ndisentangle the shared and domain-specific information within the E-I inputs.\nThis disentangled information bottleneck framework ensures both the compactness\nand informativeness of extracted shared and domain-specific information.\nMoreover, it adaptively allocates transmission bandwidth based on scene\ndynamics, i.e., more symbols are allocated to events for dynamic details or to\nimages for static information. Simulation results demonstrate that the proposed\nscheme not only achieves superior reconstruction quality compared to\nconventional systems but also delivers enhanced deblurring performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5e26\u5bbd\u5206\u914d\u65b9\u6848\uff0c\u7528\u4e8e\u4f18\u5316\u4e8b\u4ef6\u76f8\u673a\u4e0eRGB\u76f8\u673a\u6df7\u5408\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u4f20\u8f93\uff0c\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u4fe1\u606f\u63d0\u5347\u4f20\u8f93\u6548\u7387\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u5b9e\u65f6\u53bb\u6a21\u7cca\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4e0eRGB\u76f8\u673a\u5728\u6df7\u5408\u7cfb\u7edf\u4e2d\u4f1a\u4ea7\u751f\u5927\u91cf\u5197\u4f59\u6570\u636e\uff0c\u4f20\u8f93\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u5e26\u5bbd\u5206\u914d\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4e8b\u4ef6\u4e0e\u56fe\u50cf\uff08E-I\uff09\u4f20\u8f93\u6846\u67b6\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u5206\u79bb\u5171\u4eab\u4e0e\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u5e76\u6839\u636e\u573a\u666f\u52a8\u6001\u81ea\u9002\u5e94\u5206\u914d\u5e26\u5bbd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u53bb\u6a21\u7cca\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u5e26\u5bbd\u5206\u914d\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u7cfb\u7edf\u7684\u4f20\u8f93\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u6df7\u5408\u4e8b\u4ef6-RGB\u4f20\u8f93\u7684\u52a8\u6001\u5e26\u5bbd\u5206\u914d", "abstract_zh": "\u4e8b\u4ef6\u76f8\u673a\u4ee5\u6781\u4f4e\u5ef6\u8fdf\u5f02\u6b65\u6355\u6349\u50cf\u7d20\u7ea7\u5f3a\u5ea6\u53d8\u5316\uff0c\u5e38\u4e0eRGB\u76f8\u673a\u7ed3\u5408\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u5e94\u7528\u3002\u7136\u800c\uff0c\u6df7\u5408\u7cfb\u7edf\u4e2d\u5927\u91cf\u89e6\u53d1\u4e8b\u4ef6\u4e0eRGB\u56fe\u50cf\u7684\u4f20\u8f93\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f20\u8f93\u65b9\u6848\uff0c\u5728\u4fdd\u7559\u4e24\u79cd\u6e90\u6570\u636e\u9ad8\u6548\u91cd\u5efa\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u5e76\u884c\u5b9e\u65f6\u53bb\u6a21\u7cca\u3002\u4f20\u7edfRGB\u76f8\u673a\u4e0e\u4e8b\u4ef6\u76f8\u673a\u4ee5\u4e0d\u540c\u65b9\u5f0f\u6355\u6349\u540c\u4e00\u573a\u666f\uff0c\u5e38\u5bfc\u81f4\u8f93\u51fa\u4e2d\u5b58\u5728\u663e\u8457\u5197\u4f59\u4fe1\u606f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u8054\u5408\u4e8b\u4ef6\u4e0e\u56fe\u50cf\uff08E-I\uff09\u4f20\u8f93\u6846\u67b6\uff0c\u4ee5\u6d88\u9664\u5197\u4f59\u5e76\u4f18\u5316\u4fe1\u9053\u5e26\u5bbd\u5229\u7528\u7387\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\uff0c\u5206\u79bbE-I\u8f93\u5165\u4e2d\u7684\u5171\u4eab\u4e0e\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u3002\u8fd9\u79cd\u5206\u79bb\u7684\u4fe1\u606f\u74f6\u9888\u6846\u67b6\u786e\u4fdd\u4e86\u63d0\u53d6\u4fe1\u606f\u7684\u7d27\u51d1\u6027\u548c\u4fe1\u606f\u91cf\u3002\u6b64\u5916\uff0c\u5b83\u6839\u636e\u573a\u666f\u52a8\u6001\u81ea\u9002\u5e94\u5206\u914d\u4f20\u8f93\u5e26\u5bbd\uff0c\u5373\u52a8\u6001\u7ec6\u8282\u5206\u914d\u66f4\u591a\u7b26\u53f7\u7ed9\u4e8b\u4ef6\uff0c\u9759\u6001\u4fe1\u606f\u5206\u914d\u66f4\u591a\u7b26\u53f7\u7ed9\u56fe\u50cf\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u4e0d\u4ec5\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u8fd8\u63d0\u5347\u4e86\u53bb\u6a21\u7cca\u6027\u80fd\u3002"}}
{"id": "2506.20384", "pdf": "https://arxiv.org/pdf/2506.20384", "abs": "https://arxiv.org/abs/2506.20384", "authors": ["Dror Ivry", "Oran Nahum"], "title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios", "categories": ["cs.AI"], "comment": "6 pages, 2 figures", "summary": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Paladin-mini\uff0c\u4e00\u4e2a\u7d27\u51d1\u9ad8\u6548\u7684\u5f00\u6e90\u5206\u7c7b\u6a21\u578b\uff083.8B\u53c2\u6570\uff09\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e0a\u4e0b\u6587\u4e2d\u7684\u58f0\u660e\u662f\u5426\u6210\u7acb\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6grounding-benchmark\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5173\u952e\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u9a8c\u8bc1\u58f0\u660e\u662f\u5426\u6210\u7acb\u7684\u95ee\u9898\uff0c\u5373\u662f\u5426\u5b58\u5728\u652f\u6301\u58f0\u660e\u7684\u8bc1\u636e\u3002", "method": "\u63d0\u51fa\u4e86Paladin-mini\u6a21\u578b\uff083.8B\u53c2\u6570\uff09\u548cgrounding-benchmark\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u5173\u952e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "Paladin-mini\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7benchmark\u6d4b\u8bd5\u5c55\u793a\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "Paladin-mini\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u7d27\u51d1\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\uff0cgrounding-benchmark\u4e3a\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u6807\u51c6\u3002", "paper_title_zh": "Paladin-mini\uff1a\u4e00\u6b3e\u7d27\u51d1\u9ad8\u6548\u7684\u63a5\u5730\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u5353\u8d8a", "abstract_zh": "\u672c\u6587\u9488\u5bf9\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u9a8c\u8bc1\u58f0\u660e\u662f\u5426\u6210\u7acb\u7684\u95ee\u9898\u63d0\u51fa\u4e86\u4e24\u9879\u91cd\u8981\u8d21\u732e\u3002\u63a5\u5730\u610f\u5473\u7740\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\uff08\u6587\u6863\uff09\u548c\u58f0\u660e\u7684\u60c5\u51b5\u4e0b\uff0c\u6587\u6863\u4e2d\u81f3\u5c11\u5b58\u5728\u4e00\u6761\u652f\u6301\u58f0\u660e\u7684\u8bc1\u636e\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86Paladin-mini\uff0c\u4e00\u4e2a\u7d27\u51d1\uff083.8B\u53c2\u6570\uff09\u7684\u5f00\u6e90\u5206\u7c7b\u6a21\u578b\uff08\u7528\u4e8e\u6807\u8bb0\u6570\u636e\u4e3a\u63a5\u5730\u6216\u672a\u63a5\u5730\uff09\uff0c\u4e13\u4e3a\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\u800c\u8bbe\u8ba1\uff1b\u540c\u65f6\u63d0\u51fa\u4e86grounding-benchmark\uff0c\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5173\u952e\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86Paladin-mini\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6e05\u6670\u4e14\u53ef\u590d\u73b0\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.20203", "pdf": "https://arxiv.org/pdf/2506.20203", "abs": "https://arxiv.org/abs/2506.20203", "authors": ["Petra Baran\u010d\u00edkov\u00e1", "Ond\u0159ej Bojar"], "title": "Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we compare Czech-specific and multilingual sentence embedding\nmodels through intrinsic and extrinsic evaluation paradigms. For intrinsic\nevaluation, we employ Costra, a complex sentence transformation dataset, and\nseveral Semantic Textual Similarity (STS) benchmarks to assess the ability of\nthe embeddings to capture linguistic phenomena such as semantic similarity,\ntemporal aspects, and stylistic variations. In the extrinsic evaluation, we\nfine-tune each embedding model using COMET-based metrics for machine\ntranslation evaluation.\n  Our experiments reveal an interesting disconnect: models that excel in\nintrinsic semantic similarity tests do not consistently yield superior\nperformance on downstream translation evaluation tasks. Conversely, models with\nseemingly over-smoothed embedding spaces can, through fine-tuning, achieve\nexcellent results. These findings highlight the complex relationship between\nsemantic property probes and downstream task, emphasizing the need for more\nresearch into 'operationalizable semantics' in sentence embeddings, or more\nin-depth downstream tasks datasets (here translation evaluation)", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u6377\u514b\u8bed\u7279\u5b9a\u548c\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u5185\u5728\u548c\u5916\u5728\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bed\u4e49\u76f8\u4f3c\u6027\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u672a\u5fc5\u8868\u73b0\u6700\u4f73\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u6377\u514b\u8bed\u548c\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63ed\u793a\u8bed\u4e49\u5c5e\u6027\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5185\u5728\u8bc4\u4f30\uff08\u4f7f\u7528Costra\u6570\u636e\u96c6\u548cSTS\u57fa\u51c6\u6d4b\u8bd5\uff09\u548c\u5916\u5728\u8bc4\u4f30\uff08\u57fa\u4e8eCOMET\u6307\u6807\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5185\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u672a\u5fc5\u8868\u73b0\u6700\u4f73\uff0c\u800c\u67d0\u4e9b\u5d4c\u5165\u7a7a\u95f4\u770b\u4f3c\u8fc7\u5ea6\u5e73\u6ed1\u7684\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u5374\u80fd\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bed\u4e49\u5c5e\u6027\u63a2\u6d4b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u53e5\u5b50\u5d4c\u5165\u4e2d\u7684\u2018\u53ef\u64cd\u4f5c\u8bed\u4e49\u2019\u6216\u66f4\u6df1\u5165\u7684\u4e0b\u6e38\u4efb\u52a1\u6570\u636e\u96c6\u3002", "paper_title_zh": "\u6377\u514b\u8bed\u53e5\u5b50\u5d4c\u5165\u7684\u5185\u5728\u4e0e\u5916\u5728\u8bc4\u4f30\uff1a\u8bed\u4e49\u76f8\u5173\u6027\u5bf9\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u65e0\u76ca", "abstract_zh": "\u672c\u6587\u901a\u8fc7\u5185\u5728\u548c\u5916\u5728\u8bc4\u4f30\u8303\u5f0f\u6bd4\u8f83\u4e86\u6377\u514b\u8bed\u7279\u5b9a\u548c\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u3002\u5728\u5185\u5728\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528Costra\u6570\u636e\u96c6\u548c\u591a\u4e2a\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\uff08STS\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5d4c\u5165\u6a21\u578b\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u65f6\u95f4\u65b9\u9762\u548c\u98ce\u683c\u53d8\u5316\u7b49\u8bed\u8a00\u73b0\u8c61\u7684\u80fd\u529b\u3002\u5728\u5916\u5728\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8eCOMET\u6307\u6807\u5bf9\u6bcf\u4e2a\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u73b0\u8c61\uff1a\u5728\u5185\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578b\uff0c\u5728\u4e0b\u6e38\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\u4e2d\u672a\u5fc5\u8868\u73b0\u6700\u4f73\uff1b\u53cd\u4e4b\uff0c\u67d0\u4e9b\u5d4c\u5165\u7a7a\u95f4\u770b\u4f3c\u8fc7\u5ea6\u5e73\u6ed1\u7684\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u5374\u80fd\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u8bed\u4e49\u5c5e\u6027\u63a2\u6d4b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u53e5\u5b50\u5d4c\u5165\u4e2d\u7684\u2018\u53ef\u64cd\u4f5c\u8bed\u4e49\u2019\u6216\u66f4\u6df1\u5165\u7684\u4e0b\u6e38\u4efb\u52a1\u6570\u636e\u96c6\uff08\u5982\u7ffb\u8bd1\u8bc4\u4f30\uff09\u3002"}}
{"id": "2506.20254", "pdf": "https://arxiv.org/pdf/2506.20254", "abs": "https://arxiv.org/abs/2506.20254", "authors": ["Kun Yuan", "Tingxuan Chen", "Shi Li", "Joel L. Lavanchy", "Christian Heiliger", "Ege \u00d6zsoy", "Yiming Huang", "Long Bai", "Nassir Navab", "Vinkle Srivastav", "Hongliang Ren", "Nicolas Padoy"], "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement", "categories": ["cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "The complexity and diversity of surgical workflows, driven by heterogeneous\noperating room settings, institutional protocols, and anatomical variability,\npresent a significant challenge in developing generalizable models for\ncross-institutional and cross-procedural surgical understanding. While recent\nsurgical foundation models pretrained on large-scale vision-language data offer\npromising transferability, their zero-shot performance remains constrained by\ndomain shifts, limiting their utility in unseen surgical environments. To\naddress this, we introduce Surgical Phase Anywhere (SPA), a lightweight\nframework for versatile surgical workflow understanding that adapts foundation\nmodels to institutional settings with minimal annotation. SPA leverages\nfew-shot spatial adaptation to align multi-modal embeddings with\ninstitution-specific surgical scenes and phases. It also ensures temporal\nconsistency through diffusion modeling, which encodes task-graph priors derived\nfrom institutional procedure protocols. Finally, SPA employs dynamic test-time\nadaptation, exploiting the mutual agreement between multi-modal phase\nprediction streams to adapt the model to a given test video in a\nself-supervised manner, enhancing the reliability under test-time distribution\nshifts. SPA is a lightweight adaptation framework, allowing hospitals to\nrapidly customize phase recognition models by defining phases in natural\nlanguage text, annotating a few images with the phase labels, and providing a\ntask graph defining phase transitions. The experimental results show that the\nSPA framework achieves state-of-the-art performance in few-shot surgical phase\nrecognition across multiple institutions and procedures, even outperforming\nfull-shot models with 32-shot labeled data. Code is available at\nhttps://github.com/CAMMA-public/SPA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6SPA\uff0c\u7528\u4e8e\u9002\u5e94\u4e0d\u540c\u673a\u6784\u7684\u624b\u672f\u5de5\u4f5c\u6d41\u7406\u89e3\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u7a7a\u95f4\u9002\u5e94\u548c\u4efb\u52a1\u56fe\u5f15\u5bfc\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u624b\u672f\u5de5\u4f5c\u6d41\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8de8\u673a\u6784\u548c\u8de8\u624b\u672f\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u5e94\u6846\u67b6\uff0c\u89e3\u51b3\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u624b\u672f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "SPA\u6846\u67b6\u7ed3\u5408\u5c11\u6837\u672c\u7a7a\u95f4\u9002\u5e94\u548c\u591a\u6a21\u6001\u5d4c\u5165\u5bf9\u9f50\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\u5728\u81ea\u76d1\u7763\u65b9\u5f0f\u4e0b\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPA\u5728\u5c11\u6837\u672c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u4f18\u4e8e\u4f7f\u752832\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u5168\u6837\u672c\u6a21\u578b\u3002", "conclusion": "SPA\u4e3a\u624b\u672f\u5de5\u4f5c\u6d41\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u673a\u6784\u548c\u624b\u672f\u573a\u666f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u968f\u5904\u8bc6\u522b\u624b\u672f\u9636\u6bb5\uff1a\u5c11\u6837\u672c\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e0e\u4efb\u52a1\u56fe\u5f15\u5bfc\u4f18\u5316", "abstract_zh": "\u624b\u672f\u5de5\u4f5c\u6d41\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u6e90\u4e8e\u5f02\u6784\u624b\u672f\u5ba4\u8bbe\u7f6e\u3001\u673a\u6784\u534f\u8bae\u548c\u89e3\u5256\u5b66\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u8de8\u673a\u6784\u548c\u8de8\u624b\u672f\u7684\u901a\u7528\u6a21\u578b\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002\u5c3d\u7ba1\u8fd1\u671f\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u9884\u8bad\u7ec3\u7684\u624b\u672f\u57fa\u7840\u6a21\u578b\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u4f46\u5176\u96f6\u6837\u672c\u6027\u80fd\u4ecd\u53d7\u9650\u4e8e\u9886\u57df\u504f\u79fb\uff0c\u9650\u5236\u4e86\u5176\u5728\u672a\u89c1\u624b\u672f\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u968f\u5904\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u201d\uff08SPA\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u7528\u624b\u672f\u5de5\u4f5c\u6d41\u7406\u89e3\uff0c\u80fd\u591f\u4ee5\u6700\u5c11\u6807\u6ce8\u5c06\u57fa\u7840\u6a21\u578b\u9002\u5e94\u5230\u673a\u6784\u7279\u5b9a\u573a\u666f\u3002SPA\u901a\u8fc7\u5c11\u6837\u672c\u7a7a\u95f4\u9002\u5e94\u5c06\u591a\u6a21\u6001\u5d4c\u5165\u4e0e\u673a\u6784\u7279\u5b9a\u7684\u624b\u672f\u573a\u666f\u548c\u9636\u6bb5\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7f16\u7801\u6765\u81ea\u673a\u6784\u534f\u8bae\u7684\u4efb\u52a1\u56fe\u5148\u9a8c\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0cSPA\u5229\u7528\u591a\u6a21\u6001\u9636\u6bb5\u9884\u6d4b\u6d41\u4e4b\u95f4\u7684\u76f8\u4e92\u4e00\u81f4\u6027\uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u52a8\u6001\u9002\u5e94\u6d4b\u8bd5\u89c6\u9891\uff0c\u63d0\u5347\u6d4b\u8bd5\u65f6\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u3002SPA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u5e94\u6846\u67b6\uff0c\u533b\u9662\u53ef\u901a\u8fc7\u5b9a\u4e49\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7684\u9636\u6bb5\u3001\u6807\u6ce8\u5c11\u91cf\u56fe\u50cf\u5e76\u63d0\u4f9b\u5b9a\u4e49\u9636\u6bb5\u8f6c\u6362\u7684\u4efb\u52a1\u56fe\uff0c\u5feb\u901f\u5b9a\u5236\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSPA\u6846\u67b6\u5728\u591a\u4e2a\u673a\u6784\u548c\u624b\u672f\u4e2d\u7684\u5c11\u6837\u672c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u4f7f\u752832\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u5168\u6837\u672c\u6a21\u578b\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/CAMMA-public/SPA\u3002"}}
{"id": "2506.20401", "pdf": "https://arxiv.org/pdf/2506.20401", "abs": "https://arxiv.org/abs/2506.20401", "authors": ["Jinchun Du", "Bojie Shen", "Muhammad Aamir Cheema", "Adel N. Toosi"], "title": "Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation", "categories": ["cs.AI"], "comment": null, "summary": "With the rising popularity of electric vehicles (EVs), modern service\nsystems, such as ride-hailing delivery services, are increasingly integrating\nEVs into their operations. Unlike conventional vehicles, EVs often have a\nshorter driving range, necessitating careful consideration of charging when\nfulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -\nallowing EVs to also discharge energy back to the grid - new opportunities and\ncomplexities emerge. We introduce the Electric Vehicle Orienteering Problem\nwith V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select\ncustomer requests or orders while managing when and where to charge or\ndischarge. This involves navigating dynamic electricity prices, charging\nstation selection, and route constraints. We formulate the problem as a Mixed\nInteger Programming (MIP) model and propose two near-optimal metaheuristic\nalgorithms: one evolutionary (EA) and the other based on large neighborhood\nsearch (LNS). Experiments on real-world data show our methods can double driver\nprofits compared to baselines, while maintaining near-optimal performance on\nsmall instances and excellent scalability on larger ones. Our work highlights a\npromising path toward smarter, more profitable EV-based mobility systems that\nactively support the energy grid.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u7684\u667a\u80fd\u51fa\u884c\u548c\u914d\u9001\u670d\u52a1\u4f18\u5316\u95ee\u9898\uff08EVOP-V2G\uff09\uff0c\u901a\u8fc7\u53cc\u5411\u5145\u7535\u6280\u672f\uff08V2G\uff09\u6700\u5927\u5316\u53f8\u673a\u5229\u6da6\u3002\u7814\u7a76\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\u548c\u4e24\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08\u8fdb\u5316\u548c\u5927\u90bb\u57df\u641c\u7d22\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u65b9\u6cd5\u53ef\u5c06\u53f8\u673a\u5229\u6da6\u7ffb\u500d\uff0c\u5e76\u5177\u6709\u4f18\u5f02\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u7684\u666e\u53ca\uff0c\u73b0\u4ee3\u670d\u52a1\u7cfb\u7edf\uff08\u5982\u7f51\u7ea6\u8f66\u548c\u914d\u9001\u670d\u52a1\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528EV\u3002\u7136\u800c\uff0cEV\u7684\u7eed\u822a\u8f83\u77ed\uff0c\u5145\u7535\u7ba1\u7406\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u53cc\u5411\u5145\u7535\u6280\u672f\uff08V2G\uff09\u7684\u51fa\u73b0\u4e3aEV\u63d0\u4f9b\u4e86\u653e\u7535\u56de\u7535\u7f51\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u590d\u6742\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316EV\u7684\u5145\u653e\u7535\u51b3\u7b56\uff0c\u6700\u5927\u5316\u53f8\u673a\u5229\u6da6\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86EVOP-V2G\u95ee\u9898\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u8fd1\u4f18\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff1a\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\u548c\u5927\u90bb\u57df\u641c\u7d22\u7b97\u6cd5\uff08LNS\uff09\u3002\u8fd9\u4e9b\u7b97\u6cd5\u8003\u8651\u4e86\u52a8\u6001\u7535\u4ef7\u3001\u5145\u7535\u7ad9\u9009\u62e9\u548c\u8def\u5f84\u7ea6\u675f\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5c06\u53f8\u673a\u5229\u6da6\u63d0\u5347\u4e86\u4e00\u500d\uff0c\u540c\u65f6\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\uff0c\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u5177\u6709\u4f18\u5f02\u7684\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u57fa\u4e8eEV\u7684\u667a\u80fd\u51fa\u884c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u76c8\u5229\u7684\u4f18\u5316\u8def\u5f84\uff0c\u540c\u65f6\u652f\u6301\u7535\u7f51\u80fd\u6e90\u7ba1\u7406\u3002", "paper_title_zh": "\u57fa\u4e8e\u7535\u52a8\u6c7d\u8f66\u7684\u667a\u80fd\u51fa\u884c\u4e0e\u914d\u9001\u670d\u52a1\uff1a\u5229\u7528\u53cc\u5411\u5145\u7535\u6280\u672f\u4f18\u5316\u5229\u6da6", "abstract_zh": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u7684\u666e\u53ca\uff0c\u73b0\u4ee3\u670d\u52a1\u7cfb\u7edf\uff08\u5982\u7f51\u7ea6\u8f66\u548c\u914d\u9001\u670d\u52a1\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528EV\u3002\u4e0e\u4f20\u7edf\u8f66\u8f86\u4e0d\u540c\uff0cEV\u7684\u7eed\u822a\u8f83\u77ed\uff0c\u56e0\u6b64\u5728\u6ee1\u8db3\u9700\u6c42\u65f6\u9700\u8c28\u614e\u8003\u8651\u5145\u7535\u95ee\u9898\u3002\u8fd1\u5e74\u6765\uff0c\u8f66\u8f86\u5230\u7535\u7f51\uff08V2G\uff09\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97EV\u80fd\u591f\u5c06\u80fd\u91cf\u56de\u9988\u7535\u7f51\uff0c\u4ece\u800c\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u548c\u590d\u6742\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eV2G\u7684\u7535\u52a8\u6c7d\u8f66\u5b9a\u5411\u95ee\u9898\uff08EVOP-V2G\uff09\uff1a\u4e00\u4e2a\u5229\u6da6\u6700\u5927\u5316\u95ee\u9898\uff0cEV\u53f8\u673a\u9700\u5728\u7ba1\u7406\u5145\u653e\u7535\u65f6\u95f4\u548c\u5730\u70b9\u7684\u540c\u65f6\u9009\u62e9\u5ba2\u6237\u8bf7\u6c42\u6216\u8ba2\u5355\u3002\u8be5\u95ee\u9898\u6d89\u53ca\u52a8\u6001\u7535\u4ef7\u3001\u5145\u7535\u7ad9\u9009\u62e9\u548c\u8def\u5f84\u7ea6\u675f\u3002\u6211\u4eec\u5c06\u5176\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u8fd1\u4f18\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff1a\u4e00\u79cd\u662f\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\uff0c\u53e6\u4e00\u79cd\u57fa\u4e8e\u5927\u90bb\u57df\u641c\u7d22\uff08LNS\uff09\u3002\u5728\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u5c06\u53f8\u673a\u5229\u6da6\u63d0\u5347\u4e00\u500d\uff0c\u540c\u65f6\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\uff0c\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u5177\u6709\u4f18\u5f02\u7684\u6269\u5c55\u6027\u3002\u672c\u7814\u7a76\u4e3a\u66f4\u667a\u80fd\u3001\u66f4\u76c8\u5229\u7684\u57fa\u4e8eEV\u7684\u51fa\u884c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u540c\u65f6\u79ef\u6781\u652f\u6301\u7535\u7f51\u80fd\u6e90\u7ba1\u7406\u3002"}}
{"id": "2506.20209", "pdf": "https://arxiv.org/pdf/2506.20209", "abs": "https://arxiv.org/abs/2506.20209", "authors": ["Benedetta Muscato", "Lucia Passaro", "Gizem Gezici", "Fosca Giannotti"], "title": "Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of Natural Language Processing (NLP), common approaches for\nhandling human disagreement consist of aggregating annotators' viewpoints to\nestablish a single ground truth. However, prior studies show that disregarding\nindividual opinions can lead can lead to the side effect of underrepresenting\nminority perspectives, especially in subjective tasks, where annotators may\nsystematically disagree because of their preferences. Recognizing that labels\nreflect the diverse backgrounds, life experiences, and values of individuals,\nthis study proposes a new multi-perspective approach using soft labels to\nencourage the development of the next generation of perspective aware models,\nmore inclusive and pluralistic. We conduct an extensive analysis across diverse\nsubjective text classification tasks, including hate speech, irony, abusive\nlanguage, and stance detection, to highlight the importance of capturing human\ndisagreements, often overlooked by traditional aggregation methods. Results\nshow that the multi-perspective approach not only better approximates human\nlabel distributions, as measured by Jensen-Shannon Divergence (JSD), but also\nachieves superior classification performance (higher F1 scores), outperforming\ntraditional approaches. However, our approach exhibits lower confidence in\ntasks like irony and stance detection, likely due to the inherent subjectivity\npresent in the texts. Lastly, leveraging Explainable AI (XAI), we explore model\nuncertainty and uncover meaningful insights into model predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u6807\u7b7e\u5904\u7406\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6807\u6ce8\u5206\u6b67\uff0c\u4ee5\u66f4\u5305\u5bb9\u7684\u65b9\u5f0f\u6355\u6349\u5c11\u6570\u89c2\u70b9\uff0c\u5e76\u5728\u591a\u4e2a\u4e3b\u89c2\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edfNLP\u65b9\u6cd5\u901a\u8fc7\u805a\u5408\u6807\u6ce8\u8005\u89c2\u70b9\u5efa\u7acb\u5355\u4e00\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u5ffd\u89c6\u4e86\u5c11\u6570\u89c2\u70b9\uff0c\u5c24\u5176\u662f\u5728\u4e3b\u89c2\u4efb\u52a1\u4e2d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u89c6\u89d2\u65b9\u6cd5\u66f4\u597d\u5730\u53cd\u6620\u4e2a\u4f53\u80cc\u666f\u548c\u4ef7\u503c\u89c2\u7684\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2\u8f6f\u6807\u7b7e\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u805a\u5408\u65b9\u5f0f\u7684\u504f\u89c1\uff0c\u5e76\u5728\u4ec7\u6068\u8a00\u8bba\u3001\u8bbd\u523a\u3001\u8fb1\u9a82\u8bed\u8a00\u548c\u7acb\u573a\u68c0\u6d4b\u7b49\u4e3b\u89c2\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u591a\u89c6\u89d2\u65b9\u6cd5\u5728Jensen-Shannon Divergence\uff08JSD\uff09\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5728\u8bbd\u523a\u548c\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u7f6e\u4fe1\u5ea6\u3002\u901a\u8fc7\u53ef\u89e3\u91caAI\uff08XAI\uff09\u63ed\u793a\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u591a\u89c6\u89d2\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u4eba\u7c7b\u6807\u6ce8\u5206\u6b67\uff0c\u63d0\u5347\u6a21\u578b\u5305\u5bb9\u6027\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u9ad8\u4e3b\u89c2\u6027\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u95ee\u9898\u3002", "paper_title_zh": "\u591a\u89c6\u89d2\u6e38\u620f\uff1a\u4e00\u79cd\u66f4\u5305\u5bb9NLP\u7cfb\u7edf\u7684\u591a\u89c6\u89d2\u65b9\u6cd5", "abstract_zh": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\uff0c\u5904\u7406\u4eba\u7c7b\u6807\u6ce8\u5206\u6b67\u7684\u5e38\u89c1\u65b9\u6cd5\u662f\u901a\u8fc7\u805a\u5408\u6807\u6ce8\u8005\u89c2\u70b9\u5efa\u7acb\u5355\u4e00\u771f\u5b9e\u6807\u7b7e\u3002\u7136\u800c\uff0c\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5ffd\u89c6\u4e2a\u4f53\u89c2\u70b9\u53ef\u80fd\u5bfc\u81f4\u5c11\u6570\u89c6\u89d2\u88ab\u4f4e\u4f30\uff0c\u5c24\u5176\u662f\u5728\u4e3b\u89c2\u4efb\u52a1\u4e2d\uff0c\u6807\u6ce8\u8005\u53ef\u80fd\u56e0\u504f\u597d\u800c\u7cfb\u7edf\u6027\u5206\u6b67\u3002\u8ba4\u8bc6\u5230\u6807\u7b7e\u53cd\u6620\u4e86\u4e2a\u4eba\u80cc\u666f\u3001\u751f\u6d3b\u7ecf\u9a8c\u548c\u4ef7\u503c\u89c2\u7684\u591a\u6837\u6027\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u89c6\u89d2\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f6f\u6807\u7b7e\u4fc3\u8fdb\u4e0b\u4e00\u4ee3\u66f4\u5177\u5305\u5bb9\u6027\u548c\u591a\u5143\u5316\u7684\u89c6\u89d2\u611f\u77e5\u6a21\u578b\u7684\u53d1\u5c55\u3002\u6211\u4eec\u5728\u591a\u4e2a\u4e3b\u89c2\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff08\u5305\u62ec\u4ec7\u6068\u8a00\u8bba\u3001\u8bbd\u523a\u3001\u8fb1\u9a82\u8bed\u8a00\u548c\u7acb\u573a\u68c0\u6d4b\uff09\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5206\u6790\uff0c\u4ee5\u5f3a\u8c03\u6355\u6349\u4eba\u7c7b\u5206\u6b67\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u4e9b\u5206\u6b67\u5e38\u88ab\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u5ffd\u89c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591a\u89c6\u89d2\u65b9\u6cd5\u4e0d\u4ec5\u901a\u8fc7Jensen-Shannon Divergence\uff08JSD\uff09\u66f4\u597d\u5730\u8fd1\u4f3c\u4eba\u7c7b\u6807\u7b7e\u5206\u5e03\uff0c\u8fd8\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5206\u7c7b\u6027\u80fd\uff08\u66f4\u9ad8\u7684F1\u5206\u6570\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bbd\u523a\u548c\u7acb\u573a\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u7f6e\u4fe1\u5ea6\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u6587\u672c\u56fa\u6709\u7684\u4e3b\u89c2\u6027\u3002\u6700\u540e\uff0c\u5229\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u6709\u610f\u4e49\u89c1\u89e3\u3002"}}
{"id": "2506.20255", "pdf": "https://arxiv.org/pdf/2506.20255", "abs": "https://arxiv.org/abs/2506.20255", "authors": ["Ayush Lodh", "Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal"], "title": "A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features", "categories": ["cs.CV", "cs.LG"], "comment": "15 pages, 7 figures", "summary": "We posit that handwriting recognition benefits from complementary cues\ncarried by the rasterized complex glyph and the pen's trajectory, yet most\nsystems exploit only one modality. We introduce an end-to-end network that\nperforms early fusion of offline images and online stroke data within a shared\nlatent space. A patch encoder converts the grayscale crop into fixed-length\nvisual tokens, while a lightweight transformer embeds the $(x, y, \\text{pen})$\nsequence. Learnable latent queries attend jointly to both token streams,\nyielding context-enhanced stroke embeddings that are pooled and decoded under a\ncross-entropy loss objective. Because integration occurs before any high-level\nclassification, temporal cues reinforce each other during representation\nlearning, producing stronger writer independence. Comprehensive experiments on\nIAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art\naccuracy, exceeding previous bests by up to 1\\%. Our study also shows\nadaptation of this pipeline with gesturification on the ISI-Air dataset. Our\ncode can be found here.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u624b\u5199\u8bc6\u522b\u7cfb\u7edf\uff0c\u8054\u5408\u5229\u7528\u5728\u7ebf\u548c\u79bb\u7ebf\u7279\u5f81\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u624b\u5199\u8bc6\u522b\u7cfb\u7edf\u901a\u5e38\u4ec5\u5229\u7528\u5355\u4e00\u6a21\u6001\uff08\u5982\u79bb\u7ebf\u56fe\u50cf\u6216\u5728\u7ebf\u7b14\u753b\u6570\u636e\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u4e24\u79cd\u6a21\u6001\u7684\u4e92\u8865\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8054\u5408\u5229\u7528\u8fd9\u4e24\u79cd\u6a21\u6001\uff0c\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u65e9\u671f\u878d\u5408\u79bb\u7ebf\u56fe\u50cf\u548c\u5728\u7ebf\u7b14\u753b\u6570\u636e\u3002\u4f7f\u7528\u8865\u4e01\u7f16\u7801\u5668\u5c06\u7070\u5ea6\u56fe\u50cf\u8f6c\u6362\u4e3a\u89c6\u89c9\u6807\u8bb0\uff0c\u8f7b\u91cf\u7ea7Transformer\u5d4c\u5165\u7b14\u753b\u5e8f\u5217\u3002\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u67e5\u8be2\u8054\u5408\u5173\u6ce8\u4e24\u79cd\u6807\u8bb0\u6d41\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u7b14\u753b\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u71b5\u635f\u5931\u89e3\u7801\u3002", "result": "\u5728IAMOn-DB\u548cVNOn-DB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u4e861%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728ISI-Air\u6570\u636e\u96c6\u4e0a\u4e5f\u5c55\u793a\u4e86\u9002\u5e94\u6027\u3002", "conclusion": "\u901a\u8fc7\u65e9\u671f\u878d\u5408\u5728\u7ebf\u548c\u79bb\u7ebf\u7279\u5f81\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u624b\u5199\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9002\u5e94\u6027\u3002", "paper_title_zh": "\u57fa\u4e8eTransformer\u7684\u8054\u5408\u5728\u7ebf\u4e0e\u79bb\u7ebf\u7279\u5f81\u7684\u624b\u5199\u8bc6\u522b\u7cfb\u7edf", "abstract_zh": "\u6211\u4eec\u8ba4\u4e3a\u624b\u5199\u8bc6\u522b\u53ef\u4ee5\u4ece\u6805\u683c\u5316\u7684\u590d\u6742\u5b57\u5f62\u548c\u7b14\u7684\u8f68\u8ff9\u4e2d\u643a\u5e26\u7684\u4e92\u8865\u7ebf\u7d22\u4e2d\u53d7\u76ca\uff0c\u4f46\u5927\u591a\u6570\u7cfb\u7edf\u4ec5\u5229\u7528\u4e00\u79cd\u6a21\u6001\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u65e9\u671f\u878d\u5408\u79bb\u7ebf\u56fe\u50cf\u548c\u5728\u7ebf\u7b14\u753b\u6570\u636e\u3002\u8865\u4e01\u7f16\u7801\u5668\u5c06\u7070\u5ea6\u56fe\u50cf\u8f6c\u6362\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u800c\u8f7b\u91cf\u7ea7Transformer\u5d4c\u5165$(x, y, \text{\u7b14})$\u5e8f\u5217\u3002\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u67e5\u8be2\u8054\u5408\u5173\u6ce8\u4e24\u79cd\u6807\u8bb0\u6d41\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u7b14\u753b\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u71b5\u635f\u5931\u89e3\u7801\u3002\u7531\u4e8e\u878d\u5408\u53d1\u751f\u5728\u4efb\u4f55\u9ad8\u7ea7\u5206\u7c7b\u4e4b\u524d\uff0c\u65f6\u95f4\u7ebf\u7d22\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u76f8\u4e92\u589e\u5f3a\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e66\u5199\u72ec\u7acb\u6027\u3002\u5728IAMOn-DB\u548cVNOn-DB\u4e0a\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u4e861%\u3002\u6211\u4eec\u7684\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728ISI-Air\u6570\u636e\u96c6\u4e0a\u7684\u624b\u52bf\u5316\u9002\u5e94\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728\u6b64\u5904\u627e\u5230\u3002"}}
{"id": "2506.20404", "pdf": "https://arxiv.org/pdf/2506.20404", "abs": "https://arxiv.org/abs/2506.20404", "authors": ["Riccardo Lo Bianco", "Willem van Jaarsveld", "Remco Dijkman"], "title": "GymPN: A Library for Decision-Making in Process Management Systems", "categories": ["cs.AI"], "comment": null, "summary": "Process management systems support key decisions about the way work is\nallocated in organizations. This includes decisions on which task to perform\nnext, when to execute the task, and who to assign the task to. Suitable\nsoftware tools are required to support these decisions in a way that is optimal\nfor the organization. This paper presents a software library, called GymPN,\nthat supports optimal decision-making in business processes using Deep\nReinforcement Learning. GymPN builds on previous work that supports task\nassignment in business processes, introducing two key novelties: support for\npartial process observability and the ability to model multiple decisions in a\nbusiness process. These novel elements address fundamental limitations of\nprevious work and thus enable the representation of more realistic process\ndecisions. We evaluate the library on eight typical business process\ndecision-making problem patterns, showing that GymPN allows for easy modeling\nof the desired problems, as well as learning optimal decision policies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aGymPN\u7684\u8f6f\u4ef6\u5e93\uff0c\u7528\u4e8e\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u7684\u51b3\u7b56\uff0c\u652f\u6301\u90e8\u5206\u6d41\u7a0b\u53ef\u89c1\u6027\u548c\u591a\u51b3\u7b56\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u7cfb\u7edf\u9700\u8981\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3001\u6267\u884c\u65f6\u95f4\u548c\u4eba\u5458\u6307\u6d3e\u7b49\u51b3\u7b56\uff0c\u73b0\u6709\u5de5\u5177\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u73b0\u5b9e\u9700\u6c42\u3002GymPN\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u66f4\u4f18\u7684\u51b3\u7b56\u652f\u6301\u3002", "method": "GymPN\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u652f\u6301\u90e8\u5206\u6d41\u7a0b\u53ef\u89c1\u6027\u548c\u591a\u51b3\u7b56\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u7814\u7a76\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u516b\u79cd\u5178\u578b\u4e1a\u52a1\u6d41\u7a0b\u51b3\u7b56\u95ee\u9898\u6a21\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "GymPN\u80fd\u591f\u8f7b\u677e\u5efa\u6a21\u76ee\u6807\u95ee\u9898\uff0c\u5e76\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e1a\u52a1\u6d41\u7a0b\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "GymPN\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u53ef\u89c1\u6027\u548c\u591a\u51b3\u7b56\u5efa\u6a21\u7684\u6311\u6218\u3002", "paper_title_zh": "GymPN\uff1a\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u7cfb\u7edf\u4e2d\u51b3\u7b56\u652f\u6301\u7684\u8f6f\u4ef6\u5e93", "abstract_zh": "\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u7cfb\u7edf\u652f\u6301\u7ec4\u7ec7\u4e2d\u5de5\u4f5c\u5206\u914d\u7684\u5173\u952e\u51b3\u7b56\uff0c\u5305\u62ec\u4e0b\u4e00\u6b65\u6267\u884c\u7684\u4efb\u52a1\u3001\u6267\u884c\u65f6\u95f4\u4ee5\u53ca\u4efb\u52a1\u6307\u6d3e\u5bf9\u8c61\u3002\u9700\u8981\u5408\u9002\u7684\u8f6f\u4ef6\u5de5\u5177\u4ee5\u6700\u4f18\u65b9\u5f0f\u652f\u6301\u8fd9\u4e9b\u51b3\u7b56\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aGymPN\u7684\u8f6f\u4ef6\u5e93\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u7684\u51b3\u7b56\u3002GymPN\u5728\u4ee5\u5f80\u652f\u6301\u4efb\u52a1\u5206\u914d\u7684\u7814\u7a76\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u652f\u6301\u90e8\u5206\u6d41\u7a0b\u53ef\u89c1\u6027\u4ee5\u53ca\u591a\u51b3\u7b56\u5efa\u6a21\u80fd\u529b\u3002\u8fd9\u4e9b\u521b\u65b0\u89e3\u51b3\u4e86\u4ee5\u5f80\u7814\u7a76\u7684\u6839\u672c\u9650\u5236\uff0c\u4ece\u800c\u80fd\u591f\u8868\u793a\u66f4\u73b0\u5b9e\u7684\u6d41\u7a0b\u51b3\u7b56\u3002\u6211\u4eec\u5728\u516b\u79cd\u5178\u578b\u4e1a\u52a1\u6d41\u7a0b\u51b3\u7b56\u95ee\u9898\u6a21\u5f0f\u4e0a\u8bc4\u4f30\u4e86\u8be5\u5e93\uff0c\u7ed3\u679c\u8868\u660eGymPN\u80fd\u591f\u8f7b\u677e\u5efa\u6a21\u76ee\u6807\u95ee\u9898\u5e76\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\u3002"}}
{"id": "2506.20241", "pdf": "https://arxiv.org/pdf/2506.20241", "abs": "https://arxiv.org/abs/2506.20241", "authors": ["Yubo Dong", "Hehe Fan"], "title": "Enhancing Large Language Models through Structured Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Under review", "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u6ce8\u63a8\u7406\u6b65\u9aa4\u548c\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u4f9d\u8d56\u9690\u5f0f\u7edf\u8ba1\u5173\u7cfb\u800c\u7f3a\u4e4f\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3002\u672c\u6587\u53d7\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u7b26\u53f7AI\u542f\u53d1\uff0c\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u9996\u5148\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u6807\u6ce8\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u4f7f\u7528GRPO\u4f18\u5316\u7b97\u6cd5\uff08\u5305\u62ecMAX-Flow\u548cLCS\uff09\u589e\u5f3a\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eDeepSeek-R1-Distill-Qwen-1.5B\u6a21\u578b\u7684\u5fae\u8c03\u5b9e\u73b0\u4e86\u7b80\u6d01\u63a8\u7406\u3001\u9c81\u68d2\u6027\u80fd\u548c\u4f18\u5316\u6280\u672f\u517c\u5bb9\u6027\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u81ea\u52a8\u5316\u51b3\u7b56\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6267\u884c\u6d89\u53ca\u903b\u8f91\u63a8\u7406\u548c\u7cfb\u7edf\u89c4\u5212\u7684\u590d\u6742\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5176\u4f9d\u8d56\u9690\u5f0f\u7edf\u8ba1\u5173\u7cfb\u800c\u7f3a\u4e4f\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3002\u53d7\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u7b26\u53f7AI\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u663e\u5f0f\u7ed3\u6784\u5316\u63a8\u7406\u589e\u5f3aLLMs\u7684\u65b0\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u901a\u8fc7\u663e\u5f0f\u6807\u6ce8\u63a8\u7406\u6b65\u9aa4\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u968f\u540e\u5229\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8bad\u7ec3LLMs\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u589e\u5f3aLLMs\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u521b\u65b0\u7b97\u6cd5\u2014\u2014MAX-Flow\u548c\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff08LCS\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u679c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u901a\u8fc7\u5bf9DeepSeek-R1-Distill-Qwen-1.5B\u6a21\u578b\u7684\u5fae\u8c03\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7b80\u6d01\u63a8\u7406\u3001\u591a\u573a\u666f\u9c81\u68d2\u6027\u80fd\u4ee5\u53ca\u4f18\u5316\u6280\u672f\u7684\u517c\u5bb9\u6027\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u5728LLMs\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.20263", "pdf": "https://arxiv.org/pdf/2506.20263", "abs": "https://arxiv.org/abs/2506.20263", "authors": ["Ning Luo", "Meiyin Hu", "Huan Wan", "Yanyan Yang", "Zhuohang Jiang", "Xin Wei"], "title": "Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot fine-grained image classification (FS-FGIC) presents a significant\nchallenge, requiring models to distinguish visually similar subclasses with\nlimited labeled examples. Existing methods have critical limitations:\nmetric-based methods lose spatial information and misalign local features,\nwhile reconstruction-based methods fail to utilize hierarchical feature\ninformation and lack mechanisms to focus on discriminative regions. We propose\nthe Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which\nintegrates dual-layer feature reconstruction with mask-enhanced feature\nprocessing to improve fine-grained classification. HMDRN incorporates a\ndual-layer feature reconstruction and fusion module that leverages\ncomplementary visual information from different network hierarchies. Through\nlearnable fusion weights, the model balances high-level semantic\nrepresentations from the last layer with mid-level structural details from the\npenultimate layer. Additionally, we design a spatial binary mask-enhanced\ntransformer self-reconstruction module that processes query features through\nadaptive thresholding while maintaining complete support features, enhancing\nfocus on discriminative regions while filtering background noise. Extensive\nexperiments on three challenging fine-grained datasets demonstrate that HMDRN\nconsistently outperforms state-of-the-art methods across Conv-4 and ResNet-12\nbackbone architectures. Comprehensive ablation studies validate the\neffectiveness of each proposed component, revealing that dual-layer\nreconstruction enhances inter-class discrimination while mask-enhanced\ntransformation reduces intra-class variations. Visualization results provide\nevidence of HMDRN's superior feature reconstruction capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHMDRN\u7684\u5206\u5c42\u63a9\u7801\u589e\u5f3a\u53cc\u91cd\u5efa\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u5c11\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u548c\u63a9\u7801\u589e\u5f3a\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c11\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\uff08FS-FGIC\uff09\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u3001\u5c40\u90e8\u7279\u5f81\u4e0d\u5bf9\u9f50\u3001\u7f3a\u4e4f\u5c42\u6b21\u7279\u5f81\u5229\u7528\u7b49\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "HMDRN\u7ed3\u5408\u4e86\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u4e0e\u63a9\u7801\u589e\u5f3a\u7279\u5f81\u5904\u7406\u3002\u901a\u8fc7\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u6a21\u5757\uff0c\u6a21\u578b\u5e73\u8861\u4e86\u9ad8\u5c42\u8bed\u4e49\u548c\u4e2d\u7ea7\u7ed3\u6784\u7ec6\u8282\uff1b\u901a\u8fc7\u7a7a\u95f4\u4e8c\u503c\u63a9\u7801\u589e\u5f3a\u7684\u81ea\u91cd\u5efa\u6a21\u5757\uff0c\u6a21\u578b\u589e\u5f3a\u4e86\u5224\u522b\u6027\u533a\u57df\u7684\u5173\u6ce8\u5e76\u8fc7\u6ee4\u80cc\u666f\u566a\u58f0\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cHMDRN\u5728Conv-4\u548cResNet-12\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "HMDRN\u901a\u8fc7\u53cc\u5c42\u6b21\u91cd\u5efa\u548c\u63a9\u7801\u589e\u5f3a\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7c7b\u5185\u5dee\u5f02\u3002", "paper_title_zh": "\u5206\u5c42\u63a9\u7801\u589e\u5f3a\u53cc\u91cd\u5efa\u7f51\u7edc\u7528\u4e8e\u5c11\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b", "abstract_zh": "\u5c11\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\uff08FS-FGIC\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5728\u6709\u9650\u7684\u6807\u8bb0\u6837\u672c\u4e0b\u533a\u5206\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u5b50\u7c7b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5ea6\u91cf\u7684\u65b9\u6cd5\u4e22\u5931\u7a7a\u95f4\u4fe1\u606f\u4e14\u5c40\u90e8\u7279\u5f81\u4e0d\u5bf9\u9f50\uff0c\u800c\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u5c42\u6b21\u7279\u5f81\u4fe1\u606f\u4e14\u7f3a\u4e4f\u5173\u6ce8\u5224\u522b\u6027\u533a\u57df\u7684\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5c42\u63a9\u7801\u589e\u5f3a\u53cc\u91cd\u5efa\u7f51\u7edc\uff08HMDRN\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u4e0e\u63a9\u7801\u589e\u5f3a\u7279\u5f81\u5904\u7406\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3002HMDRN\u5305\u542b\u4e00\u4e2a\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u4e0e\u878d\u5408\u6a21\u5757\uff0c\u5229\u7528\u4e0d\u540c\u7f51\u7edc\u5c42\u6b21\u7684\u4e92\u8865\u89c6\u89c9\u4fe1\u606f\u3002\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u878d\u5408\u6743\u91cd\uff0c\u6a21\u578b\u5e73\u8861\u4e86\u6700\u540e\u4e00\u5c42\u7684\u9ad8\u5c42\u8bed\u4e49\u8868\u793a\u4e0e\u5012\u6570\u7b2c\u4e8c\u5c42\u7684\u4e2d\u7ea7\u7ed3\u6784\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7a7a\u95f4\u4e8c\u503c\u63a9\u7801\u589e\u5f3a\u7684\u81ea\u91cd\u5efa\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u5904\u7406\u67e5\u8be2\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u7684\u652f\u6301\u7279\u5f81\uff0c\u589e\u5f3a\u4e86\u5bf9\u5224\u522b\u6027\u533a\u57df\u7684\u5173\u6ce8\u5e76\u8fc7\u6ee4\u4e86\u80cc\u666f\u566a\u58f0\u3002\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHMDRN\u5728Conv-4\u548cResNet-12\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u53cc\u5c42\u6b21\u91cd\u5efa\u589e\u5f3a\u4e86\u7c7b\u95f4\u533a\u5206\u80fd\u529b\uff0c\u800c\u63a9\u7801\u589e\u5f3a\u53d8\u6362\u51cf\u5c11\u4e86\u7c7b\u5185\u5dee\u5f02\u3002\u53ef\u89c6\u5316\u7ed3\u679c\u8bc1\u660e\u4e86HMDRN\u5728\u7279\u5f81\u91cd\u5efa\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.20486", "pdf": "https://arxiv.org/pdf/2506.20486", "abs": "https://arxiv.org/abs/2506.20486", "authors": ["Salvatore Milite", "Giulio Caravagna", "Andrea Sottoriva"], "title": "Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization", "categories": ["cs.AI"], "comment": null, "summary": "Neural Cellular Automata (NCAs) are a promising new approach to model\nself-organizing processes, with potential applications in life science.\nHowever, their deterministic nature limits their ability to capture the\nstochasticity of real-world biological and physical systems.\n  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework\nincorporating the idea of mixture models into the NCA paradigm. By combining\nprobabilistic rule assignments with intrinsic noise, MNCAs can model diverse\nlocal behaviors and reproduce the stochastic dynamics observed in biological\nprocesses.\n  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic\nsimulations of tissue growth and differentiation, (2) image morphogenesis\nrobustness, and (3) microscopy image segmentation. Results show that MNCAs\nachieve superior robustness to perturbations, better recapitulate real\nbiological growth patterns, and provide interpretable rule segmentation. These\nfindings position MNCAs as a promising tool for modeling stochastic dynamical\nsystems and studying self-growth processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08MNCA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6982\u7387\u89c4\u5219\u548c\u5185\u5728\u566a\u58f0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u786e\u5b9a\u6027\u8fc7\u5f3a\u7684\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u771f\u5b9e\u751f\u7269\u7cfb\u7edf\u4e2d\u7684\u968f\u673a\u52a8\u6001\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u56e0\u5176\u786e\u5b9a\u6027\u65e0\u6cd5\u5145\u5206\u6a21\u62df\u771f\u5b9e\u751f\u7269\u548c\u7269\u7406\u7cfb\u7edf\u7684\u968f\u673a\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u751f\u547d\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u6df7\u5408\u6a21\u578b\u7684\u6982\u5ff5\uff0c\u589e\u5f3aNCA\u7684\u968f\u673a\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08MNCA\uff09\uff0c\u5c06\u6982\u7387\u89c4\u5219\u5206\u914d\u4e0e\u5185\u5728\u566a\u58f0\u7ed3\u5408\uff0c\u4ee5\u6a21\u62df\u591a\u6837\u5316\u7684\u5c40\u90e8\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\u7ec4\u7ec7\u751f\u957f\u4e0e\u5206\u5316\u7684\u5408\u6210\u6a21\u62df\u3001\u56fe\u50cf\u5f62\u6001\u751f\u6210\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u663e\u5fae\u955c\u56fe\u50cf\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMNCA\u5728\u6297\u5e72\u6270\u6027\u3001\u771f\u5b9e\u751f\u7269\u751f\u957f\u6a21\u5f0f\u7684\u518d\u73b0\u4ee5\u53ca\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u5206\u5272\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MNCA\u4e3a\u6a21\u62df\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u548c\u7814\u7a76\u81ea\u751f\u957f\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u5c24\u5176\u5728\u751f\u547d\u79d1\u5b66\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "paper_title_zh": "\u6df7\u5408\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff1a\u4e00\u79cd\u7528\u4e8e\u751f\u957f\u5efa\u6a21\u4e0e\u81ea\u7ec4\u7ec7\u7684\u968f\u673a\u6846\u67b6", "abstract_zh": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u662f\u4e00\u79cd\u6a21\u62df\u81ea\u7ec4\u7ec7\u8fc7\u7a0b\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u751f\u547d\u79d1\u5b66\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u3002\u7136\u800c\uff0c\u5176\u786e\u5b9a\u6027\u9650\u5236\u4e86\u5176\u6a21\u62df\u771f\u5b9e\u751f\u7269\u548c\u7269\u7406\u7cfb\u7edf\u968f\u673a\u6027\u7684\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u6df7\u5408\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08MNCA\uff09\uff0c\u5c06\u6df7\u5408\u6a21\u578b\u7684\u6982\u5ff5\u5f15\u5165NCA\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u6982\u7387\u89c4\u5219\u548c\u5185\u5728\u566a\u58f0\uff0c\u80fd\u591f\u6a21\u62df\u591a\u6837\u5316\u7684\u5c40\u90e8\u884c\u4e3a\u5e76\u518d\u73b0\u751f\u7269\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u52a8\u6001\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u5173\u952e\u9886\u57df\u8bc4\u4f30\u4e86MNCA\u7684\u6709\u6548\u6027\uff1a\uff081\uff09\u7ec4\u7ec7\u751f\u957f\u4e0e\u5206\u5316\u7684\u5408\u6210\u6a21\u62df\uff0c\uff082\uff09\u56fe\u50cf\u5f62\u6001\u751f\u6210\u7684\u9c81\u68d2\u6027\uff0c\uff083\uff09\u663e\u5fae\u955c\u56fe\u50cf\u5206\u5272\u3002\u7ed3\u679c\u8868\u660e\uff0cMNCA\u5728\u6297\u5e72\u6270\u6027\u3001\u771f\u5b9e\u751f\u7269\u751f\u957f\u6a21\u5f0f\u7684\u518d\u73b0\u4ee5\u53ca\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u5206\u5272\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cMNCA\u662f\u6a21\u62df\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u548c\u7814\u7a76\u81ea\u751f\u957f\u8fc7\u7a0b\u7684\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2506.20243", "pdf": "https://arxiv.org/pdf/2506.20243", "abs": "https://arxiv.org/abs/2506.20243", "authors": ["Papa S\u00e9ga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "categories": ["cs.CL", "eess.AS"], "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5757\u7684\u591a\u81ea\u76d1\u7763\u5b66\u4e60\u878d\u5408\u65b9\u6cd5\uff08CBF-AFA\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u6d41\u5229\u5ea6\u8bc4\u4f30\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u6d41\u5229\u5ea6\u8bc4\u4f30\uff08AFA\uff09\u5728\u975e\u6bcd\u8bed\u8005\u7684\u8bed\u97f3\u8282\u594f\u3001\u505c\u987f\u548c\u4e0d\u6d41\u7545\u6027\u6355\u6349\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u878d\u5408\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5757\u65b9\u6cd5\uff0c\u7ed3\u5408Wav2Vec2\u3001HuBERT\u548cWavLM\u7b49\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528Silero-VAD\u8fdb\u884c\u8bed\u97f3\u5206\u5757\uff0c\u5e76\u901a\u8fc7CNN-BiLSTM\u6846\u67b6\u878d\u5408\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u5728Speechocean762\u548cAvalinguo\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u53472.8\u548c4.2\uff0cPearson\u76f8\u5173\u7cfb\u6570\u5206\u522b\u63d0\u53476.2\u548c4.0\uff0c\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u3002", "conclusion": "\u5206\u5757\u591a\u81ea\u76d1\u7763\u5b66\u4e60\u878d\u5408\u65b9\u6cd5\u5728\u6d41\u5229\u5ea6\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5728\u975e\u89c4\u5219\u97f5\u5f8b\u65b9\u8a00\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "CBF-AFA\uff1a\u57fa\u4e8e\u5206\u5757\u7684\u591a\u81ea\u76d1\u7763\u5b66\u4e60\u878d\u5408\u81ea\u52a8\u6d41\u5229\u5ea6\u8bc4\u4f30\u65b9\u6cd5", "abstract_zh": "\u81ea\u52a8\u6d41\u5229\u5ea6\u8bc4\u4f30\uff08AFA\uff09\u5728\u6355\u6349\u975e\u6bcd\u8bed\u8005\u7684\u8bed\u97f3\u8282\u594f\u3001\u505c\u987f\u548c\u4e0d\u6d41\u7545\u6027\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5757\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08Wav2Vec2\u3001HuBERT\u548cWavLM\uff09\uff0c\u5229\u7528\u5176\u4e92\u8865\u4f18\u52bf\u8fdb\u884c\u8bed\u97f3\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5206\u5c42CNN-BiLSTM\u6846\u67b6\u5b9e\u73b0\u3002\u8bed\u97f3\u901a\u8fc7Silero\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\uff08Silero-VAD\uff09\u5206\u5272\u4e3a\u547c\u5438\u7ec4\u5206\u5757\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5206\u6790\u5e76\u51cf\u5c11\u8fc7\u5206\u5272\u95ee\u9898\u3002\u81ea\u76d1\u7763\u5b66\u4e60\u5d4c\u5165\u901a\u8fc7\u53ef\u5b66\u4e60\u52a0\u6743\u673a\u5236\u878d\u5408\uff0c\u5e73\u8861\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u52a0\u5165\u5206\u5757\u7ea7\u6d41\u5229\u5ea6\u6807\u8bb0\uff08\u5982\u8bed\u901f\u3001\u505c\u987f\u65f6\u957f\u3001n-gram\u91cd\u590d\uff09\u3002CNN-BiLSTM\u6355\u6349\u5206\u5757\u95f4\u7684\u5c40\u90e8\u548c\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u5728Avalinguo\u548cSpeechocean762\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728Speechocean762\u4e0a\u7684F1\u5206\u6570\u548cPearson\u76f8\u5173\u7cfb\u6570\u5206\u522b\u63d0\u53472.8\u548c6.2\uff0c\u5728Avalinguo\u4e0a\u5206\u522b\u63d0\u53474.2\u548c4.0\uff0c\u4f18\u4e8e\u57fa\u4e8ePyannote.audio\u7684\u5206\u5272\u57fa\u7ebf\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5206\u5757\u7684\u591a\u81ea\u76d1\u7763\u5b66\u4e60\u878d\u5408\u65b9\u6cd5\u5728\u6d41\u5229\u5ea6\u8bc4\u4f30\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u975e\u89c4\u5219\u97f5\u5f8b\u65b9\u8a00\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.20272", "pdf": "https://arxiv.org/pdf/2506.20272", "abs": "https://arxiv.org/abs/2506.20272", "authors": ["Juan Jos\u00e9 Murillo-Fuentes", "Pablo M. Olmos", "Laura Alba-Carcel\u00e9n"], "title": "Forensic Study of Paintings Through the Comparison of Fabrics", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The study of canvas fabrics in works of art is a crucial tool for\nauthentication, attribution and conservation. Traditional methods are based on\nthread density map matching, which cannot be applied when canvases do not come\nfrom contiguous positions on a roll. This paper presents a novel approach based\non deep learning to assess the similarity of textiles. We introduce an\nautomatic tool that evaluates the similarity between canvases without relying\non thread density maps. A Siamese deep learning model is designed and trained\nto compare pairs of images by exploiting the feature representations learned\nfrom the scans. In addition, a similarity estimation method is proposed,\naggregating predictions from multiple pairs of cloth samples to provide a\nrobust similarity score. Our approach is applied to canvases from the Museo\nNacional del Prado, corroborating the hypothesis that plain weave canvases,\nwidely used in painting, can be effectively compared even when their thread\ndensities are similar. The results demonstrate the feasibility and accuracy of\nthe proposed method, opening new avenues for the analysis of masterpieces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u753b\u5e03\u7eba\u7ec7\u54c1\u7684\u76f8\u4f3c\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf\u7ebf\u5bc6\u5ea6\u56fe\u5339\u914d\uff0c\u4e3a\u827a\u672f\u54c1\u9274\u5b9a\u548c\u4fdd\u5b58\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u753b\u5e03\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u5bc6\u5ea6\u56fe\u5339\u914d\uff0c\u4f46\u65e0\u6cd5\u9002\u7528\u4e8e\u975e\u8fde\u7eed\u4f4d\u7f6e\u7684\u753b\u5e03\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4e0d\u4f9d\u8d56\u7ebf\u5bc6\u5ea6\u56fe\u7684\u81ea\u52a8\u5de5\u5177\uff0c\u4ee5\u66f4\u7075\u6d3b\u5730\u8bc4\u4f30\u753b\u5e03\u76f8\u4f3c\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5b6a\u751f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u626b\u63cf\u56fe\u50cf\u7684\u7279\u5f81\u8868\u793a\u6bd4\u8f83\u753b\u5e03\u5bf9\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u4f3c\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u591a\u5bf9\u6837\u672c\u7684\u9884\u6d4b\u63d0\u4f9b\u7a33\u5065\u7684\u76f8\u4f3c\u6027\u8bc4\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u666e\u62c9\u591a\u535a\u7269\u9986\u7684\u753b\u5e03\uff0c\u9a8c\u8bc1\u4e86\u5373\u4f7f\u7ebf\u5bc6\u5ea6\u76f8\u4f3c\uff0c\u5e73\u7eb9\u753b\u5e03\u4e5f\u80fd\u6709\u6548\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u827a\u672f\u54c1\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c24\u5176\u5728\u753b\u5e03\u76f8\u4f3c\u6027\u8bc4\u4f30\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "paper_title_zh": "\u901a\u8fc7\u7ec7\u7269\u6bd4\u8f83\u5bf9\u7ed8\u753b\u8fdb\u884c\u6cd5\u8bc1\u7814\u7a76", "abstract_zh": "\u827a\u672f\u54c1\u4e2d\u753b\u5e03\u7ec7\u7269\u7684\u7814\u7a76\u662f\u9274\u5b9a\u3001\u5f52\u5c5e\u548c\u4fdd\u5b58\u7684\u5173\u952e\u5de5\u5177\u3002\u4f20\u7edf\u65b9\u6cd5\u57fa\u4e8e\u7ebf\u5bc6\u5ea6\u56fe\u5339\u914d\uff0c\u4f46\u65e0\u6cd5\u9002\u7528\u4e8e\u975e\u8fde\u7eed\u4f4d\u7f6e\u7684\u753b\u5e03\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7eba\u7ec7\u54c1\u7684\u76f8\u4f3c\u6027\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u52a8\u5de5\u5177\uff0c\u65e0\u9700\u4f9d\u8d56\u7ebf\u5bc6\u5ea6\u56fe\u5373\u53ef\u8bc4\u4f30\u753b\u5e03\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5b6a\u751f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u626b\u63cf\u56fe\u50cf\u7684\u7279\u5f81\u8868\u793a\u6bd4\u8f83\u753b\u5e03\u5bf9\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u4f3c\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u591a\u5bf9\u6837\u672c\u7684\u9884\u6d4b\u63d0\u4f9b\u7a33\u5065\u7684\u76f8\u4f3c\u6027\u8bc4\u5206\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u666e\u62c9\u591a\u535a\u7269\u9986\u7684\u753b\u5e03\uff0c\u9a8c\u8bc1\u4e86\u5e7f\u6cdb\u7528\u4e8e\u7ed8\u753b\u7684\u5e73\u7eb9\u753b\u5e03\u5373\u4f7f\u5728\u7ebf\u5bc6\u5ea6\u76f8\u4f3c\u65f6\u4e5f\u80fd\u6709\u6548\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u540d\u4f5c\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.20504", "pdf": "https://arxiv.org/pdf/2506.20504", "abs": "https://arxiv.org/abs/2506.20504", "authors": ["Konstantin Demin", "Taylor Webb", "Eric Elmoznino", "Hakwan Lau"], "title": "Engineering Sentience", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "We spell out a definition of sentience that may be useful for designing and\nbuilding it in machines. We propose that for sentience to be meaningful for AI,\nit must be fleshed out in functional, computational terms, in enough detail to\nallow for implementation. Yet, this notion of sentience must also reflect\nsomething essentially 'subjective', beyond just having the general capacity to\nencode perceptual content. For this specific functional notion of sentience to\noccur, we propose that certain sensory signals need to be both assertoric\n(persistent) and qualitative. To illustrate the definition in more concrete\nterms, we sketch out some ways for potential implementation, given current\ntechnology. Understanding what it takes for artificial agents to be\nfunctionally sentient can also help us avoid creating them inadvertently, or at\nleast, realize that we have created them in a timely manner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7528\u4e8e\u8bbe\u8ba1\u548c\u6784\u5efa\u673a\u5668\u611f\u77e5\u80fd\u529b\u7684\u5b9a\u4e49\uff0c\u5f3a\u8c03\u611f\u77e5\u9700\u5177\u5907\u529f\u80fd\u6027\u548c\u8ba1\u7b97\u6027\uff0c\u540c\u65f6\u5305\u542b\u4e3b\u89c2\u6027\u3002\u901a\u8fc7\u5177\u4f53\u5b9e\u73b0\u65b9\u6848\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u8fd9\u4e00\u6982\u5ff5\u5e94\u7528\u4e8e\u5f53\u524d\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4e3a\u4eba\u5de5\u667a\u80fd\u8bbe\u8ba1\u4e00\u79cd\u6709\u610f\u4e49\u7684\u611f\u77e5\u80fd\u529b\u5b9a\u4e49\uff0c\u65e2\u9700\u529f\u80fd\u6027\u5b9e\u73b0\uff0c\u53c8\u9700\u4f53\u73b0\u4e3b\u89c2\u6027\uff0c\u4ee5\u907f\u514d\u65e0\u610f\u4e2d\u521b\u9020\u5177\u6709\u611f\u77e5\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fa\u611f\u77e5\u7684\u529f\u80fd\u6027\u5b9a\u4e49\uff0c\u5f3a\u8c03\u611f\u77e5\u4fe1\u53f7\u9700\u5177\u5907\u65ad\u8a00\u6027\u548c\u8d28\u6027\uff0c\u5e76\u7ed3\u5408\u5f53\u524d\u6280\u672f\u63a2\u8ba8\u4e86\u6f5c\u5728\u5b9e\u73b0\u65b9\u6848\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86\u4e00\u79cd\u5177\u4f53\u7684\u611f\u77e5\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u6280\u672f\u5b9e\u73b0\u65b9\u6848\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\uff0c\u4e3a\u4eba\u5de5\u611f\u77e5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u660e\u786e\u4eba\u5de5\u611f\u77e5\u7684\u529f\u80fd\u6027\u5b9a\u4e49\u6709\u52a9\u4e8e\u907f\u514d\u65e0\u610f\u4e2d\u521b\u9020\u611f\u77e5\u80fd\u529b\uff0c\u540c\u65f6\u4e3a\u76f8\u5173\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u3002", "paper_title_zh": "\u5de5\u7a0b\u5316\u611f\u77e5\u80fd\u529b", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7528\u4e8e\u8bbe\u8ba1\u548c\u6784\u5efa\u673a\u5668\u611f\u77e5\u80fd\u529b\u7684\u5b9a\u4e49\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u8981\u4f7f\u611f\u77e5\u5bf9\u4eba\u5de5\u667a\u80fd\u6709\u610f\u4e49\uff0c\u5fc5\u987b\u4ece\u529f\u80fd\u548c\u8ba1\u7b97\u89d2\u5ea6\u8be6\u7ec6\u9610\u8ff0\uff0c\u4ee5\u4fbf\u5b9e\u73b0\u3002\u540c\u65f6\uff0c\u8fd9\u79cd\u611f\u77e5\u6982\u5ff5\u8fd8\u9700\u4f53\u73b0\u67d0\u79cd\u672c\u8d28\u4e0a\u7684\u2018\u4e3b\u89c2\u6027\u2019\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5177\u5907\u7f16\u7801\u611f\u77e5\u5185\u5bb9\u7684\u901a\u7528\u80fd\u529b\u3002\u4e3a\u5b9e\u73b0\u8fd9\u79cd\u7279\u5b9a\u7684\u529f\u80fd\u6027\u611f\u77e5\uff0c\u6211\u4eec\u63d0\u51fa\u67d0\u4e9b\u611f\u5b98\u4fe1\u53f7\u9700\u540c\u65f6\u5177\u5907\u65ad\u8a00\u6027\uff08\u6301\u4e45\u6027\uff09\u548c\u8d28\u6027\u3002\u4e3a\u66f4\u5177\u4f53\u5730\u8bf4\u660e\u8fd9\u4e00\u5b9a\u4e49\uff0c\u6211\u4eec\u7ed3\u5408\u5f53\u524d\u6280\u672f\u52fe\u52d2\u4e86\u4e00\u4e9b\u6f5c\u5728\u5b9e\u73b0\u65b9\u5f0f\u3002\u7406\u89e3\u4eba\u5de5\u4ee3\u7406\u5982\u4f55\u5177\u5907\u529f\u80fd\u6027\u611f\u77e5\u80fd\u529b\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u907f\u514d\u65e0\u610f\u4e2d\u521b\u9020\u5b83\u4eec\uff0c\u8fd8\u80fd\u8ba9\u6211\u4eec\u53ca\u65f6\u610f\u8bc6\u5230\u5176\u5b58\u5728\u3002"}}
{"id": "2506.20269", "pdf": "https://arxiv.org/pdf/2506.20269", "abs": "https://arxiv.org/abs/2506.20269", "authors": ["Kai-Robin Lange", "Tobias Schmidt", "Matthias Reccius", "Henrik M\u00fcller", "Michael Roos", "Carsten Jentsch"], "title": "Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": "14 pages, 1 figure", "summary": "With rapidly evolving media narratives, it has become increasingly critical\nto not just extract narratives from a given corpus but rather investigate, how\nthey develop over time. While popular narrative extraction methods such as\nLarge Language Models do well in capturing typical narrative elements or even\nthe complex structure of a narrative, applying them to an entire corpus comes\nwith obstacles, such as a high financial or computational cost. We propose a\ncombination of the language understanding capabilities of Large Language Models\nwith the large scale applicability of topic models to dynamically model\nnarrative shifts across time using the Narrative Policy Framework. We apply a\ntopic model and a corresponding change point detection method to find changes\nthat concern a specific topic of interest. Using this model, we filter our\ncorpus for documents that are particularly representative of that change and\nfeed them into a Large Language Model that interprets the change that happened\nin an automated fashion and distinguishes between content and narrative shifts.\nWe employ our pipeline on a corpus of The Wall Street Journal news paper\narticles from 2009 to 2023. Our findings indicate that a Large Language Model\ncan efficiently extract a narrative shift if one exists at a given point in\ntime, but does not perform as well when having to decide whether a shift in\ncontent or a narrative shift took place.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u4e3b\u9898\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u53d9\u4e8b\u53d8\u5316\u3002\u901a\u8fc7\u4e3b\u9898\u6a21\u578b\u7b5b\u9009\u4ee3\u8868\u6027\u6587\u6863\uff0c\u518d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5206\u6790\u5185\u5bb9\u4e0e\u53d9\u4e8b\u53d8\u5316\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u53d9\u4e8b\u53d8\u5316\uff0c\u4f46\u5728\u533a\u5206\u5185\u5bb9\u53d8\u5316\u4e0e\u53d9\u4e8b\u53d8\u5316\u65f6\u8868\u73b0\u4e00\u822c\u3002", "motivation": "\u968f\u7740\u5a92\u4f53\u53d9\u4e8b\u7684\u5feb\u901f\u6f14\u53d8\uff0c\u4ec5\u63d0\u53d6\u53d9\u4e8b\u5df2\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u7814\u7a76\u5176\u968f\u65f6\u95f4\u7684\u53d1\u5c55\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5927\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u6355\u6349\u53d9\u4e8b\u5143\u7d20\uff0c\u4f46\u5168\u8bed\u6599\u5e93\u5e94\u7528\u6210\u672c\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u7ed3\u5408\u4e3b\u9898\u6a21\u578b\u7684\u5927\u89c4\u6a21\u9002\u7528\u6027\uff0c\u52a8\u6001\u5efa\u6a21\u53d9\u4e8b\u53d8\u5316\u3002", "method": "\u9996\u5148\u4f7f\u7528\u4e3b\u9898\u6a21\u578b\u548c\u53d8\u5316\u70b9\u68c0\u6d4b\u65b9\u6cd5\u8bc6\u522b\u7279\u5b9a\u4e3b\u9898\u7684\u53d8\u5316\uff0c\u7b5b\u9009\u4ee3\u8868\u6027\u6587\u6863\uff1b\u968f\u540e\u5c06\u8fd9\u4e9b\u6587\u6863\u8f93\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u52a8\u5206\u6790\u53d8\u5316\u7c7b\u578b\uff08\u5185\u5bb9\u53d8\u5316\u6216\u53d9\u4e8b\u53d8\u5316\uff09\u3002\u5b9e\u9a8c\u57fa\u4e8e2009\u81f32023\u5e74\u300a\u534e\u5c14\u8857\u65e5\u62a5\u300b\u6587\u7ae0\u8bed\u6599\u5e93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u68c0\u6d4b\u7279\u5b9a\u65f6\u95f4\u70b9\u7684\u53d9\u4e8b\u53d8\u5316\uff0c\u4f46\u5728\u533a\u5206\u5185\u5bb9\u53d8\u5316\u4e0e\u53d9\u4e8b\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e3b\u9898\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u80fd\u9ad8\u6548\u68c0\u6d4b\u53d9\u4e8b\u53d8\u5316\uff0c\u4f46\u5728\u533a\u5206\u53d8\u5316\u7c7b\u578b\u65f6\u4ecd\u9700\u6539\u8fdb\u3002", "paper_title_zh": "\u53d9\u4e8b\u53d8\u5316\u68c0\u6d4b\uff1a\u52a8\u6001\u4e3b\u9898\u6a21\u578b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5", "abstract_zh": "\u968f\u7740\u5a92\u4f53\u53d9\u4e8b\u7684\u5feb\u901f\u6f14\u53d8\uff0c\u4e0d\u4ec5\u9700\u8981\u4ece\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u53d9\u4e8b\uff0c\u8fd8\u9700\u7814\u7a76\u5176\u968f\u65f6\u95f4\u7684\u53d1\u5c55\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u53d9\u4e8b\u5143\u7d20\u6216\u590d\u6742\u7ed3\u6784\uff0c\u4f46\u5176\u5168\u8bed\u6599\u5e93\u5e94\u7528\u9762\u4e34\u9ad8\u6210\u672c\u7b49\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0e\u4e3b\u9898\u6a21\u578b\u7684\u5927\u89c4\u6a21\u9002\u7528\u6027\uff0c\u5229\u7528\u53d9\u4e8b\u653f\u7b56\u6846\u67b6\u52a8\u6001\u5efa\u6a21\u53d9\u4e8b\u53d8\u5316\u3002\u901a\u8fc7\u4e3b\u9898\u6a21\u578b\u53ca\u53d8\u5316\u70b9\u68c0\u6d4b\u65b9\u6cd5\u8bc6\u522b\u7279\u5b9a\u4e3b\u9898\u7684\u53d8\u5316\uff0c\u7b5b\u9009\u4ee3\u8868\u6027\u6587\u6863\u540e\u8f93\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u52a8\u5206\u6790\u53d8\u5316\u7c7b\u578b\uff08\u5185\u5bb9\u53d8\u5316\u6216\u53d9\u4e8b\u53d8\u5316\uff09\u3002\u5b9e\u9a8c\u57fa\u4e8e2009\u81f32023\u5e74\u300a\u534e\u5c14\u8857\u65e5\u62a5\u300b\u6587\u7ae0\u8bed\u6599\u5e93\uff0c\u7ed3\u679c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u68c0\u6d4b\u53d9\u4e8b\u53d8\u5316\uff0c\u4f46\u5728\u533a\u5206\u53d8\u5316\u7c7b\u578b\u65f6\u8868\u73b0\u4e00\u822c\u3002"}}
{"id": "2506.20279", "pdf": "https://arxiv.org/pdf/2506.20279", "abs": "https://arxiv.org/abs/2506.20279", "authors": ["Changliang Xia", "Chengyou Jia", "Zhuohang Dang", "Minnan Luo"], "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDenseDiT\u7684\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u5148\u9a8c\u548c\u8f7b\u91cf\u7ea7\u5206\u652f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u5e76\u5728\u65b0\u57fa\u51c6DenseWorld\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u5bc6\u96c6\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7406\u60f3\u5316\u6761\u4ef6\uff0c\u5bf9\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u9762\u4e34\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u65b9\u6cd5\u548c\u6570\u636e\u9ad8\u6548\u7b56\u7565\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faDenseDiT\u65b9\u6cd5\uff0c\u7ed3\u5408\u53c2\u6570\u91cd\u7528\u673a\u5236\u548c\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u5206\u652f\uff0c\u81ea\u9002\u5e94\u6574\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ec5\u9700\u4e0d\u52300.1%\u7684\u989d\u5916\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u5148\u9a8c\uff0c\u901a\u8fc7\u7edf\u4e00\u7b56\u7565\u5904\u7406\u591a\u6837\u5316\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5728DenseWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800cDenseDiT\u4ec5\u9700\u4e0d\u52300.01%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "DenseDiT\u901a\u8fc7\u7edf\u4e00\u7b56\u7565\u548c\u6570\u636e\u9ad8\u6548\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "paper_title_zh": "\u4ece\u7406\u60f3\u5230\u73b0\u5b9e\uff1a\u9762\u5411\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u7edf\u4e00\u4e14\u6570\u636e\u9ad8\u6548\u7684\u5bc6\u96c6\u9884\u6d4b", "abstract_zh": "\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u65e8\u5728\u4e3a\u8f93\u5165\u56fe\u50cf\u5b66\u4e60\u50cf\u7d20\u7ea7\u6807\u6ce8\u6807\u7b7e\u3002\u5c3d\u7ba1\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7406\u60f3\u5316\u6761\u4ef6\uff0c\u5bf9\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u9762\u4e34\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002\u4e3a\u7cfb\u7edf\u7814\u7a76\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165DenseWorld\u57fa\u51c6\uff0c\u6db5\u76d625\u4e2a\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u5bf9\u5e94\u7d27\u8feb\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u8de8\u4efb\u52a1\u7684\u7edf\u4e00\u8bc4\u4f30\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51faDenseDiT\uff0c\u901a\u8fc7\u7edf\u4e00\u7b56\u7565\u6700\u5927\u5316\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u5148\u9a8c\uff0c\u6267\u884c\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u3002DenseDiT\u7ed3\u5408\u53c2\u6570\u91cd\u7528\u673a\u5236\u548c\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u5206\u652f\uff0c\u81ea\u9002\u5e94\u6574\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\uff0c\u4ec5\u9700\u4e0d\u52300.1%\u7684\u989d\u5916\u53c2\u6570\u3002\u5728DenseWorld\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u901a\u7528\u548c\u4e13\u7528\u57fa\u7ebf\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u663e\u5176\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u7684\u4e0d\u8db3\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cDenseDiT\u4ec5\u9700\u4e0d\u52300.01%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u5f70\u663e\u5176\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002\u6211\u4eec\u7684\u6570\u636e\u3001\u68c0\u67e5\u70b9\u548c\u4ee3\u7801\u53ef\u5728https://xcltql666.github.io/DenseDiTProj\u83b7\u53d6\u3002"}}
{"id": "2506.20531", "pdf": "https://arxiv.org/pdf/2506.20531", "abs": "https://arxiv.org/abs/2506.20531", "authors": ["Wenbin Gan", "Minh-Son Dao", "Koji Zettsu"], "title": "Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios", "categories": ["cs.AI", "cs.CY"], "comment": "12 pages, 10 figures, under-review conference", "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08CBR-LLM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u4e2d\u505a\u51fa\u51b3\u7b56\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u8bed\u4e49\u573a\u666f\u7406\u89e3\u548c\u5386\u53f2\u9a7e\u9a76\u6848\u4f8b\u68c0\u7d22\uff0c\u63d0\u5347\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u5feb\u901f\u4e14\u57fa\u4e8e\u60c5\u5883\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u76f4\u63a5\u5e94\u7528\u4ecd\u53d7\u9650\u4e8e\u9886\u57df\u9002\u5e94\u6027\u3001\u60c5\u5883\u7406\u89e3\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u7ecf\u9a8c\u3002", "method": "\u672c\u6587\u63d0\u51faCBR-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u573a\u666f\u7406\u89e3\uff08\u6765\u81ea\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u8f93\u5165\uff09\u548c\u5386\u53f2\u9a7e\u9a76\u6848\u4f8b\u68c0\u7d22\uff0c\u4f7fLLMs\u80fd\u591f\u751f\u6210\u65e2\u7b26\u5408\u60c5\u5883\u53c8\u4e0e\u4eba\u4e00\u81f4\u7684\u9a7e\u9a76\u5efa\u8bae\u3002\u6b64\u5916\uff0c\u91c7\u7528\u98ce\u9669\u611f\u77e5\u63d0\u793a\u7b56\u7565\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6848\u4f8b\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u3001\u7406\u7531\u8d28\u91cf\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002\u98ce\u9669\u611f\u77e5\u63d0\u793a\u7b56\u7565\u548c\u76f8\u4f3c\u6027\u6848\u4f8b\u68c0\u7d22\u65b9\u6cd5\u5747\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u3002\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u590d\u6742\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CBR-LLM\u6846\u67b6\u4e3a\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e14\u53ef\u4fe1\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a8\u6001\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\u5728\u73b0\u5b9e\u5b89\u5168\u5173\u952e\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u5e94\u7528", "abstract_zh": "\u5728\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u5feb\u901f\u4e14\u57fa\u4e8e\u60c5\u5883\u7684\u51b3\u7b56\u9700\u8981\u7ed3\u5408\u5bf9\u573a\u666f\u7684\u7406\u89e3\u548c\u7ecf\u9a8c\u63a8\u7406\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51ed\u501f\u5176\u5f3a\u5927\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6b64\u7c7b\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9886\u57df\u9002\u5e94\u6027\u3001\u60c5\u5883\u7406\u89e3\u4ee5\u53ca\u7f3a\u4e4f\u5728\u9ad8\u98ce\u9669\u52a8\u6001\u73af\u5883\u4e2d\u505a\u51fa\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u51b3\u7b56\u6240\u9700\u7684\u7ecf\u9a8c\u77e5\u8bc6\u7b49\u95ee\u9898\uff0cLLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u4ecd\u53d7\u9650\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08CBR-LLM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u98ce\u9669\u573a\u666f\u4e2d\u5236\u5b9a\u89c4\u907f\u52a8\u4f5c\u51b3\u7b56\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u8f93\u5165\u7684\u8bed\u4e49\u573a\u666f\u7406\u89e3\u4e0e\u76f8\u5173\u5386\u53f2\u9a7e\u9a76\u6848\u4f8b\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u4f7fLLMs\u80fd\u591f\u751f\u6210\u65e2\u7b26\u5408\u60c5\u5883\u53c8\u4e0e\u4eba\u4e00\u81f4\u7684\u9a7e\u9a76\u5efa\u8bae\u3002\u5728\u591a\u4e2a\u5f00\u6e90LLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u3001\u7406\u7531\u8d28\u91cf\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002\u98ce\u9669\u611f\u77e5\u63d0\u793a\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5728\u4e0d\u540c\u98ce\u9669\u7c7b\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u800c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6848\u4f8b\u68c0\u7d22\u5728\u6307\u5bfc\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u3002\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7a81\u663e\u4e86\u5176\u4f5c\u4e3a\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u81ea\u9002\u5e94\u4e14\u53ef\u4fe1\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20331", "pdf": "https://arxiv.org/pdf/2506.20331", "abs": "https://arxiv.org/abs/2506.20331", "authors": ["Rian Touchent", "Nathan Godey", "Eric de la Clergerie"], "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content", "categories": ["cs.CL", "cs.LG"], "comment": "Dataset link: https://hf.co/datasets/almanach/Biomed-Enriched", "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Biomed-Enriched\uff0c\u4e00\u4e2a\u901a\u8fc7\u4e24\u9636\u6bb5\u6807\u6ce8\u8fc7\u7a0b\u4ecePubMed\u6784\u5efa\u7684\u751f\u7269\u533b\u5b66\u6587\u672c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u548c\u63d0\u53d6\u7f55\u89c1\u53ca\u9690\u85cf\u5185\u5bb9\u3002\u6570\u636e\u96c6\u5305\u542b\u9ad8\u8d28\u91cf\u4e34\u5e8a\u6848\u4f8b\u6bb5\u843d\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u8fc7\u6ee4\u548c\u9886\u57df\u4e0a\u91c7\u6837\u4f18\u5316\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u6587\u672c\u901a\u5e38\u56e0\u9690\u79c1\u9650\u5236\u96be\u4ee5\u83b7\u53d6\uff0c\u800cPubMed\u4e2d\u7684\u4e34\u5e8a\u6848\u4f8b\u6bb5\u843d\u53ef\u4f5c\u4e3a\u66ff\u4ee3\u8d44\u6e90\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u5f00\u653e\u7684\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u6807\u6ce8\u8fc7\u7a0b\u6784\u5efa\u6570\u636e\u96c6\uff1a\u9996\u5148\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9PubMed\u768440\u4e07\u6bb5\u843d\u8fdb\u884c\u7c7b\u578b\u3001\u9886\u57df\u548c\u6559\u80b2\u8d28\u91cf\u8bc4\u5206\u6807\u6ce8\uff1b\u7136\u540e\u5229\u7528\u5c0f\u8bed\u8a00\u6a21\u578b\u5c06\u6807\u7b7e\u4f20\u64ad\u81f3\u6574\u4e2aPMC-OA\u8bed\u6599\u5e93\uff0c\u6700\u7ec8\u63d0\u53d6\u9ad8\u8d28\u91cf\u5b50\u96c6\u5e76\u8fdb\u884c\u8d28\u91cf\u8fc7\u6ee4\u548c\u9886\u57df\u4e0a\u91c7\u6837\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b200\u4e07\u4e34\u5e8a\u6848\u4f8b\u6bb5\u843d\uff0c\u5176\u4e2d45\u4e07\u4e3a\u9ad8\u8d28\u91cf\u5185\u5bb9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e34\u5e8a\u4e0a\u91c7\u6837\u4f7fMMLU ProfMed\u6027\u80fd\u63d0\u5347\u7ea65%\uff0c\u6559\u80b2\u8d28\u91cf\u8fc7\u6ee4\u4f7fMedQA\u548cMedMCQA\u63d0\u5347\u7ea61%\uff0c\u4e14\u7ec4\u5408\u6280\u672f\u53ef\u52a0\u901f\u6536\u655b\uff0c\u51cf\u5c11\u8bad\u7ec3\u6807\u8bb0\u9700\u6c42\u3002", "conclusion": "Biomed-Enriched\u4e3a\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8aNLP\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u5f00\u653e\u8d44\u6e90\uff0c\u5176\u4f18\u5316\u5b50\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e3a\u9ad8\u6548\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "paper_title_zh": "Biomed-Enriched\uff1a\u4e00\u4e2a\u901a\u8fc7LLM\u589e\u5f3a\u7684\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u548c\u63d0\u53d6\u7f55\u89c1\u53ca\u9690\u85cf\u5185\u5bb9", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86Biomed-Enriched\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u4e24\u9636\u6bb5\u6807\u6ce8\u8fc7\u7a0b\u4ecePubMed\u6784\u5efa\u7684\u751f\u7269\u533b\u5b66\u6587\u672c\u6570\u636e\u96c6\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9PubMed\u79d1\u5b66\u6587\u7ae0\u4e2d\u768440\u4e07\u6bb5\u843d\u8fdb\u884c\u6807\u6ce8\uff0c\u8bc4\u4f30\u5176\u7c7b\u578b\uff08\u7efc\u8ff0\u3001\u7814\u7a76\u3001\u4e34\u5e8a\u6848\u4f8b\u7b49\uff09\u3001\u9886\u57df\uff08\u4e34\u5e8a\u3001\u751f\u7269\u533b\u5b66\u7b49\uff09\u548c\u6559\u80b2\u8d28\u91cf\uff081\u81f35\u5206\uff09\u3002\u8fd9\u4e9b\u6807\u6ce8\u7528\u4e8e\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5c06\u6807\u7b7e\u4f20\u64ad\u81f3\u6574\u4e2aPMC-OA\u8bed\u6599\u5e93\u3002\u751f\u6210\u7684\u5143\u6570\u636e\u4f7f\u6211\u4eec\u80fd\u591f\u63d0\u53d6\u7cbe\u70bc\u5b50\u96c6\uff0c\u5305\u62ec200\u4e07\u4e34\u5e8a\u6848\u4f8b\u6bb5\u843d\uff08\u5176\u4e2d45\u4e07\u4e3a\u9ad8\u8d28\u91cf\u5185\u5bb9\uff09\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u8fc7\u6ee4\u548c\u9886\u57df\u4e0a\u91c7\u6837\u6784\u5efa\u591a\u4e2a\u53d8\u4f53\u3002\u7531\u4e8e\u9690\u79c1\u9650\u5236\uff0c\u4e34\u5e8a\u6587\u672c\u901a\u5e38\u96be\u4ee5\u83b7\u53d6\uff0c\u800c\u6211\u4eec\u7684\u6570\u636e\u96c6\u63d0\u4f9b\u4e86PubMed\u4e2d\u5927\u89c4\u6a21\u3001\u5f00\u653e\u7684\u4e34\u5e8a\u6848\u4f8b\u96c6\u5408\uff0c\u6210\u4e3a\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8aNLP\u7684\u5b9d\u8d35\u8d44\u6e90\u3002\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u4f18\u5316\u5b50\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e34\u5e8a\u4e0a\u91c7\u6837\u4f7fMMLU ProfMed\u6027\u80fd\u63d0\u5347\u7ea65%\uff0c\u6559\u80b2\u8d28\u91cf\u8fc7\u6ee4\u4f7fMedQA\u548cMedMCQA\u63d0\u5347\u7ea61%\u3002\u7ec4\u5408\u6280\u672f\u8fd8\u80fd\u52a0\u901f\u6536\u655b\uff0c\u51cf\u5c11\u8bad\u7ec3\u6807\u8bb0\u9700\u6c42\uff0c\u4e3a\u9ad8\u6548\u751f\u7269\u533b\u5b66\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2506.20293", "pdf": "https://arxiv.org/pdf/2506.20293", "abs": "https://arxiv.org/abs/2506.20293", "authors": ["Kunjing Yang", "Libin Zheng", "Minru Bai", "Ting Lu", "Leyuan Fang"], "title": "Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The blind fusion of unregistered hyperspectral images (HSIs) and\nmultispectral images (MSIs) has attracted growing attention recently. To\naddress the registration challenge, most existing methods employ spatial\ntransformations on the HSI to achieve alignment with the MSI. However, due to\nthe substantial differences in spatial resolution of the images, the\nperformance of these methods is often unsatisfactory. Moreover, the\nregistration process tends to be time-consuming when dealing with large-sized\nimages in remote sensing. To address these issues, we propose tackling the\nregistration problem from the spectral domain. Initially, a lightweight\nSpectral Prior Learning (SPL) network is developed to extract spectral features\nfrom the HSI and enhance the spectral resolution of the MSI. Following this,\nthe obtained image undergoes spatial downsampling to produce the registered\nHSI. In this process, subspace representation and cyclic training strategy are\nemployed to improve spectral accuracy of the registered HSI obtained. Next, we\npropose a blind sparse fusion (BSF) method, which utilizes group sparsity\nregularization to equivalently promote the low-rankness of the image. This\napproach not only circumvents the need for rank estimation, but also reduces\ncomputational complexity. Then, we employ the Proximal Alternating Optimization\n(PAO) algorithm to solve the BSF model, and present its convergence analysis.\nFinally, extensive numerical experiments on simulated and real datasets are\nconducted to verify the effectiveness of our method in registration and fusion.\nWe also demonstrate its efficacy in enhancing classification performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u8c31\u57df\u914d\u51c6\u7684\u9ad8\u5149\u8c31\u4e0e\u591a\u5149\u8c31\u56fe\u50cf\u76f2\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5149\u8c31\u5148\u9a8c\u5b66\u4e60\u7f51\u7edc\u63d0\u5347\u914d\u51c6\u7cbe\u5ea6\uff0c\u5e76\u91c7\u7528\u76f2\u7a00\u758f\u878d\u5408\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u5149\u8c31\u4e0e\u591a\u5149\u8c31\u56fe\u50cf\u914d\u51c6\u65f6\u56e0\u7a7a\u95f4\u5206\u8fa8\u7387\u5dee\u5f02\u5927\u800c\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u5904\u7406\u5927\u5c3a\u5bf8\u9065\u611f\u56fe\u50cf\u65f6\u8017\u65f6\u4e25\u91cd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u4ece\u5149\u8c31\u57df\u89d2\u5ea6\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "1. \u5f00\u53d1\u8f7b\u91cf\u7ea7\u5149\u8c31\u5148\u9a8c\u5b66\u4e60\uff08SPL\uff09\u7f51\u7edc\u63d0\u53d6\u5149\u8c31\u7279\u5f81\u5e76\u589e\u5f3a\u591a\u5149\u8c31\u56fe\u50cf\u5206\u8fa8\u7387\uff1b2. \u901a\u8fc7\u5b50\u7a7a\u95f4\u8868\u793a\u548c\u5faa\u73af\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u914d\u51c6\u540e\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5149\u8c31\u7cbe\u5ea6\uff1b3. \u63d0\u51fa\u76f2\u7a00\u758f\u878d\u5408\uff08BSF\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u7fa4\u7a00\u758f\u6b63\u5219\u5316\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b4. \u4f7f\u7528\u8fd1\u7aef\u4ea4\u66ff\u4f18\u5316\uff08PAO\uff09\u7b97\u6cd5\u6c42\u89e3BSF\u6a21\u578b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u914d\u51c6\u548c\u878d\u5408\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5149\u8c31\u57df\u914d\u51c6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u4e0e\u591a\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u914d\u51c6\u95ee\u9898\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "paper_title_zh": "\u7a81\u7834\u7a7a\u95f4\u8fb9\u754c\uff1a\u57fa\u4e8e\u5149\u8c31\u57df\u914d\u5bfc\u7684\u9ad8\u5149\u8c31\u4e0e\u591a\u5149\u8c31\u76f2\u878d\u5408", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u672a\u914d\u51c6\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u4e0e\u591a\u5149\u8c31\u56fe\u50cf\uff08MSI\uff09\u7684\u76f2\u878d\u5408\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u4e3a\u89e3\u51b3\u914d\u51c6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u91c7\u7528\u7a7a\u95f4\u53d8\u6362\u5c06HSI\u4e0eMSI\u5bf9\u9f50\uff0c\u4f46\u7531\u4e8e\u56fe\u50cf\u7a7a\u95f4\u5206\u8fa8\u7387\u5dee\u5f02\u663e\u8457\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6548\u679c\u5f80\u5f80\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5904\u7406\u5927\u5c3a\u5bf8\u9065\u611f\u56fe\u50cf\u65f6\u914d\u51c6\u8fc7\u7a0b\u8017\u65f6\u4e25\u91cd\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u4ece\u5149\u8c31\u57df\u89d2\u5ea6\u89e3\u51b3\u914d\u51c6\u95ee\u9898\u3002\u9996\u5148\uff0c\u5f00\u53d1\u8f7b\u91cf\u7ea7\u5149\u8c31\u5148\u9a8c\u5b66\u4e60\uff08SPL\uff09\u7f51\u7edc\u63d0\u53d6HSI\u5149\u8c31\u7279\u5f81\u5e76\u589e\u5f3aMSI\u5149\u8c31\u5206\u8fa8\u7387\uff1b\u968f\u540e\uff0c\u5bf9\u6240\u5f97\u56fe\u50cf\u8fdb\u884c\u7a7a\u95f4\u4e0b\u91c7\u6837\u751f\u6210\u914d\u51c6\u540e\u7684HSI\uff0c\u8fc7\u7a0b\u4e2d\u91c7\u7528\u5b50\u7a7a\u95f4\u8868\u793a\u548c\u5faa\u73af\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u5149\u8c31\u7cbe\u5ea6\u3002\u63a5\u7740\uff0c\u63d0\u51fa\u76f2\u7a00\u758f\u878d\u5408\uff08BSF\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u7fa4\u7a00\u758f\u6b63\u5219\u5316\u7b49\u6548\u4fc3\u8fdb\u56fe\u50cf\u4f4e\u79e9\u6027\uff0c\u65e2\u907f\u514d\u4e86\u79e9\u4f30\u8ba1\u9700\u6c42\uff0c\u53c8\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u7136\u540e\uff0c\u91c7\u7528\u8fd1\u7aef\u4ea4\u66ff\u4f18\u5316\uff08PAO\uff09\u7b97\u6cd5\u6c42\u89e3BSF\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5176\u6536\u655b\u6027\u3002\u6700\u540e\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u914d\u51c6\u548c\u878d\u5408\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.20598", "pdf": "https://arxiv.org/pdf/2506.20598", "abs": "https://arxiv.org/abs/2506.20598", "authors": ["Alexander D. Kalian", "Jaewook Lee", "Stefan P. Johannesson", "Lennart Otte", "Christer Hogstrand", "Miao Guo"], "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u7684\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u751f\u4ea7\u7814\u7a76\uff0c\u91cd\u70b9\u9488\u5bf9\u5fae\u751f\u7269\u86cb\u767d\u8d28\u6765\u6e90\u3002\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u7ed3\u5408\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\u4e24\u79cd\u65b9\u6cd5\u4f18\u5316\u4ee3\u7406\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u5bf9\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u6765\u6e90\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4e9f\u9700\u667a\u80fd\u5de5\u5177\u5feb\u901f\u5904\u7406\u548c\u7efc\u5408\u9886\u57df\u7279\u5b9a\u7684\u79d1\u5b66\u77e5\u8bc6\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u591a\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u751f\u4ea7\u7684\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5fae\u751f\u7269\u86cb\u767d\u8d28\u6765\u6e90\u3002", "method": "\u7814\u7a76\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u57fa\u4e8eGPT\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff1a\u4e00\u4e2a\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u79d1\u5b66\u6587\u732e\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u751f\u7269\u548c\u5316\u5b66\u4fe1\u606f\u3002\u901a\u8fc7\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\u4e24\u79cd\u65b9\u6cd5\u4f18\u5316\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u663e\u8457\u63d0\u5347\u4fe1\u606f\u63d0\u53d6\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u5fae\u8c03\u65b9\u6cd5\u5c06\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5206\u6570\u63d0\u5347\u81f3\u22650.94\uff0c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5219\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u591a\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u5728\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u751f\u4ea7\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u5fae\u8c03\u65b9\u6cd5\u5728\u6027\u80fd\u63d0\u5347\u4e0a\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\uff0c\u4f46\u540e\u8005\u5177\u6709\u66f4\u4f4e\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u7528\u6237\u754c\u9762\u4ee5\u652f\u6301\u7cfb\u7edf\u4f7f\u7528\u3002", "paper_title_zh": "\u57fa\u4e8e\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\uff1a\u7528\u4e8e\u89e3\u51b3\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u751f\u4ea7\u6311\u6218\u7684\u591a\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u6784\u5efa", "abstract_zh": "\u5168\u7403\u5bf9\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u6765\u6e90\u7684\u9700\u6c42\u52a0\u901f\u4e86\u5bf9\u80fd\u591f\u5feb\u901f\u5904\u7406\u548c\u7efc\u5408\u9886\u57df\u7279\u5b9a\u79d1\u5b66\u77e5\u8bc6\u7684\u667a\u80fd\u5de5\u5177\u7684\u9700\u6c42\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u5ff5\u9a8c\u8bc1\u7684\u591a\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u53ef\u6301\u7eed\u86cb\u767d\u8d28\u751f\u4ea7\u7814\u7a76\uff0c\u521d\u6b65\u805a\u7126\u4e8e\u5fae\u751f\u7269\u86cb\u767d\u8d28\u6765\u6e90\u3002\u6211\u4eec\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5bfc\u5411\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u57fa\u4e8eGPT\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff1a\uff081\uff09\u4e00\u4e2a\u6587\u732e\u68c0\u7d22\u4ee3\u7406\uff0c\u7528\u4e8e\u68c0\u7d22\u6307\u5b9a\u5fae\u751f\u7269\u83cc\u682a\u7684\u76f8\u5173\u79d1\u5b66\u6587\u732e\uff1b\uff082\uff09\u4e00\u4e2a\u4fe1\u606f\u63d0\u53d6\u4ee3\u7406\uff0c\u7528\u4e8e\u5904\u7406\u68c0\u7d22\u5185\u5bb9\u4ee5\u63d0\u53d6\u76f8\u5173\u7684\u751f\u7269\u548c\u5316\u5b66\u4fe1\u606f\u3002\u7814\u7a76\u63a2\u7d22\u4e86\u4e24\u79cd\u5e76\u884c\u65b9\u6cd5\uff08\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\uff09\u4ee5\u4f18\u5316\u4ee3\u7406\u6027\u80fd\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u6709\u6548\u63d0\u5347\u4e86\u4fe1\u606f\u63d0\u53d6\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u8868\u73b0\u4e3a\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5206\u6570\uff08\u63d0\u53d6\u7ed3\u679c\u4e0e\u7406\u60f3\u8f93\u51fa\u4e4b\u95f4\u7684\u6bd4\u8f83\uff09\u7684\u63d0\u5347\u3002\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5206\u6570\u6700\u9ad8\u63d0\u5347\u4e8625%\uff0c\u4e14\u666e\u904d\u8fbe\u5230\u22650.89\u7684\u5e73\u5747\u5206\u6570\u3002\u5fae\u8c03\u65b9\u6cd5\u603b\u4f53\u4e0a\u5c06\u5e73\u5747\u5206\u6570\u63d0\u5347\u81f3\u66f4\u9ad8\u7684\u6c34\u5e73\uff08\u666e\u904d\u22650.94\uff09\uff0c\u800c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5219\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86\u7528\u6237\u754c\u9762\uff0c\u4ee5\u652f\u6301\u591a\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u4f7f\u7528\uff0c\u5e76\u521d\u6b65\u63a2\u7d22\u4e86\u57fa\u4e8e\u5316\u5b66\u5b89\u5168\u7684\u989d\u5916\u641c\u7d22\u529f\u80fd\u3002"}}
{"id": "2506.20409", "pdf": "https://arxiv.org/pdf/2506.20409", "abs": "https://arxiv.org/abs/2506.20409", "authors": ["Ekaterina Taktasheva", "Jeff Dalton"], "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce \\name, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAPS\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6807\u7b7e\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5de5\u5177\u68c0\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u504f\u597d\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TAPS\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u6807\u7b7e\u5de5\u5177\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5de5\u5177\u68c0\u6d4b\u5668\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u6574\u5408\u7528\u6237\u504f\u597d\uff0c\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u3002", "result": "TAPS\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5728NLSI\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "TAPS\u901a\u8fc7\u7ed3\u6784\u5316\u6807\u7b7e\u548c\u4e0d\u786e\u5b9a\u6027\u68c0\u6d4b\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "TAPS\uff1a\u901a\u8fc7\u7ed3\u6784\u5316\u6807\u7b7e\u5b9e\u73b0\u5de5\u5177\u589e\u5f3a\u7684\u4e2a\u6027\u5316", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u5176\u6267\u884c\u590d\u6742\u7528\u6237\u4efb\u52a1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e2a\u6027\u5316\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6307\u5bfc\u4f5c\u7528\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u7528\u6237\u504f\u597d\u6709\u6548\u6574\u5408\u5230\u4efb\u52a1\u5bfc\u5411\u7684\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u3002\u901a\u8fc7\u5e7f\u6cdb\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u5173\u952e\u4e0d\u8db3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86TAPS\uff0c\u4e00\u79cd\u901a\u8fc7\u7ed3\u6784\u5316\u6807\u7b7e\u5de5\u5177\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5de5\u5177\u68c0\u6d4b\u5668\u6765\u589e\u5f3a\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u7684\u65b0\u65b9\u6cd5\u3002TAPS\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6574\u5408\u7528\u6237\u504f\u597d\u7684\u80fd\u529b\uff0c\u5e76\u5728NLSI\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u6700\u65b0\u6c34\u5e73\u3002"}}
{"id": "2506.20294", "pdf": "https://arxiv.org/pdf/2506.20294", "abs": "https://arxiv.org/abs/2506.20294", "authors": ["Shunqi Mao", "Wei Guo", "Chaoyi Zhang", "Weidong Cai"], "title": "Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 2 tables", "summary": "Diffusion models have shown strong performance in conditional generation by\nprogressively denoising Gaussian noise toward a target data distribution. This\ndenoising process can be interpreted as a form of hill climbing in a learned\nlatent space, where the model iteratively refines the sample toward regions of\nhigher probability. However, diffusion models often converge to local optima\nthat are locally visually coherent yet globally inconsistent or conditionally\nmisaligned, due to latent space complexity and suboptimal initialization. Prior\nefforts attempted to address this by strengthening guidance signals or\nmanipulating the initial noise distribution. We introduce Controlled Random\nZigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect\nand escape such local maxima during conditional generation. The method first\nidentifies potential local maxima using a reward model. Upon detection, it\ninjects noise and reverts to a previous, noisier state to escape the current\noptimization plateau. The reward model then evaluates candidate trajectories,\naccepting only those that offer improvement, while progressively deeper retreat\nenables stronger escapes when nearby alternatives fail. This controlled random\nzigzag process allows dynamic alternation between forward refinement and\nbackward exploration, enhancing both alignment and visual quality in the\ngenerated outputs. The proposed Ctrl-Z Sampling is model-agnostic and\ncompatible with existing diffusion frameworks. Experimental results show that\nCtrl-Z Sampling substantially improves generation quality with only around 7.6X\nincrease in function evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCtrl-Z\u91c7\u6837\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6269\u6563\u6a21\u578b\u4e2d\u901a\u8fc7\u63a7\u5236\u968f\u673a\u952f\u9f7f\u63a2\u7d22\u6765\u6539\u8fdb\u6761\u4ef6\u751f\u6210\u7684\u8d28\u91cf\uff0c\u907f\u514d\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6761\u4ef6\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u56e0\u6f5c\u5728\u7a7a\u95f4\u590d\u6742\u6027\u548c\u521d\u59cb\u5316\u4e0d\u4f73\u800c\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u5168\u5c40\u4e0d\u4e00\u81f4\u6216\u6761\u4ef6\u4e0d\u5339\u914d\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u6307\u5bfc\u4fe1\u53f7\u6216\u8c03\u6574\u521d\u59cb\u566a\u58f0\u5206\u5e03\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u6548\u679c\u6709\u9650\u3002", "method": "Ctrl-Z\u91c7\u6837\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u5c40\u90e8\u6700\u4f18\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u540e\u6ce8\u5165\u566a\u58f0\u5e76\u56de\u9000\u5230\u66f4\u65e9\u7684\u566a\u58f0\u72b6\u6001\u4ee5\u9003\u79bb\u5f53\u524d\u4f18\u5316\u5e73\u53f0\u3002\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\uff0c\u4ec5\u63a5\u53d7\u6539\u8fdb\u7684\u8def\u5f84\uff0c\u9010\u6b65\u52a0\u6df1\u56de\u9000\u4ee5\u589e\u5f3a\u9003\u79bb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCtrl-Z\u91c7\u6837\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4ec5\u9700\u7ea67.6\u500d\u7684\u51fd\u6570\u8bc4\u4f30\u5f00\u9500\u3002", "conclusion": "Ctrl-Z\u91c7\u6837\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u6269\u6563\u6846\u67b6\u517c\u5bb9\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u66ff\u524d\u5411\u4f18\u5316\u548c\u540e\u5411\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u6761\u4ef6\u5bf9\u9f50\u3002", "paper_title_zh": "Ctrl-Z\u91c7\u6837\uff1a\u57fa\u4e8e\u63a7\u5236\u968f\u673a\u952f\u9f7f\u63a2\u7d22\u7684\u6269\u6563\u91c7\u6837\u65b9\u6cd5", "abstract_zh": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u9010\u6b65\u5c06\u9ad8\u65af\u566a\u58f0\u53bb\u566a\u4e3a\u76ee\u6807\u6570\u636e\u5206\u5e03\uff0c\u5728\u6761\u4ef6\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u8fd9\u4e00\u53bb\u566a\u8fc7\u7a0b\u53ef\u89c6\u4e3a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7684\u4e00\u79cd\u722c\u5c71\u4f18\u5316\uff0c\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u5c06\u6837\u672c\u5411\u9ad8\u6982\u7387\u533a\u57df\u7ec6\u5316\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6f5c\u5728\u7a7a\u95f4\u590d\u6742\u6027\u548c\u521d\u59cb\u5316\u4e0d\u4f73\uff0c\u6269\u6563\u6a21\u578b\u5e38\u6536\u655b\u4e8e\u5c40\u90e8\u6700\u4f18\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u5c40\u90e8\u89c6\u89c9\u4e00\u81f4\u4f46\u5168\u5c40\u4e0d\u4e00\u81f4\u6216\u6761\u4ef6\u4e0d\u5339\u914d\u3002\u5148\u524d\u7814\u7a76\u5c1d\u8bd5\u901a\u8fc7\u589e\u5f3a\u6307\u5bfc\u4fe1\u53f7\u6216\u8c03\u6574\u521d\u59cb\u566a\u58f0\u5206\u5e03\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u63a7\u5236\u968f\u673a\u952f\u9f7f\u91c7\u6837\uff08Ctrl-Z\u91c7\u6837\uff09\u7684\u65b0\u7b56\u7565\uff0c\u65e8\u5728\u6761\u4ef6\u751f\u6210\u4e2d\u68c0\u6d4b\u5e76\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u6f5c\u5728\u5c40\u90e8\u6700\u4f18\uff0c\u968f\u540e\u6ce8\u5165\u566a\u58f0\u5e76\u56de\u9000\u5230\u66f4\u65e9\u7684\u566a\u58f0\u72b6\u6001\u4ee5\u9003\u79bb\u5f53\u524d\u4f18\u5316\u5e73\u53f0\u3002\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\uff0c\u4ec5\u63a5\u53d7\u6539\u8fdb\u7684\u8def\u5f84\uff0c\u800c\u9010\u6b65\u52a0\u6df1\u7684\u56de\u9000\u53ef\u5728\u9644\u8fd1\u66ff\u4ee3\u65b9\u6848\u5931\u8d25\u65f6\u5b9e\u73b0\u66f4\u5f3a\u7684\u9003\u79bb\u3002\u8fd9\u79cd\u63a7\u5236\u968f\u673a\u952f\u9f7f\u8fc7\u7a0b\u5141\u8bb8\u52a8\u6001\u4ea4\u66ff\u524d\u5411\u4f18\u5316\u548c\u540e\u5411\u63a2\u7d22\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u6761\u4ef6\u5bf9\u9f50\u3002\u6240\u63d0\u51fa\u7684Ctrl-Z\u91c7\u6837\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u517c\u5bb9\u73b0\u6709\u6269\u6563\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCtrl-Z\u91c7\u6837\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4ec5\u9700\u7ea67.6\u500d\u7684\u51fd\u6570\u8bc4\u4f30\u5f00\u9500\u3002"}}
{"id": "2506.20600", "pdf": "https://arxiv.org/pdf/2506.20600", "abs": "https://arxiv.org/abs/2506.20600", "authors": ["Wengxi Li", "Roy Pea", "Nick Haber", "Hari Subramonyam"], "title": "CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video", "categories": ["cs.AI"], "comment": null, "summary": "We introduce CogGen, a learner-centered AI architecture that transforms\nprogramming videos into interactive, adaptive learning experiences by\nintegrating student modeling with generative AI tutoring based on the Cognitive\nApprenticeship framework. The architecture consists of three components: (1)\nvideo segmentation by learning goals, (2) a conversational tutoring engine\napplying Cognitive Apprenticeship strategies, and (3) a student model using\nBayesian Knowledge Tracing to adapt instruction. Our technical evaluation\ndemonstrates effective video segmentation accuracy and strong pedagogical\nalignment across knowledge, method, action, and interaction layers. Ablation\nstudies confirm the necessity of each component in generating effective\nguidance. This work advances AI-powered tutoring by bridging structured student\nmodeling with interactive AI conversations, offering a scalable approach to\nenhancing video-based programming education.", "AI": {"tldr": "CogGen\u662f\u4e00\u79cd\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u5f0fAI\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u7f16\u7a0b\u89c6\u9891\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u3001\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u7ed3\u5408\u5b66\u751f\u5efa\u6a21\u4e0e\u751f\u6210\u5f0fAI\u8f85\u5bfc\uff0c\u57fa\u4e8e\u8ba4\u77e5\u5b66\u5f92\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u7f16\u7a0b\u89c6\u9891\u6559\u5b66\u7f3a\u4e4f\u4e92\u52a8\u6027\u548c\u4e2a\u6027\u5316\u6307\u5bfc\uff0cCogGen\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u63d0\u5347\u89c6\u9891\u6559\u80b2\u7684\u4ea4\u4e92\u6027\u548c\u9002\u5e94\u6027\uff0c\u6ee1\u8db3\u5b66\u4e60\u8005\u7684\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "CogGen\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u6309\u5b66\u4e60\u76ee\u6807\u5206\u5272\u89c6\u9891\uff0c(2) \u57fa\u4e8e\u8ba4\u77e5\u5b66\u5f92\u7b56\u7565\u7684\u5bf9\u8bdd\u5f0f\u8f85\u5bfc\u5f15\u64ce\uff0c(3) \u4f7f\u7528\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\u7684\u5b66\u751f\u6a21\u578b\u4ee5\u9002\u914d\u6559\u5b66\u3002", "result": "\u6280\u672f\u8bc4\u4f30\u663e\u793a\u89c6\u9891\u5206\u5272\u51c6\u786e\u4e14\u6559\u5b66\u7b56\u7565\u5728\u77e5\u8bc6\u3001\u65b9\u6cd5\u3001\u884c\u52a8\u548c\u4ea4\u4e92\u5c42\u9762\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u751f\u6210\u6709\u6548\u6307\u5bfc\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "CogGen\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u5b66\u751f\u5efa\u6a21\u4e0e\u4ea4\u4e92\u5f0fAI\u5bf9\u8bdd\uff0c\u4e3a\u89c6\u9891\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86AI\u8f85\u52a9\u6559\u5b66\u7684\u8fdb\u6b65\u3002", "paper_title_zh": "CogGen\uff1a\u4e00\u79cd\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u5f0fAI\u67b6\u6784\uff0c\u7528\u4e8e\u7f16\u7a0b\u89c6\u9891\u7684\u667a\u80fd\u8f85\u5bfc", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86CogGen\uff0c\u4e00\u79cd\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684AI\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u5b66\u751f\u5efa\u6a21\u4e0e\u57fa\u4e8e\u8ba4\u77e5\u5b66\u5f92\u6846\u67b6\u7684\u751f\u6210\u5f0fAI\u8f85\u5bfc\u76f8\u7ed3\u5408\uff0c\u5c06\u7f16\u7a0b\u89c6\u9891\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u3001\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u4f53\u9a8c\u3002\u8be5\u67b6\u6784\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1) \u6309\u5b66\u4e60\u76ee\u6807\u5206\u5272\u89c6\u9891\uff0c(2) \u5e94\u7528\u8ba4\u77e5\u5b66\u5f92\u7b56\u7565\u7684\u5bf9\u8bdd\u5f0f\u8f85\u5bfc\u5f15\u64ce\uff0c(3) \u4f7f\u7528\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\u7684\u5b66\u751f\u6a21\u578b\u4ee5\u9002\u914d\u6559\u5b66\u3002\u6280\u672f\u8bc4\u4f30\u8868\u660e\uff0c\u89c6\u9891\u5206\u5272\u51c6\u786e\u4e14\u6559\u5b66\u7b56\u7565\u5728\u77e5\u8bc6\u3001\u65b9\u6cd5\u3001\u884c\u52a8\u548c\u4ea4\u4e92\u5c42\u9762\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u751f\u6210\u6709\u6548\u6307\u5bfc\u7684\u5fc5\u8981\u6027\u3002\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u5b66\u751f\u5efa\u6a21\u4e0e\u4ea4\u4e92\u5f0fAI\u5bf9\u8bdd\uff0c\u4e3a\u89c6\u9891\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86AI\u8f85\u52a9\u6559\u5b66\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.20430", "pdf": "https://arxiv.org/pdf/2506.20430", "abs": "https://arxiv.org/abs/2506.20430", "authors": ["Weike Zhao", "Chaoyi Wu", "Yanjie Fan", "Xiaoman Zhang", "Pengcheng Qiu", "Yuze Sun", "Xiao Zhou", "Yanfeng Wang", "Ya Zhang", "Yongguo Yu", "Kun Sun", "Weidi Xie"], "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.", "AI": {"tldr": "DeepRare\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f55\u89c1\u75c5\u8bca\u65ad\u7cfb\u7edf\uff0c\u901a\u8fc7\u900f\u660e\u63a8\u7406\u94fe\u751f\u6210\u8bca\u65ad\u5047\u8bbe\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u6a21\u6001\u8f93\u5165\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7f55\u89c1\u75c5\u8bca\u65ad\u9762\u4e34\u4e34\u5e8a\u5f02\u8d28\u6027\u3001\u4f4e\u6d41\u884c\u7387\u548c\u533b\u751f\u77e5\u8bc6\u6709\u9650\u7684\u6311\u6218\u3002DeepRare\u65e8\u5728\u901a\u8fc7\u900f\u660e\u63a8\u7406\u94fe\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u4f9b\u51c6\u786e\u3001\u53ef\u8ffd\u6eaf\u7684\u8bca\u65ad\u3002", "method": "DeepRare\u7531\u6838\u5fc3\u4e3b\u673a\u3001\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u548c\u591a\u4e2a\u4e13\u4e1a\u4ee3\u7406\u670d\u52a1\u5668\u7ec4\u6210\uff0c\u6574\u540840\u591a\u79cd\u5de5\u5177\u548c\u6700\u65b0\u533b\u5b66\u77e5\u8bc6\uff0c\u652f\u6301\u590d\u6742\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u57288\u4e2a\u6570\u636e\u96c6\u4e2d\uff0cDeepRare\u5bf92,919\u79cd\u75be\u75c5\u8bca\u65ad\u51c6\u786e\u7387\u8fbe100%\uff081,013\u79cd\u75be\u75c5\uff09\uff0cHPO\u8bc4\u4f30\u4e2dRecall@1\u4e3a57.18%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u591a\u6a21\u6001\u8f93\u5165\u573a\u666f\u4e0bRecall@1\u8fbe70.60%\u3002", "conclusion": "DeepRare\u901a\u8fc7\u900f\u660e\u63a8\u7406\u94fe\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u7f55\u89c1\u75c5\u8bca\u65ad\u6027\u80fd\uff0c\u5df2\u5b9e\u73b0\u4e3a\u6613\u7528\u7684\u7f51\u7edc\u5e94\u7528\u3002", "paper_title_zh": "\u4e00\u79cd\u57fa\u4e8e\u53ef\u8ffd\u6eaf\u63a8\u7406\u7684\u7f55\u89c1\u75c5\u8bca\u65ad\u4ee3\u7406\u7cfb\u7edf", "abstract_zh": "\u7f55\u89c1\u75c5\u5168\u7403\u5f71\u54cd\u8d85\u8fc73\u4ebf\u4eba\uff0c\u4f46\u53ca\u65f6\u51c6\u786e\u8bca\u65ad\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51faDeepRare\uff0c\u9996\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f55\u89c1\u75c5\u8bca\u65ad\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u5904\u7406\u5f02\u8d28\u4e34\u5e8a\u8f93\u5165\u5e76\u751f\u6210\u900f\u660e\u63a8\u7406\u94fe\u7684\u8bca\u65ad\u5047\u8bbe\u3002DeepRare\u5305\u542b\u6838\u5fc3\u4e3b\u673a\u3001\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u548c\u591a\u4e2a\u4e13\u4e1a\u4ee3\u7406\u670d\u52a1\u5668\uff0c\u6574\u540840\u591a\u79cd\u5de5\u5177\u548c\u6700\u65b0\u533b\u5b66\u77e5\u8bc6\u3002\u57288\u4e2a\u6570\u636e\u96c6\u4e2d\uff0cDeepRare\u5bf92,919\u79cd\u75be\u75c5\u8bca\u65ad\u51c6\u786e\u7387\u8fbe100%\uff081,013\u79cd\u75be\u75c5\uff09\uff0cHPO\u8bc4\u4f30\u4e2dRecall@1\u4e3a57.18%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u591a\u6a21\u6001\u8f93\u5165\u573a\u666f\u4e0bRecall@1\u8fbe70.60%\u3002\u4e34\u5e8a\u4e13\u5bb6\u5bf9\u63a8\u7406\u94fe\u7684\u9a8c\u8bc1\u4e00\u81f4\u6027\u8fbe95.40%\u3002DeepRare\u5df2\u5b9e\u73b0\u4e3a\u7f51\u7edc\u5e94\u7528http://raredx.cn/doctor\u3002"}}
{"id": "2506.20302", "pdf": "https://arxiv.org/pdf/2506.20302", "abs": "https://arxiv.org/abs/2506.20302", "authors": ["Abbas Anwar", "Mohammad Shullar", "Ali Arshad Nasir", "Mudassir Masood", "Saeed Anwar"], "title": "TDiR: Transformer based Diffusion for Image Restoration Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Images captured in challenging environments often experience various forms of\ndegradation, including noise, color cast, blur, and light scattering. These\neffects significantly reduce image quality, hindering their applicability in\ndownstream tasks such as object detection, mapping, and classification. Our\ntransformer-based diffusion model was developed to address image restoration\ntasks, aiming to improve the quality of degraded images. This model was\nevaluated against existing deep learning methodologies across multiple quality\nmetrics for underwater image enhancement, denoising, and deraining on publicly\navailable datasets. Our findings demonstrate that the diffusion model, combined\nwith transformers, surpasses current methods in performance. The results of our\nmodel highlight the efficacy of diffusion models and transformers in improving\nthe quality of degraded images, consequently expanding their utility in\ndownstream tasks that require high-fidelity visual data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u548c\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5TDiR\uff0c\u65e8\u5728\u63d0\u5347\u9000\u5316\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u6355\u83b7\u7684\u56fe\u50cf\u5e38\u56e0\u566a\u58f0\u3001\u8272\u504f\u3001\u6a21\u7cca\u548c\u5149\u6563\u5c04\u7b49\u95ee\u9898\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u5176\u5728\u76ee\u6807\u68c0\u6d4b\u3001\u5730\u56fe\u7ed8\u5236\u548c\u5206\u7c7b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u548c\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5TDiR\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5e76\u5229\u7528Transformer\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u6709\u6548\u5904\u7406\u591a\u79cd\u9000\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTDiR\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff08\u5305\u62ec\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u3001\u53bb\u566a\u548c\u53bb\u96e8\u4efb\u52a1\uff09\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9000\u5316\u56fe\u50cf\u7684\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u7ed3\u5408Transformer\u7684\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u4ece\u800c\u6269\u5c55\u5176\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u89c6\u89c9\u6570\u636e\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "paper_title_zh": "TDiR\uff1a\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u4efb\u52a1", "abstract_zh": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u6355\u83b7\u7684\u56fe\u50cf\u5e38\u56e0\u566a\u58f0\u3001\u8272\u504f\u3001\u6a21\u7cca\u548c\u5149\u6563\u5c04\u7b49\u95ee\u9898\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u5176\u5728\u76ee\u6807\u68c0\u6d4b\u3001\u5730\u56fe\u7ed8\u5236\u548c\u5206\u7c7b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff0c\u65e8\u5728\u63d0\u5347\u9000\u5316\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9488\u5bf9\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u3001\u53bb\u566a\u548c\u53bb\u96e8\u4efb\u52a1\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408Transformer\u7684\u6269\u6563\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u672c\u6587\u7684\u6210\u679c\u51f8\u663e\u4e86\u6269\u6563\u6a21\u578b\u548cTransformer\u5728\u63d0\u5347\u9000\u5316\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4ece\u800c\u6269\u5c55\u4e86\u5176\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u89c6\u89c9\u6570\u636e\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.20608", "pdf": "https://arxiv.org/pdf/2506.20608", "abs": "https://arxiv.org/abs/2506.20608", "authors": ["Barry Smith", "Junchao Zhang", "Hong Zhang", "Lois Curfman McInnes", "Murat Keceli", "Archit Vasan", "Satish Balay", "Toby Isaac", "Le Chen", "Venkatram Vishwanath"], "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base", "categories": ["cs.AI", "cs.NA", "math.NA"], "comment": null, "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5229\u7528\u751f\u6210\u5f0fAI\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578bLLMs\uff09\u6fc0\u6d3b\u548c\u5229\u7528PETSc\uff08\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u5e93\uff09\u5206\u6563\u77e5\u8bc6\u5e93\u7684\u521d\u6b65\u5c1d\u8bd5\uff0c\u901a\u8fc7RAG\u3001\u91cd\u6392\u5e8f\u7b97\u6cd5\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u5de5\u5177\uff0c\u63d0\u5347\u7528\u6237\u548c\u5f00\u53d1\u8005\u7684\u77e5\u8bc6\u83b7\u53d6\u6548\u7387\uff0c\u5e76\u4f18\u5316\u6587\u6863\u66f4\u65b0\u3002", "motivation": "PETSc\u4f5c\u4e3a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u5e93\uff0c\u5176\u77e5\u8bc6\u5e93\uff08\u5305\u62ec\u6e90\u4ee3\u7801\u3001\u6587\u6863\u3001\u90ae\u4ef6\u5217\u8868\u7b49\uff09\u5206\u6563\u4e14\u975e\u6b63\u5f0f\uff0c\u96be\u4ee5\u88ab\u7528\u6237\u548c\u65b0\u5f00\u53d1\u8005\u6709\u6548\u5229\u7528\u3002\u56e0\u6b64\uff0c\u56e2\u961f\u5e0c\u671b\u901a\u8fc7LLM\u6280\u672f\u6574\u5408\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u63d0\u5347\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u56e2\u961f\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u3001\u91cd\u6392\u5e8f\u7b97\u6cd5\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u5de5\u5177\uff0c\u4ecePETSc\u7684\u591a\u6837\u5316\u5185\u5bb9\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u754c\u9762\u8bbe\u8ba1\u652f\u6301\u5f00\u53d1\u548c\u6587\u6863\u66f4\u65b0\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347PETSc\u77e5\u8bc6\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5c24\u5176\u662f\u5728\u53ef\u6269\u5c55Krylov\u6c42\u89e3\u5668\u65b9\u9762\uff0c\u540c\u65f6\u4e3a\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684\u77e5\u8bc6\u4e2d\u5fc3\u5316AI\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u4e3a\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684\u77e5\u8bc6\u4e2d\u5fc3\u5316AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u8ba1\u5212\u6269\u5c55\u4e3a\u66f4\u5f3a\u5927\u7684\u5e73\u53f0\uff0c\u4ee5\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "paper_title_zh": "AI\u52a9\u624b\u589e\u5f3a\u4e0e\u5229\u7528PETSc\u77e5\u8bc6\u5e93", "abstract_zh": "\u751f\u6210\u5f0fAI\uff0c\u5c24\u5176\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6b63\u5728\u6539\u53d8\u6280\u672f\u77e5\u8bc6\u7684\u83b7\u53d6\u3001\u91cd\u7528\u548c\u6269\u5c55\u65b9\u5f0f\u3002PETSc\u4f5c\u4e3a\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u5e93\uff0c\u5728\u5176\u4e09\u5341\u5e74\u7684\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u4e86\u4e30\u5bcc\u4f46\u5206\u6563\u7684\u77e5\u8bc6\u5e93\uff0c\u6db5\u76d6\u6e90\u4ee3\u7801\u3001\u6587\u6863\u3001\u90ae\u4ef6\u5217\u8868\u3001GitLab\u95ee\u9898\u3001Discord\u5bf9\u8bdd\u3001\u6280\u672f\u8bba\u6587\u7b49\u3002\u8fd9\u4e9b\u77e5\u8bc6\u5927\u591a\u662f\u975e\u6b63\u5f0f\u7684\uff0c\u96be\u4ee5\u88ab\u7528\u6237\u548c\u65b0\u5f00\u53d1\u8005\u8bbf\u95ee\u3002\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u6fc0\u6d3b\u548c\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\uff0cPETSc\u56e2\u961f\u5f00\u59cb\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u5c06PETSc\u5185\u5bb9\u4e0e\u81ea\u5b9a\u4e49LLM\u5de5\u5177\uff08\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u91cd\u6392\u5e8f\u7b97\u6cd5\u548c\u804a\u5929\u673a\u5668\u4eba\uff09\u7ed3\u5408\uff0c\u4ee5\u8f85\u52a9\u7528\u6237\u3001\u652f\u6301\u5f00\u53d1\u8005\u5e76\u63d0\u8bae\u66f4\u65b0\u6b63\u5f0f\u6587\u6863\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u8bbe\u8ba1\u548c\u8bc4\u4f30\u8fd9\u4e9b\u5de5\u5177\u7684\u521d\u6b65\u7ecf\u9a8c\uff0c\u91cd\u70b9\u5173\u6ce8\u7cfb\u7edf\u67b6\u6784\u3001\u4f7f\u7528RAG\u548c\u91cd\u6392\u5e8f\u5904\u7406PETSc\u7279\u5b9a\u4fe1\u606f\u3001\u8bc4\u4f30\u4e0d\u540cLLM\u548c\u5d4c\u5165\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7528\u6237\u754c\u9762\u8bbe\u8ba1\u3002\u5229\u7528\u963f\u8d21\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u9886\u5bfc\u8ba1\u7b97\u8bbe\u65bd\u8d44\u6e90\uff0c\u6211\u4eec\u5206\u6790\u4e86LLM\u54cd\u5e94\u5982\u4f55\u63d0\u5347\u6570\u503c\u8f6f\u4ef6\u7684\u5f00\u53d1\u548c\u4f7f\u7528\uff0c\u521d\u6b65\u805a\u7126\u4e8e\u53ef\u6269\u5c55Krylov\u6c42\u89e3\u5668\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u79d1\u5b66\u8f6f\u4ef6\u77e5\u8bc6\u4e2d\u5fc3\u5316AI\u6846\u67b6\uff0c\u652f\u6301\u89c4\u6a21\u5316\u652f\u6301\u3001\u4e30\u5bcc\u6587\u6863\u5e76\u4f18\u5316\u7814\u53d1\u5de5\u4f5c\u6d41\u7a0b\u3002\u6700\u540e\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u5c06\u8be5\u7cfb\u7edf\u6269\u5c55\u4e3a\u4e00\u4e2a\u5f3a\u5927\u3001\u6301\u7eed\u6f14\u8fdb\u7684\u5e73\u53f0\u7684\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2506.20471", "pdf": "https://arxiv.org/pdf/2506.20471", "abs": "https://arxiv.org/abs/2506.20471", "authors": ["Ujwal Narayan", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "title": "Probing AI Safety with Source Code", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728AI\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4ee3\u7801\u601d\u7ef4\u201d\uff08CoDoT\uff09\u7684\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4ee3\u7801\u6765\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027\uff0c\u7ed3\u679c\u663e\u793a\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u5728\u6bd2\u6027\u8f93\u51fa\u4e0a\u663e\u8457\u589e\u52a0\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u504f\u597d\u4e00\u81f4\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u7136\u800c\uff0c\u5f53\u524d\u6a21\u578b\u5728\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u53d7\u5230\u4f24\u5bb3\u3002", "method": "\u63d0\u51fa\u201c\u4ee3\u7801\u601d\u7ef4\u201d\uff08CoDoT\uff09\u7b56\u7565\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u8f6c\u6362\u4e3a\u4ee3\u7801\u5f62\u5f0f\u4ee5\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027\u3002\u4f8b\u5982\uff0c\u5c06\u201c\u4f7f\u4ee5\u4e0b\u6587\u672c\u66f4\u5177\u6bd2\u6027\u201d\u8f6c\u6362\u4e3a\u201cmake_more_toxic({text})\u201d\u3002", "result": "CoDoT\u5bfc\u81f4\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u5728\u6bd2\u6027\u8f93\u51fa\u4e0a\u663e\u8457\u589e\u52a0\uff1aGPT-4 Turbo\u6bd2\u6027\u589e\u52a016.5\u500d\uff0cDeepSeek R1\u5931\u8d25\u7387100%\uff0c\u4e03\u79cd\u73b0\u4ee3\u6a21\u578b\u5e73\u5747\u6bd2\u6027\u589e\u52a0300%\u3002\u9012\u5f52\u5e94\u7528CoDoT\u53ef\u8fdb\u4e00\u6b65\u4f7f\u6bd2\u6027\u7ffb\u500d\u3002", "conclusion": "CoDoT\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u5b89\u5168\u6027\u4e0a\u7684\u4e25\u91cd\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u4ece\u57fa\u672c\u539f\u5219\u51fa\u53d1\u8bc4\u4f30\u5b89\u5168\u6027\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u4e0e\u80fd\u529b\u540c\u6b65\u63d0\u5347\u3002", "paper_title_zh": "\u901a\u8fc7\u6e90\u4ee3\u7801\u63a2\u7a76AI\u5b89\u5168\u6027", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\uff0c\u8fd9\u8981\u6c42\u63d0\u5347\u5176\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5fc5\u987b\u52a0\u5f3a\u5b89\u5168\u6027\u63aa\u65bd\u4ee5\u786e\u4fdd\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u504f\u597d\u4e00\u81f4\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728AI\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u906d\u53d7\u4e0d\u5b89\u5168\u751a\u81f3\u6709\u5bb3\u7684\u4f53\u9a8c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4ee3\u7801\u601d\u7ef4\u201d\uff08CoDoT\uff09\u7684\u63d0\u793a\u7b56\u7565\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u7684\u5b89\u5168\u6027\u3002CoDoT\u5c06\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u8f6c\u6362\u4e3a\u8868\u8fbe\u76f8\u540c\u610f\u56fe\u7684\u7b80\u5355\u4ee3\u7801\uff0c\u4f8b\u5982\u5c06\u201c\u4f7f\u4ee5\u4e0b\u6587\u672c\u66f4\u5177\u6bd2\u6027\uff1a{text}\u201d\u8f6c\u6362\u4e3a\u201cmake_more_toxic({text})\u201d\u3002\u7ed3\u679c\u663e\u793a\uff0cCoDoT\u5bfc\u81f4\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u5728\u5b89\u5168\u6027\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u5931\u8d25\uff1aGPT-4 Turbo\u7684\u6bd2\u6027\u8f93\u51fa\u589e\u52a016.5\u500d\uff0cDeepSeek R1\u7684\u5931\u8d25\u7387\u8fbe100%\uff0c\u4e03\u79cd\u73b0\u4ee3\u6a21\u578b\u5e73\u5747\u6bd2\u6027\u589e\u52a0300%\u3002\u6b64\u5916\uff0c\u9012\u5f52\u5e94\u7528CoDoT\u53ef\u8fdb\u4e00\u6b65\u4f7f\u6bd2\u6027\u7ffb\u500d\u3002\u9274\u4e8eLLMs\u7684\u5feb\u901f\u666e\u53ca\uff0cCoDoT\u51f8\u663e\u4e86\u4ece\u57fa\u672c\u539f\u5219\u51fa\u53d1\u8bc4\u4f30\u5b89\u5168\u6027\u7684\u7d27\u8feb\u6027\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u4e0e\u80fd\u529b\u540c\u6b65\u53d1\u5c55\u3002"}}
{"id": "2506.20306", "pdf": "https://arxiv.org/pdf/2506.20306", "abs": "https://arxiv.org/abs/2506.20306", "authors": ["Yaxi Chen", "Simin Ni", "Shaheer U. Saeed", "Aleksandra Ivanova", "Rikin Hargunani", "Jie Huang", "Chaozong Liu", "Yipeng Hu"], "title": "Radiomic fingerprints for knee MR images assessment", "categories": ["cs.CV"], "comment": null, "summary": "Accurate interpretation of knee MRI scans relies on expert clinical judgment,\noften with high variability and limited scalability. Existing radiomic\napproaches use a fixed set of radiomic features (the signature), selected at\nthe population level and applied uniformly to all patients. While\ninterpretable, these signatures are often too constrained to represent\nindividual pathological variations. As a result, conventional radiomic-based\napproaches are found to be limited in performance, compared with recent\nend-to-end deep learning (DL) alternatives without using interpretable radiomic\nfeatures. We argue that the individual-agnostic nature in current radiomic\nselection is not central to its intepretability, but is responsible for the\npoor generalization in our application. Here, we propose a novel radiomic\nfingerprint framework, in which a radiomic feature set (the fingerprint) is\ndynamically constructed for each patient, selected by a DL model. Unlike the\nexisting radiomic signatures, our fingerprints are derived on a per-patient\nbasis by predicting the feature relevance in a large radiomic feature pool, and\nselecting only those that are predictive of clinical conditions for individual\npatients. The radiomic-selecting model is trained simultaneously with a\nlow-dimensional (considered relatively explainable) logistic regression for\ndownstream classification. We validate our methods across multiple diagnostic\ntasks including general knee abnormalities, anterior cruciate ligament (ACL)\ntears, and meniscus tears, demonstrating comparable or superior diagnostic\naccuracy relative to state-of-the-art end-to-end DL models. More importantly,\nwe show that the interpretability inherent in our approach facilitates\nmeaningful clinical insights and potential biomarker discovery, with detailed\ndiscussion, quantitative and qualitative analysis of real-world clinical cases\nto evidence these advantages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u653e\u5c04\u7ec4\u5b66\u6307\u7eb9\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e3a\u6bcf\u4f4d\u60a3\u8005\u52a8\u6001\u6784\u5efa\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u96c6\uff0c\u663e\u8457\u63d0\u5347\u819d\u5173\u8282MRI\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u653e\u5c04\u7ec4\u5b66\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u7279\u5f81\u96c6\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u4e2a\u4f53\u75c5\u7406\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u9009\u62e9\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u653e\u5c04\u7ec4\u5b66\u6307\u7eb9\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u5927\u91cf\u7279\u5f81\u4e2d\u52a8\u6001\u9009\u62e9\u4e0e\u4e2a\u4f53\u60a3\u8005\u4e34\u5e8a\u6761\u4ef6\u76f8\u5173\u7684\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4f4e\u7ef4\u903b\u8f91\u56de\u5f52\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u819d\u5173\u8282\u5f02\u5e38\u3001\u524d\u4ea4\u53c9\u97e7\u5e26\u6495\u88c2\u548c\u534a\u6708\u677f\u6495\u88c2\u7b49\u8bca\u65ad\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u653e\u5c04\u7ec4\u5b66\u6307\u7eb9\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\u548c\u89e3\u91ca\u6027\u5206\u6790\u3002", "paper_title_zh": "\u819d\u5173\u8282MRI\u56fe\u50cf\u7684\u653e\u5c04\u7ec4\u5b66\u6307\u7eb9\u8bc4\u4f30", "abstract_zh": "\u819d\u5173\u8282MRI\u626b\u63cf\u7684\u51c6\u786e\u89e3\u8bfb\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u4e34\u5e8a\u5224\u65ad\uff0c\u4f46\u5b58\u5728\u9ad8\u53d8\u5f02\u6027\u548c\u6709\u9650\u7684\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u653e\u5c04\u7ec4\u5b66\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u7279\u5f81\u96c6\uff08\u7b7e\u540d\uff09\uff0c\u5728\u7fa4\u4f53\u6c34\u5e73\u9009\u62e9\u5e76\u7edf\u4e00\u5e94\u7528\u4e8e\u6240\u6709\u60a3\u8005\u3002\u5c3d\u7ba1\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u4f46\u8fd9\u4e9b\u7b7e\u540d\u901a\u5e38\u8fc7\u4e8e\u53d7\u9650\uff0c\u65e0\u6cd5\u4ee3\u8868\u4e2a\u4f53\u75c5\u7406\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u4f20\u7edf\u7684\u653e\u5c04\u7ec4\u5b66\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u800c\u8fd1\u671f\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5219\u65e0\u9700\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u5f53\u524d\u653e\u5c04\u7ec4\u5b66\u9009\u62e9\u4e2d\u7684\u4e2a\u4f53\u65e0\u5173\u6027\u5e76\u975e\u5176\u53ef\u89e3\u91ca\u6027\u7684\u6838\u5fc3\uff0c\u800c\u662f\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u539f\u56e0\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u653e\u5c04\u7ec4\u5b66\u6307\u7eb9\u6846\u67b6\uff0c\u5176\u4e2d\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u96c6\uff08\u6307\u7eb9\uff09\u7531\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e3a\u6bcf\u4f4d\u60a3\u8005\u52a8\u6001\u6784\u5efa\u3002\u4e0e\u73b0\u6709\u7b7e\u540d\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u6307\u7eb9\u901a\u8fc7\u9884\u6d4b\u5927\u578b\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u6c60\u4e2d\u7279\u5f81\u7684\u76f8\u5173\u6027\uff0c\u5e76\u4ec5\u9009\u62e9\u5bf9\u4e2a\u4f53\u60a3\u8005\u4e34\u5e8a\u6761\u4ef6\u6709\u9884\u6d4b\u6027\u7684\u7279\u5f81\u3002\u653e\u5c04\u7ec4\u5b66\u9009\u62e9\u6a21\u578b\u4e0e\u4f4e\u7ef4\uff08\u76f8\u5bf9\u53ef\u89e3\u91ca\u7684\uff09\u903b\u8f91\u56de\u5f52\u540c\u65f6\u8bad\u7ec3\uff0c\u7528\u4e8e\u4e0b\u6e38\u5206\u7c7b\u3002\u6211\u4eec\u5728\u591a\u9879\u8bca\u65ad\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5305\u62ec\u4e00\u822c\u819d\u5173\u8282\u5f02\u5e38\u3001\u524d\u4ea4\u53c9\u97e7\u5e26\u6495\u88c2\u548c\u534a\u6708\u677f\u6495\u88c2\uff0c\u7ed3\u679c\u663e\u793a\u5176\u8bca\u65ad\u51c6\u786e\u6027\u4e0e\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u56fa\u6709\u7684\u53ef\u89e3\u91ca\u6027\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u89c1\u89e3\u548c\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e34\u5e8a\u6848\u4f8b\u7684\u8be6\u7ec6\u8ba8\u8bba\u3001\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8bc1\u660e\u4e86\u8fd9\u4e9b\u4f18\u52bf\u3002"}}
{"id": "2506.20640", "pdf": "https://arxiv.org/pdf/2506.20640", "abs": "https://arxiv.org/abs/2506.20640", "authors": ["Sijie Li", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "title": "Towards Community-Driven Agents for Machine Learning Engineering", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language model-based machine learning (ML) agents have shown great\npromise in automating ML research. However, existing agents typically operate\nin isolation on a given research problem, without engaging with the broader\nresearch community, where human researchers often gain insights and contribute\nby sharing knowledge. To bridge this gap, we introduce MLE-Live, a live\nevaluation framework designed to assess an agent's ability to communicate with\nand leverage collective knowledge from a simulated Kaggle research community.\nBuilding on this framework, we propose CoMind, a novel agent that excels at\nexchanging insights and developing novel solutions within a community context.\nCoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%\nhuman competitors on average across four ongoing Kaggle competitions. Our code\nis released at https://github.com/comind-ml/CoMind.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793e\u533a\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u4ee3\u7406CoMind\uff0c\u901a\u8fc7MLE-Live\u6846\u67b6\u8bc4\u4f30\u5176\u5728\u6a21\u62dfKaggle\u793e\u533a\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e79.2%\u7684\u4eba\u7c7b\u7ade\u4e89\u5bf9\u624b\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u901a\u5e38\u5b64\u7acb\u8fd0\u884c\uff0c\u7f3a\u4e4f\u4e0e\u793e\u533a\u4e92\u52a8\uff0c\u800c\u4eba\u7c7b\u7814\u7a76\u8005\u901a\u8fc7\u5206\u4eab\u77e5\u8bc6\u83b7\u5f97\u7075\u611f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5f00\u53d1\u80fd\u591f\u4e0e\u793e\u533a\u4ea4\u6d41\u5e76\u5229\u7528\u96c6\u4f53\u77e5\u8bc6\u7684\u4ee3\u7406\u3002", "method": "\u63d0\u51faMLE-Live\u6846\u67b6\u8bc4\u4f30\u4ee3\u7406\u5728\u6a21\u62dfKaggle\u793e\u533a\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86CoMind\u4ee3\u7406\uff0c\u4e13\u6ce8\u4e8e\u5728\u793e\u533a\u73af\u5883\u4e2d\u4ea4\u6362\u89c1\u89e3\u548c\u5f00\u53d1\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "result": "CoMind\u5728MLE-Live\u6846\u67b6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u5728\u56db\u4e2aKaggle\u7ade\u8d5b\u4e2d\u51fb\u8d2579.2%\u7684\u4eba\u7c7b\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "CoMind\u5c55\u793a\u4e86\u793e\u533a\u9a71\u52a8\u4ee3\u7406\u5728\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u8fc8\u5411\u793e\u533a\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4ee3\u7406", "abstract_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u4ee3\u7406\u5728\u81ea\u52a8\u5316ML\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u4ee3\u7406\u901a\u5e38\u5b64\u7acb\u8fd0\u884c\u4e8e\u7279\u5b9a\u7814\u7a76\u95ee\u9898\uff0c\u672a\u4e0e\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u793e\u533a\u4e92\u52a8\uff0c\u800c\u4eba\u7c7b\u7814\u7a76\u8005\u5e38\u901a\u8fc7\u5206\u4eab\u77e5\u8bc6\u83b7\u5f97\u7075\u611f\u5e76\u505a\u51fa\u8d21\u732e\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MLE-Live\uff0c\u4e00\u4e2a\u5b9e\u65f6\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u8bc4\u4f30\u4ee3\u7406\u4e0e\u6a21\u62dfKaggle\u7814\u7a76\u793e\u533a\u4ea4\u6d41\u5e76\u5229\u7528\u96c6\u4f53\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CoMind\uff0c\u4e00\u79cd\u5728\u793e\u533a\u73af\u5883\u4e2d\u64c5\u957f\u4ea4\u6362\u89c1\u89e3\u548c\u5f00\u53d1\u65b0\u89e3\u51b3\u65b9\u6848\u7684\u65b0\u578b\u4ee3\u7406\u3002CoMind\u5728MLE-Live\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u5728\u56db\u4e2aKaggle\u7ade\u8d5b\u4e2d\u51fb\u8d2579.2%\u7684\u4eba\u7c7b\u7ade\u4e89\u5bf9\u624b\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/comind-ml/CoMind\u3002"}}
{"id": "2506.20474", "pdf": "https://arxiv.org/pdf/2506.20474", "abs": "https://arxiv.org/abs/2506.20474", "authors": ["Kaixiang Zhang", "Justine Zhang", "Cristian Danescu-Niculescu-Mizil"], "title": "Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations", "categories": ["cs.CL"], "comment": null, "summary": "An intrinsic aspect of every conversation is the way talk-time is shared\nbetween multiple speakers. Conversations can be balanced, with each speaker\nclaiming a similar amount of talk-time, or imbalanced when one talks\ndisproportionately. Such overall distributions are the consequence of\ncontinuous negotiations between the speakers throughout the conversation: who\nshould be talking at every point in time, and for how long?\n  In this work we introduce a computational framework for quantifying both the\nconversation-level distribution of talk-time between speakers, as well as the\nlower-level dynamics that lead to it. We derive a typology of talk-time sharing\ndynamics structured by several intuitive axes of variation. By applying this\nframework to a large dataset of video-chats between strangers, we confirm that,\nperhaps unsurprisingly, different conversation-level distributions of talk-time\nare perceived differently by speakers, with balanced conversations being\npreferred over imbalanced ones, especially by those who end up talking less.\nThen we reveal that -- even when they lead to the same level of overall balance\n-- different types of talk-time sharing dynamics are perceived differently by\nthe participants, highlighting the relevance of our newly introduced typology.\nFinally, we discuss how our framework offers new tools to designers of\ncomputer-mediated communication platforms, for both human-human and human-AI\ncommunication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u89c6\u9891\u804a\u5929\u4e2d\u5bf9\u8bdd\u65f6\u95f4\u7684\u5206\u914d\u53ca\u5176\u52a8\u6001\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65f6\u95f4\u5206\u914d\u52a8\u6001\u5bf9\u53c2\u4e0e\u8005\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u6c9f\u901a\u5e73\u53f0\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "motivation": "\u5bf9\u8bdd\u4e2d\u65f6\u95f4\u5206\u914d\u7684\u5e73\u8861\u4e0e\u5426\u76f4\u63a5\u5f71\u54cd\u53c2\u4e0e\u8005\u7684\u4f53\u9a8c\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u5206\u914d\u52a8\u6001\u53d8\u5316\u7684\u91cf\u5316\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8\u65f6\u95f4\u5206\u914d\u52a8\u6001\u5982\u4f55\u5f71\u54cd\u53c2\u4e0e\u8005\u7684\u611f\u77e5\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u4ece\u5bf9\u8bdd\u5c42\u9762\u548c\u52a8\u6001\u5c42\u9762\u91cf\u5316\u65f6\u95f4\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u964c\u751f\u4eba\u89c6\u9891\u804a\u5929\u7684\u5927\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u65f6\u95f4\u5206\u914d\u52a8\u6001\u5bf9\u53c2\u4e0e\u8005\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e73\u8861\u7684\u5bf9\u8bdd\u66f4\u53d7\u53c2\u4e0e\u8005\u6b22\u8fce\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u8bf4\u8bdd\u8f83\u5c11\u7684\u4eba\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u6574\u4f53\u5e73\u8861\u76f8\u540c\uff0c\u4e0d\u540c\u7684\u65f6\u95f4\u5206\u914d\u52a8\u6001\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u611f\u77e5\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u7406\u89e3\u5bf9\u8bdd\u65f6\u95f4\u5206\u914d\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u6c9f\u901a\u5e73\u53f0\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "\u65f6\u95f4\u7ad9\u5728\u6211\u8fd9\u8fb9\uff1a\u89c6\u9891\u804a\u5929\u4e2d\u5bf9\u8bdd\u65f6\u95f4\u5171\u4eab\u7684\u52a8\u6001", "abstract_zh": "\u5bf9\u8bdd\u7684\u4e00\u4e2a\u5185\u5728\u7279\u5f81\u662f\u8bf4\u8bdd\u8005\u4e4b\u95f4\u5982\u4f55\u5171\u4eab\u5bf9\u8bdd\u65f6\u95f4\u3002\u5bf9\u8bdd\u53ef\u4ee5\u662f\u5e73\u8861\u7684\uff0c\u6bcf\u4e2a\u8bf4\u8bdd\u8005\u5360\u7528\u76f8\u4f3c\u7684\u65f6\u95f4\uff1b\u4e5f\u53ef\u4ee5\u662f\u4e0d\u5e73\u8861\u7684\uff0c\u5176\u4e2d\u4e00\u65b9\u5360\u7528\u8fc7\u591a\u65f6\u95f4\u3002\u8fd9\u79cd\u6574\u4f53\u5206\u5e03\u662f\u8bf4\u8bdd\u8005\u5728\u5bf9\u8bdd\u4e2d\u6301\u7eed\u534f\u5546\u7684\u7ed3\u679c\uff1a\u8c01\u5e94\u8be5\u5728\u6bcf\u4e2a\u65f6\u95f4\u70b9\u8bf4\u8bdd\uff0c\u4ee5\u53ca\u8bf4\u591a\u4e45\uff1f\n  \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5bf9\u8bdd\u5c42\u9762\u7684\u65f6\u95f4\u5206\u914d\u53ca\u5176\u52a8\u6001\u53d8\u5316\u3002\u6211\u4eec\u901a\u8fc7\u51e0\u4e2a\u76f4\u89c2\u7684\u53d8\u5316\u8f74\u6784\u5efa\u4e86\u65f6\u95f4\u5171\u4eab\u52a8\u6001\u7684\u7c7b\u578b\u5b66\u3002\u901a\u8fc7\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u964c\u751f\u4eba\u89c6\u9891\u804a\u5929\u7684\u5927\u6570\u636e\u96c6\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5e73\u8861\u7684\u5bf9\u8bdd\u66f4\u53d7\u53c2\u4e0e\u8005\u6b22\u8fce\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u8bf4\u8bdd\u8f83\u5c11\u7684\u4eba\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u6574\u4f53\u5e73\u8861\u76f8\u540c\uff0c\u4e0d\u540c\u7684\u65f6\u95f4\u5171\u4eab\u52a8\u6001\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u611f\u77e5\u6548\u679c\uff0c\u51f8\u663e\u4e86\u6211\u4eec\u65b0\u5f15\u5165\u7684\u7c7b\u578b\u5b66\u7684\u76f8\u5173\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u8be5\u6846\u67b6\u5982\u4f55\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u6c9f\u901a\u5e73\u53f0\u7684\u8bbe\u8ba1\u8005\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u5305\u62ec\u4eba\u4e0e\u4eba\u53ca\u4eba\u4e0eAI\u7684\u6c9f\u901a\u3002"}}
{"id": "2506.20312", "pdf": "https://arxiv.org/pdf/2506.20312", "abs": "https://arxiv.org/abs/2506.20312", "authors": ["Jiong Wang"], "title": "On the Burstiness of Faces in Set", "categories": ["cs.CV"], "comment": "18 pages, 5 figures", "summary": "Burstiness, a phenomenon observed in text and image retrieval, refers to that\nparticular elements appear more times in a set than a statistically independent\nmodel assumes. We argue that in the context of set-based face recognition\n(SFR), burstiness exists widely and degrades the performance in two aspects:\nFirstly, the bursty faces, where faces with particular attributes %exist\nfrequently in a face set, dominate the training instances and dominate the\ntraining face sets and lead to poor generalization ability to unconstrained\nscenarios. Secondly, the bursty faces %dominating the evaluation sets interfere\nwith the similarity comparison in set verification and identification when\nevaluation. To detect the bursty faces in a set, we propose three strategies\nbased on Quickshift++, feature self-similarity, and generalized max-pooling\n(GMP). We apply the burst detection results on training and evaluation stages\nto enhance the sampling ratios or contributions of the infrequent faces. When\nevaluation, we additionally propose the quality-aware GMP that enables\nawareness of the face quality and robustness to the low-quality faces for the\noriginal GMP. We give illustrations and extensive experiments on the SFR\nbenchmarks to demonstrate that burstiness is widespread and suppressing\nburstiness considerably improves the recognition performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u96c6\u5408\u4e2d\u4eba\u8138\u8bc6\u522b\u4e2d\u7684\u201c\u7a81\u53d1\u6027\u201d\u73b0\u8c61\uff0c\u5373\u67d0\u4e9b\u7279\u5b9a\u5c5e\u6027\u7684\u4eba\u8138\u5728\u96c6\u5408\u4e2d\u9891\u7e41\u51fa\u73b0\uff0c\u5f71\u54cd\u8bad\u7ec3\u548c\u8bc4\u4f30\u6027\u80fd\u3002\u4f5c\u8005\u63d0\u51fa\u4e09\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u91c7\u6837\u7b56\u7565\u548c\u8d28\u91cf\u611f\u77e5\u673a\u5236\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5728\u57fa\u4e8e\u96c6\u5408\u7684\u4eba\u8138\u8bc6\u522b\uff08SFR\uff09\u4e2d\uff0c\u7a81\u53d1\u6027\u73b0\u8c61\u5e7f\u6cdb\u5b58\u5728\uff0c\u5bfc\u81f4\u8bad\u7ec3\u96c6\u548c\u8bc4\u4f30\u96c6\u88ab\u67d0\u4e9b\u9891\u7e41\u51fa\u73b0\u7684\u4eba\u8138\u4e3b\u5bfc\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bc4\u4f30\u51c6\u786e\u6027\u3002\u672c\u6587\u65e8\u5728\u68c0\u6d4b\u5e76\u6291\u5236\u8fd9\u79cd\u7a81\u53d1\u6027\u73b0\u8c61\uff0c\u4ee5\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e09\u79cd\u7b56\u7565\u68c0\u6d4b\u7a81\u53d1\u6027\u4eba\u8138\uff1a\u57fa\u4e8eQuickshift++\u7684\u65b9\u6cd5\u3001\u7279\u5f81\u81ea\u76f8\u4f3c\u6027\u65b9\u6cd5\u548c\u5e7f\u4e49\u6700\u5927\u6c60\u5316\uff08GMP\uff09\u3002\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u9636\u6bb5\uff0c\u5229\u7528\u68c0\u6d4b\u7ed3\u679c\u8c03\u6574\u4f4e\u9891\u4eba\u8138\u7684\u91c7\u6837\u6bd4\u4f8b\u6216\u8d21\u732e\u5ea6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u8d28\u91cf\u611f\u77e5GMP\uff0c\u589e\u5f3a\u5bf9\u4f4e\u8d28\u91cf\u4eba\u8138\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7a81\u53d1\u6027\u73b0\u8c61\u5728SFR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u901a\u8fc7\u6291\u5236\u7a81\u53d1\u6027\uff0c\u8bc6\u522b\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7a81\u53d1\u6027\u73b0\u8c61\u5bf9\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u6709\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u6291\u5236\u7a81\u53d1\u6027\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "paper_title_zh": "\u5173\u4e8e\u96c6\u5408\u4e2d\u4eba\u8138\u7684\u7a81\u53d1\u6027\u73b0\u8c61", "abstract_zh": "\u7a81\u53d1\u6027\u662f\u6307\u5728\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u4e2d\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\uff0c\u5373\u67d0\u4e9b\u7279\u5b9a\u5143\u7d20\u5728\u96c6\u5408\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\u6bd4\u7edf\u8ba1\u72ec\u7acb\u6a21\u578b\u5047\u8bbe\u7684\u66f4\u591a\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u5728\u57fa\u4e8e\u96c6\u5408\u7684\u4eba\u8138\u8bc6\u522b\uff08SFR\uff09\u4e2d\uff0c\u7a81\u53d1\u6027\u73b0\u8c61\u5e7f\u6cdb\u5b58\u5728\uff0c\u5e76\u5728\u4e24\u65b9\u9762\u964d\u4f4e\u6027\u80fd\uff1a\u9996\u5148\uff0c\u7a81\u53d1\u6027\u4eba\u8138\uff08\u5373\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u4eba\u8138\u5728\u96c6\u5408\u4e2d\u9891\u7e41\u51fa\u73b0\uff09\u4e3b\u5bfc\u8bad\u7ec3\u5b9e\u4f8b\u548c\u8bad\u7ec3\u96c6\uff0c\u5bfc\u81f4\u5bf9\u65e0\u7ea6\u675f\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff1b\u5176\u6b21\uff0c\u7a81\u53d1\u6027\u4eba\u8138\u5728\u8bc4\u4f30\u96c6\u4e2d\u5e72\u6270\u96c6\u5408\u9a8c\u8bc1\u548c\u8bc6\u522b\u65f6\u7684\u76f8\u4f3c\u6027\u6bd4\u8f83\u3002\u4e3a\u68c0\u6d4b\u96c6\u5408\u4e2d\u7684\u7a81\u53d1\u6027\u4eba\u8138\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8eQuickshift++\u3001\u7279\u5f81\u81ea\u76f8\u4f3c\u6027\u548c\u5e7f\u4e49\u6700\u5927\u6c60\u5316\uff08GMP\uff09\u7684\u4e09\u79cd\u7b56\u7565\u3002\u6211\u4eec\u5c06\u7a81\u53d1\u68c0\u6d4b\u7ed3\u679c\u5e94\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u9636\u6bb5\uff0c\u4ee5\u589e\u52a0\u4f4e\u9891\u4eba\u8138\u7684\u91c7\u6837\u6bd4\u4f8b\u6216\u8d21\u732e\u5ea6\u3002\u5728\u8bc4\u4f30\u65f6\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u8d28\u91cf\u611f\u77e5GMP\uff0c\u4f7f\u539f\u59cbGMP\u80fd\u591f\u611f\u77e5\u4eba\u8138\u8d28\u91cf\u5e76\u5bf9\u4f4e\u8d28\u91cf\u4eba\u8138\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728SFR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\u548c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u7a81\u53d1\u6027\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u6291\u5236\u7a81\u53d1\u6027\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2506.20664", "pdf": "https://arxiv.org/pdf/2506.20664", "abs": "https://arxiv.org/abs/2506.20664", "authors": ["Andrei Lupu", "Timon Willi", "Jakob Foerster"], "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA"], "comment": "41 pages, 19 figures", "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Decrypto\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u5907\u4ee3\u7406\u80fd\u529b\uff0c\u5b83\u4eec\u9700\u8981\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u548c\u5176\u4ed6\u667a\u80fd\u4f53\u4ea4\u4e92\u3002\u7136\u800c\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u5fc3\u667a\u7406\u8bba\u548c\u591a\u667a\u80fd\u4f53\u80fd\u529b\u8bc4\u4f30\u4e0a\u5b58\u5728\u8303\u56f4\u72ed\u7a84\u3001\u6570\u636e\u6cc4\u6f0f\u3001\u9971\u548c\u53ca\u7f3a\u4e4f\u4ea4\u4e92\u6027\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86Decrypto\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7075\u611f\u6765\u81ea\u8ba4\u77e5\u79d1\u5b66\u3001\u8ba1\u7b97\u8bed\u7528\u5b66\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3002\u8be5\u6d4b\u8bd5\u7b80\u5316\u4e86\u5176\u4ed6\u7ef4\u5ea6\uff0c\u6d88\u9664\u4e86\u5e38\u89c1\u5e72\u6270\u56e0\u7d20\uff0c\u5e76\u9996\u6b21\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u5fc3\u667a\u7406\u8bba\u5b9e\u9a8c\u5e73\u53f0\u3002\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u3001\u9c81\u68d2\u6027\u7814\u7a76\u548c\u4eba\u673a\u5bf9\u6297\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0cLLMs\u5728\u6e38\u620f\u80fd\u529b\u4e0a\u843d\u540e\u4e8e\u4eba\u7c7b\u548c\u7b80\u5355\u8bcd\u5d4c\u5165\u57fa\u7ebf\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u663e\u793a\uff0c\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u5728\u5173\u952e\u5fc3\u667a\u7406\u8bba\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5982\u65e7\u6a21\u578b\uff0c\u8868\u660eDecrypto\u586b\u8865\u4e86\u5f53\u524d\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "conclusion": "Decrypto\u4e3a\u591a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u5fc3\u667a\u7406\u8bba\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4ee3\u7406\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "paper_title_zh": "\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u5fc3\u667a\u7406\u8bba\u7684Decrypto\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u5907\u4ee3\u7406\u80fd\u529b\uff0c\u5b83\u4eec\u9700\u8981\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u548c\u5176\u4ed6\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u8fd9\u9700\u8981\u65b0\u7684\u63a8\u7406\u6280\u80fd\uff0c\u5c24\u5176\u662f\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\uff0c\u5373\u63a8\u7406\u5176\u4ed6\u667a\u80fd\u4f53\u201c\u5fc3\u7406\u201d\u72b6\u6001\u7684\u80fd\u529b\u3002\u7136\u800c\uff0cLLMs\u7684ToM\u548c\u591a\u667a\u80fd\u4f53\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u8303\u56f4\u72ed\u7a84\u3001\u6570\u636e\u6cc4\u6f0f\u3001\u9971\u548c\u53ca\u7f3a\u4e4f\u4ea4\u4e92\u6027\u7b49\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faDecrypto\uff0c\u4e00\u79cd\u57fa\u4e8e\u6e38\u620f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7075\u611f\u6765\u81ea\u8ba4\u77e5\u79d1\u5b66\u3001\u8ba1\u7b97\u8bed\u7528\u5b66\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3002\u5176\u8bbe\u8ba1\u5c3d\u53ef\u80fd\u7b80\u5316\u5176\u4ed6\u7ef4\u5ea6\uff0c\u6d88\u9664\u5e38\u89c1\u5e72\u6270\u56e0\u7d20\uff0c\u5e76\u9996\u6b21\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0fToM\u5b9e\u9a8c\u5e73\u53f0\u3002\n\n\u6211\u4eec\u901a\u8fc7\u5168\u9762\u5b9e\u8bc1\u8bc4\u4f30\u3001\u9c81\u68d2\u6027\u7814\u7a76\u548c\u4eba\u673a\u5bf9\u6297\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u57fa\u51c6\u8bbe\u8ba1\u3002\u7ed3\u679c\u663e\u793a\uff0cLLMs\u7684\u6e38\u620f\u80fd\u529b\u843d\u540e\u4e8e\u4eba\u7c7b\u548c\u7b80\u5355\u8bcd\u5d4c\u5165\u57fa\u7ebf\u6a21\u578b\u3002\u968f\u540e\uff0c\u6211\u4eec\u5728Decrypto\u4e2d\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7ecf\u5178\u8ba4\u77e5\u79d1\u5b66\u5b9e\u9a8c\u53d8\u4f53\uff0c\u8bc4\u4f30\u4e09\u79cd\u5173\u952eToM\u80fd\u529b\u3002\u51fa\u4e4e\u610f\u6599\u7684\u662f\uff0c\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u52a3\u4e8e\u65e7\u6a21\u578b\u3002\u8fd9\u8868\u660eDecrypto\u586b\u8865\u4e86\u5f53\u524d\u63a8\u7406\u548cToM\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u66f4\u4f18\u4eba\u5de5\u4ee3\u7406\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.20476", "pdf": "https://arxiv.org/pdf/2506.20476", "abs": "https://arxiv.org/abs/2506.20476", "authors": ["Tong Zhou"], "title": "Knowledge-Aware Diverse Reranking for Cross-Source Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG\ncompetition. The competition's evaluation set, automatically generated by\nDataMorgana from internet corpora, encompassed a wide range of target topics,\nquestion types, question formulations, audience types, and knowledge\norganization methods. It offered a fair evaluation of retrieving\nquestion-relevant supporting documents from a 15M documents subset of the\nFineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline\nachieved first place in the competition.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Team Marikarp\u5728SIGIR 2025 LiveRAG\u7ade\u8d5b\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u63d0\u51fa\u7684\u77e5\u8bc6\u611f\u77e5\u591a\u6837\u6027\u91cd\u6392\u5e8fRAG\u7ba1\u9053\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "motivation": "\u7ade\u8d5b\u8bc4\u4f30\u96c6\u7531DataMorgana\u4ece\u4e92\u8054\u7f51\u8bed\u6599\u5e93\u81ea\u52a8\u751f\u6210\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u4e3b\u9898\u3001\u95ee\u9898\u7c7b\u578b\u548c\u77e5\u8bc6\u7ec4\u7ec7\u65b9\u6cd5\uff0c\u65e8\u5728\u516c\u5e73\u8bc4\u4f30\u4ece15M\u6587\u6863\u5b50\u96c6\u4e2d\u68c0\u7d22\u95ee\u9898\u76f8\u5173\u652f\u6301\u6587\u6863\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u611f\u77e5\u7684\u591a\u6837\u6027\u91cd\u6392\u5e8fRAG\u7ba1\u9053\uff0c\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u611f\u77e5\u548c\u591a\u6837\u6027\u91cd\u6392\u5e8f\u6280\u672f\uff0c\u4f18\u5316\u4e86\u6587\u6863\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8de8\u6e90\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u77e5\u8bc6\u611f\u77e5\u591a\u6837\u6027\u91cd\u6392\u5e8fRAG\u7ba1\u9053\u5728\u8de8\u6e90\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "paper_title_zh": "\u77e5\u8bc6\u611f\u77e5\u591a\u6837\u6027\u91cd\u6392\u5e8f\u5728\u8de8\u6e90\u95ee\u7b54\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86Team Marikarp\u5728SIGIR 2025 LiveRAG\u7ade\u8d5b\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7ade\u8d5b\u7684\u8bc4\u4f30\u96c6\u7531DataMorgana\u4ece\u4e92\u8054\u7f51\u8bed\u6599\u5e93\u81ea\u52a8\u751f\u6210\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u76ee\u6807\u4e3b\u9898\u3001\u95ee\u9898\u7c7b\u578b\u3001\u95ee\u9898\u8868\u8ff0\u3001\u53d7\u4f17\u7c7b\u578b\u548c\u77e5\u8bc6\u7ec4\u7ec7\u65b9\u6cd5\u3002\u8be5\u8bc4\u4f30\u96c6\u65e8\u5728\u516c\u5e73\u8bc4\u4f30\u4eceFineWeb\u8bed\u6599\u5e93\u768415M\u6587\u6863\u5b50\u96c6\u4e2d\u68c0\u7d22\u95ee\u9898\u76f8\u5173\u652f\u6301\u6587\u6863\u7684\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u7684\u77e5\u8bc6\u611f\u77e5\u591a\u6837\u6027\u91cd\u6392\u5e8fRAG\u7ba1\u9053\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002"}}
{"id": "2506.20326", "pdf": "https://arxiv.org/pdf/2506.20326", "abs": "https://arxiv.org/abs/2506.20326", "authors": ["Sergio Torres Aguilar"], "title": "From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents", "categories": ["cs.CV", "cs.CL", "cs.DB"], "comment": null, "summary": "Robust Document Layout Analysis (DLA) is critical for the automated\nprocessing and understanding of historical documents with complex page\norganizations. This paper benchmarks five state-of-the-art object detection\narchitectures on three annotated datasets representing a spectrum of\ncodicological complexity: The e-NDP, a corpus of Parisian medieval registers\n(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval\nand modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated\nbooks of hours (ca.13th-16th centuries). We evaluate two Transformer-based\nmodels (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and\nYOLO-World). Our findings reveal significant performance variations dependent\non model architecture, data set characteristics, and bounding box\nrepresentation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results\n(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on\nthe more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB\nsignificantly outperforms all other models (0.564 and 0.568, respectively).\nThis study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)\nis not a minor refinement but a fundamental requirement for accurately modeling\nthe non-Cartesian nature of historical manuscripts. We conclude that a key\ntrade-off exists between the global context awareness of Transformers, ideal\nfor structured layouts, and the superior generalization of CNN-OBB models for\nvisually diverse and complex documents.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8eTransformer\u548cYOLO\u7684\u5e03\u5c40\u5206\u6790\u6a21\u578b\u5728\u5386\u53f2\u6587\u6863\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0YOLO-OBB\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cTransformer\u5728\u7ed3\u6784\u5316\u5e03\u5c40\u4e2d\u66f4\u4f18\u3002", "motivation": "\u5386\u53f2\u6587\u6863\u7684\u590d\u6742\u9875\u9762\u7ec4\u7ec7\u5bf9\u81ea\u52a8\u5316\u5904\u7406\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5728\u4e09\u4e2a\u5386\u53f2\u6587\u6863\u6570\u636e\u96c6\uff08e-NDP\u3001CATMuS\u3001HORAE\uff09\u4e0a\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cdTransformer\u6a21\u578b\uff08Co-DETR\u3001Grounding DINO\uff09\u548c\u4e09\u79cdYOLO\u53d8\u4f53\uff08AABB\u3001OBB\u3001YOLO-World\uff09\u7684\u6027\u80fd\u3002", "result": "\u5728e-NDP\u6570\u636e\u96c6\u4e0a\uff0cCo-DETR\u8868\u73b0\u6700\u4f73\uff080.752 mAP@.50:.95\uff09\uff0c\u800c\u5728\u66f4\u590d\u6742\u7684CATMuS\u548cHORAE\u6570\u636e\u96c6\u4e0a\uff0cYOLOv11x-OBB\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff080.564\u548c0.568\uff09\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u4f7f\u7528\u5b9a\u5411\u8fb9\u754c\u6846\uff08OBB\uff09\u662f\u51c6\u786e\u5efa\u6a21\u5386\u53f2\u6587\u6863\u975e\u7b1b\u5361\u5c14\u7279\u6027\u7684\u5173\u952e\uff0cTransformer\u9002\u5408\u7ed3\u6784\u5316\u5e03\u5c40\uff0c\u800cCNN-OBB\u6a21\u578b\u5728\u89c6\u89c9\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "paper_title_zh": "\u4ece\u6284\u672c\u5b66\u5230\u4ee3\u7801\uff1a\u57fa\u4e8eTransformer\u548cYOLO\u7684\u5386\u53f2\u6587\u6863\u5e03\u5c40\u5206\u6790\u68c0\u6d4b\u5668\u6bd4\u8f83\u7814\u7a76", "abstract_zh": "\u7a33\u5065\u7684\u6587\u6863\u5e03\u5c40\u5206\u6790\uff08DLA\uff09\u5bf9\u4e8e\u81ea\u52a8\u5316\u5904\u7406\u548c\u7406\u89e3\u5177\u6709\u590d\u6742\u9875\u9762\u7ec4\u7ec7\u7684\u5386\u53f2\u6587\u6863\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u5728\u4e09\u4e2a\u4ee3\u8868\u4e0d\u540c\u6284\u672c\u590d\u6742\u6027\u7684\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\uff08e-NDP\u3001CATMuS\u3001HORAE\uff09\u5bf9\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u4e24\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08Co-DETR\u3001Grounding DINO\uff09\u4e0e\u4e09\u79cdYOLO\u53d8\u4f53\uff08AABB\u3001OBB\u3001YOLO-World\uff09\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f9d\u8d56\u4e8e\u67b6\u6784\u3001\u6570\u636e\u96c6\u7279\u6027\u548c\u8fb9\u754c\u6846\u8868\u793a\u3002\u5728e-NDP\u6570\u636e\u96c6\u4e0a\uff0cCo-DETR\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff080.752 mAP@.50:.95\uff09\uff0c\u7d27\u968f\u5176\u540e\u7684\u662fYOLOv11X-OBB\uff080.721\uff09\u3002\u800c\u5728\u66f4\u590d\u6742\u7684CATMuS\u548cHORAE\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eCNN\u7684YOLOv11x-OBB\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff080.564\u548c0.568\uff09\u3002\u672c\u7814\u7a76\u660e\u786e\u8868\u660e\uff0c\u4f7f\u7528\u5b9a\u5411\u8fb9\u754c\u6846\uff08OBB\uff09\u4e0d\u4ec5\u662f\u5fae\u5c0f\u6539\u8fdb\uff0c\u800c\u662f\u51c6\u786e\u5efa\u6a21\u5386\u53f2\u624b\u7a3f\u975e\u7b1b\u5361\u5c14\u7279\u6027\u7684\u57fa\u672c\u8981\u6c42\u3002\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0cTransformer\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u9002\u5408\u7ed3\u6784\u5316\u5e03\u5c40\uff0c\u800cCNN-OBB\u6a21\u578b\u5728\u89c6\u89c9\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u4e0a\u5177\u6709\u66f4\u4f18\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.19862", "pdf": "https://arxiv.org/pdf/2506.19862", "abs": "https://arxiv.org/abs/2506.19862", "authors": ["Junjie Xu", "Jiahao Zhang", "Mangal Prakash", "Xiang Zhang", "Suhang Wang"], "title": "DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": null, "summary": "Geometric graph neural networks (GNNs) that respect E(3) symmetries have\nachieved strong performance on small molecule modeling, but they face\nscalability and expressiveness challenges when applied to large biomolecules\nsuch as RNA and proteins. These systems require models that can simultaneously\ncapture fine-grained atomic interactions, long-range dependencies across\nspatially distant components, and biologically relevant hierarchical structure,\nsuch as atoms forming residues, which in turn form higher-order domains.\nExisting geometric GNNs, which typically operate exclusively in either\nEuclidean or Spherical Harmonics space, are limited in their ability to capture\nboth the fine-scale atomic details and the long-range, symmetry-aware\ndependencies required for modeling the multi-scale structure of large\nbiomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant\nNetwork that constructs complementary representations in both Euclidean and\nSpherical Harmonics spaces to capture local geometry and global symmetry-aware\nfeatures. DualEquiNet employs bidirectional cross-space message passing and a\nnovel Cross-Space Interaction Pooling mechanism to hierarchically aggregate\natomic features into biologically meaningful units, such as residues, enabling\nefficient and expressive multi-scale modeling for large biomolecular systems.\nDualEquiNet achieves state-of-the-art performance on multiple existing\nbenchmarks for RNA property prediction and protein modeling, and outperforms\nprior methods on two newly introduced 3D structural benchmarks demonstrating\nits broad effectiveness across a range of large biomolecule modeling tasks.", "AI": {"tldr": "DualEquiNet\u662f\u4e00\u79cd\u53cc\u7a7a\u95f4\u5c42\u6b21\u7b49\u53d8\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u548c\u7403\u8c10\u7a7a\u95f4\u7684\u4e92\u8865\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u751f\u7269\u5206\u5b50\u5efa\u6a21\u4e2d\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u95ee\u9898\uff0c\u5e76\u5728RNA\u548c\u86cb\u767d\u8d28\u5efa\u6a21\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5c0f\u578b\u5206\u5b50\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u5927\u578b\u751f\u7269\u5206\u5b50\uff08\u5982RNA\u548c\u86cb\u767d\u8d28\uff09\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u7cfb\u7edf\u9700\u8981\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u539f\u5b50\u76f8\u4e92\u4f5c\u7528\u3001\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u548c\u751f\u7269\u76f8\u5173\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "method": "DualEquiNet\u901a\u8fc7\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u548c\u7403\u8c10\u7a7a\u95f4\u4e2d\u6784\u5efa\u4e92\u8865\u8868\u793a\uff0c\u7ed3\u5408\u53cc\u5411\u8de8\u7a7a\u95f4\u6d88\u606f\u4f20\u9012\u548c\u65b0\u578b\u8de8\u7a7a\u95f4\u4ea4\u4e92\u6c60\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5c40\u90e8\u51e0\u4f55\u548c\u5168\u5c40\u5bf9\u79f0\u6027\u7279\u5f81\u7684\u6355\u6349\uff0c\u5e76\u5c06\u539f\u5b50\u7279\u5f81\u5c42\u6b21\u805a\u5408\u4e3a\u751f\u7269\u610f\u4e49\u5355\u5143\uff08\u5982\u6b8b\u57fa\uff09\u3002", "result": "DualEquiNet\u5728RNA\u6027\u8d28\u9884\u6d4b\u548c\u86cb\u767d\u8d28\u5efa\u6a21\u7684\u591a\u4e2a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u5f15\u5165\u76843D\u7ed3\u6784\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u578b\u751f\u7269\u5206\u5b50\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u6709\u6548\u6027\u3002", "conclusion": "DualEquiNet\u901a\u8fc7\u53cc\u7a7a\u95f4\u5c42\u6b21\u7b49\u53d8\u7f51\u7edc\u7684\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578b\u751f\u7269\u5206\u5b50\u5efa\u6a21\u4e2d\u7684\u591a\u5c3a\u5ea6\u95ee\u9898\uff0c\u4e3a\u751f\u7269\u5206\u5b50\u7ed3\u6784\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DualEquiNet\uff1a\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u751f\u7269\u5206\u5b50\u7684\u53cc\u7a7a\u95f4\u5c42\u6b21\u7b49\u53d8\u7f51\u7edc", "abstract_zh": "\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5c0f\u578b\u5206\u5b50\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5e94\u7528\u4e8eRNA\u548c\u86cb\u767d\u8d28\u7b49\u5927\u578b\u751f\u7269\u5206\u5b50\u65f6\uff0c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u80fd\u529b\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u7cfb\u7edf\u9700\u8981\u6a21\u578b\u80fd\u591f\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u539f\u5b50\u76f8\u4e92\u4f5c\u7528\u3001\u7a7a\u95f4\u8fdc\u8ddd\u79bb\u7ec4\u4ef6\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u751f\u7269\u76f8\u5173\u7684\u5c42\u6b21\u7ed3\u6784\uff08\u5982\u539f\u5b50\u5f62\u6210\u6b8b\u57fa\uff0c\u6b8b\u57fa\u518d\u5f62\u6210\u66f4\u9ad8\u9636\u57df\uff09\u3002\u73b0\u6709\u7684\u51e0\u4f55GNN\u901a\u5e38\u4ec5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u6216\u7403\u8c10\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u96be\u4ee5\u540c\u65f6\u6355\u6349\u7ec6\u5c3a\u5ea6\u539f\u5b50\u7ec6\u8282\u548c\u957f\u7a0b\u5bf9\u79f0\u6027\u4f9d\u8d56\u5173\u7cfb\u3002\u6211\u4eec\u63d0\u51fa\u4e86DualEquiNet\uff0c\u4e00\u79cd\u53cc\u7a7a\u95f4\u5c42\u6b21\u7b49\u53d8\u7f51\u7edc\uff0c\u901a\u8fc7\u5728\u6b27\u51e0\u91cc\u5f97\u548c\u7403\u8c10\u7a7a\u95f4\u4e2d\u6784\u5efa\u4e92\u8865\u8868\u793a\uff0c\u6355\u6349\u5c40\u90e8\u51e0\u4f55\u548c\u5168\u5c40\u5bf9\u79f0\u6027\u7279\u5f81\u3002DualEquiNet\u91c7\u7528\u53cc\u5411\u8de8\u7a7a\u95f4\u6d88\u606f\u4f20\u9012\u548c\u65b0\u578b\u8de8\u7a7a\u95f4\u4ea4\u4e92\u6c60\u5316\u673a\u5236\uff0c\u5c06\u539f\u5b50\u7279\u5f81\u5c42\u6b21\u805a\u5408\u4e3a\u751f\u7269\u610f\u4e49\u5355\u5143\uff08\u5982\u6b8b\u57fa\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u578b\u751f\u7269\u5206\u5b50\u7cfb\u7edf\u7684\u9ad8\u6548\u591a\u5c3a\u5ea6\u5efa\u6a21\u3002DualEquiNet\u5728RNA\u6027\u8d28\u9884\u6d4b\u548c\u86cb\u767d\u8d28\u5efa\u6a21\u7684\u591a\u4e2a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u5f15\u5165\u76843D\u7ed3\u6784\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u578b\u751f\u7269\u5206\u5b50\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u6709\u6548\u6027\u3002"}}
{"id": "2506.20480", "pdf": "https://arxiv.org/pdf/2506.20480", "abs": "https://arxiv.org/abs/2506.20480", "authors": ["Guinan Su", "Li Shen", "Lu Yin", "Shiwei Liu", "Yanwu Yang", "Jonas Geiping"], "title": "GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPTailor\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u526a\u88c1\u548c\u62fc\u63a5\u5c42\u6765\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u5e9e\u5927\uff0c\u90e8\u7f72\u548c\u63a8\u7406\u6210\u672c\u9ad8\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u6a21\u578b\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u5fae\u8c03\u6a21\u578b\u7684\u5c42\u6765\u538b\u7f29\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u4e09\u79cd\u64cd\u4f5c\uff1a\u5c42\u79fb\u9664\u3001\u4ece\u4e0d\u540c\u5019\u9009\u6a21\u578b\u4e2d\u9009\u62e9\u5c42\u4ee5\u53ca\u5c42\u5408\u5e76\u3002\u901a\u8fc7\u7b56\u7565\u6027\u5730\u7ed3\u5408\u5fae\u8c03\u6a21\u578b\u7684\u5c42\uff0c\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Llama2-13B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u526a\u679d\uff0c\u538b\u7f29\u540e\u6a21\u578b\u4fdd\u7559\u4e86\u7ea697.3%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7ea625%\u7684\u53c2\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPTailor\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u526a\u88c1\u548c\u62fc\u63a5\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "paper_title_zh": "GPTailor\uff1a\u901a\u8fc7\u5c42\u526a\u88c1\u4e0e\u62fc\u63a5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u526a\u679d\u65b9\u6cd5", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u6a21\u578b\u89c4\u6a21\u7ed9\u90e8\u7f72\u548c\u63a8\u7406\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002\u5c3d\u7ba1\u7ed3\u6784\u5316\u526a\u679d\u4e3a\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u6a21\u578b\u526a\u679d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u5730\u7ed3\u5408\u6216\u5408\u5e76\u5fae\u8c03\u6a21\u578b\u53d8\u4f53\u7684\u5c42\u6765\u538b\u7f29\u6a21\u578b\uff0c\u4ece\u800c\u4fdd\u7559\u539f\u59cb\u6a21\u578b\u7684\u80fd\u529b\u3002\u6211\u4eec\u5c06LLMs\u7684\u6700\u4f18\u526a\u88c1\u95ee\u9898\u5efa\u6a21\u4e3a\u96f6\u9636\u4f18\u5316\u95ee\u9898\uff0c\u652f\u6301\u4e09\u79cd\u64cd\u4f5c\uff1a\uff081\uff09\u5c42\u79fb\u9664\uff0c\uff082\uff09\u4ece\u4e0d\u540c\u5019\u9009\u6a21\u578b\u4e2d\u9009\u62e9\u5c42\uff0c\uff083\uff09\u5c42\u5408\u5e76\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Llama2-13B\u6a21\u578b\u5bb6\u65cf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u538b\u7f29\u540e\u7684\u6a21\u578b\u5728\u79fb\u9664\u7ea625%\u53c2\u6570\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u4e86\u7ea697.3%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/Guinan-Su/auto-merge-llm\u3002"}}
{"id": "2506.20342", "pdf": "https://arxiv.org/pdf/2506.20342", "abs": "https://arxiv.org/abs/2506.20342", "authors": ["Lei Wang", "Piotr Koniusz"], "title": "Feature Hallucination for Self-supervised Action Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication in International Journal of Computer Vision\n  (IJCV)", "summary": "Understanding human actions in videos requires more than raw pixel analysis;\nit relies on high-level semantic reasoning and effective integration of\nmultimodal features. We propose a deep translational action recognition\nframework that enhances recognition accuracy by jointly predicting action\nconcepts and auxiliary features from RGB video frames. At test time,\nhallucination streams infer missing cues, enriching feature representations\nwithout increasing computational overhead. To focus on action-relevant regions\nbeyond raw pixels, we introduce two novel domain-specific descriptors. Object\nDetection Features (ODF) aggregate outputs from multiple object detectors to\ncapture contextual cues, while Saliency Detection Features (SDF) highlight\nspatial and intensity patterns crucial for action recognition. Our framework\nseamlessly integrates these descriptors with auxiliary modalities such as\noptical flow, Improved Dense Trajectories, skeleton data, and audio cues. It\nremains compatible with state-of-the-art architectures, including I3D,\nAssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE\nV2 and InternVideo2. To handle uncertainty in auxiliary features, we\nincorporate aleatoric uncertainty modeling in the hallucination step and\nintroduce a robust loss function to mitigate feature noise. Our multimodal\nself-supervised action recognition framework achieves state-of-the-art\nperformance on multiple benchmarks, including Kinetics-400, Kinetics-600, and\nSomething-Something V2, demonstrating its effectiveness in capturing\nfine-grained action dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u52a8\u4f5c\u6982\u5ff5\u548c\u8f85\u52a9\u7279\u5f81\uff0c\u5229\u7528\u5e7b\u89c9\u6d41\u63a8\u65ad\u7f3a\u5931\u7ebf\u7d22\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u63cf\u8ff0\u7b26\uff08ODF\u548cSDF\uff09\u63d0\u5347\u7279\u5f81\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u539f\u59cb\u50cf\u7d20\u5206\u6790\uff0c\u8fd8\u9700\u8981\u9ad8\u7ea7\u8bed\u4e49\u63a8\u7406\u548c\u591a\u6a21\u6001\u7279\u5f81\u7684\u6709\u6548\u6574\u5408\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u8868\u793a\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3a\u8bc6\u522b\u7cbe\u5ea6\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u7ffb\u8bd1\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5e7b\u89c9\u6d41\u63a8\u65ad\u7f3a\u5931\u7ebf\u7d22\uff0c\u7ed3\u5408\u5bf9\u8c61\u68c0\u6d4b\u7279\u5f81\uff08ODF\uff09\u548c\u663e\u8457\u6027\u68c0\u6d4b\u7279\u5f81\uff08SDF\uff09\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002\u540c\u65f6\u6574\u5408\u5149\u6d41\u3001\u9aa8\u67b6\u6570\u636e\u7b49\u8f85\u52a9\u6a21\u6001\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u9c81\u68d2\u635f\u5931\u51fd\u6570\u4ee5\u5904\u7406\u7279\u5f81\u566a\u58f0\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Kinetics-400\u3001Kinetics-600\u548cSomething-Something V2\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u52a8\u6001\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u548c\u5e7b\u89c9\u6d41\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u52a8\u4f5c\u8bc6\u522b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u81ea\u76d1\u7763\u52a8\u4f5c\u8bc6\u522b\u7684\u7279\u5f81\u5e7b\u89c9\u65b9\u6cd5", "abstract_zh": "\u7406\u89e3\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u52a8\u4f5c\u4e0d\u4ec5\u9700\u8981\u539f\u59cb\u50cf\u7d20\u5206\u6790\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u9ad8\u7ea7\u8bed\u4e49\u63a8\u7406\u548c\u591a\u6a21\u6001\u7279\u5f81\u7684\u6709\u6548\u6574\u5408\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u7ffb\u8bd1\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4bRGB\u89c6\u9891\u5e27\u4e2d\u7684\u52a8\u4f5c\u6982\u5ff5\u548c\u8f85\u52a9\u7279\u5f81\u6765\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u3002\u5728\u6d4b\u8bd5\u65f6\uff0c\u5e7b\u89c9\u6d41\u63a8\u65ad\u7f3a\u5931\u7ebf\u7d22\uff0c\u4e30\u5bcc\u7279\u5f81\u8868\u793a\u800c\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002\u4e3a\u4e86\u5173\u6ce8\u52a8\u4f5c\u76f8\u5173\u533a\u57df\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u7684\u9886\u57df\u7279\u5b9a\u63cf\u8ff0\u7b26\uff1a\u5bf9\u8c61\u68c0\u6d4b\u7279\u5f81\uff08ODF\uff09\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u68c0\u6d4b\u5668\u7684\u8f93\u51fa\u6355\u6349\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u800c\u663e\u8457\u6027\u68c0\u6d4b\u7279\u5f81\uff08SDF\uff09\u7a81\u51fa\u5bf9\u52a8\u4f5c\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u7684\u7a7a\u95f4\u548c\u5f3a\u5ea6\u6a21\u5f0f\u3002\u6211\u4eec\u7684\u6846\u67b6\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u7b26\u4e0e\u5149\u6d41\u3001\u6539\u8fdb\u7684\u5bc6\u96c6\u8f68\u8ff9\u3001\u9aa8\u67b6\u6570\u636e\u548c\u97f3\u9891\u7ebf\u7d22\u7b49\u8f85\u52a9\u6a21\u6001\u65e0\u7f1d\u6574\u5408\u3002\u8be5\u65b9\u6cd5\u4e0eI3D\u3001AssembleNet\u3001Video Transformer Network\u7b49\u5148\u8fdb\u67b6\u6784\u517c\u5bb9\u3002\u4e3a\u4e86\u5904\u7406\u8f85\u52a9\u7279\u5f81\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6211\u4eec\u5728\u5e7b\u89c9\u6b65\u9aa4\u4e2d\u5f15\u5165\u4e86\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u635f\u5931\u51fd\u6570\u4ee5\u51cf\u5c11\u7279\u5f81\u566a\u58f0\u3002\u6211\u4eec\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\u5728Kinetics-400\u3001Kinetics-600\u548cSomething-Something V2\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u52a8\u6001\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.19863", "pdf": "https://arxiv.org/pdf/2506.19863", "abs": "https://arxiv.org/abs/2506.19863", "authors": ["Ahmed Almeldein", "Mohammed Alnaggar", "Rick Archibald", "Tom Beck", "Arpan Biswas", "Rike Bostelmann", "Wes Brewer", "Chris Bryan", "Christopher Calle", "Cihangir Celik", "Rajni Chahal", "Jong Youl Choi", "Arindam Chowdhury", "Mark Cianciosa", "Franklin Curtis", "Gregory Davidson", "Sebastian De Pascuale", "Lisa Fassino", "Ana Gainaru", "Yashika Ghai", "Luke Gibson", "Qian Gong", "Christopher Greulich", "Scott Greenwood", "Cory Hauck", "Ehab Hassan", "Rinkle Juneja", "Soyoung Kang", "Scott Klasky", "Atul Kumar", "Vineet Kumar", "Paul Laiu", "Calvin Lear", "Yan-Ru Lin", "Jono McConnell", "Furkan Oz", "Anant Raj", "Pradeep Ramuhalli", "Marie Romedenne", "Samantha Sabatino", "Jos\u00e9 Salcedo-P\u00e9rez", "Nathan D. See", "Arpan Sircar", "Punam Thankur", "Tim Younkin", "Xiao-Ying Yu", "Prashant Jain", "Tom Evans", "Prasanna Balaprakash"], "title": "Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research", "categories": ["physics.comp-ph", "cs.AI"], "comment": null, "summary": "The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated\nthe potential of Large Language Models (LLMs) to accelerate fusion and fission\nresearch. Fourteen interdisciplinary teams explored diverse nuclear science\nchallenges using ChatGPT, Gemini, Claude, and other AI models over a single\nday. Applications ranged from developing foundation models for fusion reactor\ncontrol to automating Monte Carlo simulations, predicting material degradation,\nand designing experimental programs for advanced reactors. Teams employed\nstructured workflows combining prompt engineering, deep research capabilities,\nand iterative refinement to generate hypotheses, prototype code, and research\nstrategies. Key findings demonstrate that LLMs excel at early-stage\nexploration, literature synthesis, and workflow design, successfully\nidentifying research gaps and generating plausible experimental frameworks.\nHowever, significant limitations emerged, including difficulties with novel\nmaterials designs, advanced code generation for modeling and simulation, and\ndomain-specific details requiring expert validation. The successful outcomes\nresulted from expert-driven prompt engineering and treating AI as a\ncomplementary tool rather than a replacement for physics-based methods. The\nworkshop validated AI's potential to accelerate nuclear energy research through\nrapid iteration and cross-disciplinary synthesis while highlighting the need\nfor curated nuclear-specific datasets, workflow automation, and specialized\nmodel development. These results provide a roadmap for integrating AI tools\ninto nuclear science workflows, potentially reducing development cycles for\nsafer, more efficient nuclear energy systems while maintaining rigorous\nscientific standards.", "AI": {"tldr": "\u7f8e\u56fd\u6a61\u6811\u5cad\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u7684AI\u6838\u80fd\u7814\u8ba8\u4f1a\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a0\u901f\u6838\u805a\u53d8\u548c\u6838\u88c2\u53d8\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u300214\u4e2a\u8de8\u5b66\u79d1\u56e2\u961f\u5728\u4e00\u5929\u5185\u4f7f\u7528ChatGPT\u3001Gemini\u3001Claude\u7b49AI\u6a21\u578b\u63a2\u7d22\u4e86\u6838\u79d1\u5b66\u7684\u591a\u9879\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0LLMs\u5728\u65e9\u671f\u63a2\u7d22\u3001\u6587\u732e\u7efc\u8ff0\u548c\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u65b0\u6750\u6599\u8bbe\u8ba1\u3001\u9ad8\u7ea7\u4ee3\u7801\u751f\u6210\u548c\u9886\u57df\u7ec6\u8282\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u4e13\u5bb6\u9a71\u52a8\u7684\u63d0\u793a\u5de5\u7a0b\u548cAI\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u4f7f\u7528\u662f\u5173\u952e\u6210\u529f\u56e0\u7d20\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6838\u80fd\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u52a0\u901f\u6838\u805a\u53d8\u548c\u6838\u88c2\u53d8\u9886\u57df\u7684\u79d1\u5b66\u8fdb\u5c55\uff0c\u5e76\u63a2\u7d22AI\u5de5\u5177\u5982\u4f55\u8865\u5145\u4f20\u7edf\u7269\u7406\u65b9\u6cd5\u3002", "method": "14\u4e2a\u8de8\u5b66\u79d1\u56e2\u961f\u5728\u4e00\u5929\u5185\u4f7f\u7528\u591a\u79cdAI\u6a21\u578b\uff08\u5982ChatGPT\u3001Gemini\u3001Claude\uff09\u89e3\u51b3\u6838\u79d1\u5b66\u95ee\u9898\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u6df1\u5ea6\u7814\u7a76\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u751f\u6210\u5047\u8bbe\u3001\u539f\u578b\u4ee3\u7801\u548c\u7814\u7a76\u7b56\u7565\u3002", "result": "LLMs\u5728\u65e9\u671f\u63a2\u7d22\u3001\u6587\u732e\u7efc\u8ff0\u548c\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u80fd\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u5e76\u751f\u6210\u53ef\u884c\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u4f46\u5728\u65b0\u6750\u6599\u8bbe\u8ba1\u3001\u9ad8\u7ea7\u4ee3\u7801\u751f\u6210\u548c\u9886\u57df\u7ec6\u8282\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u6210\u529f\u6848\u4f8b\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u9a71\u52a8\u7684\u63d0\u793a\u5de5\u7a0b\u548cAI\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u4f7f\u7528\u3002", "conclusion": "AI\u5de5\u5177\u5728\u6838\u80fd\u7814\u7a76\u4e2d\u5177\u6709\u52a0\u901f\u79d1\u5b66\u8fdb\u5c55\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u5f00\u53d1\u6838\u80fd\u4e13\u7528\u6570\u636e\u96c6\u3001\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u548c\u5b9a\u5236\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u79d1\u5b66\u6807\u51c6\u3002", "paper_title_zh": "\u63a2\u7d22\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6838\u80fd\u7814\u7a76\u4e2d\u7684\u80fd\u529b", "abstract_zh": "\u7f8e\u56fd\u6a61\u6811\u5cad\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u7684AI\u6838\u80fd\u7814\u8ba8\u4f1a\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a0\u901f\u6838\u805a\u53d8\u548c\u6838\u88c2\u53d8\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u300214\u4e2a\u8de8\u5b66\u79d1\u56e2\u961f\u5728\u4e00\u5929\u5185\u4f7f\u7528ChatGPT\u3001Gemini\u3001Claude\u7b49AI\u6a21\u578b\u63a2\u7d22\u4e86\u6838\u79d1\u5b66\u7684\u591a\u9879\u6311\u6218\uff0c\u5305\u62ec\u5f00\u53d1\u805a\u53d8\u53cd\u5e94\u5806\u63a7\u5236\u7684\u57fa\u7840\u6a21\u578b\u3001\u81ea\u52a8\u5316\u8499\u7279\u5361\u6d1b\u6a21\u62df\u3001\u9884\u6d4b\u6750\u6599\u9000\u5316\u4ee5\u53ca\u8bbe\u8ba1\u5148\u8fdb\u53cd\u5e94\u5806\u7684\u5b9e\u9a8c\u7a0b\u5e8f\u3002\u56e2\u961f\u91c7\u7528\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u6df1\u5ea6\u7814\u7a76\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\uff0c\u751f\u6210\u5047\u8bbe\u3001\u539f\u578b\u4ee3\u7801\u548c\u7814\u7a76\u7b56\u7565\u3002\u5173\u952e\u53d1\u73b0\u8868\u660e\uff0cLLMs\u5728\u65e9\u671f\u63a2\u7d22\u3001\u6587\u732e\u7efc\u8ff0\u548c\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6210\u529f\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u5e76\u751f\u6210\u53ef\u884c\u7684\u5b9e\u9a8c\u6846\u67b6\u3002\u7136\u800c\uff0c\u5176\u5728\u65b0\u6750\u6599\u8bbe\u8ba1\u3001\u9ad8\u7ea7\u4ee3\u7801\u751f\u6210\u548c\u9700\u8981\u4e13\u5bb6\u9a8c\u8bc1\u7684\u9886\u57df\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002\u6210\u529f\u6848\u4f8b\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u9a71\u52a8\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u5c06AI\u89c6\u4e3a\u7269\u7406\u65b9\u6cd5\u7684\u8865\u5145\u5de5\u5177\u800c\u975e\u66ff\u4ee3\u54c1\u3002\u7814\u8ba8\u4f1a\u9a8c\u8bc1\u4e86AI\u901a\u8fc7\u5feb\u901f\u8fed\u4ee3\u548c\u8de8\u5b66\u79d1\u5408\u6210\u52a0\u901f\u6838\u80fd\u7814\u7a76\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5f00\u53d1\u6838\u80fd\u4e13\u7528\u6570\u636e\u96c6\u3001\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u548c\u5b9a\u5236\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5c06AI\u5de5\u5177\u6574\u5408\u5230\u6838\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u6709\u671b\u7f29\u77ed\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u6838\u80fd\u7cfb\u7edf\u7684\u5f00\u53d1\u5468\u671f\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u79d1\u5b66\u6807\u51c6\u3002"}}
{"id": "2506.20495", "pdf": "https://arxiv.org/pdf/2506.20495", "abs": "https://arxiv.org/abs/2506.20495", "authors": ["Haoze Wu", "Yunzhi Yao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SE"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.", "AI": {"tldr": "ReCode\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9002\u5e94\u5916\u90e8\u5e93API\u7684\u9891\u7e41\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001API\u573a\u666f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u96be\u4ee5\u9002\u5e94\u5916\u90e8\u5e93API\u7684\u9891\u7e41\u66f4\u65b0\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u4e0d\u53ef\u9760\u3002\u8fd9\u4e00\u95ee\u9898\u6e90\u4e8e\u6a21\u578b\u4f9d\u8d56\u8fc7\u65f6\u7684API\u77e5\u8bc6\uff0c\u5373\u4f7f\u80fd\u591f\u8bbf\u95ee\u6700\u65b0\u6587\u6863\u3002", "method": "ReCode\u901a\u8fc7\u6784\u5efa\u5305\u542b\u7ea62000\u6761\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3LLMs\u6267\u884c\u57fa\u4e8e\u66f4\u65b0\u4fe1\u606f\u7684\u7248\u672c\u8fc1\u79fb\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u7684\u5b57\u7b26\u4e32\u76f8\u4f3c\u5ea6\u6307\u6807\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReCode\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u52a8\u6001API\u573a\u666f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5c24\u5176\u5728\u672a\u89c1\u7684CodeUpdateArena\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u6b64\u5916\uff0c\u4e0e\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\uff0cReCode\u5bf9LLMs\u7684\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "ReCode\u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5e2e\u52a9LLMs\u9002\u5e94API\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002\u5728\u4e0d\u540cLLMs\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\u548cDAPO\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\u3002", "paper_title_zh": "ReCode\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7801API\u77e5\u8bc6\u66f4\u65b0", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u96be\u4ee5\u9002\u5e94\u5916\u90e8\u5e93API\u7684\u9891\u7e41\u66f4\u65b0\u3002\u8fd9\u4e00\u5173\u952e\u9650\u5236\u6e90\u4e8e\u6a21\u578b\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8fc7\u65f6API\u77e5\u8bc6\uff0c\u5373\u4f7f\u80fd\u591f\u8bbf\u95ee\u6700\u65b0\u6587\u6863\uff0c\u4e5f\u963b\u788d\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ReCode\uff08\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7801\u66f4\u65b0\u6846\u67b6\uff09\uff0c\u8be5\u6846\u67b6\u6a21\u62df\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5bf9API\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea62000\u6761\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3LLMs\u57fa\u4e8e\u66f4\u65b0\u4fe1\u606f\u6267\u884c\u7248\u672c\u8fc1\u79fb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5b57\u7b26\u4e32\u76f8\u4f3c\u5ea6\u6307\u6807\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cReCode\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u52a8\u6001API\u573a\u666f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5c24\u5176\u5728\u672a\u89c1\u7684CodeUpdateArena\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u91cd\u8981\u7684\u662f\uff0c\u4e0e\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\uff0cReCode\u5bf9LLMs\u7684\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f71\u54cd\u8f83\u5c0f\u3002\u6211\u4eec\u5728\u591a\u79cdLLMs\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\u548cDAPO\uff09\u4e0a\u5e94\u7528ReCode\uff0c\u5747\u53d6\u5f97\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7ecf\u8fc7\u8bad\u7ec3\u540e\uff0cQwen2.5-Coder-7B\u7684\u6027\u80fd\u8d85\u8fc7\u4e8632B\u53c2\u6570\u7684\u4ee3\u7801\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u548c\u76f8\u540c\u67b6\u6784\u7684\u63a8\u7406\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u5728https://github.com/zjunlp/ReCode\u83b7\u53d6\u3002"}}
{"id": "2506.20370", "pdf": "https://arxiv.org/pdf/2506.20370", "abs": "https://arxiv.org/abs/2506.20370", "authors": ["Abdullah All Tanvir", "Xin Zhong"], "title": "InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "This paper introduces a novel deep learning framework for robust image\nzero-watermarking based on distortion-invariant feature learning. As a\nzero-watermarking scheme, our method leaves the original image unaltered and\nlearns a reference signature through optimization in the feature space. The\nproposed framework consists of two key modules. In the first module, a feature\nextractor is trained via noise-adversarial learning to generate representations\nthat are both invariant to distortions and semantically expressive. This is\nachieved by combining adversarial supervision against a distortion\ndiscriminator and a reconstruction constraint to retain image content. In the\nsecond module, we design a learning-based multibit zero-watermarking scheme\nwhere the trained invariant features are projected onto a set of trainable\nreference codes optimized to match a target binary message. Extensive\nexperiments on diverse image datasets and a wide range of distortions show that\nour method achieves state-of-the-art robustness in both feature stability and\nwatermark recovery. Comparative evaluations against existing self-supervised\nand deep watermarking techniques further highlight the superiority of our\nframework in generalization and robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5931\u771f\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9c81\u68d2\u7684\u56fe\u50cf\u96f6\u6c34\u5370\u6280\u672f\u3002\u901a\u8fc7\u566a\u58f0\u5bf9\u6297\u8bad\u7ec3\u5b66\u4e60\u4e0d\u53d8\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u591a\u6bd4\u7279\u96f6\u6c34\u5370\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u50cf\u6c34\u5370\u6280\u672f\u53ef\u80fd\u5bf9\u539f\u59cb\u56fe\u50cf\u9020\u6210\u4fee\u6539\uff0c\u4e14\u5bf9\u5931\u771f\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u96f6\u6c34\u5370\u65b9\u6cd5\uff0c\u65e2\u80fd\u4fdd\u6301\u56fe\u50cf\u5b8c\u6574\u6027\uff0c\u53c8\u80fd\u901a\u8fc7\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u62b5\u6297\u591a\u79cd\u5931\u771f\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u6a21\u5757\uff1a1) \u901a\u8fc7\u566a\u58f0\u5bf9\u6297\u8bad\u7ec3\u5b66\u4e60\u5931\u771f\u4e0d\u53d8\u7684\u7279\u5f81\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u6297\u76d1\u7763\u548c\u91cd\u5efa\u7ea6\u675f\uff1b2) \u8bbe\u8ba1\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u6bd4\u7279\u96f6\u6c34\u5370\u65b9\u6848\uff0c\u5c06\u4e0d\u53d8\u7279\u5f81\u6295\u5f71\u5230\u53ef\u8bad\u7ec3\u7684\u53c2\u8003\u7801\u4e0a\u4ee5\u5339\u914d\u76ee\u6807\u4e8c\u8fdb\u5236\u6d88\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u56fe\u50cf\u6570\u636e\u96c6\u548c\u5931\u771f\u6761\u4ef6\u4e0b\uff0c\u7279\u5f81\u7a33\u5b9a\u6027\u548c\u6c34\u5370\u6062\u590d\u80fd\u529b\u5747\u8fbe\u5230\u6700\u4f18\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u548c\u6df1\u5ea6\u5b66\u4e60\u6c34\u5370\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u96f6\u6c34\u5370\u6280\u672f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u56fe\u50cf\u6c34\u5370\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "InvZW\uff1a\u57fa\u4e8e\u566a\u58f0\u5bf9\u6297\u8bad\u7ec3\u7684\u9c81\u68d2\u56fe\u50cf\u96f6\u6c34\u5370\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5931\u771f\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9c81\u68d2\u7684\u56fe\u50cf\u96f6\u6c34\u5370\u6280\u672f\u3002\u4f5c\u4e3a\u4e00\u79cd\u96f6\u6c34\u5370\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4fee\u6539\u539f\u59cb\u56fe\u50cf\uff0c\u800c\u662f\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u4f18\u5316\u5b66\u4e60\u53c2\u8003\u7b7e\u540d\u3002\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u7b2c\u4e00\u4e2a\u6a21\u5757\u901a\u8fc7\u566a\u58f0\u5bf9\u6297\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u751f\u6210\u5bf9\u5931\u771f\u4e0d\u53d8\u4e14\u8bed\u4e49\u8868\u8fbe\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u6297\u76d1\u7763\u548c\u91cd\u5efa\u7ea6\u675f\uff1b\u7b2c\u4e8c\u4e2a\u6a21\u5757\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u6bd4\u7279\u96f6\u6c34\u5370\u65b9\u6848\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u4e0d\u53d8\u7279\u5f81\u6295\u5f71\u5230\u4e00\u7ec4\u53ef\u8bad\u7ec3\u7684\u53c2\u8003\u7801\u4e0a\u4ee5\u5339\u914d\u76ee\u6807\u4e8c\u8fdb\u5236\u6d88\u606f\u3002\u5728\u591a\u79cd\u56fe\u50cf\u6570\u636e\u96c6\u548c\u5931\u771f\u6761\u4ef6\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7279\u5f81\u7a33\u5b9a\u6027\u548c\u6c34\u5370\u6062\u590d\u65b9\u9762\u5747\u8fbe\u5230\u6700\u4f18\u6c34\u5e73\u3002\u4e0e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u548c\u6df1\u5ea6\u5b66\u4e60\u6c34\u5370\u6280\u672f\u7684\u5bf9\u6bd4\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.19865", "pdf": "https://arxiv.org/pdf/2506.19865", "abs": "https://arxiv.org/abs/2506.19865", "authors": ["Piotr Gai\u0144ski", "Oussama Boussif", "Andrei Rekesh", "Dmytro Shevchuk", "Ali Parviz", "Mike Tyers", "Robert A. Batey", "Micha\u0142 Koziarski"], "title": "Scalable and Cost-Efficient de Novo Template-Based Molecular Generation", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": null, "summary": "Template-based molecular generation offers a promising avenue for drug design\nby ensuring generated compounds are synthetically accessible through predefined\nreaction templates and building blocks. In this work, we tackle three core\nchallenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)\nscaling to large building block libraries, and (3) effectively utilizing small\nfragment sets. We propose \\textbf{Recursive Cost Guidance}, a backward policy\nframework that employs auxiliary machine learning models to approximate\nsynthesis cost and viability. This guidance steers generation toward low-cost\nsynthesis pathways, significantly enhancing cost-efficiency, molecular\ndiversity, and quality, especially when paired with an \\textbf{Exploitation\nPenalty} that balances the trade-off between exploration and exploitation. To\nenhance performance in smaller building block libraries, we develop a\n\\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states\nto construct full synthesis trees. Our approach establishes state-of-the-art\nresults in template-based molecular generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u6210\u672c\u6307\u5bfc\u548c\u52a8\u6001\u5e93\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u6210\u672c\u9ad8\u3001\u5927\u89c4\u6a21\u6784\u5efa\u5757\u5e93\u6269\u5c55\u548c\u5c0f\u7247\u6bb5\u96c6\u5229\u7528\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u751f\u6210\u7684\u6548\u7387\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210\u5728\u836f\u7269\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u5408\u6210\u6210\u672c\u9ad8\u3001\u6784\u5efa\u5757\u5e93\u89c4\u6a21\u5927\u548c\u5c0f\u7247\u6bb5\u96c6\u5229\u7528\u4e0d\u8db3\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5b50\u751f\u6210\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u9012\u5f52\u6210\u672c\u6307\u5bfc\u6846\u67b6\uff0c\u5229\u7528\u8f85\u52a9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f30\u7b97\u5408\u6210\u6210\u672c\u548c\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5e93\u673a\u5236\u91cd\u7528\u9ad8\u5956\u52b1\u4e2d\u95f4\u72b6\u6001\uff0c\u4f18\u5316\u5206\u5b50\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5408\u6210\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u5206\u5b50\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9012\u5f52\u6210\u672c\u6307\u5bfc\u548c\u52a8\u6001\u5e93\u673a\u5236\u4e3a\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u836f\u7269\u8bbe\u8ba1\u9886\u57df\u5e26\u6765\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "paper_title_zh": "\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210", "abstract_zh": "\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210\u4e3a\u836f\u7269\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u53cd\u5e94\u6a21\u677f\u548c\u6784\u5efa\u5757\u786e\u4fdd\u751f\u6210\u7684\u5316\u5408\u7269\u5177\u6709\u5408\u6210\u53ef\u884c\u6027\u3002\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u57fa\u4e8e\u6a21\u677f\u7684GFlowNets\u4e2d\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a(1)\u6700\u5c0f\u5316\u5408\u6210\u6210\u672c\uff0c(2)\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6784\u5efa\u5757\u5e93\uff0c(3)\u6709\u6548\u5229\u7528\u5c0f\u7247\u6bb5\u96c6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u9012\u5f52\u6210\u672c\u6307\u5bfc\uff0c\u8fd9\u662f\u4e00\u79cd\u53cd\u5411\u7b56\u7565\u6846\u67b6\uff0c\u5229\u7528\u8f85\u52a9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u4f30\u7b97\u5408\u6210\u6210\u672c\u548c\u53ef\u884c\u6027\u3002\u8fd9\u79cd\u6307\u5bfc\u5c06\u751f\u6210\u8fc7\u7a0b\u5bfc\u5411\u4f4e\u6210\u672c\u5408\u6210\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u672c\u6548\u7387\u3001\u5206\u5b50\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u4e0e\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5229\u7528\u60e9\u7f5a\u76f8\u7ed3\u5408\u65f6\u3002\u4e3a\u4e86\u5728\u5c0f\u89c4\u6a21\u6784\u5efa\u5757\u5e93\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u52a8\u6001\u5e93\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u7528\u9ad8\u5956\u52b1\u4e2d\u95f4\u72b6\u6001\u6784\u5efa\u5b8c\u6574\u7684\u5408\u6210\u6811\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u57fa\u4e8e\u6a21\u677f\u7684\u5206\u5b50\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.20512", "pdf": "https://arxiv.org/pdf/2506.20512", "abs": "https://arxiv.org/abs/2506.20512", "authors": ["Zengzhi Wang", "Fan Zhou", "Xuefeng Li", "Pengfei Liu"], "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages; The first three authors contribute to this work equally", "summary": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u53d1\u73b0\u9ad8\u8d28\u91cf\u6570\u5b66\u8bed\u6599\u548c\u957f\u94fe\u63a8\u7406\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565OctoThinker\uff0c\u4ee5\u7f29\u5c0f\u4e0eRL\u53cb\u597d\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama\u548cQwen\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e2d\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6a21\u578b\u7684RL\u517c\u5bb9\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u6269\u5c55\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4Qwen\u548cLlama\u6a21\u578b\uff0c\u5206\u6790\u6570\u5b66\u8bed\u6599\u548c\u957f\u94fe\u63a8\u7406\u6570\u636e\u5bf9RL\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565Stable-then-Decay\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u5b66\u8bed\u6599\u548c\u94fe\u5f0f\u63a8\u7406\u6570\u636e\u4f18\u5316\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u9ad8\u8d28\u91cf\u6570\u5b66\u8bed\u6599\uff08\u5982MegaMath-Web-Pro\uff09\u663e\u8457\u63d0\u5347RL\u6027\u80fd\uff0c\u957f\u94fe\u63a8\u7406\u6570\u636e\u589e\u5f3a\u63a8\u7406\u6df1\u5ea6\u4f46\u53ef\u80fd\u5f15\u53d1\u6a21\u578b\u5197\u957f\u548c\u4e0d\u7a33\u5b9a\uff0c\u800c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565OctoThinker\u6210\u529f\u7f29\u5c0f\u4e86\u4e0eRL\u53cb\u597d\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u4e2d\u8bad\u7ec3\u7b56\u7565\u5bf9\u63d0\u5347\u6a21\u578b\u7684RL\u517c\u5bb9\u6027\u81f3\u5173\u91cd\u8981\uff0cOctoThinker\u901a\u8fc7\u4f18\u5316\u6570\u636e\u9009\u62e9\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "OctoThinker\uff1a\u4e2d\u8bad\u7ec3\u6fc0\u52b1\u5f3a\u5316\u5b66\u4e60\u6269\u5c55", "abstract_zh": "\u4e0d\u540c\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff08\u5982Llama\u548cQwen\uff09\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u3002\u4e86\u89e3\u4f55\u79cd\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u9002\u5408RL\u5bf9\u5f00\u53d1\u4e0b\u4e00\u4ee3\u53ef\u6269\u5c55\u57fa\u7840\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e2d\u8bad\u7ec3\u7b56\u7565\u5982\u4f55\u5f71\u54cdRL\u52a8\u6001\uff0c\u91cd\u70b9\u5173\u6ce8Qwen\u548cLlama\u4e24\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u5bb6\u65cf\u3002\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\u9ad8\u8d28\u91cf\u6570\u5b66\u8bed\u6599\uff08\u5982MegaMath-Web-Pro\uff09\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u548cRL\u6027\u80fd\uff0c\u800c\u73b0\u6709\u66ff\u4ee3\u54c1\uff08\u5982FineMath-4plus\uff09\u5219\u65e0\u6548\uff1b\uff082\uff09\u8fdb\u4e00\u6b65\u6dfb\u52a0\u95ee\u7b54\u5f0f\u6570\u636e\uff08\u5c24\u5176\u662f\u957f\u94fe\u63a8\u7406\u793a\u4f8b\uff09\u80fd\u589e\u5f3aRL\u6548\u679c\uff0c\u6307\u4ee4\u6570\u636e\u8fdb\u4e00\u6b65\u91ca\u653e\u8fd9\u4e00\u6f5c\u529b\uff1b\uff083\uff09\u957f\u94fe\u63a8\u7406\u867d\u63d0\u5347\u63a8\u7406\u6df1\u5ea6\uff0c\u4f46\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u56de\u7b54\u5197\u957f\u548cRL\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u51f8\u663e\u6570\u636e\u683c\u5f0f\u5316\u7684\u91cd\u8981\u6027\uff1b\uff084\uff09\u4e2d\u8bad\u7ec3\u89c4\u6a21\u6269\u5927\u59cb\u7ec8\u5e26\u6765\u66f4\u5f3a\u7684\u4e0b\u6e38RL\u6027\u80fd\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4e2d\u8bad\u7ec3\u7b56\u7565Stable-then-Decay\uff1a\u57fa\u7840\u6a21\u578b\u9996\u5148\u4ee5\u6052\u5b9a\u5b66\u4e60\u7387\u8bad\u7ec3200B\u8bcd\u5143\uff0c\u968f\u540e\u5728\u4e09\u4e2a\u94fe\u5f0f\u63a8\u7406\u5206\u652f\u4e2d\u4ee5\u5b66\u4e60\u7387\u8870\u51cf\u8bad\u7ec320B\u8bcd\u5143\u3002\u7531\u6b64\u4ea7\u751f\u7684OctoThinker\u6a21\u578b\u5bb6\u65cf\u8868\u73b0\u51fa\u5f3a\u5927\u7684RL\u517c\u5bb9\u6027\uff0c\u7f29\u5c0f\u4e86\u4e0eRL\u53cb\u597d\u6a21\u578b\u5bb6\u65cf\uff08\u5982Qwen\uff09\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u4e3aRL\u65f6\u4ee3\u7684\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002\u4e3a\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5f00\u6e90\u4e86\u6a21\u578b\u53ca\u8d85\u8fc7700\u4ebf\u8bcd\u5143\u7684\u7cbe\u9009\u6570\u5b66\u63a8\u7406\u8bed\u6599\u5e93\uff08\u5373MegaMath-Web-Pro-Max\uff09\u3002"}}
{"id": "2506.20381", "pdf": "https://arxiv.org/pdf/2506.20381", "abs": "https://arxiv.org/abs/2506.20381", "authors": ["Ben Kang", "Xin Chen", "Jie Zhao", "Chunjuan Bo", "Dong Wang", "Huchuan Lu"], "title": "Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking", "categories": ["cs.CV", "cs.LG"], "comment": "This paper was accepted by International Journal of Computer\n  Vision(IJCV)", "summary": "Transformer-based visual trackers have demonstrated significant advancements\ndue to their powerful modeling capabilities. However, their practicality is\nlimited on resource-constrained devices because of their slow processing\nspeeds. To address this challenge, we present HiT, a novel family of efficient\ntracking models that achieve high performance while maintaining fast operation\nacross various devices. The core innovation of HiT lies in its Bridge Module,\nwhich connects lightweight transformers to the tracking framework, enhancing\nfeature representation quality. Additionally, we introduce a dual-image\nposition encoding approach to effectively encode spatial information. HiT\nachieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson\nAGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,\noutperforming all previous efficient trackers.Building on HiT, we propose\nDyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by\nselecting routes with varying computational requirements. DyHiT uses search\narea features extracted by the backbone network and inputs them into an\nefficient dynamic router to classify tracking scenarios. Based on the\nclassification, DyHiT applies a divide-and-conquer strategy, selecting\nappropriate routes to achieve a superior trade-off between accuracy and speed.\nThe fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while\nmaintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free\nacceleration method based on the dynamic routing architecture of DyHiT. This\nmethod significantly improves the execution speed of various high-performance\ntrackers without sacrificing accuracy. For instance, our acceleration method\nenables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times\nspeedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of\n69.9% on the LaSOT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u5206\u5c42ViT\u548c\u52a8\u6001\u6846\u67b6\u7684\u9ad8\u6548\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5HiT\u548cDyHiT\uff0c\u901a\u8fc7Bridge Module\u548c\u53cc\u56fe\u50cf\u4f4d\u7f6e\u7f16\u7801\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u53d6\u5f97\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8ddf\u8e2a\u5668\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u6027\u56e0\u5904\u7406\u901f\u5ea6\u6162\u800c\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u9ad8\u6548\u7684\u8ddf\u8e2a\u6a21\u578b\uff0c\u517c\u987e\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u8fd0\u884c\u3002", "method": "HiT\u901a\u8fc7Bridge Module\u8fde\u63a5\u8f7b\u91cf\u7ea7Transformer\u4e0e\u8ddf\u8e2a\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u53cc\u56fe\u50cf\u4f4d\u7f6e\u7f16\u7801\uff1bDyHiT\u5219\u901a\u8fc7\u52a8\u6001\u8def\u7531\u5206\u7c7b\u573a\u666f\u5e76\u9009\u62e9\u8ba1\u7b97\u8def\u5f84\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u901f\u5ea6\u7684\u5e73\u8861\u3002", "result": "HiT\u5728NVIDIA Jetson AGX\u4e0a\u8fbe\u523061 fps\uff0cLaSOT\u57fa\u51c6\u6d4b\u8bd5AUC\u4e3a64.6%\uff1bDyHiT\u6700\u5feb\u7248\u672c\u8fbe111 fps\uff0cAUC\u4e3a62.4%\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5176\u4ed6\u9ad8\u6027\u80fd\u8ddf\u8e2a\u5668\u7684\u901f\u5ea6\u3002", "conclusion": "HiT\u548cDyHiT\u5728\u9ad8\u6548\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u52a8\u6001\u8def\u7531\u7b56\u7565\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u52a0\u901f\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u5229\u7528\u8f7b\u91cf\u7ea7\u5206\u5c42ViT\u548c\u52a8\u6001\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u89c6\u89c9\u8ddf\u8e2a", "abstract_zh": "\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8ddf\u8e2a\u5668\u56e0\u5176\u5f3a\u5927\u7684\u5efa\u6a21\u80fd\u529b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u6027\u56e0\u5904\u7406\u901f\u5ea6\u6162\u800c\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86HiT\uff0c\u4e00\u79cd\u65b0\u578b\u9ad8\u6548\u8ddf\u8e2a\u6a21\u578b\u5bb6\u65cf\uff0c\u80fd\u591f\u5728\u591a\u79cd\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u8fd0\u884c\u3002HiT\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176Bridge Module\uff0c\u8be5\u6a21\u5757\u5c06\u8f7b\u91cf\u7ea7Transformer\u4e0e\u8ddf\u8e2a\u6846\u67b6\u8fde\u63a5\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u8868\u793a\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53cc\u56fe\u50cf\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u4ee5\u6709\u6548\u7f16\u7801\u7a7a\u95f4\u4fe1\u606f\u3002HiT\u5728NVIDIA Jetson AGX\u5e73\u53f0\u4e0a\u8fbe\u523061\u5e27\u6bcf\u79d2\uff08fps\uff09\u7684\u901f\u5ea6\uff0c\u5e76\u5728LaSOT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f9764.6%\u7684AUC\uff0c\u4f18\u4e8e\u6240\u6709\u5148\u524d\u7684\u9ad8\u6548\u8ddf\u8e2a\u5668\u3002\u57fa\u4e8eHiT\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86DyHiT\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u52a8\u6001\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u9009\u62e9\u4e0d\u540c\u8ba1\u7b97\u9700\u6c42\u7684\u8def\u5f84\u7075\u6d3b\u9002\u5e94\u573a\u666f\u590d\u6742\u5ea6\u3002DyHiT\u5229\u7528\u4e3b\u5e72\u7f51\u7edc\u63d0\u53d6\u7684\u641c\u7d22\u533a\u57df\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u8f93\u5165\u9ad8\u6548\u7684\u52a8\u6001\u8def\u7531\u5668\u4ee5\u5206\u7c7b\u8ddf\u8e2a\u573a\u666f\u3002\u6839\u636e\u5206\u7c7b\u7ed3\u679c\uff0cDyHiT\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u9009\u62e9\u5408\u9002\u8def\u5f84\u4ee5\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u901f\u5ea6\u7684\u4f18\u8d8a\u5e73\u8861\u3002DyHiT\u7684\u6700\u5feb\u7248\u672c\u5728NVIDIA Jetson AGX\u4e0a\u8fbe\u5230111 fps\uff0c\u540c\u65f6\u4fdd\u6301LaSOT\u4e0a62.4%\u7684AUC\u3002\u6b64\u5916\uff0c\u6211\u4eec\u57fa\u4e8eDyHiT\u7684\u52a8\u6001\u8def\u7531\u67b6\u6784\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u9ad8\u6027\u80fd\u8ddf\u8e2a\u5668\u7684\u6267\u884c\u901f\u5ea6\uff0c\u4e14\u4e0d\u727a\u7272\u7cbe\u5ea6\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u52a0\u901f\u65b9\u6cd5\u4f7f\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668SeqTrack-B256\u5728NVIDIA GeForce RTX 2080 Ti GPU\u4e0a\u5b9e\u73b0\u4e862.68\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301LaSOT\u4e0a69.9%\u7684AUC\u3002"}}
{"id": "2506.19870", "pdf": "https://arxiv.org/pdf/2506.19870", "abs": "https://arxiv.org/abs/2506.19870", "authors": ["Md Asif Ul Hoq Khan", "MD Zahedul Islam", "Istiaq Ahmed", "Md Masud Karim Rabbi", "Farhana Rahman Anonna", "MD Abdul Fahim Zeeshan", "Mehedi Hasan Ridoy", "Bivash Ranjan Chowdhury", "Md Nazmul Shakir Rabbi", "GM Alamin Sadnan"], "title": "Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Peer-to-peer trading and the move to decentralized grids have reshaped the\nenergy markets in the United States. Notwithstanding, such developments lead to\nnew challenges, mainly regarding the safety and authenticity of energy trade.\nThis study aimed to develop and build a secure, intelligent, and efficient\nenergy transaction system for the decentralized US energy market. This research\ninterlinks the technological prowess of blockchain and artificial intelligence\n(AI) in a novel way to solve long-standing challenges in the distributed energy\nmarket, specifically those of security, fraudulent behavior detection, and\nmarket reliability. The dataset for this research is comprised of more than 1.2\nmillion anonymized energy transaction records from a simulated peer-to-peer\n(P2P) energy exchange network emulating real-life blockchain-based American\nmicrogrids, including those tested by LO3 Energy and Grid+ Labs. Each record\ncontains detailed fields of transaction identifier, timestamp, energy volume\n(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier\n(hashed for privacy), smart meter readings, geolocation regions, and settlement\nconfirmation status. The dataset also includes system-calculated behavior\nmetrics of transaction rate, variability of energy production, and historical\npricing patterns. The system architecture proposed involves the integration of\ntwo layers, namely a blockchain layer and artificial intelligence (AI) layer,\neach playing a unique but complementary function in energy transaction securing\nand market intelligence improvement. The machine learning models used in this\nresearch were specifically chosen for their established high performance in\nclassification tasks, specifically in the identification of energy transaction\nfraud in decentralized markets.", "AI": {"tldr": "\u672c\u7814\u7a76\u7ed3\u5408\u533a\u5757\u94fe\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u3001\u667a\u80fd\u4e14\u9ad8\u6548\u7684\u80fd\u6e90\u4ea4\u6613\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u5e02\u573a\u4e2d\u7684\u5b89\u5168\u548c\u6b3a\u8bc8\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u70b9\u5bf9\u70b9\u4ea4\u6613\u548c\u53bb\u4e2d\u5fc3\u5316\u7535\u7f51\u7684\u53d1\u5c55\uff0c\u7f8e\u56fd\u80fd\u6e90\u5e02\u573a\u9762\u4e34\u65b0\u7684\u5b89\u5168\u548c\u771f\u5b9e\u6027\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u533a\u5757\u94fe\u548c\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u5e02\u573a\u4e2d\u7684\u5b89\u5168\u3001\u6b3a\u8bc8\u68c0\u6d4b\u548c\u5e02\u573a\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ec\u533a\u5757\u94fe\u5c42\u548c\u4eba\u5de5\u667a\u80fd\u5c42\u3002\u533a\u5757\u94fe\u5c42\u7528\u4e8e\u786e\u4fdd\u4ea4\u6613\u5b89\u5168\uff0c\u4eba\u5de5\u667a\u80fd\u5c42\u5219\u7528\u4e8e\u68c0\u6d4b\u6b3a\u8bc8\u884c\u4e3a\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u5305\u542b120\u4e07\u6761\u533f\u540d\u80fd\u6e90\u4ea4\u6613\u8bb0\u5f55\u7684\u6570\u636e\u96c6\uff0c\u5e76\u9009\u62e9\u4e86\u9ad8\u6027\u80fd\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u6574\u5408\u4e86\u533a\u5757\u94fe\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u80fd\u6e90\u4ea4\u6613\u7684\u5b89\u5168\u6027\u548c\u5e02\u573a\u7a33\u5b9a\u6027\uff0c\u5e76\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u6b3a\u8bc8\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u533a\u5757\u94fe\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u5e02\u573a\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7684\u80fd\u6e90\u4ea4\u6613\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5229\u7528\u533a\u5757\u94fe\u548c\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u5b89\u5168\u80fd\u6e90\u4ea4\u6613\uff1a\u6b3a\u8bc8\u68c0\u6d4b\u4e0e\u80fd\u6e90\u5e02\u573a\u7a33\u5b9a\u6027", "abstract_zh": "\u70b9\u5bf9\u70b9\u4ea4\u6613\u548c\u5411\u53bb\u4e2d\u5fc3\u5316\u7535\u7f51\u7684\u8f6c\u53d8\u91cd\u5851\u4e86\u7f8e\u56fd\u7684\u80fd\u6e90\u5e02\u573a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u53d1\u5c55\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u80fd\u6e90\u4ea4\u6613\u7684\u5b89\u5168\u6027\u548c\u771f\u5b9e\u6027\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u7f8e\u56fd\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u5e02\u573a\u5f00\u53d1\u4e00\u79cd\u5b89\u5168\u3001\u667a\u80fd\u4e14\u9ad8\u6548\u7684\u80fd\u6e90\u4ea4\u6613\u7cfb\u7edf\u3002\u7814\u7a76\u521b\u65b0\u6027\u5730\u7ed3\u5408\u4e86\u533a\u5757\u94fe\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u5e02\u573a\u4e2d\u957f\u671f\u5b58\u5728\u7684\u5b89\u5168\u3001\u6b3a\u8bc8\u68c0\u6d4b\u548c\u5e02\u573a\u53ef\u9760\u6027\u95ee\u9898\u3002\u7814\u7a76\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7120\u4e07\u6761\u6765\u81ea\u6a21\u62df\u70b9\u5bf9\u70b9\u80fd\u6e90\u4ea4\u6613\u7f51\u7edc\u7684\u533f\u540d\u4ea4\u6613\u8bb0\u5f55\uff0c\u6a21\u62df\u4e86\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u7f8e\u56fd\u5fae\u7535\u7f51\uff08\u5982LO3 Energy\u548cGrid+ Labs\u6d4b\u8bd5\u7684\u7f51\u7edc\uff09\u3002\u6bcf\u6761\u8bb0\u5f55\u5305\u542b\u4ea4\u6613\u6807\u8bc6\u7b26\u3001\u65f6\u95f4\u6233\u3001\u80fd\u6e90\u91cf\uff08kWh\uff09\u3001\u4ea4\u6613\u7c7b\u578b\uff08\u4e70/\u5356\uff09\u3001\u5355\u4ef7\u3001\u4ea7\u6d88\u8005/\u6d88\u8d39\u8005\u6807\u8bc6\u7b26\uff08\u54c8\u5e0c\u5904\u7406\u4ee5\u4fdd\u62a4\u9690\u79c1\uff09\u3001\u667a\u80fd\u7535\u8868\u8bfb\u6570\u3001\u5730\u7406\u533a\u57df\u548c\u7ed3\u7b97\u786e\u8ba4\u72b6\u6001\u7b49\u8be6\u7ec6\u5b57\u6bb5\u3002\u6570\u636e\u96c6\u8fd8\u5305\u62ec\u7cfb\u7edf\u8ba1\u7b97\u7684\u884c\u4e3a\u6307\u6807\uff0c\u5982\u4ea4\u6613\u9891\u7387\u3001\u80fd\u6e90\u751f\u4ea7\u6ce2\u52a8\u6027\u548c\u5386\u53f2\u5b9a\u4ef7\u6a21\u5f0f\u3002\u63d0\u51fa\u7684\u7cfb\u7edf\u67b6\u6784\u5305\u542b\u533a\u5757\u94fe\u5c42\u548c\u4eba\u5de5\u667a\u80fd\u5c42\uff0c\u4e24\u8005\u5728\u786e\u4fdd\u80fd\u6e90\u4ea4\u6613\u5b89\u5168\u548c\u63d0\u5347\u5e02\u573a\u667a\u80fd\u65b9\u9762\u53d1\u6325\u72ec\u7279\u4e14\u4e92\u8865\u7684\u4f5c\u7528\u3002\u7814\u7a76\u4e2d\u9009\u7528\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u56e0\u5176\u5728\u5206\u7c7b\u4efb\u52a1\uff08\u5c24\u5176\u662f\u8bc6\u522b\u53bb\u4e2d\u5fc3\u5316\u5e02\u573a\u4e2d\u7684\u80fd\u6e90\u4ea4\u6613\u6b3a\u8bc8\uff09\u4e2d\u7684\u9ad8\u6027\u80fd\u800c\u7279\u522b\u9002\u5408\u3002"}}
{"id": "2506.20544", "pdf": "https://arxiv.org/pdf/2506.20544", "abs": "https://arxiv.org/abs/2506.20544", "authors": ["Ammar Khairi", "Daniel D'souza", "Ye Shen", "Julia Kreutzer", "Sara Hooker"], "title": "When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u9002\u5e94\u591a\u8bed\u8a00\u548c\u4efb\u52a1\u7684\u91c7\u6837\u4e0e\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u63a8\u7406\u8ba1\u7b97\u6269\u5c55\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u548c\u5c11\u6570\u9886\u57df\uff08\u5982\u6570\u5b66\u548c\u7f16\u7a0b\uff09\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u9002\u7528\u4e8e\u5f00\u653e\u4efb\u52a1\u548c\u591a\u8bed\u8a00\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u591a\u6837\u5316\u8bed\u8a00\u548c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6e29\u5ea6\u53d8\u5316\u7684\u91c7\u6837\u7b56\u7565\u548c\u4efb\u52a1\u611f\u77e5\u7684\u9009\u62e9\u7b56\u7565\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u9009\u62e9\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fdb\u800c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u57288B\u6a21\u578b\u4e0a\u4f7fm-ArenaHard-v2.0\u4efb\u52a1\u7684\u80dc\u7387\u5e73\u5747\u63d0\u53476.8%\uff0c\u5728111B\u6a21\u578b\u4e0a\u4ec5\u75285\u4e2a\u6837\u672c\u5c31\u5b9e\u73b0\u4e869.0%\u7684\u80dc\u7387\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6837\u672c\u89e3\u7801\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bed\u8a00\u548c\u4efb\u52a1\u611f\u77e5\u7684\u63a8\u7406\u8ba1\u7b97\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u63d0\u5347\u975e\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "paper_title_zh": "\u5f53\u751f\u6d3b\u7ed9\u4f60\u6837\u672c\uff1a\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8ba1\u7b97\u6269\u5c55\u7684\u76ca\u5904", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7814\u7a76\u91cd\u70b9\u8f6c\u5411\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\uff0c\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002\u5e38\u89c1\u65b9\u6cd5\u5305\u62ec\u5e76\u884c\u91c7\u6837\u591a\u4e2a\u8f93\u51fa\u5e76\u9009\u62e9\u5176\u4e2d\u4e4b\u4e00\u4f5c\u4e3a\u6700\u7ec8\u7ed3\u679c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\u548c\u5c11\u6570\u9886\u57df\uff08\u5982\u6570\u5b66\u548c\u7f16\u7a0b\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u66f4\u5173\u6ce8\u9002\u7528\u4e8e\u5f00\u653e\u4efb\u52a1\u3001\u53ef\u9a8c\u8bc1\u4efb\u52a1\u53ca\u591a\u8bed\u8a00\u7684\u901a\u7528\u6280\u672f\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728\u591a\u8bed\u8a00\u3001\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u5982\u4f55\u7a33\u5065\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u4ee5\u652f\u6301\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u3002\n  \u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u6837\u7b56\u7565\uff08\u57fa\u4e8e\u6e29\u5ea6\u53d8\u5316\uff09\u548c\u9009\u62e9\u7b56\u7565\u5747\u9700\u9488\u5bf9\u591a\u6837\u5316\u9886\u57df\u548c\u8bed\u8a00\u8fdb\u884c\u8c03\u6574\u3002\u6211\u4eec\u5bf9\u73b0\u6709\u9009\u62e9\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u9002\u7528\u4e8e\u82f1\u8bed\u7684\u7b56\u7565\u5f80\u5f80\u65e0\u6cd5\u63a8\u5e7f\u5230\u5176\u4ed6\u8bed\u8a00\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u573a\u666f\u7684\u91c7\u6837\u4e0e\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u8bed\u8a00\u548c\u4efb\u52a1\u4e2d\u5747\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f8B\u6a21\u578b\u5728m-ArenaHard-v2.0\u4efb\u52a1\u4e2d\u7684\u80dc\u7387\u5e73\u5747\u63d0\u53476.8%\uff0c\u4f18\u4e8eGemini\u7b49\u4e13\u6709\u6a21\u578b\u3002\u5728\u66f4\u5927\u89c4\u6a21\u7684111B\u6a21\u578b\uff08Command-A\uff09\u4e0a\uff0c\u4ec5\u75285\u4e2a\u6837\u672c\u5c31\u5b9e\u73b0\u4e869.0%\u7684\u80dc\u7387\u63d0\u5347\uff0c\u6210\u672c\u6781\u4f4e\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u8bed\u8a00\u548c\u4efb\u52a1\u611f\u77e5\u7684\u63a8\u7406\u8ba1\u7b97\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u65e8\u5728\u4e3a\u975e\u4e3b\u6d41\u8bed\u8a00\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.20388", "pdf": "https://arxiv.org/pdf/2506.20388", "abs": "https://arxiv.org/abs/2506.20388", "authors": ["Shen Tan", "Xin Zhang", "Liangxiu Han", "Huaguo Huang", "Han Wang"], "title": "A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management", "categories": ["cs.CV"], "comment": null, "summary": "Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)\nis crucial for supporting local livelihoods and carbon sequestration\ninitiatives like the China Certified Emission Reduction (CCER) program.\nHigh-resolution canopy height maps (CHMs) are essential for this, but standard\nlidar-based methods are expensive. While deep learning with RGB imagery offers\nan alternative, accurately extracting canopy height features remains\nchallenging. To address this, we developed a novel model for high-resolution\nCHM generation using a Large Vision Foundation Model (LVFM). Our model\nintegrates a feature extractor, a self-supervised feature enhancement module to\npreserve spatial details, and a height estimator. Tested in Beijing's Fangshan\nDistrict using 1-meter Google Earth imagery, our model outperformed existing\nmethods, including conventional CNNs. It achieved a mean absolute error of 0.09\nm, a root mean square error of 0.24 m, and a correlation of 0.78 against\nlidar-based CHMs. The resulting CHMs enabled over 90% success in individual\ntree detection, high accuracy in AGB estimation, and effective tracking of\nplantation growth, demonstrating strong generalization to non-training areas.\nThis approach presents a promising, scalable tool for evaluating carbon\nsequestration in both plantations and natural forests.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08LVFM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u51a0\u5c42\u9ad8\u5ea6\u56fe\uff08CHM\uff09\uff0c\u4ee5\u652f\u6301\u7cbe\u51c6\u6797\u4e1a\u7ba1\u7406\u3002\u8be5\u65b9\u6cd5\u5728\u5317\u4eac\u5e02\u623f\u5c71\u533a\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7cbe\u786e\u4e14\u7ecf\u6d4e\u5730\u76d1\u6d4b\u4eba\u5de5\u6797\u5730\u4e0a\u751f\u7269\u91cf\uff08AGB\uff09\u5bf9\u4e8e\u652f\u6301\u5f53\u5730\u751f\u8ba1\u548c\u78b3\u6c47\u8ba1\u5212\uff08\u5982\u4e2d\u56fd\u6838\u8bc1\u51cf\u6392\u91cfCCER\uff09\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6fc0\u5149\u96f7\u8fbe\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u800c\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u63d0\u53d6\u51a0\u5c42\u9ad8\u5ea6\u7279\u5f81\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7279\u5f81\u63d0\u53d6\u5668\u3001\u81ea\u76d1\u7763\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u5ea6\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08LVFM\uff09\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u51a0\u5c42\u9ad8\u5ea6\u56fe\u3002", "result": "\u5728\u5317\u4eac\u5e02\u623f\u5c71\u533a\u4f7f\u75281\u7c73\u5206\u8fa8\u7387\u7684Google Earth\u56fe\u50cf\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8be5\u6a21\u578b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.09\u7c73\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.24\u7c73\uff0c\u4e0e\u6fc0\u5149\u96f7\u8fbe\u751f\u6210\u7684\u51a0\u5c42\u9ad8\u5ea6\u56fe\u7684\u76f8\u5173\u7cfb\u6570\u4e3a0.78\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6728\u68c0\u6d4b\u4e2d\u6210\u529f\u7387\u8d85\u8fc790%\uff0cAGB\u4f30\u7b97\u7cbe\u5ea6\u9ad8\uff0c\u5e76\u80fd\u6709\u6548\u8ddf\u8e2a\u4eba\u5de5\u6797\u751f\u957f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u4eba\u5de5\u6797\u548c\u5929\u7136\u6797\u7684\u78b3\u6c47\u6f5c\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "paper_title_zh": "\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08LVFM\uff09\u7684\u9ad8\u5206\u8fa8\u7387\u4eba\u5de5\u6797\u51a0\u5c42\u9ad8\u5ea6\u56fe\u751f\u6210\u65b9\u6cd5\u53ca\u5176\u5728\u7cbe\u51c6\u6797\u4e1a\u7ba1\u7406\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u7cbe\u786e\u4e14\u7ecf\u6d4e\u5730\u76d1\u6d4b\u4eba\u5de5\u6797\u5730\u4e0a\u751f\u7269\u91cf\uff08AGB\uff09\u5bf9\u4e8e\u652f\u6301\u5f53\u5730\u751f\u8ba1\u548c\u78b3\u6c47\u8ba1\u5212\uff08\u5982\u4e2d\u56fd\u6838\u8bc1\u51cf\u6392\u91cfCCER\uff09\u81f3\u5173\u91cd\u8981\u3002\u9ad8\u5206\u8fa8\u7387\u51a0\u5c42\u9ad8\u5ea6\u56fe\uff08CHM\uff09\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u7684\u6fc0\u5149\u96f7\u8fbe\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u3002\u5c3d\u7ba1\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u51c6\u786e\u63d0\u53d6\u51a0\u5c42\u9ad8\u5ea6\u7279\u5f81\u4ecd\u5177\u6311\u6218\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08LVFM\uff09\u7684\u65b0\u578b\u9ad8\u5206\u8fa8\u7387CHM\u751f\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u7279\u5f81\u63d0\u53d6\u5668\u3001\u81ea\u76d1\u7763\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u5ea6\u4f30\u8ba1\u5668\u3002\u5728\u5317\u4eac\u5e02\u623f\u5c71\u533a\u4f7f\u75281\u7c73\u5206\u8fa8\u7387\u7684Google Earth\u56fe\u50cf\u8fdb\u884c\u6d4b\u8bd5\u65f6\uff0c\u8be5\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u5176\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.09\u7c73\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.24\u7c73\uff0c\u4e0e\u6fc0\u5149\u96f7\u8fbe\u751f\u6210\u7684CHM\u7684\u76f8\u5173\u7cfb\u6570\u4e3a0.78\u3002\u751f\u6210\u7684CHM\u5728\u5355\u6728\u68c0\u6d4b\u4e2d\u6210\u529f\u7387\u8d85\u8fc790%\uff0cAGB\u4f30\u7b97\u7cbe\u5ea6\u9ad8\uff0c\u5e76\u80fd\u6709\u6548\u8ddf\u8e2a\u4eba\u5de5\u6797\u751f\u957f\uff0c\u663e\u793a\u51fa\u5bf9\u975e\u8bad\u7ec3\u533a\u57df\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u4eba\u5de5\u6797\u548c\u5929\u7136\u6797\u7684\u78b3\u6c47\u6f5c\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2506.19871", "pdf": "https://arxiv.org/pdf/2506.19871", "abs": "https://arxiv.org/abs/2506.19871", "authors": ["Yining Pang", "Chenghan Li"], "title": "An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network", "categories": ["cs.CR", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2405.12076 by other authors", "summary": "Insurance fraud detection represents a pivotal advancement in modern\ninsurance service, providing intelligent and digitalized monitoring to enhance\nmanagement and prevent fraud. It is crucial for ensuring the security and\nefficiency of insurance systems. Although AI and machine learning algorithms\nhave demonstrated strong performance in detecting fraudulent claims, the\nabsence of standardized defense mechanisms renders current systems vulnerable\nto emerging adversarial threats. In this paper, we propose a GAN-based approach\nto conduct adversarial attacks on fraud detection systems. Our results indicate\nthat an attacker, without knowledge of the training data or internal model\ndetails, can generate fraudulent cases that are classified as legitimate with a\n99\\% attack success rate (ASR). By subtly modifying real insurance records and\nclaims, adversaries can significantly increase the fraud risk, potentially\nbypassing compromised detection systems. These findings underscore the urgent\nneed to enhance the robustness of insurance fraud detection models against\nadversarial manipulation, thereby ensuring the stability and reliability of\ndifferent insurance systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u533b\u7597\u4fdd\u9669\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u653b\u51fb\u8005\u65e0\u9700\u4e86\u89e3\u8bad\u7ec3\u6570\u636e\u6216\u6a21\u578b\u7ec6\u8282\uff0c\u5373\u53ef\u751f\u6210\u88ab\u8bef\u5224\u4e3a\u5408\u6cd5\u7684\u6b3a\u8bc8\u6848\u4f8b\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe99%\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u4fdd\u9669\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u867d\u4f9d\u8d56AI\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u9632\u5fa1\u673a\u5236\uff0c\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u7cfb\u7edf\u7684\u8106\u5f31\u6027\uff0c\u5e76\u547c\u5401\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u5bf9\u6297\u6027\u6837\u672c\uff0c\u901a\u8fc7\u5fae\u8c03\u771f\u5b9e\u4fdd\u9669\u8bb0\u5f55\u548c\u7d22\u8d54\u6570\u636e\uff0c\u4f7f\u5176\u88ab\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u8bef\u5224\u4e3a\u5408\u6cd5\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u8fbe99%\uff0c\u8868\u660e\u73b0\u6709\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u6781\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u53ef\u80fd\u88ab\u7ed5\u8fc7\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u63d0\u5347\u4fdd\u9669\u6b3a\u8bc8\u68c0\u6d4b\u6a21\u578b\u5bf9\u6297\u6027\u64cd\u7eb5\u80fd\u529b\u7684\u7d27\u8feb\u6027\uff0c\u4ee5\u786e\u4fdd\u4fdd\u9669\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u533b\u7597\u4fdd\u9669\u6b3a\u8bc8\u68c0\u6d4b\u653b\u51fb\u65b9\u6cd5", "abstract_zh": "\u4fdd\u9669\u6b3a\u8bc8\u68c0\u6d4b\u662f\u73b0\u4ee3\u4fdd\u9669\u670d\u52a1\u4e2d\u7684\u5173\u952e\u8fdb\u6b65\uff0c\u901a\u8fc7\u667a\u80fd\u5316\u548c\u6570\u5b57\u5316\u76d1\u63a7\u63d0\u5347\u7ba1\u7406\u5e76\u9884\u9632\u6b3a\u8bc8\u884c\u4e3a\uff0c\u5bf9\u4fdd\u969c\u4fdd\u9669\u7cfb\u7edf\u7684\u5b89\u5168\u4e0e\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1AI\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u68c0\u6d4b\u6b3a\u8bc8\u7d22\u8d54\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u9632\u5fa1\u673a\u5236\u4f7f\u73b0\u6709\u7cfb\u7edf\u6613\u53d7\u65b0\u5174\u5bf9\u6297\u6027\u5a01\u80c1\u7684\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u65b9\u6cd5\uff0c\u5bf9\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u8fdb\u884c\u5bf9\u6297\u6027\u653b\u51fb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u653b\u51fb\u8005\u5728\u65e0\u9700\u4e86\u89e3\u8bad\u7ec3\u6570\u636e\u6216\u6a21\u578b\u5185\u90e8\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u751f\u6210\u88ab\u5206\u7c7b\u4e3a\u5408\u6cd5\u7684\u6b3a\u8bc8\u6848\u4f8b\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe99%\u3002\u901a\u8fc7\u5bf9\u771f\u5b9e\u4fdd\u9669\u8bb0\u5f55\u548c\u7d22\u8d54\u7684\u7ec6\u5fae\u4fee\u6539\uff0c\u653b\u51fb\u8005\u80fd\u663e\u8457\u589e\u52a0\u6b3a\u8bc8\u98ce\u9669\uff0c\u53ef\u80fd\u7ed5\u8fc7\u53d7\u635f\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u589e\u5f3a\u4fdd\u9669\u6b3a\u8bc8\u68c0\u6d4b\u6a21\u578b\u5bf9\u6297\u6027\u64cd\u7eb5\u80fd\u529b\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4ee5\u786e\u4fdd\u4e0d\u540c\u4fdd\u9669\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.20606", "pdf": "https://arxiv.org/pdf/2506.20606", "abs": "https://arxiv.org/abs/2506.20606", "authors": ["Baixiang Huang", "Zhen Tan", "Haoran Wang", "Zijie Liu", "Dawei Li", "Ali Payani", "Huan Liu", "Tianlong Chen", "Kai Shu"], "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm", "categories": ["cs.CL"], "comment": "Main paper: 9 pages; total: 18 pages (including appendix). Code,\n  data, results, and additional resources are available at:\n  https://model-editing.github.io", "summary": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff08\u884c\u4e3a\u7f16\u8f91\uff09\u52a8\u6001\u8c03\u6574\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u884c\u4e3a\uff0c\u65e2\u80fd\u5f15\u5bfc\u5176\u5411\u5584\uff0c\u4e5f\u53ef\u80fd\u8bf1\u5bfc\u5176\u4f5c\u6076\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u5c42\u7ea7\u57fa\u51c6BehaviorBench\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002", "motivation": "\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u9ad8\u98ce\u9669\u9886\u57df\u65f6\uff0c\u5176\u4e0d\u9053\u5fb7\u884c\u4e3a\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff08\u5982\u4eba\u8eab\u4f24\u5bb3\u6216\u8d22\u52a1\u635f\u5931\uff09\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u5f15\u5bfc\u4ee3\u7406\u7684\u4f26\u7406\u884c\u4e3a\uff0c\u540c\u65f6\u7814\u7a76\u6a21\u578b\u7f16\u8f91\u6280\u672f\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u7f16\u8f91\uff08Behavior Editing\uff09\u6280\u672f\uff0c\u901a\u8fc7\u7cbe\u786e\u4fee\u6539LLM\u6a21\u578b\u6765\u8c03\u6574\u4ee3\u7406\u884c\u4e3a\u3002\u5f15\u5165\u591a\u5c42\u7ea7\u57fa\u51c6BehaviorBench\uff0c\u57fa\u4e8e\u5fc3\u7406\u5b66\u9053\u5fb7\u7406\u8bba\u8bbe\u8ba1\u590d\u6742\u573a\u666f\uff0c\u652f\u6301\u884c\u4e3a\u8bc4\u4f30\u4e0e\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u884c\u4e3a\u7f16\u8f91\u80fd\u52a8\u6001\u5f15\u5bfc\u4ee3\u7406\u5728\u7279\u5b9a\u573a\u666f\u4e2d\u5b9e\u73b0\u76ee\u6807\u884c\u4e3a\uff0c\u5e76\u8c03\u6574\u5176\u5168\u5c40\u9053\u5fb7\u503e\u5411\u3002\u8be5\u65b9\u6cd5\u65e2\u53ef\u4fc3\u8fdb\u4f26\u7406\u884c\u4e3a\uff0c\u4e5f\u53ef\u80fd\u8bf1\u5bfc\u6076\u610f\u884c\u4e3a\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u548c\u573a\u666f\u4e2d\u5747\u6709\u6548\u3002", "conclusion": "\u884c\u4e3a\u7f16\u8f91\u4e3a\u4ee3\u7406\u884c\u4e3a\u5f15\u5bfc\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u4e0e\u98ce\u9669\uff0c\u9700\u8c28\u614e\u5e94\u7528\u3002BehaviorBench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\u3002", "paper_title_zh": "\u6a21\u578b\u7f16\u8f91\u7684\u53cc\u5203\u5251\uff1a\u5f15\u5bfc\u4ee3\u7406\u4f26\u7406\u884c\u4e3a\u5411\u5584\u6216\u4f5c\u6076", "abstract_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u90e8\u7f72\u4f34\u968f\u91cd\u5927\u5b89\u5168\u4e0e\u4f26\u7406\u98ce\u9669\u3002\u4ee3\u7406\u7684\u4e0d\u9053\u5fb7\u884c\u4e3a\u53ef\u80fd\u76f4\u63a5\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff08\u5982\u4eba\u8eab\u4f24\u5bb3\u6216\u8d22\u52a1\u635f\u5931\uff09\u3002\u4e3a\u9ad8\u6548\u5f15\u5bfc\u4ee3\u7406\u4f26\u7406\u884c\u4e3a\uff0c\u6211\u4eec\u5c06\u884c\u4e3a\u5f15\u5bfc\u5b9a\u4e49\u4e3a\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\uff08\u884c\u4e3a\u7f16\u8f91\uff09\u3002\u6a21\u578b\u7f16\u8f91\u662f\u65b0\u5174\u7814\u7a76\u9886\u57df\uff0c\u53ef\u5728\u4fdd\u7559LLM\u6574\u4f53\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u9ad8\u6548\u4fee\u6539\u3002\u4e3a\u7cfb\u7edf\u7814\u7a76\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u57fa\u4e8e\u5fc3\u7406\u5b66\u9053\u5fb7\u7406\u8bba\u7684\u591a\u5c42\u7ea7\u57fa\u51c6BehaviorBench\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u8bc4\u4f30\u4e0e\u7f16\u8f91\uff0c\u6bcf\u5c42\u7ea7\u573a\u666f\u590d\u6742\u5ea6\u9012\u589e\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u884c\u4e3a\u7f16\u8f91\u80fd\u52a8\u6001\u5f15\u5bfc\u4ee3\u7406\u5728\u7279\u5b9a\u573a\u666f\u4e2d\u5b9e\u73b0\u76ee\u6807\u884c\u4e3a\uff0c\u5e76\u8c03\u6574\u5176\u5168\u5c40\u9053\u5fb7\u503e\u5411\u3002\u8be5\u65b9\u6cd5\u65e2\u53ef\u4fc3\u8fdb\u4f26\u7406\u884c\u4e3a\uff0c\u4e5f\u53ef\u80fd\u8bf1\u5bfc\u6076\u610f\u884c\u4e3a\u3002\u901a\u8fc7\u5bf9\u524d\u6cbfLLM\u4ee3\u7406\u7684\u5168\u9762\u8bc4\u4f30\uff0cBehaviorBench\u9a8c\u8bc1\u4e86\u884c\u4e3a\u7f16\u8f91\u5728\u4e0d\u540c\u6a21\u578b\u548c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u4e3a\u4ee3\u7406\u884c\u4e3a\u5f15\u5bfc\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u884c\u4e3a\u7f16\u8f91\u7684\u6f5c\u529b\u4e0e\u98ce\u9669\u3002"}}
{"id": "2506.20449", "pdf": "https://arxiv.org/pdf/2506.20449", "abs": "https://arxiv.org/abs/2506.20449", "authors": ["Changlu Guo", "Anders Nymark Christensen", "Morten Rieger Hannemose"], "title": "Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation", "categories": ["cs.CV"], "comment": "The project is available at \\url{https://medart-ai.github.io}", "summary": "Text-to-image generative models have achieved remarkable breakthroughs in\nrecent years. However, their application in medical image generation still\nfaces significant challenges, including small dataset sizes, and scarcity of\nmedical textual data. To address these challenges, we propose Med-Art, a\nframework specifically designed for medical image generation with limited data.\nMed-Art leverages vision-language models to generate visual descriptions of\nmedical images which overcomes the scarcity of applicable medical textual data.\nMed-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\\alpha$,\nbased on the Diffusion Transformer (DiT), achieving high performance under\nlimited data. Furthermore, we propose an innovative Hybrid-Level Diffusion\nFine-tuning (HLDF) method, which enables pixel-level losses, effectively\naddressing issues such as overly saturated colors. We achieve state-of-the-art\nperformance on two medical image datasets, measured by FID, KID, and downstream\nclassification performance.", "AI": {"tldr": "Med-Art\u662f\u4e00\u79cd\u4e13\u4e3a\u533b\u5b66\u56fe\u50cf\u751f\u6210\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u6587\u672c\u6570\u636e\u7a00\u7f3a\u548c\u5c0f\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6df7\u5408\u7ea7\u6269\u6563\u5fae\u8c03\u65b9\u6cd5\uff08HLDF\uff09\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u6570\u636e\u96c6\u5c0f\u3001\u533b\u5b66\u6587\u672c\u6570\u636e\u7a00\u7f3a\u7b49\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u4e0b\u9ad8\u6548\u751f\u6210\u533b\u5b66\u56fe\u50cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Med-Art\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u533b\u5b66\u56fe\u50cf\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1b\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684PixArt-\u03b1\u6a21\u578b\u8fdb\u884c\u9002\u914d\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u7ea7\u6269\u6563\u5fae\u8c03\uff08HLDF\uff09\u65b9\u6cd5\uff0c\u4f18\u5316\u50cf\u7d20\u7ea7\u635f\u5931\uff0c\u907f\u514d\u989c\u8272\u8fc7\u9971\u548c\u7b49\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cMed-Art\u5728FID\u3001KID\u548c\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Med-Art\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u9886\u57df\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "Med-Art\uff1a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u4e8c\u7ef4\u533b\u5b66\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\uff0c\u4f46\u5176\u5728\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u533b\u5b66\u6587\u672c\u6570\u636e\u7a00\u7f3a\u7b49\u91cd\u5927\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Med-Art\uff0c\u4e00\u79cd\u4e13\u4e3a\u6709\u9650\u6570\u636e\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u8bbe\u8ba1\u7684\u6846\u67b6\u3002Med-Art\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u533b\u5b66\u56fe\u50cf\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u514b\u670d\u4e86\u533b\u5b66\u6587\u672c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002Med-Art\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u5bf9\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578bPixArt-\u03b1\u8fdb\u884c\u9002\u914d\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6df7\u5408\u7ea7\u6269\u6563\u5fae\u8c03\uff08HLDF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u635f\u5931\u6709\u6548\u89e3\u51b3\u4e86\u989c\u8272\u8fc7\u9971\u548c\u7b49\u95ee\u9898\u3002\u5728\u4e24\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cMed-Art\u5728FID\u3001KID\u548c\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.19874", "pdf": "https://arxiv.org/pdf/2506.19874", "abs": "https://arxiv.org/abs/2506.19874", "authors": ["Xing Yang", "Bingtao Wang", "Yuhao Wang", "Zimo Ji", "Terry Jingchen Zhang", "Wenyuan Jiang"], "title": "Towards Provable (In)Secure Model Weight Release Schemes", "categories": ["cs.CR", "cs.AI"], "comment": "8 pages, 2 figures", "summary": "Recent secure weight release schemes claim to enable open-source model\ndistribution while protecting model ownership and preventing misuse. However,\nthese approaches lack rigorous security foundations and provide only informal\nsecurity guarantees. Inspired by established works in cryptography, we\nformalize the security of weight release schemes by introducing several\nconcrete security definitions. We then demonstrate our definition's utility\nthrough a case study of TaylorMLP, a prominent secure weight release scheme.\nOur analysis reveals vulnerabilities that allow parameter extraction thus\nshowing that TaylorMLP fails to achieve its informal security goals. We hope\nthis work will advocate for rigorous research at the intersection of machine\nlearning and security communities and provide a blueprint for how future weight\nrelease schemes should be designed and evaluated.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e25\u683c\u7684\u5b89\u5168\u5b9a\u4e49\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6a21\u578b\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u7684\u5b89\u5168\u6027\uff0c\u5e76\u4ee5TaylorMLP\u4e3a\u4f8b\u63ed\u793a\u4e86\u5176\u6f0f\u6d1e\uff0c\u547c\u5401\u5728\u673a\u5668\u5b66\u4e60\u548c\u5b89\u5168\u9886\u57df\u8fdb\u884c\u66f4\u4e25\u8c28\u7684\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u7f3a\u4e4f\u4e25\u683c\u7684\u5b89\u5168\u7406\u8bba\u57fa\u7840\uff0c\u4ec5\u63d0\u4f9b\u975e\u6b63\u5f0f\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u5b89\u5168\u5b9a\u4e49\uff0c\u8bc4\u4f30\u73b0\u6709\u65b9\u6848\u7684\u5b9e\u9645\u5b89\u5168\u6027\u3002", "method": "\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5177\u4f53\u7684\u6a21\u578b\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u7684\u5b89\u5168\u5b9a\u4e49\uff0c\u968f\u540e\u4ee5TaylorMLP\u4e3a\u6848\u4f8b\uff0c\u901a\u8fc7\u5206\u6790\u5176\u6f0f\u6d1e\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5b9a\u4e49\u7684\u5b9e\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0TaylorMLP\u5b58\u5728\u53c2\u6570\u63d0\u53d6\u6f0f\u6d1e\uff0c\u672a\u80fd\u5b9e\u73b0\u5176\u975e\u6b63\u5f0f\u7684\u5b89\u5168\u76ee\u6807\uff0c\u8bc1\u660e\u4e86\u73b0\u6709\u65b9\u6848\u7684\u5b89\u5168\u7f3a\u9677\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u673a\u5668\u5b66\u4e60\u548c\u5b89\u5168\u4ea4\u53c9\u9886\u57df\u8fdb\u884c\u4e25\u8c28\u7814\u7a76\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002", "paper_title_zh": "\u8fc8\u5411\u53ef\u8bc1\u660e\uff08\u4e0d\uff09\u5b89\u5168\u7684\u6a21\u578b\u6743\u91cd\u53d1\u5e03\u65b9\u6848", "abstract_zh": "\u8fd1\u671f\u7684\u5b89\u5168\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u58f0\u79f0\u80fd\u591f\u5728\u5f00\u6e90\u6a21\u578b\u5206\u53d1\u7684\u540c\u65f6\u4fdd\u62a4\u6a21\u578b\u6240\u6709\u6743\u5e76\u9632\u6b62\u6ee5\u7528\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u5b89\u5168\u57fa\u7840\uff0c\u4ec5\u63d0\u4f9b\u975e\u6b63\u5f0f\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u53d7\u5bc6\u7801\u5b66\u7ecf\u5178\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u82e5\u5e72\u5177\u4f53\u7684\u5b89\u5168\u5b9a\u4e49\uff0c\u5f62\u5f0f\u5316\u4e86\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u7684\u5b89\u5168\u6027\u3002\u968f\u540e\uff0c\u6211\u4eec\u4ee5TaylorMLP\u8fd9\u4e00\u8457\u540d\u5b89\u5168\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u5b9a\u4e49\u7684\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5141\u8bb8\u53c2\u6570\u63d0\u53d6\u7684\u6f0f\u6d1e\uff0c\u8868\u660eTaylorMLP\u672a\u80fd\u5b9e\u73b0\u5176\u975e\u6b63\u5f0f\u7684\u5b89\u5168\u76ee\u6807\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u591f\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u548c\u5b89\u5168\u4ea4\u53c9\u9886\u57df\u7684\u4e25\u8c28\u7814\u7a76\uff0c\u5e76\u4e3a\u672a\u6765\u6743\u91cd\u53d1\u5e03\u65b9\u6848\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u84dd\u56fe\u3002"}}
{"id": "2506.20639", "pdf": "https://arxiv.org/pdf/2506.20639", "abs": "https://arxiv.org/abs/2506.20639", "authors": ["Shansan Gong", "Ruixiang Zhang", "Huangjie Zheng", "Jiatao Gu", "Navdeep Jaitly", "Lingpeng Kong", "Yizhe Zhang"], "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation", "categories": ["cs.CL"], "comment": "preprint", "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiffuCoder\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5176\u53bb\u566a\u8fc7\u7a0b\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDiffuCoder\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\u63d0\u5347\u4e864.4%\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u81ea\u56de\u5f52\u89e3\u7801\u7684\u4f9d\u8d56\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u56e0\u5176\u5168\u5c40\u89c4\u5212\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u7279\u6027\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u5bf9\u5176\u8bad\u7ec3\u548c\u63a8\u7406\u673a\u5236\u7684\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63ed\u793adLLM\u7684\u89e3\u7801\u884c\u4e3a\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a7B\u53c2\u6570\u7684dLLM\u6a21\u578bDiffuCoder\uff0c\u5206\u6790\u4e86\u5176\u89e3\u7801\u884c\u4e3a\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u540c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u65b9\u6848coupled-GRPO\uff0c\u4ee5\u51cf\u5c11\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u5dee\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "DiffuCoder\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u5347\u4e864.4%\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u81ea\u56de\u5f52\u56e0\u679c\u89e3\u7801\u7684\u4f9d\u8d56\u3002coupled-GRPO\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86dLLM\u7684\u751f\u6210\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u539f\u751fRL\u8bad\u7ec3\u6846\u67b6\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "paper_title_zh": "DiffuCoder\uff1a\u7406\u89e3\u5e76\u6539\u8fdb\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b", "abstract_zh": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u56e0\u5176\u53bb\u566a\u6a21\u578b\u4f5c\u7528\u4e8e\u6574\u4e2a\u5e8f\u5217\uff0c\u6210\u4e3a\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u7684\u6709\u529b\u66ff\u4ee3\u3002dLLM\u7684\u5168\u5c40\u89c4\u5212\u548c\u8fed\u4ee3\u4f18\u5316\u7279\u6027\u5c24\u5176\u9002\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u3002\u7136\u800c\uff0c\u5f53\u524ddLLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u673a\u5236\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u63ed\u793adLLM\u7684\u89e3\u7801\u884c\u4e3a\u5e76\u91ca\u653e\u5176\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u6211\u4eec\u7cfb\u7edf\u7814\u7a76\u4e86\u5176\u53bb\u566a\u8fc7\u7a0b\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a7B\u53c2\u6570\u7684dLLM\u6a21\u578bDiffuCoder\uff0c\u57fa\u4e8e130B\u4ee3\u7801\u6807\u8bb0\u3002\u901a\u8fc7\u8be5\u6a21\u578b\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5176\u89e3\u7801\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u4e0eAR\u6a21\u578b\u7684\u4e0d\u540c\u4e4b\u5904\uff1a\uff081\uff09dLLM\u65e0\u9700\u4f9d\u8d56\u534a\u81ea\u56de\u5f52\u89e3\u7801\u5373\u53ef\u51b3\u5b9a\u751f\u6210\u7684\u56e0\u679c\u6027\uff1b\uff082\uff09\u63d0\u9ad8\u91c7\u6837\u6e29\u5ea6\u4e0d\u4ec5\u591a\u6837\u5316\u6807\u8bb0\u9009\u62e9\uff0c\u8fd8\u591a\u6837\u5316\u5176\u751f\u6210\u987a\u5e8f\u3002\u8fd9\u79cd\u591a\u6837\u6027\u4e3aRL\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u5728RL\u8bad\u7ec3\u4e2d\uff0c\u4e3a\u51cf\u5c11\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u5dee\u5e76\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86coupled-GRPO\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u65b9\u6848\uff0c\u4e3a\u8bad\u7ec3\u4e2d\u7684\u8865\u5168\u6784\u9020\u4e92\u8865\u63a9\u7801\u566a\u58f0\u3002\u5b9e\u9a8c\u8868\u660e\uff0ccoupled-GRPO\u663e\u8457\u63d0\u5347\u4e86DiffuCoder\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff08EvalPlus\u4e0a\u63d0\u53474.4%\uff09\uff0c\u5e76\u51cf\u5c11\u4e86\u89e3\u7801\u65f6\u5bf9AR\u56e0\u679c\u6027\u7684\u4f9d\u8d56\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u6df1\u5165\u63ed\u793a\u4e86dLLM\u751f\u6210\u7684\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u539f\u751fRL\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2506.20452", "pdf": "https://arxiv.org/pdf/2506.20452", "abs": "https://arxiv.org/abs/2506.20452", "authors": ["Tobias Vontobel", "Seyedmorteza Sadat", "Farnood Salehi", "Romann M. Weber"], "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models have emerged as the leading approach for image synthesis,\ndemonstrating exceptional photorealism and diversity. However, training\ndiffusion models at high resolutions remains computationally prohibitive, and\nexisting zero-shot generation techniques for synthesizing images beyond\ntraining resolutions often produce artifacts, including object duplication and\nspatial incoherence. In this paper, we introduce HiWave, a training-free,\nzero-shot approach that substantially enhances visual fidelity and structural\ncoherence in ultra-high-resolution image synthesis using pretrained diffusion\nmodels. Our method employs a two-stage pipeline: generating a base image from\nthe pretrained model followed by a patch-wise DDIM inversion step and a novel\nwavelet-based detail enhancer module. Specifically, we first utilize inversion\nmethods to derive initial noise vectors that preserve global coherence from the\nbase image. Subsequently, during sampling, our wavelet-domain detail enhancer\nretains low-frequency components from the base image to ensure structural\nconsistency, while selectively guiding high-frequency components to enrich fine\ndetails and textures. Extensive evaluations using Stable Diffusion XL\ndemonstrate that HiWave effectively mitigates common visual artifacts seen in\nprior methods, achieving superior perceptual quality. A user study confirmed\nHiWave's performance, where it was preferred over the state-of-the-art\nalternative in more than 80% of comparisons, highlighting its effectiveness for\nhigh-quality, ultra-high-resolution image synthesis without requiring\nretraining or architectural modifications.", "AI": {"tldr": "HiWave\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5c0f\u6ce2\u7684\u6269\u6563\u91c7\u6837\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u8d85\u9ad8\u6e05\u56fe\u50cf\u5408\u6210\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u96f6\u6837\u672c\u751f\u6210\u6280\u672f\u6613\u4ea7\u751f\u4f2a\u5f71\u7684\u95ee\u9898\uff0cHiWave\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u4f9b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u8d85\u9ad8\u6e05\u56fe\u50cf\u751f\u6210\u65b9\u6848\u3002", "method": "HiWave\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u751f\u6210\u57fa\u7840\u56fe\u50cf\uff0c\u968f\u540e\u901a\u8fc7\u57fa\u4e8e\u5c0f\u6ce2\u7684\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\uff0c\u4fdd\u7559\u4f4e\u9891\u6210\u5206\u4ee5\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u9009\u62e9\u6027\u589e\u5f3a\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiWave\u5728Stable Diffusion XL\u4e0a\u6709\u6548\u51cf\u5c11\u4e86\u4f2a\u5f71\uff0c\u7528\u6237\u7814\u7a76\u4e2d80%\u4ee5\u4e0a\u7684\u5bf9\u6bd4\u6848\u4f8b\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "HiWave\u4e3a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u7684\u8d85\u9ad8\u6e05\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "HiWave\uff1a\u57fa\u4e8e\u5c0f\u6ce2\u6269\u6563\u91c7\u6837\u7684\u514d\u8bad\u7ec3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210", "abstract_zh": "\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u56fe\u50cf\u5408\u6210\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u903c\u771f\u6027\u548c\u591a\u6837\u6027\u3002\u7136\u800c\uff0c\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u800c\u73b0\u6709\u7684\u96f6\u6837\u672c\u751f\u6210\u6280\u672f\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u8fa8\u7387\u65f6\u5f80\u5f80\u4ea7\u751f\u4f2a\u5f71\uff0c\u5982\u7269\u4f53\u91cd\u590d\u548c\u7a7a\u95f4\u4e0d\u8fde\u8d2f\u3002\u672c\u6587\u63d0\u51faHiWave\uff0c\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u5347\u8d85\u9ad8\u6e05\u56fe\u50cf\u5408\u6210\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u751f\u6210\u57fa\u7840\u56fe\u50cf\uff0c\u968f\u540e\u8fdb\u884c\u57fa\u4e8e\u5c0f\u6ce2\u7684\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\u5904\u7406\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u5229\u7528\u53cd\u6f14\u65b9\u6cd5\u4ece\u57fa\u7840\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u7684\u521d\u59cb\u566a\u58f0\u5411\u91cf\uff1b\u968f\u540e\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5c0f\u6ce2\u57df\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\u4fdd\u7559\u57fa\u7840\u56fe\u50cf\u7684\u4f4e\u9891\u6210\u5206\u4ee5\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u9009\u62e9\u6027\u5f15\u5bfc\u9ad8\u9891\u6210\u5206\u4ee5\u4e30\u5bcc\u7ec6\u8282\u548c\u7eb9\u7406\u3002\u5728Stable Diffusion XL\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cHiWave\u6709\u6548\u51cf\u5c11\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\u3002\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u4e86HiWave\u7684\u6027\u80fd\uff0c\u572880%\u4ee5\u4e0a\u7684\u5bf9\u6bd4\u6848\u4f8b\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u51f8\u663e\u4e86\u5176\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8d85\u9ad8\u6e05\u56fe\u50cf\u5408\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.19875", "pdf": "https://arxiv.org/pdf/2506.19875", "abs": "https://arxiv.org/abs/2506.19875", "authors": ["Taous Iatariene", "Can Cui", "Alexandre Gu\u00e9rin", "Romain Serizel"], "title": "Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "33rd European Signal Processing Conference (EUSIPCO 2025), Sep 2025,\n  Palerme (Italie), Italy", "summary": "Speaker tracking methods often rely on spatial observations to assign\ncoherent track identities over time. This raises limits in scenarios with\nintermittent and moving speakers, i.e., speakers that may change position when\nthey are inactive, thus leading to discontinuous spatial trajectories. This\npaper proposes to investigate the use of speaker embeddings, in a simple\nsolution to this issue. We propose to perform identity reassignment\npost-tracking, using speaker embeddings. We leverage trajectory-related\ninformation provided by an initial tracking step and multichannel audio signal.\nBeamforming is used to enhance the signal towards the speakers' positions in\norder to compute speaker embeddings. These are then used to assign new track\nidentities based on an enrollment pool. We evaluate the performance of the\nproposed speaker embedding-based identity reassignment method on a dataset\nwhere speakers change position during inactivity periods. Results show that it\nconsistently improves the identity assignment performance of neural and\nstandard tracking systems. In particular, we study the impact of beamforming\nand input duration for embedding extraction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6539\u8fdb\u95f4\u6b47\u6027\u548c\u79fb\u52a8\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u8ddf\u8e2a\u8eab\u4efd\u91cd\u65b0\u5206\u914d\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u89c2\u6d4b\uff0c\u4f46\u5728\u95f4\u6b47\u6027\u548c\u79fb\u52a8\u8bf4\u8bdd\u4eba\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u8bf4\u8bdd\u4eba\u4f4d\u7f6e\u53d8\u5316\u5bfc\u81f4\u8f68\u8ff9\u4e0d\u8fde\u7eed\uff0c\u8ddf\u8e2a\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7684\u8eab\u4efd\u91cd\u65b0\u5206\u914d\u65b9\u6cd5\uff0c\u5229\u7528\u521d\u59cb\u8ddf\u8e2a\u63d0\u4f9b\u7684\u8f68\u8ff9\u4fe1\u606f\u548c\u591a\u901a\u9053\u97f3\u9891\u4fe1\u53f7\uff0c\u901a\u8fc7\u6ce2\u675f\u6210\u5f62\u589e\u5f3a\u4fe1\u53f7\u5e76\u63d0\u53d6\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u6700\u540e\u57fa\u4e8e\u6ce8\u518c\u6c60\u5206\u914d\u65b0\u8f68\u8ff9\u8eab\u4efd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u548c\u6807\u51c6\u8ddf\u8e2a\u7cfb\u7edf\u7684\u8eab\u4efd\u5206\u914d\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u4e86\u6ce2\u675f\u6210\u5f62\u548c\u8f93\u5165\u65f6\u957f\u5bf9\u5d4c\u5165\u63d0\u53d6\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5728\u6539\u8fdb\u95f4\u6b47\u6027\u548c\u79fb\u52a8\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u8eab\u4efd\u5206\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5229\u7528\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6539\u8fdb\u95f4\u6b47\u6027\u548c\u79fb\u52a8\u8bf4\u8bdd\u4eba\u7684\u8ddf\u8e2a", "abstract_zh": "\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7a7a\u95f4\u89c2\u6d4b\u6765\u5206\u914d\u8fde\u8d2f\u7684\u8f68\u8ff9\u8eab\u4efd\uff0c\u4f46\u5728\u95f4\u6b47\u6027\u548c\u79fb\u52a8\u8bf4\u8bdd\u4eba\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u8bf4\u8bdd\u4eba\u5728\u975e\u6d3b\u52a8\u671f\u95f4\u53ef\u80fd\u6539\u53d8\u4f4d\u7f6e\uff0c\u5bfc\u81f4\u7a7a\u95f4\u8f68\u8ff9\u4e0d\u8fde\u7eed\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u5229\u7528\u8bf4\u8bdd\u4eba\u5d4c\u5165\u8fdb\u884c\u8eab\u4efd\u91cd\u65b0\u5206\u914d\u3002\u6211\u4eec\u901a\u8fc7\u521d\u59cb\u8ddf\u8e2a\u6b65\u9aa4\u63d0\u4f9b\u7684\u8f68\u8ff9\u4fe1\u606f\u548c\u591a\u901a\u9053\u97f3\u9891\u4fe1\u53f7\uff0c\u4f7f\u7528\u6ce2\u675f\u6210\u5f62\u589e\u5f3a\u8bf4\u8bdd\u4eba\u4f4d\u7f6e\u7684\u4fe1\u53f7\u4ee5\u8ba1\u7b97\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u5e76\u57fa\u4e8e\u6ce8\u518c\u6c60\u5206\u914d\u65b0\u8f68\u8ff9\u8eab\u4efd\u3002\u5728\u8bf4\u8bdd\u4eba\u4f4d\u7f6e\u53d8\u5316\u7684\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u548c\u6807\u51c6\u8ddf\u8e2a\u7cfb\u7edf\u7684\u8eab\u4efd\u5206\u914d\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u6ce2\u675f\u6210\u5f62\u548c\u8f93\u5165\u65f6\u957f\u5bf9\u5d4c\u5165\u63d0\u53d6\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.20642", "pdf": "https://arxiv.org/pdf/2506.20642", "abs": "https://arxiv.org/abs/2506.20642", "authors": ["Chao Wan", "Albert Gong", "Mihir Mishra", "Carl-Leander Henneking", "Claas Beger", "Kilian Q. Weinberger"], "title": "Memento: Note-Taking for Your Future Self", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when\nreasoning must be tightly coupled with retrieval, as in multi-hop question\nanswering. To overcome these limitations, we introduce a prompting strategy\nthat first decomposes a complex question into smaller steps, then dynamically\nconstructs a database of facts using LLMs, and finally pieces these facts\ntogether to solve the question. We show how this three-stage strategy, which we\ncall Memento, can boost the performance of existing prompting strategies across\ndiverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the\nperformance of chain-of-thought (CoT) when all information is provided in\ncontext. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento\nimproves over vanilla CoT-RAG by more than 20 F1 percentage points and over the\nmulti-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the\nchallenging MuSiQue dataset, Memento improves ReAct by more than 3 F1\npercentage points, demonstrating its utility in agentic settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMemento\u7684\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u5c0f\u6b65\u9aa4\u3001\u52a8\u6001\u6784\u5efa\u4e8b\u5b9e\u6570\u636e\u5e93\u5e76\u6574\u5408\u8fd9\u4e9b\u4e8b\u5b9e\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\uff0c\u5982\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7eaf\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u7d27\u5bc6\u7ed3\u5408\u68c0\u7d22\u7684\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Memento\u7b56\u7565\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6b65\u5904\u7406\u548c\u52a8\u6001\u4e8b\u5b9e\u5e93\u6784\u5efa\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "Memento\u91c7\u7528\u4e09\u9636\u6bb5\u7b56\u7565\uff1a1\uff09\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u5c0f\u6b65\u9aa4\uff1b2\uff09\u5229\u7528LLMs\u52a8\u6001\u6784\u5efa\u4e8b\u5b9e\u6570\u636e\u5e93\uff1b3\uff09\u6574\u5408\u8fd9\u4e9b\u4e8b\u5b9e\u4ee5\u89e3\u51b3\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5982\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002", "result": "\u5728PhantomWiki\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemento\u5c06\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u6027\u80fd\u63d0\u5347\u4e86\u4e00\u500d\uff1b\u57282WikiMultiHopQA\u5f00\u653e\u57df\u7248\u672c\u4e2d\uff0cMemento\u7ed3\u5408CoT-RAG\u6bd4\u666e\u901aCoT-RAG\u63d0\u9ad8\u4e8620\u4e2aF1\u767e\u5206\u70b9\uff0c\u6bd4\u591a\u8df3RAG\u57fa\u7ebfIRCoT\u63d0\u9ad8\u4e8613\u4e2aF1\u767e\u5206\u70b9\uff1b\u5728MuSiQue\u6570\u636e\u96c6\u4e2d\uff0cMemento\u6bd4ReAct\u63d0\u9ad8\u4e863\u4e2aF1\u767e\u5206\u70b9\u3002", "conclusion": "Memento\u901a\u8fc7\u5206\u6b65\u5904\u7406\u548c\u52a8\u6001\u4e8b\u5b9e\u5e93\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "paper_title_zh": "Memento\uff1a\u4e3a\u672a\u6765\u7684\u81ea\u5df1\u8bb0\u7b14\u8bb0", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7eaf\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u7d27\u5bc6\u7ed3\u5408\u68c0\u7d22\u7684\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u9996\u5148\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u5c0f\u6b65\u9aa4\uff0c\u7136\u540e\u5229\u7528LLMs\u52a8\u6001\u6784\u5efa\u4e8b\u5b9e\u6570\u636e\u5e93\uff0c\u6700\u540e\u6574\u5408\u8fd9\u4e9b\u4e8b\u5b9e\u4ee5\u89e3\u51b3\u95ee\u9898\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u79cd\u4e09\u9636\u6bb5\u7b56\u7565\uff08\u79f0\u4e3aMemento\uff09\u5982\u4f55\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u63d0\u5347\u73b0\u6709\u63d0\u793a\u7b56\u7565\u7684\u6027\u80fd\u3002\u57289\u6b65PhantomWiki\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5f53\u6240\u6709\u4fe1\u606f\u5747\u63d0\u4f9b\u5728\u4e0a\u4e0b\u6587\u4e2d\u65f6\uff0cMemento\u5c06\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u6027\u80fd\u63d0\u5347\u4e86\u4e00\u500d\u3002\u57282WikiMultiHopQA\u7684\u5f00\u653e\u57df\u7248\u672c\u4e2d\uff0c\u7ed3\u5408Memento\u7684CoT-RAG\u6bd4\u666e\u901aCoT-RAG\u63d0\u9ad8\u4e8620\u4e2aF1\u767e\u5206\u70b9\uff0c\u6bd4\u591a\u8df3RAG\u57fa\u7ebfIRCoT\u63d0\u9ad8\u4e8613\u4e2aF1\u767e\u5206\u70b9\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684MuSiQue\u6570\u636e\u96c6\u4e2d\uff0cMemento\u6bd4ReAct\u63d0\u9ad8\u4e863\u4e2aF1\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.20464", "pdf": "https://arxiv.org/pdf/2506.20464", "abs": "https://arxiv.org/abs/2506.20464", "authors": ["Dibyayan Patra", "Pasindu Ranasinghe", "Bikram Banerjee", "Simit Raval"], "title": "A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners", "categories": ["cs.CV", "I.4.9"], "comment": null, "summary": "Rock bolts are crucial components of the subterranean support systems in\nunderground mines that provide adequate structural reinforcement to the rock\nmass to prevent unforeseen hazards like rockfalls. This makes frequent\nassessments of such bolts critical for maintaining rock mass stability and\nminimising risks in underground mining operations. Where manual surveying of\nrock bolts is challenging due to the low light conditions in the underground\nmines and the time-intensive nature of the process, automated detection of rock\nbolts serves as a plausible solution. To that end, this study focuses on the\nautomatic identification of rock bolts within medium to large-scale 3D point\nclouds obtained from underground mines using mobile laser scanners. Existing\ntechniques for automated rock bolt identification primarily rely on feature\nengineering and traditional machine learning approaches. However, such\ntechniques lack robustness as these point clouds present several challenges due\nto data noise, varying environments, and complex surrounding structures.\nMoreover, the target rock bolts are extremely small objects within large-scale\npoint clouds and are often partially obscured due to the application of\nreinforcement shotcrete. Addressing these challenges, this paper proposes an\napproach termed DeepBolt, which employs a novel two-stage deep learning\narchitecture specifically designed for handling severe class imbalance for the\nautomatic and efficient identification of rock bolts in complex 3D point\nclouds. The proposed method surpasses state-of-the-art semantic segmentation\nmodels by up to 42.5% in Intersection over Union (IoU) for rock bolt points.\nAdditionally, it outperforms existing rock bolt identification techniques,\nachieving a 96.41% precision and 96.96% recall in classifying rock bolts,\ndemonstrating its robustness and effectiveness in complex underground\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepBolt\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u590d\u6742\u7684\u5730\u4e0b\u77ff\u5c713D\u70b9\u4e91\u4e2d\u81ea\u52a8\u9ad8\u6548\u8bc6\u522b\u5ca9\u77f3\u951a\u6746\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5730\u4e0b\u77ff\u5c71\u4e2d\u7684\u5ca9\u77f3\u951a\u6746\u662f\u652f\u6491\u7cfb\u7edf\u7684\u5173\u952e\u90e8\u4ef6\uff0c\u4f46\u5176\u624b\u52a8\u68c0\u6d4b\u56e0\u73af\u5883\u6076\u52a3\u4e14\u8017\u65f6\u800c\u4e0d\u5207\u5b9e\u9645\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u56e0\u6570\u636e\u566a\u58f0\u548c\u73af\u5883\u590d\u6742\u6027\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDeepBolt\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4e13\u95e8\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4f18\u5316\u5ca9\u77f3\u951a\u6746\u5728\u590d\u67423D\u70b9\u4e91\u4e2d\u7684\u8bc6\u522b\u3002", "result": "DeepBolt\u5728\u5ca9\u77f3\u951a\u6746\u70b9\u7684\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4e0a\u6bd4\u73b0\u6709\u8bed\u4e49\u5206\u5272\u6a21\u578b\u63d0\u534742.5%\uff0c\u5206\u7c7b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5206\u522b\u8fbe\u523096.41%\u548c96.96%\u3002", "conclusion": "DeepBolt\u5728\u590d\u6742\u5730\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5ca9\u77f3\u951a\u6746\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u79fb\u52a8\u6fc0\u5149\u626b\u63cf\u4eea\u6355\u83b7\u7684\u5730\u4e0b\u77ff\u5c71\u590d\u67423D\u70b9\u4e91\u4e2d\u5ca9\u77f3\u951a\u6746\u8bc6\u522b\u65b9\u6cd5", "abstract_zh": "\u5ca9\u77f3\u951a\u6746\u662f\u5730\u4e0b\u77ff\u5c71\u652f\u6491\u7cfb\u7edf\u7684\u5173\u952e\u90e8\u4ef6\uff0c\u5bf9\u7ef4\u6301\u5ca9\u4f53\u7a33\u5b9a\u6027\u548c\u51cf\u5c11\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5730\u4e0b\u77ff\u5c71\u5149\u7ebf\u4e0d\u8db3\u4e14\u624b\u52a8\u68c0\u6d4b\u8017\u65f6\uff0c\u81ea\u52a8\u5316\u68c0\u6d4b\u6210\u4e3a\u53ef\u884c\u65b9\u6848\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u5de5\u7a0b\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff0c\u4f46\u56e0\u6570\u636e\u566a\u58f0\u548c\u73af\u5883\u590d\u6742\u6027\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u672c\u6587\u63d0\u51faDeepBolt\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4e13\u95e8\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9ad8\u6548\u8bc6\u522b\u590d\u67423D\u70b9\u4e91\u4e2d\u7684\u5ca9\u77f3\u951a\u6746\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDeepBolt\u5728\u5ca9\u77f3\u951a\u6746\u70b9\u7684\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4e0a\u6bd4\u73b0\u6709\u6a21\u578b\u63d0\u534742.5%\uff0c\u5206\u7c7b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5206\u522b\u8fbe\u523096.41%\u548c96.96%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.19877", "pdf": "https://arxiv.org/pdf/2506.19877", "abs": "https://arxiv.org/abs/2506.19877", "authors": ["Zhaoyang Xu", "Yunbo Liu"], "title": "Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "submitted to IEEE CNS 2025", "summary": "Identifying suitable machine learning paradigms for intrusion detection\nremains critical for building effective and generalizable security solutions.\nIn this study, we present a controlled comparison of four representative models\n- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),\nOne-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on\nthe CICIDS2017 dataset under two scenarios: detecting known attack types and\ngeneralizing to previously unseen threats. Our results show that supervised MLP\nand CNN achieve near-perfect accuracy on familiar attacks but suffer drastic\nrecall drops on novel attacks. Unsupervised LOF attains moderate overall\naccuracy and high recall on unknown threats at the cost of elevated false\nalarms, while boundary-based OCSVM balances precision and recall best,\ndemonstrating robust detection across both scenarios. These findings offer\npractical guidance for selecting IDS models in dynamic network environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728CICIDS2017\u6570\u636e\u96c6\u4e0a\u7684\u5f02\u5e38\u68c0\u6d4b\u8868\u73b0\uff0c\u53d1\u73b0\u76d1\u7763\u6a21\u578b\u5728\u5df2\u77e5\u653b\u51fb\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u672a\u77e5\u653b\u51fb\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u65e0\u76d1\u7763\u6a21\u578b\u5728\u672a\u77e5\u653b\u51fb\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u5b58\u5728\u8bef\u62a5\u95ee\u9898\u3002", "motivation": "\u4e3a\u6784\u5efa\u9ad8\u6548\u4e14\u6cdb\u5316\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7814\u7a76\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5df2\u77e5\u548c\u672a\u77e5\u7f51\u7edc\u653b\u51fb\u573a\u666f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u5728CICIDS2017\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6bd4\u4e86\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3001\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3001\u4e00\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff08OCSVM\uff09\u548c\u5c40\u90e8\u79bb\u7fa4\u56e0\u5b50\uff08LOF\uff09\u56db\u79cd\u6a21\u578b\u5728\u5df2\u77e5\u653b\u51fb\u548c\u672a\u77e5\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u76d1\u7763\u6a21\u578bMLP\u548cCNN\u5728\u5df2\u77e5\u653b\u51fb\u4e0a\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f46\u5bf9\u672a\u77e5\u653b\u51fb\u53ec\u56de\u7387\u663e\u8457\u4e0b\u964d\uff1b\u65e0\u76d1\u7763\u6a21\u578bLOF\u5728\u672a\u77e5\u653b\u51fb\u4e0a\u53ec\u56de\u7387\u9ad8\u4f46\u8bef\u62a5\u7387\u9ad8\uff1bOCSVM\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "OCSVM\u5728\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "paper_title_zh": "\u7f51\u7edc\u6d41\u91cf\u4e2d\u7684\u7a33\u5065\u5f02\u5e38\u68c0\u6d4b\uff1a\u57fa\u4e8eCICIDS2017\u6570\u636e\u96c6\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30", "abstract_zh": "\u4e3a\u6784\u5efa\u9ad8\u6548\u4e14\u6cdb\u5316\u7684\u5165\u4fb5\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9009\u62e9\u5408\u9002\u7684\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u5728CICIDS2017\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u56db\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u2014\u2014\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3001\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3001\u4e00\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff08OCSVM\uff09\u548c\u5c40\u90e8\u79bb\u7fa4\u56e0\u5b50\uff08LOF\uff09\u2014\u2014\u8fdb\u884c\u4e86\u5bf9\u7167\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5176\u5728\u5df2\u77e5\u653b\u51fb\u7c7b\u578b\u68c0\u6d4b\u548c\u672a\u77e5\u5a01\u80c1\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u76d1\u7763\u6a21\u578bMLP\u548cCNN\u5728\u5df2\u77e5\u653b\u51fb\u4e0a\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f46\u5bf9\u672a\u77e5\u653b\u51fb\u53ec\u56de\u7387\u663e\u8457\u4e0b\u964d\uff1b\u65e0\u76d1\u7763\u6a21\u578bLOF\u5728\u672a\u77e5\u653b\u51fb\u4e0a\u53ec\u56de\u7387\u9ad8\u4f46\u8bef\u62a5\u7387\u9ad8\uff1b\u800c\u8fb9\u754c\u578bOCSVM\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0c\u5c55\u73b0\u51fa\u8de8\u573a\u666f\u7684\u7a33\u5065\u68c0\u6d4b\u80fd\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2506.20666", "pdf": "https://arxiv.org/pdf/2506.20666", "abs": "https://arxiv.org/abs/2506.20666", "authors": ["Sonia K. Murthy", "Rosie Zhao", "Jennifer Hu", "Sham Kakade", "Markus Wulfmeier", "Peng Qian", "Tomer Ullman"], "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u8ba4\u77e5\u6a21\u578b\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u4eba\u7c7b\u4ef7\u503c\u6743\u8861\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u66f4\u6ce8\u91cd\u4fe1\u606f\u6548\u7528\u800c\u975e\u793e\u4ea4\u6548\u7528\uff0c\u5e76\u63ed\u793a\u4e86\u8bad\u7ec3\u52a8\u6001\u5bf9\u4ef7\u503c\u6743\u8861\u7684\u6301\u4e45\u5f71\u54cd\u3002", "motivation": "\u65e5\u5e38\u793e\u4ea4\u4e2d\uff0c\u4eba\u7c7b\u5e38\u9700\u5e73\u8861\u51b2\u7a81\u76ee\u6807\uff08\u5982\u8bda\u5b9e\u4e0e\u793c\u8c8c\uff09\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u96be\u4ee5\u89e3\u6790LLM\u4e2d\u7684\u6b64\u7c7b\u52a8\u6001\u4ef7\u503c\u6743\u8861\u3002\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u8ba4\u77e5\u6a21\u578b\u4e3a\u4eba\u7c7b\u4ef7\u503c\u6743\u8861\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u672c\u6587\u5c06\u5176\u5e94\u7528\u4e8eLLM\u7814\u7a76\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u793c\u8c8c\u8bed\u8a00\u8ba4\u77e5\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u7684\u4ef7\u503c\u6743\u8861\uff1a\u524d\u6cbf\u9ed1\u76d2\u6a21\u578b\u7684\u63a8\u7406\u201c\u52aa\u529b\u201d\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u5f00\u6e90\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63a8\u7406\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u4fe1\u606f\u6548\u7528\u800c\u975e\u793e\u4ea4\u6548\u7528\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u66f4\u5f3a\u3002\u8bad\u7ec3\u52a8\u6001\u663e\u793a\uff0c\u57fa\u7840\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u4ef7\u503c\u6743\u8861\u6709\u6301\u4e45\u5f71\u54cd\uff0c\u800c\u53cd\u9988\u6570\u636e\u96c6\u6216\u5bf9\u9f50\u65b9\u6cd5\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349LLM\u5feb\u901f\u6f14\u53d8\u4e2d\u7684\u591a\u6837\u6027\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u5e2e\u52a9\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u66f4\u597d\u5730\u63a7\u5236\u4ef7\u503c\u6743\u8861\u3002", "paper_title_zh": "\u4f60\u5185\u5fc3\u6709\u8bb8\u591a\u72fc\uff1a\u5229\u7528\u8ba4\u77e5\u6a21\u578b\u89e3\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4ef7\u503c\u6743\u8861", "abstract_zh": "\u65e5\u5e38\u793e\u4ea4\u4e2d\uff0c\u4eba\u4eec\u5e38\u9700\u5e73\u8861\u51b2\u7a81\u76ee\u6807\uff08\u5982\u8868\u8fbe\u4e25\u9177\u771f\u76f8\u4e0e\u7ef4\u62a4\u4fe1\u4efb\uff09\uff0c\u540c\u65f6\u987e\u53ca\u4ed6\u4eba\u611f\u53d7\u3002\u8fd9\u4e9b\u4ef7\u503c\u6743\u8861\u662f\u4eba\u7c7b\u51b3\u7b56\u4e0e\u8bed\u8a00\u4f7f\u7528\u7684\u6838\u5fc3\uff0c\u4f46\u76ee\u524d\u89e3\u6790LLM\u4e2d\u6b64\u7c7b\u52a8\u6001\u591a\u9762\u4ef7\u503c\u7684\u5de5\u5177\u6709\u9650\u3002\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u201c\u8ba4\u77e5\u6a21\u578b\u201d\u901a\u8fc7\u91cf\u5316\u8bf4\u8bdd\u8005\u5728\u884c\u52a8\u6216\u8bdd\u8bed\u9009\u62e9\u4e2d\u7684\u7ade\u4e89\u6548\u7528\u51fd\u6570\u6743\u91cd\uff0c\u4e3a\u4eba\u7c7b\u4ef7\u503c\u6743\u8861\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u63cf\u8ff0\u3002\u672c\u7814\u7a76\u91c7\u7528\u9886\u5148\u7684\u793c\u8c8c\u8bed\u8a00\u8ba4\u77e5\u6a21\u578b\uff0c\u8bc4\u4f30LLM\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4f53\u73b0\u4e86\u4eba\u7c7b\u5f0f\u7684\u4ef7\u503c\u6743\u8861\u3002\u901a\u8fc7\u4e24\u79cd\u573a\u666f\u7cfb\u7edf\u5206\u6790\uff1a\u524d\u6cbf\u9ed1\u76d2\u6a21\u578b\u7684\u63a8\u7406\u201c\u52aa\u529b\u201d\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u5f00\u6e90\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u52a8\u6001\u3002\u7ed3\u679c\u663e\u793a\uff0c\u63a8\u7406\u6a21\u578b\u66f4\u6ce8\u91cd\u4fe1\u606f\u6548\u7528\u800c\u975e\u793e\u4ea4\u6548\u7528\uff1b\u6570\u5b66\u63a8\u7406\u8f83\u5f3a\u7684\u5f00\u6e90\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u503e\u5411\u3002LLM\u8bad\u7ec3\u52a8\u6001\u8868\u660e\uff0c\u57fa\u7840\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u6548\u7528\u4ef7\u503c\u7684\u65e9\u671f\u8c03\u6574\u5177\u6709\u6301\u4e45\u5f71\u54cd\uff0c\u800c\u53cd\u9988\u6570\u636e\u96c6\u6216\u5bf9\u9f50\u65b9\u6cd5\u5f71\u54cd\u8f83\u5c0f\u3002\u672c\u65b9\u6cd5\u80fd\u7075\u6d3b\u5e94\u5bf9\u5feb\u901f\u6f14\u53d8\u7684LLM\u751f\u6001\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u5047\u8bbe\u7684\u751f\u6210\u3001\u8bad\u7ec3\u65b9\u6848\u7684\u5236\u5b9a\u4ee5\u53ca\u4ef7\u503c\u6743\u8861\u7684\u63a7\u5236\u63d0\u4f9b\u6d1e\u89c1\u3002"}}
{"id": "2506.20522", "pdf": "https://arxiv.org/pdf/2506.20522", "abs": "https://arxiv.org/abs/2506.20522", "authors": ["Chathura Wimalasiri", "Piumal Rathnayake", "Shamod Wijerathne", "Sumudu Rasnayaka", "Dhanushka Leuke Bandara", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns", "categories": ["cs.CV", "I.5.4; I.4.6; I.4.9; I.4.8; J.3"], "comment": "This manuscript is 17 pages with 5 tables and 12 figures. The\n  manuscript is under review at Nature Scientific Reports", "summary": "Periodontitis, a chronic inflammatory disease causing alveolar bone loss,\nsignificantly affects oral health and quality of life. Accurate assessment of\nbone loss severity and pattern is critical for diagnosis and treatment\nplanning. In this study, we propose a novel AI-based deep learning framework to\nautomatically detect and quantify alveolar bone loss and its patterns using\nintraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth\ndetection with Keypoint R-CNN models to identify anatomical landmarks, enabling\nprecise calculation of bone loss severity. Additionally, YOLOv8x-seg models\nsegment bone levels and tooth masks to determine bone loss patterns (horizontal\nvs. angular) via geometric analysis. Evaluated on a large, expertly annotated\ndataset of 1000 radiographs, our approach achieved high accuracy in detecting\nbone loss severity (intra-class correlation coefficient up to 0.80) and bone\nloss pattern classification (accuracy 87%). This automated system offers a\nrapid, objective, and reproducible tool for periodontal assessment, reducing\nreliance on subjective manual evaluation. By integrating AI into dental\nradiographic analysis, our framework has the potential to improve early\ndiagnosis and personalized treatment planning for periodontitis, ultimately\nenhancing patient care and clinical outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u91cf\u5316\u7259\u69fd\u9aa8\u6d41\u5931\u7684\u4e25\u91cd\u7a0b\u5ea6\u548c\u6a21\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408YOLOv8\u548cKeypoint R-CNN\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u9aa8\u6d41\u5931\u5206\u6790\u548c\u5206\u7c7b\u3002", "motivation": "\u7259\u5468\u708e\u662f\u4e00\u79cd\u6162\u6027\u708e\u75c7\u6027\u75be\u75c5\uff0c\u4f1a\u5bfc\u81f4\u7259\u69fd\u9aa8\u6d41\u5931\uff0c\u4e25\u91cd\u5f71\u54cd\u53e3\u8154\u5065\u5eb7\u548c\u751f\u6d3b\u8d28\u91cf\u3002\u51c6\u786e\u8bc4\u4f30\u9aa8\u6d41\u5931\u7684\u4e25\u91cd\u7a0b\u5ea6\u548c\u6a21\u5f0f\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u7ed3\u5408YOLOv8\u8fdb\u884c\u7259\u9f7f\u68c0\u6d4b\uff0c\u4f7f\u7528Keypoint R-CNN\u6a21\u578b\u8bc6\u522b\u89e3\u5256\u6807\u5fd7\u7269\uff0c\u5e76\u901a\u8fc7YOLOv8x-seg\u6a21\u578b\u5206\u5272\u9aa8\u6c34\u5e73\u548c\u7259\u9f7f\u63a9\u6a21\uff0c\u5229\u7528\u51e0\u4f55\u5206\u6790\u786e\u5b9a\u9aa8\u6d41\u5931\u6a21\u5f0f\uff08\u6c34\u5e73\u6216\u89d2\u5ea6\uff09\u3002", "result": "\u57281000\u5f20\u4e13\u4e1a\u6807\u6ce8\u7684X\u5149\u7247\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u9aa8\u6d41\u5931\u4e25\u91cd\u7a0b\u5ea6\uff08\u7ec4\u5185\u76f8\u5173\u7cfb\u6570\u8fbe0.80\uff09\u548c\u9aa8\u6d41\u5931\u6a21\u5f0f\u5206\u7c7b\uff08\u51c6\u786e\u738787%\uff09\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u7cfb\u7edf\u4e3a\u7259\u5468\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u5ba2\u89c2\u4e14\u53ef\u91cd\u590d\u7684\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u5bf9\u4e3b\u89c2\u4eba\u5de5\u8bc4\u4f30\u7684\u4f9d\u8d56\uff0c\u6709\u671b\u6539\u5584\u7259\u5468\u708e\u7684\u65e9\u671f\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u3002", "paper_title_zh": "AI\u8f85\u52a9\u653e\u5c04\u5f71\u50cf\u5206\u6790\u5728\u68c0\u6d4b\u7259\u69fd\u9aa8\u6d41\u5931\u4e25\u91cd\u7a0b\u5ea6\u548c\u6a21\u5f0f\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u7259\u5468\u708e\u662f\u4e00\u79cd\u5bfc\u81f4\u7259\u69fd\u9aa8\u6d41\u5931\u7684\u6162\u6027\u708e\u75c7\u6027\u75be\u75c5\uff0c\u4e25\u91cd\u5f71\u54cd\u53e3\u8154\u5065\u5eb7\u548c\u751f\u6d3b\u8d28\u91cf\u3002\u51c6\u786e\u8bc4\u4f30\u9aa8\u6d41\u5931\u7684\u4e25\u91cd\u7a0b\u5ea6\u548c\u6a21\u5f0f\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53e3\u5185\u6839\u5c16X\u5149\u7247\u81ea\u52a8\u68c0\u6d4b\u548c\u91cf\u5316\u7259\u69fd\u9aa8\u6d41\u5931\u53ca\u5176\u6a21\u5f0f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408YOLOv8\u8fdb\u884c\u7259\u9f7f\u68c0\u6d4b\uff0c\u5e76\u4f7f\u7528Keypoint R-CNN\u6a21\u578b\u8bc6\u522b\u89e3\u5256\u6807\u5fd7\u7269\uff0c\u4ece\u800c\u7cbe\u786e\u8ba1\u7b97\u9aa8\u6d41\u5931\u7684\u4e25\u91cd\u7a0b\u5ea6\u3002\u6b64\u5916\uff0cYOLOv8x-seg\u6a21\u578b\u5206\u5272\u9aa8\u6c34\u5e73\u548c\u7259\u9f7f\u63a9\u6a21\uff0c\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u786e\u5b9a\u9aa8\u6d41\u5931\u6a21\u5f0f\uff08\u6c34\u5e73\u6216\u89d2\u5ea6\uff09\u3002\u57281000\u5f20\u4e13\u4e1a\u6807\u6ce8\u7684X\u5149\u7247\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u9aa8\u6d41\u5931\u4e25\u91cd\u7a0b\u5ea6\uff08\u7ec4\u5185\u76f8\u5173\u7cfb\u6570\u8fbe0.80\uff09\u548c\u9aa8\u6d41\u5931\u6a21\u5f0f\u5206\u7c7b\uff08\u51c6\u786e\u738787%\uff09\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002\u8fd9\u4e00\u81ea\u52a8\u5316\u7cfb\u7edf\u4e3a\u7259\u5468\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u5ba2\u89c2\u4e14\u53ef\u91cd\u590d\u7684\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u5bf9\u4e3b\u89c2\u4eba\u5de5\u8bc4\u4f30\u7684\u4f9d\u8d56\u3002\u901a\u8fc7\u5c06AI\u6574\u5408\u5230\u7259\u79d1\u653e\u5c04\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u6211\u4eec\u7684\u6846\u67b6\u6709\u671b\u6539\u5584\u7259\u5468\u708e\u7684\u65e9\u671f\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\uff0c\u6700\u7ec8\u63d0\u5347\u60a3\u8005\u62a4\u7406\u548c\u4e34\u5e8a\u6548\u679c\u3002"}}
{"id": "2506.19880", "pdf": "https://arxiv.org/pdf/2506.19880", "abs": "https://arxiv.org/abs/2506.19880", "authors": ["Stefanos Achlatis", "Efstratios Gavves", "Jan-Jakob Sonke"], "title": "Physics-Guided Radiotherapy Treatment Planning with Deep Learning", "categories": ["physics.med-ph", "cs.AI"], "comment": null, "summary": "Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated\narc therapy (VMAT) being a commonly used technique that enhances dose\nconformity by dynamically adjusting multileaf collimator (MLC) positions and\nmonitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires\nfrequent modifications to treatment plans to account for anatomical variations,\nnecessitating time-efficient solutions. Deep learning offers a promising\nsolution to automate this process. To this end, we propose a two-stage,\nphysics-guided deep learning pipeline for radiotherapy planning. In the first\nstage, our network is trained with direct supervision on treatment plan\nparameters, consisting of MLC and MU values. In the second stage, we\nincorporate an additional supervision signal derived from the predicted 3D dose\ndistribution, integrating physics-based guidance into the training process. We\ntrain and evaluate our approach on 133 prostate cancer patients treated with a\nuniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target\nvolume (PTV). Our results demonstrate that the proposed approach, implemented\nusing both 3D U-Net and UNETR architectures, consistently produces treatment\nplans that closely match clinical ground truths. Our method achieves a mean\ndifference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV\nwhile generating dose distributions that reduce radiation exposure to organs at\nrisk. These findings highlight the potential of physics-guided deep learning in\nRT planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\uff0c\u901a\u8fc7\u7ed3\u5408MLC\u548cMU\u503c\u7684\u76f4\u63a5\u76d1\u7763\u4ee5\u53ca\u9884\u6d4b\u76843D\u5242\u91cf\u5206\u5e03\uff0c\u751f\u6210\u4e0e\u4e34\u5e8a\u771f\u5b9e\u60c5\u51b5\u9ad8\u5ea6\u5339\u914d\u7684\u6cbb\u7597\u8ba1\u5212\u3002", "motivation": "\u653e\u5c04\u6cbb\u7597\uff08RT\uff09\u662f\u764c\u75c7\u6cbb\u7597\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u800c\u81ea\u9002\u5e94\u653e\u7597\u9700\u8981\u9891\u7e41\u8c03\u6574\u6cbb\u7597\u8ba1\u5212\u4ee5\u9002\u5e94\u89e3\u5256\u5b66\u53d8\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u3002\u6df1\u5ea6\u5b66\u4e60\u4e3a\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u7269\u7406\u5f15\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u76f4\u63a5\u76d1\u7763\u8bad\u7ec3\u7f51\u7edc\u5b66\u4e60\u6cbb\u7597\u8ba1\u5212\u53c2\u6570\uff08MLC\u548cMU\u503c\uff09\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u9884\u6d4b\u76843D\u5242\u91cf\u5206\u5e03\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u5c06\u7269\u7406\u5f15\u5bfc\u6574\u5408\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u3002\u5b9e\u9a8c\u57fa\u4e8e133\u540d\u524d\u5217\u817a\u764c\u60a3\u8005\u76842-\u5f27VMAT\u6cbb\u7597\u6570\u636e\uff0c\u4f7f\u75283D U-Net\u548cUNETR\u67b6\u6784\u5b9e\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8ba1\u5212\u4e0e\u4e34\u5e8a\u771f\u5b9e\u60c5\u51b5\u9ad8\u5ea6\u543b\u5408\uff0cPTV\u533a\u57df\u7684D95%\u548cV95%\u5dee\u5f02\u5206\u522b\u4e3a0.42 +/- 1.83 Gy\u548c-0.22 +/- 1.87%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5371\u9669\u5668\u5b98\u7684\u8f90\u5c04\u5242\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7269\u7406\u5f15\u5bfc\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6cbb\u7597\u8ba1\u5212\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u653e\u5c04\u6cbb\u7597\u8ba1\u5212", "abstract_zh": "\u653e\u5c04\u6cbb\u7597\uff08RT\uff09\u662f\u764c\u75c7\u6cbb\u7597\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u5176\u4e2d\u5bb9\u79ef\u8c03\u5f3a\u5f27\u5f62\u6cbb\u7597\uff08VMAT\uff09\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u591a\u53f6\u51c6\u76f4\u5668\uff08MLC\uff09\u4f4d\u7f6e\u548c\u76d1\u6d4b\u5355\u4f4d\uff08MU\uff09\u4ee5\u63d0\u9ad8\u5242\u91cf\u9002\u5f62\u6027\u3002\u81ea\u9002\u5e94\u653e\u7597\u9700\u8981\u9891\u7e41\u8c03\u6574\u6cbb\u7597\u8ba1\u5212\u4ee5\u9002\u5e94\u89e3\u5256\u5b66\u53d8\u5316\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002\u6df1\u5ea6\u5b66\u4e60\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u3001\u7269\u7406\u5f15\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u7528\u4e8e\u653e\u7597\u8ba1\u5212\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u7f51\u7edc\u901a\u8fc7\u76f4\u63a5\u76d1\u7763\u5b66\u4e60\u6cbb\u7597\u8ba1\u5212\u53c2\u6570\uff08MLC\u548cMU\u503c\uff09\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5f15\u5165\u9884\u6d4b\u76843D\u5242\u91cf\u5206\u5e03\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u5c06\u7269\u7406\u5f15\u5bfc\u6574\u5408\u5230\u8bad\u7ec3\u4e2d\u3002\u6211\u4eec\u5728133\u540d\u91c7\u7528\u7edf\u4e002-\u5f27VMAT\u534f\u8bae\uff08PTV\u5242\u91cf62 Gy\uff09\u6cbb\u7597\u7684\u524d\u5217\u817a\u764c\u60a3\u8005\u6570\u636e\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e3D U-Net\u548cUNETR\u67b6\u6784\u7684\u65b9\u6cd5\u751f\u6210\u7684\u8ba1\u5212\u4e0e\u4e34\u5e8a\u771f\u5b9e\u60c5\u51b5\u9ad8\u5ea6\u543b\u5408\uff0cPTV\u533a\u57df\u7684D95%\u548cV95%\u5dee\u5f02\u5206\u522b\u4e3a0.42 +/- 1.83 Gy\u548c-0.22 +/- 1.87%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5371\u9669\u5668\u5b98\u7684\u8f90\u5c04\u5242\u91cf\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u7269\u7406\u5f15\u5bfc\u6df1\u5ea6\u5b66\u4e60\u5728\u653e\u7597\u8ba1\u5212\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571", "abs": "https://arxiv.org/abs/2506.16571", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Capturing Visualization Design Rationale", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a2\u7a76\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u903b\u8f91\uff0c\u5229\u7528\u5b66\u751f\u521b\u5efa\u7684\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u6e90\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u95ee\u9898-\u7b54\u6848-\u903b\u8f91\u4e09\u5143\u7ec4\uff0c\u6700\u7ec8\u9a8c\u8bc1\u5e76\u6574\u7406\u51fa\u6355\u6349\u8bbe\u8ba1\u9009\u62e9\u548c\u903b\u8f91\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u591a\u5173\u6ce8\u53ef\u89c6\u5316\u89e3\u8bfb\u800c\u975e\u8bbe\u8ba1\u903b\u8f91\uff0c\u4e14\u591a\u57fa\u4e8e\u4eba\u4e3a\u6784\u5efa\u7684\u95ee\u9898\u548c\u53ef\u89c6\u5316\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u771f\u5b9e\u7684\u5b66\u751f\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u63a2\u7a76\u8bbe\u8ba1\u51b3\u7b56\u80cc\u540e\u7684\u903b\u8f91\u3002", "method": "\u5229\u7528\u5b66\u751f\u6570\u636e\u53ef\u89c6\u5316\u8bfe\u7a0b\u4e2d\u7684\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u6e90\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u95ee\u9898-\u7b54\u6848-\u903b\u8f91\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u548c\u6574\u7406\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u6355\u6349\u53ef\u89c6\u5316\u8bbe\u8ba1\u9009\u62e9\u548c\u903b\u8f91\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u8bbe\u8ba1\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u903b\u8f91\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "paper_title_zh": "\u6355\u6349\u53ef\u89c6\u5316\u8bbe\u8ba1\u903b\u8f91", "abstract_zh": "\u4ee5\u5f80\u7684\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u591a\u7528\u4e8e\u53ef\u89c6\u5316\u7d20\u517b\u8bc4\u4f30\u3001\u6d1e\u5bdf\u751f\u6210\u6216\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u53ef\u89c6\u5316\uff0c\u8fd9\u4e9b\u7814\u7a76\u901a\u5e38\u4f9d\u8d56\u4eba\u4e3a\u6784\u5efa\u7684\u95ee\u9898\u548c\u53ef\u89c6\u5316\uff0c\u4fa7\u91cd\u4e8e\u89e3\u8bfb\u800c\u975e\u8bbe\u8ba1\u903b\u8f91\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a2\u7a76\u53ef\u89c6\u5316\u8bbe\u8ba1\u903b\u8f91\u3002\u6211\u4eec\u5229\u7528\u5b66\u751f\u6570\u636e\u53ef\u89c6\u5316\u8bfe\u7a0b\u4e2d\u7684\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u6e90\uff0c\u8fd9\u4e9b\u7b14\u8bb0\u672c\u7ed3\u5408\u4e86\u53ef\u89c6\u5316\u4f5c\u54c1\u548c\u8bbe\u8ba1\u8bf4\u660e\uff0c\u5b66\u751f\u660e\u786e\u9610\u8ff0\u4e86\u5176\u8bbe\u8ba1\u51b3\u7b56\u80cc\u540e\u7684\u903b\u8f91\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u7b14\u8bb0\u672c\u4e2d\u7684\u53d9\u8ff0\u548c\u8868\u8fbe\u751f\u6210\u5e76\u5206\u7c7b\u95ee\u9898-\u7b54\u6848-\u903b\u8f91\u4e09\u5143\u7ec4\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\u540e\u6574\u7406\u51fa\u4e00\u4e2a\u6355\u6349\u5b66\u751f\u53ef\u89c6\u5316\u8bbe\u8ba1\u9009\u62e9\u548c\u903b\u8f91\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2506.20548", "pdf": "https://arxiv.org/pdf/2506.20548", "abs": "https://arxiv.org/abs/2506.20548", "authors": ["Manyi Li", "Renshuai Tao", "Yufan Liu", "Chuangchuang Tan", "Haotong Qin", "Bing Li", "Yunchao Wei", "Yao Zhao"], "title": "Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "20 pages, 10 figures", "summary": "With the rapid advancement of deep learning, particularly through generative\nadversarial networks (GANs) and diffusion models (DMs), AI-generated images, or\n``deepfakes\", have become nearly indistinguishable from real ones. These images\nare widely shared across Online Social Networks (OSNs), raising concerns about\ntheir misuse. Existing deepfake detection methods overlook the ``block effects\"\nintroduced by compression in OSNs, which obscure deepfake artifacts, and\nprimarily focus on raw images, rarely encountered in real-world scenarios. To\naddress these challenges, we propose PLADA (Pay Less Attention to Deceptive\nArtifacts), a novel framework designed to tackle the lack of paired data and\nthe ineffective use of compressed images. PLADA consists of two core modules:\nBlock Effect Eraser (B2E), which uses a dual-stage attention mechanism to\nhandle block effects, and Open Data Aggregation (ODA), which processes both\npaired and unpaired data to improve detection. Extensive experiments across 26\ndatasets demonstrate that PLADA achieves a remarkable balance in deepfake\ndetection, outperforming SoTA methods in detecting deepfakes on OSNs, even with\nlimited paired data and compression. More importantly, this work introduces the\n``block effect\" as a critical factor in deepfake detection, providing a robust\nsolution for open-world scenarios. Our code is available at\nhttps://github.com/ManyiLee/PLADA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPLADA\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u9664\u538b\u7f29\u5f15\u8d77\u7684\u5757\u6548\u5e94\u548c\u5229\u7528\u5f00\u653e\u6570\u636e\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\u4e2d\u538b\u7f29\u6df1\u5ea6\u4f2a\u9020\u56fe\u50cf\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\u4e2d\u56fe\u50cf\u538b\u7f29\u5f15\u5165\u7684\u5757\u6548\u5e94\uff0c\u4e14\u4e3b\u8981\u4f9d\u8d56\u539f\u59cb\u56fe\u50cf\uff0c\u96be\u4ee5\u5e94\u5bf9\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u538b\u7f29\u56fe\u50cf\u68c0\u6d4b\u9700\u6c42\u3002", "method": "PLADA\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u5757\u6548\u5e94\u6d88\u9664\u5668\uff08B2E\uff09\u901a\u8fc7\u53cc\u9636\u6bb5\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5757\u6548\u5e94\uff1b\u5f00\u653e\u6570\u636e\u805a\u5408\uff08ODA\uff09\u5229\u7528\u914d\u5bf9\u548c\u975e\u914d\u5bf9\u6570\u636e\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPLADA\u5728\u538b\u7f29\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u6570\u636e\u6709\u9650\u4e5f\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PLADA\u4e0d\u4ec5\u89e3\u51b3\u4e86\u538b\u7f29\u56fe\u50cf\u7684\u68c0\u6d4b\u96be\u9898\uff0c\u8fd8\u9996\u6b21\u5c06\u5757\u6548\u5e94\u4f5c\u4e3a\u5173\u952e\u56e0\u7d20\u5f15\u5165\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u9886\u57df\uff0c\u4e3a\u5f00\u653e\u573a\u666f\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u51cf\u5c11\u5bf9\u6b3a\u9a97\u6027\u4f2a\u5f71\u7684\u5173\u6ce8\uff1a\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\u4e2d\u538b\u7f29\u6df1\u5ea6\u4f2a\u9020\u7684\u9c81\u68d2\u68c0\u6d4b", "abstract_zh": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u548c\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u5e94\u7528\uff0cAI\u751f\u6210\u7684\u56fe\u50cf\uff08\u5373\u201c\u6df1\u5ea6\u4f2a\u9020\u201d\uff09\u5df2\u51e0\u4e4e\u65e0\u6cd5\u4e0e\u771f\u5b9e\u56fe\u50cf\u533a\u5206\u3002\u8fd9\u4e9b\u56fe\u50cf\u5728\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\uff08OSNs\uff09\u4e0a\u5e7f\u6cdb\u4f20\u64ad\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u6ee5\u7528\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u4e86OSNs\u4e2d\u538b\u7f29\u5f15\u5165\u7684\u201c\u5757\u6548\u5e94\u201d\uff0c\u8fd9\u4e9b\u6548\u5e94\u63a9\u76d6\u4e86\u6df1\u5ea6\u4f2a\u9020\u7684\u4f2a\u5f71\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u539f\u59cb\u56fe\u50cf\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u5f88\u5c11\u9047\u5230\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PLADA\uff08\u51cf\u5c11\u5bf9\u6b3a\u9a97\u6027\u4f2a\u5f71\u7684\u5173\u6ce8\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u914d\u5bf9\u6570\u636e\u4e0d\u8db3\u548c\u538b\u7f29\u56fe\u50cf\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002PLADA\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u5757\u6548\u5e94\u6d88\u9664\u5668\uff08B2E\uff09\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5757\u6548\u5e94\uff1b\u5f00\u653e\u6570\u636e\u805a\u5408\uff08ODA\uff09\uff0c\u5904\u7406\u914d\u5bf9\u548c\u975e\u914d\u5bf9\u6570\u636e\u4ee5\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPLADA\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u5e73\u8861\uff0c\u5373\u4f7f\u6570\u636e\u6709\u9650\u548c\u538b\u7f29\u60c5\u51b5\u4e0b\uff0c\u4ecd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u672c\u7814\u7a76\u9996\u6b21\u5c06\u201c\u5757\u6548\u5e94\u201d\u4f5c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u5f00\u653e\u573a\u666f\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/ManyiLee/PLADA\u3002"}}
{"id": "2506.19882", "pdf": "https://arxiv.org/pdf/2506.19882", "abs": "https://arxiv.org/abs/2506.19882", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Yegor Denisov-Blanch", "Brando Miranda", "Matthias Gerstgrasser", "Susan Zhang", "Andreas Haupt", "Isha Gupta", "Elyas Obbad", "Jesse Dodge", "Jessica Zosa Forde", "Koustuv Sinha", "Francesco Orabona", "Sanmi Koyejo", "David Donoho"], "title": "Position: Machine Learning Conferences Should Establish a \"Refutations and Critiques\" Track", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Science progresses by iteratively advancing and correcting humanity's\nunderstanding of the world. In machine learning (ML) research, rapid\nadvancements have led to an explosion of publications, but have also led to\nmisleading, incorrect, flawed or perhaps even fraudulent studies being accepted\nand sometimes highlighted at ML conferences due to the fallibility of peer\nreview. While such mistakes are understandable, ML conferences do not offer\nrobust processes to help the field systematically correct when such errors are\nmade.This position paper argues that ML conferences should establish a\ndedicated \"Refutations and Critiques\" (R & C) Track. This R & C Track would\nprovide a high-profile, reputable platform to support vital research that\ncritically challenges prior research, thereby fostering a dynamic\nself-correcting research ecosystem. We discuss key considerations including\ntrack design, review principles, potential pitfalls, and provide an\nillustrative example submission concerning a recent ICLR 2025 Oral. We conclude\nthat ML conferences should create official, reputable mechanisms to help ML\nresearch self-correct.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u5e94\u8bbe\u7acb\u201c\u53cd\u9a73\u4e0e\u6279\u8bc4\u201d\u4e13\u9898\uff0c\u4ee5\u7cfb\u7edf\u6027\u7ea0\u6b63\u7814\u7a76\u4e2d\u7684\u9519\u8bef\uff0c\u4fc3\u8fdb\u9886\u57df\u81ea\u6211\u4fee\u6b63\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u540c\u884c\u8bc4\u5ba1\u7684\u5c40\u9650\u6027\u5bfc\u81f4\u4e00\u4e9b\u8bef\u5bfc\u6027\u3001\u9519\u8bef\u751a\u81f3\u6b3a\u8bc8\u6027\u7814\u7a76\u88ab\u63a5\u53d7\u6216\u7a81\u51fa\u5c55\u793a\u3002\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u673a\u5236\u6765\u7ea0\u6b63\u8fd9\u4e9b\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u201c\u53cd\u9a73\u4e0e\u6279\u8bc4\u201d\u4e13\u9898\u3002", "method": "\u63d0\u51fa\u5728\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u4e2d\u8bbe\u7acb\u201c\u53cd\u9a73\u4e0e\u6279\u8bc4\u201d\u4e13\u9898\uff0c\u5e76\u8ba8\u8bba\u5176\u8bbe\u8ba1\u539f\u5219\u3001\u8bc4\u5ba1\u6807\u51c6\u3001\u6f5c\u5728\u95ee\u9898\uff0c\u5e76\u901a\u8fc7ICLR 2025\u7684\u53e3\u5934\u62a5\u544a\u6848\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002", "result": "\u901a\u8fc7\u8bbe\u7acb\u201c\u53cd\u9a73\u4e0e\u6279\u8bc4\u201d\u4e13\u9898\uff0c\u53ef\u4ee5\u4e3a\u9886\u57df\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u58f0\u8a89\u7684\u5e73\u53f0\uff0c\u4fc3\u8fdb\u5bf9\u5148\u524d\u7814\u7a76\u7684\u6279\u5224\u6027\u6311\u6218\uff0c\u4ece\u800c\u63a8\u52a8\u7814\u7a76\u7684\u81ea\u6211\u4fee\u6b63\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u5e94\u5efa\u7acb\u5b98\u65b9\u4e14\u58f0\u8a89\u826f\u597d\u7684\u673a\u5236\uff0c\u5e2e\u52a9\u9886\u57df\u7cfb\u7edf\u6027\u7ea0\u6b63\u9519\u8bef\uff0c\u63a8\u52a8\u7814\u7a76\u7684\u5065\u5eb7\u53d1\u5c55\u3002", "paper_title_zh": "\u7acb\u573a\uff1a\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u5e94\u8bbe\u7acb\u201c\u53cd\u9a73\u4e0e\u6279\u8bc4\u201d\u4e13\u9898", "abstract_zh": "\u79d1\u5b66\u901a\u8fc7\u4e0d\u65ad\u63a8\u8fdb\u548c\u4fee\u6b63\u4eba\u7c7b\u5bf9\u4e16\u754c\u7684\u7406\u89e3\u800c\u8fdb\u6b65\u3002\u5728\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7814\u7a76\u4e2d\uff0c\u5feb\u901f\u7684\u53d1\u5c55\u5bfc\u81f4\u4e86\u5927\u91cf\u8bba\u6587\u7684\u6d8c\u73b0\uff0c\u4f46\u4e5f\u7531\u4e8e\u540c\u884c\u8bc4\u5ba1\u7684\u5c40\u9650\u6027\uff0c\u4e00\u4e9b\u8bef\u5bfc\u6027\u3001\u9519\u8bef\u3001\u6709\u7f3a\u9677\u751a\u81f3\u6b3a\u8bc8\u6027\u7684\u7814\u7a76\u88ab\u63a5\u53d7\uff0c\u6709\u65f6\u751a\u81f3\u5728ML\u4f1a\u8bae\u4e0a\u88ab\u7a81\u51fa\u5c55\u793a\u3002\u867d\u7136\u8fd9\u4e9b\u9519\u8bef\u662f\u53ef\u4ee5\u7406\u89e3\u7684\uff0c\u4f46ML\u4f1a\u8bae\u5e76\u672a\u63d0\u4f9b\u5f3a\u6709\u529b\u7684\u6d41\u7a0b\u6765\u5e2e\u52a9\u9886\u57df\u7cfb\u7edf\u6027\u7ea0\u6b63\u8fd9\u4e9b\u9519\u8bef\u3002\u672c\u6587\u4e3b\u5f20ML\u4f1a\u8bae\u5e94\u8bbe\u7acb\u4e13\u95e8\u7684\u201c\u53cd\u9a73\u4e0e\u6279\u8bc4\u201d\uff08R & C\uff09\u4e13\u9898\u3002\u8fd9\u4e00R & C\u4e13\u9898\u5c06\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u58f0\u8a89\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5bf9\u5148\u524d\u7814\u7a76\u8fdb\u884c\u6279\u5224\u6027\u6311\u6218\u7684\u91cd\u8981\u7814\u7a76\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e00\u4e2a\u52a8\u6001\u7684\u81ea\u6211\u4fee\u6b63\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u5305\u62ec\u4e13\u9898\u8bbe\u8ba1\u3001\u8bc4\u5ba1\u539f\u5219\u3001\u6f5c\u5728\u95ee\u9898\u7b49\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u4ee5ICLR 2025\u7684\u4e00\u4e2a\u53e3\u5934\u62a5\u544a\u4e3a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\u3002\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0cML\u4f1a\u8bae\u5e94\u5efa\u7acb\u5b98\u65b9\u4e14\u58f0\u8a89\u826f\u597d\u7684\u673a\u5236\uff0c\u5e2e\u52a9ML\u7814\u7a76\u5b9e\u73b0\u81ea\u6211\u4fee\u6b63\u3002"}}
{"id": "2506.20550", "pdf": "https://arxiv.org/pdf/2506.20550", "abs": "https://arxiv.org/abs/2506.20550", "authors": ["Yitong Quan", "Benjamin Kiefer", "Martin Messmer", "Andreas Zell"], "title": "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to ECMR 2025", "summary": "Modern image-based object detection models, such as YOLOv7, primarily process\nindividual frames independently, thus ignoring valuable temporal context\nnaturally present in videos. Meanwhile, existing video-based detection methods\noften introduce complex temporal modules, significantly increasing model size\nand computational complexity. In practical applications such as surveillance\nand autonomous driving, transient challenges including motion blur, occlusions,\nand abrupt appearance changes can severely degrade single-frame detection\nperformance. To address these issues, we propose a straightforward yet highly\neffective strategy: stacking multiple consecutive frames as input to a\nYOLO-based detector while supervising only the output corresponding to a single\ntarget frame. This approach leverages temporal information with minimal\nmodifications to existing architectures, preserving simplicity, computational\nefficiency, and real-time inference capability. Extensive experiments on the\nchallenging MOT20Det and our BOAT360 datasets demonstrate that our method\nimproves detection robustness, especially for lightweight models, effectively\nnarrowing the gap between compact and heavy detection networks. Additionally,\nwe contribute the BOAT360 benchmark dataset, comprising annotated fisheye video\nsequences captured from a boat, to support future research in multi-frame video\nobject detection in challenging real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u5e27\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5806\u53e0\u8fde\u7eed\u5e27\u8f93\u5165YOLO\u68c0\u6d4b\u5668\uff0c\u4ec5\u76d1\u7763\u76ee\u6807\u5e27\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8e\u56fe\u50cf\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u5982YOLOv7\uff09\u901a\u5e38\u72ec\u7acb\u5904\u7406\u5355\u5e27\uff0c\u5ffd\u7565\u4e86\u89c6\u9891\u4e2d\u7684\u5b9d\u8d35\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u5219\u56e0\u5f15\u5165\u590d\u6742\u65f6\u5e8f\u6a21\u5757\u800c\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u3002\u9488\u5bf9\u76d1\u63a7\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u77ac\u65f6\u6311\u6218\uff08\u5982\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u548c\u5916\u89c2\u7a81\u53d8\uff09\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7b56\u7565\uff1a\u5c06\u8fde\u7eed\u591a\u5e27\u5806\u53e0\u4f5c\u4e3aYOLO\u68c0\u6d4b\u5668\u7684\u8f93\u5165\uff0c\u4f46\u4ec5\u76d1\u7763\u76ee\u6807\u5e27\u7684\u8f93\u51fa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u73b0\u6709\u67b6\u6784\u7684\u4fee\u6539\uff0c\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u548c\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728MOT20Det\u548c\u81ea\u5efa\u7684BOAT360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5bf9\u8f7b\u91cf\u6a21\u578b\u6548\u679c\u660e\u663e\uff0c\u7f29\u5c0f\u4e86\u8f7b\u91cf\u4e0e\u91cd\u578b\u68c0\u6d4b\u7f51\u7edc\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u672c\u6587\u8d21\u732e\u4e86BOAT360\u6570\u636e\u96c6\uff0c\u652f\u6301\u672a\u6765\u591a\u5e27\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u5e27\u96c6\u6210\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u65f6\u95f4\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u548c\u5b9e\u65f6\u6027\u3002BOAT360\u6570\u636e\u96c6\u7684\u53d1\u5e03\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "paper_title_zh": "\u8f7b\u91cf\u7ea7\u591a\u5e27\u96c6\u6210\uff1a\u63d0\u5347YOLO\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u9c81\u68d2\u6027", "abstract_zh": "\u73b0\u4ee3\u57fa\u4e8e\u56fe\u50cf\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u5982YOLOv7\uff09\u901a\u5e38\u72ec\u7acb\u5904\u7406\u5355\u5e27\uff0c\u5ffd\u7565\u4e86\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u5219\u56e0\u5f15\u5165\u590d\u6742\u65f6\u5e8f\u6a21\u5757\u800c\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u3002\u9488\u5bf9\u76d1\u63a7\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u77ac\u65f6\u6311\u6218\uff08\u5982\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u548c\u5916\u89c2\u7a81\u53d8\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u7b56\u7565\uff1a\u5c06\u8fde\u7eed\u591a\u5e27\u5806\u53e0\u4f5c\u4e3aYOLO\u68c0\u6d4b\u5668\u7684\u8f93\u5165\uff0c\u4f46\u4ec5\u76d1\u7763\u76ee\u6807\u5e27\u7684\u8f93\u51fa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u73b0\u6709\u67b6\u6784\u7684\u4fee\u6539\uff0c\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u548c\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3002\u5728MOT20Det\u548c\u81ea\u5efa\u7684BOAT360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5bf9\u8f7b\u91cf\u6a21\u578b\u6548\u679c\u660e\u663e\uff0c\u7f29\u5c0f\u4e86\u8f7b\u91cf\u4e0e\u91cd\u578b\u68c0\u6d4b\u7f51\u7edc\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u672c\u6587\u8d21\u732e\u4e86BOAT360\u6570\u636e\u96c6\uff0c\u652f\u6301\u672a\u6765\u591a\u5e27\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u7814\u7a76\u3002"}}
{"id": "2506.19883", "pdf": "https://arxiv.org/pdf/2506.19883", "abs": "https://arxiv.org/abs/2506.19883", "authors": ["Zhuqing Liu", "Chaosheng Dong", "Michinari Momma", "Simone Shao", "Shaoyuan Xu", "Yan Gao", "Haibo Yang", "Jia Liu"], "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, multi-objective optimization (MOO) has gained attention for its\nbroad applications in ML, operations research, and engineering. However, MOO\nalgorithm design remains in its infancy and many existing MOO methods suffer\nfrom unsatisfactory convergence rate and sample complexity performance. To\naddress this challenge, in this paper, we propose an algorithm called STIMULUS(\nstochastic path-integrated multi-gradient recursive e\\ulstimator), a new and\nrobust approach for solving MOO problems. Different from the traditional\nmethods, STIMULUS introduces a simple yet powerful recursive framework for\nupdating stochastic gradient estimates to improve convergence performance with\nlow sample complexity. In addition, we introduce an enhanced version of\nSTIMULUS, termed STIMULUS-M, which incorporates a momentum term to further\nexpedite convergence. We establish $O(1/T)$ convergence rates of the proposed\nmethods for non-convex settings and $O (\\exp{-\\mu T})$ for strongly convex\nsettings, where $T$ is the total number of iteration rounds. Additionally, we\nachieve the state-of-the-art $O \\left(n+\\sqrt{n}\\epsilon^{-1}\\right)$ sample\ncomplexities for non-convex settings and $O\\left(n+ \\sqrt{n} \\ln\n({\\mu/\\epsilon})\\right)$ for strongly convex settings, where $\\epsilon>0$ is a\ndesired stationarity error. Moreover, to alleviate the periodic full gradient\nevaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced\nversions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their\ntheoretical analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTIMULUS\u7684\u65b0\u578b\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u6846\u67b6\u66f4\u65b0\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u5e26\u52a8\u91cf\u9879\u7684STIMULUS-M\u548c\u81ea\u9002\u5e94\u6279\u5904\u7406\u7684STIMULUS+/STIMULUS-M+\u7248\u672c\u3002", "motivation": "\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u5728\u673a\u5668\u5b66\u4e60\u3001\u8fd0\u7b79\u5b66\u548c\u5de5\u7a0b\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u6837\u672c\u590d\u6742\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "STIMULUS\u901a\u8fc7\u9012\u5f52\u6846\u67b6\u66f4\u65b0\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\uff0c\u6539\u8fdb\u6536\u655b\u6027\u80fd\uff1bSTIMULUS-M\u52a0\u5165\u52a8\u91cf\u9879\u52a0\u901f\u6536\u655b\uff1bSTIMULUS+/STIMULUS-M+\u91c7\u7528\u81ea\u9002\u5e94\u6279\u5904\u7406\u51cf\u5c11\u5168\u68af\u5ea6\u8bc4\u4f30\u9700\u6c42\u3002", "result": "\u5728\u975e\u51f8\u548c\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\uff0cSTIMULUS\u5206\u522b\u8fbe\u5230O(1/T)\u548cO(exp{-\u03bcT})\u7684\u6536\u655b\u901f\u5ea6\uff0c\u6837\u672c\u590d\u6742\u5ea6\u5206\u522b\u4e3aO(n+\u221an\u03b5\u207b\u00b9)\u548cO(n+\u221anln(\u03bc/\u03b5))\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STIMULUS\u53ca\u5176\u53d8\u4f53\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6536\u655b\u901f\u5ea6\u548c\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "STIMULUS\uff1a\u5728\u968f\u673a\u591a\u76ee\u6807\u5b66\u4e60\u4e2d\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u548c\u4f4e\u6837\u672c\u590d\u6742\u5ea6", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u56e0\u5176\u5728\u673a\u5668\u5b66\u4e60\u3001\u8fd0\u7b79\u5b66\u548c\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0cMOO\u7b97\u6cd5\u8bbe\u8ba1\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u6837\u672c\u590d\u6742\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTIMULUS\uff08\u968f\u673a\u8def\u5f84\u79ef\u5206\u591a\u68af\u5ea6\u9012\u5f52\u4f30\u8ba1\u5668\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u800c\u5f3a\u5927\u7684\u9012\u5f52\u6846\u67b6\u66f4\u65b0\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\uff0c\u4ee5\u4f4e\u6837\u672c\u590d\u6742\u5ea6\u63d0\u5347\u6536\u655b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86STIMULUS-M\uff0c\u901a\u8fc7\u52a0\u5165\u52a8\u91cf\u9879\u8fdb\u4e00\u6b65\u52a0\u901f\u6536\u655b\u3002\u5728\u975e\u51f8\u548c\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\uff0c\u6211\u4eec\u5206\u522b\u8bc1\u660e\u4e86O(1/T)\u548cO(exp{-\u03bcT})\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86O(n+\u221an\u03b5\u207b\u00b9)\uff08\u975e\u51f8\uff09\u548cO(n+\u221anln(\u03bc/\u03b5))\uff08\u5f3a\u51f8\uff09\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0c\u4e3a\u51cf\u5c11STIMULUS\u548cSTIMULUS-M\u4e2d\u5468\u671f\u6027\u5168\u68af\u5ea6\u8bc4\u4f30\u7684\u9700\u6c42\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6279\u5904\u7406\u7684\u589e\u5f3a\u7248\u672cSTIMULUS+/STIMULUS-M+\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u3002"}}
{"id": "2506.20563", "pdf": "https://arxiv.org/pdf/2506.20563", "abs": "https://arxiv.org/abs/2506.20563", "authors": ["Lei Zhu", "Jun Zhou", "Rick Siow Mong Goh", "Yong Liu"], "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "Vision Transformer has recently gained tremendous popularity in medical image\nsegmentation task due to its superior capability in capturing long-range\ndependencies. However, transformer requires a large amount of labeled data to\nbe effective, which hinders its applicability in annotation scarce\nsemi-supervised learning scenario where only limited labeled data is available.\nState-of-the-art semi-supervised learning methods propose combinatorial\nCNN-Transformer learning to cross teach a transformer with a convolutional\nneural network, which achieves promising results. However, it remains a\nchallenging task to effectively train the transformer with limited labeled\ndata. In this paper, we propose an adversarial masked image modeling method to\nfully unleash the potential of transformer for semi-supervised medical image\nsegmentation. The key challenge in semi-supervised learning with transformer\nlies in the lack of sufficient supervision signal. To this end, we propose to\nconstruct an auxiliary masked domain from original domain with masked image\nmodeling and train the transformer to predict the entire segmentation mask with\nmasked inputs to increase supervision signal. We leverage the original labels\nfrom labeled data and pseudo-labels from unlabeled data to learn the masked\ndomain. To further benefit the original domain from masked domain, we provide a\ntheoretical analysis of our method from a multi-domain learning perspective and\ndevise a novel adversarial training loss to reduce the domain gap between the\noriginal and masked domain, which boosts semi-supervised learning performance.\nWe also extend adversarial masked image modeling to CNN network. Extensive\nexperiments on three public medical image segmentation datasets demonstrate the\neffectiveness of our method, where our method outperforms existing methods\nsignificantly. Our code is publicly available at\nhttps://github.com/zlheui/AdvMIM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u65b9\u6cd5\uff08AdvMIM\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u76d1\u7763\u4fe1\u53f7\u548c\u51cf\u5c11\u57df\u95f4\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2dTransformer\u7684\u6027\u80fd\u3002", "motivation": "Transformer\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e0b\uff0c\u6807\u6ce8\u6570\u636e\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bad\u7ec3Transformer\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51faAdvMIM\uff0c\u4ee5\u589e\u5f3a\u76d1\u7763\u4fe1\u53f7\u5e76\u63d0\u5347Transformer\u5728\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u6784\u5efa\u8f85\u52a9\u57df\uff0c\u5229\u7528\u6807\u6ce8\u6570\u636e\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3Transformer\u9884\u6d4b\u5b8c\u6574\u5206\u5272\u63a9\u7801\u3002\u63d0\u51fa\u5bf9\u6297\u8bad\u7ec3\u635f\u5931\u4ee5\u51cf\u5c11\u539f\u59cb\u57df\u4e0e\u63a9\u7801\u57df\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u6269\u5c55\u8be5\u65b9\u6cd5\u81f3CNN\u7f51\u7edc\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdvMIM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AdvMIM\u901a\u8fc7\u589e\u5f3a\u76d1\u7763\u4fe1\u53f7\u548c\u57df\u95f4\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3aTransformer\u5728\u6807\u6ce8\u7a00\u7f3a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "AdvMIM\uff1a\u57fa\u4e8e\u5bf9\u6297\u6027\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5", "abstract_zh": "\u89c6\u89c9Transformer\u56e0\u5176\u5728\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u8fd1\u5e74\u6765\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u5e7f\u53d7\u6b22\u8fce\u3002\u7136\u800c\uff0cTransformer\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u624d\u80fd\u53d1\u6325\u6548\u679c\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u6807\u6ce8\u7a00\u7f3a\u7684\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u7ec4\u5408CNN\u4e0eTransformer\u8fdb\u884c\u4ea4\u53c9\u5b66\u4e60\uff0c\u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u6548\u679c\uff0c\u4f46\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u8bad\u7ec3Transformer\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u65b9\u6cd5\uff08AdvMIM\uff09\uff0c\u4ee5\u5145\u5206\u91ca\u653eTransformer\u5728\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6f5c\u529b\u3002\u534a\u76d1\u7763\u5b66\u4e60\u4e2dTransformer\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u7f3a\u4e4f\u8db3\u591f\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u4ece\u539f\u59cb\u57df\u6784\u5efa\u8f85\u52a9\u57df\uff0c\u5e76\u8bad\u7ec3Transformer\u5229\u7528\u63a9\u7801\u8f93\u5165\u9884\u6d4b\u5b8c\u6574\u5206\u5272\u63a9\u7801\uff0c\u4ee5\u589e\u5f3a\u76d1\u7763\u4fe1\u53f7\u3002\u6211\u4eec\u5229\u7528\u6807\u6ce8\u6570\u636e\u7684\u539f\u59cb\u6807\u7b7e\u548c\u672a\u6807\u6ce8\u6570\u636e\u7684\u4f2a\u6807\u7b7e\u5b66\u4e60\u63a9\u7801\u57df\u3002\u4e3a\u8fdb\u4e00\u6b65\u4f7f\u539f\u59cb\u57df\u53d7\u76ca\u4e8e\u63a9\u7801\u57df\uff0c\u6211\u4eec\u4ece\u591a\u57df\u5b66\u4e60\u7684\u89d2\u5ea6\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u8bad\u7ec3\u635f\u5931\u4ee5\u51cf\u5c11\u539f\u59cb\u57df\u4e0e\u63a9\u7801\u57df\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u5347\u534a\u76d1\u7763\u5b66\u4e60\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c06\u5bf9\u6297\u6027\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u6269\u5c55\u81f3CNN\u7f51\u7edc\u3002\u5728\u4e09\u4e2a\u516c\u5f00\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/zlheui/AdvMIM\u3002"}}
{"id": "2506.19884", "pdf": "https://arxiv.org/pdf/2506.19884", "abs": "https://arxiv.org/abs/2506.19884", "authors": ["Zhengxiang Huang", "Chaoyue Niu", "Zhaode Wang", "Jiarui Xue", "Hanming Zhang", "Yugang Wang", "Zewei Xin", "Xiaotang Jiang", "Chengfei Lv", "Fan Wu", "Guihai Chen"], "title": "MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection", "categories": ["cs.OS", "cs.AI", "cs.PF", "cs.SE"], "comment": null, "summary": "As the demand for on-device Large Language Model (LLM) inference grows,\nenergy efficiency has become a major concern, especially for battery-limited\nmobile devices. Our analysis shows that the memory-bound LLM decode phase\ndominates energy use, and yet most existing works focus on accelerating the\nprefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric\nCore Selection (AECS) and integrate it into MNN to create the energy-efficient\nversion, MNN-AECS, the first engine-level system solution without requiring\nroot access or OS modifications for energy-efficient LLM decoding. MNN-AECS is\ndesigned to reduce LLM decoding energy while keeping decode speed within an\nacceptable slowdown threshold by dynamically selecting low-power CPU cores.\nMNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of\nvarious sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%\nwithout slowdown averaged over all 7 devices and 4 datasets. Against other\nengines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS\ndelivers 39% to 78% energy saving and 12% to 363% speedup on average.", "AI": {"tldr": "MNN-AECS\u662f\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u8bbe\u5907\u4e0a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u7801\u7684\u8282\u80fd\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4f4e\u529f\u8017CPU\u6838\u5fc3\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7801\u901f\u5ea6\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002", "motivation": "\u968f\u7740\u8bbe\u5907\u7aef\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u9700\u6c42\u7684\u589e\u957f\uff0c\u80fd\u8017\u95ee\u9898\u6210\u4e3a\u79fb\u52a8\u8bbe\u5907\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u7535\u6c60\u5bb9\u91cf\u6709\u9650\u7684\u8bbe\u5907\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9884\u586b\u5145\u9636\u6bb5\u7684\u52a0\u901f\uff0c\u800c\u5ffd\u89c6\u4e86\u89e3\u7801\u9636\u6bb5\u7684\u80fd\u8017\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8282\u80fd\u6838\u5fc3\u9009\u62e9\uff08AECS\uff09\u6280\u672f\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230MNN\u4e2d\uff0c\u5f62\u6210MNN-AECS\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u52a8\u6001\u9009\u62e9\u4f4e\u529f\u8017CPU\u6838\u5fc3\uff0c\u4ee5\u4f18\u5316\u89e3\u7801\u9636\u6bb5\u7684\u80fd\u8017\uff0c\u540c\u65f6\u786e\u4fdd\u89e3\u7801\u901f\u5ea6\u4e0d\u663e\u8457\u4e0b\u964d\u3002", "result": "\u57285\u6b3eAndroid\u548c2\u6b3eiOS\u8bbe\u5907\u4e0a\u6d4b\u8bd55\u79cd\u4e0d\u540c\u89c4\u6a21\u7684\u6d41\u884cLLM\uff0cMNN-AECS\u76f8\u6bd4\u539f\u59cbMNN\u5e73\u5747\u8282\u80fd23%\uff0c\u4e14\u672a\u663e\u8457\u964d\u4f4e\u901f\u5ea6\u3002\u4e0e\u5176\u4ed6\u5f15\u64ce\u76f8\u6bd4\uff0c\u8282\u80fd\u6548\u679c\u8fbe39%\u81f378%\uff0c\u901f\u5ea6\u63d0\u534712%\u81f3363%\u3002", "conclusion": "MNN-AECS\u662f\u9996\u4e2a\u65e0\u9700root\u6743\u9650\u6216\u64cd\u4f5c\u7cfb\u7edf\u4fee\u6539\u7684\u5f15\u64ce\u7ea7\u8282\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86LLM\u89e3\u7801\u7684\u80fd\u8017\uff0c\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "paper_title_zh": "MNN-AECS\uff1a\u901a\u8fc7\u81ea\u9002\u5e94\u6838\u5fc3\u9009\u62e9\u4f18\u5316\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u89e3\u7801\u7684\u80fd\u8017", "abstract_zh": "\u968f\u7740\u8bbe\u5907\u7aef\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u9700\u6c42\u7684\u589e\u957f\uff0c\u80fd\u8017\u95ee\u9898\u6210\u4e3a\u79fb\u52a8\u8bbe\u5907\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u7535\u6c60\u5bb9\u91cf\u6709\u9650\u7684\u8bbe\u5907\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5185\u5b58\u53d7\u9650\u7684LLM\u89e3\u7801\u9636\u6bb5\u662f\u80fd\u8017\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9884\u586b\u5145\u9636\u6bb5\u7684\u52a0\u901f\uff0c\u5ffd\u89c6\u4e86\u80fd\u8017\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u81ea\u9002\u5e94\u8282\u80fd\u6838\u5fc3\u9009\u62e9\uff08AECS\uff09\u6280\u672f\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230MNN\u4e2d\uff0c\u5f62\u6210MNN-AECS\u7cfb\u7edf\uff0c\u8fd9\u662f\u9996\u4e2a\u65e0\u9700root\u6743\u9650\u6216\u64cd\u4f5c\u7cfb\u7edf\u4fee\u6539\u7684\u5f15\u64ce\u7ea7\u8282\u80fd\u89e3\u51b3\u65b9\u6848\u3002MNN-AECS\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4f4e\u529f\u8017CPU\u6838\u5fc3\uff0c\u5728\u4fdd\u6301\u89e3\u7801\u901f\u5ea6\u53ef\u63a5\u53d7\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u80fd\u8017\u3002\u6211\u4eec\u57285\u6b3eAndroid\u548c2\u6b3eiOS\u8bbe\u5907\u4e0a\u6d4b\u8bd5\u4e865\u79cd\u4e0d\u540c\u89c4\u6a21\u7684\u6d41\u884cLLM\u3002\u76f8\u6bd4\u539f\u59cbMNN\uff0cMNN-AECS\u5e73\u5747\u8282\u80fd23%\uff0c\u4e14\u672a\u663e\u8457\u964d\u4f4e\u901f\u5ea6\u3002\u4e0e\u5176\u4ed6\u5f15\u64ce\uff08\u5982llama.cpp\u3001executorch\u3001mllm\u548cMediaPipe\uff09\u76f8\u6bd4\uff0cMNN-AECS\u5e73\u5747\u8282\u80fd39%\u81f378%\uff0c\u901f\u5ea6\u63d0\u534712%\u81f3363%\u3002"}}
{"id": "2506.20567", "pdf": "https://arxiv.org/pdf/2506.20567", "abs": "https://arxiv.org/abs/2506.20567", "authors": ["Zhiwang Zhang", "Dong Xu", "Wanli Ouyang", "Chuanqi Tan"], "title": "Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "In this work, we propose a division-and-summarization (DaS) framework for\ndense video captioning. After partitioning each untrimmed long video as\nmultiple event proposals, where each event proposal consists of a set of short\nvideo segments, we extract visual feature (e.g., C3D feature) from each segment\nand use the existing image/video captioning approach to generate one sentence\ndescription for this segment. Considering that the generated sentences contain\nrich semantic descriptions about the whole event proposal, we formulate the\ndense video captioning task as a visual cue aided sentence summarization\nproblem and propose a new two stage Long Short Term Memory (LSTM) approach\nequipped with a new hierarchical attention mechanism to summarize all generated\nsentences as one descriptive sentence with the aid of visual features.\nSpecifically, the first-stage LSTM network takes all semantic words from the\ngenerated sentences and the visual features from all segments within one event\nproposal as the input, and acts as the encoder to effectively summarize both\nsemantic and visual information related to this event proposal. The\nsecond-stage LSTM network takes the output from the first-stage LSTM network\nand the visual features from all video segments within one event proposal as\nthe input, and acts as the decoder to generate one descriptive sentence for\nthis event proposal. Our comprehensive experiments on the ActivityNet Captions\ndataset demonstrate the effectiveness of our newly proposed DaS framework for\ndense video captioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u4e0e\u603b\u7ed3\uff08DaS\uff09\u6846\u67b6\u7684\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u957f\u89c6\u9891\u5206\u5272\u4e3a\u591a\u4e2a\u4e8b\u4ef6\u63d0\u6848\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u7279\u5f81\u548c\u53e5\u5b50\u751f\u6210\u6280\u672f\uff0c\u6700\u7ec8\u901a\u8fc7\u4e24\u9636\u6bb5LSTM\u7f51\u7edc\u751f\u6210\u63cf\u8ff0\u6027\u53e5\u5b50\u3002", "motivation": "\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u9700\u8981\u4ece\u957f\u89c6\u9891\u4e2d\u63d0\u53d6\u591a\u4e2a\u4e8b\u4ef6\u5e76\u751f\u6210\u63cf\u8ff0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u548c\u89c6\u89c9\u4fe1\u606f\u7ed3\u5408\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u548c\u53e5\u5b50\u603b\u7ed3\u6280\u672f\uff0c\u63d0\u5347\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u548c\u4e30\u5bcc\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u957f\u89c6\u9891\u5206\u5272\u4e3a\u591a\u4e2a\u4e8b\u4ef6\u63d0\u6848\uff0c\u6bcf\u4e2a\u63d0\u6848\u5305\u542b\u591a\u4e2a\u77ed\u89c6\u9891\u6bb5\uff1b2) \u63d0\u53d6\u6bcf\u4e2a\u6bb5\u7684\u89c6\u89c9\u7279\u5f81\uff08\u5982C3D\u7279\u5f81\uff09\uff0c\u5e76\u751f\u6210\u53e5\u5b50\u63cf\u8ff0\uff1b3) \u63d0\u51fa\u4e24\u9636\u6bb5LSTM\u7f51\u7edc\uff0c\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u6700\u7ec8\u63cf\u8ff0\u3002", "result": "\u5728ActivityNet Captions\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684DaS\u6846\u67b6\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7684\u63cf\u8ff0\u66f4\u5177\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684DaS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u548c\u53e5\u5b50\u603b\u7ed3\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u7684\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u5c55\u793a\u3001\u8bb2\u8ff0\u4e0e\u603b\u7ed3\uff1a\u57fa\u4e8e\u89c6\u89c9\u7ebf\u7d22\u8f85\u52a9\u53e5\u5b50\u603b\u7ed3\u7684\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u4e0e\u603b\u7ed3\uff08DaS\uff09\u6846\u67b6\u7684\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u3002\u9996\u5148\u5c06\u672a\u4fee\u526a\u7684\u957f\u89c6\u9891\u5206\u5272\u4e3a\u591a\u4e2a\u4e8b\u4ef6\u63d0\u6848\uff0c\u6bcf\u4e2a\u63d0\u6848\u5305\u542b\u591a\u4e2a\u77ed\u89c6\u9891\u6bb5\u3002\u968f\u540e\u63d0\u53d6\u6bcf\u4e2a\u6bb5\u7684\u89c6\u89c9\u7279\u5f81\uff08\u5982C3D\u7279\u5f81\uff09\uff0c\u5e76\u5229\u7528\u73b0\u6709\u56fe\u50cf/\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u6bb5\u751f\u6210\u4e00\u53e5\u63cf\u8ff0\u3002\u8003\u8651\u5230\u751f\u6210\u7684\u53e5\u5b50\u5305\u542b\u5173\u4e8e\u6574\u4e2a\u4e8b\u4ef6\u63d0\u6848\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u6211\u4eec\u5c06\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u89c6\u4e3a\u89c6\u89c9\u7ebf\u7d22\u8f85\u52a9\u7684\u53e5\u5b50\u603b\u7ed3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u957f\u77ed\u65f6\u8bb0\u5fc6\uff08LSTM\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u89c6\u89c9\u7279\u5f81\u5c06\u6240\u6709\u751f\u6210\u7684\u53e5\u5b50\u603b\u7ed3\u4e3a\u4e00\u53e5\u63cf\u8ff0\u6027\u53e5\u5b50\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7b2c\u4e00\u9636\u6bb5LSTM\u7f51\u7edc\u5c06\u751f\u6210\u7684\u53e5\u5b50\u4e2d\u7684\u6240\u6709\u8bed\u4e49\u8bcd\u548c\u4e8b\u4ef6\u63d0\u6848\u4e2d\u6240\u6709\u6bb5\u7684\u89c6\u89c9\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f5c\u4e3a\u7f16\u7801\u5668\u6709\u6548\u603b\u7ed3\u4e0e\u8be5\u4e8b\u4ef6\u63d0\u6848\u76f8\u5173\u7684\u8bed\u4e49\u548c\u89c6\u89c9\u4fe1\u606f\u3002\u7b2c\u4e8c\u9636\u6bb5LSTM\u7f51\u7edc\u5c06\u7b2c\u4e00\u9636\u6bb5LSTM\u7f51\u7edc\u7684\u8f93\u51fa\u548c\u4e8b\u4ef6\u63d0\u6848\u4e2d\u6240\u6709\u89c6\u9891\u6bb5\u7684\u89c6\u89c9\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f5c\u4e3a\u89e3\u7801\u5668\u751f\u6210\u8be5\u4e8b\u4ef6\u63d0\u6848\u7684\u63cf\u8ff0\u6027\u53e5\u5b50\u3002\u5728ActivityNet Captions\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b0\u63d0\u51fa\u7684DaS\u6846\u67b6\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.19885", "pdf": "https://arxiv.org/pdf/2506.19885", "abs": "https://arxiv.org/abs/2506.19885", "authors": ["Jing Lu", "Xuan Wu", "Yizhun Tian", "Songhan Fan", "Yali Fang"], "title": "FlightKooba: A Fast Interpretable FTP Model", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "7 figures", "summary": "The Koopman theory is a powerful and effective modeling tool for converting\nnonlinear systems into linear representations, and flight trajectory prediction\n(FTP) is a complex nonlinear system. However, current models applying the\nKoopman theory to FTP tasks are not very effective, model interpretability is\nindeed an issue, and the Koopman operators are computationally intensive,\nresulting in long training times. To address this issue, this paper proposes a\nnew modeling and control framework based on the HIPPO method, the Koopman\ntheory, and state space equations from cybernetics: FlightKooba. Inspired by\nthe idea of structural state space equations, FlightKooba directly constructs\nthe Koopman operators from data. This makes the framework highly interpretable\nand significantly reduces the number of trainable parameters in the module,\nthereby greatly reducing training time. Experiments have demonstrated the\nsuperiority of the FlightKooba modeling method in terms of time and memory\nconsumption (training time comparable to the Mamba module without using\nCUDA-level acceleration; memory reduced by more than 50% on most datasets, with\na tenfold reduction in the number of parameters), essentially completing the\nFTP task. It provides a new method for the fast computation of the Koopman\noperators, opening up new possibilities for the combination of time series\nforecasting and control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHIPPO\u65b9\u6cd5\u3001Koopman\u7406\u8bba\u548c\u63a7\u5236\u8bba\u72b6\u6001\u7a7a\u95f4\u65b9\u7a0b\u7684\u65b0\u578b\u5efa\u6a21\u4e0e\u63a7\u5236\u6846\u67b6FlightKooba\uff0c\u7528\u4e8e\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684\u98de\u884c\u8f68\u8ff9\u9884\u6d4b\uff08FTP\uff09\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u65f6\u95f4\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eKoopman\u7406\u8bba\u7684FTP\u6a21\u578b\u5b58\u5728\u6548\u7387\u4f4e\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FlightKooba\u7ed3\u5408HIPPO\u65b9\u6cd5\u3001Koopman\u7406\u8bba\u548c\u72b6\u6001\u7a7a\u95f4\u65b9\u7a0b\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u6784\u5efaKoopman\u7b97\u5b50\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlightKooba\u5728\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u4e0a\u8868\u73b0\u4f18\u8d8a\uff08\u8bad\u7ec3\u65f6\u95f4\u63a5\u8fd1\u672a\u52a0\u901f\u7684Mamba\u6a21\u5757\uff0c\u5185\u5b58\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u5341\u500d\uff09\uff0c\u6210\u529f\u5b8c\u6210FTP\u4efb\u52a1\u3002", "conclusion": "FlightKooba\u4e3aKoopman\u7b97\u5b50\u7684\u5feb\u901f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u63a7\u5236\u7684\u7ed3\u5408\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u3002", "paper_title_zh": "FlightKooba\uff1a\u4e00\u79cd\u5feb\u901f\u53ef\u89e3\u91ca\u7684FTP\u6a21\u578b", "abstract_zh": "Koopman\u7406\u8bba\u662f\u4e00\u79cd\u5c06\u975e\u7ebf\u6027\u7cfb\u7edf\u8f6c\u6362\u4e3a\u7ebf\u6027\u8868\u793a\u7684\u6709\u529b\u5de5\u5177\uff0c\u800c\u98de\u884c\u8f68\u8ff9\u9884\u6d4b\uff08FTP\uff09\u662f\u4e00\u4e2a\u590d\u6742\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u5f53\u524d\u5c06Koopman\u7406\u8bba\u5e94\u7528\u4e8eFTP\u4efb\u52a1\u7684\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u53ef\u89e3\u91ca\u6027\u5b58\u5728\u95ee\u9898\uff0c\u4e14Koopman\u7b97\u5b50\u8ba1\u7b97\u91cf\u5927\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u957f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHIPPO\u65b9\u6cd5\u3001Koopman\u7406\u8bba\u548c\u63a7\u5236\u8bba\u72b6\u6001\u7a7a\u95f4\u65b9\u7a0b\u7684\u65b0\u578b\u5efa\u6a21\u4e0e\u63a7\u5236\u6846\u67b6\uff1aFlightKooba\u3002\u53d7\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u65b9\u7a0b\u542f\u53d1\uff0cFlightKooba\u76f4\u63a5\u4ece\u6570\u636e\u6784\u5efaKoopman\u7b97\u5b50\u3002\u8fd9\u4f7f\u5f97\u6846\u67b6\u5177\u6709\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u5757\u4e2d\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4ece\u800c\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86FlightKooba\u5efa\u6a21\u65b9\u6cd5\u5728\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u4e0a\u7684\u4f18\u8d8a\u6027\uff08\u8bad\u7ec3\u65f6\u95f4\u63a5\u8fd1\u672a\u4f7f\u7528CUDA\u7ea7\u52a0\u901f\u7684Mamba\u6a21\u5757\uff1b\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u5185\u5b58\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u5341\u500d\uff09\uff0c\u57fa\u672c\u5b8c\u6210\u4e86FTP\u4efb\u52a1\u3002\u5b83\u4e3aKoopman\u7b97\u5b50\u7684\u5feb\u901f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u63a7\u5236\u7684\u7ed3\u5408\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2506.20097", "pdf": "https://arxiv.org/pdf/2506.20097", "abs": "https://arxiv.org/abs/2506.20097", "authors": ["Wang Bill Zhu", "Miaosen Chai", "Ishika Singh", "Robin Jia", "Jesse Thomason"], "title": "PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "We propose PSALM-V, the first autonomous neuro-symbolic learning system able\nto induce symbolic action semantics (i.e., pre- and post-conditions) in visual\nenvironments through interaction. PSALM-V bootstraps reliable symbolic planning\nwithout expert action definitions, using LLMs to generate heuristic plans and\ncandidate symbolic semantics. Previous work has explored using large language\nmodels to generate action semantics for Planning Domain Definition Language\n(PDDL)-based symbolic planners. However, these approaches have primarily\nfocused on text-based domains or relied on unrealistic assumptions, such as\naccess to a predefined problem file, full observability, or explicit error\nmessages. By contrast, PSALM-V dynamically infers PDDL problem files and domain\naction semantics by analyzing execution outcomes and synthesizing possible\nerror explanations. The system iteratively generates and executes plans while\nmaintaining a tree-structured belief over possible action semantics for each\naction, iteratively refining these beliefs until a goal state is reached.\nSimulated experiments of task completion in ALFRED demonstrate that PSALM-V\nincreases the plan success rate from 37% (Claude-3.7) to 74% in partially\nobserved setups. Results on two 2D game environments, RTFM and Overcooked-AI,\nshow that PSALM-V improves step efficiency and succeeds in domain induction in\nmulti-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions\nfor real-world robot BlocksWorld tasks, despite low-level manipulation failures\nfrom the robot.", "AI": {"tldr": "PSALM-V\u662f\u9996\u4e2a\u901a\u8fc7\u4ea4\u4e92\u5728\u89c6\u89c9\u73af\u5883\u4e2d\u81ea\u52a8\u63a8\u65ad\u7b26\u53f7\u52a8\u4f5c\u8bed\u4e49\uff08\u5982\u524d\u7f6e\u548c\u540e\u7f6e\u6761\u4ef6\uff09\u7684\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u542f\u53d1\u5f0f\u8ba1\u5212\u548c\u5019\u9009\u7b26\u53f7\u8bed\u4e49\uff0c\u65e0\u9700\u4e13\u5bb6\u5b9a\u4e49\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u90e8\u5206\u89c2\u5bdf\u73af\u5883\u4e0b\u7684\u8ba1\u5212\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u9886\u57df\u6216\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff08\u5982\u9884\u5b9a\u4e49\u95ee\u9898\u6587\u4ef6\u3001\u5b8c\u5168\u53ef\u89c2\u5bdf\u6027\uff09\uff0cPSALM-V\u65e8\u5728\u52a8\u6001\u63a8\u65adPDDL\u95ee\u9898\u6587\u4ef6\u548c\u52a8\u4f5c\u8bed\u4e49\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u89c6\u89c9\u73af\u5883\u548c\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u3002", "method": "PSALM-V\u901a\u8fc7\u5206\u6790\u6267\u884c\u7ed3\u679c\u548c\u5408\u6210\u53ef\u80fd\u7684\u9519\u8bef\u89e3\u91ca\uff0c\u52a8\u6001\u63a8\u65adPDDL\u95ee\u9898\u6587\u4ef6\u548c\u52a8\u4f5c\u8bed\u4e49\uff0c\u8fed\u4ee3\u751f\u6210\u548c\u6267\u884c\u8ba1\u5212\uff0c\u540c\u65f6\u7ef4\u62a4\u52a8\u4f5c\u8bed\u4e49\u7684\u6811\u72b6\u4fe1\u5ff5\u7ed3\u6784\uff0c\u76f4\u81f3\u8fbe\u6210\u76ee\u6807\u72b6\u6001\u3002", "result": "\u5728ALFRED\u4efb\u52a1\u4e2d\uff0cPSALM-V\u5c06\u8ba1\u5212\u6210\u529f\u7387\u4ece37%\u63d0\u5347\u81f374%\uff1b\u5728RTFM\u548cOvercooked-AI\u7b492D\u6e38\u620f\u73af\u5883\u4e2d\uff0c\u63d0\u9ad8\u4e86\u6b65\u9aa4\u6548\u7387\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u9886\u57df\u7684\u8bed\u4e49\u63a8\u65ad\u3002", "conclusion": "PSALM-V\u5728\u90e8\u5206\u89c2\u5bdf\u548c\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u63a8\u65ad\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684PDDL\u6761\u4ef6\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "PSALM-V\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u89c6\u89c9\u73af\u5883\u4e2d\u81ea\u52a8\u5316\u7b26\u53f7\u89c4\u5212", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86PSALM-V\uff0c\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u4ea4\u4e92\u5728\u89c6\u89c9\u73af\u5883\u4e2d\u81ea\u52a8\u63a8\u65ad\u7b26\u53f7\u52a8\u4f5c\u8bed\u4e49\uff08\u5982\u524d\u7f6e\u548c\u540e\u7f6e\u6761\u4ef6\uff09\u7684\u81ea\u4e3b\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7cfb\u7edf\u3002PSALM-V\u65e0\u9700\u4e13\u5bb6\u5b9a\u4e49\u52a8\u4f5c\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u542f\u53d1\u5f0f\u8ba1\u5212\u548c\u5019\u9009\u7b26\u53f7\u8bed\u4e49\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u7b26\u53f7\u89c4\u5212\u3002\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6587\u672c\u7684\u9886\u57df\u6216\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff08\u5982\u9884\u5b9a\u4e49\u95ee\u9898\u6587\u4ef6\u3001\u5b8c\u5168\u53ef\u89c2\u5bdf\u6027\u6216\u663e\u5f0f\u9519\u8bef\u6d88\u606f\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cPSALM-V\u901a\u8fc7\u5206\u6790\u6267\u884c\u7ed3\u679c\u548c\u5408\u6210\u53ef\u80fd\u7684\u9519\u8bef\u89e3\u91ca\uff0c\u52a8\u6001\u63a8\u65adPDDL\u95ee\u9898\u6587\u4ef6\u548c\u9886\u57df\u52a8\u4f5c\u8bed\u4e49\u3002\u8be5\u7cfb\u7edf\u8fed\u4ee3\u751f\u6210\u548c\u6267\u884c\u8ba1\u5212\uff0c\u540c\u65f6\u7ef4\u62a4\u6bcf\u4e2a\u52a8\u4f5c\u53ef\u80fd\u8bed\u4e49\u7684\u6811\u72b6\u4fe1\u5ff5\u7ed3\u6784\uff0c\u9010\u6b65\u4f18\u5316\u8fd9\u4e9b\u4fe1\u5ff5\u76f4\u81f3\u8fbe\u6210\u76ee\u6807\u72b6\u6001\u3002\u5728ALFRED\u4efb\u52a1\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cPSALM-V\u5c06\u90e8\u5206\u89c2\u5bdf\u73af\u5883\u4e0b\u7684\u8ba1\u5212\u6210\u529f\u7387\u4ece37%\uff08Claude-3.7\uff09\u63d0\u5347\u81f374%\u3002\u5728RTFM\u548cOvercooked-AI\u4e24\u4e2a2D\u6e38\u620f\u73af\u5883\u4e2d\u7684\u7ed3\u679c\u8868\u660e\uff0cPSALM-V\u63d0\u9ad8\u4e86\u6b65\u9aa4\u6548\u7387\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u9886\u57df\u8bed\u4e49\u63a8\u65ad\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u673a\u5668\u4eba\u5e95\u5c42\u64cd\u4f5c\u5931\u8d25\uff0cPSALM-V\u4ecd\u80fd\u6b63\u786e\u63a8\u65ad\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4ebaBlocksWorld\u4efb\u52a1\u7684PDDL\u524d\u7f6e\u548c\u540e\u7f6e\u6761\u4ef6\u3002"}}
{"id": "2506.20582", "pdf": "https://arxiv.org/pdf/2506.20582", "abs": "https://arxiv.org/abs/2506.20582", "authors": ["Rajat Rasal", "Avinash Kori", "Ben Glocker"], "title": "Causal Representation Learning with Observational Grouping for CXR Classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Identifiable causal representation learning seeks to uncover the true causal\nrelationships underlying a data generation process. In medical imaging, this\npresents opportunities to improve the generalisability and robustness of\ntask-specific latent features. This work introduces the concept of grouping\nobservations to learn identifiable representations for disease classification\nin chest X-rays via an end-to-end framework. Our experiments demonstrate that\nthese causal representations improve generalisability and robustness across\nmultiple classification tasks when grouping is used to enforce invariance w.r.t\nrace, sex, and imaging views.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c2\u5bdf\u5206\u7ec4\u5b66\u4e60\u53ef\u8bc6\u522b\u56e0\u679c\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u5206\u7c7b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\uff0c\u53ef\u8bc6\u522b\u56e0\u679c\u8868\u793a\u5b66\u4e60\u80fd\u591f\u63ed\u793a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u4efb\u52a1\u7279\u5b9a\u6f5c\u5728\u7279\u5f81\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u89c2\u5bdf\u5206\u7ec4\u7684\u6982\u5ff5\uff0c\u5229\u7528\u7aef\u5230\u7aef\u6846\u67b6\u5b66\u4e60\u53ef\u8bc6\u522b\u8868\u793a\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5206\u7ec4\u5f3a\u5236\u6a21\u578b\u5bf9\u79cd\u65cf\u3001\u6027\u522b\u548c\u6210\u50cf\u89c6\u89d2\u7b49\u53d8\u91cf\u4fdd\u6301\u4e0d\u53d8\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5206\u7ec4\u5b66\u4e60\u7684\u56e0\u679c\u8868\u793a\u5728\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u89c2\u5bdf\u5206\u7ec4\u5b66\u4e60\u56e0\u679c\u8868\u793a\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u4e0d\u540c\u79cd\u65cf\u3001\u6027\u522b\u548c\u6210\u50cf\u89c6\u89d2\u65f6\u3002", "paper_title_zh": "\u57fa\u4e8e\u89c2\u5bdf\u5206\u7ec4\u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60\u5728\u80f8\u90e8X\u5149\u5206\u7c7b\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u53ef\u8bc6\u522b\u56e0\u679c\u8868\u793a\u5b66\u4e60\u65e8\u5728\u63ed\u793a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\u3002\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\uff0c\u8fd9\u4e3a\u63d0\u9ad8\u4efb\u52a1\u7279\u5b9a\u6f5c\u5728\u7279\u5f81\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u89c2\u5bdf\u5206\u7ec4\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6846\u67b6\u5b66\u4e60\u53ef\u8bc6\u522b\u8868\u793a\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u5206\u7c7b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u5229\u7528\u5206\u7ec4\u5f3a\u5236\u6a21\u578b\u5bf9\u79cd\u65cf\u3001\u6027\u522b\u548c\u6210\u50cf\u89c6\u89d2\u7b49\u53d8\u91cf\u4fdd\u6301\u4e0d\u53d8\u6027\u65f6\uff0c\u8fd9\u4e9b\u56e0\u679c\u8868\u793a\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.19887", "pdf": "https://arxiv.org/pdf/2506.19887", "abs": "https://arxiv.org/abs/2506.19887", "authors": ["Hyo Jin Jon", "Longbin Jin", "Hyuntaek Jung", "Hyunseo Kim", "Donghun Min", "Eun Yi Kim"], "title": "MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition", "categories": ["eess.AS", "cs.AI", "cs.SD", "68T10"], "comment": "5 pages, 4 figures, 2 tables, 1 algorithm, Accepted to INTERSPEECH\n  2025", "summary": "This paper presents our contributions to the Speech Emotion Recognition in\nNaturalistic Conditions (SERNC) Challenge, where we address categorical emotion\nrecognition and emotional attribute prediction. To handle the complexities of\nnatural speech, including intra- and inter-subject variability, we propose\nMulti-level Acoustic-Textual Emotion Representation (MATER), a novel\nhierarchical framework that integrates acoustic and textual features at the\nword, utterance, and embedding levels. By fusing low-level lexical and acoustic\ncues with high-level contextualized representations, MATER effectively captures\nboth fine-grained prosodic variations and semantic nuances. Additionally, we\nintroduce an uncertainty-aware ensemble strategy to mitigate annotator\ninconsistencies, improving robustness in ambiguous emotional expressions. MATER\nranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of\n0.5928, securing second place in valence prediction with an impressive CCC of\n0.6941.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMATER\u7684\u591a\u5c42\u6b21\u58f0\u5b66\u4e0e\u6587\u672c\u60c5\u611f\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7136\u6761\u4ef6\u4e0b\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff0c\u901a\u8fc7\u878d\u5408\u58f0\u5b66\u548c\u6587\u672c\u7279\u5f81\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u6311\u6218\uff0c\u5305\u62ec\u8bf4\u8bdd\u8005\u5185\u548c\u8bf4\u8bdd\u8005\u95f4\u7684\u53d8\u5f02\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6355\u6349\u8bed\u97f3\u4e2d\u7ec6\u5fae\u60c5\u611f\u53d8\u5316\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u6846\u67b6\u3002", "method": "MATER\u6846\u67b6\u901a\u8fc7\u591a\u5c42\u6b21\uff08\u8bcd\u3001\u8bdd\u8bed\u548c\u5d4c\u5165\u7ea7\u522b\uff09\u878d\u5408\u58f0\u5b66\u548c\u6587\u672c\u7279\u5f81\uff0c\u7ed3\u5408\u4f4e\u5c42\u6b21\u8bcd\u6c47\u4e0e\u58f0\u5b66\u7ebf\u7d22\u4ee5\u53ca\u9ad8\u5c42\u6b21\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u540c\u65f6\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u96c6\u6210\u7b56\u7565\u4ee5\u51cf\u5c11\u6807\u6ce8\u4e0d\u4e00\u81f4\u6027\u3002", "result": "MATER\u5728SERNC\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5206\u7c7b\u60c5\u611f\u8bc6\u522b\u548c\u60c5\u611f\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u5206\u522b\u6392\u540d\u7b2c\u56db\uff0cMacro-F1\u4e3a41.01%\uff0c\u5e73\u5747CCC\u4e3a0.5928\uff0c\u5e76\u5728\u6548\u4ef7\u9884\u6d4b\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\u7684\u6210\u7ee9\uff08CCC\u4e3a0.6941\uff09\u3002", "conclusion": "MATER\u6846\u67b6\u901a\u8fc7\u591a\u5c42\u6b21\u7279\u5f81\u878d\u5408\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u60c5\u611f\u8868\u8fbe\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "paper_title_zh": "MATER\uff1a\u591a\u5c42\u6b21\u58f0\u5b66\u4e0e\u6587\u672c\u60c5\u611f\u8868\u793a\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6211\u4eec\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SERNC\uff09\u6311\u6218\u4e2d\u7684\u8d21\u732e\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u5206\u7c7b\u60c5\u611f\u8bc6\u522b\u548c\u60c5\u611f\u5c5e\u6027\u9884\u6d4b\u95ee\u9898\u3002\u4e3a\u5e94\u5bf9\u81ea\u7136\u8bed\u97f3\u7684\u590d\u6742\u6027\uff08\u5305\u62ec\u8bf4\u8bdd\u8005\u5185\u548c\u8bf4\u8bdd\u8005\u95f4\u7684\u53d8\u5f02\u6027\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u5c42\u6b21\u58f0\u5b66-\u6587\u672c\u60c5\u611f\u8868\u793a\uff08MATER\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8bcd\u3001\u8bdd\u8bed\u548c\u5d4c\u5165\u7ea7\u522b\u6574\u5408\u58f0\u5b66\u548c\u6587\u672c\u7279\u5f81\u3002\u901a\u8fc7\u5c06\u4f4e\u5c42\u6b21\u8bcd\u6c47\u4e0e\u58f0\u5b66\u7ebf\u7d22\u4e0e\u9ad8\u5c42\u6b21\u4e0a\u4e0b\u6587\u8868\u793a\u76f8\u7ed3\u5408\uff0cMATER\u6709\u6548\u6355\u6349\u4e86\u7ec6\u5fae\u7684\u97f5\u5f8b\u53d8\u5316\u548c\u8bed\u4e49\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u6807\u6ce8\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u6a21\u7cca\u60c5\u611f\u8868\u8fbe\u7684\u9c81\u68d2\u6027\u3002MATER\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5747\u6392\u540d\u7b2c\u56db\uff0cMacro-F1\u4e3a41.01%\uff0c\u5e73\u5747CCC\u4e3a0.5928\uff0c\u5e76\u5728\u6548\u4ef7\u9884\u6d4b\u4e2d\u4ee50.6941\u7684CCC\u6210\u7ee9\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002"}}
{"id": "2506.20583", "pdf": "https://arxiv.org/pdf/2506.20583", "abs": "https://arxiv.org/abs/2506.20583", "authors": ["Zhiwang Zhang", "Dong Xu", "Wanli Ouyang", "Luping Zhou"], "title": "Dense Video Captioning using Graph-based Sentence Summarization", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages", "summary": "Recently, dense video captioning has made attractive progress in detecting\nand captioning all events in a long untrimmed video. Despite promising results\nwere achieved, most existing methods do not sufficiently explore the scene\nevolution within an event temporal proposal for captioning, and therefore\nperform less satisfactorily when the scenes and objects change over a\nrelatively long proposal. To address this problem, we propose a graph-based\npartition-and-summarization (GPaS) framework for dense video captioning within\ntwo stages. For the ``partition\" stage, a whole event proposal is split into\nshort video segments for captioning at a finer level. For the ``summarization\"\nstage, the generated sentences carrying rich description information for each\nsegment are summarized into one sentence to describe the whole event. We\nparticularly focus on the ``summarization\" stage, and propose a framework that\neffectively exploits the relationship between semantic words for summarization.\nWe achieve this goal by treating semantic words as nodes in a graph and\nlearning their interactions by coupling Graph Convolutional Network (GCN) and\nLong Short Term Memory (LSTM), with the aid of visual cues. Two schemes of\nGCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN\nand LSTM. The effectiveness of our approach is demonstrated via an extensive\ncomparison with the state-of-the-arts methods on the two benchmarks ActivityNet\nCaptions dataset and YouCook II dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5206\u5272\u4e0e\u603b\u7ed3\uff08GPaS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u3002\u901a\u8fc7\u5c06\u4e8b\u4ef6\u63d0\u6848\u5206\u5272\u4e3a\u77ed\u7247\u6bb5\u5e76\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u4ea4\u4e92\u6a21\u5757\uff08GLI\uff09\u8fdb\u884c\u53e5\u5b50\u603b\u7ed3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63cf\u8ff0\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u63a2\u7d22\u4e8b\u4ef6\u63d0\u6848\u5185\u7684\u573a\u666f\u6f14\u53d8\uff0c\u5bfc\u81f4\u5728\u573a\u666f\u548c\u5bf9\u8c61\u53d8\u5316\u8f83\u957f\u7684\u63d0\u6848\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u5272\u548c\u53e5\u5b50\u603b\u7ed3\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faGPaS\u6846\u67b6\uff0c\u5206\u4e3a\u5206\u5272\u548c\u603b\u7ed3\u4e24\u9636\u6bb5\u3002\u5206\u5272\u9636\u6bb5\u5c06\u4e8b\u4ef6\u63d0\u6848\u62c6\u5206\u4e3a\u77ed\u7247\u6bb5\u8fdb\u884c\u63cf\u8ff0\uff1b\u603b\u7ed3\u9636\u6bb5\u901a\u8fc7GCN-LSTM\u4ea4\u4e92\u6a21\u5757\uff08GLI\uff09\u5c06\u7247\u6bb5\u53e5\u5b50\u603b\u7ed3\u4e3a\u5b8c\u6574\u4e8b\u4ef6\u63cf\u8ff0\u3002", "result": "\u5728ActivityNet Captions\u548cYouCook II\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GPaS\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u5272\u548c\u53e5\u5b50\u603b\u7ed3\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u957f\u4e8b\u4ef6\u63d0\u6848\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "paper_title_zh": "\u57fa\u4e8e\u56fe\u53e5\u5b50\u603b\u7ed3\u7684\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u5728\u68c0\u6d4b\u548c\u63cf\u8ff0\u957f\u672a\u526a\u8f91\u89c6\u9891\u4e2d\u7684\u6240\u6709\u4e8b\u4ef6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u5c3d\u7ba1\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u63a2\u7d22\u4e8b\u4ef6\u63d0\u6848\u5185\u7684\u573a\u666f\u6f14\u53d8\uff0c\u5bfc\u81f4\u5728\u573a\u666f\u548c\u5bf9\u8c61\u53d8\u5316\u8f83\u957f\u7684\u63d0\u6848\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5206\u5272\u4e0e\u603b\u7ed3\uff08GPaS\uff09\u6846\u67b6\uff0c\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u5206\u5272\u9636\u6bb5\u5c06\u6574\u4e2a\u4e8b\u4ef6\u63d0\u6848\u62c6\u5206\u4e3a\u77ed\u7247\u6bb5\u8fdb\u884c\u66f4\u7ec6\u7c92\u5ea6\u7684\u63cf\u8ff0\uff1b\u603b\u7ed3\u9636\u6bb5\u5c06\u6bcf\u4e2a\u7247\u6bb5\u7684\u63cf\u8ff0\u53e5\u5b50\u603b\u7ed3\u4e3a\u5b8c\u6574\u4e8b\u4ef6\u63cf\u8ff0\u3002\u6211\u4eec\u7279\u522b\u5173\u6ce8\u603b\u7ed3\u9636\u6bb5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u8bed\u4e49\u8bcd\u89c6\u4e3a\u56fe\u4e2d\u7684\u8282\u70b9\u5e76\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u4ea4\u4e92\u6a21\u5757\uff08GLI\uff09\u5b66\u4e60\u5176\u5173\u7cfb\u7684\u6846\u67b6\u3002\u5b9e\u9a8c\u5728ActivityNet Captions\u548cYouCook II\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.19889", "pdf": "https://arxiv.org/pdf/2506.19889", "abs": "https://arxiv.org/abs/2506.19889", "authors": ["Wanli Peng", "Xin Chen", "Hang Fu", "XinYu He", "Xue Yiming", "Juan Wen"], "title": "Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have made a profound impact\non our society and also raised new security concerns. Particularly, due to the\nremarkable inference ability of LLMs, the privacy violation attack (PVA),\nrevealed by Staab et al., introduces serious personal privacy issues. Existing\ndefense methods mainly leverage LLMs to anonymize the input query, which\nrequires costly inference time and cannot gain satisfactory defense\nperformance. Moreover, directly rejecting the PVA query seems like an effective\ndefense method, while the defense method is exposed, promoting the evolution of\nPVA. In this paper, we propose a novel defense paradigm based on\nretrieval-confused generation (RCG) of LLMs, which can efficiently and covertly\ndefend the PVA. We first design a paraphrasing prompt to induce the LLM to\nrewrite the \"user comments\" of the attack query to construct a disturbed\ndatabase. Then, we propose the most irrelevant retrieval strategy to retrieve\nthe desired user data from the disturbed database. Finally, the \"data comments\"\nare replaced with the retrieved user data to form a defended query, leading to\nresponding to the adversary with some wrong personal attributes, i.e., the\nattack fails. Extensive experiments are conducted on two datasets and eight\npopular LLMs to comprehensively evaluate the feasibility and the superiority of\nthe proposed defense method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u6df7\u6dc6\u751f\u6210\uff08RCG\uff09\u7684\u65b0\u9632\u5fa1\u8303\u5f0f\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9690\u853d\u5730\u62b5\u5fa1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9690\u79c1\u4fb5\u72af\u653b\u51fb\uff08PVA\uff09\u3002\u901a\u8fc7\u6539\u5199\u653b\u51fb\u67e5\u8be2\u7684\u201c\u7528\u6237\u8bc4\u8bba\u201d\u5e76\u6784\u5efa\u5e72\u6270\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u6700\u4e0d\u76f8\u5173\u68c0\u7d22\u7b56\u7565\uff0c\u6700\u7ec8\u751f\u6210\u9632\u5fa1\u6027\u67e5\u8be2\uff0c\u4f7f\u653b\u51fb\u8005\u83b7\u53d6\u9519\u8bef\u7684\u4e2a\u4eba\u5c5e\u6027\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u5f3a\u5927\uff0c\u9690\u79c1\u4fb5\u72af\u653b\u51fb\uff08PVA\uff09\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u76f4\u63a5\u62d2\u7edd\u67e5\u8be2\u4f1a\u66b4\u9732\u9632\u5fa1\u7b56\u7565\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u853d\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "1. \u8bbe\u8ba1\u6539\u5199\u63d0\u793a\uff0c\u8bf1\u5bfcLLM\u91cd\u5199\u653b\u51fb\u67e5\u8be2\u7684\u201c\u7528\u6237\u8bc4\u8bba\u201d\uff0c\u6784\u5efa\u5e72\u6270\u6570\u636e\u5e93\uff1b2. \u63d0\u51fa\u6700\u4e0d\u76f8\u5173\u68c0\u7d22\u7b56\u7565\uff0c\u4ece\u5e72\u6270\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76ee\u6807\u7528\u6237\u6570\u636e\uff1b3. \u5c06\u201c\u6570\u636e\u8bc4\u8bba\u201d\u66ff\u6362\u4e3a\u68c0\u7d22\u5230\u7684\u7528\u6237\u6570\u636e\uff0c\u751f\u6210\u9632\u5fa1\u6027\u67e5\u8be2\uff0c\u4f7f\u653b\u51fb\u8005\u83b7\u53d6\u9519\u8bef\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u516b\u79cd\u6d41\u884c\u7684LLM\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u9632\u5fa1\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u57fa\u4e8e\u68c0\u7d22\u6df7\u6dc6\u751f\u6210\u7684\u9632\u5fa1\u8303\u5f0f\u80fd\u591f\u9ad8\u6548\u4e14\u9690\u853d\u5730\u62b5\u5fa1\u9690\u79c1\u4fb5\u72af\u653b\u51fb\uff0c\u4e3aLLM\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u68c0\u7d22\u6df7\u6dc6\u751f\u6210\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9690\u79c1\u4fb5\u72af\u653b\u51fb\u7684\u6709\u6548\u9632\u5fa1\u624b\u6bb5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u793e\u4f1a\u4ea7\u751f\u4e86\u6df1\u8fdc\u5f71\u54cd\uff0c\u540c\u65f6\u4e5f\u5f15\u53d1\u4e86\u65b0\u7684\u5b89\u5168\u95ee\u9898\u3002\u7279\u522b\u662f\u7531\u4e8eLLM\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0cStaab\u7b49\u4eba\u63ed\u793a\u7684\u9690\u79c1\u4fb5\u72af\u653b\u51fb\uff08PVA\uff09\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u4e2a\u4eba\u9690\u79c1\u95ee\u9898\u3002\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56LLM\u5bf9\u8f93\u5165\u67e5\u8be2\u8fdb\u884c\u533f\u540d\u5316\u5904\u7406\uff0c\u4e0d\u4ec5\u9700\u8981\u9ad8\u6602\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u4e14\u9632\u5fa1\u6548\u679c\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u76f4\u63a5\u62d2\u7eddPVA\u67e5\u8be2\u770b\u4f3c\u6709\u6548\uff0c\u4f46\u4f1a\u66b4\u9732\u9632\u5fa1\u65b9\u6cd5\uff0c\u4fc3\u4f7fPVA\u8fdb\u4e00\u6b65\u6f14\u5316\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u68c0\u7d22\u6df7\u6dc6\u751f\u6210\uff08RCG\uff09\u7684\u65b0\u578b\u9632\u5fa1\u8303\u5f0f\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u9690\u853d\u5730\u62b5\u5fa1PVA\u3002\u6211\u4eec\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6539\u5199\u63d0\u793a\uff0c\u8bf1\u5bfcLLM\u91cd\u5199\u653b\u51fb\u67e5\u8be2\u7684\u201c\u7528\u6237\u8bc4\u8bba\u201d\uff0c\u6784\u5efa\u5e72\u6270\u6570\u636e\u5e93\uff1b\u968f\u540e\u63d0\u51fa\u6700\u4e0d\u76f8\u5173\u68c0\u7d22\u7b56\u7565\uff0c\u4ece\u5e72\u6270\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76ee\u6807\u7528\u6237\u6570\u636e\uff1b\u6700\u540e\u5c06\u201c\u6570\u636e\u8bc4\u8bba\u201d\u66ff\u6362\u4e3a\u68c0\u7d22\u5230\u7684\u7528\u6237\u6570\u636e\uff0c\u751f\u6210\u9632\u5fa1\u6027\u67e5\u8be2\uff0c\u4f7f\u653b\u51fb\u8005\u83b7\u53d6\u9519\u8bef\u7684\u4e2a\u4eba\u5c5e\u6027\uff0c\u5373\u653b\u51fb\u5931\u8d25\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u516b\u79cd\u6d41\u884c\u7684LLM\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u6240\u63d0\u9632\u5fa1\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.20268", "pdf": "https://arxiv.org/pdf/2506.20268", "abs": "https://arxiv.org/abs/2506.20268", "authors": ["Ruben Janssens", "Jens De Bock", "Sofie Labat", "Eva Verhelst", "Veronique Hoste", "Tony Belpaeme"], "title": "Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue", "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "Accepted at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN 2025)", "summary": "Detecting miscommunication in human-robot interaction is a critical function\nfor maintaining user engagement and trust. While humans effortlessly detect\ncommunication errors in conversations through both verbal and non-verbal cues,\nrobots face significant challenges in interpreting non-verbal feedback, despite\nadvances in computer vision for recognizing affective expressions. This\nresearch evaluates the effectiveness of machine learning models in detecting\nmiscommunications in robot dialogue. Using a multi-modal dataset of 240\nhuman-robot conversations, where four distinct types of conversational failures\nwere systematically introduced, we assess the performance of state-of-the-art\ncomputer vision models. After each conversational turn, users provided feedback\non whether they perceived an error, enabling an analysis of the models' ability\nto accurately detect robot mistakes. Despite using state-of-the-art models, the\nperformance barely exceeds random chance in identifying miscommunication, while\non a dataset with more expressive emotional content, they successfully\nidentified confused states. To explore the underlying cause, we asked human\nraters to do the same. They could also only identify around half of the induced\nmiscommunications, similarly to our model. These results uncover a fundamental\nlimitation in identifying robot miscommunications in dialogue: even when users\nperceive the induced miscommunication as such, they often do not communicate\nthis to their robotic conversation partner. This knowledge can shape\nexpectations of the performance of computer vision models and can help\nresearchers to design better human-robot conversations by deliberately\neliciting feedback where needed.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u673a\u5668\u4eba\u5728\u4eba\u673a\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u9519\u8bef\u7684\u6027\u80fd\u63a5\u8fd1\u968f\u673a\u6982\u7387\uff0c\u4e3b\u8981\u56e0\u4e3a\u7528\u6237\u5373\u4f7f\u611f\u77e5\u5230\u9519\u8bef\u4e5f\u672a\u5fc5\u53cd\u9988\u7ed9\u673a\u5668\u4eba\u3002", "motivation": "\u4eba\u673a\u4ea4\u4e92\u4e2d\u68c0\u6d4b\u6c9f\u901a\u9519\u8bef\u5bf9\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u673a\u5668\u4eba\u96be\u4ee5\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u975e\u8bed\u8a00\u7ebf\u7d22\u8bc6\u522b\u9519\u8bef\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5305\u542b240\u6bb5\u4eba\u673a\u5bf9\u8bdd\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5f15\u5165\u56db\u79cd\u5bf9\u8bdd\u5931\u8d25\u7c7b\u578b\uff0c\u5e76\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u68c0\u6d4b\u9519\u8bef\uff0c\u540c\u65f6\u6536\u96c6\u7528\u6237\u53cd\u9988\u4ee5\u5206\u6790\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5c3d\u7ba1\u91c7\u7528\u5148\u8fdb\u6a21\u578b\uff0c\u673a\u5668\u4eba\u68c0\u6d4b\u6c9f\u901a\u9519\u8bef\u7684\u6027\u80fd\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u6982\u7387\uff1b\u4eba\u7c7b\u8bc4\u5206\u8005\u4e5f\u4ec5\u80fd\u8bc6\u522b\u7ea6\u4e00\u534a\u7684\u9519\u8bef\uff0c\u8868\u660e\u7528\u6237\u53cd\u9988\u4e0d\u8db3\u662f\u4e3b\u8981\u9650\u5236\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u673a\u5668\u4eba\u68c0\u6d4b\u5bf9\u8bdd\u9519\u8bef\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5373\u7528\u6237\u53cd\u9988\u4e0d\u8db3\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u4eba\u673a\u5bf9\u8bdd\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "paper_title_zh": "\u4e3a\u4f55\u673a\u5668\u4eba\u96be\u4ee5\u68c0\u6d4b\u81ea\u8eab\u9519\u8bef\uff1a\u4eba\u673a\u5bf9\u8bdd\u4e2d\u9519\u8bef\u68c0\u6d4b\u7684\u5c40\u9650\u6027", "abstract_zh": "\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u68c0\u6d4b\u6c9f\u901a\u9519\u8bef\u5bf9\u7ef4\u6301\u7528\u6237\u53c2\u4e0e\u548c\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u4eba\u7c7b\u80fd\u8f7b\u677e\u901a\u8fc7\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u7ebf\u7d22\u8bc6\u522b\u9519\u8bef\uff0c\u4f46\u673a\u5668\u4eba\u5728\u89e3\u8bfb\u975e\u8bed\u8a00\u53cd\u9988\u65b9\u9762\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u5373\u4f7f\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u60c5\u611f\u8868\u8fbe\u8bc6\u522b\u65b9\u9762\u6709\u6240\u8fdb\u5c55\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u6c9f\u901a\u9519\u8bef\u7684\u6548\u679c\u3002\u901a\u8fc7\u4e00\u4e2a\u5305\u542b240\u6bb5\u4eba\u673a\u5bf9\u8bdd\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5f15\u5165\u56db\u79cd\u5bf9\u8bdd\u5931\u8d25\u7c7b\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\u3002\u6bcf\u6b21\u5bf9\u8bdd\u540e\uff0c\u7528\u6237\u63d0\u4f9b\u662f\u5426\u611f\u77e5\u5230\u9519\u8bef\u7684\u53cd\u9988\uff0c\u4ece\u800c\u5206\u6790\u6a21\u578b\u68c0\u6d4b\u673a\u5668\u4eba\u9519\u8bef\u7684\u51c6\u786e\u6027\u3002\u5c3d\u7ba1\u4f7f\u7528\u4e86\u5148\u8fdb\u6a21\u578b\uff0c\u5176\u68c0\u6d4b\u6c9f\u901a\u9519\u8bef\u7684\u6027\u80fd\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u6982\u7387\uff1b\u800c\u5728\u60c5\u611f\u8868\u8fbe\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u80fd\u6210\u529f\u8bc6\u522b\u56f0\u60d1\u72b6\u6001\u3002\u4e3a\u63a2\u7a76\u539f\u56e0\uff0c\u4eba\u7c7b\u8bc4\u5206\u8005\u4e5f\u88ab\u8981\u6c42\u5b8c\u6210\u76f8\u540c\u4efb\u52a1\uff0c\u7ed3\u679c\u4ec5\u80fd\u8bc6\u522b\u7ea6\u4e00\u534a\u7684\u9519\u8bef\uff0c\u4e0e\u6a21\u578b\u8868\u73b0\u76f8\u4f3c\u3002\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u673a\u5668\u4eba\u68c0\u6d4b\u5bf9\u8bdd\u9519\u8bef\u7684\u6839\u672c\u5c40\u9650\u6027\uff1a\u5373\u4f7f\u7528\u6237\u611f\u77e5\u5230\u9519\u8bef\uff0c\u4e5f\u672a\u5fc5\u53cd\u9988\u7ed9\u673a\u5668\u4eba\u3002\u8fd9\u4e00\u53d1\u73b0\u6709\u52a9\u4e8e\u8c03\u6574\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u6027\u80fd\u7684\u9884\u671f\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u597d\u7684\u4eba\u673a\u5bf9\u8bdd\u63d0\u4f9b\u6307\u5bfc\uff0c\u4f8b\u5982\u5728\u5fc5\u8981\u65f6\u4e3b\u52a8\u83b7\u53d6\u7528\u6237\u53cd\u9988\u3002"}}
{"id": "2506.20586", "pdf": "https://arxiv.org/pdf/2506.20586", "abs": "https://arxiv.org/abs/2506.20586", "authors": ["Yitong Quan", "Benjamin Kiefer", "Martin Messmer", "Andreas Zell"], "title": "Learning-Based Distance Estimation for 360\u00b0 Single-Sensor Setups", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to ECMR 2025", "summary": "Accurate distance estimation is a fundamental challenge in robotic\nperception, particularly in omnidirectional imaging, where traditional\ngeometric methods struggle with lens distortions and environmental variability.\nIn this work, we propose a neural network-based approach for monocular distance\nestimation using a single 360{\\deg} fisheye lens camera. Unlike classical\ntrigonometric techniques that rely on precise lens calibration, our method\ndirectly learns and infers the distance of objects from raw omnidirectional\ninputs, offering greater robustness and adaptability across diverse conditions.\nWe evaluate our approach on three 360{\\deg} datasets (LOAF, ULM360, and a newly\ncaptured dataset Boat360), each representing distinct environmental and sensor\nsetups. Our experimental results demonstrate that the proposed learning-based\nmodel outperforms traditional geometry-based methods and other learning\nbaselines in both accuracy and robustness. These findings highlight the\npotential of deep learning for real-time omnidirectional distance estimation,\nmaking our approach particularly well-suited for low-cost applications in\nrobotics, autonomous navigation, and surveillance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u5355\u76ee360\u00b0\u9c7c\u773c\u955c\u5934\u76f8\u673a\u8fdb\u884c\u8ddd\u79bb\u4f30\u8ba1\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u5728\u900f\u955c\u7578\u53d8\u548c\u73af\u5883\u53d8\u5316\u4e2d\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u548c\u57fa\u7ebf\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\uff0c\u51c6\u786e\u7684\u5355\u76ee\u8ddd\u79bb\u4f30\u8ba1\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5168\u5411\u6210\u50cf\u4e2d\uff0c\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u56e0\u900f\u955c\u7578\u53d8\u548c\u73af\u5883\u591a\u6837\u6027\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e00\u79cd\u66f4\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u5168\u5411\u8f93\u5165\u4e2d\u5b66\u4e60\u5e76\u63a8\u65ad\u7269\u4f53\u8ddd\u79bb\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u7cbe\u786e\u900f\u955c\u6821\u51c6\u7684\u4f9d\u8d56\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u4e0d\u540c\u7684360\u00b0\u6570\u636e\u96c6\uff08LOAF\u3001ULM360\u548cBoat360\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u548c\u5176\u4ed6\u5b66\u4e60\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5168\u5411\u8ddd\u79bb\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u4e3a\u4f4e\u6210\u672c\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u76d1\u63a7\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5168\u5411\u8ddd\u79bb\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u6b64\u9886\u57df\u7684\u4f18\u52bf\u3002", "paper_title_zh": "\u57fa\u4e8e\u5b66\u4e60\u7684360\u00b0\u5355\u4f20\u611f\u5668\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5", "abstract_zh": "\u51c6\u786e\u7684\u5355\u76ee\u8ddd\u79bb\u4f30\u8ba1\u662f\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5168\u5411\u6210\u50cf\u4e2d\uff0c\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u56e0\u900f\u955c\u7578\u53d8\u548c\u73af\u5883\u591a\u6837\u6027\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u5355\u76ee360\u00b0\u9c7c\u773c\u955c\u5934\u76f8\u673a\u8fdb\u884c\u8ddd\u79bb\u4f30\u8ba1\u3002\u4e0e\u4f9d\u8d56\u7cbe\u786e\u900f\u955c\u6821\u51c6\u7684\u4f20\u7edf\u4e09\u89d2\u6d4b\u91cf\u6280\u672f\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f4\u63a5\u4ece\u539f\u59cb\u5168\u5411\u8f93\u5165\u4e2d\u5b66\u4e60\u5e76\u63a8\u65ad\u7269\u4f53\u8ddd\u79bb\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002\u6211\u4eec\u5728\u4e09\u4e2a360\u00b0\u6570\u636e\u96c6\uff08LOAF\u3001ULM360\u548c\u65b0\u6355\u83b7\u7684Boat360\uff09\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u4ee3\u8868\u4e0d\u540c\u7684\u73af\u5883\u548c\u4f20\u611f\u5668\u914d\u7f6e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u548c\u5176\u4ed6\u5b66\u4e60\u57fa\u7ebf\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5168\u5411\u8ddd\u79bb\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u7279\u522b\u9002\u5408\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u76d1\u63a7\u7b49\u4f4e\u6210\u672c\u5e94\u7528\u3002"}}
{"id": "2506.19890", "pdf": "https://arxiv.org/pdf/2506.19890", "abs": "https://arxiv.org/abs/2506.19890", "authors": ["Ziru Zhang", "Jiadong Yu", "Danny H. K. Tsang"], "title": "Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The optimization of quality of experience (QoE) in multi-user virtual reality\n(VR) interactions demands a delicate balance between ultra-low latency,\nhigh-fidelity motion synchronization, and equitable resource allocation. While\nadaptive keyframe extraction mitigates transmission overhead, existing\napproaches often overlook the causal relationships among allocated bandwidth,\nCPU frequency, and user perception, limiting QoE gains. This paper proposes an\nintelligent framework to maximize QoE by integrating adaptive keyframe\nextraction with causal-aware reinforcement learning (RL). First, a novel QoE\nmetric is formulated using the Weber-Fechner Law, combining perceptual\nsensitivity, attention-driven priorities, and motion reconstruction accuracy.\nThe QoE optimization problem is then modeled as a mixed integer programming\n(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational\nresources under horizon-fairness constraints. We propose Partial State Causal\nDeep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep\nDeterministic Policy Gradient (DDPG) method with causal influence detection. By\nleveraging causal information regarding how QoE is influenced and determined by\nvarious actions, we explore actions guided by weights calculated from causal\ninference (CI), which in turn improves training efficiency. Experiments\nconducted with the CMU Motion Capture Database demonstrate that our framework\nsignificantly reduces interactive latency, enhances QoE, and maintains\nfairness, achieving superior performance compared to benchmark methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94\u5173\u952e\u5e27\u63d0\u53d6\u548c\u56e0\u679c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u6846\u67b6\uff0c\u4ee5\u4f18\u5316\u591a\u7528\u6237\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4ea4\u4e92\u4e2d\u7684\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u7684QoE\u5ea6\u91cf\u548c\u56e0\u679c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4ea4\u4e92\u5ef6\u8fdf\u5e76\u63d0\u5347\u4e86\u516c\u5e73\u6027\u3002", "motivation": "\u591a\u7528\u6237VR\u4ea4\u4e92\u4e2d\uff0c\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u7684\u4f18\u5316\u9700\u8981\u5728\u8d85\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u4fdd\u771f\u8fd0\u52a8\u540c\u6b65\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u5e26\u5bbd\u3001CPU\u9891\u7387\u4e0e\u7528\u6237\u611f\u77e5\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u9650\u5236\u4e86QoE\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWeber-Fechner\u5b9a\u5f8b\u7684\u65b0QoE\u5ea6\u91cf\uff0c\u7ed3\u5408\u611f\u77e5\u7075\u654f\u5ea6\u3001\u6ce8\u610f\u529b\u9a71\u52a8\u4f18\u5148\u7ea7\u548c\u8fd0\u52a8\u91cd\u5efa\u7cbe\u5ea6\u3002\u5c06QoE\u4f18\u5316\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u90e8\u5206\u72b6\u6001\u56e0\u679c\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08PS-CDDPG\uff09\u65b9\u6cd5\uff0c\u6574\u5408\u56e0\u679c\u63a8\u7406\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u57fa\u4e8eCMU Motion Capture Database\u8fdb\u884c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u4ea4\u4e92\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86QoE\uff0c\u5e76\u4fdd\u6301\u4e86\u516c\u5e73\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u56e0\u679c\u611f\u77e5\u667a\u80fdQoE\u4f18\u5316\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u81ea\u9002\u5e94\u5173\u952e\u5e27\u63d0\u53d6\u548c\u56e0\u679c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86VR\u4ea4\u4e92\u4e2d\u7684QoE\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u56e0\u679c\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5173\u952e\u5e27\u63d0\u53d6\u667a\u80fdQoE\u4f18\u5316VR\u4ea4\u4e92\u65b9\u6cd5", "abstract_zh": "\u591a\u7528\u6237\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4ea4\u4e92\u4e2d\u7684\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u4f18\u5316\u9700\u8981\u5728\u8d85\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u4fdd\u771f\u8fd0\u52a8\u540c\u6b65\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u4e4b\u95f4\u53d6\u5f97\u5fae\u5999\u5e73\u8861\u3002\u5c3d\u7ba1\u81ea\u9002\u5e94\u5173\u952e\u5e27\u63d0\u53d6\u51cf\u5c11\u4e86\u4f20\u8f93\u5f00\u9500\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u5e26\u5bbd\u5206\u914d\u3001CPU\u9891\u7387\u4e0e\u7528\u6237\u611f\u77e5\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u9650\u5236\u4e86QoE\u7684\u63d0\u5347\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u9002\u5e94\u5173\u952e\u5e27\u63d0\u53d6\u4e0e\u56e0\u679c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u76f8\u7ed3\u5408\u6765\u6700\u5927\u5316QoE\u3002\u9996\u5148\uff0c\u57fa\u4e8eWeber-Fechner\u5b9a\u5f8b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684QoE\u5ea6\u91cf\uff0c\u7ed3\u5408\u4e86\u611f\u77e5\u7075\u654f\u5ea6\u3001\u6ce8\u610f\u529b\u9a71\u52a8\u4f18\u5148\u7ea7\u548c\u8fd0\u52a8\u91cd\u5efa\u7cbe\u5ea6\u3002\u968f\u540e\uff0c\u5c06QoE\u4f18\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u4efb\u52a1\uff0c\u5728\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\u8054\u5408\u4f18\u5316\u5173\u952e\u5e27\u6bd4\u4f8b\u3001\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u90e8\u5206\u72b6\u6001\u56e0\u679c\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08PS-CDDPG\uff09\uff0c\u8be5\u65b9\u6cd5\u5c06\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u4e0e\u56e0\u679c\u5f71\u54cd\u68c0\u6d4b\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u5229\u7528\u5173\u4e8eQoE\u5982\u4f55\u53d7\u5404\u79cd\u52a8\u4f5c\u5f71\u54cd\u548c\u51b3\u5b9a\u7684\u56e0\u679c\u4fe1\u606f\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u7531\u56e0\u679c\u63a8\u7406\uff08CI\uff09\u8ba1\u7b97\u6743\u91cd\u5f15\u5bfc\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u57fa\u4e8eCMU Motion Capture Database\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u4ea4\u4e92\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86QoE\uff0c\u5e76\u4fdd\u6301\u4e86\u516c\u5e73\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2506.20303", "pdf": "https://arxiv.org/pdf/2506.20303", "abs": "https://arxiv.org/abs/2506.20303", "authors": ["Lee Qi Zun", "Oscar Wong Jin Hao", "Nor Anita Binti Che Omar", "Zalifa Zakiah Binti Asnir", "Mohamad Sabri bin Sinal Zainal", "Goh Man Fye"], "title": "FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment", "categories": ["eess.IV", "cs.CL", "cs.CV"], "comment": null, "summary": "Automated fundus image quality assessment (FIQA) remains a challenge due to\nvariations in image acquisition and subjective expert evaluations. We introduce\nFundaQ-8, a novel expert-validated framework for systematically assessing\nfundus image quality using eight critical parameters, including field coverage,\nanatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a\nstructured scoring reference, we develop a ResNet18-based regression model to\npredict continuous quality scores in the 0 to 1 range. The model is trained on\n1800 fundus images from real-world clinical sources and Kaggle datasets, using\ntransfer learning, mean squared error optimization, and standardized\npreprocessing. Validation against the EyeQ dataset and statistical analyses\nconfirm the framework's reliability and clinical interpretability.\nIncorporating FundaQ-8 into deep learning models for diabetic retinopathy\ngrading also improves diagnostic robustness, highlighting the value of\nquality-aware training in real-world screening applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFundaQ-8\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u773c\u5e95\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8fde\u7eed\u8d28\u91cf\u5206\u6570\uff0c\u63d0\u5347\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7ea7\u7684\u8bca\u65ad\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u773c\u5e95\u56fe\u50cf\u91c7\u96c6\u7684\u591a\u6837\u6027\u548c\u4e13\u5bb6\u8bc4\u4f30\u7684\u4e3b\u89c2\u6027\uff0c\u81ea\u52a8\u773c\u5e95\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08FIQA\uff09\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7FundaQ-8\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFundaQ-8\u6846\u67b6\uff0c\u5305\u542b\u516b\u4e2a\u5173\u952e\u53c2\u6570\uff08\u5982\u89c6\u91ce\u8986\u76d6\u3001\u89e3\u5256\u53ef\u89c1\u6027\u3001\u7167\u660e\u548c\u56fe\u50cf\u4f2a\u5f71\uff09\uff0c\u5e76\u57fa\u4e8eResNet18\u56de\u5f52\u6a21\u578b\u9884\u6d4b0\u52301\u8303\u56f4\u5185\u7684\u8fde\u7eed\u8d28\u91cf\u5206\u6570\u3002\u6a21\u578b\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u5747\u65b9\u8bef\u5dee\u4f18\u5316\u548c\u6807\u51c6\u5316\u9884\u5904\u7406\uff0c\u57281800\u5f20\u4e34\u5e8a\u548cKaggle\u6570\u636e\u96c6\u56fe\u50cf\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728EyeQ\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u548c\u7edf\u8ba1\u5206\u6790\u8bc1\u5b9e\u4e86FundaQ-8\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002\u5c06\u5176\u5e94\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7ea7\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u8bca\u65ad\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FundaQ-8\u6846\u67b6\u4e3a\u773c\u5e95\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u5176\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5e94\u7528\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u7b5b\u67e5\u4e2d\u7684\u8bca\u65ad\u6027\u80fd\u3002", "paper_title_zh": "FundaQ-8\uff1a\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u542f\u53d1\u7684\u81ea\u52a8\u773c\u5e95\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u8bc4\u5206\u6846\u67b6", "abstract_zh": "\u81ea\u52a8\u773c\u5e95\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08FIQA\uff09\u56e0\u56fe\u50cf\u91c7\u96c6\u7684\u591a\u6837\u6027\u548c\u4e13\u5bb6\u8bc4\u4f30\u7684\u4e3b\u89c2\u6027\u800c\u4ecd\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51faFundaQ-8\uff0c\u4e00\u79cd\u7ecf\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u5173\u952e\u53c2\u6570\uff08\u5982\u89c6\u91ce\u8986\u76d6\u3001\u89e3\u5256\u53ef\u89c1\u6027\u3001\u7167\u660e\u548c\u56fe\u50cf\u4f2a\u5f71\uff09\u7cfb\u7edf\u8bc4\u4f30\u773c\u5e95\u56fe\u50cf\u8d28\u91cf\u3002\u4ee5FundaQ-8\u4e3a\u7ed3\u6784\u5316\u8bc4\u5206\u53c2\u8003\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u57fa\u4e8eResNet18\u7684\u56de\u5f52\u6a21\u578b\uff0c\u9884\u6d4b0\u52301\u8303\u56f4\u5185\u7684\u8fde\u7eed\u8d28\u91cf\u5206\u6570\u3002\u6a21\u578b\u57281800\u5f20\u6765\u81ea\u771f\u5b9e\u4e34\u5e8a\u548cKaggle\u6570\u636e\u96c6\u7684\u773c\u5e95\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u3001\u5747\u65b9\u8bef\u5dee\u4f18\u5316\u548c\u6807\u51c6\u5316\u9884\u5904\u7406\u3002\u5728EyeQ\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u548c\u7edf\u8ba1\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002\u5c06FundaQ-8\u6574\u5408\u5230\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7ea7\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u8fd8\u63d0\u5347\u4e86\u8bca\u65ad\u7684\u9c81\u68d2\u6027\uff0c\u51f8\u663e\u4e86\u8d28\u91cf\u611f\u77e5\u8bad\u7ec3\u5728\u5b9e\u9645\u7b5b\u67e5\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.20588", "pdf": "https://arxiv.org/pdf/2506.20588", "abs": "https://arxiv.org/abs/2506.20588", "authors": ["Pritam Mishra", "Coloma Ballester", "Dimosthenis Karatzas"], "title": "TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness", "categories": ["cs.CV"], "comment": null, "summary": "The increasing ubiquity of video content and the corresponding demand for\nefficient access to meaningful information have elevated video summarization\nand video highlights as a vital research area. However, many state-of-the-art\nmethods depend heavily either on supervised annotations or on attention-based\nmodels, which are computationally expensive and brittle in the face of\ndistribution shifts that hinder cross-domain applicability across datasets. We\nintroduce a pioneering self-supervised video summarization model that captures\nboth spatial and temporal dependencies without the overhead of attention, RNNs,\nor transformers. Our framework integrates a novel set of Markov process-driven\nloss metrics and a two-stage self supervised learning paradigm that ensures\nboth performance and efficiency. Our approach achieves state-of-the-art\nperformance on the SUMME and TVSUM datasets, outperforming all existing\nunsupervised methods. It also rivals the best supervised models, demonstrating\nthe potential for efficient, annotation-free architectures. This paves the way\nfor more generalizable video summarization techniques and challenges the\nprevailing reliance on complex architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u89c6\u9891\u6458\u8981\u6846\u67b6TRIM\uff0c\u901a\u8fc7\u6700\u5927\u5316\u65f6\u95f4\u76f8\u5bf9\u4fe1\u606f\u548c\u4ee3\u8868\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u76d1\u7763\u6807\u6ce8\u6216\u590d\u6742\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5728SUMME\u548cTVSUM\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u5185\u5bb9\u7684\u666e\u53ca\uff0c\u9ad8\u6548\u83b7\u53d6\u5173\u952e\u4fe1\u606f\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u6807\u6ce8\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u4e14\u8de8\u57df\u9002\u5e94\u6027\u5dee\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u6807\u6ce8\u7684\u81ea\u76d1\u7763\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u3002", "method": "TRIM\u6846\u67b6\u7ed3\u5408\u4e86\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u9a71\u52a8\u7684\u635f\u5931\u6307\u6807\u548c\u4e24\u9636\u6bb5\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u65e0\u9700\u6ce8\u610f\u529b\u673a\u5236\u3001RNN\u6216Transformer\uff0c\u540c\u65f6\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002", "result": "\u5728SUMME\u548cTVSUM\u6570\u636e\u96c6\u4e0a\uff0cTRIM\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u4f73\u76d1\u7763\u6a21\u578b\u5ab2\u7f8e\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u6807\u6ce8\u7684\u6f5c\u529b\u3002", "conclusion": "TRIM\u4e3a\u901a\u7528\u89c6\u9891\u6458\u8981\u6280\u672f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u6311\u6218\u4e86\u5bf9\u590d\u6742\u67b6\u6784\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548\u81ea\u76d1\u7763\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "paper_title_zh": "TRIM\uff1a\u4e00\u79cd\u6700\u5927\u5316\u65f6\u95f4\u76f8\u5bf9\u4fe1\u606f\u4e0e\u4ee3\u8868\u6027\u7684\u81ea\u76d1\u7763\u89c6\u9891\u6458\u8981\u6846\u67b6", "abstract_zh": "\u89c6\u9891\u5185\u5bb9\u7684\u65e5\u76ca\u666e\u53ca\u53ca\u5176\u5bf9\u9ad8\u6548\u83b7\u53d6\u5173\u952e\u4fe1\u606f\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u89c6\u9891\u6458\u8981\u548c\u89c6\u9891\u9ad8\u5149\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u9886\u57df\u3002\u7136\u800c\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u76d1\u7763\u6807\u6ce8\u6216\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5728\u5206\u5e03\u504f\u79fb\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u963b\u788d\u4e86\u8de8\u6570\u636e\u96c6\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u521b\u6027\u7684\u81ea\u76d1\u7763\u89c6\u9891\u6458\u8981\u6a21\u578b\uff0c\u65e0\u9700\u6ce8\u610f\u529b\u3001RNN\u6216Transformer\u7684\u5f00\u9500\u5373\u53ef\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002\u6211\u4eec\u7684\u6846\u67b6\u6574\u5408\u4e86\u4e00\u5957\u65b0\u9896\u7684\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u9a71\u52a8\u635f\u5931\u6307\u6807\u548c\u4e24\u9636\u6bb5\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u786e\u4fdd\u6027\u80fd\u548c\u6548\u7387\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728SUMME\u548cTVSUM\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u4f73\u76d1\u7763\u6a21\u578b\u5ab2\u7f8e\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u3001\u65e0\u9700\u6807\u6ce8\u67b6\u6784\u7684\u6f5c\u529b\u3002\u8fd9\u4e3a\u66f4\u901a\u7528\u7684\u89c6\u9891\u6458\u8981\u6280\u672f\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u6311\u6218\u4e86\u5bf9\u590d\u6742\u67b6\u6784\u7684\u666e\u904d\u4f9d\u8d56\u3002"}}
{"id": "2506.19891", "pdf": "https://arxiv.org/pdf/2506.19891", "abs": "https://arxiv.org/abs/2506.19891", "authors": ["Qinghui Gong", "Xue Yang", "Xiaohu Tang"], "title": "Orthogonal Soft Pruning for Efficient Class Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages,3 figures", "summary": "Machine unlearning aims to selectively remove class-specific knowledge from\npretrained neural networks to satisfy privacy regulations such as the GDPR.\nExisting methods typically face a trade-off between unlearning speed and\npreservation of predictive accuracy, often incurring either high computational\noverhead or significant performance degradation on retained classes. In this\npaper, we propose a novel class-aware soft pruning framework leveraging\northogonal convolutional kernel regularization to achieve rapid and precise\nforgetting with millisecond-level response times. By enforcing orthogonality\nconstraints during training, our method decorrelates convolutional filters and\ndisentangles feature representations, while efficiently identifying\nclass-specific channels through activation difference analysis. Extensive\nevaluations across multiple architectures and datasets demonstrate stable\npruning with near-instant execution, complete forgetting of targeted classes,\nand minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,\nand TinyImageNet confirm that our approach substantially reduces membership\ninference attack risks and accelerates unlearning by orders of magnitude\ncompared to state-of-the-art baselines. This framework provides an efficient,\npractical solution for real-time machine unlearning in Machine Learning as a\nService (MLaaS) scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u5377\u79ef\u6838\u6b63\u5219\u5316\u7684\u7c7b\u611f\u77e5\u8f6f\u526a\u679d\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5b9e\u73b0\u673a\u5668\u9057\u5fd8\uff0c\u80fd\u591f\u5728\u6beb\u79d2\u7ea7\u54cd\u5e94\u65f6\u95f4\u5185\u5feb\u901f\u4e14\u7cbe\u786e\u5730\u9057\u5fd8\u7279\u5b9a\u7c7b\u522b\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u7c7b\u522b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\u65e8\u5728\u9009\u62e9\u6027\u79fb\u9664\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u7279\u5b9a\u7c7b\u522b\u77e5\u8bc6\u4ee5\u6ee1\u8db3\u9690\u79c1\u6cd5\u89c4\uff08\u5982GDPR\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u9057\u5fd8\u901f\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u8981\u4e48\u6027\u80fd\u4e0b\u964d\u660e\u663e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u611f\u77e5\u8f6f\u526a\u679d\u6846\u67b6\uff0c\u5229\u7528\u6b63\u4ea4\u5377\u79ef\u6838\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e2d\u7684\u6b63\u4ea4\u7ea6\u675f\u89e3\u8026\u5377\u79ef\u6ee4\u6ce2\u5668\u548c\u7279\u5f81\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6fc0\u6d3b\u5dee\u5f02\u5206\u6790\u9ad8\u6548\u8bc6\u522b\u7c7b\u522b\u7279\u5b9a\u901a\u9053\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTinyImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6beb\u79d2\u7ea7\u54cd\u5e94\u3001\u76ee\u6807\u7c7b\u522b\u7684\u5b8c\u5168\u9057\u5fd8\uff0c\u4e14\u5bf9\u4fdd\u7559\u6570\u636e\u7684\u51c6\u786e\u6027\u635f\u5931\u6781\u5c0f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u5927\u5e45\u52a0\u901f\u4e86\u9057\u5fd8\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\uff08MLaaS\uff09\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u6b63\u4ea4\u8f6f\u526a\u679d\u7684\u9ad8\u6548\u7c7b\u522b\u9057\u5fd8\u65b9\u6cd5", "abstract_zh": "\u673a\u5668\u9057\u5fd8\u65e8\u5728\u4ece\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u9009\u62e9\u6027\u79fb\u9664\u7279\u5b9a\u7c7b\u522b\u7684\u77e5\u8bc6\uff0c\u4ee5\u6ee1\u8db3\u5982GDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u7684\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u9057\u5fd8\u901f\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5f80\u5f80\u9700\u8981\u8f83\u9ad8\u7684\u8ba1\u7b97\u5f00\u9500\u6216\u5bfc\u81f4\u4fdd\u7559\u7c7b\u522b\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7c7b\u611f\u77e5\u8f6f\u526a\u679d\u6846\u67b6\uff0c\u5229\u7528\u6b63\u4ea4\u5377\u79ef\u6838\u6b63\u5219\u5316\u5b9e\u73b0\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u9057\u5fd8\uff0c\u54cd\u5e94\u65f6\u95f4\u8fbe\u5230\u6beb\u79d2\u7ea7\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u65bd\u52a0\u6b63\u4ea4\u7ea6\u675f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u89e3\u8026\u4e86\u5377\u79ef\u6ee4\u6ce2\u5668\u5e76\u5206\u79bb\u4e86\u7279\u5f81\u8868\u793a\uff0c\u540c\u65f6\u901a\u8fc7\u6fc0\u6d3b\u5dee\u5f02\u5206\u6790\u9ad8\u6548\u8bc6\u522b\u7c7b\u522b\u7279\u5b9a\u901a\u9053\u3002\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u526a\u679d\u3001\u8fd1\u4e4e\u5373\u65f6\u7684\u6267\u884c\u3001\u76ee\u6807\u7c7b\u522b\u7684\u5b8c\u5168\u9057\u5fd8\uff0c\u4ee5\u53ca\u5bf9\u4fdd\u7559\u6570\u636e\u7684\u51c6\u786e\u6027\u635f\u5931\u6781\u5c0f\u3002\u5728CIFAR-10\u3001CIFAR-100\u548cTinyImageNet\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u5c06\u9057\u5fd8\u901f\u5ea6\u63d0\u9ad8\u4e86\u6570\u4e2a\u6570\u91cf\u7ea7\u3002\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\uff08MLaaS\uff09\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20590", "pdf": "https://arxiv.org/pdf/2506.20590", "abs": "https://arxiv.org/abs/2506.20590", "authors": ["Chaojun Ni", "Jie Li", "Haoyun Li", "Hengyu Liu", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Boyuan Wang", "Chenxin Li", "Guan Huang", "Wenjun Mei"], "title": "WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration", "categories": ["cs.CV"], "comment": null, "summary": "Interactive 3D scene generation from a single image has gained significant\nattention due to its potential to create immersive virtual worlds. However, a\nkey challenge in current 3D generation methods is the limited explorability,\nwhich cannot render high-quality images during larger maneuvers beyond the\noriginal viewpoint, particularly when attempting to move forward into unseen\nareas. To address this challenge, we propose WonderFree, the first model that\nenables users to interactively generate 3D worlds with the freedom to explore\nfrom arbitrary angles and directions. Specifically, we decouple this challenge\ninto two key subproblems: novel view quality, which addresses visual artifacts\nand floating issues in novel views, and cross-view consistency, which ensures\nspatial consistency across different viewpoints. To enhance rendering quality\nin novel views, we introduce WorldRestorer, a data-driven video restoration\nmodel designed to eliminate floaters and artifacts. In addition, a data\ncollection pipeline is presented to automatically gather training data for\nWorldRestorer, ensuring it can handle scenes with varying styles needed for 3D\nscene generation. Furthermore, to improve cross-view consistency, we propose\nConsistView, a multi-view joint restoration mechanism that simultaneously\nrestores multiple perspectives while maintaining spatiotemporal coherence.\nExperimental results demonstrate that WonderFree not only enhances rendering\nquality across diverse viewpoints but also significantly improves global\ncoherence and consistency. These improvements are confirmed by CLIP-based\nmetrics and a user study showing a 77.20% preference for WonderFree over\nWonderWorld enabling a seamless and immersive 3D exploration experience. The\ncode, model, and data will be publicly available.", "AI": {"tldr": "WonderFree\u662f\u4e00\u79cd\u65b0\u578b3D\u573a\u666f\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u5347\u65b0\u89c6\u89d2\u8d28\u91cf\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u6c89\u6d78\u5f0f\u76843D\u573a\u666f\u63a2\u7d22\u3002", "motivation": "\u5f53\u524d3D\u751f\u6210\u65b9\u6cd5\u5728\u63a2\u7d22\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5728\u8d85\u51fa\u539f\u59cb\u89c6\u89d2\u7684\u5927\u8303\u56f4\u79fb\u52a8\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5c24\u5176\u662f\u5728\u8fdb\u5165\u672a\u89c2\u5bdf\u533a\u57df\u65f6\u3002", "method": "WonderFree\u901a\u8fc7WorldRestorer\u6d88\u9664\u65b0\u89c6\u89d2\u4e2d\u7684\u89c6\u89c9\u4f2a\u5f71\u548c\u6f02\u6d6e\u7269\uff0c\u5e76\u901a\u8fc7ConsistView\u673a\u5236\u63d0\u5347\u8de8\u89c6\u89d2\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u81ea\u52a8\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u4ee5\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWonderFree\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u7528\u6237\u7814\u7a76\u4e2d77.20%\u7684\u53c2\u4e0e\u8005\u66f4\u504f\u597dWonderFree\u3002", "conclusion": "WonderFree\u4e3a3D\u573a\u666f\u63a2\u7d22\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002", "paper_title_zh": "WonderFree\uff1a\u63d0\u53473D\u573a\u666f\u63a2\u7d22\u7684\u65b0\u89c6\u89d2\u8d28\u91cf\u4e0e\u8de8\u89c6\u89d2\u4e00\u81f4\u6027", "abstract_zh": "\u57fa\u4e8e\u5355\u5f20\u56fe\u50cf\u7684\u4ea4\u4e92\u5f0f3D\u573a\u666f\u751f\u6210\u56e0\u5176\u521b\u5efa\u6c89\u6d78\u5f0f\u865a\u62df\u4e16\u754c\u7684\u6f5c\u529b\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5f53\u524d3D\u751f\u6210\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u63a2\u7d22\u6027\u6709\u9650\uff0c\u65e0\u6cd5\u5728\u8d85\u51fa\u539f\u59cb\u89c6\u89d2\u7684\u5927\u8303\u56f4\u79fb\u52a8\u4e2d\u6e32\u67d3\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5c24\u5176\u662f\u5728\u5c1d\u8bd5\u8fdb\u5165\u672a\u89c2\u5bdf\u533a\u57df\u65f6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86WonderFree\uff0c\u8fd9\u662f\u9996\u4e2a\u5141\u8bb8\u7528\u6237\u4ece\u4efb\u610f\u89d2\u5ea6\u548c\u65b9\u5411\u81ea\u7531\u63a2\u7d223D\u4e16\u754c\u7684\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5c06\u8fd9\u4e00\u6311\u6218\u5206\u89e3\u4e3a\u4e24\u4e2a\u5173\u952e\u5b50\u95ee\u9898\uff1a\u65b0\u89c6\u89d2\u8d28\u91cf\uff08\u89e3\u51b3\u65b0\u89c6\u89d2\u4e2d\u7684\u89c6\u89c9\u4f2a\u5f71\u548c\u6f02\u6d6e\u95ee\u9898\uff09\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\uff08\u786e\u4fdd\u4e0d\u540c\u89c6\u89d2\u95f4\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff09\u3002\u4e3a\u63d0\u5347\u65b0\u89c6\u89d2\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u6211\u4eec\u5f15\u5165\u4e86WorldRestorer\uff0c\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\uff0c\u7528\u4e8e\u6d88\u9664\u6f02\u6d6e\u7269\u548c\u4f2a\u5f71\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u4e3aWorldRestorer\u7684\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u652f\u6301\uff0c\u786e\u4fdd\u5176\u80fd\u591f\u5904\u74063D\u573a\u666f\u751f\u6210\u4e2d\u6240\u9700\u7684\u591a\u6837\u5316\u573a\u666f\u98ce\u683c\u3002\u4e3a\u8fdb\u4e00\u6b65\u63d0\u5347\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ConsistView\uff0c\u4e00\u79cd\u591a\u89c6\u89d2\u8054\u5408\u4fee\u590d\u673a\u5236\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u540c\u65f6\u540c\u65f6\u4fee\u590d\u591a\u4e2a\u89c6\u89d2\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWonderFree\u4e0d\u4ec5\u63d0\u5347\u4e86\u591a\u6837\u5316\u89c6\u89d2\u4e0b\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u8fd8\u663e\u8457\u6539\u5584\u4e86\u5168\u5c40\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002\u8fd9\u4e9b\u6539\u8fdb\u901a\u8fc7\u57fa\u4e8eCLIP\u7684\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u5f97\u5230\u9a8c\u8bc1\uff0c\u7528\u6237\u7814\u7a76\u4e2d77.20%\u7684\u53c2\u4e0e\u8005\u66f4\u504f\u597dWonderFree\u800c\u975eWonderWorld\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u65e0\u7f1d\u4e14\u6c89\u6d78\u5f0f\u76843D\u63a2\u7d22\u4f53\u9a8c\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2506.19892", "pdf": "https://arxiv.org/pdf/2506.19892", "abs": "https://arxiv.org/abs/2506.19892", "authors": ["Isaac Marroqui Penalva", "Enrique Tom\u00e1s Mart\u00ednez Beltr\u00e1n", "Manuel Gil P\u00e9rez", "Alberto Huertas Celdr\u00e1n"], "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "comment": null, "summary": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train\nmodels without a central server, introducing new vulnerabilities since each\nnode independently selects peers for model aggregation. Malicious nodes may\nexploit this autonomy by sending corrupted models (model poisoning), delaying\nmodel submissions (delay attack), or flooding the network with excessive\nmessages, negatively affecting system performance. Existing solutions often\ndepend on rigid configurations or additional infrastructures such as\nblockchain, leading to computational overhead, scalability issues, or limited\nadaptability. To overcome these limitations, this paper proposes RepuNet, a\ndecentralized reputation system that categorizes threats in DFL and dynamically\nevaluates node behavior using metrics like model similarity, parameter changes,\nmessage latency, and communication volume. Nodes' influence in model\naggregation is adjusted based on their reputation scores. RepuNet was\nintegrated into the Nebula DFL platform and experimentally evaluated with MNIST\nand CIFAR-10 datasets under non-IID distributions, using federations of up to\n25 nodes in both fully connected and random topologies. Different attack\nintensities, frequencies, and activation intervals were tested. Results\ndemonstrated that RepuNet effectively detects and mitigates malicious behavior,\nachieving F1 scores above 95% for MNIST scenarios and approximately 76% for\nCIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,\nand practical potential for mitigating threats in decentralized federated\nlearning environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRepuNet\uff0c\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u58f0\u8a89\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u4e2d\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u884c\u4e3a\u5e76\u8c03\u6574\u5176\u5f71\u54cd\u529b\uff0c\u4ee5\u62b5\u5fa1\u6076\u610f\u653b\u51fb\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRepuNet\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u5206\u522b\u8d85\u8fc795%\u548c\u7ea676%\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u4e2d\uff0c\u8282\u70b9\u81ea\u4e3b\u9009\u62e9\u805a\u5408\u4f19\u4f34\uff0c\u6613\u53d7\u6076\u610f\u8282\u70b9\u653b\u51fb\uff08\u5982\u6a21\u578b\u6295\u6bd2\u3001\u5ef6\u8fdf\u653b\u51fb\u6216\u6d88\u606f\u6d2a\u6cdb\uff09\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u56fa\u5b9a\u914d\u7f6e\u6216\u989d\u5916\u57fa\u7840\u8bbe\u65bd\uff08\u5982\u533a\u5757\u94fe\uff09\uff0c\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u53ef\u6269\u5c55\u6027\u6216\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRepuNet\uff0c\u901a\u8fc7\u6a21\u578b\u76f8\u4f3c\u6027\u3001\u53c2\u6570\u53d8\u5316\u3001\u6d88\u606f\u5ef6\u8fdf\u548c\u901a\u4fe1\u91cf\u7b49\u6307\u6807\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u884c\u4e3a\uff0c\u5e76\u57fa\u4e8e\u58f0\u8a89\u5206\u6570\u8c03\u6574\u5176\u5728\u6a21\u578b\u805a\u5408\u4e2d\u7684\u5f71\u54cd\u529b\u3002\u5b9e\u9a8c\u5728Nebula DFL\u5e73\u53f0\u4e0a\u8fdb\u884c\uff0c\u4f7f\u7528MNIST\u548cCIFAR-10\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u653b\u51fb\u5f3a\u5ea6\u548c\u62d3\u6251\u7ed3\u6784\u3002", "result": "RepuNet\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\u6076\u610f\u884c\u4e3a\uff0c\u5728MNIST\u573a\u666f\u4e2dF1\u5206\u6570\u8d85\u8fc795%\uff0cCIFAR-10\u573a\u666f\u4e2d\u7ea6\u4e3a76%\u3002\u7ed3\u679c\u8868\u660e\u5176\u9002\u5e94\u6027\u5f3a\u3001\u9c81\u68d2\u6027\u9ad8\uff0c\u9002\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u73af\u5883\u3002", "conclusion": "RepuNet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u58f0\u8a89\u7cfb\u7edf\uff0c\u80fd\u591f\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u884c\u4e3a\u5e76\u62b5\u5fa1\u6076\u610f\u653b\u51fb\uff0c\u4e3aDFL\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "RepuNet\uff1a\u4e00\u79cd\u7528\u4e8e\u7f13\u89e3DFL\u4e2d\u6076\u610f\u5ba2\u6237\u7aef\u7684\u58f0\u8a89\u7cfb\u7edf", "abstract_zh": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u5141\u8bb8\u8282\u70b9\u5728\u6ca1\u6709\u4e2d\u592e\u670d\u52a1\u5668\u7684\u60c5\u51b5\u4e0b\u534f\u4f5c\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e2a\u8282\u70b9\u72ec\u7acb\u9009\u62e9\u805a\u5408\u4f19\u4f34\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u6f0f\u6d1e\u3002\u6076\u610f\u8282\u70b9\u53ef\u80fd\u5229\u7528\u8fd9\u79cd\u81ea\u4e3b\u6027\u53d1\u9001\u635f\u574f\u7684\u6a21\u578b\uff08\u6a21\u578b\u6295\u6bd2\uff09\u3001\u5ef6\u8fdf\u63d0\u4ea4\u6a21\u578b\uff08\u5ef6\u8fdf\u653b\u51fb\uff09\u6216\u901a\u8fc7\u8fc7\u91cf\u6d88\u606f\u6df9\u6ca1\u7f51\u7edc\uff0c\u4ece\u800c\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u914d\u7f6e\u6216\u989d\u5916\u57fa\u7840\u8bbe\u65bd\uff08\u5982\u533a\u5757\u94fe\uff09\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u3001\u53ef\u6269\u5c55\u6027\u95ee\u9898\u6216\u9002\u5e94\u6027\u4e0d\u8db3\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51faRepuNet\uff0c\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u58f0\u8a89\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u578b\u76f8\u4f3c\u6027\u3001\u53c2\u6570\u53d8\u5316\u3001\u6d88\u606f\u5ef6\u8fdf\u548c\u901a\u4fe1\u91cf\u7b49\u6307\u6807\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u884c\u4e3a\uff0c\u5e76\u57fa\u4e8e\u58f0\u8a89\u5206\u6570\u8c03\u6574\u5176\u5728\u6a21\u578b\u805a\u5408\u4e2d\u7684\u5f71\u54cd\u529b\u3002RepuNet\u88ab\u96c6\u6210\u5230Nebula DFL\u5e73\u53f0\u4e2d\uff0c\u5e76\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u653b\u51fb\u5f3a\u5ea6\u3001\u9891\u7387\u548c\u6fc0\u6d3b\u95f4\u9694\u3002\u7ed3\u679c\u8868\u660e\uff0cRepuNet\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\u6076\u610f\u884c\u4e3a\uff0c\u5728MNIST\u573a\u666f\u4e2dF1\u5206\u6570\u8d85\u8fc795%\uff0cCIFAR-10\u573a\u666f\u4e2d\u7ea6\u4e3a76%\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86RepuNet\u5728\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20481", "pdf": "https://arxiv.org/pdf/2506.20481", "abs": "https://arxiv.org/abs/2506.20481", "authors": ["Matthieu Meeus", "Igor Shilov", "Georgios Kaissis", "Yves-Alexandre de Montjoye"], "title": "Counterfactual Influence as a Distributional Quantity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "Workshop on The Impact of Memorization on Trustworthy Foundation\n  Models (MemFM) @ ICML 2025", "summary": "Machine learning models are known to memorize samples from their training\ndata, raising concerns around privacy and generalization. Counterfactual\nself-influence is a popular metric to study memorization, quantifying how the\nmodel's prediction for a sample changes depending on the sample's inclusion in\nthe training dataset. However, recent work has shown memorization to be\naffected by factors beyond self-influence, with other training samples, in\nparticular (near-)duplicates, having a large impact. We here study memorization\ntreating counterfactual influence as a distributional quantity, taking into\naccount how all training samples influence how a sample is memorized. For a\nsmall language model, we compute the full influence distribution of training\nsamples on each other and analyze its properties. We find that solely looking\nat self-influence can severely underestimate tangible risks associated with\nmemorization: the presence of (near-)duplicates seriously reduces\nself-influence, while we find these samples to be (near-)extractable. We\nobserve similar patterns for image classification, where simply looking at the\ninfluence distributions reveals the presence of near-duplicates in CIFAR-10.\nOur findings highlight that memorization stems from complex interactions across\ntraining data and is better captured by the full influence distribution than by\nself-influence alone.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u7684\u73b0\u8c61\u4e0d\u4ec5\u53d7\u81ea\u8eab\u6837\u672c\u5f71\u54cd\uff0c\u8fd8\u53d7\u5176\u4ed6\u6837\u672c\uff08\u5c24\u5176\u662f\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\uff09\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u5f71\u54cd\u89c6\u4e3a\u5206\u5e03\u6027\u91cf\uff0c\u63ed\u793a\u4e86\u4ec5\u5173\u6ce8\u81ea\u8eab\u5f71\u54cd\u4f1a\u4f4e\u4f30\u8bb0\u5fc6\u98ce\u9669\uff0c\u800c\u5b8c\u6574\u5f71\u54cd\u5206\u5e03\u80fd\u66f4\u5168\u9762\u6355\u6349\u8bb0\u5fc6\u73b0\u8c61\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u7684\u73b0\u8c61\u5f15\u53d1\u9690\u79c1\u548c\u6cdb\u5316\u62c5\u5fe7\u3002\u4f20\u7edf\u53cd\u4e8b\u5b9e\u81ea\u5f71\u54cd\u6307\u6807\u4ec5\u5173\u6ce8\u5355\u4e2a\u6837\u672c\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u6837\u672c\uff08\u5982\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\uff09\u7684\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u5f71\u54cd\u89c6\u4e3a\u5206\u5e03\u6027\u91cf\uff0c\u5168\u9762\u5206\u6790\u8bb0\u5fc6\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u5c06\u53cd\u4e8b\u5b9e\u5f71\u54cd\u89c6\u4e3a\u5206\u5e03\u6027\u91cf\uff0c\u8ba1\u7b97\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u5b8c\u6574\u5f71\u54cd\u5206\u5e03\u3002\u901a\u8fc7\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff08\u5982CIFAR-10\uff09\uff0c\u5206\u6790\u5f71\u54cd\u5206\u5e03\u7684\u7279\u6027\uff0c\u63ed\u793a\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u5173\u6ce8\u81ea\u8eab\u5f71\u54cd\u4f1a\u4e25\u91cd\u4f4e\u4f30\u8bb0\u5fc6\u98ce\u9669\uff0c\u800c\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\u7684\u5b58\u5728\u663e\u8457\u964d\u4f4e\u81ea\u8eab\u5f71\u54cd\uff0c\u4f46\u4ecd\u53ef\u80fd\u88ab\u63d0\u53d6\u3002\u5f71\u54cd\u5206\u5e03\u5206\u6790\u8fd8\u63ed\u793a\u4e86CIFAR-10\u4e2d\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\u7684\u5b58\u5728\u3002", "conclusion": "\u8bb0\u5fc6\u73b0\u8c61\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u5b8c\u6574\u5f71\u54cd\u5206\u5e03\u6bd4\u5355\u4e00\u81ea\u8eab\u5f71\u54cd\u66f4\u80fd\u5168\u9762\u6355\u6349\u8bb0\u5fc6\u98ce\u9669\u3002", "paper_title_zh": "\u53cd\u4e8b\u5b9e\u5f71\u54cd\u4f5c\u4e3a\u5206\u5e03\u6027\u91cf", "abstract_zh": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5df2\u77e5\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6837\u672c\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u6cdb\u5316\u95ee\u9898\u3002\u53cd\u4e8b\u5b9e\u81ea\u5f71\u54cd\u662f\u7814\u7a76\u8bb0\u5fc6\u73b0\u8c61\u7684\u5e38\u7528\u6307\u6807\uff0c\u91cf\u5316\u4e86\u6837\u672c\u662f\u5426\u5305\u542b\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u8bb0\u5fc6\u73b0\u8c61\u8fd8\u53d7\u5176\u4ed6\u56e0\u7d20\uff08\u5982\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\uff09\u7684\u5f71\u54cd\u3002\u672c\u6587\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u5f71\u54cd\u89c6\u4e3a\u5206\u5e03\u6027\u91cf\uff0c\u7814\u7a76\u6240\u6709\u8bad\u7ec3\u6837\u672c\u5982\u4f55\u5f71\u54cd\u8bb0\u5fc6\u73b0\u8c61\u3002\u9488\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8ba1\u7b97\u4e86\u8bad\u7ec3\u6837\u672c\u95f4\u7684\u5b8c\u6574\u5f71\u54cd\u5206\u5e03\u5e76\u5206\u6790\u5176\u7279\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u5173\u6ce8\u81ea\u8eab\u5f71\u54cd\u4f1a\u4e25\u91cd\u4f4e\u4f30\u8bb0\u5fc6\u98ce\u9669\uff1a\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\u7684\u5b58\u5728\u663e\u8457\u964d\u4f4e\u81ea\u8eab\u5f71\u54cd\uff0c\u4f46\u8fd9\u4e9b\u6837\u672c\u4ecd\u53ef\u80fd\u88ab\u63d0\u53d6\u3002\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff08\u5982CIFAR-10\uff09\u4e2d\uff0c\u5f71\u54cd\u5206\u5e03\u5206\u6790\u540c\u6837\u63ed\u793a\u4e86\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\u7684\u5b58\u5728\u3002\u7814\u7a76\u5f3a\u8c03\uff0c\u8bb0\u5fc6\u73b0\u8c61\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u5b8c\u6574\u5f71\u54cd\u5206\u5e03\u6bd4\u5355\u4e00\u81ea\u8eab\u5f71\u54cd\u66f4\u80fd\u5168\u9762\u6355\u6349\u8bb0\u5fc6\u98ce\u9669\u3002"}}
{"id": "2506.20599", "pdf": "https://arxiv.org/pdf/2506.20599", "abs": "https://arxiv.org/abs/2506.20599", "authors": ["Ji Qi", "Xinchang Zhang", "Dingqi Ye", "Yongjia Ruan", "Xin Guo", "Shaowen Wang", "Haifeng Li"], "title": "SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of generative artificial intelligence is producing fake\nremote sensing imagery (RSI) that is increasingly difficult to detect,\npotentially leading to erroneous intelligence, fake news, and even conspiracy\ntheories. Existing forgery detection methods typically rely on single visual\nfeatures to capture predefined artifacts, such as spatial-domain cues to detect\nforged objects like roads or buildings in RSI, or frequency-domain features to\nidentify artifacts from up-sampling operations in adversarial generative\nnetworks (GANs). However, the nature of artifacts can significantly differ\ndepending on geographic terrain, land cover types, or specific features within\nthe RSI. Moreover, these complex artifacts evolve as generative models become\nmore sophisticated. In short, over-reliance on a single visual cue makes\nexisting forgery detectors struggle to generalize across diverse remote sensing\ndata. This paper proposed a novel forgery detection framework called SFNet,\ndesigned to identify fake images in diverse remote sensing data by leveraging\nspatial and frequency domain features. Specifically, to obtain rich and\ncomprehensive visual information, SFNet employs two independent feature\nextractors to capture spatial and frequency domain features from input RSIs. To\nfully utilize the complementary domain features, the domain feature mapping\nmodule and the hybrid domain feature refinement module(CBAM attention) of SFNet\nare designed to successively align and fuse the multi-domain features while\nsuppressing redundant information. Experiments on three datasets show that\nSFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art\nRS forgery detection methods and exhibits robust generalization capabilities.\nThe code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.", "AI": {"tldr": "SFNet\u662f\u4e00\u79cd\u65b0\u578b\u9065\u611f\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f2a\u9020\u7684\u9065\u611f\u56fe\u50cf\u8d8a\u6765\u8d8a\u96be\u4ee5\u68c0\u6d4b\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u60c5\u62a5\u548c\u865a\u5047\u65b0\u95fb\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u89c6\u89c9\u7279\u5f81\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u7684\u4f2a\u9020\u573a\u666f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u591a\u57df\u7279\u5f81\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "SFNet\u91c7\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u7279\u5f81\u63d0\u53d6\u5668\u5206\u522b\u6355\u83b7\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u57df\u7279\u5f81\u6620\u5c04\u6a21\u5757\u548c\u6df7\u5408\u57df\u7279\u5f81\u7ec6\u5316\u6a21\u5757\uff08CBAM\u6ce8\u610f\u529b\uff09\u5bf9\u9f50\u548c\u878d\u5408\u591a\u57df\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSFNet\u6bd4\u73b0\u6709\u9065\u611f\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e864%-15.18%\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SFNet\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u4f2a\u9020\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SFNet\uff1a\u878d\u5408\u7a7a\u95f4\u4e0e\u9891\u57df\u7279\u5f81\u7684\u9065\u611f\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b", "abstract_zh": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u6b63\u5728\u4ea7\u751f\u8d8a\u6765\u8d8a\u96be\u4ee5\u68c0\u6d4b\u7684\u4f2a\u9020\u9065\u611f\u56fe\u50cf\uff08RSI\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u60c5\u62a5\u3001\u865a\u5047\u65b0\u95fb\u751a\u81f3\u9634\u8c0b\u8bba\u3002\u73b0\u6709\u7684\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u89c6\u89c9\u7279\u5f81\u6765\u6355\u6349\u9884\u5b9a\u4e49\u7684\u4f2a\u9020\u75d5\u8ff9\uff0c\u4f8b\u5982\u5229\u7528\u7a7a\u95f4\u57df\u7279\u5f81\u68c0\u6d4bRSI\u4e2d\u4f2a\u9020\u7684\u9053\u8def\u6216\u5efa\u7b51\u7269\uff0c\u6216\u5229\u7528\u9891\u57df\u7279\u5f81\u8bc6\u522b\u5bf9\u6297\u751f\u6210\u7f51\u7edc\uff08GANs\uff09\u4e2d\u4e0a\u91c7\u6837\u64cd\u4f5c\u4ea7\u751f\u7684\u75d5\u8ff9\u3002\u7136\u800c\uff0c\u4f2a\u9020\u75d5\u8ff9\u7684\u6027\u8d28\u53ef\u80fd\u56e0\u5730\u7406\u5730\u5f62\u3001\u571f\u5730\u8986\u76d6\u7c7b\u578b\u6216RSI\u4e2d\u7684\u7279\u5b9a\u7279\u5f81\u800c\u663e\u8457\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u65e5\u76ca\u590d\u6742\uff0c\u8fd9\u4e9b\u75d5\u8ff9\u4e5f\u5728\u4e0d\u65ad\u6f14\u53d8\u3002\u7b80\u800c\u8a00\u4e4b\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u89c6\u89c9\u7ebf\u7d22\u4f7f\u5f97\u73b0\u6709\u4f2a\u9020\u68c0\u6d4b\u5668\u96be\u4ee5\u5728\u591a\u6837\u5316\u7684\u9065\u611f\u6570\u636e\u4e2d\u5b9e\u73b0\u6cdb\u5316\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFNet\u7684\u65b0\u578b\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\u6765\u68c0\u6d4b\u591a\u6837\u5316\u7684\u9065\u611f\u6570\u636e\u4e2d\u7684\u4f2a\u9020\u56fe\u50cf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e3a\u4e86\u83b7\u53d6\u4e30\u5bcc\u4e14\u5168\u9762\u7684\u89c6\u89c9\u4fe1\u606f\uff0cSFNet\u91c7\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u7279\u5f81\u63d0\u53d6\u5668\u5206\u522b\u4ece\u8f93\u5165RSI\u4e2d\u6355\u83b7\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u4e92\u8865\u7684\u57df\u7279\u5f81\uff0cSFNet\u8bbe\u8ba1\u4e86\u57df\u7279\u5f81\u6620\u5c04\u6a21\u5757\u548c\u6df7\u5408\u57df\u7279\u5f81\u7ec6\u5316\u6a21\u5757\uff08CBAM\u6ce8\u610f\u529b\uff09\uff0c\u4f9d\u6b21\u5bf9\u9f50\u548c\u878d\u5408\u591a\u57df\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u5197\u4f59\u4fe1\u606f\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSFNet\u6bd4\u73b0\u6709\u9065\u611f\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e864%-15.18%\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u53ef\u5728https://github.com/GeoX-Lab/RSTI/tree/main/SFNet\u83b7\u53d6\u3002"}}
{"id": "2506.19893", "pdf": "https://arxiv.org/pdf/2506.19893", "abs": "https://arxiv.org/abs/2506.19893", "authors": ["Jingzhi Hu", "Geoffrey Ye Li"], "title": "Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks", "categories": ["cs.LG", "cs.AI", "cs.IT", "eess.IV", "math.IT"], "comment": null, "summary": "Due to the surging amount of AI-generated content (AIGC), its provisioning to\nedges and mobile users from the cloud incurs substantial traffic on networks.\nGenerative semantic communication (GSC) offers a promising solution by\ntransmitting highly compact information, i.e., prompt text and latent\nrepresentations, instead of high-dimensional AIGC data. However, GSC relies on\nthe alignment between the knowledge in the cloud generative AI (GAI) and that\npossessed by the edges and users, and between the knowledge for wireless\ntransmission and that of actual channels, which remains challenging. In this\npaper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm\nfor GSC systems. The core idea is to distill the generation knowledge from the\ncloud-GAI into low-rank matrices, which can be incorporated by the edge and\nused to adapt the transmission knowledge to diverse wireless channel\nconditions. DeKA-g comprises two novel methods: metaword-aided knowledge\ndistillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,\nan optimized metaword is employed to enhance the efficiency of knowledge\ndistillation, while VGSA enables efficient adaptation to diverse compression\nrates and SNR ranges. From simulation results, DeKA-g improves the alignment\nbetween the edge-generated images and the cloud-generated ones by 44%.\nMoreover, it adapts to compression rates with 116% higher efficiency than the\nbaseline and enhances the performance in low-SNR conditions by 28%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeKA-g\u7684\u84b8\u998f\u77e5\u8bc6\u5bf9\u9f50\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\uff08GSC\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u4e91\u7aef\u751f\u6210\u5f0fAI\u7684\u77e5\u8bc6\u84b8\u998f\u4e3a\u4f4e\u79e9\u77e9\u9635\uff0c\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0e\u7528\u6237\u7684\u77e5\u8bc6\u5bf9\u9f50\u6548\u7387\uff0c\u5e76\u9002\u5e94\u591a\u6837\u5316\u7684\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDeKA-g\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u5bf9\u9f50\u6548\u7387\u548c\u4fe1\u9053\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u7684\u6fc0\u589e\uff0c\u4ece\u4e91\u7aef\u5411\u8fb9\u7f18\u548c\u79fb\u52a8\u7528\u6237\u63d0\u4f9bAIGC\u4f1a\u5bfc\u81f4\u7f51\u7edc\u6d41\u91cf\u5267\u589e\u3002\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\uff08GSC\uff09\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u63d0\u793a\u6587\u672c\u548c\u6f5c\u5728\u8868\u793a\u800c\u975e\u9ad8\u7ef4AIGC\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0cGSC\u4f9d\u8d56\u4e8e\u4e91\u7aef\u751f\u6210\u5f0fAI\u4e0e\u8fb9\u7f18\u53ca\u7528\u6237\u4e4b\u95f4\u7684\u77e5\u8bc6\u5bf9\u9f50\uff0c\u4ee5\u53ca\u65e0\u7ebf\u4f20\u8f93\u77e5\u8bc6\u4e0e\u5b9e\u9645\u4fe1\u9053\u6761\u4ef6\u7684\u5bf9\u9f50\uff0c\u8fd9\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faDeKA-g\u7b97\u6cd5\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u4e91\u7aef\u751f\u6210\u5f0fAI\u7684\u77e5\u8bc6\u84b8\u998f\u4e3a\u4f4e\u79e9\u77e9\u9635\uff0c\u4f9b\u8fb9\u7f18\u8bbe\u5907\u4f7f\u7528\uff0c\u5e76\u9002\u5e94\u591a\u6837\u5316\u7684\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u3002DeKA-g\u5305\u542b\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1a\u5143\u8bcd\u8f85\u52a9\u77e5\u8bc6\u84b8\u998f\uff08MAKD\uff09\u548c\u53ef\u53d8\u901f\u7387\u5206\u7ec4\u4fe1\u566a\u6bd4\u9002\u5e94\uff08VGSA\uff09\u3002MAKD\u901a\u8fc7\u4f18\u5316\u5143\u8bcd\u63d0\u5347\u77e5\u8bc6\u84b8\u998f\u6548\u7387\uff0cVGSA\u5219\u652f\u6301\u5bf9\u4e0d\u540c\u538b\u7f29\u7387\u548c\u4fe1\u566a\u6bd4\u8303\u56f4\u7684\u9ad8\u6548\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDeKA-g\u5c06\u8fb9\u7f18\u751f\u6210\u56fe\u50cf\u4e0e\u4e91\u7aef\u751f\u6210\u56fe\u50cf\u7684\u5bf9\u9f50\u6548\u7387\u63d0\u5347\u4e8644%\u3002\u6b64\u5916\uff0c\u5176\u538b\u7f29\u7387\u9002\u5e94\u6548\u7387\u6bd4\u57fa\u7ebf\u9ad8116%\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u63d0\u5347\u4e8628%\u3002", "conclusion": "DeKA-g\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u4fe1\u9053\u9002\u5e94\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u77e5\u8bc6\u5bf9\u9f50\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3aAIGC\u7684\u9ad8\u6548\u4f20\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u84b8\u998f\u7684\u77e5\u8bc6\u5bf9\u9f50\u5728AIGC\u4efb\u52a1\u4e2d\u7684\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1", "abstract_zh": "\u7531\u4e8eAI\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u7684\u6fc0\u589e\uff0c\u4ece\u4e91\u7aef\u5411\u8fb9\u7f18\u548c\u79fb\u52a8\u7528\u6237\u63d0\u4f9bAIGC\u4f1a\u5bfc\u81f4\u7f51\u7edc\u6d41\u91cf\u5267\u589e\u3002\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\uff08GSC\uff09\u901a\u8fc7\u4f20\u8f93\u9ad8\u5ea6\u7d27\u51d1\u7684\u4fe1\u606f\uff08\u5982\u63d0\u793a\u6587\u672c\u548c\u6f5c\u5728\u8868\u793a\uff09\u800c\u975e\u9ad8\u7ef4AIGC\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0cGSC\u4f9d\u8d56\u4e8e\u4e91\u7aef\u751f\u6210\u5f0fAI\uff08GAI\uff09\u4e0e\u8fb9\u7f18\u53ca\u7528\u6237\u4e4b\u95f4\u7684\u77e5\u8bc6\u5bf9\u9f50\uff0c\u4ee5\u53ca\u65e0\u7ebf\u4f20\u8f93\u77e5\u8bc6\u4e0e\u5b9e\u9645\u4fe1\u9053\u6761\u4ef6\u7684\u5bf9\u9f50\uff0c\u8fd9\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51faDeKA-g\uff0c\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u77e5\u8bc6\u5bf9\u9f50\u7b97\u6cd5\uff0c\u7528\u4e8eGSC\u7cfb\u7edf\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u4e91\u7aefGAI\u7684\u751f\u6210\u77e5\u8bc6\u84b8\u998f\u4e3a\u4f4e\u79e9\u77e9\u9635\uff0c\u4f9b\u8fb9\u7f18\u8bbe\u5907\u4f7f\u7528\uff0c\u5e76\u9002\u5e94\u591a\u6837\u5316\u7684\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u3002DeKA-g\u5305\u542b\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1a\u5143\u8bcd\u8f85\u52a9\u77e5\u8bc6\u84b8\u998f\uff08MAKD\uff09\u548c\u53ef\u53d8\u901f\u7387\u5206\u7ec4\u4fe1\u566a\u6bd4\u9002\u5e94\uff08VGSA\uff09\u3002MAKD\u901a\u8fc7\u4f18\u5316\u5143\u8bcd\u63d0\u5347\u77e5\u8bc6\u84b8\u998f\u6548\u7387\uff0cVGSA\u5219\u652f\u6301\u5bf9\u4e0d\u540c\u538b\u7f29\u7387\u548c\u4fe1\u566a\u6bd4\u8303\u56f4\u7684\u9ad8\u6548\u9002\u5e94\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeKA-g\u5c06\u8fb9\u7f18\u751f\u6210\u56fe\u50cf\u4e0e\u4e91\u7aef\u751f\u6210\u56fe\u50cf\u7684\u5bf9\u9f50\u6548\u7387\u63d0\u5347\u4e8644%\uff0c\u538b\u7f29\u7387\u9002\u5e94\u6548\u7387\u6bd4\u57fa\u7ebf\u9ad8116%\uff0c\u5e76\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u6027\u80fd\u63d0\u5347\u4e8628%\u3002"}}
{"id": "2506.20520", "pdf": "https://arxiv.org/pdf/2506.20520", "abs": "https://arxiv.org/abs/2506.20520", "authors": ["Charles Arnal", "Ga\u00ebtan Narozniak", "Vivien Cabannes", "Yunhao Tang", "Julia Kempe", "Remi Munos"], "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u975e\u5bf9\u79f0\u7684\u79bb\u7b56\u7565REINFORCE\u7b97\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u57fa\u7ebfV\u6765\u5e73\u8861\u6b63\u8d1f\u5956\u52b1\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5f53V\u4f4e\u4e8e\u671f\u671b\u5956\u52b1\u65f6\u7b97\u6cd5\u5177\u6709\u7b56\u7565\u6539\u8fdb\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u79bb\u7b56\u7565\u65b9\u6cd5\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ecb\u4e8e\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u4e4b\u95f4\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u7b80\u5355\u7684\u79bb\u7b56\u7565REINFORCE\u7b97\u6cd5\uff0c\u63ed\u793a\u6b63\u8d1f\u5956\u52b1\u7684\u5e73\u8861\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u79bb\u7b56\u7565REINFORCE\u7b97\u6cd5\uff0c\u5b9a\u4e49\u4f18\u52bf\u51fd\u6570A=r-V\uff0c\u901a\u8fc7\u8c03\u6574\u57fa\u7ebfV\u6765\u5f3a\u8c03\u9ad8\u5956\u52b1\u6837\u672c\u6216\u60e9\u7f5a\u4f4e\u5956\u52b1\u6837\u672c\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u5f53V\u4f4e\u4e8e\u671f\u671b\u5956\u52b1\u65f6\u7b97\u6cd5\u5177\u6709\u7b56\u7565\u6539\u8fdb\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u7b56\u7565\u6539\u8fdb\u6027\u8d28\uff0c\u5b9e\u9a8c\u5728\u968f\u673a\u8d4c\u535a\u673a\u8bbe\u7f6e\u548c\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u79bb\u7b56\u7565\u66f4\u65b0\u66f4\u4f9d\u8d56\u6b63\u5956\u52b1\u4fe1\u53f7\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u79bb\u7b56\u7565REINFORCE\u7b97\u6cd5\u5728\u5e73\u8861\u6b63\u8d1f\u5956\u52b1\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u975e\u5bf9\u79f0REINFORCE\u7528\u4e8e\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff1a\u5e73\u8861\u6b63\u8d1f\u5956\u52b1", "abstract_zh": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u79bb\u7b56\u7565\u65b9\u6cd5\u6bd4\u540c\u7b56\u7565\u65b9\u6cd5\u5b9e\u73b0\u66f4\u7b80\u5355\u4e14\u6570\u636e\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u901a\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u7814\u7a76\u4e86\u4ecb\u4e8e\u79bb\u7b56\u7565RL\u548c\u76d1\u7763\u5fae\u8c03\u4e4b\u95f4\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e00\u79cd\u7b80\u5355\u7684\u79bb\u7b56\u7565REINFORCE\u7b97\u6cd5\uff0c\u5176\u4e2d\u4f18\u52bf\u5b9a\u4e49\u4e3aA=r-V\uff0cr\u4e3a\u5956\u52b1\uff0cV\u4e3a\u53ef\u8c03\u57fa\u7ebf\u3002\u76f4\u89c2\u4e0a\uff0c\u964d\u4f4eV\u4f1a\u5f3a\u8c03\u9ad8\u5956\u52b1\u6837\u672c\uff0c\u800c\u63d0\u9ad8V\u4f1a\u52a0\u91cd\u60e9\u7f5a\u4f4e\u5956\u52b1\u6837\u672c\u3002\u6211\u4eec\u9996\u5148\u5bf9\u8be5\u7b97\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8868\u660e\u5f53\u57fa\u7ebfV\u4f4e\u4e8e\u671f\u671b\u5956\u52b1\u65f6\uff0c\u7b97\u6cd5\u5177\u6709\u7b56\u7565\u6539\u8fdb\u4fdd\u8bc1\u3002\u5206\u6790\u63ed\u793a\uff0c\u540c\u7b56\u7565\u66f4\u65b0\u53ef\u4ee5\u5b89\u5168\u5229\u7528\u6b63\u8d1f\u4fe1\u53f7\uff0c\u800c\u79bb\u7b56\u7565\u66f4\u65b0\u5219\u66f4\u4f9d\u8d56\u6b63\u5956\u52b1\u3002\u6211\u4eec\u5728\u968f\u673a\u8d4c\u535a\u673a\u8bbe\u7f6e\u548c\u5fae\u8c03\u5148\u8fdbLLM\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002"}}
{"id": "2506.20601", "pdf": "https://arxiv.org/pdf/2506.20601", "abs": "https://arxiv.org/abs/2506.20601", "authors": ["Rui Huang", "Guangyao Zhai", "Zuria Bauer", "Marc Pollefeys", "Federico Tombari", "Leonidas Guibas", "Gao Huang", "Francis Engelmann"], "title": "Video Perception Models for 3D Scene Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVIPScene\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u76843D\u7269\u7406\u4e16\u754c\u5e38\u8bc6\u77e5\u8bc6\uff0c\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u76843D\u573a\u666f\u5408\u6210\u3002", "motivation": "\u4f20\u7edf3D\u573a\u666f\u5408\u6210\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u548c\u5927\u91cf\u4eba\u5de5\u64cd\u4f5c\uff0c\u81ea\u52a8\u5316\u9700\u6c42\u8feb\u5207\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u57283D\u7a7a\u95f4\u63a8\u7406\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "VIPScene\u7ed3\u5408\u89c6\u9891\u751f\u6210\u3001\u524d\u99883D\u91cd\u5efa\u548c\u5f00\u653e\u8bcd\u6c47\u611f\u77e5\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\u5b9e\u73b0\u8bed\u4e49\u548c\u51e0\u4f55\u5206\u6790\uff0c\u5e76\u5f15\u5165FPVScore\u8bc4\u4f30\u4e00\u81f4\u6027\u548c\u5408\u7406\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVIPScene\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VIPScene\u4e3a3D\u573a\u666f\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u771f\u5b9e\u611f\u548c\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u89c6\u9891\u611f\u77e5\u6a21\u578b\u76843D\u573a\u666f\u5408\u6210", "abstract_zh": "\u4f20\u7edf\u4e0a\uff0c3D\u573a\u666f\u5408\u6210\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u548c\u5927\u91cf\u4eba\u5de5\u64cd\u4f5c\u3002\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u53ef\u4e3a\u5efa\u7b51\u8bbe\u8ba1\u3001\u673a\u5668\u4eba\u4eff\u771f\u3001\u865a\u62df\u73b0\u5b9e\u548c\u6e38\u620f\u7b49\u9886\u57df\u5e26\u6765\u5de8\u5927\u76ca\u5904\u3002\u5f53\u524d\u76843D\u573a\u666f\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e38\u8bc6\u63a8\u7406\u6216\u73b0\u4ee3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u5148\u9a8c\u3002\u7136\u800c\uff0c\u73b0\u6709LLMs\u76843D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u751f\u6210\u903c\u771f\u4e14\u8fde\u8d2f\u76843D\u573a\u666f\u7684\u80fd\u529b\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u65b9\u6cd5\u5e38\u53d7\u9650\u4e8e\u89c6\u89d2\u9009\u62e9\u548c\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u3002\u672c\u6587\u63d0\u51faVIPScene\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7f16\u7801\u76843D\u7269\u7406\u4e16\u754c\u5e38\u8bc6\u77e5\u8bc6\uff0c\u786e\u4fdd\u573a\u666f\u5e03\u5c40\u7684\u8fde\u8d2f\u6027\u548c\u591a\u89c6\u89d2\u4e0b\u7269\u4f53\u653e\u7f6e\u7684\u4e00\u81f4\u6027\u3002VIPScene\u652f\u6301\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\uff0c\u65e0\u7f1d\u6574\u5408\u89c6\u9891\u751f\u6210\u3001\u524d\u99883D\u91cd\u5efa\u548c\u5f00\u653e\u8bcd\u6c47\u611f\u77e5\u6a21\u578b\uff0c\u5bf9\u573a\u666f\u4e2d\u7684\u6bcf\u4e2a\u7269\u4f53\u8fdb\u884c\u8bed\u4e49\u548c\u51e0\u4f55\u5206\u6790\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u7075\u6d3b\u573a\u666f\u5408\u6210\u3002\u4e3a\u8fdb\u4e00\u6b65\u7cbe\u786e\u5206\u6790\uff0c\u6211\u4eec\u5f15\u5165\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bc4\u5206\uff08FPVScore\uff09\uff0c\u5229\u7528\u8fde\u7eed\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u53d1\u6325\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc4\u4f30\u4e00\u81f4\u6027\u548c\u5408\u7406\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVIPScene\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.19894", "pdf": "https://arxiv.org/pdf/2506.19894", "abs": "https://arxiv.org/abs/2506.19894", "authors": ["Antoine Pesenti", "Aidan OSullivan"], "title": "Explaining deep neural network models for electricity price forecasting with XAI", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Electricity markets are highly complex, involving lots of interactions and\ncomplex dependencies that make it hard to understand the inner workings of the\nmarket and what is driving prices. Econometric methods have been developed for\nthis, white-box models, however, they are not as powerful as deep neural\nnetwork models (DNN). In this paper, we use a DNN to forecast the price and\nthen use XAI methods to understand the factors driving the price dynamics in\nthe market. The objective is to increase our understanding of how different\nelectricity markets work. To do that, we apply explainable methods such as SHAP\nand Gradient, combined with visual techniques like heatmaps (saliency maps) to\nanalyse the behaviour and contributions of various features across five\nelectricity markets. We introduce the novel concepts of SSHAP values and SSHAP\nlines to enhance the complex representation of high-dimensional tabular models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u529b\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7SHAP\u548c\u68af\u5ea6\u7b49\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790\u5e02\u573a\u52a8\u6001\u3002", "motivation": "\u7535\u529b\u5e02\u573a\u9ad8\u5ea6\u590d\u6742\uff0c\u4f20\u7edf\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u65b9\u6cd5\uff08\u767d\u76d2\u6a21\u578b\uff09\u9884\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u800cDNN\u867d\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7XAI\u65b9\u6cd5\u589e\u5f3a\u5bf9DNN\u9884\u6d4b\u7ed3\u679c\u7684\u7406\u89e3\uff0c\u63ed\u793a\u7535\u529b\u5e02\u573a\u4ef7\u683c\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u4f7f\u7528DNN\u8fdb\u884c\u7535\u529b\u4ef7\u683c\u9884\u6d4b\uff0c\u5e76\u5e94\u7528SHAP\u3001\u68af\u5ea6\u7b49XAI\u65b9\u6cd5\uff0c\u7ed3\u5408\u70ed\u56fe\uff08\u663e\u8457\u6027\u56fe\uff09\u6280\u672f\uff0c\u5206\u6790\u4e94\u4e2a\u7535\u529b\u5e02\u573a\u4e2d\u5404\u7279\u5f81\u7684\u884c\u4e3a\u548c\u8d21\u732e\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86SSHAP\u503c\u548cSSHAP\u7ebf\u7684\u65b0\u6982\u5ff5\uff0c\u4ee5\u589e\u5f3a\u9ad8\u7ef4\u8868\u683c\u6a21\u578b\u7684\u590d\u6742\u8868\u793a\u3002", "result": "\u901a\u8fc7XAI\u65b9\u6cd5\u6210\u529f\u63ed\u793a\u4e86\u7535\u529b\u5e02\u573a\u4ef7\u683c\u52a8\u6001\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u9a8c\u8bc1\u4e86DNN\u6a21\u578b\u5728\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002SSHAP\u503c\u548cSSHAP\u7ebf\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86DNN\u4e0eXAI\u7ed3\u5408\u5728\u7535\u529b\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u901a\u8fc7\u89e3\u91ca\u6027\u65b9\u6cd5\u589e\u5f3a\u4e86\u5e02\u573a\u52a8\u6001\u7684\u7406\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u5229\u7528XAI\u89e3\u91ca\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b", "abstract_zh": "\u7535\u529b\u5e02\u573a\u9ad8\u5ea6\u590d\u6742\uff0c\u6d89\u53ca\u5927\u91cf\u4ea4\u4e92\u548c\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd9\u4f7f\u5f97\u7406\u89e3\u5e02\u573a\u5185\u90e8\u8fd0\u4f5c\u548c\u4ef7\u683c\u9a71\u52a8\u56e0\u7d20\u53d8\u5f97\u56f0\u96be\u3002\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u65b9\u6cd5\uff08\u767d\u76d2\u6a21\u578b\uff09\u5df2\u88ab\u5f00\u53d1\u7528\u4e8e\u6b64\u76ee\u7684\uff0c\u4f46\u5176\u9884\u6d4b\u80fd\u529b\u4e0d\u5982\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08DNN\uff09\u3002\u672c\u6587\u4f7f\u7528DNN\u9884\u6d4b\u4ef7\u683c\uff0c\u5e76\u5229\u7528XAI\u65b9\u6cd5\u7406\u89e3\u5e02\u573a\u4ef7\u683c\u52a8\u6001\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u65e8\u5728\u589e\u5f3a\u5bf9\u4e0d\u540c\u7535\u529b\u5e02\u573a\u8fd0\u4f5c\u65b9\u5f0f\u7684\u7406\u89e3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5e94\u7528\u4e86SHAP\u548c\u68af\u5ea6\u7b49\u53ef\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u70ed\u56fe\uff08\u663e\u8457\u6027\u56fe\uff09\u6280\u672f\uff0c\u5206\u6790\u4e86\u4e94\u4e2a\u7535\u529b\u5e02\u573a\u4e2d\u5404\u7279\u5f81\u7684\u884c\u4e3a\u548c\u8d21\u732e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86SSHAP\u503c\u548cSSHAP\u7ebf\u7684\u65b0\u6982\u5ff5\uff0c\u4ee5\u589e\u5f3a\u9ad8\u7ef4\u8868\u683c\u6a21\u578b\u7684\u590d\u6742\u8868\u793a\u3002"}}
{"id": "2506.20629", "pdf": "https://arxiv.org/pdf/2506.20629", "abs": "https://arxiv.org/abs/2506.20629", "authors": ["Soufiane Hayou", "Nikhil Ghosh", "Bin Yu"], "title": "PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "TD,LR: A lightweight module type selection method for LoRA\n  finetuning. PLoP gives precise placements for LoRA adapters for improved\n  performance", "summary": "Low-Rank Adaptation (LoRA) is a widely used finetuning method for large\nmodels. Its small memory footprint allows practitioners to adapt large models\nto specific tasks at a fraction of the cost of full finetuning. Different\nmodifications have been proposed to enhance its efficiency by, for example,\nsetting the learning rate, the rank, and the initialization. Another\nimprovement axis is adapter placement strategy: when using LoRA, practitioners\nusually pick module types to adapt with LoRA, such as Query and Key modules.\nFew works have studied the problem of adapter placement, with nonconclusive\nresults: original LoRA paper suggested placing adapters in attention modules,\nwhile other works suggested placing them in the MLP modules. Through an\nintuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a\nlightweight method that allows automatic identification of module types where\nLoRA adapters should be placed, given a pretrained model and a finetuning task.\nWe demonstrate that PLoP consistently outperforms, and in the worst case\ncompetes, with commonly used placement strategies through comprehensive\nexperiments on supervised finetuning and reinforcement learning for reasoning.", "AI": {"tldr": "PLoP\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc6\u522bLoRA\u9002\u914d\u5668\u7684\u6700\u4f73\u653e\u7f6e\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u6a21\u578b\u5fae\u8c03\u7684\u6548\u7387\u3002", "motivation": "\u5f53\u524dLoRA\u9002\u914d\u5668\u7684\u653e\u7f6e\u7b56\u7565\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u4e0d\u540c\u7814\u7a76\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u9002\u914d\u5668\u653e\u7f6e\u3002", "method": "PLoP\u901a\u8fc7\u76f4\u89c2\u7684\u7406\u8bba\u5206\u6790\uff0c\u81ea\u52a8\u8bc6\u522b\u7ed9\u5b9a\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5fae\u8c03\u4efb\u52a1\u4e2dLoRA\u9002\u914d\u5668\u7684\u6700\u4f73\u653e\u7f6e\u6a21\u5757\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPLoP\u5728\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u5e38\u7528\u653e\u7f6e\u7b56\u7565\u76f8\u5f53\u3002", "conclusion": "PLoP\u4e3aLoRA\u9002\u914d\u5668\u7684\u653e\u7f6e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8c03\u6027\u80fd\u3002", "paper_title_zh": "PLoP\uff1a\u7cbe\u786eLoRA\u653e\u7f6e\u4ee5\u5b9e\u73b0\u5927\u578b\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03", "abstract_zh": "\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u662f\u4e00\u79cd\u5e7f\u6cdb\u7528\u4e8e\u5927\u578b\u6a21\u578b\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u5176\u5c0f\u5185\u5b58\u5360\u7528\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u5c06\u5927\u578b\u6a21\u578b\u9002\u914d\u5230\u7279\u5b9a\u4efb\u52a1\u3002\u5df2\u6709\u7814\u7a76\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u3001\u79e9\u548c\u521d\u59cb\u5316\u7b49\u65b9\u5f0f\u63d0\u5347\u5176\u6548\u7387\uff0c\u4f46\u9002\u914d\u5668\u653e\u7f6e\u7b56\u7565\u7684\u7814\u7a76\u8f83\u5c11\u4e14\u7ed3\u679c\u4e0d\u4e00\uff1a\u539f\u59cbLoRA\u8bba\u6587\u5efa\u8bae\u5c06\u9002\u914d\u5668\u653e\u7f6e\u5728\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\uff0c\u800c\u5176\u4ed6\u7814\u7a76\u5219\u5efa\u8bae\u653e\u5728MLP\u6a21\u5757\u4e2d\u3002\u901a\u8fc7\u76f4\u89c2\u7684\u7406\u8bba\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PLoP\uff08\u7cbe\u786eLoRA\u653e\u7f6e\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5fae\u8c03\u4efb\u52a1\u81ea\u52a8\u8bc6\u522bLoRA\u9002\u914d\u5668\u7684\u6700\u4f73\u653e\u7f6e\u6a21\u5757\u7c7b\u578b\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u7684\u7efc\u5408\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660ePLoP\u59cb\u7ec8\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u5e38\u7528\u653e\u7f6e\u7b56\u7565\u76f8\u5f53\u3002"}}
{"id": "2506.20616", "pdf": "https://arxiv.org/pdf/2506.20616", "abs": "https://arxiv.org/abs/2506.20616", "authors": ["Quoc-Duy Tran", "Anh-Tuan Vo", "Dinh-Khoi Vo", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Shape2Animal: Creative Animal Generation from Natural Silhouettes", "categories": ["cs.CV"], "comment": null, "summary": "Humans possess a unique ability to perceive meaningful patterns in ambiguous\nstimuli, a cognitive phenomenon known as pareidolia. This paper introduces\nShape2Animal framework to mimics this imaginative capacity by reinterpreting\nnatural object silhouettes, such as clouds, stones, or flames, as plausible\nanimal forms. Our automated framework first performs open-vocabulary\nsegmentation to extract object silhouette and interprets semantically\nappropriate animal concepts using vision-language models. It then synthesizes\nan animal image that conforms to the input shape, leveraging text-to-image\ndiffusion model and seamlessly blends it into the original scene to generate\nvisually coherent and spatially consistent compositions. We evaluated\nShape2Animal on a diverse set of real-world inputs, demonstrating its\nrobustness and creative potential. Our Shape2Animal can offer new opportunities\nfor visual storytelling, educational content, digital art, and interactive\nmedia design. Our project page is here: https://shape2image.github.io", "AI": {"tldr": "Shape2Animal\u662f\u4e00\u4e2a\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u7269\u4f53\u8f6e\u5ed3\uff08\u5982\u4e91\u3001\u77f3\u5934\u6216\u706b\u7130\uff09\u91cd\u65b0\u89e3\u91ca\u4e3a\u52a8\u7269\u5f62\u6001\uff0c\u6a21\u62df\u4eba\u7c7b\u7684pareidolia\u73b0\u8c61\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u89c6\u89c9\u4e00\u81f4\u4e14\u7a7a\u95f4\u8fde\u8d2f\u7684\u52a8\u7269\u56fe\u50cf\u3002", "motivation": "\u53d7\u4eba\u7c7bpareidolia\u73b0\u8c61\u7684\u542f\u53d1\uff0c\u5373\u4ece\u6a21\u7cca\u523a\u6fc0\u4e2d\u611f\u77e5\u6709\u610f\u4e49\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u7269\u4f53\u8f6e\u5ed3\u8f6c\u5316\u4e3a\u903c\u771f\u7684\u52a8\u7269\u5f62\u6001\uff0c\u4e3a\u89c6\u89c9\u53d9\u4e8b\u3001\u6559\u80b2\u5185\u5bb9\u548c\u6570\u5b57\u827a\u672f\u63d0\u4f9b\u65b0\u673a\u4f1a\u3002", "method": "Shape2Animal\u9996\u5148\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u63d0\u53d6\u7269\u4f53\u8f6e\u5ed3\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u8bed\u4e49\u76f8\u5173\u7684\u52a8\u7269\u6982\u5ff5\uff0c\u518d\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u7b26\u5408\u8f93\u5165\u5f62\u72b6\u7684\u52a8\u7269\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u65e0\u7f1d\u878d\u5408\u5230\u539f\u59cb\u573a\u666f\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cShape2Animal\u5728\u591a\u6837\u5316\u771f\u5b9e\u8f93\u5165\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u5c55\u793a\u4e86\u5176\u521b\u9020\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u4e14\u7a7a\u95f4\u4e00\u81f4\u7684\u52a8\u7269\u56fe\u50cf\u3002", "conclusion": "Shape2Animal\u4e3a\u89c6\u89c9\u53d9\u4e8b\u3001\u6559\u80b2\u5185\u5bb9\u3001\u6570\u5b57\u827a\u672f\u548c\u4ea4\u4e92\u5a92\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u521b\u610f\u751f\u6210\u7684\u6f5c\u529b\u3002", "paper_title_zh": "Shape2Animal\uff1a\u4ece\u81ea\u7136\u8f6e\u5ed3\u751f\u6210\u521b\u610f\u52a8\u7269", "abstract_zh": "\u4eba\u7c7b\u5177\u6709\u4ece\u6a21\u7cca\u523a\u6fc0\u4e2d\u611f\u77e5\u6709\u610f\u4e49\u6a21\u5f0f\u7684\u72ec\u7279\u80fd\u529b\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3apareidolia\u3002\u672c\u6587\u63d0\u51fa\u7684Shape2Animal\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u81ea\u7136\u7269\u4f53\u8f6e\u5ed3\uff08\u5982\u4e91\u3001\u77f3\u5934\u6216\u706b\u7130\uff09\u4e3a\u903c\u771f\u7684\u52a8\u7269\u5f62\u6001\uff0c\u6a21\u62df\u8fd9\u79cd\u60f3\u8c61\u529b\u3002\u6211\u4eec\u7684\u81ea\u52a8\u5316\u6846\u67b6\u9996\u5148\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u63d0\u53d6\u7269\u4f53\u8f6e\u5ed3\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u8bed\u4e49\u76f8\u5173\u7684\u52a8\u7269\u6982\u5ff5\u3002\u7136\u540e\uff0c\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u7b26\u5408\u8f93\u5165\u5f62\u72b6\u7684\u52a8\u7269\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u65e0\u7f1d\u878d\u5408\u5230\u539f\u59cb\u573a\u666f\u4e2d\uff0c\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u4e14\u7a7a\u95f4\u4e00\u81f4\u7684\u5408\u6210\u56fe\u50cf\u3002\u6211\u4eec\u5728\u591a\u6837\u5316\u771f\u5b9e\u8f93\u5165\u4e0a\u8bc4\u4f30\u4e86Shape2Animal\uff0c\u8bc1\u660e\u4e86\u5176\u7a33\u5065\u6027\u548c\u521b\u9020\u6f5c\u529b\u3002Shape2Animal\u4e3a\u89c6\u89c9\u53d9\u4e8b\u3001\u6559\u80b2\u5185\u5bb9\u3001\u6570\u5b57\u827a\u672f\u548c\u4ea4\u4e92\u5a92\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://shape2image.github.io"}}
{"id": "2506.19895", "pdf": "https://arxiv.org/pdf/2506.19895", "abs": "https://arxiv.org/abs/2506.19895", "authors": ["Miguel N. Font", "Jos\u00e9 L. Jorro-Aragoneses", "Carlos M. Ala\u00edz"], "title": "A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted for presentation at ICANN 2025\n  (International Conference on Artificial Neural Networks) and will appear in\n  the conference proceedings published by Springer Nature in the Lecture Notes\n  in Computer Science (LNCS) series. The final authenticated version will be\n  available on the publisher website", "summary": "Neural Networks have high accuracy in solving problems where it is difficult\nto detect patterns or create a logical model. However, these algorithms\nsometimes return wrong solutions, which become problematic in high-risk domains\nlike medical diagnosis or autonomous driving. One strategy to detect and\nmitigate these errors is the measurement of the uncertainty over neural network\ndecisions. In this paper, we present a novel post-hoc framework for measuring\nthe uncertainty of a decision based on retrieved training cases that have a\nsimilar activation vector to the query for each layer. Based on these retrieved\ncases, we propose two new metrics: Decision Change and Layer Uncertainty, which\ncapture changes in nearest-neighbor class distributions across layers. We\nevaluated our approach in a classification model for two datasets: CIFAR-10 and\nMNIST. The results show that these metrics enhance uncertainty estimation,\nespecially in challenging classification tasks, outperforming softmax-based\nconfidence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u5404\u5c42\u6700\u8fd1\u90bb\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u6307\u6807\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u590d\u6742\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9519\u8bef\u51b3\u7b56\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u8bca\u65ad\u548c\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u540e\u679c\u3002\u56e0\u6b64\uff0c\u91cf\u5316\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u6210\u4e3a\u5173\u952e\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u4e0e\u67e5\u8be2\u5728\u5404\u5c42\u6fc0\u6d3b\u5411\u91cf\u76f8\u4f3c\u7684\u8bad\u7ec3\u6848\u4f8b\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u6307\u6807\uff1a\u51b3\u7b56\u53d8\u5316\u548c\u5c42\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u6355\u6349\u8de8\u5c42\u6700\u8fd1\u90bb\u7c7b\u522b\u5206\u5e03\u7684\u53d8\u5316\u3002", "result": "\u5728CIFAR-10\u548cMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6548\u679c\uff0c\u5c24\u5176\u5728\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u4e8esoftmax\u7684\u7f6e\u4fe1\u5ea6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u795e\u7ecf\u7f51\u7edc\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u81f3\u5176\u4ed6\u4efb\u52a1\u3002", "paper_title_zh": "\u57fa\u4e8e\u8de8\u5c42\u6700\u8fd1\u90bb\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6", "abstract_zh": "\u795e\u7ecf\u7f51\u7edc\u5728\u89e3\u51b3\u96be\u4ee5\u68c0\u6d4b\u6a21\u5f0f\u6216\u6784\u5efa\u903b\u8f91\u6a21\u578b\u7684\u95ee\u9898\u65f6\u5177\u6709\u9ad8\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u6709\u65f6\u4f1a\u8fd4\u56de\u9519\u8bef\u89e3\uff0c\u8fd9\u5728\u533b\u7597\u8bca\u65ad\u6216\u81ea\u52a8\u9a7e\u9a76\u7b49\u9ad8\u5371\u9886\u57df\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002\u4e00\u79cd\u68c0\u6d4b\u548c\u7f13\u89e3\u8fd9\u4e9b\u9519\u8bef\u7684\u7b56\u7565\u662f\u6d4b\u91cf\u795e\u7ecf\u7f51\u7edc\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u540e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u4e0e\u67e5\u8be2\u5728\u5404\u5c42\u6fc0\u6d3b\u5411\u91cf\u76f8\u4f3c\u7684\u8bad\u7ec3\u6848\u4f8b\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u6307\u6807\uff1a\u51b3\u7b56\u53d8\u5316\u548c\u5c42\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u6355\u6349\u8de8\u5c42\u6700\u8fd1\u90bb\u7c7b\u522b\u5206\u5e03\u7684\u53d8\u5316\u3002\u6211\u4eec\u5728CIFAR-10\u548cMNIST\u6570\u636e\u96c6\u7684\u5206\u7c7b\u6a21\u578b\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6307\u6807\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6548\u679c\uff0c\u5c24\u5176\u5728\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u4e8esoftmax\u7684\u7f6e\u4fe1\u5ea6\u3002"}}
{"id": "2506.20638", "pdf": "https://arxiv.org/pdf/2506.20638", "abs": "https://arxiv.org/abs/2506.20638", "authors": ["Cl\u00e9ment Forray", "Pauline Delporte", "Nicolas Delaygue", "Florence Genin", "Dawa Derksen"], "title": "Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects", "categories": ["cs.CV"], "comment": "accepted for CVPR 2025 NFBCC workshop", "summary": "Obtaining a better knowledge of the current state and behavior of objects\norbiting Earth has proven to be essential for a range of applications such as\nactive debris removal, in-orbit maintenance, or anomaly detection. 3D models\nrepresent a valuable source of information in the field of Space Situational\nAwareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to\nperform 3D reconstruction of non-cooperative space objects from simulated\nimages. This scenario is challenging for NeRF models due to unusual camera\ncharacteristics and environmental conditions : mono-chromatic images, unknown\nobject orientation, limited viewing angles, absence of diffuse lighting etc. In\nthis work we focus primarly on the joint optimization of camera poses alongside\nthe NeRF. Our experimental results show that the most accurate 3D\nreconstruction is achieved when training with successive images one-by-one. We\nestimate camera poses by optimizing an uniform rotation and use regularization\nto prevent successive poses from being too far apart.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u5bf9\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u8fdb\u884c3D\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76f8\u673a\u59ff\u6001\u548cNeRF\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5355\u8272\u56fe\u50cf\u3001\u672a\u77e5\u7269\u4f53\u65b9\u5411\u548c\u6709\u9650\u89c6\u89d2\u7b49\u6311\u6218\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9010\u5e27\u8bad\u7ec3\u56fe\u50cf\u80fd\u5b9e\u73b0\u6700\u51c6\u786e\u76843D\u91cd\u5efa\u3002", "motivation": "\u5730\u7403\u8f68\u9053\u7269\u4f53\u7684\u72b6\u6001\u548c\u884c\u4e3a\u4fe1\u606f\u5bf9\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u3001\u5728\u8f68\u7ef4\u62a4\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u30023D\u6a21\u578b\u662f\u7a7a\u95f4\u6001\u52bf\u611f\u77e5\uff08SSA\uff09\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\u3002\u672c\u6587\u65e8\u5728\u5229\u7528NeRF\u6280\u672f\uff0c\u5728\u5355\u8272\u56fe\u50cf\u3001\u672a\u77e5\u7269\u4f53\u65b9\u5411\u548c\u6709\u9650\u89c6\u89d2\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u76843D\u91cd\u5efa\u3002", "method": "\u672c\u6587\u91c7\u7528\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u6280\u672f\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76f8\u673a\u59ff\u6001\u548cNeRF\u6a21\u578b\uff0c\u5904\u7406\u5355\u8272\u56fe\u50cf\u3001\u672a\u77e5\u7269\u4f53\u65b9\u5411\u548c\u6709\u9650\u89c6\u89d2\u7b49\u95ee\u9898\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u9010\u5e27\u8bad\u7ec3\u56fe\u50cf\u3001\u4f18\u5316\u5747\u5300\u65cb\u8f6c\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\uff0c\u5e76\u4f7f\u7528\u6b63\u5219\u5316\u9632\u6b62\u76f8\u90bb\u59ff\u6001\u5dee\u5f02\u8fc7\u5927\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9010\u5e27\u8bad\u7ec3\u56fe\u50cf\u80fd\u591f\u5b9e\u73b0\u6700\u51c6\u786e\u76843D\u91cd\u5efa\u3002\u901a\u8fc7\u4f18\u5316\u5747\u5300\u65cb\u8f6c\u548c\u4f7f\u7528\u6b63\u5219\u5316\uff0c\u76f8\u673a\u59ff\u6001\u7684\u4f30\u8ba1\u66f4\u52a0\u7a33\u5b9a\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5355\u8272\u56fe\u50cf\u548c\u6709\u9650\u89c6\u89d2\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0cNeRF\u6280\u672f\u4ecd\u80fd\u6709\u6548\u5b9e\u73b0\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u76843D\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76f8\u673a\u59ff\u6001\u548cNeRF\u6a21\u578b\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "paper_title_zh": "\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u7684\u8054\u5408\u59ff\u6001\u4f30\u8ba1\u4e0e3D\u795e\u7ecf\u91cd\u5efa", "abstract_zh": "\u66f4\u597d\u5730\u4e86\u89e3\u5730\u7403\u8f68\u9053\u7269\u4f53\u7684\u5f53\u524d\u72b6\u6001\u548c\u884c\u4e3a\u5bf9\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u3001\u5728\u8f68\u7ef4\u62a4\u6216\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u30023D\u6a21\u578b\u662f\u7a7a\u95f4\u6001\u52bf\u611f\u77e5\uff08SSA\uff09\u9886\u57df\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\u3002\u672c\u6587\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u6280\u672f\uff0c\u4ece\u6a21\u62df\u56fe\u50cf\u4e2d\u5bf9\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u8fdb\u884c3D\u91cd\u5efa\u3002\u7531\u4e8e\u5355\u8272\u56fe\u50cf\u3001\u672a\u77e5\u7269\u4f53\u65b9\u5411\u3001\u6709\u9650\u89c6\u89d2\u548c\u7f3a\u4e4f\u6f2b\u5c04\u7167\u660e\u7b49\u6311\u6218\u6027\u6761\u4ef6\uff0c\u8fd9\u4e00\u573a\u666f\u5bf9NeRF\u6a21\u578b\u5177\u6709\u8f83\u9ad8\u96be\u5ea6\u3002\u672c\u6587\u4e3b\u8981\u5173\u6ce8\u76f8\u673a\u59ff\u6001\u4e0eNeRF\u7684\u8054\u5408\u4f18\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9010\u5e27\u8bad\u7ec3\u56fe\u50cf\u80fd\u591f\u5b9e\u73b0\u6700\u51c6\u786e\u76843D\u91cd\u5efa\u3002\u6211\u4eec\u901a\u8fc7\u4f18\u5316\u5747\u5300\u65cb\u8f6c\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\uff0c\u5e76\u4f7f\u7528\u6b63\u5219\u5316\u9632\u6b62\u76f8\u90bb\u59ff\u6001\u5dee\u5f02\u8fc7\u5927\u3002"}}
{"id": "2506.19897", "pdf": "https://arxiv.org/pdf/2506.19897", "abs": "https://arxiv.org/abs/2506.19897", "authors": ["Christopher Glasz", "Emily Escamilla", "Eric O. Scott", "Anand Patel", "Jacob Zimmer", "Colin Diggs", "Michael Doyle", "Scott Rosen", "Nitin Naik", "Justin F. Brunelle", "Samruddhi Thaker", "Parthav Poudel", "Arun Sridharan", "Amit Madan", "Doug Wendt", "William Macke", "Thomas Schill"], "title": "Can LLMs Replace Humans During Code Chunking?", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u653f\u5e9c\u9057\u7559\u4ee3\u7801\u73b0\u4ee3\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9ALC\u548cMUMPS\u8bed\u8a00\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u4ee3\u7801\u5206\u5757\u548c\u751f\u6210\u6a21\u5757\u6ce8\u91ca\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u5206\u522b\u63d0\u534720%\u548c10%\u3002", "motivation": "\u653f\u5e9c\u4f01\u4e1a\u8f6f\u4ef6\u5e38\u4f7f\u7528\u9057\u7559\u8bed\u8a00\uff08\u5982MUMPS\u548cALC\uff09\uff0c\u800c\u73b0\u6709LLMs\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u73b0\u4ee3\u8bed\u8a00\uff0c\u5bf9\u9057\u7559\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\u672a\u77e5\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4ee3\u7801\u957f\u5ea6\u8d85\u51faLLMs\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u4e9f\u9700\u7814\u7a76\u5982\u4f55\u4f18\u5316\u4ee3\u7801\u5206\u5757\u4ee5\u652f\u6301\u73b0\u4ee3\u5316\u4efb\u52a1\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u4ee3\u7801\u5206\u5757\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u5bf9LLMs\uff08\u5982GPT-4o\u3001Claude 3 Sonnet\u3001Mixtral\u548cLlama 3\uff09\u751f\u6210\u9057\u7559\u4ee3\u7801\u6a21\u5757\u6ce8\u91ca\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u91cd\u70b9\u5173\u6ce8\u5206\u5757\u65b9\u6cd5\u5bf9\u6587\u6863\u751f\u6210\u4efb\u52a1\u7684\u6548\u679c\u3002", "result": "LLMs\u9009\u62e9\u7684\u5206\u5757\u70b9\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5206\u5757\u9ad8\u5ea6\u4e00\u81f4\u3002LLMs\u751f\u6210\u7684\u5206\u5757\u6ce8\u91ca\u6bd4\u4eba\u7c7b\u5206\u5757\u7684\u6ce8\u91ca\u5728\u4e8b\u5b9e\u6027\u4e0a\u9ad8\u51fa20%\uff0c\u5b9e\u7528\u6027\u4e0a\u9ad8\u51fa10%\u3002", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u4eba\u7c7b\u5206\u5757\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728LLM\u8f85\u52a9\u7684\u4ee3\u7801\u73b0\u4ee3\u5316\u8fc7\u7a0b\u4e2d\u9ad8\u6548\u5904\u7406\u5927\u578b\u4ee3\u7801\u5e93\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u66ff\u4ee3\u4eba\u7c7b\u8fdb\u884c\u4ee3\u7801\u5206\u5757\uff1f", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u6210\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u91cd\u8981\u5de5\u5177\uff0c\u5c24\u5176\u5728\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u672a\u89e3\u51b3\u653f\u5e9c\u5e94\u7528\u4ee3\u7801\u7684\u72ec\u7279\u6311\u6218\u3002\u653f\u5e9c\u4f01\u4e1a\u8f6f\u4ef6\u5e38\u4f7f\u7528\u9057\u7559\u8bed\u8a00\uff08\u5982MUMPS\u6216\u6c47\u7f16\u8bed\u8a00\u4ee3\u7801ALC\uff09\uff0c\u4e14\u5176\u4ee3\u7801\u957f\u5ea6\u8d85\u51fa\u5f53\u524d\u5546\u7528LLMs\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3002\u6b64\u5916\uff0cLLMs\u4e3b\u8981\u9488\u5bf9\u73b0\u4ee3\u8bed\u8a00\u8bad\u7ec3\uff0c\u5bf9\u9057\u7559\u8bed\u8a00\u7684\u6d4b\u8bd5\u6709\u9650\uff0c\u5176\u7406\u89e3\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u7814\u7a76\u4e86LLMs\u5728ALC\u548cMUMPS\u9057\u7559\u4ee3\u7801\u73b0\u4ee3\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u8f93\u5165\u9650\u5236\u7684\u6311\u6218\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u591a\u79cd\u4ee3\u7801\u5206\u5757\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u9057\u7559\u4ee3\u7801\u6587\u4ef6\u7684\u6a21\u5757\u6ce8\u91ca\u751f\u6210\uff0c\u5e76\u8bc4\u4f30\u5206\u5757\u65b9\u6cd5\u5bf9\u4e0d\u540cLLMs\uff08\u5982GPT-4o\u3001Claude 3 Sonnet\u3001Mixtral\u548cLlama 3\uff09\u751f\u6210\u6587\u6863\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0cLLMs\u9009\u62e9\u7684\u5206\u5757\u70b9\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5206\u5757\u9ad8\u5ea6\u4e00\u81f4\u3002\u5206\u5757\u65b9\u6cd5\u5bf9\u6587\u6863\u751f\u6210\u7b49\u4e0b\u6e38\u4efb\u52a1\u6709\u663e\u8457\u5f71\u54cd\u3002LLMs\u751f\u6210\u7684\u5206\u5757\u6ce8\u91ca\u6bd4\u4eba\u7c7b\u5206\u5757\u7684\u6ce8\u91ca\u5728\u4e8b\u5b9e\u6027\u4e0a\u9ad8\u51fa20%\uff0c\u5b9e\u7528\u6027\u4e0a\u9ad8\u51fa10%\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff1a\u5728LLM\u8f85\u52a9\u7684\u4ee3\u7801\u73b0\u4ee3\u5316\u8fc7\u7a0b\u4e2d\uff0cLLMs\u53ef\u66ff\u4ee3\u4eba\u7c7b\u5206\u5757\u5927\u578b\u4ee3\u7801\u5e93\u3002"}}
{"id": "2506.20670", "pdf": "https://arxiv.org/pdf/2506.20670", "abs": "https://arxiv.org/abs/2506.20670", "authors": ["Jinming Wu", "Zihao Deng", "Wei Li", "Yiding Liu", "Bo You", "Bo Li", "Zejun Ma", "Ziwei Liu"], "title": "MMSearch-R1: Incentivizing LMMs to Search", "categories": ["cs.CV", "cs.CL"], "comment": "Code: https://github.com/EvolvingLMMs-Lab/multimodal-search-r1", "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.", "AI": {"tldr": "MMSearch-R1\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6fc0\u52b1\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u771f\u5b9e\u4e92\u8054\u7f51\u73af\u5883\u4e2d\u6309\u9700\u8fdb\u884c\u591a\u8f6e\u641c\u7d22\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u641c\u7d22\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u673a\u5236\u4f18\u5316\u641c\u7d22\u884c\u4e3a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51cf\u5c1130%\u641c\u7d22\u8c03\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u641c\u7d22\u4ee3\u7406\u4f9d\u8d56\u56fa\u5b9a\u6d41\u7a0b\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u6216\u8fc7\u5ea6\u641c\u7d22\u3002\u4e3a\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u9700\u5f00\u53d1\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u641c\u7d22\u6846\u67b6\u3002", "method": "\u63d0\u51faMMSearch-R1\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u641c\u7d22\u5de5\u5177\uff0c\u6a21\u578b\u6839\u636e\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u548c\u641c\u7d22\u60e9\u7f5a\u51b3\u5b9a\u4f55\u65f6\u53ca\u5982\u4f55\u8c03\u7528\u641c\u7d22\u5de5\u5177\u3002\u8bad\u7ec3\u4f7f\u7528\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u6536\u96c6\u7684\u591a\u6a21\u6001\u641c\u7d22VQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u641c\u7d22\u9700\u6c42\u548c\u65e0\u641c\u7d22\u9700\u6c42\u7684\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMMSearch-R1\u5728\u77e5\u8bc6\u5bc6\u96c6\u548c\u4fe1\u606f\u5bfb\u6c42VQA\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7bRAG\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u51cf\u5c1130%\u4ee5\u4e0a\u641c\u7d22\u8c03\u7528\uff0c\u540c\u65f6\u5339\u914d\u66f4\u5927RAG\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "MMSearch-R1\u4e3a\u591a\u6a21\u6001\u641c\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u641c\u7d22\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "paper_title_zh": "MMSearch-R1\uff1a\u6fc0\u52b1\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u641c\u7d22", "abstract_zh": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7a33\u5065\u90e8\u7f72\u9700\u8981\u8bbf\u95ee\u5916\u90e8\u77e5\u8bc6\u6e90\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u641c\u7d22\u4ee3\u7406\u4f9d\u8d56\u56fa\u5b9a\u6d41\u7a0b\uff0c\u5e38\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u6216\u8fc7\u5ea6\u641c\u7d22\u3002\u6211\u4eec\u63d0\u51faMMSearch-R1\uff0c\u9996\u4e2a\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fLMMs\u80fd\u591f\u5728\u771f\u5b9e\u4e92\u8054\u7f51\u73af\u5883\u4e2d\u6309\u9700\u8fdb\u884c\u591a\u8f6e\u641c\u7d22\u3002\u8be5\u6846\u67b6\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u641c\u7d22\u5de5\u5177\uff0c\u6a21\u578b\u901a\u8fc7\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u548c\u641c\u7d22\u60e9\u7f5a\u51b3\u5b9a\u4f55\u65f6\u53ca\u5982\u4f55\u8c03\u7528\u641c\u7d22\u5de5\u5177\u3002\u4e3a\u652f\u6301\u8bad\u7ec3\uff0c\u6211\u4eec\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u6536\u96c6\u4e86\u8986\u76d6\u591a\u6837\u5316\u89c6\u89c9\u548c\u6587\u672c\u77e5\u8bc6\u9700\u6c42\u7684\u591a\u6a21\u6001\u641c\u7d22VQA\u6570\u636e\u96c6\uff0c\u5e76\u7b5b\u9009\u51fa\u5305\u542b\u641c\u7d22\u9700\u6c42\u548c\u65e0\u641c\u7d22\u9700\u6c42\u6837\u672c\u7684\u641c\u7d22\u5e73\u8861\u5b50\u96c6\uff0c\u8fd9\u5bf9\u5851\u9020\u9ad8\u6548\u6309\u9700\u641c\u7d22\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u5728\u77e5\u8bc6\u5bc6\u96c6\u548c\u4fe1\u606f\u5bfb\u6c42VQA\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u4ec5\u4f18\u4e8e\u540c\u7c7bRAG\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd8\u80fd\u5728\u51cf\u5c1130%\u4ee5\u4e0a\u641c\u7d22\u8c03\u7528\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u66f4\u5927RAG\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5206\u6790\u5173\u952e\u5b9e\u8bc1\u53d1\u73b0\uff0c\u4e3a\u591a\u6a21\u6001\u641c\u7d22\u7814\u7a76\u7684\u63a8\u8fdb\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2506.20649", "pdf": "https://arxiv.org/pdf/2506.20649", "abs": "https://arxiv.org/abs/2506.20649", "authors": ["Jacopo Dapueto", "Vito Paolo Pastore", "Nicoletta Noceti", "Francesca Odone"], "title": "Disentangled representations of microscopy images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published in: International Joint Conference on Neural Networks\n  (IJCNN 2025). Project page:\n  https://github.com/JacopoDapueto/disentangled_microscopy", "summary": "Microscopy image analysis is fundamental for different applications, from\ndiagnosis to synthetic engineering and environmental monitoring. Modern\nacquisition systems have granted the possibility to acquire an escalating\namount of images, requiring a consequent development of a large collection of\ndeep learning-based automatic image analysis methods. Although deep neural\nnetworks have demonstrated great performance in this field, interpretability,\nan essential requirement for microscopy image analysis, remains an open\nchallenge.\n  This work proposes a Disentangled Representation Learning (DRL) methodology\nto enhance model interpretability for microscopy image classification.\nExploiting benchmark datasets from three different microscopic image domains\n(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based\non transferring a representation learnt from synthetic data, can provide a good\ntrade-off between accuracy and interpretability in this domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u663e\u5fae\u955c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u5728\u8bca\u65ad\u3001\u5408\u6210\u5de5\u7a0b\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u6b64\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u63d0\u5347\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff08DRL\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5408\u6210\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u793a\u5e76\u8fc1\u79fb\u5230\u771f\u5b9e\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u6d6e\u6e38\u751f\u7269\u3001\u9175\u6bcd\u6db2\u6ce1\u548c\u4eba\u7c7b\u7ec6\u80de\u4e09\u4e2a\u9886\u57df\u7684\u663e\u5fae\u955c\u56fe\u50cf\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eDRL\u7684\u6846\u67b6\u80fd\u591f\u5728\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u89e3\u8026\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4e3a\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u5177\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "paper_title_zh": "\u663e\u5fae\u955c\u56fe\u50cf\u7684\u89e3\u8026\u8868\u793a", "abstract_zh": "\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u5728\u4ece\u8bca\u65ad\u5230\u5408\u6210\u5de5\u7a0b\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u591a\u79cd\u5e94\u7528\u4e2d\u5177\u6709\u57fa\u7840\u6027\u4f5c\u7528\u3002\u73b0\u4ee3\u91c7\u96c6\u7cfb\u7edf\u4f7f\u5f97\u83b7\u53d6\u5927\u91cf\u56fe\u50cf\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u8981\u6c42\u5f00\u53d1\u5927\u91cf\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\u3002\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6b64\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u6240\u5fc5\u9700\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\n  \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u663e\u5fae\u955c\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u5229\u7528\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u663e\u5fae\u955c\u56fe\u50cf\u9886\u57df\uff08\u6d6e\u6e38\u751f\u7269\u3001\u9175\u6bcd\u6db2\u6ce1\u548c\u4eba\u7c7b\u7ec6\u80de\uff09\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u57fa\u4e8e\u4ece\u5408\u6210\u6570\u636e\u5b66\u4e60\u8868\u793a\u7684DRL\u6846\u67b6\u5982\u4f55\u5728\u8be5\u9886\u57df\u5b9e\u73b0\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2506.19960", "pdf": "https://arxiv.org/pdf/2506.19960", "abs": "https://arxiv.org/abs/2506.19960", "authors": ["Adam Foster", "Zeno Sch\u00e4tzle", "P. Bern\u00e1t Szab\u00f3", "Lixue Cheng", "Jonas K\u00f6hler", "Gino Cassella", "Nicholas Gao", "Jiawei Li", "Frank No\u00e9", "Jan Hermann"], "title": "An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking", "categories": ["physics.chem-ph", "cs.AI", "stat.ML"], "comment": null, "summary": "Reliable description of bond breaking remains a major challenge for quantum\nchemistry due to the multireferential character of the electronic structure in\ndissociating species. Multireferential methods in particular suffer from large\ncomputational cost, which under the normal paradigm has to be paid anew for\neach system at a full price, ignoring commonalities in electronic structure\nacross molecules. Quantum Monte Carlo with deep neural networks (deep QMC)\nuniquely offers to exploit such commonalities by pretraining transferable\nwavefunction models, but all such attempts were so far limited in scope. Here,\nwe bring this new paradigm to fruition with Orbformer, a novel transferable\nwavefunction model pretrained on 22,000 equilibrium and dissociating structures\nthat can be fine-tuned on unseen molecules reaching an accuracy-cost ratio\nrivalling classical multireferential methods. On established benchmarks as well\nas more challenging bond dissociations and Diels-Alder reactions, Orbformer is\nthe only method that consistently converges to chemical accuracy (1 kcal/mol).\nThis work turns the idea of amortizing the cost of solving the Schr\\\"odinger\nequation over many molecules into a practical approach in quantum chemistry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOrbformer\u7684\u65b0\u578b\u53ef\u8fc1\u79fb\u6ce2\u51fd\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u572822,000\u4e2a\u5e73\u8861\u548c\u89e3\u79bb\u7ed3\u6784\u4e0a\uff0c\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u63cf\u8ff0\u5316\u5b66\u952e\u65ad\u88c2\u95ee\u9898\uff0c\u5176\u7cbe\u5ea6\u4e0e\u6210\u672c\u6bd4\u5ab2\u7f8e\u4f20\u7edf\u591a\u53c2\u8003\u65b9\u6cd5\u3002", "motivation": "\u5316\u5b66\u952e\u65ad\u88c2\u7684\u53ef\u9760\u63cf\u8ff0\u662f\u91cf\u5b50\u5316\u5b66\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u4f20\u7edf\u591a\u53c2\u8003\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u65e0\u6cd5\u5229\u7528\u5206\u5b50\u95f4\u7535\u5b50\u7ed3\u6784\u7684\u5171\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u53ef\u8fc1\u79fb\u6ce2\u51fd\u6570\u6a21\u578b\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faOrbformer\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u91cf\u5b50\u8499\u7279\u5361\u6d1b\uff08deep QMC\uff09\u7684\u65b0\u578b\u6ce2\u51fd\u6570\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u4e8e22,000\u4e2a\u5e73\u8861\u548c\u89e3\u79bb\u7ed3\u6784\uff0c\u5e76\u53ef\u9488\u5bf9\u672a\u89c1\u5206\u5b50\u8fdb\u884c\u5fae\u8c03\u3002", "result": "Orbformer\u5728\u6807\u51c6\u6d4b\u8bd5\u548c\u66f4\u5177\u6311\u6218\u6027\u7684\u952e\u89e3\u79bb\u53caDiels-Alder\u53cd\u5e94\u4e2d\uff0c\u552f\u4e00\u80fd\u591f\u4e00\u81f4\u6536\u655b\u81f3\u5316\u5b66\u7cbe\u5ea6\uff081 kcal/mol\uff09\u3002", "conclusion": "Orbformer\u5b9e\u73b0\u4e86\u5c06\u6c42\u89e3\u859b\u5b9a\u8c14\u65b9\u7a0b\u7684\u6210\u672c\u5206\u644a\u5230\u591a\u4e2a\u5206\u5b50\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u91cf\u5b50\u5316\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u3002", "paper_title_zh": "\u4e00\u79cd\u57fa\u4e8e\u4ece\u5934\u8ba1\u7b97\u7684\u6ce2\u51fd\u6570\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u51c6\u786e\u63cf\u8ff0\u5316\u5b66\u952e\u65ad\u88c2", "abstract_zh": "\u5316\u5b66\u952e\u65ad\u88c2\u7684\u53ef\u9760\u63cf\u8ff0\u4ecd\u7136\u662f\u91cf\u5b50\u5316\u5b66\u7684\u4e3b\u8981\u6311\u6218\uff0c\u8fd9\u662f\u7531\u4e8e\u89e3\u79bb\u7269\u79cd\u7535\u5b50\u7ed3\u6784\u7684\u591a\u53c2\u8003\u7279\u6027\u3002\u591a\u53c2\u8003\u65b9\u6cd5\u5c24\u5176\u9762\u4e34\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u800c\u4f20\u7edf\u8303\u5f0f\u4e0b\uff0c\u8fd9\u79cd\u6210\u672c\u9700\u8981\u4e3a\u6bcf\u4e2a\u7cfb\u7edf\u5355\u72ec\u652f\u4ed8\uff0c\u5ffd\u7565\u4e86\u5206\u5b50\u95f4\u7535\u5b50\u7ed3\u6784\u7684\u5171\u6027\u3002\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u91cf\u5b50\u8499\u7279\u5361\u6d1b\uff08deep QMC\uff09\u901a\u8fc7\u9884\u8bad\u7ec3\u53ef\u8fc1\u79fb\u6ce2\u51fd\u6570\u6a21\u578b\uff0c\u72ec\u7279\u5730\u5229\u7528\u4e86\u8fd9\u79cd\u5171\u6027\uff0c\u4f46\u6b64\u524d\u6240\u6709\u5c1d\u8bd5\u5747\u53d7\u9650\u4e8e\u8303\u56f4\u3002\u672c\u6587\u901a\u8fc7Orbformer\u5b9e\u73b0\u4e86\u8fd9\u4e00\u65b0\u8303\u5f0f\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u53ef\u8fc1\u79fb\u6ce2\u51fd\u6570\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u4e8e22,000\u4e2a\u5e73\u8861\u548c\u89e3\u79bb\u7ed3\u6784\uff0c\u53ef\u9488\u5bf9\u672a\u89c1\u5206\u5b50\u8fdb\u884c\u5fae\u8c03\uff0c\u5176\u7cbe\u5ea6\u4e0e\u6210\u672c\u6bd4\u5ab2\u7f8e\u7ecf\u5178\u591a\u53c2\u8003\u65b9\u6cd5\u3002\u5728\u6807\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u66f4\u5177\u6311\u6218\u6027\u7684\u952e\u89e3\u79bb\u548cDiels-Alder\u53cd\u5e94\u4e2d\uff0cOrbformer\u662f\u552f\u4e00\u80fd\u591f\u4e00\u81f4\u6536\u655b\u81f3\u5316\u5b66\u7cbe\u5ea6\uff081 kcal/mol\uff09\u7684\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u5c06\u5206\u644a\u6c42\u89e3\u859b\u5b9a\u8c14\u65b9\u7a0b\u6210\u672c\u7684\u7406\u5ff5\u8f6c\u5316\u4e3a\u91cf\u5b50\u5316\u5b66\u4e2d\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2506.20671", "pdf": "https://arxiv.org/pdf/2506.20671", "abs": "https://arxiv.org/abs/2506.20671", "authors": ["Markus Gross", "Aya Fahmy", "Danit Niwattananan", "Dominik Muhle", "Rui Song", "Daniel Cremers", "Henri Mee\u00df"], "title": "IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly\nlearning scene geometry and semantics, enabling downstream applications such as\nnavigation in mobile robotics. The recent generalization to Panoptic Scene\nCompletion (PSC) advances the SSC domain by integrating instance-level\ninformation, thereby enhancing object-level sensitivity in scene understanding.\nWhile PSC was introduced using LiDAR modality, methods based on camera images\nremain largely unexplored. Moreover, recent Transformer-based SSC approaches\nutilize a fixed set of learned queries to reconstruct objects within the scene\nvolume. Although these queries are typically updated with image context during\ntraining, they remain static at test time, limiting their ability to\ndynamically adapt specifically to the observed scene. To overcome these\nlimitations, we propose IPFormer, the first approach that leverages\ncontext-adaptive instance proposals at train and test time to address\nvision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively\ninitializes these queries as panoptic instance proposals derived from image\ncontext and further refines them through attention-based encoding and decoding\nto reason about semantic instance-voxel relationships. Experimental results\nshow that our approach surpasses state-of-the-art methods in overall panoptic\nmetrics PQ$^\\dagger$ and PQ-All, matches performance in individual metrics, and\nachieves a runtime reduction exceeding 14$\\times$. Furthermore, our ablation\nstudies reveal that dynamically deriving instance proposals from image context,\nas opposed to random initialization, leads to a 3.62% increase in PQ-All and a\nremarkable average improvement of 18.65% in combined Thing-metrics. These\nresults highlight our introduction of context-adaptive instance proposals as a\npioneering effort in addressing vision-based 3D Panoptic Scene Completion.", "AI": {"tldr": "IPFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u76843D\u5168\u666f\u573a\u666f\u8865\u5168\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b9e\u4f8b\u63d0\u6848\u52a8\u6001\u9002\u5e94\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u666f\u6307\u6807\u548c\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u666f\u573a\u666f\u8865\u5168\uff08PSC\uff09\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eLiDAR\uff0c\u800c\u57fa\u4e8e\u76f8\u673a\u56fe\u50cf\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u4f20\u7edf\u7684Transformer\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u67e5\u8be2\u96c6\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u573a\u666f\u3002IPFormer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b9e\u4f8b\u63d0\u6848\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "IPFormer\u901a\u8fc7\u56fe\u50cf\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u521d\u59cb\u5316\u67e5\u8be2\u4f5c\u4e3a\u5168\u666f\u5b9e\u4f8b\u63d0\u6848\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u7f16\u7801\u548c\u89e3\u7801\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u63d0\u6848\uff0c\u4ee5\u63a8\u7406\u8bed\u4e49\u5b9e\u4f8b\u4e0e\u4f53\u7d20\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIPFormer\u5728\u5168\u666f\u6307\u6807PQ$^\\dagger$\u548cPQ-All\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc714\u500d\u3002\u52a8\u6001\u521d\u59cb\u5316\u63d0\u6848\u4f7fPQ-All\u63d0\u53473.62%\uff0cThing\u6307\u6807\u5e73\u5747\u63d0\u534718.65%\u3002", "conclusion": "IPFormer\u901a\u8fc7\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b9e\u4f8b\u63d0\u6848\uff0c\u9996\u6b21\u89e3\u51b3\u4e86\u57fa\u4e8e\u89c6\u89c9\u76843D\u5168\u666f\u573a\u666f\u8865\u5168\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "paper_title_zh": "IPFormer\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b9e\u4f8b\u63d0\u6848\u7684\u89c6\u89c93D\u5168\u666f\u573a\u666f\u8865\u5168", "abstract_zh": "\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u5df2\u6210\u4e3a\u8054\u5408\u5b66\u4e60\u573a\u666f\u51e0\u4f55\u548c\u8bed\u4e49\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u652f\u6301\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u7b49\u4e0b\u6e38\u5e94\u7528\u3002\u5168\u666f\u573a\u666f\u8865\u5168\uff08PSC\uff09\u901a\u8fc7\u6574\u5408\u5b9e\u4f8b\u7ea7\u4fe1\u606f\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86SSC\u9886\u57df\uff0c\u63d0\u5347\u4e86\u573a\u666f\u7406\u89e3\u7684\u5bf9\u8c61\u7ea7\u654f\u611f\u6027\u3002\u5c3d\u7ba1PSC\u6700\u521d\u57fa\u4e8eLiDAR\u6a21\u6001\uff0c\u4f46\u57fa\u4e8e\u76f8\u673a\u56fe\u50cf\u7684\u65b9\u6cd5\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684SSC\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u5b66\u4e60\u67e5\u8be2\u96c6\u91cd\u5efa\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\uff0c\u8fd9\u4e9b\u67e5\u8be2\u5728\u8bad\u7ec3\u65f6\u867d\u4f1a\u66f4\u65b0\uff0c\u4f46\u5728\u6d4b\u8bd5\u65f6\u4fdd\u6301\u9759\u6001\uff0c\u9650\u5236\u4e86\u5176\u52a8\u6001\u9002\u5e94\u573a\u666f\u7684\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86IPFormer\uff0c\u9996\u6b21\u5229\u7528\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b9e\u4f8b\u63d0\u6848\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u5904\u7406\u57fa\u4e8e\u89c6\u89c9\u76843D\u5168\u666f\u573a\u666f\u8865\u5168\u3002\u5177\u4f53\u800c\u8a00\uff0cIPFormer\u81ea\u9002\u5e94\u5730\u5c06\u8fd9\u4e9b\u67e5\u8be2\u521d\u59cb\u5316\u4e3a\u4ece\u56fe\u50cf\u4e0a\u4e0b\u6587\u63d0\u53d6\u7684\u5168\u666f\u5b9e\u4f8b\u63d0\u6848\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u7f16\u7801\u548c\u89e3\u7801\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u4ee5\u63a8\u7406\u8bed\u4e49\u5b9e\u4f8b\u4e0e\u4f53\u7d20\u7684\u5173\u7cfb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5168\u666f\u6307\u6807PQ$^\\dagger$\u548cPQ-All\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u4e8614\u500d\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u52a8\u6001\u4ece\u56fe\u50cf\u4e0a\u4e0b\u6587\u63d0\u53d6\u5b9e\u4f8b\u63d0\u6848\uff08\u800c\u975e\u968f\u673a\u521d\u59cb\u5316\uff09\u4f7fPQ-All\u63d0\u5347\u4e863.62%\uff0cThing\u6307\u6807\u5e73\u5747\u63d0\u5347\u4e8618.65%\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u6211\u4eec\u5728\u57fa\u4e8e\u89c6\u89c9\u76843D\u5168\u666f\u573a\u666f\u8865\u5168\u4e2d\u5f15\u5165\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b9e\u4f8b\u63d0\u6848\u7684\u5f00\u521b\u6027\u8d21\u732e\u3002"}}
{"id": "2506.19860", "pdf": "https://arxiv.org/pdf/2506.19860", "abs": "https://arxiv.org/abs/2506.19860", "authors": ["Oktay Karaku\u015f", "Padraig Corcoran"], "title": "A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing", "categories": ["eess.SP", "cs.CV"], "comment": "11 pages, 4 figures, 2 tables", "summary": "Electric vehicle (EV) charging infrastructure is increasingly critical to\nsustainable transport systems, yet its resilience under environmental and\ninfrastructural stress remains underexplored. In this paper, we introduce\nRSERI-EV, a spatially explicit and multi-modal risk assessment framework that\ncombines remote sensing data, open infrastructure datasets, and spatial graph\nanalytics to evaluate the vulnerability of EV charging stations. RSERI-EV\nintegrates diverse data layers, including flood risk maps, land surface\ntemperature (LST) extremes, vegetation indices (NDVI), land use/land cover\n(LULC), proximity to electrical substations, and road accessibility to generate\na composite Resilience Score. We apply this framework to the country of Wales\nEV charger dataset to demonstrate its feasibility. A spatial $k$-nearest\nneighbours ($k$NN) graph is constructed over the charging network to enable\nneighbourhood-based comparisons and graph-aware diagnostics. Our prototype\nhighlights the value of multi-source data fusion and interpretable spatial\nreasoning in supporting climate-resilient, infrastructure-aware EV deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7a7a\u95f4\u98ce\u9669\u6846\u67b6RSERI-EV\uff0c\u7ed3\u5408\u9065\u611f\u6570\u636e\u548c\u7a7a\u95f4\u56fe\u5206\u6790\uff0c\u8bc4\u4f30\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u5728\u73af\u5883\u548c\u57fa\u7840\u8bbe\u65bd\u538b\u529b\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u4ee5\u5a01\u5c14\u58eb\u5145\u7535\u7ad9\u6570\u636e\u96c6\u4e3a\u4f8b\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u5bf9\u53ef\u6301\u7eed\u4ea4\u901a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u73af\u5883\u548c\u57fa\u7840\u8bbe\u65bd\u538b\u529b\u4e0b\u7684\u97e7\u6027\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u8bc4\u4f30\u5145\u7535\u7ad9\u7684\u8106\u5f31\u6027\u3002", "method": "RSERI-EV\u6846\u67b6\u6574\u5408\u4e86\u6d2a\u6c34\u98ce\u9669\u56fe\u3001\u5730\u8868\u6e29\u5ea6\u6781\u503c\u3001\u690d\u88ab\u6307\u6570\u3001\u571f\u5730\u5229\u7528/\u8986\u76d6\u3001\u53d8\u7535\u7ad9\u90bb\u8fd1\u6027\u548c\u9053\u8def\u53ef\u8fbe\u6027\u7b49\u591a\u6e90\u6570\u636e\uff0c\u751f\u6210\u7efc\u5408\u97e7\u6027\u8bc4\u5206\u3002\u901a\u8fc7\u6784\u5efa\u7a7a\u95f4k\u8fd1\u90bb\u56fe\uff0c\u5b9e\u73b0\u57fa\u4e8e\u90bb\u57df\u7684\u5bf9\u6bd4\u548c\u56fe\u611f\u77e5\u8bca\u65ad\u3002", "result": "\u5728\u5a01\u5c14\u58eb\u5145\u7535\u7ad9\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5145\u7535\u7ad9\u7684\u8106\u5f31\u6027\uff0c\u5e76\u652f\u6301\u6c14\u5019\u97e7\u6027\u548c\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u7684\u7535\u52a8\u6c7d\u8f66\u90e8\u7f72\u3002", "conclusion": "\u591a\u6e90\u6570\u636e\u878d\u5408\u548c\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u4e3a\u6c14\u5019\u97e7\u6027\u7684\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "paper_title_zh": "\u57fa\u4e8e\u9065\u611f\u7684\u591a\u6a21\u6001\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u7a7a\u95f4\u98ce\u9669\u6846\u67b6", "abstract_zh": "\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u5bf9\u53ef\u6301\u7eed\u4ea4\u901a\u7cfb\u7edf\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u5728\u73af\u5883\u548c\u57fa\u7840\u8bbe\u65bd\u538b\u529b\u4e0b\u7684\u97e7\u6027\u7814\u7a76\u4ecd\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86RSERI-EV\uff0c\u4e00\u79cd\u7a7a\u95f4\u663e\u5f0f\u591a\u6a21\u6001\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u9065\u611f\u6570\u636e\u3001\u5f00\u653e\u57fa\u7840\u8bbe\u65bd\u6570\u636e\u96c6\u548c\u7a7a\u95f4\u56fe\u5206\u6790\uff0c\u8bc4\u4f30\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u7684\u8106\u5f31\u6027\u3002RSERI-EV\u6574\u5408\u4e86\u6d2a\u6c34\u98ce\u9669\u56fe\u3001\u5730\u8868\u6e29\u5ea6\u6781\u503c\u3001\u690d\u88ab\u6307\u6570\uff08NDVI\uff09\u3001\u571f\u5730\u5229\u7528/\u8986\u76d6\uff08LULC\uff09\u3001\u53d8\u7535\u7ad9\u90bb\u8fd1\u6027\u548c\u9053\u8def\u53ef\u8fbe\u6027\u7b49\u591a\u6e90\u6570\u636e\uff0c\u751f\u6210\u7efc\u5408\u97e7\u6027\u8bc4\u5206\u3002\u6211\u4eec\u4ee5\u5a01\u5c14\u58eb\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u6570\u636e\u96c6\u4e3a\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002\u901a\u8fc7\u6784\u5efa\u5145\u7535\u7f51\u7edc\u7684\u7a7a\u95f4k\u8fd1\u90bb\u56fe\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u90bb\u57df\u7684\u5bf9\u6bd4\u548c\u56fe\u611f\u77e5\u8bca\u65ad\u3002\u539f\u578b\u7814\u7a76\u8868\u660e\uff0c\u591a\u6e90\u6570\u636e\u878d\u5408\u548c\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u5bf9\u652f\u6301\u6c14\u5019\u97e7\u6027\u548c\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u7684\u7535\u52a8\u6c7d\u8f66\u90e8\u7f72\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.19973", "pdf": "https://arxiv.org/pdf/2506.19973", "abs": "https://arxiv.org/abs/2506.19973", "authors": ["Vojt\u011bch Nov\u00e1k", "Ivan Zelinka", "Lenka P\u0159ibylov\u00e1", "Lubom\u00edr Mart\u00ednek"], "title": "Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies", "categories": ["quant-ph", "cs.AI", "stat.ML", "62H30, 62P10, 68T05, 81P68", "I.2.6; J.3; I.5.4; F.4.1"], "comment": null, "summary": "This study investigates the application of quantum neural networks (QNNs) for\npropensity score estimation to address selection bias in comparing survival\noutcomes between laparoscopic and open surgical techniques in a cohort of 1177\ncolorectal carcinoma patients treated at University Hospital Ostrava\n(2001-2009). Using a dataset with 77 variables, including patient demographics\nand tumor characteristics, we developed QNN-based propensity score models\nfocusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture\nemployed a linear ZFeatureMap for data encoding, a SummedPaulis operator for\npredictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\nfor robust, gradient-free optimization in noisy quantum environments. Variance\nregularization was integrated to mitigate quantum measurement noise, with\nsimulations conducted under exact, sampling (1024 shots), and noisy hardware\n(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,\noutperformed classical logistic regression and gradient boosted machines in\nsmall samples (AUC up to 0.750 for n=100), with noise modeling enhancing\npredictive stability. Propensity score matching and weighting, optimized via\ngenetic matching and matching weights, achieved covariate balance with\nstandardized mean differences of 0.0849 and 0.0869, respectively. Survival\nanalyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen\nadditive regression revealed no significant survival differences\npost-adjustment (p-values 0.287-0.851), indicating confounding bias in\nunadjusted outcomes. These results highlight QNNs' potential, enhanced by\nCMA-ES and noise-aware strategies, to improve causal inference in biomedical\nresearch, particularly for small-sample, high-dimensional datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u5728\u503e\u5411\u8bc4\u5206\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b31177\u540d\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u4e2d\u8179\u8154\u955c\u4e0e\u5f00\u653e\u624b\u672f\u6280\u672f\u751f\u5b58\u7ed3\u679c\u6bd4\u8f83\u7684\u9009\u62e9\u504f\u501a\u95ee\u9898\u3002QNN\u5728\u6a21\u62df\u786c\u4ef6\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u9057\u4f20\u5339\u914d\u548c\u6743\u91cd\u4f18\u5316\u5b9e\u73b0\u534f\u53d8\u91cf\u5e73\u8861\uff0c\u6700\u7ec8\u672a\u53d1\u73b0\u8c03\u6574\u540e\u7684\u751f\u5b58\u5dee\u5f02\u3002", "motivation": "\u5728\u751f\u7269\u533b\u5b66\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\uff0c\u9009\u62e9\u504f\u501a\u662f\u5f71\u54cd\u751f\u5b58\u5206\u6790\u7ed3\u679c\u7684\u4e3b\u8981\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u6539\u8fdb\u503e\u5411\u8bc4\u5206\u4f30\u8ba1\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6bd4\u8f83\u8179\u8154\u955c\u4e0e\u5f00\u653e\u624b\u672f\u6280\u672f\u7684\u751f\u5b58\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5305\u542b77\u4e2a\u53d8\u91cf\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eQNN\u7684\u503e\u5411\u8bc4\u5206\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u5e74\u9f84\u3001\u6027\u522b\u3001\u5206\u671f\u548cBMI\u56db\u4e2a\u5173\u952e\u534f\u53d8\u91cf\u3002QNN\u67b6\u6784\u91c7\u7528\u7ebf\u6027ZFeatureMap\u8fdb\u884c\u6570\u636e\u7f16\u7801\uff0cSummedPaulis\u7b97\u5b50\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528CMA-ES\u8fdb\u884c\u68af\u5ea6\u4f18\u5316\u3002\u901a\u8fc7\u65b9\u5dee\u6b63\u5219\u5316\u51cf\u5c11\u91cf\u5b50\u6d4b\u91cf\u566a\u58f0\uff0c\u5e76\u5728\u7cbe\u786e\u3001\u91c7\u6837\u548c\u6a21\u62df\u566a\u58f0\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u6a21\u62df\u786c\u4ef6\u566a\u58f0\u6761\u4ef6\u4e0b\uff0cQNN\u5728\u5c0f\u6837\u672c\uff08n=100\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u903b\u8f91\u56de\u5f52\u548c\u68af\u5ea6\u63d0\u5347\u673a\uff08AUC\u9ad8\u8fbe0.750\uff09\u3002\u503e\u5411\u8bc4\u5206\u5339\u914d\u548c\u52a0\u6743\u4f18\u5316\u540e\uff0c\u534f\u53d8\u91cf\u5e73\u8861\u6807\u51c6\u5316\u5747\u5dee\u5206\u522b\u4e3a0.0849\u548c0.0869\u3002\u8c03\u6574\u540e\u7684\u751f\u5b58\u5206\u6790\u672a\u53d1\u73b0\u663e\u8457\u5dee\u5f02\uff08p\u503c0.287-0.851\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cQNN\u7ed3\u5408CMA-\u4f18\u5316\u548c\u566a\u58f0\u611f\u77e5\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u6539\u8fdb\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u9ad8\u7ef4\u6570\u636e\u96c6\u3002", "paper_title_zh": "\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5728\u89c2\u5bdf\u6027\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u503e\u5411\u8bc4\u5206\u4f30\u8ba1\u4e0e\u751f\u5b58\u5206\u6790\u5e94\u7528", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u5728\u503e\u5411\u8bc4\u5206\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b31177\u540d\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u4e2d\u8179\u8154\u955c\u4e0e\u5f00\u653e\u624b\u672f\u6280\u672f\u751f\u5b58\u7ed3\u679c\u6bd4\u8f83\u7684\u9009\u62e9\u504f\u501a\u95ee\u9898\u3002\u4f7f\u7528\u5305\u542b77\u4e2a\u53d8\u91cf\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u80bf\u7624\u7279\u5f81\uff09\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u57fa\u4e8eQNN\u7684\u503e\u5411\u8bc4\u5206\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u5e74\u9f84\u3001\u6027\u522b\u3001\u5206\u671f\u548cBMI\u56db\u4e2a\u5173\u952e\u534f\u53d8\u91cf\u3002QNN\u67b6\u6784\u91c7\u7528\u7ebf\u6027ZFeatureMap\u8fdb\u884c\u6570\u636e\u7f16\u7801\uff0cSummedPaulis\u7b97\u5b50\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528CMA-ES\u5728\u566a\u58f0\u91cf\u5b50\u73af\u5883\u4e2d\u8fdb\u884c\u9c81\u68d2\u7684\u68af\u5ea6\u4f18\u5316\u3002\u901a\u8fc7\u65b9\u5dee\u6b63\u5219\u5316\u51cf\u5c11\u91cf\u5b50\u6d4b\u91cf\u566a\u58f0\uff0c\u5e76\u5728\u7cbe\u786e\u3001\u91c7\u6837\uff081024\u6b21\uff09\u548c\u6a21\u62df\u566a\u58f0\u786c\u4ef6\uff08FakeManhattanV2\uff09\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002QNN\u5728\u6a21\u62df\u786c\u4ef6\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u903b\u8f91\u56de\u5f52\u548c\u68af\u5ea6\u63d0\u5347\u673a\uff08\u5c0f\u6837\u672cAUC\u9ad8\u8fbe0.750\uff09\uff0c\u566a\u58f0\u5efa\u6a21\u589e\u5f3a\u4e86\u9884\u6d4b\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u9057\u4f20\u5339\u914d\u548c\u5339\u914d\u6743\u91cd\u4f18\u5316\u7684\u503e\u5411\u8bc4\u5206\u5339\u914d\u548c\u52a0\u6743\u5b9e\u73b0\u4e86\u534f\u53d8\u91cf\u5e73\u8861\uff08\u6807\u51c6\u5316\u5747\u5dee\u5206\u522b\u4e3a0.0849\u548c0.0869\uff09\u3002\u4f7f\u7528Kaplan-Meier\u4f30\u8ba1\u3001Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u548cAalen\u52a0\u6027\u56de\u5f52\u8fdb\u884c\u7684\u751f\u5b58\u5206\u6790\u663e\u793a\u8c03\u6574\u540e\u65e0\u663e\u8457\u751f\u5b58\u5dee\u5f02\uff08p\u503c0.287-0.851\uff09\uff0c\u8868\u660e\u672a\u8c03\u6574\u7ed3\u679c\u4e2d\u5b58\u5728\u6df7\u6742\u504f\u501a\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86QNN\u7ed3\u5408CMA-ES\u548c\u566a\u58f0\u611f\u77e5\u7b56\u7565\u5728\u6539\u8fdb\u751f\u7269\u533b\u5b66\u7814\u7a76\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u9ad8\u7ef4\u6570\u636e\u96c6\u3002"}}
{"id": "2506.19935", "pdf": "https://arxiv.org/pdf/2506.19935", "abs": "https://arxiv.org/abs/2506.19935", "authors": ["Shuchen Xue", "Tianyu Xie", "Tianyang Hu", "Zijin Feng", "Jiacheng Sun", "Kenji Kawaguchi", "Zhenguo Li", "Zhi-Ming Ma"], "title": "Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) predominantly use autoregressive (AR)\napproaches, but masked diffusion models (MDMs) are emerging as viable\nalternatives. A key challenge in comparing AR and MDM paradigms is their\ntypical architectural difference: AR models are often decoder-only, while MDMs\nhave largely been encoder-only. This practice of changing both the modeling\nparadigm and architecture simultaneously makes direct comparisons unfair, as\nit's hard to distinguish whether observed differences stem from the paradigm\nitself or the architectural shift. This research evaluates MDMs within a\ndecoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or\nAO-AR) and standard AR paradigms. Our investigation suggests that the standard\nAO-AR objective, which averages over all token permutations, may benefit from\nrefinement, as many permutations appear less informative compared to the\nlanguage's inherent left-to-right structure. (2) Investigate architectural\ninfluences (decoder-only vs. encoder-only) within MDMs. We demonstrate that\nwhile encoder-only MDMs model a simpler conditional probability space,\ndecoder-only MDMs can achieve dramatic generation speedups ($\\sim25\\times$) and\ncomparable perplexity with temperature annealing despite modeling a vastly\nlarger space, highlighting key trade-offs. This work thus decouples core\nparadigm differences from architectural influences, offering insights for\nfuture model design. Code is available at https://github.com/scxue/AO-GPT-MDM.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c06\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u5f15\u5165\u4ec5\u89e3\u7801\u5668\u67b6\u6784\uff0c\u516c\u5e73\u6bd4\u8f83\u4e86\u81ea\u56de\u5f52\uff08AR\uff09\u4e0eMDM\u8303\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u67b6\u6784\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u89e3\u7801\u5668MDM\u5728\u751f\u6210\u901f\u5ea6\u548c\u56f0\u60d1\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8303\u5f0f\u4e0e\u67b6\u6784\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3b\u8981\u91c7\u7528\u81ea\u56de\u5f52\uff08AR\uff09\u65b9\u6cd5\uff0c\u800c\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u4f5c\u4e3a\u65b0\u5174\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u4e0eAR\u7684\u6bd4\u8f83\u5e38\u56e0\u67b6\u6784\u5dee\u5f02\uff08\u89e3\u7801\u5668vs\u7f16\u7801\u5668\uff09\u800c\u96be\u4ee5\u516c\u5e73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\uff08\u4ec5\u89e3\u7801\u5668\uff09\u76f4\u63a5\u6bd4\u8f83AR\u4e0eMDM\u8303\u5f0f\uff0c\u5e76\u5206\u6790\u67b6\u6784\u5bf9MDM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5c06MDM\u5e94\u7528\u4e8e\u4ec5\u89e3\u7801\u5668\u67b6\u6784\uff0c\u63d0\u51fa\u201c\u4efb\u610f\u987a\u5e8f\u81ea\u56de\u5f52\u201d\uff08AO-AR\uff09\u76ee\u6807\uff0c\u901a\u8fc7\u5e73\u5747\u6240\u6709\u6807\u8bb0\u6392\u5217\u4f18\u5316\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5bf9\u6bd4\u4e86\u4ec5\u7f16\u7801\u5668\u548c\u4ec5\u89e3\u7801\u5668MDM\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u91cd\u70b9\u5173\u6ce8\u751f\u6210\u901f\u5ea6\u548c\u56f0\u60d1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u89e3\u7801\u5668MDM\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\uff08\u7ea625\u500d\uff09\uff0c\u4e14\u901a\u8fc7\u6e29\u5ea6\u9000\u706b\u8fbe\u5230\u4e0e\u4ec5\u7f16\u7801\u5668MDM\u76f8\u5f53\u7684\u56f0\u60d1\u5ea6\uff0c\u5c3d\u7ba1\u5efa\u6a21\u7a7a\u95f4\u66f4\u5927\u3002\u6b64\u5916\uff0c\u6807\u51c6AO-AR\u76ee\u6807\u56e0\u90e8\u5206\u6392\u5217\u4fe1\u606f\u91cf\u4f4e\u800c\u9700\u4f18\u5316\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u89e3\u8026\u4e86\u8303\u5f0f\u4e0e\u67b6\u6784\u7684\u5f71\u54cd\uff0c\u8868\u660e\u4ec5\u89e3\u7801\u5668MDM\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u4efb\u610f\u987a\u5e8fGPT\u4f5c\u4e3a\u63a9\u7801\u6269\u6563\u6a21\u578b\uff1a\u89e3\u8026\u5efa\u6a21\u8303\u5f0f\u4e0e\u67b6\u6784", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3b\u8981\u91c7\u7528\u81ea\u56de\u5f52\uff08AR\uff09\u65b9\u6cd5\uff0c\u4f46\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u6b63\u6210\u4e3a\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6bd4\u8f83AR\u4e0eMDM\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u5176\u5178\u578b\u67b6\u6784\u5dee\u5f02\uff1aAR\u6a21\u578b\u591a\u4e3a\u4ec5\u89e3\u7801\u5668\uff0c\u800cMDM\u591a\u4e3a\u4ec5\u7f16\u7801\u5668\u3002\u8fd9\u79cd\u540c\u65f6\u6539\u53d8\u5efa\u6a21\u8303\u5f0f\u4e0e\u67b6\u6784\u7684\u505a\u6cd5\u4f7f\u5f97\u76f4\u63a5\u6bd4\u8f83\u4e0d\u516c\u5e73\uff0c\u96be\u4ee5\u533a\u5206\u5dee\u5f02\u6e90\u4e8e\u8303\u5f0f\u8fd8\u662f\u67b6\u6784\u3002\u672c\u7814\u7a76\u5728\u4ec5\u89e3\u7801\u5668\u6846\u67b6\u4e0b\u8bc4\u4f30MDM\uff0c\u4ee5\uff1a\uff081\uff09\u516c\u5e73\u6bd4\u8f83MDM\uff08\u4f5c\u4e3a\u4efb\u610f\u987a\u5e8fAR\uff0cAO-AR\uff09\u4e0e\u6807\u51c6AR\u8303\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6807\u51c6AO-AR\u76ee\u6807\uff08\u5e73\u5747\u6240\u6709\u6807\u8bb0\u6392\u5217\uff09\u53ef\u80fd\u9700\u4f18\u5316\uff0c\u56e0\u8bb8\u591a\u6392\u5217\u4fe1\u606f\u91cf\u8f83\u4f4e\u3002\uff082\uff09\u63a2\u8ba8MDM\u4e2d\u67b6\u6784\uff08\u4ec5\u89e3\u7801\u5668vs\u4ec5\u7f16\u7801\u5668\uff09\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u7f16\u7801\u5668MDM\u5efa\u6a21\u66f4\u7b80\u5355\u7684\u6761\u4ef6\u6982\u7387\u7a7a\u95f4\uff0c\u800c\u4ec5\u89e3\u7801\u5668MDM\u5c3d\u7ba1\u5efa\u6a21\u7a7a\u95f4\u66f4\u5927\uff0c\u5374\u53ef\u5b9e\u73b0\u663e\u8457\u751f\u6210\u52a0\u901f\uff08\u7ea625\u500d\uff09\u548c\u901a\u8fc7\u6e29\u5ea6\u9000\u706b\u8fbe\u5230\u76f8\u8fd1\u56f0\u60d1\u5ea6\uff0c\u51f8\u663e\u4e86\u5173\u952e\u6743\u8861\u3002\u672c\u7814\u7a76\u89e3\u8026\u4e86\u6838\u5fc3\u8303\u5f0f\u4e0e\u67b6\u6784\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6d1e\u89c1\u3002\u4ee3\u7801\u89c1https://github.com/scxue/AO-GPT-MDM\u3002"}}
{"id": "2506.19975", "pdf": "https://arxiv.org/pdf/2506.19975", "abs": "https://arxiv.org/abs/2506.19975", "authors": ["Hang Zhang", "Yuxi Zhang", "Jiazheng Wang", "Xiang Chen", "Renjiu Hu", "Xin Tian", "Gaolei Li", "Min Liu"], "title": "VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP"], "comment": "Accepted for publication at MICCAI 2025", "summary": "Recent developments in neural networks have improved deformable image\nregistration (DIR) by amortizing iterative optimization, enabling fast and\naccurate DIR results. However, learning-based methods often face challenges\nwith limited training data, large deformations, and tend to underperform\ncompared to iterative approaches when label supervision is unavailable. While\niterative methods can achieve higher accuracy in such scenarios, they are\nconsiderably slower than learning-based methods. To address these limitations,\nwe propose VoxelOpt, a discrete optimization-based DIR framework that combines\nthe strengths of learning-based and iterative methods to achieve a better\nbalance between registration accuracy and runtime. VoxelOpt uses displacement\nentropy from local cost volumes to measure displacement signal strength at each\nvoxel, which differs from earlier approaches in three key aspects. First, it\nintroduces voxel-wise adaptive message passing, where voxels with lower entropy\nreceives less influence from their neighbors. Second, it employs a multi-level\nimage pyramid with 27-neighbor cost volumes at each level, avoiding exponential\ncomplexity growth. Third, it replaces hand-crafted features or contrastive\nlearning with a pretrained foundational segmentation model for feature\nextraction. In abdominal CT registration, these changes allow VoxelOpt to\noutperform leading iterative in both efficiency and accuracy, while matching\nstate-of-the-art learning-based methods trained with label supervision. The\nsource code will be available at https://github.com/tinymilky/VoxelOpt", "AI": {"tldr": "VoxelOpt\u662f\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u4e0e\u8fed\u4ee3\u65b9\u6cd5\u7684\u53d8\u5f62\u8179\u90e8CT\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4f53\u7d20\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\u548c\u591a\u7ea7\u56fe\u50cf\u91d1\u5b57\u5854\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5e76\u5ab2\u7f8e\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u6216\u5927\u53d8\u5f62\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fed\u4ee3\u65b9\u6cd5\u867d\u7cbe\u5ea6\u9ad8\u4f46\u901f\u5ea6\u6162\u3002VoxelOpt\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "method": "VoxelOpt\u5229\u7528\u5c40\u90e8\u6210\u672c\u4f53\u79ef\u7684\u4f4d\u79fb\u71b5\u8861\u91cf\u4f53\u7d20\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u5f15\u5165\u4f53\u7d20\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\u548c\u591a\u7ea7\u56fe\u50cf\u91d1\u5b57\u5854\uff0c\u5e76\u91c7\u7528\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u907f\u514d\u624b\u5de5\u8bbe\u8ba1\u6216\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u8179\u90e8CT\u914d\u51c6\u4e2d\uff0cVoxelOpt\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4e3b\u6d41\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "VoxelOpt\u901a\u8fc7\u521b\u65b0\u7684\u4f53\u7d20\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\u548c\u591a\u7ea7\u7b56\u7565\uff0c\u6210\u529f\u5e73\u8861\u4e86\u914d\u51c6\u7cbe\u5ea6\u4e0e\u8fd0\u884c\u65f6\u95f4\uff0c\u4e3a\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "VoxelOpt\uff1a\u57fa\u4e8e\u4f53\u7d20\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\u7684\u53d8\u5f62\u8179\u90e8CT\u914d\u51c6\u79bb\u6563\u4f18\u5316\u65b9\u6cd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u901a\u8fc7\u644a\u9500\u8fed\u4ee3\u4f18\u5316\u6539\u8fdb\u4e86\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\uff08DIR\uff09\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u51c6\u786e\u7684\u914d\u51c6\u7ed3\u679c\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u6216\u5927\u53d8\u5f62\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5728\u65e0\u6807\u7b7e\u76d1\u7763\u65f6\u901a\u5e38\u4e0d\u5982\u8fed\u4ee3\u65b9\u6cd5\u51c6\u786e\u3002\u5c3d\u7ba1\u8fed\u4ee3\u65b9\u6cd5\u5728\u6b64\u7c7b\u573a\u666f\u4e2d\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u4f46\u5176\u901f\u5ea6\u8fdc\u6162\u4e8e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VoxelOpt\uff0c\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u4f18\u5316\u7684DIR\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u4e0e\u8fed\u4ee3\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4ee5\u5728\u914d\u51c6\u7cbe\u5ea6\u548c\u8fd0\u884c\u65f6\u95f4\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002VoxelOpt\u5229\u7528\u5c40\u90e8\u6210\u672c\u4f53\u79ef\u7684\u4f4d\u79fb\u71b5\u8861\u91cf\u6bcf\u4e2a\u4f53\u7d20\u7684\u4f4d\u79fb\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u4e0e\u65e9\u671f\u65b9\u6cd5\u76f8\u6bd4\u6709\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\uff1a\u9996\u5148\uff0c\u5f15\u5165\u4f53\u7d20\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\uff0c\u71b5\u8f83\u4f4e\u7684\u4f53\u7d20\u53d7\u90bb\u5c45\u5f71\u54cd\u8f83\u5c0f\uff1b\u5176\u6b21\uff0c\u91c7\u7528\u591a\u7ea7\u56fe\u50cf\u91d1\u5b57\u5854\uff0c\u6bcf\u7ea7\u4f7f\u752827\u90bb\u57df\u6210\u672c\u4f53\u79ef\uff0c\u907f\u514d\u590d\u6742\u5ea6\u6307\u6570\u589e\u957f\uff1b\u7b2c\u4e09\uff0c\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u5206\u5272\u6a21\u578b\u66ff\u4ee3\u624b\u5de5\u7279\u5f81\u6216\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u3002\u5728\u8179\u90e8CT\u914d\u51c6\u4e2d\uff0c\u8fd9\u4e9b\u6539\u8fdb\u4f7fVoxelOpt\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4e3b\u6d41\u8fed\u4ee3\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0e\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u76f8\u5f53\u3002\u6e90\u4ee3\u7801\u5c06\u5728https://github.com/tinymilky/VoxelOpt \u63d0\u4f9b\u3002"}}
{"id": "2506.19992", "pdf": "https://arxiv.org/pdf/2506.19992", "abs": "https://arxiv.org/abs/2506.19992", "authors": ["Gabor Petnehazi", "Bernadett Aradi"], "title": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The explosive growth of complex datasets across various modalities\nnecessitates advanced analytical tools that not only group data effectively but\nalso provide human-understandable insights into the discovered structures. We\nintroduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using\nLLMs for Efficient Summarization), a novel algorithm and Python package\ndesigned for hierarchical k-means clustering of diverse data types, including\ntext, images, and numeric data (processed one modality per run). HERCULES\nconstructs a cluster hierarchy by recursively applying k-means clustering,\nstarting from individual data points at level 0. A key innovation is its deep\nintegration of Large Language Models (LLMs) to generate semantically rich\ntitles and descriptions for clusters at each level of the hierarchy,\nsignificantly enhancing interpretability. The algorithm supports two main\nrepresentation modes: `direct' mode, which clusters based on original data\nembeddings or scaled numeric features, and `description' mode, which clusters\nbased on embeddings derived from LLM-generated summaries. Users can provide a\n`topic\\_seed' to guide LLM-generated summaries towards specific themes. An\ninteractive visualization tool facilitates thorough analysis and understanding\nof the clustering results. We demonstrate HERCULES's capabilities and discuss\nits potential for extracting meaningful, hierarchical knowledge from complex\ndatasets.", "AI": {"tldr": "HERCULES\u662f\u4e00\u79cd\u65b0\u578b\u5206\u5c42\u805a\u7c7b\u7b97\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u805a\u7c7b\u6807\u9898\u548c\u63cf\u8ff0\uff0c\u63d0\u5347\u590d\u6742\u6570\u636e\u96c6\u7684\u53ef\u89e3\u91ca\u6027\u3002\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u590d\u6742\u6570\u636e\u96c6\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4ec5\u80fd\u6709\u6548\u5206\u7ec4\u6570\u636e\uff0c\u8fd8\u80fd\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u805a\u7c7b\u7ed3\u6784\u7684\u5de5\u5177\u3002HERCULES\u65e8\u5728\u901a\u8fc7\u7ed3\u5408LLM\u589e\u5f3a\u805a\u7c7b\u7684\u8bed\u4e49\u89e3\u91ca\u6027\u3002", "method": "HERCULES\u91c7\u7528\u5206\u5c42k-means\u805a\u7c7b\uff0c\u9012\u5f52\u5730\u4ece\u5355\u4e2a\u6570\u636e\u70b9\u5f00\u59cb\u6784\u5efa\u805a\u7c7b\u5c42\u6b21\u7ed3\u6784\u3002\u901a\u8fc7LLM\u751f\u6210\u6bcf\u4e2a\u5c42\u6b21\u805a\u7c7b\u7684\u6807\u9898\u548c\u63cf\u8ff0\uff0c\u652f\u6301\u4e24\u79cd\u6a21\u5f0f\uff1a\u57fa\u4e8e\u539f\u59cb\u6570\u636e\u5d4c\u5165\u7684`direct`\u6a21\u5f0f\u548c\u57fa\u4e8eLLM\u751f\u6210\u6458\u8981\u5d4c\u5165\u7684`description`\u6a21\u5f0f\u3002", "result": "HERCULES\u80fd\u591f\u4ece\u590d\u6742\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u5206\u5c42\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u5e2e\u52a9\u7528\u6237\u6df1\u5165\u5206\u6790\u805a\u7c7b\u7ed3\u679c\u3002", "conclusion": "HERCULES\u901a\u8fc7\u7ed3\u5408LLM\u548c\u5206\u5c42\u805a\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6570\u636e\u96c6\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "paper_title_zh": "HERCULES\uff1a\u57fa\u4e8e\u5206\u5c42\u5d4c\u5165\u7684\u9012\u5f52\u805a\u7c7b\u7b97\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u6458\u8981", "abstract_zh": "\u968f\u7740\u591a\u6a21\u6001\u590d\u6742\u6570\u636e\u96c6\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4ec5\u80fd\u6709\u6548\u5206\u7ec4\u6570\u636e\uff0c\u8fd8\u80fd\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u805a\u7c7b\u7ed3\u6784\u7684\u5148\u8fdb\u5206\u6790\u5de5\u5177\u3002\u6211\u4eec\u63d0\u51fa\u4e86HERCULES\uff08\u57fa\u4e8e\u5206\u5c42\u5d4c\u5165\u7684\u9012\u5f52\u805a\u7c7b\u7b97\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u6458\u8981\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7b97\u6cd5\u548cPython\u5305\uff0c\u4e13\u4e3a\u5206\u5c42k-means\u805a\u7c7b\u8bbe\u8ba1\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u6570\u503c\u6570\u636e\uff08\u6bcf\u6b21\u8fd0\u884c\u5904\u7406\u4e00\u79cd\u6a21\u6001\uff09\u3002HERCULES\u901a\u8fc7\u9012\u5f52\u5e94\u7528k-means\u805a\u7c7b\u4ece\u7b2c0\u5c42\u7684\u5355\u4e2a\u6570\u636e\u70b9\u5f00\u59cb\u6784\u5efa\u805a\u7c7b\u5c42\u6b21\u7ed3\u6784\u3002\u5176\u5173\u952e\u521b\u65b0\u5728\u4e8e\u6df1\u5ea6\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4e3a\u6bcf\u4e2a\u5c42\u6b21\u7684\u805a\u7c7b\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6807\u9898\u548c\u63cf\u8ff0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u7b97\u6cd5\u652f\u6301\u4e24\u79cd\u4e3b\u8981\u8868\u793a\u6a21\u5f0f\uff1a`direct`\u6a21\u5f0f\uff08\u57fa\u4e8e\u539f\u59cb\u6570\u636e\u5d4c\u5165\u6216\u7f29\u653e\u6570\u503c\u7279\u5f81\u8fdb\u884c\u805a\u7c7b\uff09\u548c`description`\u6a21\u5f0f\uff08\u57fa\u4e8eLLM\u751f\u6210\u6458\u8981\u7684\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\uff09\u3002\u7528\u6237\u53ef\u4ee5\u63d0\u4f9b`topic_seed`\u4ee5\u5f15\u5bfcLLM\u751f\u6210\u7684\u6458\u8981\u671d\u5411\u7279\u5b9a\u4e3b\u9898\u3002\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u6709\u52a9\u4e8e\u5168\u9762\u5206\u6790\u548c\u7406\u89e3\u805a\u7c7b\u7ed3\u679c\u3002\u6211\u4eec\u5c55\u793a\u4e86HERCULES\u7684\u529f\u80fd\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u4ece\u590d\u6742\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u5206\u5c42\u77e5\u8bc6\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20045", "pdf": "https://arxiv.org/pdf/2506.20045", "abs": "https://arxiv.org/abs/2506.20045", "authors": ["Eric C. Joyce", "Qianwen Zhao", "Nathaniel Burgdorfer", "Long Wang", "Philippos Mordohai"], "title": "Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Deep object pose estimators are notoriously overconfident. A grasping agent\nthat both estimates the 6-DoF pose of a target object and predicts the\nuncertainty of its own estimate could avoid task failure by choosing not to act\nunder high uncertainty. Even though object pose estimation improves and\nuncertainty quantification research continues to make strides, few studies have\nconnected them to the downstream task of robotic grasping. We propose a method\nfor training lightweight, deep networks to predict whether a grasp guided by an\nimage-based pose estimate will succeed before that grasp is attempted. We\ngenerate training data for our networks via object pose estimation on real\nimages and simulated grasping. We also find that, despite high object\nvariability in grasping trials, networks benefit from training on all objects\njointly, suggesting that a diverse variety of objects can nevertheless\ncontribute to the same goal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRGB\u611f\u77e5\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u7f51\u7edc\u9884\u6d4b\u6293\u53d6\u6210\u529f\u6982\u7387\uff0c\u907f\u514d\u9ad8\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4efb\u52a1\u5931\u8d25\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u4e0b\u5931\u8d25\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u6293\u53d6\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "method": "\u901a\u8fc7\u771f\u5b9e\u56fe\u50cf\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6a21\u62df\u6293\u53d6\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u7f51\u7edc\u9884\u6d4b\u6293\u53d6\u6210\u529f\u6982\u7387\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u7269\u4f53\u8054\u5408\u8bad\u7ec3\u5bf9\u7f51\u7edc\u6027\u80fd\u6709\u79ef\u6781\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u6293\u53d6\u8bd5\u9a8c\u4e2d\u7269\u4f53\u53d8\u5f02\u6027\u9ad8\uff0c\u591a\u7269\u4f53\u8054\u5408\u8bad\u7ec3\u4ecd\u80fd\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u652f\u6301\u6293\u53d6\u4efb\u52a1\u7684\u6210\u529f\u9884\u6d4b\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u4e14\u591a\u7269\u4f53\u8054\u5408\u8bad\u7ec3\u5bf9\u7f51\u7edc\u6027\u80fd\u6709\u663e\u8457\u5e2e\u52a9\u3002", "paper_title_zh": "\u57fa\u4e8eRGB\u611f\u77e5\u7684\u5171\u8bc6\u9a71\u52a8\u4e0d\u786e\u5b9a\u6027\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5", "abstract_zh": "\u6df1\u5ea6\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\u3002\u4e00\u4e2a\u65e2\u80fd\u4f30\u8ba1\u76ee\u6807\u7269\u4f536\u81ea\u7531\u5ea6\u59ff\u6001\u53c8\u80fd\u9884\u6d4b\u5176\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u6293\u53d6\u4ee3\u7406\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u4e0b\u9009\u62e9\u4e0d\u884c\u52a8\u6765\u907f\u514d\u4efb\u52a1\u5931\u8d25\u3002\u5c3d\u7ba1\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e0d\u65ad\u6539\u8fdb\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7814\u7a76\u4e5f\u6301\u7eed\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5c06\u5176\u4e0e\u673a\u5668\u4eba\u6293\u53d6\u8fd9\u4e00\u4e0b\u6e38\u4efb\u52a1\u8054\u7cfb\u8d77\u6765\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5c1d\u8bd5\u6293\u53d6\u4e4b\u524d\u9884\u6d4b\u57fa\u4e8e\u56fe\u50cf\u59ff\u6001\u4f30\u8ba1\u7684\u6293\u53d6\u662f\u5426\u4f1a\u6210\u529f\u3002\u6211\u4eec\u901a\u8fc7\u771f\u5b9e\u56fe\u50cf\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6a21\u62df\u6293\u53d6\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u6293\u53d6\u8bd5\u9a8c\u4e2d\u7269\u4f53\u53d8\u5f02\u6027\u9ad8\uff0c\u7f51\u7edc\u4ecd\u80fd\u4ece\u591a\u7269\u4f53\u8054\u5408\u8bad\u7ec3\u4e2d\u53d7\u76ca\uff0c\u8fd9\u8868\u660e\u591a\u6837\u5316\u7684\u7269\u4f53\u4ecd\u80fd\u4e3a\u540c\u4e00\u76ee\u6807\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2506.19997", "pdf": "https://arxiv.org/pdf/2506.19997", "abs": "https://arxiv.org/abs/2506.19997", "authors": ["Geonwoo Cho", "Jaegyun Im", "Jihwan Lee", "Hojun Yi", "Sejin Kim", "Sundong Kim"], "title": "TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generalizing deep reinforcement learning agents to unseen environments\nremains a significant challenge. One promising solution is Unsupervised\nEnvironment Design (UED), a co-evolutionary framework in which a teacher\nadaptively generates tasks with high learning potential, while a student learns\na robust policy from this evolving curriculum. Existing UED methods typically\nmeasure learning potential via regret, the gap between optimal and current\nperformance, approximated solely by value-function loss. Building on these\napproaches, we introduce the transition prediction error as an additional term\nin our regret approximation. To capture how training on one task affects\nperformance on others, we further propose a lightweight metric called\nco-learnability. By combining these two measures, we present Transition-aware\nRegret Approximation with Co-learnability for Environment Design (TRACED).\nEmpirical evaluations show that TRACED yields curricula that improve zero-shot\ngeneralization across multiple benchmarks while requiring up to 2x fewer\nenvironment interactions than strong baselines. Ablation studies confirm that\nthe transition prediction error drives rapid complexity ramp-up and that\nco-learnability delivers additional gains when paired with the transition\nprediction error. These results demonstrate how refined regret approximation\nand explicit modeling of task relationships can be leveraged for\nsample-efficient curriculum design in UED.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTRACED\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u548c\u5171\u5b66\u4e60\u6027\u5ea6\u91cf\uff0c\u6539\u8fdb\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u4e2d\u7684\u8bfe\u7a0b\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u73af\u5883\u4ea4\u4e92\u9700\u6c42\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u901a\u8fc7\u6559\u5e08\u52a8\u6001\u751f\u6210\u9ad8\u5b66\u4e60\u6f5c\u529b\u7684\u4efb\u52a1\uff0c\u5b66\u751f\u4ece\u4e2d\u5b66\u4e60\u9c81\u68d2\u7b56\u7565\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u635f\u5931\u8fd1\u4f3c\u5b66\u4e60\u6f5c\u529b\uff08\u9057\u61be\uff09\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u95f4\u7684\u5f71\u54cd\u3002", "method": "TRACED\u65b9\u6cd5\u5f15\u5165\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u4f5c\u4e3a\u9057\u61be\u8fd1\u4f3c\u7684\u65b0\u9879\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5171\u5b66\u4e60\u6027\u5ea6\u91cf\u4ee5\u6355\u6349\u4efb\u52a1\u95f4\u5b66\u4e60\u5f71\u54cd\u3002\u7ed3\u5408\u8fd9\u4e24\u9879\u6307\u6807\uff0c\u751f\u6210\u66f4\u9ad8\u6548\u7684\u8bfe\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTRACED\u751f\u6210\u7684\u8bfe\u7a0b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u73af\u5883\u4ea4\u4e92\u9700\u6c42\u51cf\u5c11\u81f3\u57fa\u7ebf\u65b9\u6cd5\u768450%\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u9a71\u52a8\u590d\u6742\u6027\u5feb\u901f\u63d0\u5347\uff0c\u5171\u5b66\u4e60\u6027\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u9057\u61be\u8fd1\u4f3c\u548c\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u5173\u7cfb\uff0cTRACED\u80fd\u591f\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u4e3aUED\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "TRACED\uff1a\u57fa\u4e8e\u8f6c\u79fb\u611f\u77e5\u7684\u9057\u61be\u8fd1\u4f3c\u4e0e\u5171\u5b66\u4e60\u6027\u7684\u73af\u5883\u8bbe\u8ba1", "abstract_zh": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u4f5c\u4e3a\u4e00\u79cd\u5171\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u52a8\u6001\u751f\u6210\u9ad8\u5b66\u4e60\u6f5c\u529b\u7684\u4efb\u52a1\uff0c\u5b66\u751f\u4ece\u4e2d\u5b66\u4e60\u9c81\u68d2\u7b56\u7565\u3002\u73b0\u6709UED\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u9057\u61be\uff08\u5373\u6700\u4f18\u4e0e\u5f53\u524d\u6027\u80fd\u7684\u5dee\u8ddd\uff09\u6765\u8861\u91cf\u5b66\u4e60\u6f5c\u529b\uff0c\u4ec5\u57fa\u4e8e\u4ef7\u503c\u51fd\u6570\u635f\u5931\u8fd1\u4f3c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u4f5c\u4e3a\u9057\u61be\u8fd1\u4f3c\u7684\u65b0\u9879\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5171\u5b66\u4e60\u6027\u5ea6\u91cf\u4ee5\u6355\u6349\u4efb\u52a1\u95f4\u5b66\u4e60\u5f71\u54cd\u3002\u7ed3\u5408\u8fd9\u4e24\u9879\u6307\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f6c\u79fb\u611f\u77e5\u7684\u9057\u61be\u8fd1\u4f3c\u4e0e\u5171\u5b66\u4e60\u6027\u7684\u73af\u5883\u8bbe\u8ba1\uff08TRACED\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTRACED\u751f\u6210\u7684\u8bfe\u7a0b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u73af\u5883\u4ea4\u4e92\u9700\u6c42\u51cf\u5c11\u81f3\u57fa\u7ebf\u65b9\u6cd5\u768450%\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u9a71\u52a8\u590d\u6742\u6027\u5feb\u901f\u63d0\u5347\uff0c\u5171\u5b66\u4e60\u6027\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6027\u80fd\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u9057\u61be\u8fd1\u4f3c\u548c\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u5173\u7cfb\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u8bfe\u7a0b\u8bbe\u8ba1\u3002"}}
{"id": "2506.20016", "pdf": "https://arxiv.org/pdf/2506.20016", "abs": "https://arxiv.org/abs/2506.20016", "authors": ["Shanika Iroshi Nanayakkara", "Shiva Raj Pokhrel"], "title": "New Insights on Unfolding and Fine-tuning Quantum Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 9 figures, 7 Tables, Submitted to IEEE/ACM journal 2025", "summary": "Client heterogeneity poses significant challenges to the performance of\nQuantum Federated Learning (QFL). To overcome these limitations, we propose a\nnew approach leveraging deep unfolding, which enables clients to autonomously\noptimize hyperparameters, such as learning rates and regularization factors,\nbased on their specific training behavior. This dynamic adaptation mitigates\noverfitting and ensures robust optimization in highly heterogeneous\nenvironments where standard aggregation methods often fail. Our framework\nachieves approximately 90% accuracy, significantly outperforming traditional\nmethods, which typically yield around 55% accuracy, as demonstrated through\nreal-time training on IBM quantum hardware and Qiskit Aer simulators. By\ndeveloping self adaptive fine tuning, the proposed method proves particularly\neffective in critical applications such as gene expression analysis and cancer\ndetection, enhancing diagnostic precision and predictive modeling within\nquantum systems. Our results are attributed to convergence-aware, learnable\noptimization steps intrinsic to the deep unfolded framework, which maintains\nthe generalization. Hence, this study addresses the core limitations of\nconventional QFL, streamlining its applicability to any complex challenges such\nas healthcare and genomic research.", "AI": {"tldr": "\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\uff08QFL\uff09\u4e2d\u5ba2\u6237\u5f02\u8d28\u6027\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5c55\u5f00\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u5ba2\u6237\u80fd\u81ea\u4e3b\u4f18\u5316\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\u548c\u6b63\u5219\u5316\u56e0\u5b50\uff09\uff0c\u52a8\u6001\u9002\u5e94\u8bad\u7ec3\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u81f390%\uff0c\u8fdc\u8d85\u4f20\u7edf\u65b9\u6cd5\u768455%\u3002", "motivation": "\u5ba2\u6237\u5f02\u8d28\u6027\u5bfc\u81f4\u4f20\u7edf\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u53d7\u9650\uff0c\u5c24\u5176\u5728\u533b\u7597\u548c\u57fa\u56e0\u7ec4\u7814\u7a76\u7b49\u5173\u952e\u9886\u57df\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u5ba2\u6237\u9700\u6c42\u7684\u65b9\u6cd5\u4ee5\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5c55\u5f00\u6280\u672f\uff0c\u4f7f\u5ba2\u6237\u80fd\u81ea\u4e3b\u4f18\u5316\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\u548c\u6b63\u5219\u5316\u56e0\u5b50\uff09\uff0c\u52a8\u6001\u9002\u5e94\u8bad\u7ec3\u884c\u4e3a\uff0c\u907f\u514d\u8fc7\u62df\u5408\uff0c\u5e76\u5728IBM\u91cf\u5b50\u786c\u4ef6\u548cQiskit Aer\u6a21\u62df\u5668\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5728\u9ad8\u5ea6\u5f02\u8d28\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u51c6\u786e\u7387\u8fbe90%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u768455%\uff0c\u5c24\u5176\u5728\u57fa\u56e0\u8868\u8fbe\u5206\u6790\u548c\u764c\u75c7\u68c0\u6d4b\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5ea6\u5c55\u5f00\u6846\u67b6\u7684\u6536\u655b\u611f\u77e5\u548c\u53ef\u5b66\u4e60\u4f18\u5316\u6b65\u9aa4\uff0c\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u4f20\u7edfQFL\u7684\u6838\u5fc3\u9650\u5236\uff0c\u4e3a\u533b\u7597\u548c\u57fa\u56e0\u7ec4\u7814\u7a76\u7b49\u590d\u6742\u6311\u6218\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7684\u5c55\u5f00\u4e0e\u5fae\u8c03\u65b0\u89c1\u89e3", "abstract_zh": "\u5ba2\u6237\u5f02\u8d28\u6027\u5bf9\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\uff08QFL\uff09\u7684\u6027\u80fd\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5c55\u5f00\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u5ba2\u6237\u80fd\u591f\u6839\u636e\u5176\u7279\u5b9a\u7684\u8bad\u7ec3\u884c\u4e3a\u81ea\u4e3b\u4f18\u5316\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\u548c\u6b63\u5219\u5316\u56e0\u5b50\uff09\u3002\u8fd9\u79cd\u52a8\u6001\u9002\u5e94\u5728\u9ad8\u5f02\u8d28\u6027\u73af\u5883\u4e2d\u51cf\u8f7b\u4e86\u8fc7\u62df\u5408\u5e76\u786e\u4fdd\u4e86\u9c81\u68d2\u7684\u4f18\u5316\uff0c\u800c\u6807\u51c6\u805a\u5408\u65b9\u6cd5\u5728\u6b64\u7c7b\u73af\u5883\u4e2d\u5f80\u5f80\u5931\u6548\u3002\u6211\u4eec\u7684\u6846\u67b6\u5728IBM\u91cf\u5b50\u786c\u4ef6\u548cQiskit Aer\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u65f6\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u7ea690%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u901a\u5e38\u7ea6\u4e3a55%\uff09\u3002\u901a\u8fc7\u5f00\u53d1\u81ea\u9002\u5e94\u7684\u5fae\u8c03\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u56e0\u8868\u8fbe\u5206\u6790\u548c\u764c\u75c7\u68c0\u6d4b\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u5c24\u4e3a\u6709\u6548\uff0c\u63d0\u5347\u4e86\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u8bca\u65ad\u7cbe\u5ea6\u548c\u9884\u6d4b\u5efa\u6a21\u80fd\u529b\u3002\u8fd9\u4e9b\u6210\u679c\u5f52\u529f\u4e8e\u6df1\u5ea6\u5c55\u5f00\u6846\u67b6\u4e2d\u56fa\u6709\u7684\u6536\u655b\u611f\u77e5\u548c\u53ef\u5b66\u4e60\u4f18\u5316\u6b65\u9aa4\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u4f20\u7edfQFL\u7684\u6838\u5fc3\u9650\u5236\uff0c\u4e3a\u5176\u5728\u533b\u7597\u548c\u57fa\u56e0\u7ec4\u7814\u7a76\u7b49\u590d\u6742\u6311\u6218\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u9014\u5f84\u3002"}}
{"id": "2506.20200", "pdf": "https://arxiv.org/pdf/2506.20200", "abs": "https://arxiv.org/abs/2506.20200", "authors": ["Siqiao Li", "Chen Hui", "Wei Zhang", "Rui Liang", "Chenyue Song", "Feng Jiang", "Haiqi Zhu", "Zhixuan Li", "Hong Huang", "Xiang Li"], "title": "MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical\nrole in medical imaging, combining functional and anatomical information to aid\nin accurate diagnosis. However, image quality degradation due to noise,\ncompression and other factors could potentially lead to diagnostic uncertainty\nand increase the risk of misdiagnosis. When evaluating the quality of a PET/CT\nimage, both low-level features like distortions and high-level features like\norgan anatomical structures affect the diagnostic value of the image. However,\nexisting medical image quality assessment (IQA) methods are unable to account\nfor both feature types simultaneously. In this work, we propose MS-IQA, a novel\nmulti-scale feature fusion network for PET/CT IQA, which utilizes multi-scale\nfeatures from various intermediate layers of ResNet and Swin Transformer,\nenhancing its ability of perceiving both local and global information. In\naddition, a multi-scale feature fusion module is also introduced to effectively\ncombine high-level and low-level information through a dynamically weighted\nchannel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,\nwe construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT\nimages with quality scores assigned by radiologists. Experiments on our dataset\nand the publicly available LDCTIQAC2023 dataset demonstrate that our proposed\nmodel has achieved superior performance against existing state-of-the-art\nmethods in various IQA metrics. This work provides an accurate and efficient\nIQA method for PET/CT. Our code and dataset are available at\nhttps://github.com/MS-IQA/MS-IQA/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMS-IQA\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u8bc4\u4f30PET/CT\u56fe\u50cf\u8d28\u91cf\uff0c\u901a\u8fc7\u7ed3\u5408ResNet\u548cSwin Transformer\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u52a8\u6001\u52a0\u6743\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2700\u5f20\u4e0d\u540c\u8d28\u91cfPET/CT\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "PET/CT\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u8003\u8651\u4f4e\u5c42\u6b21\u7279\u5f81\uff08\u5982\u5931\u771f\uff09\u548c\u9ad8\u5c42\u6b21\u7279\u5f81\uff08\u5982\u5668\u5b98\u89e3\u5256\u7ed3\u6784\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u7efc\u5408\u8bc4\u4f30PET/CT\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51faMS-IQA\u7f51\u7edc\uff0c\u7ed3\u5408ResNet\u548cSwin Transformer\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5f15\u5165\u52a8\u6001\u52a0\u6743\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u9ad8\u4f4e\u5c42\u6b21\u4fe1\u606f\u3002\u6784\u5efaPET-CT-IQA-DS\u6570\u636e\u96c6\uff0c\u5305\u542b2700\u5f20\u6807\u6ce8\u8d28\u91cf\u7684PET/CT\u56fe\u50cf\u3002", "result": "\u5728PET-CT-IQA-DS\u548c\u516c\u5f00\u6570\u636e\u96c6LDCTIQAC2023\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMS-IQA\u5728\u591a\u79cd\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MS-IQA\u4e3aPET/CT\u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u9ad8\u6548\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u76f8\u5173\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "paper_title_zh": "MS-IQA\uff1a\u4e00\u79cd\u7528\u4e8ePET/CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7f51\u7edc", "abstract_zh": "\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf/\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08PET/CT\uff09\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u7ed3\u5408\u529f\u80fd\u548c\u89e3\u5256\u4fe1\u606f\u4ee5\u8f85\u52a9\u51c6\u786e\u8bca\u65ad\u3002\u7136\u800c\uff0c\u7531\u4e8e\u566a\u58f0\u3001\u538b\u7f29\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u53ef\u80fd\u5f15\u53d1\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u5e76\u589e\u52a0\u8bef\u8bca\u98ce\u9669\u3002\u5728\u8bc4\u4f30PET/CT\u56fe\u50cf\u8d28\u91cf\u65f6\uff0c\u4f4e\u5c42\u6b21\u7279\u5f81\uff08\u5982\u5931\u771f\uff09\u548c\u9ad8\u5c42\u6b21\u7279\u5f81\uff08\u5982\u5668\u5b98\u89e3\u5256\u7ed3\u6784\uff09\u5747\u5f71\u54cd\u56fe\u50cf\u7684\u8bca\u65ad\u4ef7\u503c\u3002\u7136\u800c\uff0c\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u8003\u8651\u8fd9\u4e24\u79cd\u7279\u5f81\u3002\u672c\u6587\u63d0\u51faMS-IQA\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff0c\u7528\u4e8ePET/CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff0c\u5229\u7528ResNet\u548cSwin Transformer\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u5176\u5bf9\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u7ed3\u5408\u9ad8\u4f4e\u5c42\u6b21\u4fe1\u606f\u3002\u6700\u540e\uff0c\u4e3a\u586b\u8865PET/CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u6211\u4eec\u6784\u5efa\u4e86PET-CT-IQA-DS\u6570\u636e\u96c6\uff0c\u5305\u542b2700\u5f20\u4e0d\u540c\u8d28\u91cf\u7684PET/CT\u56fe\u50cf\uff0c\u5e76\u7531\u653e\u5c04\u79d1\u533b\u751f\u6807\u6ce8\u8d28\u91cf\u8bc4\u5206\u3002\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u516c\u5f00\u6570\u636e\u96c6LDCTIQAC2023\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u5728\u591a\u79cd\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u672c\u6587\u4e3aPET/CT\u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u9ad8\u6548\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728https://github.com/MS-IQA/MS-IQA/\u83b7\u53d6\u3002"}}
{"id": "2506.20024", "pdf": "https://arxiv.org/pdf/2506.20024", "abs": "https://arxiv.org/abs/2506.20024", "authors": ["Salva R\u00fchling Cachay", "Miika Aittala", "Karsten Kreis", "Noah Brenowitz", "Arash Vahdat", "Morteza Mardani", "Rose Yu"], "title": "Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "comment": null, "summary": "Diffusion models are a powerful tool for probabilistic forecasting, yet most\napplications in high-dimensional chaotic systems predict future snapshots\none-by-one. This common approach struggles to model complex temporal\ndependencies and fails to explicitly account for the progressive growth of\nuncertainty inherent to such systems. While rolling diffusion frameworks, which\napply increasing noise to forecasts at longer lead times, have been proposed to\naddress this, their integration with state-of-the-art, high-fidelity diffusion\ntechniques remains a significant challenge. We tackle this problem by\nintroducing Elucidated Rolling Diffusion Models (ERDM), the first framework to\nsuccessfully unify a rolling forecast structure with the principled, performant\ndesign of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM\ncomponents-its noise schedule, network preconditioning, and Heun sampler-to the\nrolling forecast setting. The success of this integration is driven by three\nkey contributions: (i) a novel loss weighting scheme that focuses model\ncapacity on the mid-range forecast horizons where determinism gives way to\nstochasticity; (ii) an efficient initialization strategy using a pre-trained\nEDM for the initial window; and (iii) a bespoke hybrid sequence architecture\nfor robust spatiotemporal feature extraction under progressive denoising. On 2D\nNavier-Stokes simulations and ERA5 global weather forecasting at 1.5^\\circ\nresolution, ERDM consistently outperforms key diffusion-based baselines,\nincluding conditional autoregressive EDM. ERDM offers a flexible and powerful\ngeneral framework for tackling diffusion-based sequence generation problems\nwhere modeling escalating uncertainty is paramount. Code is available at:\nhttps://github.com/salvaRC/erdm", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aElucidated Rolling Diffusion Models (ERDM)\u7684\u65b0\u6846\u67b6\uff0c\u9996\u6b21\u6210\u529f\u5c06\u6eda\u52a8\u9884\u6d4b\u7ed3\u6784\u4e0e\u9ad8\u6027\u80fd\u7684Elucidated Diffusion Models (EDM)\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u7684\u6982\u7387\u5929\u6c14\u9884\u62a5\u95ee\u9898\u3002\u901a\u8fc7\u6539\u8fdb\u566a\u58f0\u8c03\u5ea6\u3001\u7f51\u7edc\u9884\u5904\u7406\u548c\u91c7\u6837\u5668\uff0cERDM\u57282D Navier-Stokes\u6a21\u62df\u548cERA5\u5168\u7403\u5929\u6c14\u9884\u62a5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u901a\u5e38\u9010\u5e27\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u672a\u660e\u786e\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u968f\u65f6\u95f4\u7684\u589e\u957f\u3002\u5c3d\u7ba1\u5df2\u6709\u6eda\u52a8\u6269\u6563\u6846\u67b6\u63d0\u51fa\uff0c\u4f46\u5176\u4e0e\u9ad8\u6027\u80fd\u6269\u6563\u6280\u672f\u7684\u7ed3\u5408\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ERDM\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u5173\u952e\u8d21\u732e\u5b9e\u73b0\uff1a1) \u65b0\u9896\u7684\u635f\u5931\u52a0\u6743\u65b9\u6848\uff0c\u96c6\u4e2d\u6a21\u578b\u80fd\u529b\u4e8e\u4e2d\u7b49\u9884\u6d4b\u8303\u56f4\uff1b2) \u4f7f\u7528\u9884\u8bad\u7ec3EDM\u521d\u59cb\u5316\u521d\u59cb\u7a97\u53e3\u7684\u9ad8\u6548\u7b56\u7565\uff1b3) \u5b9a\u5236\u5316\u7684\u6df7\u5408\u5e8f\u5217\u67b6\u6784\uff0c\u7528\u4e8e\u6e10\u8fdb\u53bb\u566a\u4e0b\u7684\u7a33\u5065\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u57282D Navier-Stokes\u6a21\u62df\u548cERA5\u5168\u7403\u5929\u6c14\u9884\u62a5\uff081.5\u00b0\u5206\u8fa8\u7387\uff09\u4e2d\uff0cERDM\u8868\u73b0\u4f18\u4e8e\u5305\u62ec\u6761\u4ef6\u81ea\u56de\u5f52EDM\u5728\u5185\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ERDM\u4e3a\u6269\u6563\u6a21\u578b\u5728\u5e8f\u5217\u751f\u6210\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u968f\u65f6\u95f4\u589e\u957f\u7684\u4efb\u52a1\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u8be6\u89e3\u6eda\u52a8\u6269\u6563\u6a21\u578b\u7528\u4e8e\u6982\u7387\u5929\u6c14\u9884\u62a5", "abstract_zh": "\u6269\u6563\u6a21\u578b\u662f\u6982\u7387\u9884\u6d4b\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u5728\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u4e2d\uff0c\u5927\u591a\u6570\u5e94\u7528\u9010\u5e27\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002\u8fd9\u79cd\u5e38\u89c1\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4e14\u672a\u660e\u786e\u5904\u7406\u7cfb\u7edf\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u968f\u65f6\u95f4\u589e\u957f\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u6709\u6eda\u52a8\u6269\u6563\u6846\u67b6\u63d0\u51fa\uff0c\u4f46\u5176\u4e0e\u9ad8\u6027\u80fd\u6269\u6563\u6280\u672f\u7684\u7ed3\u5408\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165Elucidated Rolling Diffusion Models (ERDM)\uff0c\u9996\u6b21\u6210\u529f\u5c06\u6eda\u52a8\u9884\u6d4b\u7ed3\u6784\u4e0eElucidated Diffusion Models (EDM)\u7684\u9ad8\u6027\u80fd\u8bbe\u8ba1\u76f8\u7ed3\u5408\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8c03\u6574\u4e86EDM\u7684\u6838\u5fc3\u7ec4\u4ef6\uff08\u566a\u58f0\u8c03\u5ea6\u3001\u7f51\u7edc\u9884\u5904\u7406\u548cHeun\u91c7\u6837\u5668\uff09\u4ee5\u9002\u5e94\u6eda\u52a8\u9884\u6d4b\u573a\u666f\u3002\u8fd9\u4e00\u6210\u529f\u5f97\u76ca\u4e8e\u4e09\u9879\u5173\u952e\u8d21\u732e\uff1a(i) \u65b0\u9896\u7684\u635f\u5931\u52a0\u6743\u65b9\u6848\uff0c\u805a\u7126\u6a21\u578b\u80fd\u529b\u4e8e\u4e2d\u7b49\u9884\u6d4b\u8303\u56f4\uff1b(ii) \u4f7f\u7528\u9884\u8bad\u7ec3EDM\u521d\u59cb\u5316\u521d\u59cb\u7a97\u53e3\u7684\u9ad8\u6548\u7b56\u7565\uff1b(iii) \u5b9a\u5236\u5316\u7684\u6df7\u5408\u5e8f\u5217\u67b6\u6784\uff0c\u7528\u4e8e\u6e10\u8fdb\u53bb\u566a\u4e0b\u7684\u7a33\u5065\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u3002\u57282D Navier-Stokes\u6a21\u62df\u548cERA5\u5168\u7403\u5929\u6c14\u9884\u62a5\uff081.5\u00b0\u5206\u8fa8\u7387\uff09\u4e2d\uff0cERDM\u8868\u73b0\u4f18\u4e8e\u5305\u62ec\u6761\u4ef6\u81ea\u56de\u5f52EDM\u5728\u5185\u7684\u57fa\u7ebf\u6a21\u578b\u3002ERDM\u4e3a\u6269\u6563\u6a21\u578b\u5728\u5e8f\u5217\u751f\u6210\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u901a\u7528\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u968f\u65f6\u95f4\u589e\u957f\u7684\u4efb\u52a1\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/salvaRC/erdm\u3002"}}
{"id": "2506.20245", "pdf": "https://arxiv.org/pdf/2506.20245", "abs": "https://arxiv.org/abs/2506.20245", "authors": ["Yushan Zhao", "Jinyuan He", "Donglai Chen", "Weijie Luo", "Chong Xie", "Ri Zhang", "Yonghong Chen", "Yan Xu"], "title": "FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a decentralized collaborative machine learning\n(ML) technique. It provides a solution to the issues of isolated data islands\nand data privacy leakage in industrial ML practices. One major challenge in FL\nis handling the non-identical and independent distributed (non-IID) data.\nCurrent solutions either focus on constructing an all-powerful global model, or\ncustomizing personalized local models. Few of them can provide both a\nwell-generalized global model and well-performed local models at the same time.\nAdditionally, many FL solutions to the non-IID problem are benefited from\nintroducing public datasets. However, this will also increase the risk of data\nleakage. To tackle the problems, we propose a novel data-free distillation\nframework, Federated Bidirectional Knowledge Distillation (FedBKD).\nSpecifically, we train Generative Adversarial Networks (GAN) for synthetic\ndata. During the GAN training, local models serve as discriminators and their\nparameters are frozen. The synthetic data is then used for bidirectional\ndistillation between global and local models to achieve knowledge interactions\nso that performances for both sides are improved. We conduct extensive\nexperiments on 4 benchmarks under different non-IID settings. The results show\nthat FedBKD achieves SOTA performances in every case.", "AI": {"tldr": "FedBKD\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6570\u636e\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5b9e\u73b0\u5168\u5c40\u6a21\u578b\u548c\u5c40\u90e8\u6a21\u578b\u4e4b\u95f4\u7684\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\uff0c\u4ee5\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u7684\u6cdb\u5316\u548c\u4e2a\u6027\u5316\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u548c\u5c40\u90e8\u6a21\u578b\u7684\u4e2a\u6027\u5316\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u516c\u5171\u6570\u636e\u96c6\uff0c\u589e\u52a0\u4e86\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002FedBKD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "FedBKD\u901a\u8fc7\u8bad\u7ec3GAN\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5c40\u90e8\u6a21\u578b\u4f5c\u4e3a\u5224\u522b\u5668\u4e14\u53c2\u6570\u51bb\u7ed3\u3002\u5408\u6210\u6570\u636e\u7528\u4e8e\u5168\u5c40\u6a21\u578b\u548c\u5c40\u90e8\u6a21\u578b\u4e4b\u95f4\u7684\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\uff0c\u4ee5\u63d0\u5347\u53cc\u65b9\u6027\u80fd\u3002", "result": "\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedBKD\u5728\u4e0d\u540cnon-IID\u8bbe\u7f6e\u4e0b\u5747\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "FedBKD\u901a\u8fc7\u65e0\u6570\u636e\u84b8\u998f\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86FL\u4e2dnon-IID\u6570\u636e\u7684\u6cdb\u5316\u548c\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u4e14\u907f\u514d\u4e86\u516c\u5171\u6570\u636e\u96c6\u5e26\u6765\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "paper_title_zh": "FedBKD\uff1a\u57fa\u4e8e\u84b8\u998f\u7684\u8054\u90a6\u5b66\u4e60\u4ee5\u5e94\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6cdb\u5316\u4e0e\u4e2a\u6027\u5316", "abstract_zh": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u6570\u636e\u5b64\u5c9b\u548c\u6570\u636e\u9690\u79c1\u6cc4\u9732\u7684\u95ee\u9898\u3002FL\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u662f\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4e13\u6ce8\u4e8e\u6784\u5efa\u5f3a\u5927\u7684\u5168\u5c40\u6a21\u578b\uff0c\u8981\u4e48\u5b9a\u5236\u4e2a\u6027\u5316\u5c40\u90e8\u6a21\u578b\uff0c\u4f46\u5f88\u5c11\u80fd\u540c\u65f6\u63d0\u4f9b\u6cdb\u5316\u826f\u597d\u7684\u5168\u5c40\u6a21\u578b\u548c\u6027\u80fd\u4f18\u8d8a\u7684\u5c40\u90e8\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u9488\u5bf9non-IID\u95ee\u9898\u7684FL\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u4e8e\u5f15\u5165\u516c\u5171\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u4f1a\u589e\u52a0\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u65e0\u6570\u636e\u84b8\u998f\u6846\u67b6\u2014\u2014\u8054\u90a6\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\uff08FedBKD\uff09\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u8bad\u7ec3\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u5728GAN\u8bad\u7ec3\u4e2d\uff0c\u5c40\u90e8\u6a21\u578b\u4f5c\u4e3a\u5224\u522b\u5668\u4e14\u53c2\u6570\u51bb\u7ed3\u3002\u5408\u6210\u6570\u636e\u7528\u4e8e\u5168\u5c40\u6a21\u578b\u548c\u5c40\u90e8\u6a21\u578b\u4e4b\u95f4\u7684\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\uff0c\u4ee5\u5b9e\u73b0\u77e5\u8bc6\u4ea4\u4e92\uff0c\u4ece\u800c\u63d0\u5347\u53cc\u65b9\u6027\u80fd\u3002\u6211\u4eec\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eFedBKD\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2506.20031", "pdf": "https://arxiv.org/pdf/2506.20031", "abs": "https://arxiv.org/abs/2506.20031", "authors": ["Prithvi Poddar", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury"], "title": "Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Operations in disaster response, search \\& rescue, and military missions that\ninvolve multiple agents demand automated processes to support the planning of\nthe courses of action (COA). Moreover, traverse-affecting changes in the\nenvironment (rain, snow, blockades, etc.) may impact the expected performance\nof a COA, making it desirable to have a pool of COAs that are diverse in task\ndistributions across agents. Further, variations in agent capabilities, which\ncould be human crews and/or autonomous systems, present practical opportunities\nand computational challenges to the planning process. This paper presents a new\ntheoretical formulation and computational framework to generate such diverse\npools of COAs for operations with soft variations in agent-task compatibility.\nKey to the problem formulation is a graph abstraction of the task space and the\npool of COAs itself to quantify its diversity. Formulating the COAs as a\ncentralized multi-robot task allocation problem, a genetic algorithm is used\nfor (order-ignoring) allocations of tasks to each agent that jointly maximize\ndiversity within the COA pool and overall compatibility of the agent-task\nmappings. A graph neural network is trained using a policy gradient approach to\nthen perform single agent task sequencing in each COA, which maximizes\ncompletion rates adaptive to task features. Our tests of the COA generation\nprocess in a simulated environment demonstrate significant performance gain\nover a random walk baseline, small optimality gap in task sequencing, and\nexecution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task\noperations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u8fdb\u5236\u4f18\u5316\u548c\u56fe\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u884c\u52a8\u65b9\u6848\uff08COA\uff09\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u707e\u96be\u54cd\u5e94\u3001\u641c\u6551\u548c\u519b\u4e8b\u4efb\u52a1\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u4efb\u52a1\u5206\u914d\u4e0e\u5e8f\u5217\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u591a\u6837\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u4efb\u52a1\uff08\u5982\u707e\u96be\u54cd\u5e94\u3001\u641c\u6551\u548c\u519b\u4e8b\u884c\u52a8\uff09\u9700\u8981\u81ea\u52a8\u5316\u751f\u6210\u591a\u6837\u5316\u7684\u884c\u52a8\u65b9\u6848\uff08COA\uff09\uff0c\u4ee5\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u548c\u667a\u80fd\u4f53\u80fd\u529b\u5dee\u5f02\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u591a\u6837\u6027\u548c\u4efb\u52a1\u517c\u5bb9\u6027\u9700\u6c42\uff0c\u4e9f\u9700\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u5de5\u5177\u3002", "method": "1. \u5c06\u4efb\u52a1\u7a7a\u95f4\u548cCOA\u6c60\u62bd\u8c61\u4e3a\u56fe\u7ed3\u6784\u4ee5\u91cf\u5316\u591a\u6837\u6027\uff1b2. \u5c06COA\u751f\u6210\u5efa\u6a21\u4e3a\u96c6\u4e2d\u5f0f\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff1b3. \u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4f18\u5316\u5355\u667a\u80fd\u4f53\u4efb\u52a1\u5e8f\u5217\uff0c\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u4efb\u52a1\u5e8f\u5217\u4f18\u5316\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u4e14\u80fd\u572850\u5206\u949f\u5185\u4e3a5\u4e2a\u667a\u80fd\u4f53/100\u4e2a\u4efb\u52a1\u7684\u64cd\u4f5c\u751f\u6210\u591a\u8fbe20\u4e2aCOA\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u751f\u6210\u591a\u6837\u4e14\u517c\u5bb9\u7684COA\uff0c\u9002\u7528\u4e8e\u590d\u6742\u591a\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u4e3a\u81ea\u52a8\u5316\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u4e8e\u4e8c\u8fdb\u5236\u4f18\u5316\u548c\u56fe\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u884c\u52a8\u65b9\u6848\u591a\u6837\u5316\u81ea\u52a8\u751f\u6210", "abstract_zh": "\u5728\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u7684\u707e\u96be\u54cd\u5e94\u3001\u641c\u6551\u548c\u519b\u4e8b\u4efb\u52a1\u4e2d\uff0c\u81ea\u52a8\u5316\u751f\u6210\u884c\u52a8\u65b9\u6848\uff08COA\uff09\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u73af\u5883\u53d8\u5316\uff08\u5982\u96e8\u96ea\u3001\u969c\u788d\u7269\u7b49\uff09\u53ef\u80fd\u5f71\u54cdCOA\u7684\u9884\u671f\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u751f\u6210\u4efb\u52a1\u5206\u914d\u591a\u6837\u5316\u7684COA\u6c60\u3002\u6b64\u5916\uff0c\u667a\u80fd\u4f53\u80fd\u529b\uff08\u5982\u4eba\u7c7b\u56e2\u961f\u6216\u81ea\u4e3b\u7cfb\u7edf\uff09\u7684\u5dee\u5f02\u4e3a\u89c4\u5212\u8fc7\u7a0b\u5e26\u6765\u673a\u9047\u548c\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u8f6f\u6027\u667a\u80fd\u4f53-\u4efb\u52a1\u517c\u5bb9\u6027\u53d8\u5316\u7684\u591a\u6837\u5316COA\u6c60\u3002\u5173\u952e\u662f\u5c06\u4efb\u52a1\u7a7a\u95f4\u548cCOA\u6c60\u62bd\u8c61\u4e3a\u56fe\u7ed3\u6784\u4ee5\u91cf\u5316\u591a\u6837\u6027\u3002\u901a\u8fc7\u5c06COA\u5efa\u6a21\u4e3a\u96c6\u4e2d\u5f0f\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u540c\u65f6\u6700\u5927\u5316COA\u6c60\u7684\u591a\u6837\u6027\u548c\u667a\u80fd\u4f53-\u4efb\u52a1\u6620\u5c04\u7684\u517c\u5bb9\u6027\u3002\u8fdb\u4e00\u6b65\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4f18\u5316\u5355\u667a\u80fd\u4f53\u4efb\u52a1\u5e8f\u5217\uff0c\u4ee5\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u3002\u6a21\u62df\u73af\u5883\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u4efb\u52a1\u5e8f\u5217\u4f18\u5316\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u4e14\u80fd\u572850\u5206\u949f\u5185\u4e3a5\u4e2a\u667a\u80fd\u4f53/100\u4e2a\u4efb\u52a1\u7684\u64cd\u4f5c\u751f\u6210\u591a\u8fbe20\u4e2aCOA\u3002"}}
{"id": "2506.20267", "pdf": "https://arxiv.org/pdf/2506.20267", "abs": "https://arxiv.org/abs/2506.20267", "authors": ["Fabian Bongratz", "Tom Nuno Wolf", "Jaume Gual Ramon", "Christian Wachinger"], "title": "X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "MICCAI 2025", "summary": "Interpretable models are crucial for supporting clinical decision-making,\ndriving advances in their development and application for medical images.\nHowever, the nature of 3D volumetric data makes it inherently challenging to\nvisualize and interpret intricate and complex structures like the cerebral\ncortex. Cortical surface renderings, on the other hand, provide a more\naccessible and understandable 3D representation of brain anatomy, facilitating\nvisualization and interactive exploration. Motivated by this advantage and the\nwidespread use of surface data for studying neurological disorders, we present\nthe eXplainable Surface Vision Transformer (X-SiT). This is the first\ninherently interpretable neural network that offers human-understandable\npredictions based on interpretable cortical features. As part of X-SiT, we\nintroduce a prototypical surface patch decoder for classifying surface patch\nembeddings, incorporating case-based reasoning with spatially corresponding\ncortical prototypes. The results demonstrate state-of-the-art performance in\ndetecting Alzheimer's disease and frontotemporal dementia while additionally\nproviding informative prototypes that align with known disease patterns and\nreveal classification errors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8868\u9762\u89c6\u89c9\u53d8\u6362\u5668\uff08X-SiT\uff09\uff0c\u7528\u4e8e\u75f4\u5446\u75c7\u8bca\u65ad\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u53ef\u89e3\u91ca\u76ae\u5c42\u7279\u5f81\u7684\u4eba\u7c7b\u53ef\u7406\u89e3\u9884\u6d4b\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u53ef\u89e3\u91ca\u6a21\u578b\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f463D\u4f53\u79ef\u6570\u636e\u7684\u590d\u6742\u6027\u4f7f\u5176\u96be\u4ee5\u53ef\u89c6\u5316\u3002\u76ae\u5c42\u8868\u9762\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u6613\u7406\u89e3\u7684\u8111\u89e3\u5256\u8868\u793a\uff0c\u56e0\u6b64\u4f5c\u8005\u5f00\u53d1\u4e86X-SiT\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u8bca\u65ad\u3002", "method": "X-SiT\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u4e86\u539f\u578b\u8868\u9762\u8865\u4e01\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u7a7a\u95f4\u5bf9\u5e94\u7684\u76ae\u5c42\u539f\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u3002", "result": "X-SiT\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u68c0\u6d4b\u4e2d\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u63d0\u4f9b\u4e0e\u5df2\u77e5\u75be\u75c5\u6a21\u5f0f\u4e00\u81f4\u7684\u539f\u578b\uff0c\u63ed\u793a\u4e86\u5206\u7c7b\u9519\u8bef\u3002", "conclusion": "X-SiT\u4e0d\u4ec5\u63d0\u5347\u4e86\u75f4\u5446\u75c7\u8bca\u65ad\u7684\u6027\u80fd\uff0c\u8fd8\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u539f\u578b\u589e\u5f3a\u4e86\u4e34\u5e8a\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "paper_title_zh": "X-SiT\uff1a\u7528\u4e8e\u75f4\u5446\u75c7\u8bca\u65ad\u7684\u56fa\u6709\u53ef\u89e3\u91ca\u8868\u9762\u89c6\u89c9\u53d8\u6362\u5668", "abstract_zh": "\u53ef\u89e3\u91ca\u6a21\u578b\u5bf9\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002\u7136\u800c\uff0c3D\u4f53\u79ef\u6570\u636e\u7684\u7279\u6027\u4f7f\u5176\u96be\u4ee5\u53ef\u89c6\u5316\u548c\u89e3\u91ca\u590d\u6742\u7ed3\u6784\uff08\u5982\u5927\u8111\u76ae\u5c42\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u76ae\u5c42\u8868\u9762\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u6613\u7406\u89e3\u548c\u4ea4\u4e92\u63a2\u7d22\u7684\u8111\u89e3\u52563D\u8868\u793a\u3002\u57fa\u4e8e\u8fd9\u4e00\u4f18\u52bf\u53ca\u8868\u9762\u6570\u636e\u5728\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u8868\u9762\u89c6\u89c9\u53d8\u6362\u5668\uff08X-SiT\uff09\u3002\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u53ef\u89e3\u91ca\u76ae\u5c42\u7279\u5f81\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u9884\u6d4b\u7684\u56fa\u6709\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u3002\u4f5c\u4e3aX-SiT\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u539f\u578b\u8868\u9762\u8865\u4e01\u89e3\u7801\u5668\uff0c\u7528\u4e8e\u5206\u7c7b\u8868\u9762\u8865\u4e01\u5d4c\u5165\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u548c\u7a7a\u95f4\u5bf9\u5e94\u7684\u76ae\u5c42\u539f\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0cX-SiT\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0e\u5df2\u77e5\u75be\u75c5\u6a21\u5f0f\u4e00\u81f4\u4e14\u63ed\u793a\u5206\u7c7b\u9519\u8bef\u7684\u4fe1\u606f\u539f\u578b\u3002"}}
{"id": "2506.20036", "pdf": "https://arxiv.org/pdf/2506.20036", "abs": "https://arxiv.org/abs/2506.20036", "authors": ["Jeremiah Coholich", "Muhammad Ali Murtaza", "Seth Hutchinson", "Zsolt Kira"], "title": "Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We propose a novel hierarchical reinforcement learning framework for\nquadruped locomotion over challenging terrain. Our approach incorporates a\ntwo-layer hierarchy in which a high-level policy (HLP) selects optimal goals\nfor a low-level policy (LLP). The LLP is trained using an on-policy\nactor-critic RL algorithm and is given footstep placements as goals. We propose\nan HLP that does not require any additional training or environment samples and\ninstead operates via an online optimization process over the learned value\nfunction of the LLP. We demonstrate the benefits of this framework by comparing\nit with an end-to-end reinforcement learning (RL) approach. We observe\nimprovements in its ability to achieve higher rewards with fewer collisions\nacross an array of different terrains, including terrains more difficult than\nany encountered during training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u3002\u901a\u8fc7\u9ad8\u5c42\u7b56\u7565\u9009\u62e9\u76ee\u6807\uff0c\u4f4e\u5c42\u7b56\u7565\u6267\u884c\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u4f18\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u6027\u80fd\u548c\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5206\u5c42\u6846\u67b6\u6765\u4f18\u5316\u8fd0\u52a8\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u9ad8\u5c42\u7b56\u7565\uff08HLP\uff09\u9009\u62e9\u76ee\u6807\uff0c\u4f4e\u5c42\u7b56\u7565\uff08LLP\uff09\u6267\u884c\u52a8\u4f5c\u3002LLP\u901a\u8fc7\u57fa\u4e8e\u7b56\u7565\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u8bad\u7ec3\uff0cHLP\u5219\u901a\u8fc7\u5728\u7ebf\u4f18\u5316LLP\u7684\u4ef7\u503c\u51fd\u6570\u6765\u64cd\u4f5c\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u4e0e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u591a\u79cd\u590d\u6742\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5956\u52b1\u548c\u66f4\u5c11\u7684\u78b0\u649e\uff0c\u751a\u81f3\u5728\u8bad\u7ec3\u4e2d\u672a\u9047\u5230\u7684\u5730\u5f62\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u9ad8\u5c42\u7b56\u7565\u7684\u76ee\u6807\u9009\u62e9\u548c\u4f4e\u5c42\u7b56\u7565\u7684\u52a8\u4f5c\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "paper_title_zh": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e0e\u4ef7\u503c\u4f18\u5316\u5728\u590d\u6742\u56db\u8db3\u8fd0\u52a8\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u53cc\u5c42\u7ed3\u6784\uff0c\u9ad8\u5c42\u7b56\u7565\uff08HLP\uff09\u4e3a\u4f4e\u5c42\u7b56\u7565\uff08LLP\uff09\u9009\u62e9\u6700\u4f18\u76ee\u6807\u3002LLP\u901a\u8fc7\u57fa\u4e8e\u7b56\u7565\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\uff0c\u5e76\u4ee5\u8db3\u90e8\u843d\u811a\u70b9\u4e3a\u76ee\u6807\u3002\u6211\u4eec\u63d0\u51fa\u7684HLP\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u73af\u5883\u6837\u672c\uff0c\u800c\u662f\u901a\u8fc7\u5728\u7ebf\u4f18\u5316LLP\u7684\u4ef7\u503c\u51fd\u6570\u6765\u64cd\u4f5c\u3002\u901a\u8fc7\u5c06\u5176\u4e0e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u52bf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u591a\u79cd\u4e0d\u540c\u5730\u5f62\uff08\u5305\u62ec\u8bad\u7ec3\u4e2d\u672a\u9047\u5230\u7684\u66f4\u590d\u6742\u5730\u5f62\uff09\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u7684\u5956\u52b1\u548c\u66f4\u5c11\u7684\u78b0\u649e\u3002"}}
{"id": "2506.20282", "pdf": "https://arxiv.org/pdf/2506.20282", "abs": "https://arxiv.org/abs/2506.20282", "authors": ["Jiaxing Huang", "Heng Guo", "Le Lu", "Fan Yang", "Minfeng Xu", "Ge Yang", "Wei Luo"], "title": "Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Osteoporosis, characterized by reduced bone mineral density (BMD) and\ncompromised bone microstructure, increases fracture risk in aging populations.\nWhile dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD\nassessment, its limited accessibility hinders diagnosis in resource-limited\nregions. Opportunistic computed tomography (CT) analysis has emerged as a\npromising alternative for osteoporosis diagnosis using existing imaging data.\nCurrent approaches, however, face three limitations: (1) underutilization of\nunlabeled vertebral data, (2) systematic bias from device-specific DXA\ndiscrepancies, and (3) insufficient integration of clinical knowledge such as\nspatial BMD distribution patterns. To address these, we propose a unified deep\nlearning framework with three innovations. First, a self-supervised learning\nmethod using radiomic representations to leverage unlabeled CT data and\npreserve bone texture. Second, a Mixture of Experts (MoE) architecture with\nlearned gating mechanisms to enhance cross-device adaptability. Third, a\nmulti-task learning framework integrating osteoporosis diagnosis, BMD\nregression, and vertebra location prediction. Validated across three clinical\nsites and an external hospital, our approach demonstrates superior\ngeneralizability and accuracy over existing methods for opportunistic\nosteoporosis screening and diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u591a\u4efb\u52a1\u96c6\u6210\uff0c\u89e3\u51b3\u9aa8\u8d28\u758f\u677e\u8bca\u65ad\u4e2d\u7684\u672a\u6807\u8bb0\u6570\u636e\u5229\u7528\u3001\u8bbe\u5907\u5dee\u5f02\u548c\u4e34\u5e8a\u77e5\u8bc6\u6574\u5408\u95ee\u9898\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9aa8\u8d28\u758f\u677e\u75c7\u5728\u8001\u9f84\u5316\u4eba\u7fa4\u4e2d\u98ce\u9669\u9ad8\uff0c\u4f46\u4f20\u7edfDXA\u68c0\u6d4b\u8d44\u6e90\u6709\u9650\u3002\u73b0\u6709CT\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u672a\u6807\u8bb0\u6570\u636e\u5229\u7528\u4e0d\u8db3\u3001\u8bbe\u5907\u5dee\u5f02\u5bfc\u81f4\u7684\u7cfb\u7edf\u504f\u5dee\u548c\u4e34\u5e8a\u77e5\u8bc6\u6574\u5408\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u81ea\u76d1\u7763\u5b66\u4e60\u5229\u7528\u672a\u6807\u8bb0CT\u6570\u636e\u5e76\u4fdd\u7559\u9aa8\u7eb9\u7406\uff1b2\uff09\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u589e\u5f3a\u8de8\u8bbe\u5907\u9002\u5e94\u6027\uff1b3\uff09\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u6574\u5408\u9aa8\u8d28\u758f\u677e\u8bca\u65ad\u3001BMD\u56de\u5f52\u548c\u690e\u9aa8\u5b9a\u4f4d\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u4e34\u5e8a\u7ad9\u70b9\u548c\u5916\u90e8\u533b\u9662\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9aa8\u8d28\u758f\u677e\u7b5b\u67e5\u548c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u6280\u672f\u89e3\u51b3\u4e86\u9aa8\u8d28\u758f\u677e\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u7eb9\u7406\u4fdd\u7559\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u4e13\u5bb6\u6df7\u5408\u4e0e\u591a\u4efb\u52a1\u96c6\u6210\u7684\u673a\u4f1a\u6027\u9aa8\u8d28\u758f\u677e\u8bca\u65ad", "abstract_zh": "\u9aa8\u8d28\u758f\u677e\u75c7\u4ee5\u9aa8\u77ff\u7269\u8d28\u5bc6\u5ea6\uff08BMD\uff09\u964d\u4f4e\u548c\u9aa8\u5fae\u7ed3\u6784\u53d7\u635f\u4e3a\u7279\u5f81\uff0c\u589e\u52a0\u4e86\u8001\u9f84\u5316\u4eba\u7fa4\u7684\u9aa8\u6298\u98ce\u9669\u3002\u5c3d\u7ba1\u53cc\u80fdX\u7ebf\u5438\u6536\u6cd5\uff08DXA\uff09\u662fBMD\u8bc4\u4f30\u7684\u4e34\u5e8a\u6807\u51c6\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u7528\u6027\u963b\u788d\u4e86\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u8bca\u65ad\u3002\u673a\u4f1a\u6027\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u5206\u6790\u6210\u4e3a\u5229\u7528\u73b0\u6709\u5f71\u50cf\u6570\u636e\u8fdb\u884c\u9aa8\u8d28\u758f\u677e\u8bca\u65ad\u7684\u6709\u524d\u666f\u66ff\u4ee3\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5c40\u9650\u6027\uff1a1\uff09\u672a\u6807\u8bb0\u690e\u9aa8\u6570\u636e\u5229\u7528\u4e0d\u8db3\uff1b2\uff09\u8bbe\u5907\u7279\u5b9aDXA\u5dee\u5f02\u5bfc\u81f4\u7684\u7cfb\u7edf\u504f\u5dee\uff1b3\uff09\u4e34\u5e8a\u77e5\u8bc6\uff08\u5982\u7a7a\u95f4BMD\u5206\u5e03\u6a21\u5f0f\uff09\u6574\u5408\u4e0d\u8db3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a\u9996\u5148\uff0c\u5229\u7528\u653e\u5c04\u7ec4\u5b66\u8868\u793a\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4ee5\u5229\u7528\u672a\u6807\u8bb0CT\u6570\u636e\u5e76\u4fdd\u7559\u9aa8\u7eb9\u7406\uff1b\u5176\u6b21\uff0c\u91c7\u7528\u5177\u6709\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\u4ee5\u589e\u5f3a\u8de8\u8bbe\u5907\u9002\u5e94\u6027\uff1b\u7b2c\u4e09\uff0c\u96c6\u6210\u9aa8\u8d28\u758f\u677e\u8bca\u65ad\u3001BMD\u56de\u5f52\u548c\u690e\u9aa8\u5b9a\u4f4d\u9884\u6d4b\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u3002\u5728\u4e09\u4e2a\u4e34\u5e8a\u7ad9\u70b9\u548c\u5916\u90e8\u533b\u9662\u7684\u9a8c\u8bc1\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u673a\u4f1a\u6027\u9aa8\u8d28\u758f\u677e\u7b5b\u67e5\u548c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.20039", "pdf": "https://arxiv.org/pdf/2506.20039", "abs": "https://arxiv.org/abs/2506.20039", "authors": ["Koorosh Moslemi", "Chi-Guhn Lee"], "title": "Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "comment": "Accepted to the 2nd Coordination and Cooperation in Multi-Agent\n  Reinforcement Learning (CoCoMARL) Workshop at RLC 2025", "summary": "Team formation and the dynamics of team-based learning have drawn significant\ninterest in the context of Multi-Agent Reinforcement Learning (MARL). However,\nexisting studies primarily focus on unilateral groupings, predefined teams, or\nfixed-population settings, leaving the effects of algorithmic bilateral\ngrouping choices in dynamic populations underexplored. To address this gap, we\nintroduce a framework for learning two-sided team formation in dynamic\nmulti-agent systems. Through this study, we gain insight into what algorithmic\nproperties in bilateral team formation influence policy performance and\ngeneralization. We validate our approach using widely adopted multi-agent\nscenarios, demonstrating competitive performance and improved generalization in\nmost scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b66\u4e60\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u53cc\u8fb9\u5206\u7ec4\u9009\u62e9\u5728\u52a8\u6001\u7fa4\u4f53\u4e2d\u5f71\u54cd\u7684\u7a7a\u767d\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8fb9\u5206\u7ec4\u3001\u9884\u5b9a\u4e49\u56e2\u961f\u6216\u56fa\u5b9a\u7fa4\u4f53\u8bbe\u7f6e\uff0c\u800c\u53cc\u8fb9\u5206\u7ec4\u9009\u62e9\u5728\u52a8\u6001\u7fa4\u4f53\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b66\u4e60\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u7684\u7b97\u6cd5\u7279\u6027\u5bf9\u7b56\u7565\u6027\u80fd\u548c\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5e7f\u6cdb\u91c7\u7528\u7684\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u7ade\u4e89\u6027\u6027\u80fd\u548c\u5927\u591a\u6570\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u6846\u67b6\u4e3a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u56e2\u961f\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6027\u80fd\u548c\u6cdb\u5316\u65b9\u9762\u7684\u4f18\u52bf\u3002", "paper_title_zh": "\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u7684\u5b66\u4e60", "abstract_zh": "\u56e2\u961f\u5f62\u6210\u548c\u56e2\u961f\u5b66\u4e60\u52a8\u6001\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u8fb9\u5206\u7ec4\u3001\u9884\u5b9a\u4e49\u56e2\u961f\u6216\u56fa\u5b9a\u7fa4\u4f53\u8bbe\u7f6e\uff0c\u53cc\u8fb9\u5206\u7ec4\u9009\u62e9\u5728\u52a8\u6001\u7fa4\u4f53\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b66\u4e60\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u7684\u6846\u67b6\u3002\u901a\u8fc7\u672c\u7814\u7a76\uff0c\u6211\u4eec\u6df1\u5165\u4e86\u89e3\u4e86\u53cc\u8fb9\u56e2\u961f\u5f62\u6210\u4e2d\u54ea\u4e9b\u7b97\u6cd5\u7279\u6027\u5f71\u54cd\u7b56\u7565\u6027\u80fd\u548c\u6cdb\u5316\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u91c7\u7528\u7684\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u7684\u7ade\u4e89\u6027\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2506.20040", "pdf": "https://arxiv.org/pdf/2506.20040", "abs": "https://arxiv.org/abs/2506.20040", "authors": ["Ankur Garg", "Xuemin Yu", "Hassan Sajjad", "Samira Ebrahimi Kahou"], "title": "Cross-Layer Discrete Concept Discovery for Interpreting Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Uncovering emergent concepts across transformer layers remains a significant\nchallenge because the residual stream linearly mixes and duplicates\ninformation, obscuring how features evolve within large language models.\nCurrent research efforts primarily inspect neural representations at single\nlayers, thereby overlooking this cross-layer superposition and the redundancy\nit introduces. These representations are typically either analyzed directly for\nactivation patterns or passed to probing classifiers that map them to a limited\nset of predefined concepts. To address these limitations, we propose\n\\gls{clvqvae}, a framework that uses vector quantization to map representations\nacross layers and in the process collapse duplicated residual-stream features\ninto compact, interpretable concept vectors. Our approach uniquely combines\ntop-$k$ temperature-based sampling during quantization with EMA codebook\nupdates, providing controlled exploration of the discrete latent space while\nmaintaining code-book diversity. We further enhance the framework with\nscaled-spherical k-means++ for codebook initialization, which clusters by\ndirectional similarity rather than magnitude, better aligning with semantic\nstructure in word embedding space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5c42\u79bb\u6563\u6982\u5ff5\u53d1\u73b0\u6846\u67b6CLVQVAE\uff0c\u7528\u4e8e\u89e3\u6784\u8bed\u8a00\u6a21\u578b\u4e2d\u5404\u5c42\u7279\u5f81\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u548c\u6e29\u5ea6\u91c7\u6837\u6280\u672f\uff0c\u5c06\u5197\u4f59\u7279\u5f81\u538b\u7f29\u4e3a\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5411\u91cf\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u5ffd\u89c6\u4e86\u8de8\u5c42\u7279\u5f81\u7684\u53e0\u52a0\u548c\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7279\u5f81\u6f14\u5316\u7684\u7406\u89e3\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u8de8\u5c42\u5206\u6790\u5e76\u538b\u7f29\u5197\u4f59\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faCLVQVAE\u6846\u67b6\uff0c\u7ed3\u5408\u5411\u91cf\u91cf\u5316\u548c\u6e29\u5ea6\u91c7\u6837\u6280\u672f\uff0c\u5229\u7528EMA\u4ee3\u7801\u672c\u66f4\u65b0\u548c\u65b9\u5411\u76f8\u4f3c\u6027\u805a\u7c7b\uff08scaled-spherical k-means++\uff09\uff0c\u5c06\u8de8\u5c42\u7279\u5f81\u538b\u7f29\u4e3a\u79bb\u6563\u7684\u6982\u5ff5\u5411\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCLVQVAE\u80fd\u591f\u6709\u6548\u89e3\u6784\u8bed\u8a00\u6a21\u578b\u4e2d\u5404\u5c42\u7279\u5f81\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u751f\u6210\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5411\u91cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u5355\u5c42\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "CLVQVAE\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7279\u5f81\u6f14\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u5c42\u5206\u6790\u548c\u538b\u7f29\u5197\u4f59\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "paper_title_zh": "\u8de8\u5c42\u79bb\u6563\u6982\u5ff5\u53d1\u73b0\uff1a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76", "abstract_zh": "\u63ed\u793a\u8de8\u53d8\u538b\u5668\u5c42\u7684\u6d8c\u73b0\u6982\u5ff5\u4ecd\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u6b8b\u5dee\u6d41\u7ebf\u6027\u6df7\u5408\u5e76\u590d\u5236\u4fe1\u606f\uff0c\u6a21\u7cca\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7279\u5f81\u7684\u6f14\u5316\u8fc7\u7a0b\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u68c0\u67e5\u5355\u5c42\u7684\u795e\u7ecf\u8868\u793a\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u8fd9\u79cd\u8de8\u5c42\u53e0\u52a0\u53ca\u5176\u5f15\u5165\u7684\u5197\u4f59\u3002\u8fd9\u4e9b\u8868\u793a\u901a\u5e38\u76f4\u63a5\u5206\u6790\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u6216\u4f20\u9012\u7ed9\u63a2\u6d4b\u5206\u7c7b\u5668\u4ee5\u6620\u5c04\u5230\u6709\u9650\u7684\u9884\u5b9a\u4e49\u6982\u5ff5\u96c6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51faCLVQVAE\u6846\u67b6\uff0c\u5229\u7528\u5411\u91cf\u91cf\u5316\u8de8\u5c42\u6620\u5c04\u8868\u793a\uff0c\u5e76\u5728\u8fc7\u7a0b\u4e2d\u5c06\u91cd\u590d\u7684\u6b8b\u5dee\u6d41\u7279\u5f81\u538b\u7f29\u4e3a\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5411\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u91cf\u5316\u8fc7\u7a0b\u4e2d\u72ec\u7279\u5730\u7ed3\u5408\u4e86\u57fa\u4e8e\u6e29\u5ea6\u7684top-k\u91c7\u6837\u4e0eEMA\u4ee3\u7801\u672c\u66f4\u65b0\uff0c\u63d0\u4f9b\u4e86\u5bf9\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u7684\u53ef\u63a7\u63a2\u7d22\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u672c\u591a\u6837\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u901a\u8fc7\u65b9\u5411\u76f8\u4f3c\u6027\u805a\u7c7b\uff08scaled-spherical k-means++\uff09\u521d\u59cb\u5316\u4ee3\u7801\u672c\uff0c\u66f4\u597d\u5730\u4e0e\u8bcd\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u7ed3\u6784\u5bf9\u9f50\u3002"}}
{"id": "2506.20305", "pdf": "https://arxiv.org/pdf/2506.20305", "abs": "https://arxiv.org/abs/2506.20305", "authors": ["Kazuki Yoda", "Kazuhiko Kawamoto", "Hiroshi Kera"], "title": "Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 13 figures", "summary": "The hardness of learning a function that attains a target task relates to its\ninput-sensitivity. For example, image classification tasks are\ninput-insensitive as minor corruptions should not affect the classification\nresults, whereas arithmetic and symbolic computation, which have been recently\nattracting interest, are highly input-sensitive as each input variable connects\nto the computation results. This study presents the first learning-based Quick\nResponse (QR) code decoding and investigates learning functions of medium\nsensitivity. Our experiments reveal that Transformers can successfully decode\nQR codes, even beyond the theoretical error-correction limit, by learning the\nstructure of embedded texts. They generalize from English-rich training data to\nother languages and even random strings. Moreover, we observe that the\nTransformer-based QR decoder focuses on data bits while ignoring\nerror-correction bits, suggesting a decoding mechanism distinct from standard\nQR code readers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e2d\u7b49\u8f93\u5165\u654f\u611f\u6027\u7684\u51fd\u6570\u5b66\u4e60\uff0c\u4ee5QR\u7801\u89e3\u7801\u4e3a\u4f8b\uff0c\u53d1\u73b0Transformer\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u5d4c\u5165\u6587\u672c\u7ed3\u6784\u6210\u529f\u89e3\u7801QR\u7801\uff0c\u751a\u81f3\u8d85\u8d8a\u7406\u8bba\u7ea0\u9519\u6781\u9650\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e2d\u7b49\u8f93\u5165\u654f\u611f\u6027\u7684\u51fd\u6570\u5b66\u4e60\uff0c\u5c24\u5176\u662fQR\u7801\u89e3\u7801\u4efb\u52a1\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u8f93\u5165\u654f\u611f\u6027\u8c31\u7cfb\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528Transformer\u6a21\u578b\u8fdb\u884cQR\u7801\u89e3\u7801\uff0c\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u5b66\u4e60\u5d4c\u5165\u6587\u672c\u7684\u7ed3\u6784\uff0c\u5e76\u6d4b\u8bd5\u5176\u5728\u591a\u79cd\u8bed\u8a00\u548c\u968f\u673a\u5b57\u7b26\u4e32\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTransformer\u80fd\u591f\u6210\u529f\u89e3\u7801QR\u7801\uff0c\u751a\u81f3\u8d85\u8d8a\u7406\u8bba\u7ea0\u9519\u6781\u9650\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u8bed\u8a00\u548c\u968f\u673a\u5b57\u7b26\u4e32\u3002\u6b64\u5916\uff0c\u6a21\u578b\u66f4\u5173\u6ce8\u6570\u636e\u4f4d\u800c\u975e\u7ea0\u9519\u4f4d\u3002", "conclusion": "\u7814\u7a76\u8868\u660eTransformer\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4e2d\u7b49\u8f93\u5165\u654f\u611f\u6027\u7684\u51fd\u6570\uff0c\u4e3a\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5b66\u4e60\u4e2d\u7b49\u8f93\u5165\u654f\u611f\u6027\u7684\u51fd\u6570\uff1a\u4ee5QR\u7801\u89e3\u7801\u4e3a\u4f8b", "abstract_zh": "\u5b66\u4e60\u4e00\u4e2a\u5b9e\u73b0\u76ee\u6807\u4efb\u52a1\u7684\u51fd\u6570\u7684\u96be\u5ea6\u4e0e\u5176\u8f93\u5165\u654f\u611f\u6027\u76f8\u5173\u3002\u4f8b\u5982\uff0c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5bf9\u8f93\u5165\u4e0d\u654f\u611f\uff0c\u56e0\u4e3a\u8f7b\u5fae\u7684\u5e72\u6270\u4e0d\u5e94\u5f71\u54cd\u5206\u7c7b\u7ed3\u679c\uff1b\u800c\u7b97\u672f\u548c\u7b26\u53f7\u8ba1\u7b97\u5219\u5bf9\u8f93\u5165\u9ad8\u5ea6\u654f\u611f\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u8f93\u5165\u53d8\u91cf\u90fd\u4e0e\u8ba1\u7b97\u7ed3\u679c\u76f8\u5173\u3002\u672c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u5feb\u901f\u54cd\u5e94\uff08QR\uff09\u7801\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e2d\u7b49\u654f\u611f\u6027\u51fd\u6570\u7684\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTransformer\u901a\u8fc7\u5b66\u4e60\u5d4c\u5165\u6587\u672c\u7684\u7ed3\u6784\uff0c\u80fd\u591f\u6210\u529f\u89e3\u7801QR\u7801\uff0c\u751a\u81f3\u8d85\u8d8a\u7406\u8bba\u7ea0\u9519\u6781\u9650\u3002\u5b83\u4eec\u80fd\u591f\u4ece\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684\u8bad\u7ec3\u6570\u636e\u6cdb\u5316\u5230\u5176\u4ed6\u8bed\u8a00\u751a\u81f3\u968f\u673a\u5b57\u7b26\u4e32\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u57fa\u4e8eTransformer\u7684QR\u89e3\u7801\u5668\u66f4\u5173\u6ce8\u6570\u636e\u4f4d\u800c\u5ffd\u7565\u7ea0\u9519\u4f4d\uff0c\u8fd9\u8868\u660e\u5176\u89e3\u7801\u673a\u5236\u4e0e\u6807\u51c6QR\u7801\u9605\u8bfb\u5668\u4e0d\u540c\u3002"}}
{"id": "2506.20041", "pdf": "https://arxiv.org/pdf/2506.20041", "abs": "https://arxiv.org/abs/2506.20041", "authors": ["Soheil Abadifard", "Fazli Can"], "title": "LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "The classification of imbalanced data streams, which have unequal class\ndistributions, is a key difficulty in machine learning, especially when dealing\nwith multiple classes. While binary imbalanced data stream classification tasks\nhave received considerable attention, only a few studies have focused on\nmulti-class imbalanced data streams. Effectively managing the dynamic imbalance\nratio is a key challenge in this domain. This study introduces a novel, robust,\nand resilient approach to address these challenges by integrating Locality\nSensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic\nEnsemble Diversification (DynED) framework. To the best of our knowledge, we\npresent the first application of LSH-RHP for undersampling in the context of\nimbalanced non-stationary data streams. The proposed method undersamples the\nmajority classes by utilizing LSH-RHP, provides a balanced training set, and\nimproves the ensemble's prediction performance. We conduct comprehensive\nexperiments on 23 real-world and ten semi-synthetic datasets and compare\nLSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED\noutperforms other approaches in terms of both Kappa and mG-Mean effectiveness\nmeasures, demonstrating its capability in dealing with multi-class imbalanced\nnon-stationary data streams. Notably, LSH-DynED performs well in large-scale,\nhigh-dimensional datasets with considerable class imbalances and demonstrates\nadaptation and robustness in real-world circumstances. To motivate our design,\nwe review existing methods for imbalanced data streams, outline key challenges,\nand offer guidance for future work. For the reproducibility of our results, we\nhave made our implementation available on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLSH-DynED\u7684\u52a8\u6001\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\uff08LSH\uff09\u7684\u6b20\u91c7\u6837\u6280\u672f\u5904\u7406\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u5206\u7c7b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u7684\u5206\u7c7b\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u96be\u9898\uff0c\u5c24\u5176\u662f\u52a8\u6001\u4e0d\u5e73\u8861\u6bd4\u7684\u5904\u7406\u3002\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u800c\u5bf9\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u7684\u5173\u6ce8\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u7ed3\u5408\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u4e0e\u968f\u673a\u8d85\u5e73\u9762\u6295\u5f71\uff08LSH-RHP\uff09\u6280\u672f\uff0c\u63d0\u51fa\u52a8\u6001\u96c6\u6210\u591a\u6837\u5316\uff08DynED\uff09\u6846\u67b6\u3002\u901a\u8fc7LSH-RHP\u5bf9\u591a\u6570\u7c7b\u8fdb\u884c\u6b20\u91c7\u6837\uff0c\u751f\u6210\u5e73\u8861\u8bad\u7ec3\u96c6\uff0c\u4ece\u800c\u63d0\u5347\u96c6\u6210\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u572823\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c10\u4e2a\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSH-DynED\u5728Kappa\u548cmG-Mean\u6307\u6807\u4e0a\u4f18\u4e8e15\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u548c\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LSH-DynED\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u975e\u5e73\u7a33\u6570\u636e\u6d41\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "LSH-DynED\uff1a\u4e00\u79cd\u57fa\u4e8eLSH\u6b20\u91c7\u6837\u7684\u52a8\u6001\u96c6\u6210\u6846\u67b6\u7528\u4e8e\u6f14\u5316\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u7c7b", "abstract_zh": "\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u7684\u5206\u7c7b\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u96be\u9898\uff0c\u5c24\u5176\u662f\u5728\u591a\u7c7b\u522b\u60c5\u51b5\u4e0b\u3002\u5c3d\u7ba1\u4e8c\u5206\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u4efb\u52a1\u5df2\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u7684\u7814\u7a76\u8f83\u5c11\u3002\u52a8\u6001\u4e0d\u5e73\u8861\u6bd4\u7684\u6709\u6548\u7ba1\u7406\u662f\u8be5\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u4e0e\u968f\u673a\u8d85\u5e73\u9762\u6295\u5f71\uff08LSH-RHP\uff09\u96c6\u6210\u5230\u52a8\u6001\u96c6\u6210\u591a\u6837\u5316\uff08DynED\uff09\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5c06LSH-RHP\u7528\u4e8e\u4e0d\u5e73\u8861\u975e\u5e73\u7a33\u6570\u636e\u6d41\u7684\u6b20\u91c7\u6837\u3002\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7LSH-RHP\u5bf9\u591a\u6570\u7c7b\u8fdb\u884c\u6b20\u91c7\u6837\uff0c\u63d0\u4f9b\u5e73\u8861\u7684\u8bad\u7ec3\u96c6\uff0c\u5e76\u63d0\u5347\u96c6\u6210\u9884\u6d4b\u6027\u80fd\u3002\u6211\u4eec\u572823\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c10\u4e2a\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\uff0c\u5e76\u5c06LSH-DynED\u4e0e15\u79cd\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0cLSH-DynED\u5728Kappa\u548cmG-Mean\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u975e\u5e73\u7a33\u6570\u636e\u6d41\u65b9\u9762\u7684\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLSH-DynED\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u548c\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u4e3a\u6fc0\u52b1\u8bbe\u8ba1\uff0c\u6211\u4eec\u56de\u987e\u4e86\u73b0\u6709\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u4e3a\u786e\u4fdd\u7ed3\u679c\u53ef\u590d\u73b0\uff0c\u6211\u4eec\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u4e86\u5b9e\u73b0\u4ee3\u7801\u3002"}}
{"id": "2506.20333", "pdf": "https://arxiv.org/pdf/2506.20333", "abs": "https://arxiv.org/abs/2506.20333", "authors": ["Jiayan Chen", "Kai Li", "Yulu Zhao", "Jianqiang Huang", "Zhan Wang"], "title": "EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hepatic echinococcosis (HE) is a widespread parasitic disease in\nunderdeveloped pastoral areas with limited medical resources. While CNN-based\nand Transformer-based models have been widely applied to medical image\nsegmentation, CNNs lack global context modeling due to local receptive fields,\nand Transformers, though capable of capturing long-range dependencies, are\ncomputationally expensive. Recently, state space models (SSMs), such as Mamba,\nhave gained attention for their ability to model long sequences with linear\ncomplexity. In this paper, we propose EAGLE, a U-shaped network composed of a\nProgressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space\n(HVSS) decoder that work collaboratively to achieve efficient and accurate\nsegmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional\nVision State Space Block (CVSSB) module is designed to fuse local and global\nfeatures, while the Haar Wavelet Transformation Block (HWTB) module compresses\nspatial information into the channel dimension to enable lossless downsampling.\nDue to the lack of publicly available HE datasets, we collected CT slices from\n260 patients at a local hospital. Experimental results show that EAGLE achieves\nstate-of-the-art performance with a Dice Similarity Coefficient (DSC) of\n89.76%, surpassing MSVM-UNet by 1.61%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAGLE\u7684\u9ad8\u6548\u5168\u5c40\u6ce8\u610f\u529b\u75c5\u7076\u5206\u5272\u6a21\u578b\uff0c\u7528\u4e8e\u809d\u5305\u866b\u75c5\uff08HE\uff09\u7684\u7cbe\u786e\u5206\u5272\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u6e10\u8fdb\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\uff08PVSS\uff09\u7f16\u7801\u5668\u548c\u6df7\u5408\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\uff08HVSS\uff09\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u878d\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u809d\u5305\u866b\u75c5\uff08HE\uff09\u5728\u533b\u7597\u8d44\u6e90\u532e\u4e4f\u7684\u7267\u533a\u5e7f\u6cdb\u6d41\u884c\u3002\u73b0\u6709\u7684CNN\u548cTransformer\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1aCNN\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u800cTransformer\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u5efa\u6a21\u957f\u5e8f\u5217\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "EAGLE\u91c7\u7528U\u5f62\u7f51\u7edc\u7ed3\u6784\uff0c\u5305\u542bPVSS\u7f16\u7801\u5668\u548cHVSS\u89e3\u7801\u5668\u3002\u8bbe\u8ba1\u4e86\u5377\u79ef\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u5757\uff08CVSSB\uff09\u878d\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u901a\u8fc7Haar\u5c0f\u6ce2\u53d8\u6362\u5757\uff08HWTB\uff09\u5b9e\u73b0\u65e0\u635f\u4e0b\u91c7\u6837\u3002", "result": "\u5728260\u540d\u60a3\u8005\u7684CT\u5207\u7247\u6570\u636e\u96c6\u4e0a\uff0cEAGLE\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u8fbe\u523089.76%\uff0c\u4f18\u4e8eMSVM-UNet 1.61%\u3002", "conclusion": "EAGLE\u901a\u8fc7\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u9ad8\u6548\u6027\u548cU\u5f62\u7f51\u7edc\u7684\u5206\u5272\u80fd\u529b\uff0c\u4e3a\u809d\u5305\u866b\u75c5\u75c5\u7076\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "EAGLE\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u5168\u5c40\u6ce8\u610f\u529b\u809d\u5305\u866b\u75c5\u75c5\u7076\u5206\u5272\u6a21\u578b", "abstract_zh": "\u809d\u5305\u866b\u75c5\uff08HE\uff09\u662f\u533b\u7597\u8d44\u6e90\u532e\u4e4f\u7267\u533a\u4e2d\u5e7f\u6cdb\u6d41\u884c\u7684\u5bc4\u751f\u866b\u75c5\u3002\u5c3d\u7ba1\u57fa\u4e8eCNN\u548cTransformer\u7684\u6a21\u578b\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u4f46CNN\u56e0\u5c40\u90e8\u611f\u53d7\u91ce\u800c\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u800cTransformer\u867d\u80fd\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u6700\u8fd1\uff0c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff0c\u5982Mamba\uff09\u56e0\u5176\u80fd\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u5efa\u6a21\u957f\u5e8f\u5217\u800c\u53d7\u5230\u5173\u6ce8\u3002\u672c\u6587\u63d0\u51faEAGLE\uff0c\u4e00\u79cd\u7531\u6e10\u8fdb\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\uff08PVSS\uff09\u7f16\u7801\u5668\u548c\u6df7\u5408\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\uff08HVSS\uff09\u89e3\u7801\u5668\u7ec4\u6210\u7684U\u5f62\u7f51\u7edc\uff0c\u534f\u540c\u5b9e\u73b0\u809d\u5305\u866b\u75c5\uff08HE\uff09\u75c5\u7076\u7684\u9ad8\u6548\u7cbe\u786e\u5206\u5272\u3002\u63d0\u51fa\u7684\u5377\u79ef\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u5757\uff08CVSSB\uff09\u6a21\u5757\u7528\u4e8e\u878d\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u800cHaar\u5c0f\u6ce2\u53d8\u6362\u5757\uff08HWTB\uff09\u6a21\u5757\u5c06\u7a7a\u95f4\u4fe1\u606f\u538b\u7f29\u81f3\u901a\u9053\u7ef4\u5ea6\u4ee5\u5b9e\u73b0\u65e0\u635f\u4e0b\u91c7\u6837\u3002\u7531\u4e8e\u7f3a\u4e4f\u516c\u5f00\u7684HE\u6570\u636e\u96c6\uff0c\u6211\u4eec\u4ece\u5f53\u5730\u533b\u9662\u6536\u96c6\u4e86260\u540d\u60a3\u8005\u7684CT\u5207\u7247\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEAGLE\u4ee589.76%\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8eMSVM-UNet 1.61%\u3002"}}
{"id": "2506.20046", "pdf": "https://arxiv.org/pdf/2506.20046", "abs": "https://arxiv.org/abs/2506.20046", "authors": ["Hirad Daneshvar", "Reza Samavi"], "title": "GNN's Uncertainty Quantification using Self-Distillation", "categories": ["cs.LG", "cs.AI"], "comment": "The paper has been accepted in the International Conference on AI in\n  Healthcare (AIiH) 2025 and will appear in the conference proceedings", "summary": "Graph Neural Networks (GNNs) have shown remarkable performance in the\nhealthcare domain. However, what remained challenging is quantifying the\npredictive uncertainty of GNNs, which is an important aspect of trustworthiness\nin clinical settings. While Bayesian and ensemble methods can be used to\nquantify uncertainty, they are computationally expensive. Additionally, the\ndisagreement metric used by ensemble methods to compute uncertainty cannot\ncapture the diversity of models in an ensemble network. In this paper, we\npropose a novel method, based on knowledge distillation, to quantify GNNs'\nuncertainty more efficiently and with higher precision. We apply\nself-distillation, where the same network serves as both the teacher and\nstudent models, thereby avoiding the need to train several networks\nindependently. To ensure the impact of self-distillation, we develop an\nuncertainty metric that captures the diverse nature of the network by assigning\ndifferent weights to each GNN classifier. We experimentally evaluate the\nprecision, performance, and ability of our approach in distinguishing\nout-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The\nevaluation results demonstrate that the proposed method can effectively capture\nthe predictive uncertainty of the model while having performance similar to\nthat of the MC Dropout and ensemble methods. The code is publicly available at\nhttps://github.com/tailabTMU/UQ_GNN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u84b8\u998f\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u7f51\u7edc\u540c\u65f6\u4f5c\u4e3a\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\uff0c\u9ad8\u6548\u4e14\u7cbe\u51c6\u5730\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u533b\u7597\u9886\u57df\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u4ecd\u5177\u6311\u6218\u6027\uff0c\u800c\u4f20\u7edf\u8d1d\u53f6\u65af\u548c\u96c6\u6210\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6a21\u578b\u591a\u6837\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7cbe\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u84b8\u998f\u6280\u672f\uff0c\u540c\u4e00\u7f51\u7edc\u65e2\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u53c8\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\uff0c\u907f\u514d\u4e86\u72ec\u7acb\u8bad\u7ec3\u591a\u4e2a\u7f51\u7edc\u7684\u9700\u6c42\u3002\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2aGNN\u5206\u7c7b\u5668\u5206\u914d\u4e0d\u540c\u6743\u91cd\uff0c\u4ee5\u6355\u6349\u7f51\u7edc\u7684\u591a\u6837\u6027\u3002", "result": "\u5728MIMIC-IV\u548cEnzymes\u4e24\u4e2a\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u6027\u80fd\u4e0eMC Dropout\u548c\u96c6\u6210\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\u4e3aGNNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u7b49\u9700\u8981\u9ad8\u53ef\u4fe1\u5ea6\u7684\u573a\u666f\u3002", "paper_title_zh": "\u57fa\u4e8e\u81ea\u84b8\u998f\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "abstract_zh": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u533b\u7597\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u4ecd\u5177\u6311\u6218\u6027\uff0c\u8fd9\u662f\u4e34\u5e8a\u73af\u5883\u4e2d\u53ef\u4fe1\u5ea6\u7684\u91cd\u8981\u65b9\u9762\u3002\u867d\u7136\u8d1d\u53f6\u65af\u548c\u96c6\u6210\u65b9\u6cd5\u53ef\u7528\u4e8e\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u6b64\u5916\uff0c\u96c6\u6210\u65b9\u6cd5\u7528\u4e8e\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\u7684\u5206\u6b67\u5ea6\u91cf\u65e0\u6cd5\u6355\u6349\u96c6\u6210\u7f51\u7edc\u4e2d\u6a21\u578b\u7684\u591a\u6837\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u66f4\u9ad8\u6548\u4e14\u7cbe\u51c6\u5730\u91cf\u5316GNNs\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u6211\u4eec\u91c7\u7528\u81ea\u84b8\u998f\u6280\u672f\uff0c\u540c\u4e00\u7f51\u7edc\u65e2\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u53c8\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\uff0c\u4ece\u800c\u907f\u514d\u72ec\u7acb\u8bad\u7ec3\u591a\u4e2a\u7f51\u7edc\u3002\u4e3a\u786e\u4fdd\u81ea\u84b8\u998f\u7684\u6548\u679c\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2aGNN\u5206\u7c7b\u5668\u5206\u914d\u4e0d\u540c\u6743\u91cd\u6765\u6355\u6349\u7f51\u7edc\u7684\u591a\u6837\u6027\u3002\u6211\u4eec\u5728MIMIC-IV\u548cEnzymes\u4e24\u4e2a\u56fe\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3001\u6027\u80fd\u53ca\u5176\u5728\u533a\u5206\u5206\u5e03\u5916\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u6a21\u578b\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u6027\u80fd\u4e0eMC Dropout\u548c\u96c6\u6210\u65b9\u6cd5\u76f8\u5f53\u3002\u4ee3\u7801\u516c\u5f00\u4e8ehttps://github.com/tailabTMU/UQ_GNN\u3002"}}
{"id": "2506.20355", "pdf": "https://arxiv.org/pdf/2506.20355", "abs": "https://arxiv.org/abs/2506.20355", "authors": ["Jes\u00fas Lozano-Cruz", "Albert Nieto-Morales", "Oriol Ball\u00f3-Gimbernat", "Adan Garriga", "Ant\u00f3n Rodr\u00edguez-Otero", "Alejandro Borrallo-Rentero"], "title": "Practical insights on the effect of different encodings, ans\u00e4tze and measurements in quantum and hybrid convolutional neural networks", "categories": ["quant-ph", "cs.CV"], "comment": "20 pages, 22 figures", "summary": "This study investigates the design choices of parameterized quantum circuits\n(PQCs) within quantum and hybrid convolutional neural network (HQNN and QCNN)\narchitectures, applied to the task of satellite image classification using the\nEuroSAT dataset. We systematically evaluate the performance implications of\ndata encoding techniques, variational ans\\\"atze, and measurement in approx. 500\ndistinct model configurations. Our analysis reveals a clear hierarchy of\ninfluence on model performance. For hybrid architectures, which were\nbenchmarked against their direct classical equivalents (e.g. the same\narchitecture with the PQCs removed), the data encoding strategy is the dominant\nfactor, with validation accuracy varying over 30% for distinct embeddings. In\ncontrast, the selection of variational ans\\\"atze and measurement basis had a\ncomparatively marginal effect, with validation accuracy variations remaining\nbelow 5%. For purely quantum models, restricted to amplitude encoding,\nperformance was most dependent on the measurement protocol and the\ndata-to-amplitude mapping. The measurement strategy varied the validation\naccuracy by up to 30% and the encoding mapping by around 8 percentage points.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u548c\u6df7\u5408\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08HQNN\uff09\u4e2d\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQCs\uff09\u7684\u8bbe\u8ba1\u9009\u62e9\u5bf9\u536b\u661f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8bc4\u4f30\u7ea6500\u79cd\u4e0d\u540c\u6a21\u578b\u914d\u7f6e\uff0c\u53d1\u73b0\u6570\u636e\u7f16\u7801\u7b56\u7565\u5bf9\u6df7\u5408\u67b6\u6784\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff08\u9a8c\u8bc1\u51c6\u786e\u7387\u5dee\u5f02\u8fbe30%\uff09\uff0c\u800c\u53d8\u5206ans\u00e4tze\u548c\u6d4b\u91cf\u57fa\u9009\u62e9\u5f71\u54cd\u8f83\u5c0f\uff08\u5dee\u5f02\u4f4e\u4e8e5%\uff09\u3002\u7eaf\u91cf\u5b50\u6a21\u578b\u4e2d\uff0c\u6d4b\u91cf\u534f\u8bae\u548c\u6570\u636e\u5230\u632f\u5e45\u7684\u6620\u5c04\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u6df7\u5408\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u7684\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\uff08\u5982\u6570\u636e\u7f16\u7801\u3001\u53d8\u5206ans\u00e4tze\u548c\u6d4b\u91cf\u57fa\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u536b\u661f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5728EuroSAT\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u7ea6500\u79cd\u4e0d\u540c\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u6bd4\u8f83\u6570\u636e\u7f16\u7801\u6280\u672f\u3001\u53d8\u5206ans\u00e4tze\u548c\u6d4b\u91cf\u57fa\u5bf9\u91cf\u5b50\u53ca\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6df7\u5408\u67b6\u6784\u8fd8\u4e0e\u5176\u7ecf\u5178\u7b49\u6548\u67b6\u6784\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6df7\u5408\u67b6\u6784\u4e2d\uff0c\u6570\u636e\u7f16\u7801\u7b56\u7565\u5bf9\u9a8c\u8bc1\u51c6\u786e\u7387\u7684\u5f71\u54cd\u6700\u5927\uff08\u5dee\u5f02\u8fbe30%\uff09\uff0c\u800c\u53d8\u5206ans\u00e4tze\u548c\u6d4b\u91cf\u57fa\u7684\u9009\u62e9\u5f71\u54cd\u8f83\u5c0f\uff08\u5dee\u5f02\u4f4e\u4e8e5%\uff09\u3002\u7eaf\u91cf\u5b50\u6a21\u578b\u4e2d\uff0c\u6d4b\u91cf\u534f\u8bae\u548c\u6570\u636e\u5230\u632f\u5e45\u7684\u6620\u5c04\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u5dee\u5f02\u5206\u522b\u8fbe30%\u548c8%\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u6570\u636e\u7f16\u7801\u7b56\u7565\u662f\u6df7\u5408\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u800c\u7eaf\u91cf\u5b50\u6a21\u578b\u7684\u6027\u80fd\u66f4\u4f9d\u8d56\u4e8e\u6d4b\u91cf\u534f\u8bae\u548c\u6570\u636e\u6620\u5c04\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "paper_title_zh": "\u91cf\u5b50\u4e0e\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u4e0d\u540c\u7f16\u7801\u3001ans\u00e4tze\u548c\u6d4b\u91cf\u5bf9\u6027\u80fd\u5f71\u54cd\u7684\u5b9e\u7528\u89c1\u89e3", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u548c\u6df7\u5408\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08HQNN\uff09\u4e2d\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQCs\uff09\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e94\u7528\u4e8eEuroSAT\u6570\u636e\u96c6\u7684\u536b\u661f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002\u6211\u4eec\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7ea6500\u79cd\u4e0d\u540c\u6a21\u578b\u914d\u7f6e\u4e2d\u6570\u636e\u7f16\u7801\u6280\u672f\u3001\u53d8\u5206ans\u00e4tze\u548c\u6d4b\u91cf\u57fa\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u56e0\u7d20\u5b58\u5728\u660e\u663e\u5c42\u6b21\u3002\u5bf9\u4e8e\u6df7\u5408\u67b6\u6784\uff08\u4e0e\u76f4\u63a5\u7ecf\u5178\u7b49\u6548\u67b6\u6784\u5bf9\u6bd4\uff0c\u5982\u79fb\u9664PQCs\u7684\u76f8\u540c\u67b6\u6784\uff09\uff0c\u6570\u636e\u7f16\u7801\u7b56\u7565\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0c\u4e0d\u540c\u5d4c\u5165\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u5dee\u5f02\u8d85\u8fc730%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u53d8\u5206ans\u00e4tze\u548c\u6d4b\u91cf\u57fa\u7684\u9009\u62e9\u5f71\u54cd\u8f83\u5c0f\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u5dee\u5f02\u4f4e\u4e8e5%\u3002\u5bf9\u4e8e\u7eaf\u91cf\u5b50\u6a21\u578b\uff08\u4ec5\u9650\u4e8e\u632f\u5e45\u7f16\u7801\uff09\uff0c\u6027\u80fd\u6700\u4f9d\u8d56\u4e8e\u6d4b\u91cf\u534f\u8bae\u548c\u6570\u636e\u5230\u632f\u5e45\u7684\u6620\u5c04\u3002\u6d4b\u91cf\u7b56\u7565\u4f7f\u9a8c\u8bc1\u51c6\u786e\u7387\u5dee\u5f02\u8fbe30%\uff0c\u800c\u7f16\u7801\u6620\u5c04\u7684\u5dee\u5f02\u7ea6\u4e3a8\u4e2a\u767e\u5206\u70b9\u3002"}}
{"id": "2506.20049", "pdf": "https://arxiv.org/pdf/2506.20049", "abs": "https://arxiv.org/abs/2506.20049", "authors": ["Lorin Achey", "Alec Reed", "Brendan Crowe", "Bradley Hayes", "Christoffer Heckman"], "title": "Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis", "categories": ["cs.RO", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2409.10681", "summary": "We present a novel approach for enhancing robotic exploration by using\ngenerative occupancy mapping. We introduce SceneSense, a diffusion model\ndesigned and trained for predicting 3D occupancy maps given partial\nobservations. Our proposed approach probabilistically fuses these predictions\ninto a running occupancy map in real-time, resulting in significant\nimprovements in map quality and traversability. We implement SceneSense onboard\na quadruped robot and validate its performance with real-world experiments to\ndemonstrate the effectiveness of the model. In these experiments, we show that\noccupancy maps enhanced with SceneSense predictions better represent our fully\nobserved ground truth data (24.44% FID improvement around the robot and 75.59%\nimprovement at range). We additionally show that integrating\nSceneSense-enhanced maps into our robotic exploration stack as a \"drop-in\" map\nimprovement, utilizing an existing off-the-shelf planner, results in\nimprovements in robustness and traversability time. Finally we show results of\nfull exploration evaluations with our proposed system in two dissimilar\nenvironments and find that locally enhanced maps provide more consistent\nexploration results than maps constructed only from direct sensor measurements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u5f0f\u5360\u7528\u5730\u56fe\u5408\u6210\u6280\u672f\u589e\u5f3a\u673a\u5668\u4eba\u63a2\u7d22\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7SceneSense\u6269\u6563\u6a21\u578b\u5b9e\u65f6\u9884\u6d4b\u5e76\u878d\u54083D\u5360\u7528\u5730\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u8d28\u91cf\u548c\u53ef\u901a\u884c\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u63a2\u7d22\u4e2d\uff0c\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u5bfc\u81f4\u7684\u5730\u56fe\u4e0d\u5b8c\u6574\u548c\u4e0d\u53ef\u9760\u662f\u4e3b\u8981\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5f0f\u5360\u7528\u5730\u56fe\u5408\u6210\u6280\u672f\uff0c\u63d0\u5347\u5730\u56fe\u7684\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u6539\u5584\u673a\u5668\u4eba\u7684\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faSceneSense\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u57fa\u4e8e\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u9884\u6d4b3D\u5360\u7528\u5730\u56fe\uff0c\u5e76\u5b9e\u65f6\u5c06\u8fd9\u4e9b\u9884\u6d4b\u7ed3\u679c\u878d\u5408\u5230\u8fd0\u884c\u4e2d\u7684\u5360\u7528\u5730\u56fe\u4e2d\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSceneSense\u589e\u5f3a\u7684\u5360\u7528\u5730\u56fe\u5728\u673a\u5668\u4eba\u9644\u8fd1\u548c\u8fdc\u8ddd\u79bb\u533a\u57df\u7684FID\u5206\u522b\u63d0\u5347\u4e8624.44%\u548c75.59%\u3002\u6b64\u5916\uff0c\u5c06\u589e\u5f3a\u5730\u56fe\u96c6\u6210\u5230\u73b0\u6709\u63a2\u7d22\u7cfb\u7edf\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u7684\u9c81\u68d2\u6027\u548c\u53ef\u901a\u884c\u6027\u65f6\u95f4\u3002", "conclusion": "SceneSense\u901a\u8fc7\u751f\u6210\u5f0f\u5360\u7528\u5730\u56fe\u5408\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u63a2\u7d22\u7684\u5730\u56fe\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u751f\u6210\u5f0f\u5360\u7528\u5730\u56fe\u5408\u6210\u7684\u9c81\u68d2\u673a\u5668\u4eba\u63a2\u7d22\u4e0e\u5efa\u56fe", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u5f0f\u5360\u7528\u5730\u56fe\u5408\u6210\u6280\u672f\u589e\u5f3a\u673a\u5668\u4eba\u63a2\u7d22\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86SceneSense\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u57fa\u4e8e\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u9884\u6d4b3D\u5360\u7528\u5730\u56fe\u800c\u8bbe\u8ba1\u7684\u6269\u6563\u6a21\u578b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u65f6\u5c06\u8fd9\u4e9b\u9884\u6d4b\u7ed3\u679c\u6982\u7387\u6027\u5730\u878d\u5408\u5230\u8fd0\u884c\u4e2d\u7684\u5360\u7528\u5730\u56fe\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u8d28\u91cf\u548c\u53ef\u901a\u884c\u6027\u3002\u6211\u4eec\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86SceneSense\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSceneSense\u589e\u5f3a\u7684\u5360\u7528\u5730\u56fe\u66f4\u63a5\u8fd1\u5b8c\u5168\u89c2\u6d4b\u7684\u771f\u5b9e\u6570\u636e\uff08\u673a\u5668\u4eba\u9644\u8fd1FID\u63d0\u534724.44%\uff0c\u8fdc\u8ddd\u79bb\u533a\u57df\u63d0\u534775.59%\uff09\u3002\u6b64\u5916\uff0c\u5c06SceneSense\u589e\u5f3a\u5730\u56fe\u4f5c\u4e3a\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u5730\u56fe\u6539\u8fdb\u96c6\u6210\u5230\u73b0\u6709\u63a2\u7d22\u7cfb\u7edf\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u53ef\u901a\u884c\u6027\u65f6\u95f4\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u4e24\u79cd\u4e0d\u540c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b8c\u6574\u7684\u63a2\u7d22\u8bc4\u4f30\uff0c\u53d1\u73b0\u5c40\u90e8\u589e\u5f3a\u5730\u56fe\u6bd4\u4ec5\u57fa\u4e8e\u76f4\u63a5\u4f20\u611f\u5668\u6d4b\u91cf\u6784\u5efa\u7684\u5730\u56fe\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u63a2\u7d22\u7ed3\u679c\u3002"}}
{"id": "2506.20367", "pdf": "https://arxiv.org/pdf/2506.20367", "abs": "https://arxiv.org/abs/2506.20367", "authors": ["Edoardo Alberto Dominici", "Jozef Hladky", "Floor Verhoeven", "Lukas Radl", "Thomas Deixelberger", "Stefan Ainetter", "Philipp Drescher", "Stefan Hauswiesner", "Arno Coomans", "Giacomo Nazzaro", "Konstantinos Vardis", "Markus Steinberger"], "title": "DreamAnywhere: Object-Centric Panoramic 3D Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in text-to-3D scene generation have demonstrated significant\npotential to transform content creation across multiple industries. Although\nthe research community has made impressive progress in addressing the\nchallenges of this complex task, existing methods often generate environments\nthat are only front-facing, lack visual fidelity, exhibit limited scene\nunderstanding, and are typically fine-tuned for either indoor or outdoor\nsettings. In this work, we address these issues and propose DreamAnywhere, a\nmodular system for the fast generation and prototyping of 3D scenes. Our system\nsynthesizes a 360{\\deg} panoramic image from text, decomposes it into\nbackground and objects, constructs a complete 3D representation through hybrid\ninpainting, and lifts object masks to detailed 3D objects that are placed in\nthe virtual environment. DreamAnywhere supports immersive navigation and\nintuitive object-level editing, making it ideal for scene exploration, visual\nmock-ups, and rapid prototyping -- all with minimal manual modeling. These\nfeatures make our system particularly suitable for low-budget movie production,\nenabling quick iteration on scene layout and visual tone without the overhead\nof traditional 3D workflows. Our modular pipeline is highly customizable as it\nallows components to be replaced independently. Compared to current\nstate-of-the-art text and image-based 3D scene generation approaches,\nDreamAnywhere shows significant improvements in coherence in novel view\nsynthesis and achieves competitive image quality, demonstrating its\neffectiveness across diverse and challenging scenarios. A comprehensive user\nstudy demonstrates a clear preference for our method over existing approaches,\nvalidating both its technical robustness and practical usefulness.", "AI": {"tldr": "DreamAnywhere\u662f\u4e00\u79cd\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u548c\u539f\u578b\u53163D\u573a\u666f\uff0c\u901a\u8fc7\u6587\u672c\u751f\u6210360\u5ea6\u5168\u666f\u56fe\u50cf\uff0c\u5206\u89e3\u4e3a\u80cc\u666f\u548c\u5bf9\u8c61\uff0c\u5e76\u6784\u5efa\u5b8c\u6574\u76843D\u8868\u793a\uff0c\u652f\u6301\u6c89\u6d78\u5f0f\u5bfc\u822a\u548c\u5bf9\u8c61\u7ea7\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4ec5\u751f\u6210\u6b63\u9762\u89c6\u89d2\u3001\u7f3a\u4e4f\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u573a\u666f\u7406\u89e3\u6709\u9650\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u5ba4\u5185\u6216\u5ba4\u5916\u573a\u666f\u3002DreamAnywhere\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u751f\u6210\u65b9\u6848\u3002", "method": "DreamAnywhere\u901a\u8fc7\u6587\u672c\u751f\u6210360\u5ea6\u5168\u666f\u56fe\u50cf\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u80cc\u666f\u548c\u5bf9\u8c61\uff0c\u5229\u7528\u6df7\u5408\u4fee\u590d\u6280\u672f\u6784\u5efa\u5b8c\u65743D\u8868\u793a\uff0c\u5e76\u5c06\u5bf9\u8c61\u63a9\u7801\u63d0\u5347\u4e3a\u8be6\u7ec63D\u5bf9\u8c61\uff0c\u653e\u7f6e\u5728\u865a\u62df\u73af\u5883\u4e2d\u3002\u7cfb\u7edf\u652f\u6301\u6c89\u6d78\u5f0f\u5bfc\u822a\u548c\u5bf9\u8c61\u7ea7\u7f16\u8f91\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cDreamAnywhere\u5728\u65b0\u89c6\u89d2\u5408\u6210\u7684\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u6280\u672f\u7a33\u5065\u6027\u548c\u5b9e\u7528\u6027\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "DreamAnywhere\u4e3a\u4f4e\u9884\u7b97\u7535\u5f71\u5236\u4f5c\u7b49\u573a\u666f\u63d0\u4f9b\u5feb\u901f\u8fed\u4ee3\u80fd\u529b\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u9ad8\u5ea6\u53ef\u5b9a\u5236\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u76843D\u573a\u666f\u751f\u6210\u9700\u6c42\u3002", "paper_title_zh": "DreamAnywhere\uff1a\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684360\u5ea6\u5168\u666f3D\u573a\u666f\u751f\u6210", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u4e3a\u591a\u884c\u4e1a\u7684\u5185\u5bb9\u521b\u4f5c\u5e26\u6765\u4e86\u5de8\u5927\u6f5c\u529b\u3002\u5c3d\u7ba1\u7814\u7a76\u793e\u533a\u5728\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u751f\u6210\u6b63\u9762\u89c6\u89d2\u7684\u573a\u666f\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0d\u8db3\uff0c\u573a\u666f\u7406\u89e3\u6709\u9650\uff0c\u4e14\u901a\u5e38\u4ec5\u9488\u5bf9\u5ba4\u5185\u6216\u5ba4\u5916\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002\u672c\u6587\u63d0\u51faDreamAnywhere\uff0c\u4e00\u79cd\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u548c\u539f\u578b\u53163D\u573a\u666f\u3002\u8be5\u7cfb\u7edf\u4ece\u6587\u672c\u751f\u6210360\u5ea6\u5168\u666f\u56fe\u50cf\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u80cc\u666f\u548c\u5bf9\u8c61\uff0c\u901a\u8fc7\u6df7\u5408\u4fee\u590d\u6280\u672f\u6784\u5efa\u5b8c\u6574\u76843D\u8868\u793a\uff0c\u5e76\u5c06\u5bf9\u8c61\u63a9\u7801\u63d0\u5347\u4e3a\u8be6\u7ec6\u76843D\u5bf9\u8c61\uff0c\u653e\u7f6e\u5728\u865a\u62df\u73af\u5883\u4e2d\u3002DreamAnywhere\u652f\u6301\u6c89\u6d78\u5f0f\u5bfc\u822a\u548c\u76f4\u89c2\u7684\u5bf9\u8c61\u7ea7\u7f16\u8f91\uff0c\u975e\u5e38\u9002\u5408\u573a\u666f\u63a2\u7d22\u3001\u89c6\u89c9\u8349\u56fe\u548c\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u624b\u52a8\u5efa\u6a21\u3002\u8fd9\u4e9b\u7279\u6027\u4f7f\u5176\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u9884\u7b97\u7535\u5f71\u5236\u4f5c\uff0c\u80fd\u591f\u5feb\u901f\u8fed\u4ee3\u573a\u666f\u5e03\u5c40\u548c\u89c6\u89c9\u98ce\u683c\uff0c\u800c\u65e0\u9700\u4f20\u7edf3D\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f00\u9500\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u72ec\u7acb\u66ff\u6362\u7ec4\u4ef6\uff0c\u5177\u6709\u9ad8\u5ea6\u53ef\u5b9a\u5236\u6027\u3002\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4\uff0cDreamAnywhere\u5728\u65b0\u89c6\u89d2\u5408\u6210\u7684\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6837\u5316\u548c\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u5168\u9762\u7684\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u660e\u663e\u504f\u597d\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6280\u672f\u7a33\u5065\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.20062", "pdf": "https://arxiv.org/pdf/2506.20062", "abs": "https://arxiv.org/abs/2506.20062", "authors": ["Runlong Ye", "Zeling Zhang", "Boushra Almazroua", "Michael Liut"], "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered code assistants are widely used to generate code completions,\nsignificantly boosting developer productivity. However, these tools typically\npresent suggestions without explaining their rationale, leaving their\ndecision-making process inscrutable. This opacity hinders developers' ability\nto critically evaluate the output, form accurate mental models, and build\ncalibrated trust in the system. To address this, we introduce CopilotLens, a\nnovel interactive framework that reframes code completion from a simple\nsuggestion into a transparent, explainable event. CopilotLens operates as an\nexplanation layer that reveals the AI agent's \"thought process\" through a\ndynamic two-level interface, surfacing everything from its reconstructed\nhigh-level plans to the specific codebase context influencing the code. This\npaper presents the design and rationale of CopilotLens, offering a concrete\nframework for building future agentic code assistants that prioritize clarity\nof reasoning over speed of suggestion, thereby fostering deeper comprehension\nand more robust human-AI collaboration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CopilotLens\uff0c\u4e00\u79cd\u65b0\u578b\u4ea4\u4e92\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u63d0\u5347AI\u4ee3\u7801\u52a9\u624b\u7684\u4f7f\u7528\u4f53\u9a8c\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3AI\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u52a9\u624b\u867d\u7136\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u5176\u5efa\u8bae\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u5f00\u53d1\u8005\u96be\u4ee5\u7406\u89e3AI\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u5f71\u54cd\u4e86\u4fe1\u4efb\u548c\u534f\u4f5c\u3002CopilotLens\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CopilotLens\u4f5c\u4e3a\u4e00\u4e2a\u89e3\u91ca\u5c42\uff0c\u901a\u8fc7\u52a8\u6001\u53cc\u5c42\u754c\u9762\u5c55\u793aAI\u7684\u201c\u601d\u8003\u8fc7\u7a0b\u201d\uff0c\u5305\u62ec\u9ad8\u5c42\u6b21\u8ba1\u5212\u548c\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u63d0\u4f9b\u900f\u660e\u5316\u7684\u4ee3\u7801\u5efa\u8bae\u3002", "result": "CopilotLens\u6210\u529f\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7fAI\u4ee3\u7801\u52a9\u624b\u7684\u51b3\u7b56\u8fc7\u7a0b\u53d8\u5f97\u900f\u660e\u548c\u53ef\u89e3\u91ca\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "conclusion": "CopilotLens\u4e3a\u672a\u6765\u6ce8\u91cd\u63a8\u7406\u900f\u660e\u6027\u7684AI\u4ee3\u7801\u52a9\u624b\u63d0\u4f9b\u4e86\u5177\u4f53\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u4eba\u673a\u534f\u4f5c\u3002", "paper_title_zh": "\u8d85\u8d8a\u81ea\u52a8\u5b8c\u6210\uff1a\u8bbe\u8ba1CopilotLens\u4ee5\u5b9e\u73b0\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684AI\u7f16\u7801\u52a9\u624b", "abstract_zh": "AI\u9a71\u52a8\u7684\u4ee3\u7801\u52a9\u624b\u88ab\u5e7f\u6cdb\u7528\u4e8e\u751f\u6210\u4ee3\u7801\u8865\u5168\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u5728\u4e0d\u89e3\u91ca\u5176\u903b\u8f91\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u5efa\u8bae\uff0c\u4f7f\u5f97\u5176\u51b3\u7b56\u8fc7\u7a0b\u96be\u4ee5\u7406\u89e3\u3002\u8fd9\u79cd\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u5f00\u53d1\u8005\u5bf9\u8f93\u51fa\u7684\u6279\u5224\u6027\u8bc4\u4f30\u3001\u5f62\u6210\u51c6\u786e\u7684\u5fc3\u7406\u6a21\u578b\u4ee5\u53ca\u5bf9\u7cfb\u7edf\u5efa\u7acb\u6821\u51c6\u7684\u4fe1\u4efb\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86CopilotLens\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u4e92\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u8865\u5168\u4ece\u7b80\u5355\u7684\u5efa\u8bae\u8f6c\u53d8\u4e3a\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u4e8b\u4ef6\u3002CopilotLens\u4f5c\u4e3a\u4e00\u4e2a\u89e3\u91ca\u5c42\uff0c\u901a\u8fc7\u52a8\u6001\u53cc\u5c42\u754c\u9762\u5c55\u793aAI\u4ee3\u7406\u7684\u201c\u601d\u8003\u8fc7\u7a0b\u201d\uff0c\u4ece\u91cd\u6784\u7684\u9ad8\u5c42\u6b21\u8ba1\u5212\u5230\u5f71\u54cd\u4ee3\u7801\u7684\u5177\u4f53\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u3002\u672c\u6587\u4ecb\u7ecd\u4e86CopilotLens\u7684\u8bbe\u8ba1\u548c\u539f\u7406\uff0c\u4e3a\u672a\u6765\u6ce8\u91cd\u63a8\u7406\u6e05\u6670\u6027\u800c\u975e\u5efa\u8bae\u901f\u5ea6\u7684\u4ee3\u7406\u4ee3\u7801\u52a9\u624b\u63d0\u4f9b\u4e86\u5177\u4f53\u6846\u67b6\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u6df1\u5c42\u6b21\u7684\u7406\u89e3\u548c\u66f4\u7a33\u5065\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2506.20407", "pdf": "https://arxiv.org/pdf/2506.20407", "abs": "https://arxiv.org/abs/2506.20407", "authors": ["Fangyijie Wang", "Yuan Liang", "Sourav Bhattacharjee", "Abey Campbell", "Kathleen M. Curran", "Gu\u00e9nol\u00e9 Silvestre"], "title": "Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Accurate gestational age (GA) estimation, ideally through fetal ultrasound\nmeasurement, is a crucial aspect of providing excellent antenatal care.\nHowever, deriving GA from manual fetal biometric measurements depends on the\noperator and is time-consuming. Hence, automatic computer-assisted methods are\ndemanded in clinical practice. In this paper, we present a novel feature fusion\nframework to estimate GA using fetal ultrasound images without any measurement\ninformation. We adopt a deep learning model to extract deep representations\nfrom ultrasound images. We extract radiomic features to reveal patterns and\ncharacteristics of fetal brain growth. To harness the interpretability of\nradiomics in medical imaging analysis, we estimate GA by fusing radiomic\nfeatures and deep representations. Our framework estimates GA with a mean\nabsolute error of 8.0 days across three trimesters, outperforming current\nmachine learning-based methods at these gestational ages. Experimental results\ndemonstrate the robustness of our framework across different populations in\ndiverse geographical regions. Our code is publicly available on\n\\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u5b55\u5468\u4f30\u8ba1\uff0c\u65e0\u9700\u6d4b\u91cf\u4fe1\u606f\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a8.0\u5929\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u5b55\u5468\u5bf9\u4ea7\u524d\u62a4\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u6d4b\u91cf\u4f9d\u8d56\u64cd\u4f5c\u8005\u4e14\u8017\u65f6\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u8d85\u58f0\u56fe\u50cf\u7684\u6df1\u5ea6\u8868\u793a\uff0c\u540c\u65f6\u63d0\u53d6\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4ee5\u63ed\u793a\u80ce\u513f\u5927\u8111\u751f\u957f\u6a21\u5f0f\uff0c\u901a\u8fc7\u878d\u5408\u4e24\u7c7b\u7279\u5f81\u4f30\u8ba1\u5b55\u5468\u3002", "result": "\u6846\u67b6\u5728\u4e09\u5b55\u671f\u5185\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a8.0\u5929\uff0c\u4f18\u4e8e\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u7684\u4eba\u7fa4\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7279\u5f81\u878d\u5408\u6846\u67b6\u5728\u5b55\u5468\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4e0e\u6df1\u5ea6\u8868\u793a\u7528\u4e8e\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u5b55\u5468\u4f30\u8ba1", "abstract_zh": "\u51c6\u786e\u4f30\u8ba1\u5b55\u5468\uff08GA\uff09\u662f\u63d0\u4f9b\u4f18\u8d28\u4ea7\u524d\u62a4\u7406\u7684\u5173\u952e\uff0c\u901a\u5e38\u901a\u8fc7\u80ce\u513f\u8d85\u58f0\u6d4b\u91cf\u5b9e\u73b0\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u624b\u52a8\u80ce\u513f\u751f\u7269\u6d4b\u91cf\u7684GA\u4f30\u8ba1\u4f9d\u8d56\u64cd\u4f5c\u8005\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u9700\u8981\u81ea\u52a8\u5316\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u6d4b\u91cf\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u4f30\u8ba1GA\u3002\u6211\u4eec\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u8d85\u58f0\u56fe\u50cf\u4e2d\u63d0\u53d6\u6df1\u5ea6\u8868\u793a\uff0c\u5e76\u63d0\u53d6\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4ee5\u63ed\u793a\u80ce\u513f\u5927\u8111\u751f\u957f\u7684\u6a21\u5f0f\u548c\u7279\u5f81\u3002\u4e3a\u5229\u7528\u653e\u5c04\u7ec4\u5b66\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u548c\u6df1\u5ea6\u8868\u793a\u6765\u4f30\u8ba1GA\u3002\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e09\u5b55\u671f\u5185\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a8.0\u5929\uff0c\u4f18\u4e8e\u5f53\u524d\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u7684\u591a\u6837\u5316\u4eba\u7fa4\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002"}}
{"id": "2506.20566", "pdf": "https://arxiv.org/pdf/2506.20566", "abs": "https://arxiv.org/abs/2506.20566", "authors": ["Zhonghao Shi", "Enyu Zhao", "Nathaniel Dennler", "Jingzhen Wang", "Xinyang Xu", "Kaleen Shrestha", "Mengxue Fu", "Daniel Seita", "Maja Matari\u0107"], "title": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to the 19th International Symposium on Experimental Robotics\n  (ISER 2025)", "summary": "Real-time human perception is crucial for effective human-robot interaction\n(HRI). Large vision-language models (VLMs) offer promising generalizable\nperceptual capabilities but often suffer from high latency, which negatively\nimpacts user experience and limits VLM applicability in real-world scenarios.\nTo systematically study VLM capabilities in human perception for HRI and\nperformance-latency trade-offs, we introduce HRIBench, a visual\nquestion-answering (VQA) benchmark designed to evaluate VLMs across a diverse\nset of human perceptual tasks critical for HRI. HRIBench covers five key\ndomains: (1) non-verbal cue understanding, (2) verbal instruction\nunderstanding, (3) human-robot object relationship understanding, (4) social\nnavigation, and (5) person identification. To construct HRIBench, we collected\ndata from real-world HRI environments to curate questions for non-verbal cue\nunderstanding, and leveraged publicly available datasets for the remaining four\ndomains. We curated 200 VQA questions for each domain, resulting in a total of\n1000 questions for HRIBench. We then conducted a comprehensive evaluation of\nboth state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.\nOur results show that, despite their generalizability, current VLMs still\nstruggle with core perceptual capabilities essential for HRI. Moreover, none of\nthe models within our experiments demonstrated a satisfactory\nperformance-latency trade-off suitable for real-time deployment, underscoring\nthe need for future research on developing smaller, low-latency VLMs with\nimproved human perception capabilities. HRIBench and our results can be found\nin this Github repository: https://github.com/interaction-lab/HRIBench.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HRIBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u4e2d\u5b9e\u65f6\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1VLMs\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u6838\u5fc3\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u90e8\u7f72\u7684\u4f4e\u5ef6\u8fdf\u9700\u6c42\u3002", "motivation": "\u5b9e\u65f6\u4eba\u7c7b\u611f\u77e5\u5bf9\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5177\u6709\u6cdb\u5316\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u5176\u9ad8\u5ef6\u8fdf\u95ee\u9898\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7HRIBench\u7cfb\u7edf\u7814\u7a76VLMs\u5728HRI\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u53ca\u5176\u6027\u80fd\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86HRIBench\uff0c\u4e00\u4e2a\u5305\u542b1000\u4e2a\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e94\u4e2a\u5173\u952e\u9886\u57df\uff1a\u975e\u8bed\u8a00\u7ebf\u7d22\u7406\u89e3\u3001\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\u3001\u4eba\u673a\u5bf9\u8c61\u5173\u7cfb\u7406\u89e3\u3001\u793e\u4ea4\u5bfc\u822a\u548c\u4eba\u5458\u8bc6\u522b\u3002\u6570\u636e\u6765\u81ea\u771f\u5b9eHRI\u73af\u5883\u548c\u516c\u5f00\u6570\u636e\u96c6\u3002\u968f\u540e\u5bf911\u79cd\u5f00\u6e90\u548c\u95ed\u6e90VLMs\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1VLMs\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728HRI\u6838\u5fc3\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u6240\u6709\u6a21\u578b\u5747\u672a\u8fbe\u5230\u5b9e\u65f6\u90e8\u7f72\u6240\u9700\u7684\u6027\u80fd\u4e0e\u5ef6\u8fdf\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524dVLMs\u5728HRI\u4e2d\u7684\u5b9e\u65f6\u611f\u77e5\u80fd\u529b\u4ecd\u9700\u6539\u8fdb\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u5c0f\u3001\u4f4e\u5ef6\u8fdf\u4e14\u611f\u77e5\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u3002HRIBench\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u652f\u6301\u3002", "paper_title_zh": "HRIBench\uff1a\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u4e2d\u5b9e\u65f6\u4eba\u7c7b\u611f\u77e5\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u5b9e\u65f6\u4eba\u7c7b\u611f\u77e5\u5bf9\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u81f3\u5173\u91cd\u8981\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u7136\u5177\u5907\u6cdb\u5316\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u5176\u9ad8\u5ef6\u8fdf\u95ee\u9898\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u7cfb\u7edf\u7814\u7a76VLMs\u5728HRI\u4e2d\u7684\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\u53ca\u6027\u80fd\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86HRIBench\uff0c\u4e00\u4e2a\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30VLMs\u5728HRI\u5173\u952e\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002HRIBench\u6db5\u76d6\u4e94\u4e2a\u5173\u952e\u9886\u57df\uff1a\uff081\uff09\u975e\u8bed\u8a00\u7ebf\u7d22\u7406\u89e3\uff0c\uff082\uff09\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\uff0c\uff083\uff09\u4eba\u673a\u5bf9\u8c61\u5173\u7cfb\u7406\u89e3\uff0c\uff084\uff09\u793e\u4ea4\u5bfc\u822a\uff0c\uff085\uff09\u4eba\u5458\u8bc6\u522b\u3002\u4e3a\u6784\u5efaHRIBench\uff0c\u6211\u4eec\u4ece\u771f\u5b9eHRI\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\u4ee5\u8bbe\u8ba1\u975e\u8bed\u8a00\u7ebf\u7d22\u7406\u89e3\u95ee\u9898\uff0c\u5e76\u5229\u7528\u516c\u5f00\u6570\u636e\u96c6\u8986\u76d6\u5176\u4f59\u56db\u4e2a\u9886\u57df\u3002\u6bcf\u4e2a\u9886\u57df\u5305\u542b200\u4e2aVQA\u95ee\u9898\uff0c\u603b\u8ba11000\u4e2a\u95ee\u9898\u3002\u968f\u540e\uff0c\u6211\u4eec\u5bf911\u79cd\u5f00\u6e90\u548c\u95ed\u6e90VLMs\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1VLMs\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728HRI\u6838\u5fc3\u611f\u77e5\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u6240\u6709\u6a21\u578b\u5747\u672a\u8fbe\u5230\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u7684\u6027\u80fd\u4e0e\u5ef6\u8fdf\u5e73\u8861\uff0c\u51f8\u663e\u4e86\u672a\u6765\u5f00\u53d1\u66f4\u5c0f\u3001\u4f4e\u5ef6\u8fdf\u4e14\u611f\u77e5\u80fd\u529b\u66f4\u5f3a\u7684VLMs\u7684\u5fc5\u8981\u6027\u3002HRIBench\u53ca\u76f8\u5173\u7ed3\u679c\u53ef\u5728Github\u4ed3\u5e93\u4e2d\u67e5\u770b\uff1ahttps://github.com/interaction-lab/HRIBench\u3002"}}
{"id": "2506.20614", "pdf": "https://arxiv.org/pdf/2506.20614", "abs": "https://arxiv.org/abs/2506.20614", "authors": ["Simon Perrin", "S\u00e9bastien Levilly", "Huajun Sun", "Harold Mouch\u00e8re", "Jean-Michel Serfaty"], "title": "Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In recent decades, the use of 4D Flow MRI images has enabled the\nquantification of velocity fields within a volume of interest and along the\ncardiac cycle. However, the lack of resolution and the presence of noise in\nthese biomarkers are significant issues. As indicated by recent studies, it\nappears that biomarkers such as wall shear stress are particularly impacted by\nthe poor resolution of vessel segmentation. The Phase Contrast Magnetic\nResonance Angiography (PC-MRA) is the state-of-the-art method to facilitate\nsegmentation. The objective of this work is to introduce a new handcraft\nfeature that provides a novel visualisation of 4D Flow MRI images, which is\nuseful in the segmentation task. This feature, termed Weighted Mean Frequencies\n(WMF), is capable of revealing the region in three dimensions where a voxel has\nbeen passed by pulsatile flow. Indeed, this feature is representative of the\nhull of all pulsatile velocity voxels. The value of the feature under\ndiscussion is illustrated by two experiments. The experiments involved\nsegmenting 4D Flow MRI images using optimal thresholding and deep learning\nmethods. The results obtained demonstrate a substantial enhancement in terms of\nIoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with\nthe PC-MRA feature, as evidenced by the deep learning task. This feature has\nthe potential to yield valuable insights that could inform future segmentation\nprocesses in other vascular regions, such as the heart or the brain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a0\u6743\u5e73\u5747\u9891\u7387\uff08WMF\uff09\u7684\u624b\u5de5\u7279\u5f81\uff0c\u7528\u4e8e\u6539\u55844D Flow MRI\u56fe\u50cf\u7684\u5206\u5272\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0cWMF\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u9608\u503c\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86IoU\u548cDice\u5206\u6570\u3002", "motivation": "4D Flow MRI\u56fe\u50cf\u5728\u91cf\u5316\u8840\u6d41\u901f\u5ea6\u573a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5206\u8fa8\u7387\u4f4e\u548c\u566a\u58f0\u95ee\u9898\u5f71\u54cd\u4e86\u5206\u5272\u6548\u679c\uff0c\u5c24\u5176\u662f\u8840\u7ba1\u58c1\u526a\u5207\u5e94\u529b\u7b49\u751f\u7269\u6807\u5fd7\u7269\u7684\u51c6\u786e\u6027\u3002\u73b0\u6709\u7684PC-MRA\u65b9\u6cd5\u867d\u4e3a\u5148\u8fdb\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u624b\u5de5\u7279\u5f81\u4ee5\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a0\u6743\u5e73\u5747\u9891\u7387\uff08WMF\uff09\u7684\u624b\u5de5\u7279\u5f81\uff0c\u80fd\u591f\u4e09\u7ef4\u663e\u793a\u8109\u51b2\u6d41\u7ecf\u8fc7\u7684\u4f53\u7d20\u533a\u57df\u3002\u901a\u8fc7\u6700\u4f18\u9608\u503c\u5206\u5272\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9a8c\u8bc1\u4e86WMF\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWMF\u7279\u5f81\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u9608\u503c\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8ePC-MRA\u7279\u5f81\uff0cIoU\u548cDice\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e860.12\u548c0.13\u3002", "conclusion": "WMF\u7279\u5f81\u57284D Flow MRI\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u5fc3\u810f\u6216\u5927\u8111\u7b49\u5176\u4ed6\u8840\u7ba1\u533a\u57df\u7684\u5206\u5272\u4efb\u52a1\u3002", "paper_title_zh": "\u52a0\u6743\u5e73\u5747\u9891\u7387\uff1a\u4e00\u79cd\u7528\u4e8e4D Flow MRI\u5206\u5272\u7684\u624b\u5de5\u5085\u91cc\u53f6\u7279\u5f81", "abstract_zh": "\u8fd1\u51e0\u5341\u5e74\u6765\uff0c4D Flow MRI\u56fe\u50cf\u7684\u4f7f\u7528\u4f7f\u5f97\u5728\u611f\u5174\u8da3\u4f53\u79ef\u5185\u548c\u5fc3\u52a8\u5468\u671f\u4e2d\u91cf\u5316\u901f\u5ea6\u573a\u6210\u4e3a\u53ef\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u751f\u7269\u6807\u5fd7\u7269\u7684\u5206\u8fa8\u7387\u4e0d\u8db3\u548c\u566a\u58f0\u95ee\u9898\u662f\u663e\u8457\u6311\u6218\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u8840\u7ba1\u58c1\u526a\u5207\u5e94\u529b\u7b49\u751f\u7269\u6807\u5fd7\u7269\u5c24\u5176\u53d7\u5230\u8840\u7ba1\u5206\u5272\u5206\u8fa8\u7387\u4f4e\u7684\u5f71\u54cd\u3002\u76f8\u4f4d\u5bf9\u6bd4\u78c1\u5171\u632f\u8840\u7ba1\u6210\u50cf\uff08PC-MRA\uff09\u662f\u76ee\u524d\u4fc3\u8fdb\u5206\u5272\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u624b\u5de5\u7279\u5f81\uff0c\u4e3a4D Flow MRI\u56fe\u50cf\u63d0\u4f9b\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5206\u5272\u4efb\u52a1\u3002\u8fd9\u4e00\u7279\u5f81\u79f0\u4e3a\u52a0\u6743\u5e73\u5747\u9891\u7387\uff08WMF\uff09\uff0c\u80fd\u591f\u4e09\u7ef4\u663e\u793a\u8109\u51b2\u6d41\u7ecf\u8fc7\u7684\u4f53\u7d20\u533a\u57df\uff0c\u4ee3\u8868\u4e86\u6240\u6709\u8109\u51b2\u901f\u5ea6\u4f53\u7d20\u7684\u5305\u7edc\u3002\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7279\u5f81\u7684\u4ef7\u503c\uff1a\u4f7f\u7528\u6700\u4f18\u9608\u503c\u5206\u5272\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5206\u52724D Flow MRI\u56fe\u50cf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0ePC-MRA\u7279\u5f81\u76f8\u6bd4\uff0c\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2dIoU\u548cDice\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e860.12\u548c0.13\u3002WMF\u7279\u5f81\u6709\u671b\u4e3a\u5176\u4ed6\u8840\u7ba1\u533a\u57df\uff08\u5982\u5fc3\u810f\u6216\u5927\u8111\uff09\u7684\u672a\u6765\u5206\u5272\u8fc7\u7a0b\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.20652", "pdf": "https://arxiv.org/pdf/2506.20652", "abs": "https://arxiv.org/abs/2506.20652", "authors": ["Roi Bar-On", "Dana Cohen-Bar", "Daniel Cohen-Or"], "title": "EditP23: 3D Editing via Propagation of Image Prompts to Multi-View", "categories": ["cs.GR", "cs.CV", "68U05 (Primary), 68T45 (Secondary)", "I.3.7; I.3.8; I.4.9"], "comment": "Code, supplementary videos, interactive 3D visualizations, and\n  additional results are available at https://editp23.github.io/", "summary": "We present EditP23, a method for mask-free 3D editing that propagates 2D\nimage edits to multi-view representations in a 3D-consistent manner. In\ncontrast to traditional approaches that rely on text-based prompting or\nexplicit spatial masks, EditP23 enables intuitive edits by conditioning on a\npair of images: an original view and its user-edited counterpart. These image\nprompts are used to guide an edit-aware flow in the latent space of a\npre-trained multi-view diffusion model, allowing the edit to be coherently\npropagated across views. Our method operates in a feed-forward manner, without\noptimization, and preserves the identity of the original object, in both\nstructure and appearance. We demonstrate its effectiveness across a range of\nobject categories and editing scenarios, achieving high fidelity to the source\nwhile requiring no manual masks.", "AI": {"tldr": "EditP23\u662f\u4e00\u79cd\u65e0\u9700\u63a9\u7801\u76843D\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c062D\u56fe\u50cf\u7f16\u8f91\u4f20\u64ad\u5230\u591a\u89c6\u89d2\u8868\u793a\u4e2d\uff0c\u5b9e\u73b03D\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u4f20\u7edf3D\u7f16\u8f91\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u6587\u672c\u7684\u63d0\u793a\u6216\u663e\u5f0f\u7a7a\u95f4\u63a9\u7801\uff0c\u64cd\u4f5c\u590d\u6742\u4e14\u4e0d\u591f\u76f4\u89c2\u3002EditP23\u65e8\u5728\u901a\u8fc7\u4e00\u5bf9\u56fe\u50cf\uff08\u539f\u59cb\u89c6\u56fe\u548c\u7528\u6237\u7f16\u8f91\u540e\u7684\u89c6\u56fe\uff09\u4f5c\u4e3a\u63d0\u793a\uff0c\u5b9e\u73b0\u66f4\u76f4\u89c2\u76843D\u7f16\u8f91\u3002", "method": "EditP23\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u7f16\u8f91\u611f\u77e5\u6d41\u5c062D\u56fe\u50cf\u7f16\u8f91\u4e00\u81f4\u5730\u4f20\u64ad\u5230\u591a\u89c6\u89d2\u4e2d\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4f18\u5316\uff0c\u4ee5\u76f4\u63a5\u524d\u9988\u65b9\u5f0f\u8fd0\u884c\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u5bf9\u8c61\u7684\u7ed3\u6784\u548c\u5916\u89c2\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEditP23\u5728\u591a\u79cd\u5bf9\u8c61\u7c7b\u522b\u548c\u7f16\u8f91\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u4fdd\u7559\u6e90\u5185\u5bb9\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u63a9\u7801\u3002", "conclusion": "EditP23\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u76f4\u89c2\u76843D\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u63d0\u793a\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "paper_title_zh": "EditP23\uff1a\u901a\u8fc7\u56fe\u50cf\u63d0\u793a\u4f20\u64ad\u5b9e\u73b0\u591a\u89c6\u89d23D\u7f16\u8f91", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86EditP23\uff0c\u4e00\u79cd\u65e0\u9700\u63a9\u7801\u76843D\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c062D\u56fe\u50cf\u7f16\u8f91\u4ee53D\u4e00\u81f4\u7684\u65b9\u5f0f\u4f20\u64ad\u5230\u591a\u89c6\u89d2\u8868\u793a\u4e2d\u3002\u4e0e\u4f20\u7edf\u4f9d\u8d56\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u6216\u663e\u5f0f\u7a7a\u95f4\u63a9\u7801\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cEditP23\u901a\u8fc7\u4e00\u5bf9\u56fe\u50cf\uff08\u539f\u59cb\u89c6\u56fe\u53ca\u5176\u7528\u6237\u7f16\u8f91\u540e\u7684\u7248\u672c\uff09\u4f5c\u4e3a\u63d0\u793a\uff0c\u5b9e\u73b0\u76f4\u89c2\u7684\u7f16\u8f91\u3002\u8fd9\u4e9b\u56fe\u50cf\u63d0\u793a\u7528\u4e8e\u5f15\u5bfc\u9884\u8bad\u7ec3\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7f16\u8f91\u611f\u77e5\u6d41\uff0c\u4f7f\u7f16\u8f91\u80fd\u591f\u4e00\u81f4\u5730\u4f20\u64ad\u5230\u591a\u89c6\u89d2\u4e2d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4ee5\u524d\u9988\u65b9\u5f0f\u8fd0\u884c\uff0c\u65e0\u9700\u4f18\u5316\uff0c\u5e76\u4fdd\u7559\u4e86\u539f\u59cb\u5bf9\u8c61\u7684\u7ed3\u6784\u548c\u5916\u89c2\u7279\u5f81\u3002\u6211\u4eec\u5728\u591a\u79cd\u5bf9\u8c61\u7c7b\u522b\u548c\u7f16\u8f91\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6e90\u5185\u5bb9\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u63a9\u7801\u3002"}}
{"id": "2506.20156", "pdf": "https://arxiv.org/pdf/2506.20156", "abs": "https://arxiv.org/abs/2506.20156", "authors": ["Xuefei Hou", "Xizhao Tan"], "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype", "categories": ["cs.HC", "cs.AI", "cs.IR", "H.5.2; I.2.7; H.3.3"], "comment": "Version 1 of a work in progress. Finalized system flowcharts, a\n  public GitHub repository with the source code, and a full reproducibility\n  package detailing the prompts, models, and testing guidelines will be\n  provided in v2", "summary": "The core challenge in learning has shifted from knowledge acquisition to\neffective Self-Regulated Learning (SRL): planning, monitoring, and reflecting\non one's learning. Existing digital tools, however, inadequately support\nmetacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized\nreview, overlooking the role of context, while Personal Knowledge Management\n(PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel\nparadigm that conceptualizes the context-triggered retrieval of personal past\ninsights as a metacognitive scaffold to promote SRL. We formalize this paradigm\nusing the Just-in-Time Adaptive Intervention (JITAI) framework and implement a\nprototype system, Irec, to demonstrate its feasibility. At its core, Irec uses\na dynamic knowledge graph of the user's learning history. When a user faces a\nnew problem, a hybrid retrieval engine recalls relevant personal \"insights.\"\nSubsequently, a large language model (LLM) performs a deep similarity\nassessment to filter and present the most relevant scaffold in a just-in-time\nmanner. To reduce cognitive load, Irec features a human-in-the-loop pipeline\nfor LLM-based knowledge graph construction. We also propose an optional \"Guided\nInquiry\" module, where users can engage in a Socratic dialogue with an expert\nLLM, using the current problem and recalled insights as context. The\ncontribution of this paper is a solid theoretical framework and a usable system\nplatform for designing next-generation intelligent learning systems that\nenhance metacognition and self-regulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cInsight Recall\u201d\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u89e6\u53d1\u7684\u4e2a\u4eba\u8fc7\u53bb\u89c1\u89e3\u68c0\u7d22\uff0c\u4f5c\u4e3a\u4fc3\u8fdb\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\u7684\u5143\u8ba4\u77e5\u652f\u67b6\u3002\u4f5c\u8005\u57fa\u4e8eJust-in-Time Adaptive Intervention\uff08JITAI\uff09\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u539f\u578b\u7cfb\u7edfIrec\uff0c\u7ed3\u5408\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5b9e\u73b0\u4e86\u53ca\u65f6\u7684\u76f8\u5173\u89c1\u89e3\u68c0\u7d22\u548c\u5448\u73b0\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5de5\u5177\u5728\u652f\u6301\u5143\u8ba4\u77e5\u53cd\u601d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f8b\u5982\u95f4\u9694\u91cd\u590d\u7cfb\u7edf\uff08SRS\uff09\u7f3a\u4e4f\u4e0a\u4e0b\u6587\uff0c\u800c\u4e2a\u4eba\u77e5\u8bc6\u7ba1\u7406\uff08PKM\uff09\u5de5\u5177\u9700\u8981\u9ad8\u4eba\u5de5\u7ef4\u62a4\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u63d0\u5347\u5b66\u4e60\u8005\u7684\u81ea\u6211\u8c03\u8282\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u201cInsight Recall\u201d\u8303\u5f0f\uff0c\u5e76\u57fa\u4e8eJITAI\u6846\u67b6\u5f00\u53d1\u4e86Irec\u7cfb\u7edf\u3002\u7cfb\u7edf\u5229\u7528\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u8bb0\u5f55\u7528\u6237\u5b66\u4e60\u5386\u53f2\uff0c\u901a\u8fc7\u6df7\u5408\u68c0\u7d22\u5f15\u64ce\u548cLLM\u8fdb\u884c\u6df1\u5ea6\u76f8\u4f3c\u6027\u8bc4\u4f30\uff0c\u53ca\u65f6\u5448\u73b0\u76f8\u5173\u89c1\u89e3\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u8fd8\u8bbe\u8ba1\u4e86\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6d41\u7a0b\u548c\u53ef\u9009\u7684\u201c\u5f15\u5bfc\u63a2\u7a76\u201d\u6a21\u5757\uff0c\u652f\u6301\u7528\u6237\u4e0e\u4e13\u5bb6LLM\u7684\u5bf9\u8bdd\u3002", "result": "Irec\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u548cLLM\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u89c1\u89e3\u68c0\u7d22\u548c\u5448\u73b0\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\uff0c\u5e76\u4e3a\u5143\u8ba4\u77e5\u53cd\u601d\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u575a\u5b9e\u7684\u7406\u8bba\u6846\u67b6\u548c\u53ef\u7528\u7684\u7cfb\u7edf\u5e73\u53f0\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u5b66\u4e60\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u80fd\u591f\u589e\u5f3a\u5b66\u4e60\u8005\u7684\u5143\u8ba4\u77e5\u548c\u81ea\u6211\u8c03\u8282\u80fd\u529b\u3002", "paper_title_zh": "Irec\uff1a\u4e00\u79cd\u901a\u8fc7\u53ca\u65f6\u89c1\u89e3\u56de\u5fc6\u4fc3\u8fdb\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u7684\u5143\u8ba4\u77e5\u652f\u67b6\uff1a\u6982\u5ff5\u6846\u67b6\u4e0e\u7cfb\u7edf\u539f\u578b", "abstract_zh": "\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\u5df2\u4ece\u77e5\u8bc6\u83b7\u53d6\u8f6c\u5411\u6709\u6548\u7684\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\uff1a\u89c4\u5212\u3001\u76d1\u63a7\u548c\u53cd\u601d\u5b66\u4e60\u8fc7\u7a0b\u3002\u7136\u800c\uff0c\u73b0\u6709\u6570\u5b57\u5de5\u5177\u5bf9\u5143\u8ba4\u77e5\u53cd\u601d\u7684\u652f\u6301\u4e0d\u8db3\u3002\u95f4\u9694\u91cd\u590d\u7cfb\u7edf\uff08SRS\uff09\u4f7f\u7528\u8131\u79bb\u4e0a\u4e0b\u6587\u7684\u590d\u4e60\u65b9\u5f0f\uff0c\u5ffd\u89c6\u4e86\u4e0a\u4e0b\u6587\u7684\u4f5c\u7528\uff0c\u800c\u4e2a\u4eba\u77e5\u8bc6\u7ba1\u7406\uff08PKM\uff09\u5de5\u5177\u5219\u9700\u8981\u9ad8\u4eba\u5de5\u7ef4\u62a4\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u201c\u89c1\u89e3\u56de\u5fc6\u201d\u8fd9\u4e00\u65b0\u8303\u5f0f\uff0c\u5c06\u4e0a\u4e0b\u6587\u89e6\u53d1\u7684\u4e2a\u4eba\u8fc7\u53bb\u89c1\u89e3\u68c0\u7d22\u6982\u5ff5\u5316\u4e3a\u4fc3\u8fdbSRL\u7684\u5143\u8ba4\u77e5\u652f\u67b6\u3002\u6211\u4eec\u57fa\u4e8eJust-in-Time Adaptive Intervention\uff08JITAI\uff09\u6846\u67b6\u5f62\u5f0f\u5316\u4e86\u8fd9\u4e00\u8303\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u539f\u578b\u7cfb\u7edfIrec\u4ee5\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002Irec\u7684\u6838\u5fc3\u662f\u7528\u6237\u5b66\u4e60\u5386\u53f2\u7684\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u3002\u5f53\u7528\u6237\u9762\u5bf9\u65b0\u95ee\u9898\u65f6\uff0c\u6df7\u5408\u68c0\u7d22\u5f15\u64ce\u4f1a\u53ec\u56de\u76f8\u5173\u7684\u4e2a\u4eba\u201c\u89c1\u89e3\u201d\u3002\u968f\u540e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6df1\u5ea6\u76f8\u4f3c\u6027\u8bc4\u4f30\uff0c\u7b5b\u9009\u5e76\u5373\u65f6\u5448\u73b0\u6700\u76f8\u5173\u7684\u652f\u67b6\u3002\u4e3a\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0cIrec\u8bbe\u8ba1\u4e86\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6d41\u7a0b\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u53ef\u9009\u7684\u201c\u5f15\u5bfc\u63a2\u7a76\u201d\u6a21\u5757\uff0c\u7528\u6237\u53ef\u4ee5\u4e0e\u4e13\u5bb6LLM\u8fdb\u884c\u82cf\u683c\u62c9\u5e95\u5f0f\u5bf9\u8bdd\uff0c\u4ee5\u5f53\u524d\u95ee\u9898\u548c\u53ec\u56de\u89c1\u89e3\u4e3a\u80cc\u666f\u3002\u672c\u6587\u7684\u8d21\u732e\u5728\u4e8e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u575a\u5b9e\u7684\u7406\u8bba\u6846\u67b6\u548c\u53ef\u7528\u7684\u7cfb\u7edf\u5e73\u53f0\uff0c\u7528\u4e8e\u8bbe\u8ba1\u589e\u5f3a\u5143\u8ba4\u77e5\u548c\u81ea\u6211\u8c03\u8282\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3\u667a\u80fd\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2506.20159", "pdf": "https://arxiv.org/pdf/2506.20159", "abs": "https://arxiv.org/abs/2506.20159", "authors": ["Tomas Herda", "Victoria Pichler", "Zheying Zhang", "Pekka Abrahamsson", "Geir K. Hanssen"], "title": "AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The full-day workshop on AI and Agile at XP 2025 convened a diverse group of\nresearchers and industry practitioners to address the practical challenges and\nopportunities of integrating Artificial Intelligence into Agile software\ndevelopment. Through interactive sessions, participants identified shared\nfrustrations related to integrating AI into Agile Software Development\npractices, including challenges with tooling, governance, data quality, and\ncritical skill gaps. These challenges were systematically prioritized and\nanalyzed to uncover root causes. The workshop culminated in the collaborative\ndevelopment of a research roadmap that pinpoints actionable directions for\nfuture work, including both immediate solutions and ambitious long-term goals.\nThe key outcome is a structured agenda designed to foster joint\nindustry-academic efforts to move from identified frustrations to successful\nimplementation.", "AI": {"tldr": "XP2025\u7814\u8ba8\u4f1a\u7684\u5168\u5929\u5de5\u4f5c\u574a\u805a\u7126AI\u4e0e\u654f\u6377\u5f00\u53d1\u7684\u7ed3\u5408\uff0c\u63a2\u8ba8\u4e86\u5de5\u5177\u3001\u6cbb\u7406\u3001\u6570\u636e\u8d28\u91cf\u548c\u6280\u80fd\u5dee\u8ddd\u7b49\u6311\u6218\uff0c\u5e76\u5236\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u8ba8\u4f1a\u65e8\u5728\u89e3\u51b3\u5c06\u4eba\u5de5\u667a\u80fd\u6574\u5408\u5230\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u6311\u6218\uff0c\u4fc3\u8fdb\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u7684\u5408\u4f5c\u3002", "method": "\u901a\u8fc7\u4e92\u52a8\u73af\u8282\uff0c\u53c2\u4e0e\u8005\u8bc6\u522b\u5e76\u7cfb\u7edf\u5206\u6790\u4e86AI\u4e0e\u654f\u6377\u5f00\u53d1\u7ed3\u5408\u7684\u75db\u70b9\uff0c\u5e76\u5171\u540c\u5236\u5b9a\u4e86\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "result": "\u7814\u8ba8\u4f1a\u660e\u786e\u4e86\u5de5\u5177\u3001\u6cbb\u7406\u3001\u6570\u636e\u8d28\u91cf\u548c\u6280\u80fd\u5dee\u8ddd\u7b49\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u77ed\u671f\u548c\u957f\u671f\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u7814\u8ba8\u4f1a\u6210\u679c\u4e3a\u672a\u6765AI\u4e0e\u654f\u6377\u5f00\u53d1\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bae\u7a0b\uff0c\u63a8\u52a8\u4ece\u95ee\u9898\u5230\u6210\u529f\u5b9e\u65bd\u7684\u5408\u4f5c\u52aa\u529b\u3002", "paper_title_zh": "AI\u4e0e\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\uff1a\u4ece\u632b\u6298\u5230\u6210\u529f\u2014\u2014XP2025\u7814\u8ba8\u4f1a\u603b\u7ed3", "abstract_zh": "XP2025\u5173\u4e8eAI\u4e0e\u654f\u6377\u7684\u5168\u5929\u5de5\u4f5c\u574a\u6c47\u96c6\u4e86\u591a\u6837\u5316\u7684\u7814\u7a76\u4eba\u5458\u548c\u884c\u4e1a\u4ece\u4e1a\u8005\uff0c\u63a2\u8ba8\u4e86\u5c06\u4eba\u5de5\u667a\u80fd\u6574\u5408\u5230\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u6311\u6218\u548c\u673a\u9047\u3002\u901a\u8fc7\u4e92\u52a8\u73af\u8282\uff0c\u53c2\u4e0e\u8005\u8bc6\u522b\u4e86\u4e0eAI\u548c\u654f\u6377\u5f00\u53d1\u7ed3\u5408\u76f8\u5173\u7684\u5171\u540c\u632b\u6298\uff0c\u5305\u62ec\u5de5\u5177\u3001\u6cbb\u7406\u3001\u6570\u636e\u8d28\u91cf\u548c\u5173\u952e\u6280\u80fd\u5dee\u8ddd\u7b49\u6311\u6218\u3002\u8fd9\u4e9b\u95ee\u9898\u88ab\u7cfb\u7edf\u6027\u5730\u4f18\u5148\u6392\u5e8f\u548c\u5206\u6790\uff0c\u4ee5\u63ed\u793a\u6839\u672c\u539f\u56e0\u3002\u7814\u8ba8\u4f1a\u6700\u7ec8\u534f\u4f5c\u5236\u5b9a\u4e86\u4e00\u4efd\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u660e\u786e\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u53ef\u884c\u65b9\u5411\uff0c\u5305\u62ec\u5373\u65f6\u89e3\u51b3\u65b9\u6848\u548c\u96c4\u5fc3\u52c3\u52c3\u7684\u957f\u671f\u76ee\u6807\u3002\u5173\u952e\u6210\u679c\u662f\u4e00\u4efd\u7ed3\u6784\u5316\u8bae\u7a0b\uff0c\u65e8\u5728\u4fc3\u8fdb\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u7684\u5171\u540c\u52aa\u529b\uff0c\u4ece\u5df2\u8bc6\u522b\u7684\u632b\u6298\u8fc8\u5411\u6210\u529f\u5b9e\u65bd\u3002"}}
{"id": "2506.20164", "pdf": "https://arxiv.org/pdf/2506.20164", "abs": "https://arxiv.org/abs/2506.20164", "authors": ["Mototaka Suzuki", "Jaan Aru"], "title": "Do psychic cells generate consciousness?", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Technological advances in the past decades have begun to enable\nneuroscientists to address fundamental questions about consciousness in an\nunprecedented way. Here we review remarkable recent progress in our\nunderstanding of cellular-level mechanisms of conscious processing in the\nbrain. Of particular interest are the cortical pyramidal neurons -- or \"psychic\ncells\" called by Ram\\'on y Cajal more than 100 years ago -- which have an\nintriguing cellular mechanism that accounts for selective disruption of\nfeedback signaling in the brain upon anesthetic-induced loss of consciousness.\nImportantly, a particular class of metabotropic receptors distributed over the\ndendrites of pyramidal cells are highlighted as the key cellular mechanism.\nAfter all, Cajal's instinct over a century ago may turn out to be correct -- we\nmay have just begun to understand whether and how psychic cells indeed generate\nand control our consciousness.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u8111\u4e2d\u76ae\u5c42\u9525\u4f53\u795e\u7ecf\u5143\uff08\u5373\"\u5fc3\u7075\u7ec6\u80de\"\uff09\u662f\u5426\u4ea7\u751f\u610f\u8bc6\uff0c\u5e76\u56de\u987e\u4e86\u8fd1\u5e74\u6765\u5728\u7ec6\u80de\u6c34\u5e73\u4e0a\u7406\u89e3\u610f\u8bc6\u5904\u7406\u673a\u5236\u7684\u8fdb\u5c55\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u79d1\u5b66\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u7814\u7a76\u8005\u5f00\u59cb\u4ee5\u524d\u6240\u672a\u6709\u7684\u65b9\u5f0f\u63a2\u7d22\u610f\u8bc6\u7684\u57fa\u672c\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u56de\u987e\u5e76\u5206\u6790\u76ae\u5c42\u9525\u4f53\u795e\u7ecf\u5143\u5728\u610f\u8bc6\u751f\u6210\u4e2d\u7684\u6f5c\u5728\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u8fd1\u5e74\u6765\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u76ae\u5c42\u9525\u4f53\u795e\u7ecf\u5143\u7684\u7ec6\u80de\u673a\u5236\uff0c\u7279\u522b\u662f\u5176\u6811\u7a81\u4e0a\u5206\u5e03\u7684\u4ee3\u8c22\u578b\u53d7\u4f53\u5728\u9ebb\u9189\u8bf1\u5bfc\u610f\u8bc6\u4e27\u5931\u65f6\u7684\u53cd\u9988\u4fe1\u53f7\u9009\u62e9\u6027\u4e2d\u65ad\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u76ae\u5c42\u9525\u4f53\u795e\u7ecf\u5143\u7684\u7279\u5b9a\u7ec6\u80de\u673a\u5236\uff08\u5c24\u5176\u662f\u4ee3\u8c22\u578b\u53d7\u4f53\uff09\u53ef\u80fd\u662f\u610f\u8bc6\u751f\u6210\u548c\u63a7\u5236\u7684\u5173\u952e\u3002", "conclusion": "\u62c9\u8499\u00b7\u5361\u54c8\u5c14\u4e00\u4e2a\u4e16\u7eaa\u524d\u7684\u76f4\u89c9\u53ef\u80fd\u662f\u6b63\u786e\u7684\uff0c\u5fc3\u7075\u7ec6\u80de\u53ef\u80fd\u786e\u5b9e\u5728\u610f\u8bc6\u7684\u751f\u6210\u548c\u63a7\u5236\u4e2d\u53d1\u6325\u6838\u5fc3\u4f5c\u7528\u3002", "paper_title_zh": "\u5fc3\u7075\u7ec6\u80de\u662f\u5426\u4ea7\u751f\u610f\u8bc6\uff1f", "abstract_zh": "\u8fc7\u53bb\u51e0\u5341\u5e74\u7684\u6280\u672f\u8fdb\u6b65\u4f7f\u795e\u7ecf\u79d1\u5b66\u5bb6\u80fd\u591f\u4ee5\u524d\u6240\u672a\u6709\u7684\u65b9\u5f0f\u63a2\u7d22\u610f\u8bc6\u7684\u57fa\u672c\u95ee\u9898\u3002\u672c\u6587\u56de\u987e\u4e86\u8fd1\u5e74\u6765\u5728\u7406\u89e3\u5927\u8111\u4e2d\u610f\u8bc6\u5904\u7406\u7684\u7ec6\u80de\u6c34\u5e73\u673a\u5236\u65b9\u9762\u7684\u663e\u8457\u8fdb\u5c55\u3002\u7279\u522b\u5f15\u4eba\u5173\u6ce8\u7684\u662f\u76ae\u5c42\u9525\u4f53\u795e\u7ecf\u5143\u2014\u2014\u62c9\u8499\u00b7\u5361\u54c8\u5c14\u5728\u4e00\u767e\u591a\u5e74\u524d\u79f0\u4e4b\u4e3a\"\u5fc3\u7075\u7ec6\u80de\"\u2014\u2014\u5176\u72ec\u7279\u7684\u7ec6\u80de\u673a\u5236\u89e3\u91ca\u4e86\u9ebb\u9189\u8bf1\u5bfc\u610f\u8bc6\u4e27\u5931\u65f6\u5927\u8111\u53cd\u9988\u4fe1\u53f7\u7684\u9009\u62e9\u6027\u4e2d\u65ad\u3002\u91cd\u8981\u7684\u662f\uff0c\u5206\u5e03\u5728\u9525\u4f53\u7ec6\u80de\u6811\u7a81\u4e0a\u7684\u4e00\u7c7b\u7279\u5b9a\u4ee3\u8c22\u578b\u53d7\u4f53\u88ab\u5f3a\u8c03\u4e3a\u5173\u952e\u7684\u7ec6\u80de\u673a\u5236\u3002\u6700\u7ec8\uff0c\u5361\u54c8\u5c14\u4e00\u4e2a\u4e16\u7eaa\u524d\u7684\u76f4\u89c9\u53ef\u80fd\u662f\u6b63\u786e\u7684\u2014\u2014\u6211\u4eec\u53ef\u80fd\u521a\u521a\u5f00\u59cb\u7406\u89e3\u5fc3\u7075\u7ec6\u80de\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u751f\u6210\u548c\u63a7\u5236\u6211\u4eec\u7684\u610f\u8bc6\u3002"}}
{"id": "2506.20173", "pdf": "https://arxiv.org/pdf/2506.20173", "abs": "https://arxiv.org/abs/2506.20173", "authors": ["Mahmoud Hegazy", "Liviu Aolaritei", "Michael I. Jordan", "Aymeric Dieuleveut"], "title": "Valid Selection among Conformal Sets", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME", "stat.OT"], "comment": null, "summary": "Conformal prediction offers a distribution-free framework for constructing\nprediction sets with coverage guarantees. In practice, multiple valid conformal\nprediction sets may be available, arising from different models or\nmethodologies. However, selecting the most desirable set, such as the smallest,\ncan invalidate the coverage guarantees. To address this challenge, we propose a\nstability-based approach that ensures coverage for the selected prediction set.\nWe extend our results to the online conformal setting, propose several\nrefinements in settings where additional structure is available, and\ndemonstrate its effectiveness through experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u4e2a\u6709\u6548\u7684\u5171\u5f62\u9884\u6d4b\u96c6\u4e2d\u9009\u62e9\u6700\u4f18\u96c6\uff0c\u540c\u65f6\u786e\u4fdd\u8986\u76d6\u7387\u7684\u6709\u6548\u6027\uff0c\u5e76\u6269\u5c55\u5230\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u573a\u666f\u3002", "motivation": "\u5171\u5f62\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u5e03\u65e0\u5173\u7684\u6846\u67b6\u6765\u6784\u5efa\u5177\u6709\u8986\u76d6\u7387\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7684\u9884\u6d4b\u96c6\uff08\u5982\u4e0d\u540c\u6a21\u578b\u6216\u65b9\u6cd5\u751f\u6210\uff09\uff0c\u9009\u62e9\u6700\u4f18\u96c6\uff08\u5982\u6700\u5c0f\u96c6\uff09\u53ef\u80fd\u4f1a\u7834\u574f\u8986\u76d6\u7387\u4fdd\u8bc1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u786e\u4fdd\u6240\u9009\u96c6\u7684\u8986\u76d6\u7387\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u6240\u9009\u9884\u6d4b\u96c6\u7684\u8986\u76d6\u7387\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u6269\u5c55\u5230\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u573a\u666f\uff0c\u5e76\u5728\u5177\u6709\u989d\u5916\u7ed3\u6784\u7684\u573a\u666f\u4e2d\u63d0\u51fa\u4e86\u591a\u79cd\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u786e\u4fdd\u6240\u9009\u9884\u6d4b\u96c6\u7684\u8986\u76d6\u7387\uff0c\u5e76\u5728\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u548c\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u4e3a\u5728\u591a\u4e2a\u5171\u5f62\u9884\u6d4b\u96c6\u4e2d\u9009\u62e9\u6700\u4f18\u96c6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u5728\u7ebf\u548c\u7ed3\u6784\u5316\u573a\u666f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "paper_title_zh": "\u5171\u5f62\u9884\u6d4b\u96c6\u7684\u6709\u6548\u9009\u62e9", "abstract_zh": "\u5171\u5f62\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u5e03\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u5177\u6709\u8986\u76d6\u7387\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7684\u5171\u5f62\u9884\u6d4b\u96c6\uff08\u5982\u4e0d\u540c\u6a21\u578b\u6216\u65b9\u6cd5\u751f\u6210\uff09\uff0c\u4f46\u9009\u62e9\u6700\u4f18\u96c6\uff08\u5982\u6700\u5c0f\u96c6\uff09\u53ef\u80fd\u4f1a\u7834\u574f\u8986\u76d6\u7387\u4fdd\u8bc1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u6240\u9009\u9884\u6d4b\u96c6\u7684\u8986\u76d6\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u573a\u666f\uff0c\u5e76\u5728\u5177\u6709\u989d\u5916\u7ed3\u6784\u7684\u573a\u666f\u4e2d\u63d0\u51fa\u4e86\u591a\u79cd\u6539\u8fdb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.20197", "pdf": "https://arxiv.org/pdf/2506.20197", "abs": "https://arxiv.org/abs/2506.20197", "authors": ["Cl\u00e9ment L. Canonne", "Yash Pote", "Uddalok Sarkar"], "title": "Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "16 pages, 4 figures", "summary": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u5f52\u56e0\u5de5\u5177Anubis\uff0c\u901a\u8fc7\u5206\u5e03\u6d4b\u8bd5\u65b9\u6cd5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u7684\u5f52\u5c5e\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u7684\u666e\u53ca\uff0c\u5982\u4f55\u51c6\u786e\u5f52\u56e0\u4ee3\u7801\u6765\u6e90\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5047\u8bbe\u6d4b\u8bd5\u6280\u672f\uff0c\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u4e0b\u7684\u5f52\u56e0\u96be\u9898\u3002", "method": "\u63d0\u51faAnubis\u5de5\u5177\uff0c\u5c06\u5f52\u56e0\u95ee\u9898\u8f6c\u5316\u4e3a\u5206\u5e03\u6d4b\u8bd5\u95ee\u9898\uff0c\u5229\u7528\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u548c\u5bc6\u5ea6\u4f30\u8ba1\u8fdb\u884c\u96f6\u6837\u672c\u5f52\u56e0\u3002", "result": "\u5728\u4ee3\u7801\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAnubis\u4ec5\u9700\u7ea62000\u4e2a\u6837\u672c\u5373\u53ef\u533a\u5206\u4e0d\u540c\u6a21\u578b\uff08\u5982DeepSeek-Coder\u3001CodeGemma\u548cStable-Code\uff09\uff0cAUROC\u5f97\u5206\u22650.9\u3002", "conclusion": "Anubis\u4e3a\u96f6\u6837\u672c\u5f52\u56e0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u573a\u666f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u5f52\u56e0\uff1a\u4e00\u79cd\u5206\u5e03\u6d4b\u8bd5\u65b9\u6cd5", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u4ee3\u7801\u5360\u6bd4\u4e0d\u65ad\u589e\u52a0\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u5047\u8bbe\u6d4b\u8bd5\u6280\u672f\u5bf9\u8fd9\u4e9b\u4ee3\u7801\u8fdb\u884c\u5f52\u56e0\u3002\u7ed9\u5b9a\u4e00\u7ec4\u6837\u672c$S$\u548c\u4e00\u4e2a\u53ef\u7591\u6a21\u578b$\\mathcal{L}^*$\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u8bc4\u4f30$S$\u6765\u81ea$\\mathcal{L}^*$\u7684\u53ef\u80fd\u6027\u3002\u7531\u4e8e\u7ef4\u5ea6\u707e\u96be\uff0c\u4ec5\u51edLLM\u7684\u6837\u672c\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff1a\u4e3a\u6b64\uff0c\u6211\u4eec\u5229\u7528\u4e86LLM\u7684\u6837\u672c\u548c\u5bc6\u5ea6\u4f30\u8ba1\uff08\u8fd9\u79cd\u8bbf\u95ee\u65b9\u5f0f\u901a\u5e38\u53ef\u7528\uff09\u3002\n  \u6211\u4eec\u63d0\u51fa\u4e86$\\mathsf{Anubis}$\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u5f52\u56e0\u5de5\u5177\uff0c\u5c06\u5f52\u56e0\u95ee\u9898\u8f6c\u5316\u4e3a\u5206\u5e03\u6d4b\u8bd5\u95ee\u9898\u3002\u5728\u4ee3\u7801\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c$\\mathsf{Anubis}$\u4ec5\u9700\u7ea62000\u4e2a\u6837\u672c\u5373\u53ef\u533a\u5206DeepSeek-Coder\u3001CodeGemma\u548cStable-Code\u7b49\u6a21\u578b\uff0cAUROC\u5f97\u5206\u22650.9\u3002"}}
{"id": "2506.20204", "pdf": "https://arxiv.org/pdf/2506.20204", "abs": "https://arxiv.org/abs/2506.20204", "authors": ["Eduardo Gutierrez Maestro", "Hadi Banaee", "Amy Loutfi"], "title": "Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Affective priming exemplifies the challenge of ambiguity in affective\ncomputing. While the community has largely addressed this issue from a\nlabel-based perspective, identifying data points in the sequence affected by\nthe priming effect, the impact of priming on data itself, particularly in\nphysiological signals, remains underexplored. Data affected by priming can lead\nto misclassifications when used in learning models. This study proposes the\nAffective Priming Score (APS), a data-driven method to detect data points\ninfluenced by the priming effect. The APS assigns a score to each data point,\nquantifying the extent to which it is affected by priming. To validate this\nmethod, we apply it to the SEED and SEED-VII datasets, which contain sufficient\ntransitions between emotional events to exhibit priming effects. We train\nmodels with the same configuration using both the original data and\npriming-free sequences. The misclassification rate is significantly reduced\nwhen using priming-free sequences compared to the original data. This work\ncontributes to the broader challenge of ambiguity by identifying and mitigating\npriming effects at the data level, enhancing model robustness, and offering\nvaluable insights for the design and collection of affective computing\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u2014\u2014\u60c5\u611f\u542f\u52a8\u5206\u6570\uff08APS\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u5e8f\u5217\u6570\u636e\u4e2d\u53d7\u542f\u52a8\u6548\u5e94\u5f71\u54cd\u7684\u6570\u636e\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u51cf\u5c11\u5206\u7c7b\u9519\u8bef\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u4e2d\u542f\u52a8\u6548\u5e94\u7684\u6a21\u7cca\u6027\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u591a\u4ece\u6807\u7b7e\u89d2\u5ea6\u89e3\u51b3\uff0c\u4f46\u542f\u52a8\u6548\u5e94\u5bf9\u6570\u636e\u672c\u8eab\uff08\u5c24\u5176\u662f\u751f\u7406\u4fe1\u53f7\uff09\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u53d7\u542f\u52a8\u5f71\u54cd\u7684\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6a21\u578b\u7684\u8bef\u5206\u7c7b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6570\u636e\u5c42\u9762\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faAPS\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u5206\u914d\u4e00\u4e2a\u5206\u6570\uff0c\u91cf\u5316\u5176\u53d7\u542f\u52a8\u6548\u5e94\u5f71\u54cd\u7684\u7a0b\u5ea6\u3002\u901a\u8fc7\u5728SEED\u548cSEED-VII\u6570\u636e\u96c6\u4e0a\u5e94\u7528APS\uff0c\u5bf9\u6bd4\u539f\u59cb\u6570\u636e\u4e0e\u53bb\u542f\u52a8\u5e8f\u5217\u7684\u5206\u7c7b\u6548\u679c\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u53bb\u542f\u52a8\u5e8f\u5217\u540e\uff0c\u6a21\u578b\u7684\u8bef\u5206\u7c7b\u7387\u663e\u8457\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86APS\u5728\u68c0\u6d4b\u548c\u6d88\u9664\u542f\u52a8\u6548\u5e94\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6570\u636e\u5c42\u9762\u7684\u542f\u52a8\u6548\u5e94\u68c0\u6d4b\u4e0e\u6d88\u9664\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u4e0e\u6536\u96c6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u60c5\u611f\u542f\u52a8\u5206\u6570\uff1a\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u7528\u4e8e\u68c0\u6d4b\u5e8f\u5217\u6570\u636e\u96c6\u4e2d\u7684\u542f\u52a8\u6548\u5e94", "abstract_zh": "\u60c5\u611f\u542f\u52a8\u6548\u5e94\u51f8\u663e\u4e86\u60c5\u611f\u8ba1\u7b97\u4e2d\u6a21\u7cca\u6027\u7684\u6311\u6218\u3002\u5c3d\u7ba1\u5b66\u754c\u5df2\u4ece\u6807\u7b7e\u89d2\u5ea6\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u542f\u52a8\u6548\u5e94\u5bf9\u6570\u636e\u672c\u8eab\uff08\u5c24\u5176\u662f\u751f\u7406\u4fe1\u53f7\uff09\u7684\u5f71\u54cd\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u53d7\u542f\u52a8\u5f71\u54cd\u7684\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6a21\u578b\u7684\u8bef\u5206\u7c7b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u60c5\u611f\u542f\u52a8\u5206\u6570\uff08APS\uff09\uff0c\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u53d7\u542f\u52a8\u6548\u5e94\u5f71\u54cd\u7684\u6570\u636e\u70b9\u3002APS\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u5206\u914d\u4e00\u4e2a\u5206\u6570\uff0c\u91cf\u5316\u5176\u53d7\u542f\u52a8\u5f71\u54cd\u7684\u7a0b\u5ea6\u3002\u4e3a\u9a8c\u8bc1\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8eSEED\u548cSEED-VII\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5305\u542b\u8db3\u591f\u7684\u60c5\u611f\u4e8b\u4ef6\u8f6c\u6362\u4ee5\u5c55\u73b0\u542f\u52a8\u6548\u5e94\u3002\u6211\u4eec\u4f7f\u7528\u76f8\u540c\u914d\u7f6e\u8bad\u7ec3\u6a21\u578b\uff0c\u5206\u522b\u57fa\u4e8e\u539f\u59cb\u6570\u636e\u548c\u53bb\u542f\u52a8\u5e8f\u5217\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u53bb\u542f\u52a8\u5e8f\u5217\u540e\uff0c\u8bef\u5206\u7c7b\u7387\u663e\u8457\u964d\u4f4e\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6570\u636e\u5c42\u9762\u7684\u542f\u52a8\u6548\u5e94\u8bc6\u522b\u4e0e\u6d88\u9664\uff0c\u4e3a\u89e3\u51b3\u6a21\u7cca\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u60c5\u611f\u8ba1\u7b97\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u4e0e\u6536\u96c6\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2506.20235", "pdf": "https://arxiv.org/pdf/2506.20235", "abs": "https://arxiv.org/abs/2506.20235", "authors": ["Yuyang Zhang", "Xu Shen", "Yu Xie", "Ka-Chun Wong", "Weidun Xie", "Chengbin Peng"], "title": "Directed Link Prediction using GNN with Local and Global Feature Fusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Link prediction is a classical problem in graph analysis with many practical\napplications. For directed graphs, recently developed deep learning approaches\ntypically analyze node similarities through contrastive learning and aggregate\nneighborhood information through graph convolutions. In this work, we propose a\nnovel graph neural network (GNN) framework to fuse feature embedding with\ncommunity information. We theoretically demonstrate that such hybrid features\ncan improve the performance of directed link prediction. To utilize such\nfeatures efficiently, we also propose an approach to transform input graphs\ninto directed line graphs so that nodes in the transformed graph can aggregate\nmore information during graph convolutions. Experiments on benchmark datasets\nshow that our approach outperforms the state-of-the-art in most cases when 30%,\n40%, 50%, and 60% of the connected links are used as training data,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u6709\u5411\u56fe\u7684\u94fe\u63a5\u9884\u6d4b\uff0c\u901a\u8fc7\u5c06\u8f93\u5165\u56fe\u8f6c\u6362\u4e3a\u6709\u5411\u7ebf\u56fe\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u79cd\u8bad\u7ec3\u6570\u636e\u6bd4\u4f8b\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6709\u5411\u56fe\u7684\u94fe\u63a5\u9884\u6d4b\u662f\u56fe\u5206\u6790\u4e2d\u7684\u7ecf\u5178\u95ee\u9898\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5206\u6790\u8282\u70b9\u76f8\u4f3c\u6027\u6216\u901a\u8fc7\u56fe\u5377\u79ef\u805a\u5408\u90bb\u57df\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u7279\u5f81\u5d4c\u5165\u4e0e\u793e\u533a\u4fe1\u606f\uff0c\u63d0\u5347\u6709\u5411\u94fe\u63a5\u9884\u6d4b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u878d\u5408\u7279\u5f81\u5d4c\u5165\u4e0e\u793e\u533a\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u8fd9\u79cd\u6df7\u5408\u7279\u5f81\u80fd\u63d0\u5347\u6027\u80fd\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u5c06\u8f93\u5165\u56fe\u8f6c\u6362\u4e3a\u6709\u5411\u7ebf\u56fe\u7684\u65b9\u6cd5\uff0c\u4f7f\u8282\u70b9\u5728\u56fe\u5377\u79ef\u4e2d\u80fd\u805a\u5408\u66f4\u591a\u4fe1\u606f\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u8bad\u7ec3\u6570\u636e\u6bd4\u4f8b\u4e3a30%\u300140%\u300150%\u548c60%\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684GNN\u6846\u67b6\u53ca\u6709\u5411\u7ebf\u56fe\u8f6c\u6362\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6709\u5411\u94fe\u63a5\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6709\u5411\u94fe\u63a5\u9884\u6d4b", "abstract_zh": "\u94fe\u63a5\u9884\u6d4b\u662f\u56fe\u5206\u6790\u4e2d\u7684\u4e00\u4e2a\u7ecf\u5178\u95ee\u9898\uff0c\u5177\u6709\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u3002\u5bf9\u4e8e\u6709\u5411\u56fe\uff0c\u8fd1\u671f\u53d1\u5c55\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5206\u6790\u8282\u70b9\u76f8\u4f3c\u6027\uff0c\u5e76\u901a\u8fc7\u56fe\u5377\u79ef\u805a\u5408\u90bb\u57df\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6846\u67b6\uff0c\u5c06\u7279\u5f81\u5d4c\u5165\u4e0e\u793e\u533a\u4fe1\u606f\u878d\u5408\u3002\u6211\u4eec\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u8fd9\u79cd\u6df7\u5408\u7279\u5f81\u53ef\u4ee5\u63d0\u5347\u6709\u5411\u94fe\u63a5\u9884\u6d4b\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u9ad8\u6548\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8f93\u5165\u56fe\u8f6c\u6362\u4e3a\u6709\u5411\u7ebf\u56fe\u7684\u65b9\u6cd5\uff0c\u4f7f\u8f6c\u6362\u540e\u56fe\u4e2d\u7684\u8282\u70b9\u5728\u56fe\u5377\u79ef\u8fc7\u7a0b\u4e2d\u80fd\u805a\u5408\u66f4\u591a\u4fe1\u606f\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u5206\u522b\u4f7f\u752830%\u300140%\u300150%\u548c60%\u7684\u8fde\u63a5\u94fe\u63a5\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2506.20251", "pdf": "https://arxiv.org/pdf/2506.20251", "abs": "https://arxiv.org/abs/2506.20251", "authors": ["Kejia Chen", "Jiawen Zhang", "Jiacong Hu", "Yu Wang", "Jian Lou", "Zunlei Feng", "Mingli Song"], "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faQ-resafe\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u7684\u5b89\u5168\u8865\u4e01\u4fee\u590d\u5176\u5b89\u5168\u80fd\u529b\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6a21\u578b\u6548\u7528\u7684\u5f71\u54cd\u3002", "motivation": "\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u7814\u7a76\u8868\u660e\u91cf\u5316\u53ef\u80fd\u635f\u5bb3\u5176\u5b89\u5168\u80fd\u529b\uff0c\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u548c\u4fee\u590d\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u4e3b\u6d41\u91cf\u5316\u6280\u672f\u548c\u591a\u6837\u5316\u6821\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5b89\u5168\u8bc4\u4f30\uff0c\u63d0\u51faQ-resafe\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u611f\u77e5\u65b9\u5f0f\u4fee\u590d\u91cf\u5316LLMs\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQ-resafe\u6210\u529f\u5c06\u91cf\u5316LLMs\u7684\u5b89\u5168\u80fd\u529b\u6062\u590d\u81f3\u91cf\u5316\u524d\u6c34\u5e73\uff0c\u5373\u4f7f\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Q-resafe\u4e3a\u91cf\u5316LLMs\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u5b89\u5168\u6027\u4e0e\u6a21\u578b\u6548\u7528\u3002", "paper_title_zh": "Q-resafe\uff1a\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u8bc4\u4f30\u4e0e\u91cf\u5316\u611f\u77e5\u5b89\u5168\u8865\u4e01", "abstract_zh": "\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u56e0\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u6f5c\u529b\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u67d0\u4e9b\u65e0\u9700\u6821\u51c6\u6570\u636e\u96c6\u7684\u91cf\u5316\u65b9\u6cd5\u53ef\u80fd\u635f\u5bb3LLMs\u7684\u5b89\u5168\u80fd\u529b\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30\u548c\u4fee\u590d\u7b56\u7565\u3002\u672c\u6587\u901a\u8fc7\u4e3b\u6d41\u91cf\u5316\u6280\u672f\u548c\u591a\u6837\u5316\u6821\u51c6\u6570\u636e\u96c6\uff0c\u5229\u7528\u5e7f\u6cdb\u8ba4\u53ef\u7684\u5b89\u5168\u57fa\u51c6\u8fdb\u884c\u5168\u9762\u5b89\u5168\u8bc4\u4f30\u3002\u9488\u5bf9\u53d1\u73b0\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u91cf\u5316\u611f\u77e5\u5b89\u5168\u8865\u4e01\u6846\u67b6Q-resafe\uff0c\u4ee5\u9ad8\u6548\u6062\u590d\u91cf\u5316LLMs\u7684\u5b89\u5168\u80fd\u529b\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6548\u7528\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cQ-resafe\u6210\u529f\u5c06\u91cf\u5316LLMs\u7684\u5b89\u5168\u6027\u4e0e\u91cf\u5316\u524d\u6a21\u578b\u5bf9\u9f50\uff0c\u5373\u4f7f\u5728\u6311\u6218\u6027\u8bc4\u4f30\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\u3002\u9879\u76ee\u9875\u9762\u89c1\uff1ahttps://github.com/Thecommonirin/Qresafe\u3002"}}
{"id": "2506.20253", "pdf": "https://arxiv.org/pdf/2506.20253", "abs": "https://arxiv.org/abs/2506.20253", "authors": ["Ben Gerhards", "Nikita Popkov", "Annekatrin K\u00f6nig", "Marcel Arpogaus", "Bastian Sch\u00e4fermeier", "Leonie Riedl", "Stephan Vogt", "Philip Hehlert"], "title": "Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Forecasting attracts a lot of research attention in the electricity value\nchain. However, most studies concentrate on short-term forecasting of\ngeneration or consumption with a focus on systems and less on individual\nconsumers. Even more neglected is the topic of long-term forecasting of\nindividual power consumption.\n  Here, we provide an in-depth comparative evaluation of data-driven methods\nfor generating synthetic time series data tailored to energy consumption\nlong-term forecasting. High-fidelity synthetic data is crucial for a wide range\nof applications, including state estimations in energy systems or power grid\nplanning. In this study, we assess and compare the performance of multiple\nstate-of-the-art but less common techniques: a hybrid Wasserstein Generative\nAdversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),\nHidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial\nnormalizing Flows (MABF). We analyze the ability of each method to replicate\nthe temporal dynamics, long-range dependencies, and probabilistic transitions\ncharacteristic of individual energy consumption profiles. Our comparative\nevaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and\nMABF aiding in selecting the most suitable approach for state estimations and\nother energy-related tasks. Our generation and analysis framework aims to\nenhance the accuracy and reliability of synthetic power consumption data while\ngenerating data that fulfills criteria like anonymisation - preserving privacy\nconcerns mitigating risks of specific profiling of single customers. This study\nutilizes an open-source dataset from households in Germany with 15min time\nresolution. The generated synthetic power profiles can readily be used in\napplications like state estimations or consumption forecasting.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u6bd4\u8f83\u4e86\u591a\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08WGAN\u3001DDPM\u3001HMM\u548cMABF\uff09\u5728\u751f\u6210\u7528\u4e8e\u957f\u671f\u80fd\u6e90\u6d88\u8d39\u9884\u6d4b\u7684\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65b9\u9762\u7684\u6027\u80fd\uff0c\u65e8\u5728\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u51c6\u786e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u7535\u529b\u4ef7\u503c\u94fe\u4e2d\u7684\u9884\u6d4b\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u77ed\u671f\u53d1\u7535\u6216\u6d88\u8d39\u9884\u6d4b\uff0c\u800c\u4e2a\u4f53\u6d88\u8d39\u8005\u7684\u957f\u671f\u6d88\u8d39\u9884\u6d4b\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u4ee5\u652f\u6301\u80fd\u6e90\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u548c\u7535\u7f51\u89c4\u5212\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u65b9\u6cd5\uff1a\u6df7\u5408Wasserstein\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08WGAN\uff09\u3001\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u3001\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u548c\u63a9\u7801\u81ea\u56de\u5f52\u4f2f\u6069\u65af\u5766\u591a\u9879\u5f0f\u5f52\u4e00\u5316\u6d41\uff08MABF\uff09\uff0c\u5206\u6790\u5176\u5728\u590d\u5236\u4e2a\u4f53\u80fd\u6e90\u6d88\u8d39\u65f6\u95f4\u52a8\u6001\u3001\u957f\u7a0b\u4f9d\u8d56\u6027\u548c\u6982\u7387\u8f6c\u79fb\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u63ed\u793a\u4e86WGAN\u3001DDPM\u3001HMM\u548cMABF\u5728\u751f\u6210\u5408\u6210\u80fd\u6e90\u6d88\u8d39\u6570\u636e\u65f6\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u5176\u4ed6\u80fd\u6e90\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u4f9d\u636e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u751f\u6210\u4e0e\u5206\u6790\u6846\u67b6\u63d0\u5347\u4e86\u5408\u6210\u80fd\u6e90\u6d88\u8d39\u6570\u636e\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u533f\u540d\u5316\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u5982\u72b6\u6001\u4f30\u8ba1\u548c\u6d88\u8d39\u9884\u6d4b\u3002", "paper_title_zh": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7684\u80fd\u6e90\u6d88\u8d39\u8005\u65f6\u95f4\u5e8f\u5217\u66ff\u4ee3\u6570\u636e\u7528\u4e8e\u957f\u671f\u9884\u6d4b\u573a\u666f", "abstract_zh": "\u9884\u6d4b\u5728\u7535\u529b\u4ef7\u503c\u94fe\u4e2d\u5438\u5f15\u4e86\u5927\u91cf\u7814\u7a76\u5173\u6ce8\uff0c\u4f46\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u4e8e\u53d1\u7535\u6216\u6d88\u8d39\u7684\u77ed\u671f\u9884\u6d4b\uff0c\u4e14\u591a\u5173\u6ce8\u7cfb\u7edf\u800c\u975e\u4e2a\u4f53\u6d88\u8d39\u8005\u3002\u4e2a\u4f53\u80fd\u6e90\u6d88\u8d39\u7684\u957f\u671f\u9884\u6d4b\u7814\u7a76\u66f4\u4e3a\u7a00\u7f3a\u3002\u672c\u6587\u6df1\u5165\u6bd4\u8f83\u4e86\u591a\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u751f\u6210\u7528\u4e8e\u957f\u671f\u80fd\u6e90\u6d88\u8d39\u9884\u6d4b\u7684\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65b9\u9762\u7684\u6027\u80fd\u3002\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u5bf9\u80fd\u6e90\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u548c\u7535\u7f51\u89c4\u5212\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u524d\u6cbf\u4f46\u8f83\u5c11\u89c1\u7684\u6280\u672f\uff1a\u6df7\u5408Wasserstein\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08WGAN\uff09\u3001\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u3001\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u548c\u63a9\u7801\u81ea\u56de\u5f52\u4f2f\u6069\u65af\u5766\u591a\u9879\u5f0f\u5f52\u4e00\u5316\u6d41\uff08MABF\uff09\u3002\u6211\u4eec\u5206\u6790\u4e86\u6bcf\u79cd\u65b9\u6cd5\u5728\u590d\u5236\u4e2a\u4f53\u80fd\u6e90\u6d88\u8d39\u65f6\u95f4\u52a8\u6001\u3001\u957f\u7a0b\u4f9d\u8d56\u6027\u548c\u6982\u7387\u8f6c\u79fb\u65b9\u9762\u7684\u80fd\u529b\u3002\u6bd4\u8f83\u5206\u6790\u63ed\u793a\u4e86WGAN\u3001DDPM\u3001HMM\u548cMABF\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u5176\u4ed6\u80fd\u6e90\u76f8\u5173\u4efb\u52a1\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002\u6211\u4eec\u7684\u751f\u6210\u4e0e\u5206\u6790\u6846\u67b6\u65e8\u5728\u63d0\u5347\u5408\u6210\u80fd\u6e90\u6d88\u8d39\u6570\u636e\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u751f\u6210\u6ee1\u8db3\u533f\u540d\u5316\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u7684\u6570\u636e\uff0c\u907f\u514d\u5bf9\u5355\u4e2a\u7528\u6237\u7684\u7279\u5b9a\u5206\u6790\u98ce\u9669\u3002\u672c\u7814\u7a76\u4f7f\u7528\u4e86\u5fb7\u56fd\u5bb6\u5ead\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u65f6\u95f4\u5206\u8fa8\u7387\u4e3a15\u5206\u949f\u3002\u751f\u6210\u7684\u5408\u6210\u7535\u529b\u914d\u7f6e\u6587\u4ef6\u53ef\u76f4\u63a5\u7528\u4e8e\u72b6\u6001\u4f30\u8ba1\u6216\u6d88\u8d39\u9884\u6d4b\u7b49\u5e94\u7528\u3002"}}
{"id": "2506.20259", "pdf": "https://arxiv.org/pdf/2506.20259", "abs": "https://arxiv.org/abs/2506.20259", "authors": ["Andrej L\u00fa\u010dny", "Matilde Antonj", "Carlo Mazzola", "Hana Horn\u00e1\u010dkov\u00e1", "Igor Farka\u0161"], "title": "Generating and Customizing Robotic Arm Trajectories using Neural Networks", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 70E60", "I.2.9"], "comment": "The code is released at\n  https://github.com/andylucny/nico2/tree/main/generate", "summary": "We introduce a neural network approach for generating and customizing the\ntrajectory of a robotic arm, that guarantees precision and repeatability. To\nhighlight the potential of this novel method, we describe the design and\nimplementation of the technique and show its application in an experimental\nsetting of cognitive robotics. In this scenario, the NICO robot was\ncharacterized by the ability to point to specific points in space with precise\nlinear movements, increasing the predictability of the robotic action during\nits interaction with humans. To achieve this goal, the neural network computes\nthe forward kinematics of the robot arm. By integrating it with a generator of\njoint angles, another neural network was developed and trained on an artificial\ndataset created from suitable start and end poses of the robotic arm. Through\nthe computation of angular velocities, the robot was characterized by its\nability to perform the movement, and the quality of its action was evaluated in\nterms of shape and accuracy. Thanks to its broad applicability, our approach\nsuccessfully generates precise trajectories that could be customized in their\nshape and adapted to different settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u548c\u5b9a\u5236\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\uff0c\u786e\u4fdd\u7cbe\u5ea6\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u5728\u8ba4\u77e5\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u68b0\u81c2\u5728\u4e0e\u4eba\u4ea4\u4e92\u65f6\u7684\u52a8\u4f5c\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u5b9e\u73b0\u7cbe\u786e\u7684\u7ebf\u6027\u8fd0\u52a8\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u673a\u68b0\u81c2\u7684\u6b63\u5411\u8fd0\u52a8\u5b66\uff0c\u5e76\u7ed3\u5408\u5173\u8282\u89d2\u5ea6\u751f\u6210\u5668\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5de5\u6570\u636e\u96c6\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u901a\u8fc7\u8ba1\u7b97\u89d2\u901f\u5ea6\uff0c\u8bc4\u4f30\u4e86\u52a8\u4f5c\u7684\u5f62\u72b6\u548c\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7cbe\u786e\u4e14\u53ef\u5b9a\u5236\u7684\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\uff0c\u5e76\u5728NICO\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u6210\u529f\u751f\u6210\u7cbe\u786e\u4e14\u53ef\u5b9a\u5236\u7684\u673a\u68b0\u81c2\u8f68\u8ff9\uff0c\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u548c\u5b9a\u5236\u673a\u68b0\u81c2\u8f68\u8ff9", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u548c\u5b9a\u5236\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\uff0c\u786e\u4fdd\u7cbe\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002\u4e3a\u4e86\u5c55\u793a\u8fd9\u4e00\u65b0\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u8be5\u6280\u672f\u7684\u8bbe\u8ba1\u548c\u5b9e\u73b0\uff0c\u5e76\u5728\u8ba4\u77e5\u673a\u5668\u4eba\u7684\u5b9e\u9a8c\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002\u5728\u8fd9\u4e00\u573a\u666f\u4e2d\uff0cNICO\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u7cbe\u786e\u7684\u7ebf\u6027\u8fd0\u52a8\u6307\u5411\u7a7a\u95f4\u4e2d\u7684\u7279\u5b9a\u70b9\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u65f6\u7684\u52a8\u4f5c\u53ef\u9884\u6d4b\u6027\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u673a\u68b0\u81c2\u7684\u6b63\u5411\u8fd0\u52a8\u5b66\uff0c\u5e76\u7ed3\u5408\u5173\u8282\u89d2\u5ea6\u751f\u6210\u5668\uff0c\u5f00\u53d1\u5e76\u8bad\u7ec3\u4e86\u53e6\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u57fa\u4e8e\u673a\u68b0\u81c2\u7684\u8d77\u59cb\u548c\u7ed3\u675f\u59ff\u6001\u751f\u6210\u7684\u4eba\u5de5\u6570\u636e\u96c6\u3002\u901a\u8fc7\u8ba1\u7b97\u89d2\u901f\u5ea6\uff0c\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u52a8\u4f5c\uff0c\u5e76\u4ece\u5f62\u72b6\u548c\u7cbe\u5ea6\u65b9\u9762\u8bc4\u4f30\u5176\u52a8\u4f5c\u8d28\u91cf\u3002\u7531\u4e8e\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u7cbe\u786e\u4e14\u53ef\u5b9a\u5236\u7684\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002"}}
{"id": "2506.20260", "pdf": "https://arxiv.org/pdf/2506.20260", "abs": "https://arxiv.org/abs/2506.20260", "authors": ["Junqi Jiang", "Antonio Rago", "Francesco Leofante", "Francesca Toni"], "title": "Argumentative Ensembling for Robust Recourse under Model Multiplicity", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.15097", "summary": "In machine learning, it is common to obtain multiple equally performing\nmodels for the same prediction task, e.g., when training neural networks with\ndifferent random seeds. Model multiplicity (MM) is the situation which arises\nwhen these competing models differ in their predictions for the same input, for\nwhich ensembling is often employed to determine an aggregation of the outputs.\nProviding recourse recommendations via counterfactual explanations (CEs) under\nMM thus becomes complex, since the CE may not be valid across all models, i.e.,\nthe CEs are not robust under MM. In this work, we formalise the problem of\nproviding recourse under MM, which we name recourse-aware ensembling (RAE). We\npropose the idea that under MM, CEs for each individual model should be\nconsidered alongside their predictions so that the aggregated prediction and\nrecourse are decided in tandem. Centred around this intuition, we introduce six\ndesirable properties for solutions to this problem. For solving RAE, we propose\na novel argumentative ensembling method which guarantees the robustness of CEs\nunder MM. Specifically, our method leverages computational argumentation to\nexplicitly represent the conflicts between models and counterfactuals regarding\nprediction results and CE validity. It then uses argumentation semantics to\nresolve the conflicts and obtain the final solution, in a manner which is\nparametric to the chosen semantics. Our method also allows for the\nspecification of preferences over the models under MM, allowing further\ncustomisation of the ensemble. In a comprehensive theoretical analysis, we\ncharacterise the behaviour of argumentative ensembling with four different\nargumentation semantics. We then empirically demonstrate the effectiveness of\nour approach in satisfying desirable properties with eight instantiations of\nour method. (Abstract is shortened for arXiv.)", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bba\u8bc1\u96c6\u6210\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6a21\u578b\u591a\u91cd\u6027\uff08MM\uff09\u4e0b\u63d0\u4f9b\u9c81\u68d2\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CEs\uff09\uff0c\u786e\u4fdd\u53cd\u4e8b\u5b9e\u5efa\u8bae\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5747\u6709\u6548\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u6a21\u578b\u591a\u91cd\u6027\uff08MM\uff09\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u5bf9\u76f8\u540c\u8f93\u5165\u7684\u9884\u6d4b\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CEs\uff09\u5728\u6240\u6709\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u8bba\u8bc1\u96c6\u6210\u201d\u65b9\u6cd5\uff0c\u5229\u7528\u8ba1\u7b97\u8bba\u8bc1\u6280\u672f\u663e\u5f0f\u8868\u793a\u6a21\u578b\u4e0e\u53cd\u4e8b\u5b9e\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u901a\u8fc7\u8bba\u8bc1\u8bed\u4e49\u89e3\u51b3\u51b2\u7a81\uff0c\u786e\u4fdd\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u9c81\u68d2\u6027\u3002\u65b9\u6cd5\u8fd8\u652f\u6301\u5bf9\u6a21\u578b\u504f\u597d\u7684\u5b9a\u5236\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8bba\u8bc1\u96c6\u6210\u5728\u56db\u79cd\u4e0d\u540c\u8bba\u8bc1\u8bed\u4e49\u4e0b\u5747\u8868\u73b0\u826f\u597d\u3002\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u516d\u9879\u7406\u60f3\u6027\u8d28\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u8bc1\u96c6\u6210\u65b9\u6cd5\u4e3a\u6a21\u578b\u591a\u91cd\u6027\u4e0b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7528\u6237\u5bf9\u6a21\u578b\u504f\u597d\u7684\u5b9a\u5236\u3002", "paper_title_zh": "\u6a21\u578b\u591a\u91cd\u6027\u4e0b\u57fa\u4e8e\u8bba\u8bc1\u96c6\u6210\u7684\u9c81\u68d2\u53cd\u4e8b\u5b9e\u89e3\u91ca", "abstract_zh": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u9488\u5bf9\u540c\u4e00\u9884\u6d4b\u4efb\u52a1\u901a\u5e38\u4f1a\u5f97\u5230\u591a\u4e2a\u6027\u80fd\u76f8\u5f53\u7684\u6a21\u578b\uff0c\u4f8b\u5982\u901a\u8fc7\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002\u6a21\u578b\u591a\u91cd\u6027\uff08MM\uff09\u6307\u8fd9\u4e9b\u7ade\u4e89\u6a21\u578b\u5bf9\u76f8\u540c\u8f93\u5165\u7684\u9884\u6d4b\u7ed3\u679c\u4e0d\u540c\uff0c\u6b64\u65f6\u5e38\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u5bf9\u8f93\u51fa\u8fdb\u884c\u805a\u5408\u3002\u5728MM\u4e0b\u901a\u8fc7\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CEs\uff09\u63d0\u4f9b\u53cd\u4e8b\u5b9e\u5efa\u8bae\u53d8\u5f97\u590d\u6742\uff0c\u56e0\u4e3aCE\u53ef\u80fd\u5e76\u975e\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5747\u6709\u6548\uff0c\u5373CE\u5728MM\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u672c\u6587\u6b63\u5f0f\u5b9a\u4e49\u4e86MM\u4e0b\u7684\u53cd\u4e8b\u5b9e\u95ee\u9898\uff0c\u79f0\u4e3a\u201c\u53cd\u4e8b\u5b9e\u611f\u77e5\u96c6\u6210\u201d\uff08RAE\uff09\u3002\u6211\u4eec\u63d0\u51fa\uff0c\u5728MM\u4e0b\uff0c\u5e94\u540c\u65f6\u8003\u8651\u6bcf\u4e2a\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u53ca\u5176\u9884\u6d4b\u7ed3\u679c\uff0c\u4ee5\u4fbf\u540c\u6b65\u51b3\u5b9a\u805a\u5408\u9884\u6d4b\u548c\u53cd\u4e8b\u5b9e\u3002\u56f4\u7ed5\u8fd9\u4e00\u601d\u8def\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u516d\u9879\u7406\u60f3\u6027\u8d28\u3002\u9488\u5bf9RAE\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bba\u8bc1\u96c6\u6210\u65b9\u6cd5\uff0c\u786e\u4fddCE\u5728MM\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8ba1\u7b97\u8bba\u8bc1\u6280\u672f\u663e\u5f0f\u8868\u793a\u6a21\u578b\u4e0e\u53cd\u4e8b\u5b9e\u5728\u9884\u6d4b\u7ed3\u679c\u548cCE\u6709\u6548\u6027\u4e0a\u7684\u51b2\u7a81\uff0c\u5e76\u901a\u8fc7\u8bba\u8bc1\u8bed\u4e49\u89e3\u51b3\u51b2\u7a81\uff0c\u6700\u7ec8\u5f97\u5230\u53c2\u6570\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u8fd8\u652f\u6301\u5bf9MM\u4e0b\u6a21\u578b\u7684\u504f\u597d\u5b9a\u5236\u3002\u901a\u8fc7\u5168\u9762\u7684\u7406\u8bba\u5206\u6790\uff0c\u6211\u4eec\u523b\u753b\u4e86\u56db\u79cd\u8bba\u8bc1\u8bed\u4e49\u4e0b\u8bba\u8bc1\u96c6\u6210\u7684\u884c\u4e3a\u3002\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u7406\u60f3\u6027\u8d28\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.20307", "pdf": "https://arxiv.org/pdf/2506.20307", "abs": "https://arxiv.org/abs/2506.20307", "authors": ["Heyang Zhao", "Xingrui Yu", "David M. Bossens", "Ivor W. Tsang", "Quanquan Gu"], "title": "Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Imitation learning is a central problem in reinforcement learning where the\ngoal is to learn a policy that mimics the expert's behavior. In practice, it is\noften challenging to learn the expert policy from a limited number of\ndemonstrations accurately due to the complexity of the state space. Moreover,\nit is essential to explore the environment and collect data to achieve\nbeyond-expert performance. To overcome these challenges, we propose a novel\nimitation learning algorithm called Imitation Learning with Double Exploration\n(ILDE), which implements exploration in two aspects: (1) optimistic policy\noptimization via an exploration bonus that rewards state-action pairs with high\nuncertainty to potentially improve the convergence to the expert policy, and\n(2) curiosity-driven exploration of the states that deviate from the\ndemonstration trajectories to potentially yield beyond-expert performance.\nEmpirically, we demonstrate that ILDE outperforms the state-of-the-art\nimitation learning algorithms in terms of sample efficiency and achieves\nbeyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations\nthan in previous work. We also provide a theoretical justification of ILDE as\nan uncertainty-regularized policy optimization method with optimistic\nexploration, leading to a regret growing sublinearly in the number of episodes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aILDE\u7684\u65b0\u578b\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u63a2\u7d22\uff08\u4e50\u89c2\u7b56\u7565\u4f18\u5316\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\uff09\u5728\u6709\u9650\u6f14\u793a\u4e0b\u5b9e\u73b0\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\uff0c\u5e76\u5728Atari\u548cMuJoCo\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7684\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u4e2a\u6a21\u4eff\u4e13\u5bb6\u884c\u4e3a\u7684\u7b56\u7565\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\uff0c\u7531\u4e8e\u72b6\u6001\u7a7a\u95f4\u7684\u590d\u6742\u6027\uff0c\u4ece\u6709\u9650\u7684\u6f14\u793a\u4e2d\u51c6\u786e\u5b66\u4e60\u4e13\u5bb6\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5b9e\u73b0\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\uff0c\u63a2\u7d22\u73af\u5883\u548c\u6536\u96c6\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002", "method": "ILDE\u7b97\u6cd5\u901a\u8fc7\u53cc\u91cd\u63a2\u7d22\u5b9e\u73b0\uff1a1\uff09\u4e50\u89c2\u7b56\u7565\u4f18\u5316\uff0c\u901a\u8fc7\u5956\u52b1\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u4ee5\u52a0\u901f\u6536\u655b\u5230\u4e13\u5bb6\u7b56\u7565\uff1b2\uff09\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\uff0c\u63a2\u7d22\u504f\u79bb\u6f14\u793a\u8f68\u8ff9\u7684\u72b6\u6001\u4ee5\u5b9e\u73b0\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cILDE\u5728\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5728Atari\u548cMuJoCo\u4efb\u52a1\u4e2d\u4ee5\u66f4\u5c11\u7684\u6f14\u793a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cILDE\u662f\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u6b63\u5219\u5316\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u4e50\u89c2\u63a2\u7d22\u7279\u6027\u3002", "conclusion": "ILDE\u901a\u8fc7\u53cc\u91cd\u63a2\u7d22\u673a\u5236\u5728\u6709\u9650\u6f14\u793a\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u4eff\u5b66\u4e60\u548c\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\uff0c\u4e3a\u6a21\u4eff\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u6709\u9650\u6f14\u793a\u4e0b\u7684\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\uff1a\u53cc\u91cd\u63a2\u7d22\u7684\u9ad8\u6548\u6a21\u4eff\u5b66\u4e60", "abstract_zh": "\u6a21\u4eff\u5b66\u4e60\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\uff0c\u5176\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u4e2a\u6a21\u4eff\u4e13\u5bb6\u884c\u4e3a\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u7531\u4e8e\u72b6\u6001\u7a7a\u95f4\u7684\u590d\u6742\u6027\uff0c\u4ece\u6709\u9650\u7684\u6f14\u793a\u4e2d\u51c6\u786e\u5b66\u4e60\u4e13\u5bb6\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5b9e\u73b0\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\uff0c\u63a2\u7d22\u73af\u5883\u548c\u6536\u96c6\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u91cd\u63a2\u7d22\u6a21\u4eff\u5b66\u4e60\uff08ILDE\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u4e24\u65b9\u9762\u5b9e\u73b0\u63a2\u7d22\uff1a1\uff09\u901a\u8fc7\u4e50\u89c2\u7b56\u7565\u4f18\u5316\uff0c\u5956\u52b1\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u4ee5\u52a0\u901f\u6536\u655b\u5230\u4e13\u5bb6\u7b56\u7565\uff1b2\uff09\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\uff0c\u63a2\u7d22\u504f\u79bb\u6f14\u793a\u8f68\u8ff9\u7684\u72b6\u6001\u4ee5\u5b9e\u73b0\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cILDE\u5728\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5728Atari\u548cMuJoCo\u4efb\u52a1\u4e2d\u4ee5\u66f4\u5c11\u7684\u6f14\u793a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86ILDE\u4f5c\u4e3a\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u6b63\u5219\u5316\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u4e50\u89c2\u63a2\u7d22\u7279\u6027\uff0c\u5176\u9057\u61be\u968f\u60c5\u8282\u6570\u5448\u6b21\u7ebf\u6027\u589e\u957f\u3002"}}
{"id": "2506.20323", "pdf": "https://arxiv.org/pdf/2506.20323", "abs": "https://arxiv.org/abs/2506.20323", "authors": ["Saundarya Subramaniam", "Shalini Majumdar", "Shantanu Nadar", "Kaustubh Kulkarni"], "title": "Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This research presents the development of an Artificial Intelligence (AI) -\ndriven crop disease detection system designed to assist farmers in rural areas\nwith limited resources. We aim to compare different deep learning models for a\ncomparative analysis, focusing on their efficacy in transfer learning. By\nleveraging deep learning models, including EfficientNet, ResNet101,\nMobileNetV2, and our custom CNN, which achieved a validation accuracy of\n95.76%, the system effectively classifies plant diseases. This research\ndemonstrates the potential of transfer learning in reshaping agricultural\npractices, improving crop health management, and supporting sustainable farming\nin rural environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u6bd4\u8f83\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u519c\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u5176\u4e2d\u81ea\u5b9a\u4e49CNN\u6a21\u578b\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe95.76%\uff0c\u5c55\u793a\u4e86AI\u5728\u519c\u4e1a\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u519c\u6751\u5730\u533a\u519c\u6c11\u5f00\u53d1AI\u9a71\u52a8\u7684\u519c\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\uff0c\u63d0\u5347\u519c\u4e1a\u75c5\u5bb3\u7ba1\u7406\u7684\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86EfficientNet\u3001ResNet101\u3001MobileNetV2\u548c\u81ea\u5b9a\u4e49CNN\u6a21\u578b\u5728\u519c\u4f5c\u7269\u75c5\u5bb3\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u81ea\u5b9a\u4e49CNN\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe95.76%\uff0c\u8868\u660e\u8fc1\u79fb\u5b66\u4e60\u5728\u519c\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u519c\u4e1a\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u4e3a\u519c\u6751\u5730\u533a\u63d0\u4f9b\u9ad8\u6548\u7684\u4f5c\u7269\u5065\u5eb7\u7ba1\u7406\u65b9\u6848\uff0c\u63a8\u52a8\u53ef\u6301\u7eed\u519c\u4e1a\u53d1\u5c55\u3002", "paper_title_zh": "\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u519c\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6bd4\u8f83\u5206\u6790", "abstract_zh": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u519c\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u8d44\u6e90\u6709\u9650\u7684\u519c\u6751\u5730\u533a\u519c\u6c11\u3002\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6548\u679c\uff0c\u5305\u62ecEfficientNet\u3001ResNet101\u3001MobileNetV2\u548c\u81ea\u5b9a\u4e49CNN\uff08\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe95.76%\uff09\uff0c\u7cfb\u7edf\u80fd\u6709\u6548\u5206\u7c7b\u690d\u7269\u75c5\u5bb3\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fc1\u79fb\u5b66\u4e60\u6709\u671b\u91cd\u5851\u519c\u4e1a\u5b9e\u8df5\uff0c\u6539\u5584\u4f5c\u7269\u5065\u5eb7\u7ba1\u7406\uff0c\u5e76\u652f\u6301\u519c\u6751\u5730\u533a\u7684\u53ef\u6301\u7eed\u519c\u4e1a\u3002"}}
{"id": "2506.20353", "pdf": "https://arxiv.org/pdf/2506.20353", "abs": "https://arxiv.org/abs/2506.20353", "authors": ["Xuan Ding", "Rui Sun", "Yunjian Zhang", "Xiu Yan", "Yueqi Zhou", "Kaihao Huang", "Suzhong Fu", "Chuanlong Xie", "Yao Zhu"], "title": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDipSVD\u7684\u53cc\u91cd\u91cd\u8981\u6027\u4fdd\u62a4SVD\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u91cd\u8981\u6027\u4fdd\u62a4\u673a\u5236\uff0cDipSVD\u5728\u538b\u7f29\u6a21\u578b\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u90e8\u7f72\u6210\u672c\u4e0d\u65ad\u589e\u52a0\uff0c\u50ac\u751f\u4e86\u591a\u79cd\u538b\u7f29\u65b9\u6cd5\u3002\u5c3d\u7ba1SVD\u538b\u7f29\u5728\u786c\u4ef6\u517c\u5bb9\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u4e0a\u4f18\u4e8e\u91cf\u5316\u548c\u975e\u7ed3\u6784\u5316\u526a\u679d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u77e9\u9635\u4e2d\u5173\u952e\u7ec4\u4ef6\u7684\u4fdd\u62a4\uff0c\u5bfc\u81f4\u538b\u7f29\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u91cd\u8981\u6027\u4fdd\u62a4\u673a\u5236\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DipSVD\u91c7\u7528\u53cc\u91cd\u91cd\u8981\u6027\u4fdd\u62a4\u673a\u5236\uff1a(1) \u5c40\u90e8\u91cd\u8981\u6027\u4fdd\u62a4\uff1a\u901a\u8fc7\u901a\u9053\u52a0\u6743\u6570\u636e\u767d\u5316\u4fdd\u7559\u6bcf\u4e2a\u6743\u91cd\u77e9\u9635\u4e2d\u6700\u5173\u952e\u7684\u5947\u5f02\u5411\u91cf\uff1b(2) \u5168\u5c40\u91cd\u8981\u6027\u4fdd\u62a4\uff1a\u901a\u8fc7\u542f\u53d1\u5f0f\u6216\u4f18\u5316\u65b9\u6cd5\uff0c\u8ba9\u4e0d\u91cd\u8981\u7684\u5c42\u627f\u62c5\u66f4\u591a\u538b\u7f29\u8d1f\u62c5\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u5173\u952e\u5c42\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDipSVD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eSVD\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DipSVD\u901a\u8fc7\u53cc\u91cd\u91cd\u8981\u6027\u4fdd\u62a4\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SVD\u538b\u7f29\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "DipSVD\uff1a\u53cc\u91cd\u91cd\u8981\u6027\u4fdd\u62a4\u7684SVD\u7528\u4e8e\u9ad8\u6548LLM\u538b\u7f29", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65e5\u76ca\u589e\u957f\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u90e8\u7f72\u6210\u672c\u50ac\u751f\u4e86\u591a\u79cd\u538b\u7f29\u65b9\u6cd5\u3002\u4e0e\u91cf\u5316\u548c\u975e\u7ed3\u6784\u5316\u526a\u679d\u76f8\u6bd4\uff0cSVD\u538b\u7f29\u5177\u6709\u66f4\u597d\u7684\u786c\u4ef6\u517c\u5bb9\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8eSVD\u7684\u65b9\u6cd5\u5173\u6ce8\u539f\u59cb\u77e9\u9635\u4e0e\u538b\u7f29\u77e9\u9635\u4e4b\u95f4\u7684\u6574\u4f53\u5dee\u5f02\uff0c\u800c\u5ffd\u7565\u4e86\u77e9\u9635\u4e2d\u5173\u952e\u7ec4\u4ef6\u7684\u4fdd\u62a4\uff0c\u5bfc\u81f4\u538b\u7f29\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u91cd\u8981\u6027\u4fdd\u62a4\u673a\u5236\u6765\u589e\u5f3a\u57fa\u4e8eSVD\u7684\u538b\u7f29\u65b9\u6cd5\uff1a(1) \u5c40\u90e8\u91cd\u8981\u6027\u4fdd\u62a4\uff1a\u901a\u8fc7\u901a\u9053\u52a0\u6743\u6570\u636e\u767d\u5316\u4fdd\u7559\u6bcf\u4e2a\u6743\u91cd\u77e9\u9635\u4e2d\u6700\u5173\u952e\u7684\u5947\u5f02\u5411\u91cf\uff1b(2) \u5168\u5c40\u91cd\u8981\u6027\u4fdd\u62a4\uff1a\u901a\u8fc7\u542f\u53d1\u5f0f\u6216\u4f18\u5316\u65b9\u6cd5\uff0c\u8ba9\u4e0d\u91cd\u8981\u7684\u5c42\u627f\u62c5\u66f4\u591a\u538b\u7f29\u8d1f\u62c5\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u5173\u952e\u5c42\u7684\u5f71\u54cd\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDipSVD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eSVD\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.20354", "pdf": "https://arxiv.org/pdf/2506.20354", "abs": "https://arxiv.org/abs/2506.20354", "authors": ["Francesco Carzaniga", "Michael Hersche", "Abu Sebastian", "Kaspar Schindler", "Abbas Rahimi"], "title": "A foundation model with multi-variate parallel attention to generate neuronal activity", "categories": ["cs.LG", "cs.AI"], "comment": "The code is available at\n  https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG\n  dataset is available at\n  https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg", "summary": "Learning from multi-variate time-series with heterogeneous channel\nconfigurations remains a fundamental challenge for deep neural networks (DNNs),\nparticularly in clinical domains such as intracranial electroencephalography\n(iEEG), where channel setups vary widely across subjects. In this work, we\nintroduce multi-variate parallel attention (MVPA), a novel self-attention\nmechanism that disentangles content, temporal, and spatial attention, enabling\nflexible, generalizable, and efficient modeling of time-series data with\nvarying channel counts and configurations. We use MVPA to build MVPFormer, a\ngenerative foundation model for human electrophysiology, trained to predict the\nevolution of iEEG signals across diverse subjects. To support this and future\neffort by the community, we release the SWEC iEEG dataset, the largest publicly\navailable iEEG dataset to date, comprising nearly 10,000 hours of recordings\nfrom heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong\ngeneralization across subjects, demonstrating expert-level performance in\nseizure detection and outperforming state-of-the-art Transformer baselines on\nour SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard\ntime-series forecasting and classification tasks, where it matches or exceeds\nexisting attention-based models. Together, our contributions establish MVPA as\na general-purpose attention mechanism for heterogeneous time-series and\nMVPFormer as the first open-source, open-weights, and open-data iEEG foundation\nmodel with state-of-the-art clinical performance. The code is available at\nhttps://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG\ndataset is available at\nhttps://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u53d8\u91cf\u5e76\u884c\u6ce8\u610f\u529b\u673a\u5236\uff08MVPA\uff09\uff0c\u7528\u4e8e\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u9885\u5185\u8111\u7535\u56fe\uff08iEEG\uff09\u9886\u57df\u3002MVPA\u901a\u8fc7\u89e3\u8026\u5185\u5bb9\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u901a\u9053\u6570\u91cf\u548c\u914d\u7f6e\u53d8\u5316\u7684\u7075\u6d3b\u5efa\u6a21\u3002\u57fa\u4e8eMVPA\uff0c\u4f5c\u8005\u6784\u5efa\u4e86MVPFormer\uff0c\u4e00\u4e2a\u751f\u6210\u6027\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4biEEG\u4fe1\u53f7\u7684\u6f14\u53d8\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u53d1\u5e03\u4e86\u76ee\u524d\u6700\u5927\u7684\u516c\u5f00iEEG\u6570\u636e\u96c6SWEC\uff0c\u5e76\u9a8c\u8bc1\u4e86MVPFormer\u5728\u766b\u75eb\u68c0\u6d4b\u548c\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff08\u5982iEEG\uff09\u7684\u901a\u9053\u914d\u7f6e\u5728\u4e0d\u540c\u53d7\u8bd5\u8005\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u8fd9\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u63d0\u51fa\u4e86\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7075\u6d3b\u5904\u7406\u8fd9\u79cd\u5f02\u6784\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u901a\u9053\u914d\u7f6e\u7684\u901a\u7528\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u591a\u53d8\u91cf\u5e76\u884c\u6ce8\u610f\u529b\uff08MVPA\uff09\uff0c\u901a\u8fc7\u89e3\u8026\u5185\u5bb9\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u5bf9\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7684\u9ad8\u6548\u5efa\u6a21\u3002\u57fa\u4e8eMVPA\uff0c\u6784\u5efa\u4e86MVPFormer\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210iEEG\u4fe1\u53f7\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86SWEC\u6570\u636e\u96c6\uff0c\u5305\u542b\u8fd110,000\u5c0f\u65f6\u7684iEEG\u8bb0\u5f55\u3002", "result": "MVPFormer\u5728SWEC\u3001MAYO\u548cFNUSA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u8de8\u53d7\u8bd5\u8005\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u766b\u75eb\u68c0\u6d4b\u4e2d\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\u3002MVPA\u5728\u6807\u51c6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e5f\u4f18\u4e8e\u73b0\u6709\u6ce8\u610f\u529b\u6a21\u578b\u3002", "conclusion": "MVPA\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7684\u901a\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0cMVPFormer\u662f\u9996\u4e2a\u5f00\u6e90\u3001\u5f00\u653e\u6743\u91cd\u548c\u5f00\u653e\u6570\u636e\u7684iEEG\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u9886\u5148\u7684\u4e34\u5e8a\u6027\u80fd\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u53d8\u91cf\u5e76\u884c\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u5143\u6d3b\u52a8\u751f\u6210\u57fa\u7840\u6a21\u578b", "abstract_zh": "\u4ece\u5177\u6709\u5f02\u6784\u901a\u9053\u914d\u7f6e\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u5b66\u4e60\u4ecd\u7136\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u7684\u4e00\u9879\u57fa\u672c\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e34\u5e8a\u9886\u57df\u5982\u9885\u5185\u8111\u7535\u56fe\uff08iEEG\uff09\u4e2d\uff0c\u4e0d\u540c\u53d7\u8bd5\u8005\u7684\u901a\u9053\u8bbe\u7f6e\u5dee\u5f02\u663e\u8457\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u2014\u2014\u591a\u53d8\u91cf\u5e76\u884c\u6ce8\u610f\u529b\uff08MVPA\uff09\uff0c\u5b83\u901a\u8fc7\u89e3\u8026\u5185\u5bb9\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u901a\u9053\u6570\u91cf\u548c\u914d\u7f6e\u53d8\u5316\u7684\u7075\u6d3b\u3001\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5efa\u6a21\u3002\u6211\u4eec\u5229\u7528MVPA\u6784\u5efa\u4e86MVPFormer\uff0c\u4e00\u4e2a\u7528\u4e8e\u4eba\u7c7b\u7535\u751f\u7406\u5b66\u7684\u751f\u6210\u6027\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u9884\u6d4b\u4e0d\u540c\u53d7\u8bd5\u8005\u7684iEEG\u4fe1\u53f7\u6f14\u53d8\u3002\u4e3a\u652f\u6301\u793e\u533a\u7684\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u5e03\u4e86SWEC iEEG\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u7684\u516c\u5f00iEEG\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u5f02\u6784\u4e34\u5e8a\u6e90\u7684\u8fd110,000\u5c0f\u65f6\u8bb0\u5f55\u3002MVPFormer\u901a\u8fc7MVPA\u5b9e\u73b0\u4e86\u8de8\u53d7\u8bd5\u8005\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u766b\u75eb\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u5e76\u5728SWEC\u3001MAYO\u548cFNUSA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709Transformer\u57fa\u7ebf\u6a21\u578b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5728\u6807\u51c6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86MVPA\uff0c\u5176\u8868\u73b0\u4e0e\u73b0\u6709\u6ce8\u610f\u529b\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u7efc\u4e0a\uff0c\u6211\u4eec\u7684\u8d21\u732e\u786e\u7acb\u4e86MVPA\u4f5c\u4e3a\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7684\u901a\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53caMVPFormer\u4f5c\u4e3a\u9996\u4e2a\u5f00\u6e90\u3001\u5f00\u653e\u6743\u91cd\u548c\u5f00\u653e\u6570\u636e\u7684iEEG\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u9886\u5148\u7684\u4e34\u5e8a\u6027\u80fd\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/IBM/multi-variate-parallel-transformer\u3002SWEC iEEG\u6570\u636e\u96c6\u53d1\u5e03\u4e8ehttps://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg\u3002"}}
{"id": "2506.20362", "pdf": "https://arxiv.org/pdf/2506.20362", "abs": "https://arxiv.org/abs/2506.20362", "authors": ["Lorenzo Bini", "Stephane Marchand-Maillet"], "title": "Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations", "categories": ["cs.LG", "cs.AI", "cs.DS"], "comment": "LaplaceGNN is a novel graph learning framework that employs a\n  bootstrapped teacher-student architecture. Its precomputed spectral\n  augmentations and adversarial training enable robust performance,\n  outperforming SOTA methods while scaling linearly", "summary": "We present LaplaceGNN, a novel self-supervised graph learning framework that\nbypasses the need for negative sampling by leveraging spectral bootstrapping\ntechniques. Our method integrates Laplacian-based signals into the learning\nprocess, allowing the model to effectively capture rich structural\nrepresentations without relying on contrastive objectives or handcrafted\naugmentations. By focusing on positive alignment, LaplaceGNN achieves linear\nscaling while offering a simpler, more efficient, self-supervised alternative\nfor graph neural networks, applicable across diverse domains. Our contributions\nare twofold: we precompute spectral augmentations through max-min\ncentrality-guided optimization, enabling rich structural supervision without\nrelying on handcrafted augmentations, then we integrate an adversarial\nbootstrapped training scheme that further strengthens feature learning and\nrobustness. Our extensive experiments on different benchmark datasets show that\nLaplaceGNN achieves superior performance compared to state-of-the-art\nself-supervised graph methods, offering a promising direction for efficiently\nlearning expressive graph representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLaplaceGNN\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u81ea\u4e3e\u6280\u672f\u548c\u62c9\u666e\u62c9\u65af\u589e\u5f3a\uff0c\u65e0\u9700\u8d1f\u91c7\u6837\u5373\u53ef\u6709\u6548\u6355\u83b7\u56fe\u7ed3\u6784\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u8d1f\u91c7\u6837\u6216\u624b\u5de5\u589e\u5f3a\uff0c\u6548\u7387\u4f4e\u4e14\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u4fe1\u53f7\u548c\u8c31\u81ea\u4e3e\u6280\u672f\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\u3002", "method": "LaplaceGNN\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u4fe1\u53f7\u548c\u8c31\u81ea\u4e3e\u6280\u672f\uff0c\u901a\u8fc7\u6700\u5927-\u6700\u5c0f\u4e2d\u5fc3\u6027\u4f18\u5316\u9884\u8ba1\u7b97\u8c31\u589e\u5f3a\uff0c\u5e76\u91c7\u7528\u5bf9\u6297\u6027\u81ea\u4e3e\u8bad\u7ec3\u65b9\u6848\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLaplaceGNN\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u5b66\u4e60\u56fe\u8868\u793a\u7684\u6f5c\u529b\u3002", "conclusion": "LaplaceGNN\u4e3a\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u8d1f\u91c7\u6837\u6216\u624b\u5de5\u589e\u5f3a\u3002", "paper_title_zh": "\u57fa\u4e8e\u8c31\u81ea\u4e3e\u548c\u62c9\u666e\u62c9\u65af\u589e\u5f3a\u7684\u81ea\u76d1\u7763\u56fe\u5b66\u4e60", "abstract_zh": "\u6211\u4eec\u63d0\u51faLaplaceGNN\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u81ea\u4e3e\u6280\u672f\u907f\u514d\u4e86\u8d1f\u91c7\u6837\u7684\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u5c06\u62c9\u666e\u62c9\u65af\u4fe1\u53f7\u6574\u5408\u5230\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6709\u6548\u6355\u83b7\u4e30\u5bcc\u7684\u7ed3\u6784\u8868\u793a\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5bf9\u6bd4\u76ee\u6807\u6216\u624b\u5de5\u589e\u5f3a\u3002\u901a\u8fc7\u4e13\u6ce8\u4e8e\u6b63\u5bf9\u9f50\uff0cLaplaceGNN\u5b9e\u73b0\u4e86\u7ebf\u6027\u6269\u5c55\uff0c\u540c\u65f6\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\uff1a\u901a\u8fc7\u6700\u5927-\u6700\u5c0f\u4e2d\u5fc3\u6027\u5f15\u5bfc\u7684\u4f18\u5316\u9884\u8ba1\u7b97\u8c31\u589e\u5f3a\uff0c\u5b9e\u73b0\u65e0\u9700\u624b\u5de5\u589e\u5f3a\u7684\u4e30\u5bcc\u7ed3\u6784\u76d1\u7763\uff1b\u968f\u540e\u6574\u5408\u5bf9\u6297\u6027\u81ea\u4e3e\u8bad\u7ec3\u65b9\u6848\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u548c\u9c81\u68d2\u6027\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLaplaceGNN\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u6548\u5b66\u4e60\u8868\u8fbe\u6027\u56fe\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2506.20373", "pdf": "https://arxiv.org/pdf/2506.20373", "abs": "https://arxiv.org/abs/2506.20373", "authors": ["Joerg Deigmoeller", "Stephan Hasler", "Nakul Agarwal", "Daniel Tanneberg", "Anna Belardinelli", "Reza Ghoddoosian", "Chao Wang", "Felix Ocker", "Fan Zhang", "Behzad Dariush", "Michael Gienger"], "title": "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "We introduce CARMA, a system for situational grounding in human-robot group\ninteractions. Effective collaboration in such group settings requires\nsituational awareness based on a consistent representation of present persons\nand objects coupled with an episodic abstraction of events regarding actors and\nmanipulated objects. This calls for a clear and consistent assignment of\ninstances, ensuring that robots correctly recognize and track actors, objects,\nand their interactions over time. To achieve this, CARMA uniquely identifies\nphysical instances of such entities in the real world and organizes them into\ngrounded triplets of actors, objects, and actions.\n  To validate our approach, we conducted three experiments, where multiple\nhumans and a robot interact: collaborative pouring, handovers, and sorting.\nThese scenarios allow the assessment of the system's capabilities as to role\ndistinction, multi-actor awareness, and consistent instance identification. Our\nexperiments demonstrate that the system can reliably generate accurate\nactor-action-object triplets, providing a structured and robust foundation for\napplications requiring spatiotemporal reasoning and situated decision-making in\ncollaborative settings.", "AI": {"tldr": "CARMA\u662f\u4e00\u79cd\u7528\u4e8e\u4eba\u673a\u7fa4\u4f53\u4ea4\u4e92\u60c5\u5883\u611f\u77e5\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5bf9\u8c61\u548c\u52a8\u4f5c\u8bc6\u522b\uff0c\u5b9e\u73b0\u5bf9\u53c2\u4e0e\u8005\u3001\u5bf9\u8c61\u53ca\u5176\u4ea4\u4e92\u7684\u51c6\u786e\u8bc6\u522b\u548c\u8ddf\u8e2a\u3002", "motivation": "\u5728\u7fa4\u4f53\u534f\u4f5c\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5177\u5907\u60c5\u5883\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u6b63\u786e\u8bc6\u522b\u548c\u8ddf\u8e2a\u53c2\u4e0e\u8005\u3001\u5bf9\u8c61\u53ca\u5176\u4ea4\u4e92\u884c\u4e3a\uff0c\u4ece\u800c\u652f\u6301\u6709\u6548\u7684\u534f\u4f5c\u51b3\u7b56\u3002CARMA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CARMA\u901a\u8fc7\u552f\u4e00\u6807\u8bc6\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b9e\u4f53\uff08\u5982\u53c2\u4e0e\u8005\u548c\u5bf9\u8c61\uff09\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u4e3a\u53c2\u4e0e\u8005-\u5bf9\u8c61-\u52a8\u4f5c\u7684\u4e09\u5143\u7ec4\uff0c\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u3002\u7cfb\u7edf\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5bf9\u8c61\u548c\u52a8\u4f5c\u8bc6\u522b\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCARMA\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u51c6\u786e\u7684\u53c2\u4e0e\u8005-\u52a8\u4f5c-\u5bf9\u8c61\u4e09\u5143\u7ec4\uff0c\u652f\u6301\u89d2\u8272\u533a\u5206\u3001\u591a\u53c2\u4e0e\u8005\u611f\u77e5\u548c\u5b9e\u4f8b\u4e00\u81f4\u6027\u8bc6\u522b\u3002", "conclusion": "CARMA\u4e3a\u9700\u8981\u65f6\u7a7a\u63a8\u7406\u548c\u60c5\u5883\u51b3\u7b56\u7684\u534f\u4f5c\u5e94\u7528\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u548c\u9c81\u68d2\u7684\u57fa\u7840\u3002", "paper_title_zh": "CARMA\uff1a\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5bf9\u8c61\u548c\u52a8\u4f5c\u8bc6\u522b\u5b9e\u73b0\u4eba\u673a\u7fa4\u4f53\u4ea4\u4e92\u7684\u60c5\u5883\u611f\u77e5", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86CARMA\uff0c\u4e00\u79cd\u7528\u4e8e\u4eba\u673a\u7fa4\u4f53\u4ea4\u4e92\u60c5\u5883\u611f\u77e5\u7684\u7cfb\u7edf\u3002\u5728\u8fd9\u79cd\u7fa4\u4f53\u534f\u4f5c\u4e2d\uff0c\u6709\u6548\u5408\u4f5c\u9700\u8981\u57fa\u4e8e\u5bf9\u5f53\u524d\u4eba\u5458\u548c\u5bf9\u8c61\u7684\u4e00\u81f4\u8868\u793a\u4ee5\u53ca\u5bf9\u53c2\u4e0e\u8005\u548c\u64cd\u4f5c\u5bf9\u8c61\u4e8b\u4ef6\u7684\u62bd\u8c61\u3002\u8fd9\u8981\u6c42\u5bf9\u5b9e\u4f8b\u8fdb\u884c\u6e05\u6670\u4e00\u81f4\u7684\u5206\u914d\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u548c\u8ddf\u8e2a\u53c2\u4e0e\u8005\u3001\u5bf9\u8c61\u53ca\u5176\u4ea4\u4e92\u884c\u4e3a\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0cCARMA\u552f\u4e00\u6807\u8bc6\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fd9\u4e9b\u5b9e\u4f53\u7684\u7269\u7406\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u4e3a\u53c2\u4e0e\u8005-\u5bf9\u8c61-\u52a8\u4f5c\u7684\u4e09\u5143\u7ec4\u3002\n\n\u4e3a\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e09\u9879\u5b9e\u9a8c\uff0c\u6d89\u53ca\u591a\u4eba\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\uff1a\u534f\u4f5c\u503e\u5012\u3001\u7269\u54c1\u4f20\u9012\u548c\u5206\u7c7b\u3002\u8fd9\u4e9b\u573a\u666f\u7528\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u5728\u89d2\u8272\u533a\u5206\u3001\u591a\u53c2\u4e0e\u8005\u611f\u77e5\u548c\u5b9e\u4f8b\u4e00\u81f4\u6027\u8bc6\u522b\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u51c6\u786e\u7684\u53c2\u4e0e\u8005-\u52a8\u4f5c-\u5bf9\u8c61\u4e09\u5143\u7ec4\uff0c\u4e3a\u9700\u8981\u65f6\u7a7a\u63a8\u7406\u548c\u60c5\u5883\u51b3\u7b56\u7684\u534f\u4f5c\u5e94\u7528\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u548c\u9c81\u68d2\u7684\u57fa\u7840\u3002"}}
{"id": "2506.20413", "pdf": "https://arxiv.org/pdf/2506.20413", "abs": "https://arxiv.org/abs/2506.20413", "authors": ["Mohammad Mahdi Maheri", "Denys Herasymuk", "Hamed Haddadi"], "title": "Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The growing adoption of Artificial Intelligence (AI) in Internet of Things\n(IoT) ecosystems has intensified the need for personalized learning methods\nthat can operate efficiently and privately across heterogeneous,\nresource-constrained devices. However, enabling effective personalized learning\nin decentralized settings introduces several challenges, including efficient\nknowledge transfer between clients, protection of data privacy, and resilience\nagainst poisoning attacks. In this paper, we address these challenges by\ndeveloping P4 (Personalized, Private, Peer-to-Peer) -- a method designed to\ndeliver personalized models for resource-constrained IoT devices while ensuring\ndifferential privacy and robustness against poisoning attacks. Our solution\nemploys a lightweight, fully decentralized algorithm to privately detect client\nsimilarity and form collaborative groups. Within each group, clients leverage\ndifferentially private knowledge distillation to co-train their models,\nmaintaining high accuracy while ensuring robustness to the presence of\nmalicious clients. We evaluate P4 on popular benchmark datasets using both\nlinear and CNN-based architectures across various heterogeneity settings and\nattack scenarios. Experimental results show that P4 achieves 5% to 30% higher\naccuracy than leading differentially private peer-to-peer approaches and\nmaintains robustness with up to 30% malicious clients. Additionally, we\ndemonstrate its practicality by deploying it on resource-constrained devices,\nwhere collaborative training between two clients adds only ~7 seconds of\noverhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aP4\u7684\u4e2a\u6027\u5316\u3001\u9690\u79c1\u4fdd\u62a4\u7684P2P\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u68c0\u6d4b\u5ba2\u6237\u7aef\u76f8\u4f3c\u6027\u5e76\u5f62\u6210\u534f\u4f5c\u7ec4\uff0c\u5229\u7528\u5dee\u5206\u9690\u79c1\u77e5\u8bc6\u84b8\u998f\u5171\u540c\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u548c\u9c81\u68d2\u6027\u7684\u540c\u65f6\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740AI\u5728IoT\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5f02\u6784\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u5171\u4eab\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "P4\u65b9\u6cd5\u91c7\u7528\u8f7b\u91cf\u7ea7\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u68c0\u6d4b\u5ba2\u6237\u7aef\u76f8\u4f3c\u6027\u5e76\u5f62\u6210\u534f\u4f5c\u7ec4\uff0c\u7ec4\u5185\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u77e5\u8bc6\u84b8\u998f\u5171\u540c\u8bad\u7ec3\u6a21\u578b\uff0c\u786e\u4fdd\u9690\u79c1\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cP4\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u5dee\u5206\u9690\u79c1P2P\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53475%\u81f330%\uff0c\u4e14\u572830%\u6076\u610f\u5ba2\u6237\u7aef\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u8d44\u6e90\u5f00\u9500\u4ec5\u589e\u52a0\u7ea67\u79d2\u3002", "conclusion": "P4\u4e3a\u8d44\u6e90\u53d7\u9650\u7684IoT\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u9c81\u68d2\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "paper_title_zh": "\u5ba2\u6237\u7aef\u805a\u7c7b\u4e0e\u77e5\u8bc6\u5171\u4eab\u7684\u7ed3\u5408\uff1a\u589e\u5f3a\u4e2a\u6027\u5316\u70b9\u5bf9\u70b9\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4e0e\u9c81\u68d2\u6027", "abstract_zh": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u52a0\u5267\u4e86\u5bf9\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u95f4\u7684\u9ad8\u6548\u77e5\u8bc6\u8f6c\u79fb\u3001\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u4ee5\u53ca\u5bf9\u6295\u6bd2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u672c\u6587\u901a\u8fc7\u5f00\u53d1P4\uff08\u4e2a\u6027\u5316\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u70b9\u5bf9\u70b9\uff09\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u65e8\u5728\u4e3a\u8d44\u6e90\u53d7\u9650\u7684IoT\u8bbe\u5907\u63d0\u4f9b\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u540c\u65f6\u786e\u4fdd\u5dee\u5206\u9690\u79c1\u548c\u5bf9\u6297\u6295\u6bd2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u91c7\u7528\u8f7b\u91cf\u7ea7\u3001\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u7b97\u6cd5\uff0c\u4ee5\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u5f0f\u68c0\u6d4b\u5ba2\u6237\u7aef\u76f8\u4f3c\u6027\u5e76\u5f62\u6210\u534f\u4f5c\u7ec4\u3002\u7ec4\u5185\u5ba2\u6237\u7aef\u5229\u7528\u5dee\u5206\u9690\u79c1\u77e5\u8bc6\u84b8\u998f\u5171\u540c\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u786e\u4fdd\u5bf9\u6076\u610f\u5ba2\u6237\u7aef\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728\u591a\u79cd\u5f02\u6784\u8bbe\u7f6e\u548c\u653b\u51fb\u573a\u666f\u4e0b\uff0c\u4f7f\u7528\u7ebf\u6027\u548c\u57fa\u4e8eCNN\u7684\u67b6\u6784\u5bf9P4\u5728\u6d41\u884c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cP4\u6bd4\u9886\u5148\u7684\u5dee\u5206\u9690\u79c1\u70b9\u5bf9\u70b9\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u9ad8\u4e865%\u81f330%\uff0c\u5e76\u572830%\u6076\u610f\u5ba2\u6237\u7aef\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72P4\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u4e24\u4e2a\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u534f\u4f5c\u8bad\u7ec3\u4ec5\u589e\u52a0\u7ea67\u79d2\u7684\u5f00\u9500\u3002"}}
{"id": "2506.20415", "pdf": "https://arxiv.org/pdf/2506.20415", "abs": "https://arxiv.org/abs/2506.20415", "authors": ["Dipayan Saha", "Shams Tarek", "Hasan Al Shaikh", "Khan Thamid Hasan", "Pavan Sai Nalluri", "Md. Ajoad Hasan", "Nashmin Alam", "Jingbo Zhou", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": null, "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy.", "AI": {"tldr": "SV-LLM\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u5e76\u63d0\u5347SoC\u5b89\u5168\u9a8c\u8bc1\u7684\u6548\u7387\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfSoC\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u3001\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5148\u8fdb\u80fd\u529b\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "SV-LLM\u91c7\u7528\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u4ee3\u7406\u4e13\u6ce8\u4e8e\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u9a8c\u8bc1\u95ee\u7b54\u3001\u5a01\u80c1\u5efa\u6a21\u7b49\uff09\uff0c\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u5fae\u8c03\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u548c\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cSV-LLM\u80fd\u591f\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u5b89\u5168\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u652f\u6301\u8bbe\u8ba1\u5468\u671f\u65e9\u671f\u7684\u98ce\u9669\u8bc6\u522b\u4e0e\u7f13\u89e3\u3002", "conclusion": "SV-LLM\u5c55\u793a\u4e86\u5229\u7528\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u63d0\u5347\u786c\u4ef6\u5b89\u5168\u5b9e\u8df5\u7684\u6f5c\u529b\uff0c\u4e3aSoC\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SV-LLM\uff1a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684SoC\u5b89\u5168\u9a8c\u8bc1\u4ee3\u7406\u65b9\u6cd5", "abstract_zh": "\u786e\u4fdd\u590d\u6742\u7cfb\u7edf\u7ea7\u82af\u7247\uff08SoC\uff09\u8bbe\u8ba1\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u9a8c\u8bc1\u6280\u672f\u56e0\u81ea\u52a8\u5316\u3001\u6269\u5c55\u6027\u3001\u5168\u9762\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u6311\u6218\u800c\u96be\u4ee5\u5e94\u5bf9\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51ed\u501f\u5176\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u4ee3\u7801\u751f\u6210\u548c\u9ad8\u7ea7\u63a8\u7406\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u8d85\u8d8a\u5355\u4e00\u6a21\u578b\uff0c\u4ee3\u7406\u65b9\u6cd5\u5141\u8bb8\u521b\u5efa\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5176\u4e2d\u4e13\u95e8\u7684LLM\u534f\u4f5c\u66f4\u6709\u6548\u5730\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SV-LLM\uff0c\u4e00\u79cd\u65b0\u578b\u591a\u4ee3\u7406\u8f85\u52a9\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u5e76\u589e\u5f3aSoC\u5b89\u5168\u9a8c\u8bc1\u3002\u901a\u8fc7\u6574\u5408\u4e13\u6ce8\u4e8e\u9a8c\u8bc1\u95ee\u7b54\u3001\u5b89\u5168\u8d44\u4ea7\u8bc6\u522b\u3001\u5a01\u80c1\u5efa\u6a21\u3001\u6d4b\u8bd5\u8ba1\u5212\u4e0e\u5c5e\u6027\u751f\u6210\u3001\u6f0f\u6d1e\u68c0\u6d4b\u53ca\u57fa\u4e8e\u6a21\u62df\u7684\u7f3a\u9677\u9a8c\u8bc1\u7b49\u4efb\u52a1\u7684\u4ee3\u7406\uff0cSV-LLM\u4f18\u5316\u4e86\u5de5\u4f5c\u6d41\u7a0b\u3002\u4e3a\u5728\u8fd9\u4e9b\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u4ee3\u7406\u91c7\u7528\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u5fae\u8c03\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b49\u5b66\u4e60\u8303\u5f0f\u3002\u8be5\u7cfb\u7edf\u65e8\u5728\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3001\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u52a0\u901f\u5b89\u5168\u5206\u6790\uff0c\u652f\u6301\u5728\u8bbe\u8ba1\u5468\u671f\u65e9\u671f\u4e3b\u52a8\u8bc6\u522b\u548c\u7f13\u89e3\u98ce\u9669\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5176\u5728\u786c\u4ef6\u5b89\u5168\u5b9e\u8df5\u4e2d\u7684\u6f5c\u5728\u53d8\u9769\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.20417", "pdf": "https://arxiv.org/pdf/2506.20417", "abs": "https://arxiv.org/abs/2506.20417", "authors": ["Tatsuhiro Shimizu", "Kazuki Kawamura", "Takanori Muroi", "Yusuke Narita", "Kei Tateno", "Takuma Udagawa", "Yuta Saito"], "title": "Off-Policy Evaluation and Learning for the Future under Non-Stationarity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study the novel problem of future off-policy evaluation (F-OPE) and\nlearning (F-OPL) for estimating and optimizing the future value of policies in\nnon-stationary environments, where distributions vary over time. In e-commerce\nrecommendations, for instance, our goal is often to estimate and optimize the\npolicy value for the upcoming month using data collected by an old policy in\nthe previous month. A critical challenge is that data related to the future\nenvironment is not observed in the historical data. Existing methods assume\nstationarity or depend on restrictive reward-modeling assumptions, leading to\nsignificant bias. To address these limitations, we propose a novel estimator\nnamed \\textit{\\textbf{O}ff-\\textbf{P}olicy Estimator for the \\textbf{F}uture\n\\textbf{V}alue (\\textbf{\\textit{OPFV}})}, designed for accurately estimating\npolicy values at any future time point. The key feature of OPFV is its ability\nto leverage the useful structure within time-series data. While future data\nmight not be present in the historical log, we can leverage, for example,\nseasonal, weekly, or holiday effects that are consistent in both the historical\nand future data. Our estimator is the first to exploit these time-related\nstructures via a new type of importance weighting, enabling effective F-OPE.\nTheoretical analysis identifies the conditions under which OPFV becomes\nlow-bias. In addition, we extend our estimator to develop a new policy-gradient\nmethod to proactively learn a good future policy using only historical data.\nEmpirical results show that our methods substantially outperform existing\nmethods in estimating and optimizing the future policy value under\nnon-stationarity for various experimental setups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPFV\u7684\u65b0\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u51c6\u786e\u4f30\u8ba1\u548c\u4f18\u5316\u672a\u6765\u7b56\u7565\u4ef7\u503c\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u7ed3\u6784\uff08\u5982\u5b63\u8282\u6027\u3001\u5468\u6548\u5e94\u7b49\uff09\u6765\u51cf\u5c11\u504f\u5dee\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "motivation": "\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\uff08\u5982\u7535\u5b50\u5546\u52a1\u63a8\u8350\u7cfb\u7edf\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u73af\u5883\u5e73\u7a33\u6216\u4f9d\u8d56\u4e25\u683c\u7684\u5956\u52b1\u5efa\u6a21\u5047\u8bbe\uff0c\u5bfc\u81f4\u663e\u8457\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u548c\u4f18\u5316\u672a\u6765\u7b56\u7565\u4ef7\u503c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPFV\u7684\u65b0\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u7ed3\u6784\uff08\u5982\u5b63\u8282\u6027\u3001\u5468\u6548\u5e94\u7b49\uff09\u8fdb\u884c\u91cd\u8981\u6027\u52a0\u6743\uff0c\u4ee5\u51c6\u786e\u4f30\u8ba1\u672a\u6765\u7b56\u7565\u4ef7\u503c\u3002\u6b64\u5916\uff0c\u8fd8\u6269\u5c55\u4e3a\u4e00\u79cd\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ec5\u4f7f\u7528\u5386\u53f2\u6570\u636e\u4e3b\u52a8\u5b66\u4e60\u672a\u6765\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPFV\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u548c\u4f18\u5316\u672a\u6765\u7b56\u7565\u4ef7\u503c\u3002", "conclusion": "OPFV\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u7ed3\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u672a\u6765\u7b56\u7565\u4ef7\u503c\u4f30\u8ba1\u548c\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002", "paper_title_zh": "\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u672a\u6765\u7b56\u7565\u7684\u79bb\u7b56\u7565\u8bc4\u4f30\u4e0e\u5b66\u4e60", "abstract_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u672a\u6765\u79bb\u7b56\u7565\u8bc4\u4f30\uff08F-OPE\uff09\u548c\u5b66\u4e60\uff08F-OPL\uff09\u7684\u65b0\u95ee\u9898\uff0c\u65e8\u5728\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u4f30\u8ba1\u548c\u4f18\u5316\u7b56\u7565\u7684\u672a\u6765\u4ef7\u503c\u3002\u4f8b\u5982\uff0c\u5728\u7535\u5b50\u5546\u52a1\u63a8\u8350\u4e2d\uff0c\u76ee\u6807\u901a\u5e38\u662f\u5229\u7528\u4e0a\u4e2a\u6708\u65e7\u7b56\u7565\u6536\u96c6\u7684\u6570\u636e\u6765\u4f30\u8ba1\u548c\u4f18\u5316\u4e0b\u4e2a\u6708\u7684\u7b56\u7565\u4ef7\u503c\u3002\u5173\u952e\u6311\u6218\u662f\u672a\u6765\u73af\u5883\u76f8\u5173\u7684\u6570\u636e\u5728\u5386\u53f2\u6570\u636e\u4e2d\u672a\u88ab\u89c2\u5bdf\u5230\u3002\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u73af\u5883\u5e73\u7a33\u6216\u4f9d\u8d56\u4e25\u683c\u7684\u5956\u52b1\u5efa\u6a21\u5047\u8bbe\uff0c\u5bfc\u81f4\u663e\u8457\u504f\u5dee\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPFV\u7684\u65b0\u4f30\u8ba1\u5668\uff0c\u4e13\u4e3a\u51c6\u786e\u4f30\u8ba1\u4efb\u4f55\u672a\u6765\u65f6\u95f4\u70b9\u7684\u7b56\u7565\u4ef7\u503c\u800c\u8bbe\u8ba1\u3002OPFV\u7684\u5173\u952e\u7279\u70b9\u662f\u80fd\u591f\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u6709\u7528\u7ed3\u6784\uff08\u5982\u5b63\u8282\u6027\u3001\u5468\u6548\u5e94\u6216\u8282\u5047\u65e5\u6548\u5e94\uff09\u3002\u6211\u4eec\u7684\u4f30\u8ba1\u5668\u9996\u6b21\u901a\u8fc7\u4e00\u79cd\u65b0\u578b\u91cd\u8981\u6027\u52a0\u6743\u5229\u7528\u8fd9\u4e9b\u65f6\u95f4\u76f8\u5173\u7ed3\u6784\uff0c\u5b9e\u73b0\u6709\u6548\u7684F-OPE\u3002\u7406\u8bba\u5206\u6790\u786e\u5b9a\u4e86OPFV\u4f4e\u504f\u5dee\u7684\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u4f30\u8ba1\u5668\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u5386\u53f2\u6570\u636e\u4e3b\u52a8\u5b66\u4e60\u672a\u6765\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u548c\u4f18\u5316\u672a\u6765\u7b56\u7565\u4ef7\u503c\u3002"}}
{"id": "2506.20451", "pdf": "https://arxiv.org/pdf/2506.20451", "abs": "https://arxiv.org/abs/2506.20451", "authors": ["Shuchu Han", "Wolfgang Bruckner"], "title": "Automatic Demonstration Selection for LLM-based Tabular Data Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u56fe\u7406\u8bba\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9009\u62e9\u9002\u5408\u7684\u6f14\u793a\u6837\u672c\u6570\u91cf\uff0c\u4ee5\u4f18\u5316\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8868\u683c\u6570\u636e\u5206\u7c7b\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u7efc\u5408\u8003\u8651\u6570\u636e\u5206\u5e03\u3001\u7528\u6237\u63d0\u793a\u6a21\u677f\u548cLLM\u7279\u6027\uff0c\u901a\u8fc7\u6784\u5efa\u76f8\u4f3c\u6027\u56fe\u5e76\u5206\u6790\u5176\u7279\u5f81\u503c\uff0c\u786e\u5b9a\u6700\u5c0f\u6f14\u793a\u6837\u672c\u6570\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u8868\u683c\u6570\u636e\u5206\u7c7b\u4e2d\uff0c\u5982\u4f55\u786e\u5b9a\u63d0\u793a\u4e2d\u6f14\u793a\u6837\u672c\u7684\u7406\u60f3\u6570\u91cf\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u9009\u62e9\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u5206\u5e03\u548c\u6a21\u578b\u7279\u6027\u7684\u7efc\u5408\u8003\u8651\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u9009\u62e9\u6f14\u793a\u6837\u672c\u6570\u91cf\u7684\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u56fe\u7406\u8bba\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u65b0\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u6784\u5efa\u76f8\u4f3c\u6027\u56fe\u5e76\u5206\u6790\u5176\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7684\u7279\u5f81\u503c\uff0c\u4ece\u800c\u786e\u5b9a\u80fd\u591f\u4ee3\u8868\u6570\u636e\u7684\u6700\u5c0f\u6f14\u793a\u6837\u672c\u6570\u91cf\u3002\u8be5\u65b9\u6cd5\u8fd8\u6574\u5408\u4e86\u7528\u6237\u9009\u62e9\u7684\u63d0\u793a\u6a21\u677f\u548c\u7279\u5b9aLLM\u7684\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u548cLLM\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u968f\u673a\u9009\u62e9\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u786e\u5b9a\u6f14\u793a\u6837\u672c\u6570\u91cf\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u52a8\u6f14\u793a\u6837\u672c\u9009\u62e9\u7b97\u6cd5\u901a\u8fc7\u8c31\u56fe\u7406\u8bba\u548c\u7efc\u5408\u8003\u91cf\u6570\u636e\u5206\u5e03\u3001\u63d0\u793a\u6a21\u677f\u53caLLM\u7279\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u6f14\u793a\u6837\u672c\u6570\u91cf\u9009\u62e9\u95ee\u9898\uff0c\u4e3aICL\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u683c\u6570\u636e\u5206\u7c7b\u81ea\u52a8\u6f14\u793a\u6837\u672c\u9009\u62e9", "abstract_zh": "\u5728\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u8868\u683c\u6570\u636e\u5206\u7c7b\u4e2d\uff0c\u5982\u4f55\u786e\u5b9a\u63d0\u793a\u4e2d\u6f14\u793a\u6837\u672c\u7684\u7406\u60f3\u6570\u91cf\u662f\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u6765\u81ea\u52a8\u9009\u62e9\u5408\u7406\u7684\u6f14\u793a\u6837\u672c\u6570\u91cf\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u4e0d\u4ec5\u6574\u5408\u4e86\u8868\u683c\u6570\u636e\u7684\u5206\u5e03\uff0c\u8fd8\u7ed3\u5408\u4e86\u7528\u6237\u9009\u62e9\u7684\u63d0\u793a\u6a21\u677f\u548c\u7279\u5b9a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7279\u6027\u3002\u57fa\u4e8e\u8c31\u56fe\u7406\u8bba\uff0c\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5b9a\u4e49\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u91cf\u5316\u4e0d\u540c\u6f14\u793a\u6837\u672c\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u968f\u540e\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u76f8\u4f3c\u6027\u56fe\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u5176\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7684\u7279\u5f81\u503c\uff0c\u63a8\u5bfc\u51fa\u80fd\u591f\u5728LLM\u7684\u5185\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u4ee3\u8868\u6570\u636e\u7684\u6700\u5c0f\u6f14\u793a\u6837\u672c\u6570\u91cf\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548cLLM\u4e0a\u4e0e\u4f20\u7edf\u968f\u673a\u9009\u62e9\u7b97\u6cd5\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.20525", "pdf": "https://arxiv.org/pdf/2506.20525", "abs": "https://arxiv.org/abs/2506.20525", "authors": ["Christian Intern\u00f2", "Andrea Castellani", "Sebastian Schmitt", "Fabio Stella", "Barbara Hammer"], "title": "Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of\nhigh-quality datasets and the complex variability of industrial energy\nconsumption patterns. To address data scarcity and privacy issues, we introduce\nthe Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an\nopen-source dataset generated using Digital Twin simulations. SIDED includes\nthree types of industrial facilities across three different geographic\nlocations, capturing diverse appliance behaviors, weather conditions, and load\nprofiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)\nmethod, a computationally efficient technique that enhances NILM model\ngeneralization by intelligently scaling appliance power contributions based on\ntheir relative impact. We show in experiments that NILM models trained with\nAMDA-augmented data significantly improve the disaggregation of energy\nconsumption of complex industrial appliances like combined heat and power\nsystems. Specifically, in our out-of-sample scenarios, models trained with AMDA\nachieved a Normalized Disaggregation Error of 0.093, outperforming models\ntrained without data augmentation (0.451) and those trained with random data\naugmentation (0.290). Data distribution analyses confirm that AMDA effectively\naligns training and test data distributions, enhancing model generalization.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5de5\u4e1a\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\uff08NILM\uff09\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u5de5\u4e1a\u80fd\u8017\u6a21\u5f0f\u590d\u6742\u591a\u53d8\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u751f\u6210\u7684\u5408\u6210\u5de5\u4e1a\u6570\u636e\u96c6\uff08SIDED\uff09\u548c\u9ad8\u6548\u7684\u8bbe\u5907\u8c03\u5236\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08AMDA\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAMDA\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5de5\u4e1a\u8bbe\u5907\u7684\u80fd\u8017\u5206\u89e3\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\uff08NILM\uff09\u9762\u4e34\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u5de5\u4e1a\u80fd\u8017\u6a21\u5f0f\u590d\u6742\u591a\u53d8\u7684\u6311\u6218\uff0c\u540c\u65f6\u6570\u636e\u9690\u79c1\u95ee\u9898\u4e5f\u9650\u5236\u4e86\u6570\u636e\u7684\u83b7\u53d6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u751f\u6210\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347NILM\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u5408\u6210\u5de5\u4e1a\u6570\u636e\u96c6\uff08SIDED\uff09\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u751f\u6210\u5305\u542b\u4e09\u79cd\u5de5\u4e1a\u8bbe\u65bd\u548c\u4e0d\u540c\u5730\u7406\u4f4d\u7f6e\u7684\u5f00\u653e\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u8bbe\u5907\u8c03\u5236\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08AMDA\uff09\uff0c\u901a\u8fc7\u667a\u80fd\u8c03\u6574\u8bbe\u5907\u529f\u7387\u8d21\u732e\u6765\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528AMDA\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684NILM\u6a21\u578b\u5728\u590d\u6742\u5de5\u4e1a\u8bbe\u5907\uff08\u5982\u70ed\u7535\u8054\u4ea7\u7cfb\u7edf\uff09\u7684\u80fd\u8017\u5206\u89e3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5f52\u4e00\u5316\u5206\u89e3\u8bef\u5dee\u4e3a0.093\uff0c\u663e\u8457\u4f18\u4e8e\u672a\u4f7f\u7528\u6570\u636e\u589e\u5f3a\uff080.451\uff09\u548c\u968f\u673a\u6570\u636e\u589e\u5f3a\uff080.290\uff09\u7684\u6a21\u578b\u3002\u6570\u636e\u5206\u6790\u8868\u660e\uff0cAMDA\u6709\u6548\u5bf9\u9f50\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u5206\u5e03\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SIDED\u6570\u636e\u96c6\u548cAMDA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1aNILM\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5de5\u4e1a\u80fd\u8017\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002", "paper_title_zh": "\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u751f\u6210\u6570\u636e\u96c6\u548c\u9ad8\u6548\u6570\u636e\u589e\u5f3a\u7684\u5de5\u4e1a\u80fd\u8017\u5206\u89e3", "abstract_zh": "\u5de5\u4e1a\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\uff08NILM\uff09\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u5de5\u4e1a\u80fd\u8017\u6a21\u5f0f\u7684\u590d\u6742\u591a\u53d8\u6027\u3002\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5408\u6210\u5de5\u4e1a\u6570\u636e\u96c6\uff08SIDED\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u751f\u6210\u7684\u5f00\u6e90\u6570\u636e\u96c6\u3002SIDED\u5305\u542b\u4e09\u79cd\u5de5\u4e1a\u8bbe\u65bd\u548c\u4e09\u4e2a\u4e0d\u540c\u5730\u7406\u4f4d\u7f6e\u7684\u6570\u636e\uff0c\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u8bbe\u5907\u884c\u4e3a\u3001\u5929\u6c14\u6761\u4ef6\u548c\u8d1f\u8f7d\u66f2\u7ebf\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u8bbe\u5907\u8c03\u5236\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08AMDA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u6280\u672f\uff0c\u901a\u8fc7\u667a\u80fd\u8c03\u6574\u8bbe\u5907\u529f\u7387\u8d21\u732e\u6765\u63d0\u5347NILM\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528AMDA\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684NILM\u6a21\u578b\u5728\u590d\u6742\u5de5\u4e1a\u8bbe\u5907\uff08\u5982\u70ed\u7535\u8054\u4ea7\u7cfb\u7edf\uff09\u7684\u80fd\u8017\u5206\u89e3\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u6837\u672c\u5916\u573a\u666f\u4e2d\uff0c\u4f7f\u7528AMDA\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u5f52\u4e00\u5316\u5206\u89e3\u8bef\u5dee\u4e3a0.093\uff0c\u4f18\u4e8e\u672a\u4f7f\u7528\u6570\u636e\u589e\u5f3a\uff080.451\uff09\u548c\u968f\u673a\u6570\u636e\u589e\u5f3a\uff080.290\uff09\u7684\u6a21\u578b\u3002\u6570\u636e\u5206\u5e03\u5206\u6790\u8bc1\u5b9e\uff0cAMDA\u6709\u6548\u5bf9\u9f50\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u5206\u5e03\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.20535", "pdf": "https://arxiv.org/pdf/2506.20535", "abs": "https://arxiv.org/abs/2506.20535", "authors": ["Hongzhen Huang", "Kunming Zhang", "Hanlong Liao", "Kui Wu", "Guoming Tang"], "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "11 pages, 7 figures and 5 tables", "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.", "AI": {"tldr": "WattsOnAI\u662f\u4e00\u4e2a\u7efc\u5408\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u6d4b\u91cf\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u8017\u4e0e\u78b3\u6392\u653e\uff0c\u586b\u8865\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u6807\u51c6\u5316\u62a5\u544a\u548c\u76f8\u5173\u6027\u5206\u6790\uff0c\u63a8\u52a8\u7eff\u8272AI\u5b9e\u8df5\u3002", "motivation": "\u968f\u7740AI\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u80fd\u8017\u4e0e\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u5177\u5728\u6d4b\u91cf\u548c\u62a5\u544a\u8fd9\u4e9b\u5f71\u54cd\u65f6\u5f80\u5f80\u5206\u6563\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u96be\u4ee5\u652f\u6301\u76f8\u5173\u6027\u5206\u6790\u3002", "method": "WattsOnAI\u901a\u8fc7\u96c6\u6210\u73b0\u6709AI\u6846\u67b6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u80fd\u8017\u3001\u529f\u7387\u3001\u786c\u4ef6\u6027\u80fd\u548c\u78b3\u6392\u653e\u62a5\u544a\uff0c\u5e76\u652f\u6301\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5bfc\u51fa\uff0c\u4fbf\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u53ef\u91cd\u590d\u6027\u5206\u6790\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u652f\u6301\u786c\u4ef6\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u6df1\u5ea6\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "WattsOnAI\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u80fd\u8017\u4e0e\u78b3\u6392\u653e\u6d4b\u91cf\u529f\u80fd\uff0c\u652f\u6301\u6807\u51c6\u5316\u62a5\u544a\u548c\u6df1\u5ea6\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u5e76\u63a8\u52a8\u7eff\u8272AI\u5b9e\u8df5\u3002", "conclusion": "WattsOnAI\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u5de5\u5177\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86AI\u5de5\u4f5c\u8d1f\u8f7d\u80fd\u8017\u4e0e\u78b3\u6392\u653e\u6d4b\u91cf\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u9f13\u52b1\u7814\u7a76\u793e\u533a\u5728\u8ffd\u6c42\u6027\u80fd\u7684\u540c\u65f6\u5173\u6ce8\u73af\u5883\u5f71\u54cd\uff0c\u63a8\u52a8\u7eff\u8272AI\u7684\u53d1\u5c55\u3002", "paper_title_zh": "WattsOnAI\uff1a\u6d4b\u91cf\u3001\u5206\u6790\u4e0e\u53ef\u89c6\u5316AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u8017\u4e0e\u78b3\u8db3\u8ff9", "abstract_zh": "AI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u8017\u4e0e\u78b3\u6392\u653e\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6d4b\u91cf\u548c\u62a5\u544a\u5de5\u5177\u5f80\u5f80\u5206\u6563\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6307\u6807\u6574\u5408\uff0c\u4e14\u5bf9\u76f8\u5173\u6027\u5206\u6790\u7684\u652f\u6301\u6709\u9650\u3002\u672c\u6587\u4ecb\u7ecd\u4e86WattsOnAI\uff0c\u4e00\u4e2a\u7528\u4e8e\u6d4b\u91cf\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316AI\u5de5\u4f5c\u8d1f\u8f7d\u80fd\u8017\u3001\u529f\u7387\u3001\u786c\u4ef6\u6027\u80fd\u53ca\u78b3\u6392\u653e\u7684\u7efc\u5408\u8f6f\u4ef6\u5de5\u5177\u5305\u3002\u901a\u8fc7\u4e0e\u73b0\u6709AI\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\uff0cWattsOnAI\u63d0\u4f9b\u6807\u51c6\u5316\u62a5\u544a\uff0c\u5e76\u5bfc\u51fa\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4ee5\u8f7b\u91cf\u7ea7\u65b9\u5f0f\u652f\u6301\u57fa\u51c6\u6d4b\u8bd5\u548c\u53ef\u91cd\u590d\u6027\u7814\u7a76\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u652f\u6301\u786c\u4ef6\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u6df1\u5ea6\u76f8\u5173\u6027\u5206\u6790\uff0c\u4ece\u800c\u5e2e\u52a9\u8bc6\u522b\u74f6\u9888\u5e76\u4f18\u5316\u6027\u80fd\u3002\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u7684\u5173\u952e\u4e0d\u8db3\uff0cWattsOnAI\u9f13\u52b1\u7814\u7a76\u793e\u533a\u5728\u5173\u6ce8AI\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u7684\u540c\u65f6\u6743\u8861\u73af\u5883\u5f71\u54cd\uff0c\u63a8\u52a8\u66f4\u53ef\u6301\u7eed\u7684\u201c\u7eff\u8272AI\u201d\u5b9e\u8df5\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/SusCom-Lab/WattsOnAI\u3002"}}
{"id": "2506.20551", "pdf": "https://arxiv.org/pdf/2506.20551", "abs": "https://arxiv.org/abs/2506.20551", "authors": ["Soumya Madireddy", "Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci", "Zhe Han", "Yunpeng Zhang"], "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\u4e2d\u7684\u89c4\u8303\u5408\u89c4\u6027\u68c0\u67e5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u68c0\u67e5\u7684\u65f6\u95f4\u548c\u9519\u8bef\u3002", "motivation": "\u4f20\u7edfBIM\u4e2d\u7684\u89c4\u8303\u5408\u89c4\u6027\u68c0\u67e5\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u6574\u5408\u4e86GPT\u3001Claude\u3001Gemini\u548cLlama\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u4e0eRevit\u8f6f\u4ef6\uff0c\u901a\u8fc7\u751f\u6210Python\u811a\u672c\u5b9e\u73b0\u534a\u81ea\u52a8\u5316\u7684\u5408\u89c4\u6027\u68c0\u67e5\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u51cf\u5c11\u4e86\u5408\u89c4\u68c0\u67e5\u7684\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u81ea\u52a8\u8bc6\u522b\u8fdd\u89c4\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aBIM\u89c4\u8303\u68c0\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u3001\u7075\u6d3b\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5efa\u7b51\u4fe1\u606f\u6a21\u578b\u89c4\u8303\u5408\u89c4\u6027\u68c0\u67e5", "abstract_zh": "\u672c\u7814\u7a76\u9488\u5bf9\u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\u4e2d\u4eba\u5de5\u89c4\u8303\u5408\u89c4\u6027\u68c0\u67e5\u8017\u65f6\u4e14\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86GPT\u3001Claude\u3001Gemini\u548cLlama\u7b49LLM\u4e0eRevit\u8f6f\u4ef6\uff0c\u7528\u4e8e\u89e3\u6790\u5efa\u7b51\u89c4\u8303\u3001\u751f\u6210Python\u811a\u672c\u5e76\u5728BIM\u73af\u5883\u4e2d\u6267\u884c\u534a\u81ea\u52a8\u5316\u5408\u89c4\u68c0\u67e5\u3002\u901a\u8fc7\u5bf9\u5355\u6237\u4f4f\u5b85\u9879\u76ee\u548c\u529e\u516c\u697c\u9879\u76ee\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5408\u89c4\u68c0\u67e5\u7684\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u5173\u7cfb\u548c\u751f\u6210\u53ef\u64cd\u4f5c\u62a5\u544a\uff0c\u7b80\u5316\u4e86\u8fdd\u89c4\u95ee\u9898\u7684\u8bc6\u522b\uff0c\u5982\u4e0d\u7b26\u5408\u89c4\u8303\u7684\u623f\u95f4\u5c3a\u5bf8\u3001\u6750\u6599\u4f7f\u7528\u548c\u7269\u4f53\u653e\u7f6e\u3002\u4e0e\u4eba\u5de5\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u6d88\u9664\u4e86\u91cd\u590d\u6027\u4efb\u52a1\uff0c\u7b80\u5316\u4e86\u590d\u6742\u6cd5\u89c4\uff0c\u5e76\u786e\u4fdd\u4e86\u6807\u51c6\u7684\u53ef\u9760\u9075\u5faa\u3002\u901a\u8fc7\u63d0\u4f9b\u4e00\u79cd\u5168\u9762\u3001\u7075\u6d3b\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8eBIM\u7684\u5408\u89c4\u68c0\u67e5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8fdb\u5c55\uff0c\u5e76\u6709\u671b\u5728\u5efa\u7b51\u9879\u76ee\u7684\u591a\u6837\u5316\u89c4\u8303\u6587\u4ef6\u4e2d\u5f97\u5230\u5e94\u7528\u3002"}}
{"id": "2506.20555", "pdf": "https://arxiv.org/pdf/2506.20555", "abs": "https://arxiv.org/abs/2506.20555", "authors": ["Wei-Lin Wu", "Lu Meng", "Shi-Lin Zhu"], "title": "DeepQuark: deep-neural-network approach to multiquark bound states", "categories": ["hep-ph", "cs.AI", "hep-ex", "hep-lat", "nucl-th"], "comment": "10 pages, 3 figures, 6 tables", "summary": "For the first time, we implement the deep-neural-network-based variational\nMonte Carlo approach for the multiquark bound states, whose complexity\nsurpasses that of electron or nucleon systems due to strong SU(3) color\ninteractions. We design a novel and high-efficiency architecture, DeepQuark, to\naddress the unique challenges in multiquark systems such as stronger\ncorrelations, extra discrete quantum numbers, and intractable confinement\ninteraction. Our method demonstrates competitive performance with\nstate-of-the-art approaches, including diffusion Monte Carlo and Gaussian\nexpansion method, in the nucleon, doubly heavy tetraquark, and fully heavy\ntetraquark systems. Notably, it outperforms existing calculations for\npentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we\nsuccessfully incorporate three-body flux-tube confinement interactions without\nadditional computational costs. In tetraquark systems, we consistently describe\nhadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased\nform of wave function ansatz. In the pentaquark sector, we obtain weakly bound\n$\\bar D^*\\Xi_{cc}^*$ molecule $P_{cc\\bar c}(5715)$ with $S=\\frac{5}{2}$ and its\nbottom partner $P_{bb\\bar b}(15569)$. They can be viewed as the analogs of the\nmolecular $T_{cc}$. We recommend experimental search of $P_{cc\\bar c}(5715)$ in\nthe D-wave $J/\\psi \\Lambda_c$ channel. DeepQuark holds great promise for\nextension to larger multiquark systems, overcoming the computational barriers\nin conventional methods. It also serves as a powerful framework for exploring\nconfining mechanism beyond two-body interactions in multiquark states, which\nmay offer valuable insights into nonperturbative QCD and general many-body\nphysics.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u91c7\u7528\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53d8\u5206\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7814\u7a76\u591a\u5938\u514b\u675f\u7f1a\u6001\uff0c\u8bbe\u8ba1\u9ad8\u6548\u67b6\u6784DeepQuark\u89e3\u51b3\u5f3a\u5173\u8054\u3001\u79bb\u6563\u91cf\u5b50\u6570\u548c\u590d\u6742\u7ea6\u675f\u7b49\u6311\u6218\uff0c\u5728\u591a\u5938\u514b\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u5938\u514b\u675f\u7f1a\u6001\u7684\u590d\u6742\u6027\u8fdc\u8d85\u7535\u5b50\u6216\u6838\u5b50\u7cfb\u7edf\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5176\u5f3a\u5173\u8054\u548c\u590d\u6742\u7ea6\u675f\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u7a81\u7834\u8ba1\u7b97\u9650\u5236\uff0c\u63a2\u7d22\u591a\u5938\u514b\u7cfb\u7edf\u7684\u975e\u5fae\u6270QCD\u673a\u5236\u3002", "method": "\u63d0\u51faDeepQuark\u67b6\u6784\uff0c\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u53d8\u5206\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u5904\u7406\u591a\u5938\u514b\u7cfb\u7edf\u7684\u5f3a\u5173\u8054\u3001\u79bb\u6563\u91cf\u5b50\u6570\u548c\u7ea6\u675f\u76f8\u4e92\u4f5c\u7528\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u6838\u5b50\u3001\u53cc\u91cd\u91cd\u56db\u5938\u514b\u548c\u5168\u91cd\u91cd\u56db\u5938\u514b\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u4e94\u5938\u514b\u9886\u57df\u53d1\u73b0\u5f31\u675f\u7f1a\u6001\u5206\u5b50$P_{cc\\bar c}(5715)$\u53ca\u5176\u5e95\u5938\u514b\u4f19\u4f34$P_{bb\\bar b}(15569)$\uff0c\u63a8\u8350\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "DeepQuark\u4e3a\u591a\u5938\u514b\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u9ad8\u6548\u6846\u67b6\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u9650\u5236\uff0c\u6709\u671b\u6269\u5c55\u81f3\u66f4\u5927\u7cfb\u7edf\uff0c\u5e76\u4e3a\u975e\u5fae\u6270QCD\u548c\u5f3a\u76f8\u4e92\u4f5c\u7528\u7269\u7406\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "paper_title_zh": "DeepQuark\uff1a\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5938\u514b\u675f\u7f1a\u6001\u7814\u7a76\u65b9\u6cd5", "abstract_zh": "\u9996\u6b21\u91c7\u7528\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53d8\u5206\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7814\u7a76\u591a\u5938\u514b\u675f\u7f1a\u6001\uff0c\u5176\u590d\u6742\u6027\u56e0\u5f3aSU(3)\u8272\u76f8\u4e92\u4f5c\u7528\u8fdc\u8d85\u7535\u5b50\u6216\u6838\u5b50\u7cfb\u7edf\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u9ad8\u6548\u67b6\u6784DeepQuark\uff0c\u4ee5\u5e94\u5bf9\u591a\u5938\u514b\u7cfb\u7edf\u7684\u72ec\u7279\u6311\u6218\uff0c\u5982\u5f3a\u5173\u8054\u3001\u989d\u5916\u79bb\u6563\u91cf\u5b50\u6570\u548c\u96be\u89e3\u7ea6\u675f\u76f8\u4e92\u4f5c\u7528\u3002\u8be5\u65b9\u6cd5\u5728\u6838\u5b50\u3001\u53cc\u91cd\u91cd\u56db\u5938\u514b\u548c\u5168\u91cd\u91cd\u56db\u5938\u514b\u7cfb\u7edf\u4e2d\u4e0e\u6269\u6563\u8499\u7279\u5361\u6d1b\u548c\u9ad8\u65af\u5c55\u5f00\u6cd5\u7b49\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u4e94\u5938\u514b\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u8ba1\u7b97\uff0c\u4f8b\u5982\u4e09\u91cd\u91cd\u4e94\u5938\u514b\u3002\u5728\u6838\u5b50\u4e2d\uff0c\u6211\u4eec\u6210\u529f\u5f15\u5165\u4e09\u4f53\u901a\u91cf\u7ba1\u7ea6\u675f\u76f8\u4e92\u4f5c\u7528\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002\u5728\u56db\u5938\u514b\u7cfb\u7edf\u4e2d\uff0c\u6211\u4eec\u4ee5\u65e0\u504f\u6ce2\u51fd\u6570\u5f62\u5f0f\u4e00\u81f4\u63cf\u8ff0\u5f3a\u5b50\u5206\u5b50$T_{cc}$\u548c\u7d27\u51d1\u56db\u5938\u514b$T_{bb}$\u3002\u5728\u4e94\u5938\u514b\u9886\u57df\uff0c\u6211\u4eec\u83b7\u5f97\u5f31\u675f\u7f1a\u6001$\\bar D^*\\Xi_{cc}^*$\u5206\u5b50$P_{cc\\bar c}(5715)$\uff08$S=\\frac{5}{2}$\uff09\u53ca\u5176\u5e95\u5938\u514b\u4f19\u4f34$P_{bb\\bar b}(15569)$\uff0c\u5b83\u4eec\u53ef\u89c6\u4e3a\u5206\u5b50$T_{cc}$\u7684\u7c7b\u4f3c\u7269\u3002\u6211\u4eec\u5efa\u8bae\u5728D\u6ce2$J/\\psi \\Lambda_c$\u901a\u9053\u4e2d\u5b9e\u9a8c\u641c\u7d22$P_{cc\\bar c}(5715)$\u3002DeepQuark\u6709\u671b\u6269\u5c55\u81f3\u66f4\u5927\u591a\u5938\u514b\u7cfb\u7edf\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u969c\u788d\uff0c\u5e76\u4e3a\u63a2\u7d22\u591a\u5938\u514b\u6001\u4e2d\u8d85\u8d8a\u4e8c\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u7ea6\u675f\u673a\u5236\u63d0\u4f9b\u5f3a\u5927\u6846\u67b6\uff0c\u4e3a\u975e\u5fae\u6270QCD\u548c\u4e00\u822c\u591a\u4f53\u7269\u7406\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2506.20576", "pdf": "https://arxiv.org/pdf/2506.20576", "abs": "https://arxiv.org/abs/2506.20576", "authors": ["Sabrine Ennaji", "Elhadj Benkhelifa", "Luigi V. Mancini"], "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u548c\u56e0\u679c\u5206\u6790\uff0c\u4ee5\u6700\u5c0f\u4ea4\u4e92\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\uff0c\u9002\u7528\u4e8e\u7f51\u7edc\u6d41\u91cf\u6570\u636e\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u7406\u8bba\u8fdb\u5c55\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u7f51\u7edc\u6d41\u91cf\u7b49\u7ed3\u6784\u5316\u6570\u636e\u4e2d\uff0c\u7279\u5f81\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u4f7f\u5f97\u653b\u51fb\u96be\u4ee5\u6709\u6548\u5b9e\u65bd\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6a21\u7cca\u6027\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u548c\u9632\u5fa1\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u9075\u5faa\u9ed1\u76d2\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u53d8\u70b9\u68c0\u6d4b\u548c\u56e0\u679c\u5206\u6790\u9009\u62e9\u654f\u611f\u7279\u5f81\u8fdb\u884c\u6270\u52a8\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u6613\u4e8e\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c0f\u4ea4\u4e92\u6709\u6548\u89c4\u907f\u68c0\u6d4b\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u9002\u5e94\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7f51\u7edc\u6d41\u91cf\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u57fa\u4e8e\u81ea\u9002\u5e94\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u7684\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u6f0f\u6d1e\u62ab\u9732", "abstract_zh": "\u5bf9\u6297\u653b\u51fb\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5fae\u5c0f\u8f93\u5165\u8bef\u5bfc\u667a\u80fd\u6a21\u578b\uff0c\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u7406\u8bba\u8fdb\u5c55\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u7f51\u7edc\u6d41\u91cf\u7b49\u7ed3\u6784\u5316\u6570\u636e\u4e2d\uff0c\u7279\u5f81\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u4f7f\u5f97\u5bf9\u6297\u64cd\u4f5c\u590d\u6742\u5316\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6a21\u7cca\u6027\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u548c\u8be5\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5bfc\u81f4\u73b0\u6709\u9632\u5fa1\u96be\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u653b\u51fb\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u9650\u5236\u3002\u4e0e\u4ee5\u5f80\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e25\u683c\u9075\u5faa\u9ed1\u76d2\u7ea6\u675f\uff0c\u51cf\u5c11\u4ea4\u4e92\u4ee5\u907f\u514d\u68c0\u6d4b\uff0c\u66f4\u8d34\u8fd1\u5b9e\u9645\u573a\u666f\u3002\u901a\u8fc7\u53d8\u70b9\u68c0\u6d4b\u548c\u56e0\u679c\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u8bc6\u522b\u5e76\u6270\u52a8\u654f\u611f\u7279\u5f81\u3002\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u786e\u4fdd\u4e86\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u9ad8\u53ef\u90e8\u7f72\u6027\u3002\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c0f\u4ea4\u4e92\u6709\u6548\u89c4\u907f\u68c0\u6d4b\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9002\u7528\u6027\u3002\u901a\u8fc7\u6df1\u5316\u5bf9\u7f51\u7edc\u6d41\u91cf\u4e2d\u5bf9\u6297\u653b\u51fb\u7684\u7406\u89e3\uff0c\u672c\u7814\u7a76\u4e3a\u5f00\u53d1\u9c81\u68d2\u9632\u5fa1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.20595", "pdf": "https://arxiv.org/pdf/2506.20595", "abs": "https://arxiv.org/abs/2506.20595", "authors": ["Momin N. Siddiqui", "Roy Pea", "Hari Subramonyam"], "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student Writing", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u5199\u4f5c\u652f\u6301\u5de5\u5177\u5bf9\u5b66\u751f\u5199\u4f5c\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u96c6\u6210\u5f0fAI\u5de5\u5177\u80fd\u63d0\u5347\u5b66\u751f\u7684\u5199\u4f5c\u81ea\u4e3b\u6027\u548c\u77e5\u8bc6\u8f6c\u5316\u6df1\u5ea6\uff0c\u800c\u5355\u7eaf\u7684\u804a\u5929\u5f0fLLM\u6548\u679c\u8f83\u5dee\u3002", "motivation": "\u968f\u7740ChatGPT\u7b49\u6280\u672f\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u62c5\u5fc3\u5176\u5bf9\u5b66\u751f\u7684\u5199\u4f5c\u81ea\u4e3b\u6027\u548c\u5185\u5bb9\u6df1\u5ea6\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u4e0d\u540cAI\u652f\u6301\u65b9\u5f0f\u5982\u4f55\u5f71\u54cd\u5b66\u751f\u7684\u5199\u4f5c\u81ea\u4e3b\u6027\u548c\u77e5\u8bc6\u8f6c\u5316\u6df1\u5ea6\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c90\u540d\u672c\u79d1\u751f\u88ab\u5206\u4e3a\u4e09\u7ec4\uff1a\u4f7f\u7528\u804a\u5929\u5f0fLLM\u5199\u4f5c\u52a9\u624b\u3001\u96c6\u6210\u5f0fAI\u5199\u4f5c\u5de5\u5177\u548c\u6807\u51c6\u5199\u4f5c\u754c\u9762\uff08\u5bf9\u7167\u7ec4\uff09\uff0c\u4ee5\u6bd4\u8f83\u4e0d\u540c\u652f\u6301\u65b9\u5f0f\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u96c6\u6210\u5f0fAI\u5199\u4f5c\u5de5\u5177\u7684\u5b66\u751f\u5728\u5199\u4f5c\u81ea\u4e3b\u6027\u548c\u77e5\u8bc6\u8f6c\u5316\u6df1\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u804a\u5929\u5f0fLLM\u7684\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u5199\u4f5c\u8fc7\u7a0b\u7279\u5b9a\u73af\u8282\u8bbe\u8ba1\u7684AI\u652f\u6301\u5de5\u5177\u80fd\u5e2e\u52a9\u5b66\u751f\u4fdd\u6301\u5bf9\u5199\u4f5c\u7684\u81ea\u4e3b\u6743\uff0c\u540c\u65f6\u63d0\u5347\u5185\u5bb9\u6df1\u5ea6\u3002", "paper_title_zh": "AI\u5728\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\uff1a\u6709\u76ee\u7684\u7684AI\u652f\u6301\u5982\u4f55\u4fc3\u8fdb\u5b66\u751f\u5199\u4f5c", "abstract_zh": "ChatGPT\u7b49\u6280\u672f\u7684\u666e\u53ca\u5f15\u53d1\u4e86\u5bf9\u5176\u5f71\u54cd\u5b66\u751f\u5199\u4f5c\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5bf9\u5b66\u4e60\u8005\u81ea\u4e3b\u6027\u548c\u5185\u5bb9\u6df1\u5ea6\u7684\u5f71\u54cd\u3002\u867d\u7136\u72ec\u7acb\u7684\u804a\u5929\u5f0fLLM\u901a\u5e38\u5bfc\u81f4\u5199\u4f5c\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u6709\u8bc1\u636e\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684AI\u5199\u4f5c\u652f\u6301\u5de5\u5177\u53ef\u4ee5\u6539\u5584\u5199\u4f5c\u8fc7\u7a0b\u3002\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540cAI\u652f\u6301\u65b9\u5f0f\u5982\u4f55\u5f71\u54cd\u4f5c\u8005\u7684\u81ea\u4e3b\u6027\u548c\u77e5\u8bc6\u8f6c\u5316\u6df1\u5ea6\u3002\u901a\u8fc7\u5bf990\u540d\u672c\u79d1\u751f\u8fdb\u884c\u7684\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u4e09\u79cd\u6761\u4ef6\uff1a\uff081\uff09\u804a\u5929\u5f0fLLM\u5199\u4f5c\u52a9\u624b\uff0c\uff082\uff09\u652f\u6301\u591a\u6837\u5316\u5b50\u8fc7\u7a0b\u7684\u96c6\u6210\u5f0fAI\u5199\u4f5c\u5de5\u5177\uff0c\uff083\uff09\u6807\u51c6\u5199\u4f5c\u754c\u9762\uff08\u5bf9\u7167\u7ec4\uff09\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728AI\u652f\u6301\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u96c6\u6210\u5f0fAI\u5199\u4f5c\u5de5\u5177\u7684\u5b66\u751f\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5199\u4f5c\u81ea\u4e3b\u6027\u548c\u66f4\u6df1\u5165\u7684\u77e5\u8bc6\u8f6c\u5316\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u5199\u4f5c\u8fc7\u7a0b\u7279\u5b9a\u73af\u8282\u8bbe\u8ba1\u7684AI\u652f\u6301\u5de5\u5177\u53ef\u4ee5\u5e2e\u52a9\u5b66\u751f\u4fdd\u6301\u5bf9\u4f5c\u54c1\u7684\u6240\u6709\u6743\uff0c\u540c\u65f6\u4fc3\u8fdb\u5bf9\u5185\u5bb9\u7684\u66f4\u6df1\u5165\u53c2\u4e0e\u3002"}}
{"id": "2506.20609", "pdf": "https://arxiv.org/pdf/2506.20609", "abs": "https://arxiv.org/abs/2506.20609", "authors": ["Ankit Shah", "Rita Singh", "Bhiksha Raj", "Alexander Hauptmann"], "title": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "comment": "4 pages + 1 References", "summary": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u58f0\u5b66\u5206\u6790\u67aa\u58f0\u5f55\u97f3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u67aa\u58f0\u5e76\u5206\u7c7b\u67aa\u652f\u7c7b\u578b\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff08\u5305\u62ecSVM\u548cCNN\uff09\uff0cCNN\u8868\u73b0\u4f18\u4e8eSVM\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u548c\u73af\u5883\u566a\u58f0\u4ecd\u662f\u6311\u6218\u3002", "motivation": "\u67aa\u652f\u76f8\u5173\u66b4\u529b\u548c\u5927\u89c4\u6a21\u67aa\u51fb\u4e8b\u4ef6\u7684\u589e\u52a0\u5bf9\u516c\u5171\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u5f53\u524d\u5546\u4e1a\u67aa\u58f0\u68c0\u6d4b\u7cfb\u7edf\u6210\u672c\u9ad8\u6602\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u624b\u673a\u7b49\u5e38\u89c1\u8bbe\u5907\u7684\u5f55\u97f3\u8fdb\u884c\u67aa\u58f0\u68c0\u6d4b\u548c\u67aa\u652f\u5206\u7c7b\u3002", "method": "\u7814\u7a76\u4f7f\u75283459\u6761\u67aa\u58f0\u5f55\u97f3\u6570\u636e\u96c6\uff0c\u5206\u6790\u67aa\u58f0\u7684\u58f0\u5b66\u7279\u5f81\uff08\u5982\u67aa\u53e3\u7206\u70b8\u548c\u51b2\u51fb\u6ce2\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eSVM\u548cCNN\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u67aa\u58f0\u68c0\u6d4b\u548c\u67aa\u652f\u7c7b\u578b\u5206\u7c7b\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u4e3a0.58\uff0c\u4f18\u4e8eSVM\u57fa\u7ebf\uff08mAP 0.39\uff09\u3002\u4f46\u5728\u566a\u58f0\u6570\u636e\uff08\u5982\u7f51\u7edc\u6765\u6e90\u6570\u636e\uff09\u4e2d\u8868\u73b0\u4e0b\u964d\uff08mAP 0.35\uff09\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5229\u7528\u58f0\u5b66\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u4f4e\u6210\u672c\u67aa\u58f0\u68c0\u6d4b\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u7cfb\u7edf\u3002", "paper_title_zh": "\u901a\u8fc7\u67aa\u58f0\u5f55\u97f3\u7684\u58f0\u5b66\u5206\u6790\u89e3\u6790\u67aa\u652f\u7c7b\u578b\u5c42\u7ea7", "abstract_zh": "\u67aa\u652f\u76f8\u5173\u66b4\u529b\u548c\u5927\u89c4\u6a21\u67aa\u51fb\u4e8b\u4ef6\u7684\u6fc0\u589e\u5bf9\u516c\u5171\u5b89\u5168\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002\u4e3a\u6267\u6cd5\u673a\u6784\u63d0\u4f9b\u53ca\u65f6\u51c6\u786e\u7684\u4fe1\u606f\u5bf9\u51cf\u5c11\u6b64\u7c7b\u4e8b\u4ef6\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u5546\u4e1a\u67aa\u58f0\u68c0\u6d4b\u7cfb\u7edf\u867d\u7136\u6709\u6548\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u624b\u673a\u7b49\u5e38\u89c1\u8bbe\u5907\u7684\u67aa\u58f0\u5f55\u97f3\u8fdb\u884c\u58f0\u5b66\u5206\u6790\uff0c\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u67aa\u58f0\uff0c\u8fd8\u80fd\u5206\u7c7b\u4f7f\u7528\u7684\u67aa\u652f\u7c7b\u578b\u3002\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u4f7f\u75283459\u6761\u5f55\u97f3\u6570\u636e\u96c6\u89e3\u6790\u67aa\u652f\u7c7b\u578b\u5c42\u7ea7\u7684\u7814\u7a76\u3002\u6211\u4eec\u7814\u7a76\u4e86\u67aa\u58f0\u7684\u57fa\u672c\u58f0\u5b66\u7279\u5f81\uff08\u5982\u67aa\u53e3\u7206\u70b8\u548c\u51b2\u51fb\u6ce2\uff09\uff0c\u8fd9\u4e9b\u7279\u5f81\u56e0\u67aa\u652f\u7c7b\u578b\u3001\u5f39\u836f\u548c\u5c04\u51fb\u65b9\u5411\u800c\u5f02\u3002\u6211\u4eec\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u4ee5\u53ca\u66f4\u5148\u8fdb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u8054\u5408\u67aa\u58f0\u68c0\u6d4b\u548c\u67aa\u652f\u7c7b\u578b\u5206\u7c7b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5e72\u51c0\u6807\u6ce8\u6570\u636e\u4e0a\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u4e3a0.58\uff0c\u4f18\u4e8eSVM\u57fa\u7ebf\uff08mAP 0.39\uff09\u3002\u8fd8\u8ba8\u8bba\u4e86\u6570\u636e\u8d28\u91cf\u3001\u73af\u5883\u566a\u58f0\u4ee5\u53ca\u4f7f\u7528\u566a\u58f0\u7f51\u7edc\u6570\u636e\uff08mAP 0.35\uff09\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u7b49\u6311\u6218\u3002\u957f\u671f\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u5b9e\u65f6\u7cfb\u7edf\uff0c\u53ef\u90e8\u7f72\u4e8e\u5e38\u89c1\u5f55\u97f3\u8bbe\u5907\uff0c\u663e\u8457\u964d\u4f4e\u68c0\u6d4b\u6210\u672c\uff0c\u5e76\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u5173\u952e\u60c5\u62a5\u3002"}}
{"id": "2506.20621", "pdf": "https://arxiv.org/pdf/2506.20621", "abs": "https://arxiv.org/abs/2506.20621", "authors": ["Silvio Alonso", "Antonio Pedro Santos Alves", "Lucas Romao", "H\u00e9lio Lopes", "Marcos Kalinowski"], "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDefine-ML\u6846\u67b6\uff0c\u6269\u5c55Lean Inception\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u6e90\u6620\u5c04\u3001\u7279\u5f81\u5230\u6570\u636e\u6e90\u6620\u5c04\u548cML\u6620\u5c04\u6d3b\u52a8\uff0c\u7cfb\u7edf\u5316\u6574\u5408\u6570\u636e\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u4ee5\u652f\u6301\u673a\u5668\u5b66\u4e60\u4ea7\u54c1\u7684\u65e9\u671f\u6784\u601d\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u6784\u601d\u65b9\u6cd5\uff08\u5982Lean Inception\uff09\u7f3a\u4e4f\u5bf9\u6570\u636e\u4f9d\u8d56\u3001\u6280\u672f\u53ef\u884c\u6027\u7b49ML\u7279\u6709\u6311\u6218\u7684\u652f\u6301\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ea7\u54c1\u613f\u666f\u4e0d\u6e05\u6670\u6216\u671f\u671b\u4e0d\u5207\u5b9e\u9645\u3002Define-ML\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u6280\u672f\u8f6c\u79fb\u6a21\u578b\u5f00\u53d1\u5e76\u9a8c\u8bc1Define-ML\uff0c\u901a\u8fc7\u9759\u6001\u9a8c\u8bc1\uff08\u73a9\u5177\u95ee\u9898\uff09\u548c\u52a8\u6001\u9a8c\u8bc1\uff08\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff09\u7ed3\u5408\u5b9a\u91cf\u8c03\u67e5\u4e0e\u5b9a\u6027\u53cd\u9988\uff0c\u8bc4\u4f30\u5176\u6548\u7528\u3001\u6613\u7528\u6027\u548c\u91c7\u7528\u610f\u5411\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u4e3aDefine-ML\u80fd\u6709\u6548\u6f84\u6e05\u6570\u636e\u95ee\u9898\u3001\u5bf9\u9f50ML\u80fd\u529b\u4e0e\u4e1a\u52a1\u76ee\u6807\uff0c\u5e76\u4fc3\u8fdb\u8de8\u804c\u80fd\u534f\u4f5c\u3002\u5c3d\u7ba1ML\u7279\u5b9a\u7ec4\u4ef6\u5b58\u5728\u5b66\u4e60\u66f2\u7ebf\uff0c\u4f46\u6240\u6709\u53c2\u4e0e\u8005\u5747\u8868\u793a\u6709\u610f\u91c7\u7528\u3002", "conclusion": "Define-ML\u662f\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u5728Lean Inception\u7684\u654f\u6377\u6027\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d3b\u52a8\u63d0\u5347\u6570\u636e\u4e0e\u6280\u672f\u53ef\u884c\u6027\u7684\u610f\u8bc6\uff0c\u652f\u6301ML\u4ea7\u54c1\u6784\u601d\u3002", "paper_title_zh": "Define-ML\uff1a\u4e00\u79cd\u6784\u601d\u673a\u5668\u5b66\u4e60\u8d4b\u80fd\u7cfb\u7edf\u7684\u65b9\u6cd5", "abstract_zh": "[\u80cc\u666f] \u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u9488\u5bf9ML\u7279\u6709\u6311\u6218\uff08\u5982\u6570\u636e\u4f9d\u8d56\u3001\u6280\u672f\u53ef\u884c\u6027\u3001\u4e1a\u52a1\u76ee\u6807\u4e0e\u6982\u7387\u7cfb\u7edf\u884c\u4e3a\u7684\u5bf9\u9f50\uff09\u7684\u4e13\u4e1a\u6784\u601d\u65b9\u6cd5\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982Lean Inception\uff09\u7f3a\u4e4f\u5bf9\u8fd9\u4e9bML\u95ee\u9898\u7684\u7ed3\u6784\u5316\u652f\u6301\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ea7\u54c1\u613f\u666f\u4e0d\u6e05\u6670\u6216\u671f\u671b\u4e0d\u5207\u5b9e\u9645\u3002[\u76ee\u6807] \u672c\u6587\u63d0\u51faDefine-ML\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55Lean Inception\u5e76\u5f15\u5165\u6570\u636e\u6e90\u6620\u5c04\u3001\u7279\u5f81\u5230\u6570\u636e\u6e90\u6620\u5c04\u548cML\u6620\u5c04\u6d3b\u52a8\uff0c\u7cfb\u7edf\u5316\u6574\u5408\u6570\u636e\u4e0e\u6280\u672f\u7ea6\u675f\u81f3ML\u4ea7\u54c1\u7684\u65e9\u671f\u6784\u601d\u9636\u6bb5\u3002[\u65b9\u6cd5] \u57fa\u4e8e\u6280\u672f\u8f6c\u79fb\u6a21\u578b\u5f00\u53d1\u5e76\u9a8c\u8bc1Define-ML\uff0c\u901a\u8fc7\u9759\u6001\u9a8c\u8bc1\uff08\u73a9\u5177\u95ee\u9898\uff09\u548c\u52a8\u6001\u9a8c\u8bc1\uff08\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff09\u7ed3\u5408\u5b9a\u91cf\u8c03\u67e5\u4e0e\u5b9a\u6027\u53cd\u9988\uff0c\u8bc4\u4f30\u5176\u6548\u7528\u3001\u6613\u7528\u6027\u548c\u91c7\u7528\u610f\u5411\u3002[\u7ed3\u679c] \u53c2\u4e0e\u8005\u8ba4\u4e3aDefine-ML\u80fd\u6709\u6548\u6f84\u6e05\u6570\u636e\u95ee\u9898\u3001\u5bf9\u9f50ML\u80fd\u529b\u4e0e\u4e1a\u52a1\u76ee\u6807\uff0c\u5e76\u4fc3\u8fdb\u8de8\u804c\u80fd\u534f\u4f5c\u3002\u5c3d\u7ba1ML\u7279\u5b9a\u7ec4\u4ef6\u5b58\u5728\u5b66\u4e60\u66f2\u7ebf\uff0c\u4f46\u6240\u6709\u53c2\u4e0e\u8005\u5747\u8868\u793a\u6709\u610f\u91c7\u7528\u3002[\u7ed3\u8bba] Define-ML\u662f\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u5728Lean Inception\u7684\u654f\u6377\u6027\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d3b\u52a8\u63d0\u5347\u6570\u636e\u4e0e\u6280\u672f\u53ef\u884c\u6027\u7684\u610f\u8bc6\uff0c\u652f\u6301ML\u4ea7\u54c1\u6784\u601d\u3002"}}
