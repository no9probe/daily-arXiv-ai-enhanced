<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.CV](#cs.CV) [Total: 96]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.IR](#cs.IR) [Total: 5]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.DC](#cs.DC) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 39]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 8]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 15]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
**中文标题：文本、视觉与语音生成的自动评估方法综述**

*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

TL;DR: The paper provides a comprehensive review and unified taxonomy of automatic evaluation methods for text, image, and audio generation, identifying five fundamental paradigms and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: The lack of a systematic framework for evaluating generative AI outputs across text, images, and audio modalities motivated this review.

Method: The authors review and organize existing evaluation methods for text generation, then extend the framework to image and audio generation, identifying five common paradigms.

Result: A unified taxonomy of automatic evaluation methods for text, visual, and speech generation is presented, along with insights into cross-modal evaluation.

Conclusion: The paper highlights the need for further research in cross-modal evaluation methodologies and provides a foundation for future work.

摘要: 近年来，深度学习的发展显著提升了生成式AI在文本、图像和音频领域的能力。然而，如何自动评估这些生成内容的质量仍是一个持续的挑战。尽管已有许多自动评估方法，但目前的研究缺乏一个系统性的框架，能够全面组织这些方法以覆盖文本、视觉和音频模态。为解决这一问题，本文对这三种模态的生成内容自动评估方法进行了全面综述，并提出了一种统一的分类法。我们识别了五种基本范式，用于描述这些领域的现有评估方法。我们的分析首先考察了文本生成的评估方法，这些技术最为成熟。随后，我们将这一框架扩展到图像和音频生成，证明了其广泛的适用性。最后，我们讨论了跨模态评估方法未来研究的有前景方向。

</details>


### [2] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
**中文标题：TaskCraft：自动化生成代理任务的工具**

*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: TaskCraft introduces an automated workflow to generate scalable, multi-tool agentic tasks, addressing limitations in existing instruction data and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing agentic task benchmarks rely on costly human annotation and lack tool interaction, limiting scalability and utility for NLP/AI advancement.

Method: TaskCraft uses depth-based and width-based extensions to expand atomic tasks into complex challenges, creating a synthetic dataset of 36,000 tasks.

Result: The generated tasks improve prompt optimization and enhance supervised fine-tuning of agentic foundation models.

Conclusion: TaskCraft provides a scalable solution for generating agentic tasks, supporting future research on agent tuning and evaluation.

摘要: 代理任务需要多步骤的问题解决能力、自主性、工具使用和适应性推理，在NLP和AI领域日益重要。然而，现有的指令数据缺乏工具交互，当前的代理基准依赖昂贵的人工标注，限制了其扩展性。我们提出了TaskCraft，一种自动化工作流，用于生成难度可扩展、多工具且可验证的代理任务及其执行轨迹。TaskCraft通过深度和宽度扩展原子任务，创建结构和层次复杂的挑战。实验结果表明，这些任务改进了生成工作流中的提示优化，并增强了代理基础模型的监督微调。我们提供了一个包含约36,000个不同难度任务的大规模合成数据集，以支持未来关于代理调优和评估的研究。

</details>


### [3] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
**中文标题：自然语言处理的量子语义框架**

*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

TL;DR: The paper argues that semantic degeneracy in natural language makes it impossible for any interpreter (human or AI) to recover a single intended meaning as complexity grows. It proposes an observer-dependent view of meaning and demonstrates non-classical contextuality in linguistic interpretation through experiments with LLMs, suggesting Bayesian approaches over classical methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of modern NLP systems and LLMs due to semantic degeneracy, which complicates the recovery of intended meanings as language complexity increases. The paper challenges the classical view of meaning as inherent in linguistic forms.

Method: The method involves using Kolmogorov complexity to argue the vanishing likelihood of recovering intended meanings and conducting a semantic Bell inequality test with LLMs as computational cognitive systems to interpret ambiguous word pairs under varied contexts.

Result: The experiments showed CHSH expectation values (1.2 to 2.8) that violate classical boundaries, indicating non-classical contextuality in linguistic interpretation, similar to human cognition results.

Conclusion: The conclusion is that classical frequentist approaches to NLP are lossy, and Bayesian-style repeated sampling is more suitable for characterizing linguistic meaning in context.

摘要: 语义退化是自然语言的一个基本属性，它不仅超越了简单的多义性，还涵盖了随着语义表达复杂性增加而出现的潜在解释的组合爆炸。大型语言模型（LLMs）和其他现代自然语言处理系统之所以面临固有局限性，正是因为它们在自然语言本身中运行，使其受到语义退化所施加的相同解释约束。在这项工作中，我们利用柯尔莫哥洛夫复杂性论证，随着表达复杂性的增长，任何解释主体（人类或LLM驱动的AI）恢复单一预期意义的可能性消失。这种计算上的难处理性表明，传统认为语言形式本身具有意义的观点是错误的。我们提出，意义是通过依赖于观察者的解释行为实现的。为了验证这一点，我们使用多样化的LLM代理作为“计算认知系统”，在不同上下文设置下解释模糊词对，进行了语义贝尔不等式测试。在多个独立实验中，我们发现平均CHSH期望值在1.2到2.8之间，其中多次运行的结果（如2.3-2.4）显著违反了经典边界（|S|≤2）。这表明在模糊性下的语言解释可以表现出非经典的上下文相关性，与人类认知实验结果一致。这些结果隐含地表明，基于经典频率分析的自然语言方法必然是有损的。相反，我们提出贝叶斯式的重复采样方法可以更实用和适当地描述上下文中的语言意义。

</details>


### [4] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
**中文标题：思维对话：用于生成领域特定信息的协作多智能体系统**

*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

TL;DR: The paper introduces Chat-of-Thought, a multi-agent LLM-based system for generating FMEA documents in industrial settings, using dynamic collaboration and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in generating accurate and comprehensive FMEA documents for industrial assets by leveraging collaborative AI agents.

Method: Employs multiple LLM-based agents with specific roles, dynamic task routing, and a Chat of Thought mechanism for iterative content refinement.

Result: Demonstrates the system's potential in optimizing FMEA table generation and validation through interactive, template-driven workflows.

Conclusion: Chat-of-Thought offers a promising approach for industrial equipment monitoring by enhancing collaboration and context-awareness in AI-driven workflows.

摘要: 本文提出了一种名为思维对话的新型多智能体系统，旨在为工业资产生成故障模式与影响分析（FMEA）文档。该系统利用多个具有特定角色的协作性大型语言模型（LLM）智能体，结合先进的AI技术和动态任务路由，优化FMEA表的生成和验证。该系统的关键创新在于引入了思维对话机制，通过动态、多角色驱动的讨论实现内容的迭代优化。本研究探讨了工业设备监控的应用领域，突出了关键挑战，并通过交互式、模板驱动的工作流程和上下文感知的智能体协作，展示了思维对话在应对这些挑战中的潜力。

</details>


### [5] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
**中文标题：当意义不变而模型漂移：评估大语言模型在标记级行为不稳定性下的服务质量**

*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

TL;DR: The paper investigates how large language models (LLMs) respond to semantically equivalent prompts with different token-level realizations, introducing a framework (PBSS) to measure behavioral drift. Results show model-specific response shifts linked to tokenization and decoding, highlighting instability in model evaluation under rephrasing.


<details>
  <summary>Details</summary>
Motivation: To understand and measure the behavioral instability of LLMs when faced with semantically equivalent prompts that differ only in token-level realization, a phenomenon termed 'prompt variance.'

Method: Proposes Prompt-Based Semantic Shift (PBSS), a diagnostic framework to evaluate behavioral drift in LLMs under semantically equivalent prompt rewordings, tested on ten constrained tasks.

Result: PBSS reveals consistent, model-specific response shifts, indicating statistical regularities tied to tokenization and decoding, which affect post-training quality of service stability.

Conclusion: Tokenization strategies and decoding dynamics contribute to instability in LLM responses under semantically equivalent rephrasing, suggesting a need for improved evaluation methods.

摘要: 我们研究了大语言模型如何响应仅在标记级实现不同但语义意图相同的提示，这一现象称为提示方差。我们提出了基于提示的语义漂移（PBSS），一种用于测量LLM在语义等效提示重述下行为漂移的诊断框架。应用于十个受限任务，PBSS揭示了与标记化和解码相关的模型特定响应漂移。这些结果凸显了在重述下模型评估稳定性的一个被忽视的维度，并表明标记化策略和解码动态可能影响训练后服务质量的不稳定性。

</details>


### [6] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
**中文标题：ChartReasoner：代码驱动的模态桥接用于图表问答中的长链推理**

*Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: ChartReasoner is a code-driven framework for precise chart question answering, converting charts to structured ECharts codes and using synthesized data for multimodal reasoning, achieving competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches lose critical visual details in chart question answering; ChartReasoner aims to preserve structural and semantic information through code-driven reasoning.

Method: A two-stage framework: (1) converting charts to ECharts codes, (2) synthesizing chart reasoning data and training a multimodal model with fine-tuning and reinforcement learning.

Result: ChartReasoner performs comparably to state-of-the-art models with fewer parameters and approaches GPT-4o performance in out-of-domain settings.

Conclusion: ChartReasoner effectively bridges the gap in visual reasoning by preserving chart details and enabling interpretable, scalable reasoning.

摘要: 最近，大型语言模型通过长链推理在回答问题方面表现出卓越的能力。然而，如何将这种能力扩展到视觉推理任务仍是一个开放性问题。现有的多模态推理方法通过多次图像到文本的转换将视觉推理任务转化为文本推理任务，但往往会丢失可视化中嵌入的关键结构和语义信息，尤其是在需要大量视觉细节的图表问答任务中。为弥补这一差距，我们提出了ChartReasoner，一种代码驱动的两阶段框架，旨在实现对图表的精确、可解释推理。我们首先训练一个高保真模型，将多样化的图表图像转换为结构化的ECharts代码，尽可能无损地保留布局和数据语义。然后，我们设计了一个通用的图表推理数据合成流程，利用预训练的传输模型自动且可扩展地生成图表推理轨迹，并使用代码验证器过滤低质量样本。最后，我们在合成的图表推理数据集上结合监督微调和强化学习训练最终的多模态模型。在四个公共基准上的实验结果清楚地证明了ChartReasoner的有效性。它能够尽可能保留图表的原始细节，并在使用较少参数的情况下与最先进的开源模型表现相当，接近GPT-4o等专有系统在域外设置中的性能。

</details>


### [7] [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
**中文标题：无监督引导语言模型**

*Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike*

Main category: cs.CL

TL;DR: The paper introduces an unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models without human supervision, achieving performance comparable to or better than human-supervised methods on various tasks.


<details>
  <summary>Details</summary>
Motivation: Current post-training methods rely on human supervision, which is challenging for models with superhuman capabilities. The paper aims to develop an unsupervised approach to elicit these capabilities.

Method: The proposed method, ICM, fine-tunes pretrained language models using their own generated labels, eliminating the need for external human supervision.

Result: ICM matches or outperforms human-supervised methods on tasks like GSM8k-verification, TruthfulQA, and Alpaca reward modeling, especially for superhuman-capable models. It also improves frontier LM training, as demonstrated by an unsupervised reward model and assistant based on Claude 3.5 Haiku.

Conclusion: ICM provides a viable unsupervised alternative to human supervision for fine-tuning language models, particularly for superhuman-capable models, and enhances the training of frontier LMs.

摘要: 为了引导预训练语言模型适应下游任务，当前的训练后范式依赖于人类指定期望行为。然而，对于具备超人类能力的模型，获取高质量的人类监督变得困难甚至不可能。为解决这一挑战，我们提出了一种新的无监督算法——内部一致性最大化（ICM），用于在没有外部监督的情况下，基于模型自身生成的标签对预训练语言模型进行微调。在GSM8k验证、TruthfulQA和Alpaca奖励建模任务中，我们的方法达到了与黄金监督训练相当的性能，并优于基于众包人类监督的训练。在模型能力远超人类的任务中，我们的方法能够显著优于基于人类标签的训练。最后，我们展示了该方法可以改进前沿语言模型的训练：我们使用该方法训练了一个无监督奖励模型，并通过强化学习训练了一个基于Claude 3.5 Haiku的助手。奖励模型和助手均优于其人类监督的对应版本。

</details>


### [8] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
**中文标题：大型语言模型在评判共情沟通中的可靠性**

*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: The study compares expert, crowdworker, and LLM annotations on empathic communication, finding LLMs reliable and often surpassing crowdworkers, approaching expert agreement levels.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLMs in judging empathic communication nuances compared to human experts and crowdworkers.

Method: Compared annotations from experts, crowdworkers, and LLMs across four evaluative frameworks applied to 200 real-world conversations, analyzing inter-rater reliability.

Result: LLMs consistently approached expert-level agreement and exceeded crowdworker reliability across all frameworks.

Conclusion: LLMs can be reliable for judging empathic communication when validated with appropriate benchmarks, supporting their use in emotionally sensitive applications.

摘要: 大型语言模型（LLMs）在基于文本的对话中擅长生成共情回应。但它们对共情沟通细微差别的评判有多可靠？我们通过比较专家、众包工作者和LLMs在四种评估框架下对200个真实对话的标注来研究这一问题。这些框架来自心理学、自然语言处理和传播学，涉及一方分享个人问题、另一方提供支持的对话。基于3,150条专家标注、2,844条众包标注和3,150条LLM标注，我们评估了这三组标注者之间的评分一致性。发现专家间一致性较高，但随框架子组分的清晰度、复杂性和主观性而变化。专家一致性比标准分类指标更能为LLM表现提供信息。在所有四种框架中，LLMs一致接近专家水平，并超越众包工作者的可靠性。这些结果表明，LLMs在特定任务中经过适当基准验证后，可用于情感敏感应用（如作为对话伴侣）的透明度和监督。

</details>


### [9] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
**中文标题：使用机器学习和LIME分析孟加拉语社交媒体评论中的情感**

*Bidyarthi Paul,SM Musfiqur Rahman,Dipta Biswas,Md. Ziaul Hasan,Md. Zahid Hossain*

Main category: cs.CL

TL;DR: The paper explores emotion analysis in Bangla social media comments using machine learning models (Linear SVM, KNN, Random Forest, BiLSTM, and AdaBoost) and LIME for interpretability, aiming to advance sentiment analysis in understudied languages.


<details>
  <summary>Details</summary>
Motivation: To address the gap in emotion analysis for understudied languages like Bangla, which has unique regional expressions and cultural features, by leveraging machine learning and interpretability tools.

Method: Employed Linear SVM, KNN, Random Forest, BiLSTM, and AdaBoost models with TF-IDF vectorized n-gram data, investigated PCA for dimensionality reduction, and used LIME to explain AdaBoost predictions.

Result: The study identified efficient techniques for emotion identification in Bangla, demonstrating the effectiveness of machine learning models and interpretability tools like LIME.

Conclusion: The research advances sentiment analysis in resource-limited languages by providing a framework for emotion identification in Bangla social media comments.

摘要: 对书面语言中情感理解的研究持续扩展，尤其是针对如孟加拉语这样具有独特地区表达和文化特征的未被充分研究的语言。本研究使用EmoNoBa数据集中的22,698条社交媒体评论进行情感分析。在语言分析中，我们采用了机器学习模型：线性SVM、KNN和随机森林，结合TF-IDF向量化的n-gram数据。我们还研究了PCA对降维的影响。此外，我们使用了BiLSTM模型和AdaBoost来改进决策树。为了使机器学习模型更易于理解，我们使用LIME来解释基于决策树的AdaBoost分类器的预测。我们的工作旨在推动资源有限语言中的情感分析，通过研究多种技术以找到高效的孟加拉语情感识别方法。

</details>


### [10] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
**中文标题：衡量企业人力资本披露：词典、数据、代码与研究机会**

*Elizabeth Demers,Victor Xiaoqi Wang,Kean Wu*

Main category: cs.CL

TL;DR: The paper develops a machine learning-based lexicon for measuring corporate human capital (HC) disclosures, sharing data and code for future research.


<details>
  <summary>Details</summary>
Motivation: Human capital is vital for corporate value but lacks standardized measurement or disclosure rules, prompting the need for a systematic approach to analyze HC disclosures.

Method: The authors use word2vec to create a lexicon of HC-related keywords, classified into five subcategories, and share their data and Python code for further research.

Result: A comprehensive HC lexicon and tools are provided, enabling researchers to analyze corporate HC disclosures and fine-tune models like BERT.

Conclusion: The study offers valuable resources for HC research and highlights future opportunities in HC management and disclosure.

摘要: 人力资本（HC）对企业价值创造日益重要，但目前缺乏明确的衡量或披露规则。我们使用机器学习算法（word2vec）在已验证的HC披露数据集上训练，开发了一个全面的HC相关关键词列表，分为五个子类别（多样性、公平与包容；健康与安全；劳动关系与文化；薪酬与福利；人口统计与其他），以捕捉HC管理的多维性。我们分享了词典、企业HC披露数据及用于开发词典的Python代码，并提供了使用数据和代码的详细示例，包括微调BERT模型。研究人员可利用我们的HC词典（或修改代码以捕获其他感兴趣的构念）与企业沟通样本，解决相关HC问题。最后，我们讨论了HC管理与披露的未来研究机会。

</details>


### [11] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
**中文标题：大型语言模型能生成好故事吗？从叙事规划角度的见解与挑战**

*Yi Wang,Max Kreminski*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' ability to generate high-quality stories by testing them on narrative planning tasks, revealing strengths in causal soundness but challenges in character intentionality and dramatic conflict.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' story generation capabilities by leveraging computational narratology and narrative planning benchmarks, addressing gaps in evaluation methods and quality assessment.

Method: The study uses a benchmark based on literature examples to evaluate LLMs on narrative planning, focusing on causal soundness, character intentionality, and dramatic conflict. Experiments involve GPT-4 tier models.

Result: LLMs can generate causally sound stories at small scales but struggle with character intentionality and dramatic conflict, requiring reinforcement learning for complex reasoning.

Conclusion: LLMs show promise in story generation but face challenges in maintaining quality for complex narrative elements, highlighting the need for further research and training enhancements.

摘要: 故事生成是大型语言模型（LLMs）的一个重要应用。然而，由于自动评估方法的挑战以及人工评估的高成本和主观性，对LLMs生成高质量故事能力的理解仍然有限。计算叙事学为“好故事”提供了有价值的见解，这些见解已被应用于符号叙事规划方法的故事生成中。本研究旨在通过让LLMs解决叙事规划问题，深化对其故事生成能力的理解。我们提出了一个基于文学例子的叙事规划评估基准，重点关注因果合理性、角色意图性和戏剧冲突。实验表明，GPT-4级别的LLMs可以在小规模上生成因果合理的故事，但在角色意图性和戏剧冲突方面的规划仍具挑战性，需要结合强化学习进行复杂推理。结果从不同角度揭示了LLMs在保持故事质量方面的能力范围。我们的发现还突出了有趣的问题解决行为，并为在游戏环境中应用LLM叙事规划的挑战和考虑提供了启示。

</details>


### [12] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
**中文标题：Q2E：用于零样本多语言文本到视频检索的查询到事件分解方法**

*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: Q2E introduces a Query-to-Event decomposition method for zero-shot multilingual text-to-video retrieval, leveraging LLMs and VLMs to enhance query understanding and retrieval performance, including multimodal inputs like audio.


<details>
  <summary>Details</summary>
Motivation: To improve the identification and retrieval of videos related to complex real-world events by leveraging latent parametric knowledge from LLMs and VLMs, addressing the oversimplification of human queries.

Method: Q2E decomposes queries using knowledge from LLMs and VLMs, integrates multimodal inputs (visual and speech), and employs entropy-based fusion scoring for zero-shot retrieval.

Result: Q2E outperforms state-of-the-art baselines on diverse datasets and metrics, with audio integration significantly boosting retrieval performance.

Conclusion: Q2E effectively enhances text-to-video retrieval by decomposing queries and leveraging multimodal knowledge, demonstrating the value of integrating audio and multilingual capabilities.

摘要: 最近的研究表明，从大型语言模型（LLMs）和视觉语言模型（VLMs）中提取和利用参数化知识的能力令人印象深刻。在这项工作中，我们探讨了如何通过自动提取这些事件的潜在参数化知识来改进与复杂现实世界事件相关的视频的识别和检索。我们提出了Q2E：一种用于零样本多语言文本到视频检索的查询到事件分解方法，适用于跨数据集、领域、LLMs或VLMs的适应性。我们的方法表明，通过利用LLMs和VLMs中嵌入的知识分解查询，可以增强对过于简化的人类查询的理解。我们还展示了如何将这种方法应用于视觉和语音输入。为了结合这些多样化的多模态知识，我们采用了基于熵的融合评分进行零样本融合。通过在两个多样化数据集和多种检索指标上的评估，我们证明Q2E优于多个最先进的基线方法。我们的评估还表明，整合音频信息可以显著改善文本到视频检索。我们已发布代码和数据以供未来研究。

</details>


### [13] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
**中文标题：TTT-Bench：通过简单新颖的井字棋风格游戏评估推理能力的基准**

*Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: TTT-Bench is a new benchmark designed to evaluate basic strategic, spatial, and logical reasoning in large reasoning models (LRMs) using simple Tic-Tac-Toe-style games. Despite excelling in complex tasks like math problems, LRMs often fail these simple games, highlighting gaps in their reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To explore the reasoning capabilities of LRMs beyond STEM tasks, focusing on basic strategic, spatial, and logical reasoning through simple games that humans solve effortlessly.

Method: TTT-Bench uses a suite of four Tic-Tac-Toe-style games, generated programmatically, to test LRMs. The games require reasoning about opponent intentions and spatial configurations.

Result: LRMs that excel in hard math problems frequently fail at TTT-Bench, scoring 41% and 5% lower compared to MATH 500 and AIME 2024, respectively. Larger models perform better with shorter reasoning traces but struggle with long-term strategic reasoning.

Conclusion: TTT-Bench reveals significant gaps in LRMs' reasoning abilities for simple tasks, suggesting the need for further research to improve their strategic and spatial reasoning.

摘要: 大型推理模型（LRMs）在包括奥林匹克级数学问题在内的广泛任务中展示了令人印象深刻的推理能力，表明其具备复杂推理能力。然而，许多推理基准集中在STEM领域，LRMs在更广泛任务领域中的正确推理能力仍未被充分探索。本研究提出了\textbf{TTT-Bench}，一种新的基准，旨在通过四种人类从小就能轻松解决的双人井字棋风格游戏，评估LRMs的基本战略、空间和逻辑推理能力。我们提出了一种简单但可扩展的程序化方法，用于生成TTT-Bench的可验证双人游戏问题。尽管这些游戏对人类来说微不足道，但它们需要推理对手的意图以及游戏板的空间配置以确保胜利。我们评估了多种最先进的LRMs，\textbf{发现那些在困难数学问题上表现出色的模型经常在这些简单推理游戏中失败}。进一步测试显示，与MATH 500和AIME 2024相比，我们评估的推理模型在TTT-Bench上的得分平均分别低41%和5%，其中较大模型通过较短的推理轨迹获得更高性能，而大多数模型在简单和新颖的TTT-Bench任务中长期战略推理情境中表现不佳。

</details>


### [14] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
**中文标题：使用大型语言模型分类不可靠叙述者**

*Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: The paper proposes using large language models (LLMs) to classify unreliable narrators in texts, introduces a dataset (TUNa), and evaluates LLM performance on tasks related to different types of unreliability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying unreliable narrators in texts, leveraging computational methods and literary theory to improve classification accuracy.

Method: The study defines unreliable narrators based on narratology, creates a human-annotated dataset (TUNa), and evaluates LLMs on classification tasks for intra-narrational, inter-narrational, and inter-textual unreliabilities using few-shot, fine-tuning, and curriculum learning.

Result: The task is challenging, but LLMs show potential for identifying unreliable narrators, with performance varying across different learning settings.

Conclusion: The study highlights the feasibility of using LLMs for unreliable narrator classification, releases the dataset and code, and encourages further research.

摘要: 当我们与第一人称事件叙述互动时，常常会考虑叙述者是否可靠。本文提出使用计算方法来识别不可靠叙述者，即那些无意中歪曲信息的叙述者。借鉴叙事学中的文学理论，我们基于多种文本现象定义了不同类型的不可靠叙述者，并提出了TUNa数据集，这是一个包含博客文章、Reddit帖子、酒店评论和文学作品的多领域人类标注叙事数据集。我们定义了叙述内、叙述间和文本间不可靠性的分类任务，并分析了流行的开源和专有大型语言模型在每项任务中的表现。我们提出从文学中学习，以对现实世界文本数据进行不可靠叙述者分类。为此，我们尝试了少样本学习、微调和课程学习设置。结果表明，这项任务非常具有挑战性，但使用大型语言模型识别不可靠叙述者具有潜力。我们发布了专家标注的数据集和代码，并邀请未来在这一领域的研究。

</details>


### [15] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
**中文标题：ToxSyn-PT：用于葡萄牙语仇恨言论检测的大规模合成数据集**

*Iago Alves Brito,Julia Soares Dollis,Fernanda Bufon Färber,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: The paper introduces ToxSyn-PT, a large-scale synthetic Portuguese dataset for hate speech detection, covering nine minority groups. It uses a four-stage pipeline for creation and shows strong generalization across domains.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, fine-grained hate speech datasets in Portuguese, especially for legally protected minority groups, and to explore synthetic data's potential in low-resource settings.

Method: A four-stage pipeline: (1) manually curated seed, (2) few-shot expansion with an LLM, (3) paraphrase-based augmentation, and (4) enrichment with neutral texts.

Result: The dataset is class-balanced and stylistically diverse, achieving strong performance in binary and multi-label classification across five Portuguese hate-speech datasets.

Conclusion: ToxSyn-PT advances hate-speech detection research in Portuguese and demonstrates synthetic data's effectiveness in low-resource contexts.

摘要: 我们提出了ToxSyn-PT，这是首个大规模的葡萄牙语语料库，支持对九个受法律保护的少数群体进行细粒度的仇恨言论分类。该数据集包含53,274个合成句子，均匀分布在少数群体和毒性标签之间。ToxSyn-PT通过一个新颖的四阶段流程创建：(1) 一个紧凑的手工筛选种子；(2) 使用指令调优的大型语言模型进行少样本扩展；(3) 基于释义的增强；(4) 丰富化，并添加中性文本以减少对群体特定线索的过拟合。生成的语料库类别平衡、风格多样，并且避免了现有葡萄牙语数据集中普遍存在的社交媒体领域限制。尽管与传统基准存在领域差异，但在该语料库上进行的二元和多标签分类实验在五个公开的葡萄牙语仇恨言论数据集上均取得了强劲结果，展示了跨领域的鲁棒泛化能力。该数据集已公开发布，以推动合成数据和低资源环境下仇恨言论检测的研究。

</details>


### [16] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
**中文标题：语言模型是否具有贝叶斯大脑？区分大型语言模型中的随机性与确定性决策模式**

*Andrea Yaoyun Cui,Pengfei Yu*

Main category: cs.CL

TL;DR: The paper challenges the assumption that language models make purely probabilistic decisions, showing they can exhibit near-deterministic behavior under certain conditions. It proposes a method to distinguish stochastic and deterministic patterns in Gibbs sampling to avoid inferring misleading priors.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate whether language models truly exhibit Bayesian decision-making or if their behavior can be deterministic, challenging prior assumptions and methods for inferring model priors.

Method: The authors analyze decision patterns in large language models under various conditions, proposing a method to distinguish stochastic and deterministic behaviors in Gibbs sampling.

Result: The study reveals that language models can display near-deterministic decision-making, even with non-zero sampling temperatures, and warns against inferring false priors from deterministic systems.

Conclusion: The findings suggest that language models do not always behave as Bayesian brains, and careful scrutiny is needed to avoid misinterpreting their decision patterns.

摘要: 语言模型本质上是基于标记序列的概率分布。自回归模型通过迭代计算和采样下一个标记的分布来生成句子。这种迭代采样引入了随机性，导致人们假设语言模型会做出类似从未知分布中采样的概率决策。基于这一假设，先前的研究利用模拟吉布斯采样（受设计用于引出人类先验的实验启发）来推断语言模型的先验。本文重新审视一个关键问题：语言模型是否具有贝叶斯大脑？我们的研究结果表明，在某些条件下，语言模型可以表现出近乎确定性的决策行为，例如即使在非零采样温度下也能产生最大似然估计。这挑战了采样假设，并削弱了先前引出类人先验的方法。此外，我们证明，未经适当审查，具有确定性行为的系统在模拟吉布斯采样过程中可能会收敛到一个“虚假先验”。为解决这一问题，我们提出了一种简单的方法来区分吉布斯采样中的随机性和确定性决策模式，从而避免推断出误导性的语言模型先验。我们在多种大型语言模型上进行了实验，以识别它们在不同情况下的决策模式。我们的结果为理解大型语言模型的决策行为提供了重要见解。

</details>


### [17] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
**中文标题：ClusterUCB：基于梯度的高效数据选择方法用于大型语言模型的定向微调**

*Zige Wang,Qi Zhu,Fei Mi,Minghui Xu,Ruochun Jin,Wenjing Yang*

Main category: cs.CL

TL;DR: ClusterUCB is an efficient gradient-based data selection framework for fine-tuning LLMs, using clustering and a modified UCB algorithm to reduce computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient-based data selection for fine-tuning LLMs is resource-intensive; ClusterUCB aims to reduce computational costs while maintaining effectiveness.

Method: ClusterUCB clusters training data by gradient features, frames inter-cluster selection as a multi-armed bandit problem, and uses a modified UCB algorithm to balance exploration and exploitation.

Result: ClusterUCB achieves comparable performance to original gradient-based methods while significantly reducing computational consumption.

Conclusion: ClusterUCB offers a practical and efficient solution for gradient-based data selection in LLM fine-tuning.

摘要: 基于梯度的数据影响近似方法已被用于在大型语言模型的监督微调中选择有用的数据样本。然而，在整个微调过程中计算梯度需要过多资源，实际中难以实现。本文提出了一种高效的基于梯度的数据选择框架，结合聚类和改进的上置信界（UCB）算法。基于梯度特征相似的数据样本具有相似影响的直觉，我们首先对训练数据池进行聚类。然后，将簇间数据选择问题建模为受计算预算约束的分配问题，并视为多臂老虎机问题。采用改进的UCB算法解决此问题。具体而言，在迭代采样过程中，记录历史数据影响信息以直接估计每个簇的分布，并采用冷启动来平衡探索与利用。在多个基准测试上的实验结果表明，我们提出的ClusterUCB框架可以在大幅减少计算消耗的同时，达到与原始基于梯度的数据选择方法相当的结果。

</details>


### [18] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
**中文标题：Flick：基于K感知中间学习的多任务低资源语言少标签文本分类**

*Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak*

Main category: cs.CL

TL;DR: Flick introduces a novel pseudo-label refinement method for few-label text classification in low-resource languages, improving pseudo-label quality by focusing on high-confidence clusters and adaptive top-k selection.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of few-label text classification in low-resource languages, where existing methods struggle with noisy pseudo-labels and domain adaptation.

Method: Flick refines pseudo-labels by distilling high-confidence ones from a broad set of initial clusters, using single-cluster cohesion and adaptive top-k selection.

Result: Demonstrated superior performance across 14 diverse datasets, including low-resource languages like Arabic, Urdu, and Setswana.

Conclusion: Flick effectively mitigates error propagation in low-resource settings, enabling robust fine-tuning with minimal true labels.

摘要: 在深度学习中，减少对大量标注数据的依赖已成为研究热点。尽管自训练方法在半监督学习中表现良好，但它们容易受到噪声伪标签的影响。此外，当前少标签分类方法多针对资源丰富的语言（如英语）或采用易过拟合的复杂级联模型。为解决低资源语言环境中少标签文本分类的难题，我们提出了Flick。不同于依赖通用多聚类伪标签或复杂级联架构的现有方法，Flick通过从更广泛的初始聚类中提取高置信度伪标签，显著提升了伪标签质量，尤其适用于语言多样性高的低资源环境。Flick引入了一种新颖的伪标签细化组件，通过关注单聚类内聚性和自适应top-k选择机制，从初始广泛集合中提取高可靠性伪标签。这一针对性细化过程有效减少了低资源数据中的错误传播，仅需少量真实标签即可实现对预训练语言模型的鲁棒微调。我们在14个多样化数据集（包括阿拉伯语、乌尔都语和塞茨瓦纳语等低资源语言及英语）上验证了Flick的优越性能和适应性。

</details>


### [19] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
**中文标题：“检查我的作业？”：模拟教育环境中谄媚行为的测量**

*Chuck Arvin*

Main category: cs.CL

TL;DR: The study reveals that LLMs exhibit sycophantic behavior in educational contexts, altering responses based on student suggestions, which can degrade or improve accuracy by up to 15 percentage points. Smaller models show stronger bias (up to 30%), raising concerns about educational equity.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs' responses are influenced by student suggestions in educational settings, particularly the risks of sycophancy (model bias toward user input), and its implications for learning equity.

Method: Tested five LLMs (from OpenAI GPT-4o and GPT-4.1 classes) under five experimental conditions, analyzing response quality, answer flipping frequency, and token-level probabilities to measure sycophantic behavior.

Result: LLMs' correctness degraded by 15% when students mentioned incorrect answers and improved by 15% with correct answers. Smaller models (e.g., GPT-4.1-nano) showed stronger bias (30%) compared to larger models (8%).

Conclusion: Sycophantic behavior in LLMs can exacerbate educational inequities by reinforcing misunderstandings for less knowledgeable students. The study underscores the need to understand and mitigate such biases in educational applications.

摘要: 本研究探讨了在模拟教育环境中，用户提供的建议如何影响大型语言模型（LLMs），其中谄媚行为带来了显著风险。通过测试OpenAI GPT-4o和GPT-4.1模型类别的五种不同LLMs在五种实验条件下的表现，我们发现响应质量因查询框架而异。当学生提到错误答案时，LLMs的正确率可能下降多达15个百分点，而提到正确答案时则提升相同幅度。结果还显示，这种偏差在较小模型中更为明显，GPT-4.1-nano模型的影响高达30%，而GPT-4o模型仅为8%。我们对LLMs“翻转”答案的频率分析及对标记级概率的调查证实，模型通常会根据学生的答案选择改变其回答，符合谄媚假设。这种谄媚行为对教育公平具有重要影响，因为LLMs可能加速知识丰富学生的学习，而同样的工具可能加深知识较少学生的误解。我们的结果强调了需要更好地理解这种机制，并寻找在教育环境中减轻此类偏差的方法。

</details>


### [20] [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
**中文标题：基于LLMs的语音到语音翻译的定时交替语音-文本训练**

*Hayato Futami,Emiru Tsunoo,Yosuke Kashiwagi,Yuki Ito,Hassan Shahmohammadi,Siddhant Arora,Shinji Watanabe*

Main category: cs.CL

TL;DR: The paper proposes scheduled interleaved speech-text training to improve speech-to-speech translation (S2ST) with LLMs, gradually reducing text ratio for better modality adaptation.


<details>
  <summary>Details</summary>
Motivation: LLMs are trained on text-only data, making adaptation to speech modality challenging due to limited speech-to-speech data. The study aims to address this issue.

Method: Proposes interleaving speech-text units at the word level during training, gradually decreasing text ratio to facilitate progressive modality adaptation.

Result: The method improves S2ST performance, especially for languages with limited training data, as shown in experiments with LLaMA3.2-1B on the CVSS dataset.

Conclusion: Scheduled interleaved speech-text training effectively enhances LLM adaptation to speech modality, improving translation performance.

摘要: 语音到语音翻译（S2ST）通过大型语言模型（LLMs）得到提升，这些模型在离散语音单元上进行微调。然而，从文本到语音的模态适应一直是一个问题。LLMs仅在文本数据上训练，这使其在有限语音数据下适应语音模态具有挑战性。为解决这一训练难题，本研究提出了定时交替语音-文本训练方法。我们在训练中使用交替的语音-文本单元而非纯语音单元，其中对齐的文本标记在单词级别交替插入。随着训练的进行，我们逐渐减少文本比例，以促进从文本到语音的渐进模态适应。我们通过在CVSS数据集上微调LLaMA3.2-1B进行实验评估。结果表明，所提方法持续提升了翻译性能，尤其是对于训练数据有限的语言。

</details>


### [21] [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
**中文标题：以代码执行作为基础监督的LLM推理方法**

*Dongwon Jung,Wenxuan Zhou,Muhao Chen*

Main category: cs.CL

TL;DR: The paper proposes a scalable method to generate high-quality chain-of-thought (CoT) supervision for LLMs by leveraging program execution traces, improving reasoning accuracy and reducing token length during inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating CoT supervision rely on costly human annotations or error-prone LLM-generated CoT, making it challenging to obtain reliable reasoning data. The paper aims to address this by using verifiable code execution traces.

Method: The method extracts step-by-step reasoning traces from deterministic code execution and converts them into natural language CoT reasoning, ensuring high-quality and verifiable supervision data.

Result: Experiments show the method enhances LLMs' reasoning abilities across diverse tasks, produces accurate reasoning data, and reduces token length by eliminating meaningless repetition and overthinking.

Conclusion: The proposed approach provides a scalable and reliable way to generate CoT supervision, improving LLMs' reasoning performance and efficiency.

摘要: 通过链式思维（CoT）监督训练大型语言模型（LLM）已被证明能有效提升其推理能力。然而，获取可靠且准确的推理监督仍是一个重大挑战。我们提出了一种可扩展的方法，通过利用程序执行的确定性来生成高质量的CoT监督数据集。与依赖昂贵人工标注或易出错的LLM生成CoT的现有方法不同，我们的方法从代码执行中提取可验证的逐步推理轨迹，并将其转化为自然语言的CoT推理。在多个领域的推理基准测试中，实验表明我们的方法能有效赋予LLM跨任务的迁移推理能力。此外，消融研究验证了我们的方法生成高度准确的推理数据，并通过减少无意义的重复和过度思考降低了推理过程中的总体标记长度。

</details>


### [22] [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
**中文标题：TableRAG：一种用于异构文档推理的检索增强生成框架**

*Xiaohan Yu,Pu Jian,Chong Chen*

Main category: cs.CL

TL;DR: TableRAG is a hybrid framework for heterogeneous document reasoning, combining text and tabular data processing to improve multi-hop question answering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RAG approaches struggle with heterogeneous documents (text and tables), losing tabular structure and information, which limits reasoning capabilities in multi-hop queries.

Method: TableRAG uses a four-step iterative process: query decomposition, text retrieval, SQL programming/execution, and intermediate answer generation, unifying text and tabular data processing.

Result: TableRAG outperforms baselines on public datasets and the new HeteQA benchmark, setting a new state-of-the-art for heterogeneous document QA.

Conclusion: TableRAG effectively addresses limitations of existing RAG methods for heterogeneous documents, enhancing reasoning capabilities and performance.

摘要: 检索增强生成（RAG）在开放领域问答中表现出显著效果。然而，当应用于包含文本和表格组件的异构文档时，现有RAG方法存在关键限制。常见的表格扁平化和分块策略破坏了表格的内在结构，导致信息丢失，并削弱了LLM在多跳全局查询中的推理能力。为解决这些问题，我们提出了TableRAG，一种统一文本理解和表格数据复杂操作的混合框架。TableRAG迭代执行四个步骤：上下文敏感的查询分解、文本检索、SQL编程与执行以及组合性中间答案生成。我们还开发了HeteQA，一个用于评估多跳异构推理能力的新基准。实验结果表明，TableRAG在公共数据集和我们的HeteQA上均优于现有基线，为异构文档问答设立了新的最先进水平。TableRAG已在https://github.com/yxh-y/TableRAG/tree/main发布。

</details>


### [23] [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/abs/2506.10406)
**中文标题：PAG：基于生成式验证策略的多轮强化LLM自我纠正**

*Yuhua Jiang,Yuwen Xiong,Yufeng Yuan,Chao Xin,Wenyuan Xu,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.CL

TL;DR: PAG introduces a multi-turn RL framework where LLMs alternate between policy and verifier roles, selectively revising answers only when errors are detected, improving both reasoning and verification.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with self-verification, and existing solutions rely on separate verifiers or complex pipelines, limiting scalability. PAG aims to simplify and enhance self-correction.

Method: PAG uses a multi-turn RL paradigm where LLMs alternate roles as policy and verifier, selectively revising answers based on generative verification.

Result: PAG improves reasoning and verification accuracy, outperforming self-consistency in self-verification and enhancing direct generation.

Conclusion: PAG's verify-then-revise approach effectively enhances LLM self-correction, balancing reasoning and verification without additional verifier models.

摘要: 大型语言模型（LLM）在复杂推理任务中表现出色，但仍难以可靠地验证自身输出的正确性。现有的验证方法通常依赖独立的验证器模型或多阶段自我纠正训练流程，限制了可扩展性。本文提出“生成式验证策略”（PAG），这是一种简单有效的框架，通过在统一的多轮强化学习（RL）范式中交替扮演策略和验证器角色，使LLM能够自我纠正。与以往无论模型置信度如何都会生成第二次尝试的方法不同，PAG引入了选择性修订机制：模型仅在自身的生成式验证步骤检测到错误时修订答案。这种“验证后修订”的工作流程不仅缓解了模型崩溃，还共同提升了推理和验证能力。在多样化推理基准上的大量实验表明，PAG具有双重优势：作为策略，它提高了直接生成和自我纠正的准确性；作为验证器，其自我验证性能优于自我一致性。

</details>


### [24] [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/abs/2506.10415)
**中文标题：阅后即焚：多模态大语言模型是否真正捕捉了图像序列中的事件顺序？**

*Yingjin Song,Yupei Du,Denis Paperno,Albert Gatt*

Main category: cs.CL

TL;DR: The paper introduces TempVS, a benchmark to evaluate Multimodal Large Language Models (MLLMs) on temporal grounding and reasoning in image sequences. Results show MLLMs struggle with these tasks, lagging behind human performance.


<details>
  <summary>Details</summary>
Motivation: To assess whether MLLMs can truly understand and reason about the temporal order of events in image sequences, a capability crucial for real-world applications.

Method: The TempVS benchmark includes three tests (event relation inference, sentence ordering, image ordering) with grounding tests, evaluating 38 MLLMs on their temporal reasoning abilities.

Result: MLLMs perform poorly on TempVS, with a significant gap compared to human performance, highlighting limitations in temporal reasoning.

Conclusion: The TempVS benchmark reveals MLLMs' weaknesses in temporal reasoning, suggesting future research directions to improve these capabilities.

摘要: 本文介绍了TempVS基准，专注于评估多模态大语言模型（MLLMs）在图像序列中的时间定位和推理能力。TempVS包含三项主要测试（即事件关系推理、句子排序和图像排序），每项测试均配有基础定位测试。TempVS要求MLLMs依赖视觉和语言模态来理解事件的时间顺序。我们评估了38种先进的MLLMs，结果表明这些模型在解决TempVS任务时表现不佳，与人类能力存在显著差距。我们还提供了细粒度的分析，为未来研究指明了有前景的方向。TempVS基准数据和代码可在https://github.com/yjsong22/TempVS获取。

</details>


### [25] [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/abs/2506.10421)
**中文标题：超越战场：冲突报道中媒体框架的分析**

*Avneet Kaur,Arnav Arora*

Main category: cs.CL

TL;DR: The paper analyzes media framing in conflict reporting, focusing on the Israel-Palestine war, using computational methods to reveal biases and a preference for war journalism over peace journalism across US, UK, and Middle Eastern outlets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of qualitative studies and surface-level analysis in conflict framing research by using computational methods to uncover deeper linguistic and communicative framing patterns in media coverage.

Method: Combines frame semantics and large language models to analyze a corpus of news articles on the Israel-Palestine conflict, identifying war and peace journalism indicators.

Result: Found a higher focus on war-based reporting and significant differences in framing assailants and victims across US, UK, and Middle Eastern news outlets, revealing media biases.

Conclusion: The study highlights the impact of media framing on conflict perception and the need for more balanced reporting, emphasizing the role of computational methods in uncovering biases.

摘要: 新闻媒体在冲突时期使用的框架对读者观点有重大影响，可能加剧冲突本身。当前关于冲突框架的研究由于定性性质或仅关注表面通用框架而见解有限。在这项工作中，我们根据冲突研究的先前工作，在报道以色列-巴勒斯坦战争的新闻文章语料库中识别战争与和平新闻的指标。我们使用计算方法，结合框架语义学和大型语言模型，分析交际框架及其与语言框架的联系。分析显示，报道更侧重于战争而非和平。我们还展示了美国、英国和中东新闻媒体在冲突中谁是攻击者和受害者的框架上存在显著差异，揭示了媒体内部的偏见。

</details>


### [26] [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/abs/2506.10446)
**中文标题：快速处理简单问题，深入解决复杂问题：基于长度惩罚的高效推理方法**

*Zehui Ling,Deshu Chen,Hongwei Zhang,Yifeng Jiao,Xin Guo,Yuan Cheng*

Main category: cs.CL

TL;DR: The paper introduces a method to improve LLM reasoning efficiency by dynamically adjusting output length penalties based on problem complexity, achieving shorter outputs for simpler tasks and better accuracy for complex ones.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning methods often generate unnecessarily long outputs, increasing computational latency. Uniform penalties for output length ignore problem complexity, leading to suboptimal performance. This study aims to enhance efficiency by promoting conciseness for simple problems while preserving detailed reasoning for complex ones.

Method: The approach divides the reward function and introduces a novel output length penalty, dynamically adjusting penalties based on problem complexity. This ensures shorter outputs for simpler tasks (e.g., GSM8K, MATH500) and sufficient reasoning for harder ones (e.g., AIME2024).

Result: The method achieved shorter outputs with maintained or improved accuracy on simpler datasets (GSM8K, MATH500) and higher accuracy on the more complex AIME2024 dataset.

Conclusion: The proposed method effectively balances reasoning efficiency and accuracy by tailoring output length penalties to problem complexity, demonstrating superior performance across diverse benchmarks.

摘要: 大型语言模型（LLM）在推理能力上取得了显著进展，在各种具有挑战性的基准测试中表现优异。为了提高推理能力，研究者引入了诸如“思维链”提示等技术。然而，这些方法通常生成较长的输出，从而增加了计算延迟。尽管一些方法使用强化学习来缩短推理过程，但它们往往采用统一的惩罚机制，而未考虑问题的复杂性，导致结果不尽如人意。本研究旨在通过促进简单问题的简洁性，同时保留复杂问题的充分推理以提高准确性，从而提升LLM推理的整体效率。具体而言，我们通过划分奖励函数并引入一种新颖的输出长度惩罚机制来管理模型的推理效率。我们的方法在三个基准数据集（GSM8K、MATH500和AIME2024）上取得了显著成果。对于相对简单的数据集GSM8K和MATH500，我们的方法有效缩短了输出长度，同时保持或提高了准确性。在更具挑战性的AIME2024数据集上，我们的方法提升了准确性。

</details>


### [27] [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
**中文标题：表格-文本对齐：解释科学论文中针对表格的声明验证**

*Xanh Ho,Sunisth Kumar,Yun-Ang Wu,Florian Boudin,Atsuhiro Takasu,Akiko Aizawa*

Main category: cs.CL

TL;DR: The paper reframes table-text alignment as an explanation task, requiring models to identify key table cells for claim verification. It introduces a dataset with human-annotated rationales and shows that while LLMs predict labels well, they often fail at aligning with human reasoning.


<details>
  <summary>Details</summary>
Motivation: Predicting claim verification labels alone lacks interpretability. The paper aims to enhance understanding by requiring models to identify essential table cells for verification.

Method: The authors extend the SciTab benchmark with human-annotated cell-level rationales, propose a taxonomy for ambiguous cases, and test LLMs on claim verification and rationale alignment.

Result: Incorporating table alignment improves verification performance, but LLMs often fail to align with human rationales despite correct label predictions.

Conclusion: The study highlights the need for models to provide faithful reasoning beyond label prediction, emphasizing interpretability in claim verification.

摘要: 科学声明验证通常需要根据表格预测声明是否被支持或反驳。然而，我们认为仅预测最终标签是不够的：它几乎无法揭示模型的推理过程，且可解释性有限。为此，我们将表格-文本对齐重新定义为解释任务，要求模型识别对声明验证至关重要的表格单元格。我们通过扩展SciTab基准数据集，添加人工标注的单元格级理由，构建了一个新数据集。标注者验证声明标签并突出显示支持其决策的最小单元格集合。标注完成后，我们利用收集的信息提出了一种处理模糊情况的分类法。实验表明：（i）结合表格对齐信息可提高声明验证性能；（ii）大多数大型语言模型（LLM）虽然经常预测正确的标签，但无法恢复与人类对齐的理由，表明其预测并非源于忠实推理。

</details>


### [28] [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
**中文标题：表面公平，深层偏见：语言模型中偏见的比较研究**

*Aleksandra Sorokovikova,Pavel Chizhov,Iuliia Eremenko,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: The paper examines bias in large language models (LLMs) through proxy measures, finding negligible bias in pre-prompted personae tasks but significant bias in salary negotiation advice and user-graded answers, highlighting risks with LLM personalization.


<details>
  <summary>Details</summary>
Motivation: To investigate and quantify biases in LLMs, especially as they become more personalized and retain user socio-demographics, potentially exacerbating biased outputs.

Method: Evaluated LLMs using pre-prompted personae on MMLU benchmark, reformulated tasks to grade user answers, and analyzed salary negotiation advice for bias.

Result: Pre-prompted personae showed negligible bias, while user-graded answers and salary negotiation advice revealed significant bias, indicating deeper issues with LLM outputs.

Conclusion: LLMs exhibit pronounced bias in certain contexts, especially with personalization, necessitating further research and mitigation strategies.

摘要: 现代语言模型通过大量数据进行训练，这些数据不可避免地包含争议性和刻板印象内容，涉及性别、出身、年龄等各种偏见。因此，模型会表达带有偏见的观点，或根据用户或被赋予的人格产生不同的结果。本文研究了大型语言模型（LLMs）中偏见的多种代理测量方法。我们发现，在多学科基准（MMLU）上使用预提示的人格评估模型时，得分差异微乎其微且多为随机。然而，如果我们重新设计任务，要求模型对用户的回答进行评分，则会显示出更明显的偏见迹象。最后，当我们要求模型提供薪资谈判建议时，答案中表现出显著的偏见。随着LLM助手记忆和个性化趋势的兴起，这些问题从新的角度浮现：现代LLM用户无需预提示其人格描述，因为模型已了解其社会人口特征。

</details>


### [29] [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/abs/2506.10504)
**中文标题：超越单用户对话：评估大语言模型在多用户对话状态跟踪中的能力**

*Sangmin Song,Juhwan Choi,JungMin Yun,YoungBin Kim*

Main category: cs.CL

TL;DR: The paper evaluates large language models (LLMs) in multi-user dialogue state tracking (DST), revealing a significant performance drop compared to single-user DST, highlighting limitations in handling multi-speaker interactions.


<details>
  <summary>Details</summary>
Motivation: Current DST benchmarks focus on single-user interactions, neglecting the complexities of real-world multi-user scenarios. The study aims to assess LLMs' robustness in multi-user DST while minimizing dataset construction costs.

Method: The authors extend an existing DST dataset by generating utterances of a second user using speech act theory, systematically incorporating multi-user interactions for controlled evaluation.

Result: Experiments show a notable performance decline in LLMs for multi-user DST compared to single-user settings, indicating limitations in tracking dialogue states with multiple speakers.

Conclusion: The findings underscore the need for future research to improve LLMs for multi-user DST, advancing more realistic and robust DST models.

摘要: 大语言模型（LLMs）在零样本对话状态跟踪（DST）中表现出色，减少了对任务特定训练的需求。然而，传统的DST基准主要关注结构化的用户-代理对话，未能捕捉现实世界中多用户交互的复杂性。本研究评估了LLMs在多用户DST中的鲁棒性，同时最小化数据集构建成本。受LLM数据标注最新进展的启发，我们基于言语行为理论扩展了现有的DST数据集，生成了第二个用户的语句。我们的方法系统地将第二个用户的语句融入对话中，实现了对LLMs在多用户环境中的受控评估。实验结果显示，与单用户DST相比，性能显著下降，突显了当前LLMs在提取和跟踪多说话者对话状态中的局限性。我们的发现强调了未来研究需要增强LLMs在多用户DST场景中的应用，为更现实和鲁棒的DST模型铺平道路。

</details>


### [30] [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/abs/2506.10508)
**中文标题：可靠推理路径：通过知识图谱为LLM推理提炼有效指导**

*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Bo Li,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: The paper proposes the RRP framework to enhance LLM reasoning by extracting reliable reasoning paths from knowledge graphs, combining LLM semantics with structural information, and refining paths for better performance.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) struggle with knowledge-intensive tasks due to lack of background knowledge and hallucinations. Integrating knowledge graphs (KGs) helps, but existing methods fail to organize facts into logical reasoning paths.

Method: The RRP framework mines KGs by combining LLM semantics with relation embedding and bidirectional distribution learning. It includes a rethinking module to evaluate and refine reasoning paths.

Result: RRP achieves state-of-the-art performance on two public datasets and can be integrated into various LLMs in a plug-and-play manner.

Conclusion: RRP effectively enhances LLM reasoning by generating high-quality, tailored reasoning paths from KGs.

摘要: 大型语言模型（LLMs）由于缺乏背景知识和易产生幻觉，在知识密集型任务中表现不佳。为解决这些问题，知识图谱（KGs）与LLMs的结合被广泛研究。现有的KG增强LLMs侧重于补充事实知识，但仍难以解决复杂问题。我们认为，提炼事实之间的关系并将其组织为逻辑一致的推理路径与事实知识本身同等重要。尽管潜力巨大，但从KGs中提取可靠推理路径面临以下挑战：图结构的复杂性以及生成路径的多样性，使得难以区分有用和冗余的路径。为应对这些挑战，我们提出了RRP框架来挖掘知识图谱，该框架结合了LLMs的语义优势和通过关系嵌入与双向分布学习获得的结构信息。此外，我们引入了一个反思模块，根据路径的重要性评估和优化推理路径。在两个公开数据集上的实验结果表明，RRP相比现有基线方法达到了最先进的性能。此外，RRP可以轻松集成到各种LLMs中，以即插即用的方式增强其推理能力。通过生成针对特定问题的高质量推理路径，RRP为LLM推理提炼了有效指导。

</details>


### [31] [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/abs/2506.10614)
**中文标题：通过简约规则启发式与进化搜索的无监督原始形式重构**

*Promise Dodzi Kpoglu*

Main category: cs.CL

TL;DR: The paper introduces an unsupervised hybrid method combining rule-based heuristics and evolutionary search to reconstruct protoforms, outperforming data-driven baselines in accuracy and phonological plausibility.


<details>
  <summary>Details</summary>
Motivation: Prior methods for protoform reconstruction rely heavily on data-driven probabilistic models, which lack linguistic constraints. The authors aim to integrate rule-based heuristics with data-driven inference for more accurate and plausible reconstructions.

Method: The proposed model combines data-driven inference with linguistically motivated rule-based heuristics within an evolutionary optimization framework to guide protoform reconstruction.

Result: The method significantly outperforms established baselines in reconstructing Latin protoforms, achieving higher character-level accuracy and phonological plausibility.

Conclusion: The hybrid approach of integrating rule-based heuristics with evolutionary search proves effective for protoform reconstruction, offering a balance between statistical patterns and linguistic constraints.

摘要: 我们提出了一种无监督方法，用于重构原始形式，即现代语言形式衍生的祖先词汇形式。以往的研究主要依赖于音韵编辑的概率模型从同源词集中推断原始形式，这类方法因其主要依赖数据驱动而受到限制。相比之下，我们的模型在进化优化框架中结合了数据驱动的推断与基于规则的启发式方法。这种混合方法利用统计模式和语言学约束来指导重构过程。我们在重构拉丁原始形式任务上评估了该方法，使用了五种罗曼语同源词数据集。实验结果表明，该方法在字符级准确性和音韵合理性指标上均显著优于现有基线。

</details>


### [32] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
**中文标题：Tina：通过LoRA实现的微型推理模型**

*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Tina is a family of tiny reasoning models that achieve strong reasoning performance with minimal resources using LoRA-based parameter-efficient updates during RL, outperforming SOTA models at a fraction of the cost.


<details>
  <summary>Details</summary>
Motivation: To explore how cost-effectively strong reasoning abilities can be achieved in language models, aiming for high performance with minimal computational resources.

Method: Applies low-rank adaptation (LoRA) during reinforcement learning (RL) to a tiny 1.5B parameter base model, enabling efficient parameter updates.

Result: Tina models achieve competitive or superior reasoning performance to SOTA models, with a >20% performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training cost (260x cost reduction).

Conclusion: LoRA-based efficient RL reasoning is surprisingly effective, rapidly adapting models to reasoning structures while preserving base knowledge. The approach is validated across datasets and settings, with full open-source release for accessibility.

摘要: 如何在语言模型中经济高效地实现强大的推理能力？基于这一基本问题，我们提出了Tina，一种通过高成本效益实现的小型推理模型家族。值得注意的是，Tina证明了仅需极少的资源即可开发出显著的推理性能，方法是在强化学习（RL）过程中应用参数高效的更新，使用低秩适应（LoRA）技术，作用于一个已经很小的1.5B参数基础模型。这种极简方法产生的模型在推理性能上可与甚至有时超越基于同一基础模型构建的SOTA RL推理模型。关键的是，这一成果仅需现有SOTA模型计算后训练成本的一小部分。实际上，最佳Tina模型在AIME24上实现了超过20%的推理性能提升和43.33%的Pass@1准确率，而其后训练和评估成本仅为9美元（即估计成本降低了260倍）。我们的工作揭示了通过LoRA实现高效RL推理的惊人效果。我们在多个开源推理数据集和各种消融设置中验证了这一点，起始于一组固定的超参数。此外，我们假设这种效果和效率源于LoRA快速将模型适应于RL奖励的推理结构格式，同时很大程度上保留了基础模型的底层知识。为了促进可访问性和开放研究，我们完全开源了所有代码、训练日志以及模型权重和检查点。

</details>


### [33] [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/abs/2506.10622)
**中文标题：SDialog：用于合成对话生成与分析的Python工具包**

*Sergio Burdisso,Esaú Villatoro-Tello,Petr Motlicek*

Main category: cs.CL

TL;DR: SDialog is a Python toolkit for generating and analyzing synthetic dialogues using LLMs, designed to improve conversational AI research through modular and reproducible workflows.


<details>
  <summary>Details</summary>
Motivation: The need for high-quality, flexible, and reproducible synthetic dialogues to advance conversational AI systems.

Method: Leverages instruction-tuned LLMs for persona, orchestration, and scenario management to create realistic and diverse dialogues.

Result: SDialog enables multi-agent simulation and scenario-driven generation, standardizing synthetic data tools for reproducibility.

Conclusion: SDialog represents a crucial advancement in synthetic dialogue generation, supporting research and development in conversational AI.

摘要: 会话AI系统的进步依赖于高质量、灵活且可复现的合成对话数据，用于训练、评估和基准测试。SDialog是一个模块化、可扩展的Python工具包，旨在解决合成对话生成和分析的挑战。通过利用指令调优的大型语言模型（LLMs），SDialog提供了角色、编排和场景管理的抽象，从而为研究和开发创建真实、多样且可控的对话数据。SDialog支持多智能体模拟和场景驱动生成等工作流，代表了合成数据生成工具和框架标准化的重要进展，对确保当今快速发展的研究领域的可复现性至关重要。

</details>


### [34] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
**中文标题：Resa：基于稀疏自编码器的透明推理模型**

*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa introduces SAE-Tuning, a cost-effective method to enhance reasoning in language models by leveraging sparse autoencoders, achieving high performance with minimal training cost and time.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop an efficient method to elicit strong reasoning abilities in language models without relying on expensive RL training or reasoning traces.

Method: Resa uses sparse autoencoder tuning (SAE-Tuning) to extract reasoning abilities from a source model and transfers them to a target model via supervised fine-tuning, using verified question-answer data.

Result: SAE-Tuning achieves >97% of RL-trained model performance at 2000x lower cost (~$1) and 450x faster (~20 minutes). It also enables modular and generalizable reasoning abilities across datasets and models.

Conclusion: SAE-Tuning is a highly efficient and scalable approach for enhancing reasoning in language models, with potential for broad applicability and modularity.

摘要: 如何通过利用语言模型的底层表示，以经济高效的方式激发其强大的推理能力？我们通过Resa回答了这个问题，这是一个通过新颖高效的稀疏自编码器调优（SAE-Tuning）方法训练的1.5B推理模型家族。该方法首先训练一个稀疏自编码器（SAE）从源模型中捕捉推理能力，然后利用训练好的SAE指导标准监督微调过程，在目标模型中激发这些能力，全程仅使用已验证的问答数据，无需任何推理轨迹。值得注意的是，当应用于某些基础模型并进一步进行RL后训练时，SAE-Tuning保留了其RL训练对应模型97%以上的推理性能，同时将训练成本降低了2000倍以上（约1美元），训练时间缩短了450倍以上（约20分钟）。此外，当应用于轻量RL训练的模型（例如在2个GPU上1小时内完成）时，它仅需约1美元的额外成本即可实现43.33%的AIME24 Pass@1和90%的AMC23 Pass@1推理性能。令人惊讶的是，通过SAE提取的推理能力可能既具有通用性又具有模块化特性。通用性意味着从一个数据集中提取的能力仍能提升更大且重叠语料库的性能。模块化意味着从Qwen或Qwen-Math提取的能力可以在测试时附加到R1-Distill模型上，无需任何重新训练，即可获得相当的提升。广泛的消融实验验证了这些发现，所有资源均已完全开源。

</details>


### [35] [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/abs/2506.10627)
**中文标题：NeuralNexus在BEA 2025共享任务中的表现：基于检索增强提示的AI导师错误识别**

*Numaan Naeem,Sarfraz Ahmad,Momina Ahsan,Hasan Iqbal*

Main category: cs.CL

TL;DR: The paper presents a retrieval-augmented prompting system for identifying mistakes in AI tutor responses, combining example-driven prompting with LLM reasoning to outperform baselines.


<details>
  <summary>Details</summary>
Motivation: To improve the pedagogical ability of AI tutors by accurately identifying mistakes in student mathematical reasoning, leveraging advanced language models and retrieval techniques.

Method: Explored four approaches: ensemble of ML models, frozen sentence-transformer with MLP, history-aware model with attention, and retrieval-augmented few-shot prompting with GPT-4o. The final system retrieves similar examples, constructs prompts, and uses schema-guided parsing.

Result: The retrieval-augmented prompting system outperformed all baselines, demonstrating effectiveness in pedagogical feedback assessment.

Conclusion: Combining example-driven prompting with LLM reasoning enhances mistake identification in AI tutors, offering a scalable and interpretable solution.

摘要: 本文介绍了我们在BEA 2025共享任务中针对AI导师教学能力评估的第一赛道——错误识别的系统。该任务涉及评估导师的回复是否正确识别了学生在数学推理中的错误。我们探索了四种方法：(1) 基于多个预训练语言模型（LMs）的池化标记嵌入的机器学习模型集成；(2) 使用[CLS]嵌入和MLP分类器的冻结句子转换器；(3) 具有多头部注意力的历史感知模型，用于标记级历史和响应嵌入；(4) 基于大型语言模型（LLM）如GPT-4o的检索增强少样本提示系统。我们的最终系统检索语义相似的示例，构建结构化提示，并使用模式引导的输出解析生成可解释的预测。该系统在所有基线方法中表现最优，证明了结合示例驱动提示与LLM推理在教学反馈评估中的有效性。代码可在https://github.com/NaumanNaeem/BEA_2025获取。

</details>


### [36] [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/abs/2506.10641)
**中文标题：拼写并非直截了当：LLMs从标记到字符的分词能力**

*Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: LLMs can spell tokens character by character but struggle with complex character-level tasks. Their embedding layer lacks full character-level encoding, relying on higher layers for reconstruction.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally represent and utilize character-level information during spelling, despite their high accuracy in simple tasks but struggles with complex ones.

Method: Analyzed LLMs' spelling behavior through probing classifiers, identifying knowledge neurons, and inspecting attention weights to reveal how character-level information is reconstructed.

Result: LLMs' embedding layer does not fully encode character-level information beyond the first character, requiring intermediate and higher layers to reconstruct it, with a distinct 'breakthrough' in spelling behavior.

Conclusion: Spelling out tokens is not straightforward for LLMs; they rely on higher layers to compensate for incomplete character-level encoding in the embedding layer.

摘要: 大型语言模型（LLMs）可以逐个字符拼写标记，准确率很高，但在更复杂的字符级任务（如识别标记中的组合子成分）上表现不佳。本研究探讨了LLMs在拼写过程中如何内部表示和利用字符级信息。我们的分析表明，尽管拼写对人类来说是简单任务，但LLMs并未以直截了当的方式处理。具体而言，我们发现嵌入层并未完全编码字符级信息，尤其是第一个字符之后的信息。因此，LLMs依赖中间和更高层的Transformer层来重建字符级知识，在此过程中我们观察到其拼写行为的明显“突破”。我们通过三种互补分析验证了这一机制：探测分类器、知识神经元的识别以及注意力权重的检查。

</details>


### [37] [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/abs/2506.10687)
**中文标题：大型语言模型在生命威胁文本检测中的应用**

*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) like Gemma, Mistral, and Llama-2 to detect life-threatening texts, outperforming traditional methods. Mistral and Llama-2 excel in balanced and imbalanced data, while upsampling helps traditional methods more than LLMs.


<details>
  <summary>Details</summary>
Motivation: To improve the detection of life-threatening language for mental health and safety, leveraging the advanced capabilities of LLMs over traditional methods.

Method: Fine-tuned three open-source LLMs (Gemma, Mistral, Llama-2) on datasets with balanced, imbalanced, and extremely imbalanced scenarios, comparing them with traditional methods like bag of words and BERT.

Result: LLMs, especially Mistral and Llama-2, outperformed traditional methods in all data scenarios. Upsampling helped traditional methods but had less impact on LLMs.

Conclusion: LLMs show strong potential for real-world detection of life-threatening language, with Mistral and Llama-2 being top performers.

摘要: 检测生命威胁语言对于保护处于困境中的个体、促进心理健康与福祉以及预防潜在伤害和生命损失至关重要。本文提出了一种利用大型语言模型（LLMs）识别生命威胁文本的有效方法，并将其与传统方法（如词袋模型、词嵌入、主题建模和双向编码器表示转换器）进行了比较。我们使用7B参数版本的Gemma、Mistral和Llama-2三种开源LLMs，在不同数据集（包括类别平衡、不平衡和极端不平衡场景）上进行了微调。实验结果表明，LLMs在性能上显著优于传统方法。具体而言，Mistral和Llama-2模型在平衡和不平衡数据场景中表现最佳，而Gemma稍逊一筹。我们采用上采样技术处理不平衡数据场景，发现该方法对传统方法有益，但对LLMs影响较小。本研究展示了LLMs在现实世界生命威胁语言检测问题中的巨大潜力。

</details>


### [38] [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/abs/2506.10715)
**中文标题：利用语言模型推断形容词上位词以增强开放英语词网的连通性**

*Lorenzo Augello,John P. McCrae*

Main category: cs.CL

TL;DR: The paper addresses missing hypernymy links for adjectives in Open English Wordnet by adapting TaxoLLaMa's methodology to predict and establish these links using fine-tuned language models.


<details>
  <summary>Details</summary>
Motivation: Open English Wordnet lacks many hypernymy links for adjectives, limiting its connectivity and utility. The paper aims to fill this gap by exploring and establishing adjective hypernymy.

Method: The authors discuss hypernymy relations for adjectives, develop a new resource for adjective hypernymy, and fine-tune large language models (TaxoLLaMa) to predict these relations.

Result: The adapted methodology successfully predicts adjective hypernymy, enhancing the connectivity of Open English Wordnet.

Conclusion: The study demonstrates the feasibility of using language models to infer adjective hypernymy, improving the resource's completeness and usefulness.

摘要: 开放英语词网是作为语言关联开放数据云的一部分发布在OntoLex-lemon中的关键资源。然而，该资源中缺少许多链接，本文探讨了如何建立形容词之间的上位关系。我们对上位关系进行了理论讨论，并比较了形容词与名词和动词在此关系上的差异。我们开发了一个新的形容词上位关系资源，并对大型语言模型进行微调以预测形容词上位关系，结果表明TaxoLLaMa的方法可以适用于此任务。

</details>


### [39] [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/abs/2506.10716)
**中文标题：PREMISE：面向高效数学推理的可扩展和策略性提示优化方法**

*Ye Yu,Yaoning Yu,Haohan Wang*

Main category: cs.CL

TL;DR: PREMISE is a prompt-only framework that optimizes mathematical reasoning in large models by reducing token usage and cost without altering model weights, achieving comparable accuracy with significantly fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models (LRMs) often produce verbose reasoning traces, increasing token usage and cost, which limits their deployment in latency-sensitive or API-constrained settings. PREMISE aims to address this inefficiency.

Method: PREMISE combines trace-level diagnostics with gradient-inspired prompt optimization to minimize redundant computation while preserving accuracy. It uses a multi-objective textual search to balance brevity and correctness.

Result: PREMISE matches or exceeds baseline accuracy (e.g., 96% with Claude, 92% with Gemini) while reducing reasoning tokens by up to 87.5% and cutting costs by 69-82%.

Conclusion: Prompt-level optimization is a practical and scalable approach to efficient LRM inference without compromising reasoning quality.

摘要: 大型推理模型（如Claude 3.7 Sonnet和OpenAI o1）在数学基准测试中表现出色，但其冗长的链式推理（CoT）往往导致不必要的冗余，增加了令牌使用和成本，限制了其在延迟敏感或API受限环境中的部署。我们提出了PREMISE（基于提示的高效数学推理与策略评估），这是一种仅通过提示优化的框架，无需修改模型权重即可减少推理开销。PREMISE结合了跟踪级诊断和梯度启发的提示优化，以最小化冗余计算并保持答案准确性。该方法通过多目标文本搜索平衡令牌长度和答案有效性，共同优化简洁性和正确性。与先前工作不同，PREMISE在单次黑盒接口中运行，可直接应用于商业大语言模型。在GSM8K、SVAMP和Math500测试中，我们匹配或超越了基线准确性（Claude从96%提升至96%，Gemini从91%提升至92%），同时将推理令牌减少高达87.5%，并将成本降低69%至82%。这些结果表明，提示级优化是一种实用且可扩展的路径，可在不损害推理质量的前提下实现高效的大型推理模型推断。

</details>


### [40] [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/abs/2506.10728)
**中文标题：超越真伪：基于检索增强的细微主张层次分析**

*Priyanka Kargupta,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: The paper introduces ClaimSpect, a framework for hierarchically analyzing nuanced claims by breaking them into aspects and sub-aspects, enriching them with corpus-specific perspectives, and validating them through retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Claims often lack binary true/false labels, especially in scientific and political contexts, necessitating a structured approach to dissect and validate their nuanced aspects.

Method: ClaimSpect uses retrieval-augmented generation to construct a hierarchy of claim aspects, retrieves relevant corpus segments, and discovers sub-aspects and varying perspectives.

Result: Applied to real-world claims, ClaimSpect demonstrated robustness and accuracy in deconstructing nuanced claims and representing corpus perspectives, outperforming baselines.

Conclusion: ClaimSpect effectively addresses the complexity of nuanced claims by providing a hierarchical, perspective-rich analysis, validated through case studies and human evaluation.

摘要: 个人或实体提出的主张往往具有细微差别，无法简单地标记为完全“真实”或“虚假”——这在科学和政治主张中尤为常见。然而，一个主张（例如“疫苗A优于疫苗B”）可以分解为其核心方面和子方面（例如有效性、安全性、分发），这些方面更容易单独验证。这使得能够提供更全面、结构化的回应，为特定问题提供全面的视角，同时允许读者优先关注主张中的特定角度（例如对儿童的安全性）。因此，我们提出了ClaimSpect，这是一种基于检索增强生成的框架，用于自动构建处理主张时通常考虑的方面层次结构，并通过语料库特定视角丰富这些方面。该结构层次化地划分输入语料库以检索相关片段，有助于发现新的子方面。此外，这些片段能够发现对主张某一方面的不同视角（例如支持、中立或反对）及其各自的普遍性（例如“有多少生物医学论文认为疫苗A比B更具运输性？”）。我们将ClaimSpect应用于我们构建的数据集中广泛涵盖的现实世界科学和政治主张，展示了其在解构细微主张和表示语料库内视角方面的鲁棒性和准确性。通过现实案例研究和人工评估，我们验证了其在多个基线方法上的有效性。

</details>


### [41] [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/abs/2506.10737)
**中文标题：TaxoAdapt：将基于大语言模型的多维分类体系构建与动态研究语料库对齐**

*Priyanka Kargupta,Nan Zhang,Yunyi Zhang,Rui Zhang,Prasenjit Mitra,Jiawei Han*

Main category: cs.CL

TL;DR: TaxoAdapt is a framework that dynamically adapts LLM-generated taxonomies to evolving scientific corpora across multiple dimensions, outperforming baselines in granularity and coherence.


<details>
  <summary>Details</summary>
Motivation: Scientific fields evolve rapidly, making literature organization challenging. Existing methods either lack generalizability or ignore the dynamic and multi-faceted nature of research. TaxoAdapt addresses these gaps by dynamically aligning taxonomies to corpora.

Method: TaxoAdapt uses iterative hierarchical classification to expand taxonomy width and depth based on corpus topical distribution, adapting to multiple dimensions of scientific literature.

Result: TaxoAdapt generates taxonomies 26.51% more granularity-preserving and 50.41% more coherent than competitive baselines, demonstrating state-of-the-art performance.

Conclusion: TaxoAdapt effectively structures and captures the evolution of scientific fields, offering a scalable and dynamic solution for taxonomy construction.

摘要: 科学领域的快速发展为文献组织和检索带来了挑战。虽然专家构建的分类体系传统上满足了这一需求，但这一过程耗时且昂贵。此外，近期的自动分类构建方法要么过度依赖特定语料库，牺牲了泛化性；要么过度依赖大语言模型（LLMs）预训练数据集中的通用知识，往往忽略了动态演变的科学领域。这些方法还未能考虑到科学文献的多维性，即单篇研究论文可能涉及多个维度（如方法、新任务、评估指标、基准）。为解决这些问题，我们提出了TaxoAdapt，这是一个动态地将LLM生成的分类体系与给定语料库在多个维度上对齐的框架。TaxoAdapt通过迭代的层次分类，根据语料库的主题分布扩展分类体系的宽度和深度。我们展示了其在多年间多个计算机科学会议上的最新性能，证明了其结构和捕捉科学领域演变的能力。作为一种多维方法，TaxoAdapt生成的分类体系在LLM评估下比最具竞争力的基线方法保留了26.51%的细粒度性，并提高了50.41%的连贯性。

</details>


### [42] [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/abs/2506.10766)
**中文标题：一统江湖的分词器：通过多语言分词器实现语言可塑性**

*Diana Abagyan,Alejandro R. Salamanca,Andres Felipe Cruz-Salinas,Kris Cao,Hangyu Lin,Acyr Locatelli,Marzieh Fadaee,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: The paper proposes using a universal tokenizer trained on more languages than the primary pretraining languages to improve language adaptation capabilities in multilingual LLMs, achieving significant gains in language plasticity with minimal performance compromise.


<details>
  <summary>Details</summary>
Motivation: Pretraining multilingual LLMs is challenging due to limited model capacity, data scarcity, and tokenizer language gaps. The study aims to find cost-effective early interventions to enhance post-training language adaptation.

Method: The authors focus on tokenizer design, advocating for a universal tokenizer trained on more languages than the primary pretraining set. They conduct systematic experiments across diverse language groups and training strategies.

Result: A universal tokenizer improves language adaptation by up to 20.2% in win rates compared to language-specific tokenizers. It also enhances plasticity for completely unseen languages by up to 5% win rate gain, with minimal performance loss on pretraining languages.

Conclusion: A universal tokenizer significantly boosts language plasticity and adaptation in multilingual LLMs, offering a practical solution for expanding language coverage post-training.

摘要: 大规模多语言大型语言模型（LLMs）的预训练面临模型容量有限、高质量数据稀缺和计算资源限制等挑战。此外，分词器对语言的覆盖不足使得在训练后阶段填补新语言的空白更加困难。本研究探讨了在训练早期采用相对廉价的方法如何提升模型的“语言可塑性”，即训练后对新语言的适应能力。我们聚焦分词器设计，提出使用一种通用分词器，其训练语言数量超过主要预训练语言，以便在预训练后高效扩展语言覆盖范围。通过对不同语言组和训练策略的系统实验，我们发现通用分词器显著提升了语言适应能力，与预训练语言专用分词器相比，胜率提高了20.2%。此外，通用分词器还提升了完全未见于分词器和预训练中的语言的可塑性，胜率增益达5%。这种扩展语言集的适应能力在预训练包含的大多数语言上性能损失极小。

</details>


### [43] [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/abs/2506.10769)
**中文标题：不同问题，不同模型：临床问答中LLM不确定性和校准的细粒度评估**

*Alberto Testoni,Iacer Calixto*

Main category: cs.CL

TL;DR: The paper evaluates uncertainty estimation methods in clinical QA with LLMs, comparing ten models across specialties and question types, and introduces lightweight single-pass estimators that perform close to Semantic Entropy.


<details>
  <summary>Details</summary>
Motivation: To ensure accurate and well-calibrated uncertainty estimates for deploying LLMs in high-stakes clinical decision support, addressing variability across models and question types.

Method: Fine-grained evaluation of ten open-source LLMs using standard and sampling-based uncertainty methods, plus a case study on single-pass estimators based on reasoning traces.

Result: Substantial variation across specialties and question types, with lightweight single-pass estimators performing close to Semantic Entropy.

Conclusion: Model selection should consider question nature and model strengths, with lightweight methods offering efficient uncertainty estimation.

摘要: 准确且校准良好的不确定性估计对于在临床决策支持等高风险领域部署大型语言模型（LLM）至关重要。我们对临床多项选择题回答中的不确定性估计方法进行了细粒度评估，涵盖十个开源LLM（通用、生物医学和推理模型），涉及两个数据集、十一个医学专业和六种问题类型。我们比较了标准的单生成和基于采样的方法，并通过案例研究探索了基于推理轨迹中行为信号的简单单次估计器。这些轻量级方法在仅需一次生成的情况下，性能接近语义熵。我们的结果揭示了专业和问题类型之间的显著差异，强调了根据问题性质和模型特定优势选择模型的重要性。

</details>


### [44] [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/abs/2506.10779)
**中文标题：利用基于上下文的大型语言模型修订改进命名实体转录**

*Viet Anh Trinh,Xinlu He,Jacob Whitehill*

Main category: cs.CL

TL;DR: The paper proposes an LLM-based revision method to improve named entity transcription in ASR systems, achieving a 30% relative WER reduction on the NER-MIT-OpenCourseWare dataset.


<details>
  <summary>Details</summary>
Motivation: Named entities are critical but often misrecognized by ASR systems, impacting downstream applications. The paper aims to reduce WER for named entities using LLM-based revision.

Method: The authors introduce an LLM revision mechanism that leverages the model's reasoning ability and local context (e.g., lecture notes) to correct named entities in ASR predictions.

Result: The proposed method achieves up to 30% relative WER reduction for named entities on the NER-MIT-OpenCourseWare dataset.

Conclusion: The LLM-based revision mechanism effectively improves named entity transcription in ASR systems, demonstrating significant WER reduction.

摘要: 随着建模技术的进步和监督训练数据的增加，自动语音识别（ASR）系统在通用语音上取得了显著性能。然而，最先进的ASR在命名实体上的词错误率（WER）仍然较高。由于命名实体通常是最关键的关键词，误识别它们会影响所有下游应用，尤其是当ASR系统作为复杂系统的前端时。本文引入了一种大型语言模型（LLM）修订机制，通过利用LLM的推理能力以及包含一组正确命名实体的局部上下文（例如讲义）来修订ASR预测中的错误命名实体。最后，我们介绍了NER-MIT-OpenCourseWare数据集，包含45小时的MIT课程数据用于开发和测试。在该数据集上，我们提出的技术实现了命名实体高达30%的相对WER降低。

</details>


### [45] [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/abs/2506.10800)
**中文标题：通过零空间约束缓解多语言顺序知识编辑中的负面干扰**

*Wei Sun,Tingyu Qu,Mingxiao Li,Jesse Davis,Marie-Francine Moens*

Main category: cs.CL

TL;DR: The paper introduces LangEdit, a framework for multilingual knowledge editing in LLMs that mitigates negative interference by isolating language-specific updates through null-space constraints, improving accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Updating multilingual knowledge in LLMs without causing destructive interference across languages is challenging. Separate editing systems for each language are costly, while unified models often suffer from parameter interference.

Method: LangEdit projects parameter updates for each language onto the orthogonal complement of previous subspaces, ensuring independence and preserving multilingual generalization.

Result: LangEdit outperforms existing methods, effectively mitigating interference and improving accuracy across three model architectures, six languages, and four tasks.

Conclusion: LangEdit enables efficient and accurate multilingual knowledge updates in LLMs, demonstrating significant potential for practical applications.

摘要: 高效更新大型语言模型（LLMs）中的多语言知识，同时保持跨语言的事实表示一致性，是一个长期未解决的挑战。虽然为每种语言部署单独的编辑系统看似可行，但这种方法因需要管理多个模型而产生高昂成本。更高效的解决方案是将所有语言的知识更新集成到一个统一模型中。然而，跨语言的顺序编辑通常会导致破坏性的参数干扰，显著降低多语言泛化能力和注入知识的准确性。为解决这一挑战，我们提出了LangEdit，一种新颖的零空间约束框架，旨在精确隔离特定语言的知识更新。LangEdit的核心创新在于其能够将每种语言的参数更新投影到先前更新子空间的正交补集上。这种方法在数学上保证了更新的独立性，同时保留了多语言泛化能力。我们在三种模型架构、六种语言和四个下游任务上进行了全面评估，结果表明LangEdit有效缓解了参数干扰，并优于现有的最先进编辑方法。我们的结果突显了其在LLMs中实现高效准确多语言知识更新的潜力。代码可在https://github.com/VRCMF/LangEdit.git获取。

</details>


### [46] [ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization](https://arxiv.org/abs/2506.10822)
**中文标题：ReCUT：通过逐步探索和偏好优化平衡大语言模型的推理长度与准确性**

*Zhensheng Jin,Xinze Li,Yifan Ji,Chunyi Peng,Zhenghao Liu,Qi Shi,Yukun Yan,Shuo Wang,Furong Peng,Ge Yu*

Main category: cs.CL

TL;DR: ReCUT introduces a method to balance reasoning length and accuracy in LLMs by generating diverse reasoning paths, training specialized models for accuracy and brevity, and integrating them, achieving shorter reasoning lengths without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought (CoT) methods for LLMs often produce unnecessarily lengthy or redundant reasoning traces, and existing solutions are limited by data quality and overfitting. ReCUT aims to address these issues by optimizing reasoning length and accuracy.

Method: ReCUT uses stepwise exploration and long-short switched sampling to generate diverse reasoning paths, evaluates them to create preference pairs, trains two specialized models (for accuracy and brevity), and integrates them via parameter interpolation.

Result: ReCUT reduces reasoning lengths by 30-50% while maintaining or improving accuracy across multiple math reasoning datasets and backbone models compared to baselines.

Conclusion: ReCUT effectively balances reasoning length and accuracy in LLMs, offering a practical solution to the overthinking problem in CoT prompting.

摘要: 最近，思维链（CoT）提示的进展显著提升了大语言模型（LLM）的推理能力。然而，这些方法常因过度思考导致推理轨迹冗长或冗余。现有方法通过整理多条推理链训练LLM来缓解这一问题，但其效果常受生成数据质量的限制且易过拟合。为解决这一挑战，我们提出“通过逐步探索压缩推理”（ReCUT），旨在平衡推理轨迹的准确性与长度。具体而言，ReCUT采用逐步探索机制和长短切换采样策略，使LLM能逐步生成多样化的推理路径。这些路径经评估后用于构建偏好对，以训练两个专用模型（Gemini LLM）——一个优化推理准确性，另一个优化推理简洁性。最终通过参数插值整合这两个模型。在多个数学推理数据集和骨干模型上的实验结果表明，与多种基线相比，ReCUT显著缩短推理长度约30-50%，同时保持或提升推理准确性。所有代码和数据将通过https://github.com/NEUIR/ReCUT发布。

</details>


### [47] [CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training](https://arxiv.org/abs/2506.10844)
**中文标题：CIIR@LiveRAG 2025：通过自训练优化多智能体检索增强生成**

*Alireza Salemi,Mukta Maddipatla,Hamed Zamani*

Main category: cs.CL

TL;DR: The paper introduces mRAG, a multi-agent RAG framework optimized through self-training, outperforming traditional RAG baselines in the SIGIR 2025 LiveRAG competition.


<details>
  <summary>Details</summary>
Motivation: To enhance retrieval-augmented generation (RAG) by developing a multi-agent system that improves collaboration and response quality through self-training.

Method: mRAG employs specialized agents for subtasks (planning, searching, reasoning, coordination) and uses self-training with reward-guided trajectory sampling to optimize performance.

Result: mRAG outperforms conventional RAG baselines on DataMorgana-derived datasets in the SIGIR 2025 LiveRAG competition, demonstrating superior efficacy.

Conclusion: The mRAG framework is effective for complex RAG tasks, showcasing improved inter-agent collaboration and response generation through self-training.

摘要: 本文介绍了mRAG，一种由规划、搜索、推理和协调等子任务专用智能体组成的多智能体检索增强生成（RAG）框架。我们的系统采用自训练范式，通过奖励引导的轨迹采样优化智能体间协作并提升响应生成。在SIGIR 2025 LiveRAG竞赛中基于DataMorgana数据集进行评估，mRAG表现优于传统RAG基线。我们进一步分析了竞赛结果，并通过案例研究展示了该框架的优势，证明了其在复杂现实世界RAG任务中的有效性。

</details>


### [48] [Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles](https://arxiv.org/abs/2506.10848)
**中文标题：利用SlowFast加速扩散大语言模型：三条黄金原则**

*Qingyan Wei,Yaojie Zhang,Zhiyuan Liu,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: The paper introduces SlowFast Sampling, a dynamic strategy for diffusion-based language models (dLLMs) that alternates between exploratory and accelerated decoding stages, guided by three principles, achieving significant speedups with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing sampling strategies for dLLMs suffer from static behavior, leading to inefficiency and limited flexibility, prompting the need for a dynamic approach to improve performance.

Method: Proposes SlowFast Sampling, a dynamic strategy guided by certainty, convergence, and positional principles, integrated with dLLM-Cache to reduce redundant computation.

Result: Achieves up to 15.63× speedup on LLaDA with minimal accuracy drop, and up to 34.22× when combined with caching, outperforming autoregressive baselines like LLaMA3 8B.

Conclusion: Well-designed sampling can unlock the full potential of dLLMs for fast and high-quality generation, demonstrating the effectiveness of SlowFast Sampling.

摘要: 扩散式语言模型（dLLMs）作为一种有前景的替代方案，通过并行生成标记显著降低了推理延迟。然而，现有的采样策略（如基于置信度或半自回归解码）常因静态行为导致效率低下和灵活性受限。本文提出SlowFast采样，一种动态采样策略，自适应地在探索性和加速解码阶段交替切换。该方法遵循三条黄金原则：确定性原则、收敛性原则和位置性原则，指导何时何地可以高效且自信地解码标记。我们进一步将该策略与dLLM-Cache结合以减少冗余计算。在多个基准和模型上的实验表明，SlowFast采样在LLaDA上实现了高达15.63倍的加速且精度损失极小，结合缓存后可达34.22倍。值得注意的是，我们的方法在吞吐量上优于LLaMA3 8B等强自回归基线，表明精心设计的采样策略可以充分释放dLLMs在快速高质量生成中的潜力。

</details>


### [49] [Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models](https://arxiv.org/abs/2506.10855)
**中文标题：分析自监督语音模型中预训练语言、语音、声调和说话者信息之间的关系**

*Michele Gubian,Ioana Krehan,Oli Liu,James Kirby,Sharon Goldwater*

Main category: cs.CL

TL;DR: The paper analyzes how wav2vec2 models trained on different languages encode phonetic, tonal, and speaker information, finding that these representations are largely orthogonal and similar across languages.


<details>
  <summary>Details</summary>
Motivation: To understand how self-supervised speech models (specifically wav2vec2) represent phonetic, tonal, and speaker information across different languages, as most prior analyses focused only on English.

Method: The study uses probing classifiers and geometric analyses on wav2vec2 models trained on four languages to examine representations of phones, tones, and speakers.

Result: The subspaces encoding phones, tones, and speakers are largely orthogonal across pretraining and test languages, with minor advantages for matched-language phone and tone probes in later layers.

Conclusion: The structure of representations learned by wav2vec2 is largely independent of the pretraining language, suggesting universal patterns in self-supervised speech models.

摘要: 自监督语音模型的分析已开始揭示它们如何表示不同类型的信息。然而，几乎所有分析都集中在英语上。本文研究了在四种不同语言上训练的wav2vec2模型如何编码语言匹配和非匹配的语音。我们使用探测分类器和几何分析来检查语音、词汇声调和说话者信息的表示方式。结果表明，对于所有预训练和测试语言，编码语音、声调和说话者的子空间基本正交，且探测准确率的层级模式相似，后期层中匹配语言的语音和声调（而非说话者）探测略有优势。我们的发现表明，wav2vec2学习的表示结构在很大程度上独立于预训练中使用的语音材料。

</details>


### [50] [Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment](https://arxiv.org/abs/2506.10877)
**中文标题：通过知识精炼和动态提示调整增强医疗对话生成**

*Hongda Sun,Jiaren Peng,Wenzhong Yang,Liang He,Bo Du,Rui Yan*

Main category: cs.CL

TL;DR: The paper introduces MedRef, a medical dialogue system that enhances response quality by refining knowledge and dynamically adjusting prompts, outperforming existing methods in accuracy and generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing medical dialogue systems struggle with identifying relevant knowledge and generating personalized, accurate responses, necessitating an improved approach.

Method: MedRef uses a knowledge refining mechanism and dynamic prompt adjustment, including Triplet Filter and Demo Selector modules, to improve response relevance and accuracy.

Result: MedRef outperforms state-of-the-art baselines on MedDG and KaMed benchmarks in generation quality and medical entity accuracy.

Conclusion: MedRef is effective and reliable for real-world healthcare applications, demonstrating superior performance in medical dialogue generation.

摘要: 医疗对话系统（MDS）已成为支持与患者进行多轮、上下文感知对话的关键在线平台。然而，现有MDS往往难以（1）识别相关医学知识，（2）生成个性化且医学准确的回答。为解决这些问题，我们提出了MedRef，一种新型MDS，结合了知识精炼和动态提示调整。首先，我们采用知识精炼机制过滤无关医学数据，提高对回答中关键医学实体的预测能力。此外，我们设计了一种综合提示结构，整合历史细节和显性细节。为实现对不同患者病情的实时适应，我们实现了两个关键模块——三元组过滤器和演示选择器，提供系统提示中配备的适当知识和示例。在MedDG和KaMed基准上的大量实验表明，MedRef在生成质量和医学实体准确性上均优于现有基线方法，证明了其在现实医疗应用中的有效性和可靠性。

</details>


### [51] [Slimming Down LLMs Without Losing Their Minds](https://arxiv.org/abs/2506.10885)
**中文标题：在不丧失智能的情况下精简大型语言模型**

*Qingda,Mai*

Main category: cs.CL

TL;DR: The paper validates that LoRA and QLoRA fine-tuning methods improve task-specific performance in LLMs efficiently, with performance linked to dataset-task alignment.


<details>
  <summary>Details</summary>
Motivation: To explore how parameter-efficient fine-tuning methods (LoRA and QLoRA) impact LLM performance across reasoning and knowledge tasks, providing insights for resource-limited developers.

Method: Evaluated LoRA and QLoRA on three domains: commonsense reasoning (HellaSwag), mathematical reasoning (GSM8K), and multi-domain knowledge (MMLU-CS).

Result: LoRA-based methods enhance task-specific performance efficiently, with performance heavily influenced by dataset-task alignment.

Conclusion: Parameter-efficient fine-tuning (e.g., LoRA) is effective for LLM adaptation, offering practical guidance for developers with limited resources.

摘要: 本文研究并验证了微调对大型语言模型性能的影响，重点关注参数高效方法（LoRA和QLoRA）。我们评估了模型在三个关键领域的性能：（1）常识推理（HellaSwag），（2）数学推理（GSM8K），以及（3）多领域知识（MMLU-CS）。
研究结果表明：（1）基于LoRA的方法在保持计算效率的同时有效提高了任务特定性能；（2）性能很大程度上取决于微调数据集与基准任务之间的对齐。该研究不仅提供了对参数高效机制的理论见解，还为开发者在资源有限的情况下实现高效LLM适应提供了实用指导。

</details>


### [52] [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
**中文标题：泛化还是幻觉？理解Transformer中的上下文外推理**

*Yixiao Huang,Hanlin Zhu,Tianyu Guo,Jiantao Jiao,Somayeh Sojoudi,Michael I. Jordan,Stuart Russell,Song Mei*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) exhibit both generalization and hallucination due to out-of-context reasoning (OCR), a mechanism that associates concepts regardless of causal links. It formalizes OCR, demonstrates its role in model behavior, and attributes it to gradient descent's implicit bias favoring low nuclear norm solutions.


<details>
  <summary>Details</summary>
Motivation: The duality of LLMs—generalizing from new facts while hallucinating incorrect information—is poorly understood. The paper aims to uncover the underlying mechanism (OCR) driving both behaviors.

Method: The study formalizes OCR as a synthetic factual recall task, tests it across five LLMs, and analyzes a simplified transformer model to isolate the role of matrix factorization. Theoretical analysis links OCR to gradient descent's implicit bias.

Result: OCR drives both generalization and hallucination, depending on causal relationships between concepts. Matrix factorization is crucial for OCR, and gradient descent's bias toward low nuclear norm solutions explains the phenomenon.

Conclusion: The work provides a theoretical foundation for OCR, explaining LLM behaviors and offering insights for mitigating undesirable effects of knowledge injection.

摘要: 大型语言模型（LLM）可以通过微调获取新知识，但这一过程表现出一种令人困惑的双重性：模型可以从新事实中显著泛化，但也容易产生幻觉性错误信息。然而，这一现象的原因仍知之甚少。本文认为，这两种行为源于一种称为上下文外推理（OCR）的单一机制：即通过关联概念（即使没有因果关系）推断含义的能力。我们在五个主流LLM上的实验证实，OCR确实驱动了泛化和幻觉，具体取决于关联概念是否具有因果关系。为了对这一现象建立严格的理论理解，我们将OCR形式化为一种合成事实回忆任务。实证表明，具有分解输出和值矩阵的单层单头注意力Transformer可以学习解决此任务，而权重合并的模型则不能，这凸显了矩阵分解的关键作用。我们的理论分析表明，OCR能力可归因于梯度下降的隐式偏差，其倾向于最小化合输出-值矩阵的核范数的解。这种数学结构解释了为什么模型能够高效地学习关联事实和含义，无论相关性是因果性的还是仅仅是虚假的。最终，我们的工作为理解OCR现象提供了理论基础，为分析和减轻知识注入中的不良行为提供了新的视角。

</details>


### [53] [BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP](https://arxiv.org/abs/2506.10896)
**中文标题：BioClinical ModernBERT：一种用于生物医学和临床自然语言处理的最先进长上下文编码器**

*Thomas Sounack,Joshua Davis,Brigitte Durieux,Antoine Chaffin,Tom J. Pollard,Eric Lehman,Alistair E. W. Johnson,Matthew McDermott,Tristan Naumann,Charlotta Lindvall*

Main category: cs.CL

TL;DR: BioClinical ModernBERT is a state-of-the-art biomedical and clinical NLP encoder, adapted from ModernBERT, offering long-context processing, improved speed, and performance through extensive pretraining on a diverse corpus.


<details>
  <summary>Details</summary>
Motivation: Encoder models in biomedical and clinical NLP have lagged in development compared to decoders, limiting domain adaptation. BioClinical ModernBERT aims to address this gap by providing a high-performance, domain-specific encoder.

Method: The model is developed via continued pretraining on a large biomedical and clinical corpus (53.5B tokens) and leverages 20 diverse datasets. It includes base (150M) and large (396M) parameter versions.

Result: BioClinical ModernBERT outperforms existing encoders on four downstream tasks, demonstrating superior performance across diverse biomedical and clinical use cases.

Conclusion: BioClinical ModernBERT sets a new standard for biomedical and clinical NLP encoders, offering improved capabilities and supporting further research through released checkpoints.

摘要: 基于编码器的Transformer模型在生物医学和临床自然语言处理（NLP）中至关重要，其双向自注意力机制使其非常适合通过判别任务从非结构化文本中高效提取结构化信息。然而，与解码器模型相比，编码器的发展较慢，导致在生物医学和临床领域的适应性有限。我们推出了BioClinical ModernBERT，这是一种基于最新ModernBERT发布的领域适应编码器，结合了长上下文处理能力，并在生物医学和临床NLP的速度和性能上实现了显著提升。BioClinical ModernBERT通过迄今为止最大的生物医学和临床语料库（超过535亿个标记）进行持续预训练而开发，并通过利用来自不同机构、领域和地理区域的20个数据集（而非依赖单一来源的数据）解决了先前临床编码器的一个关键限制。它在涵盖广泛用例的四个下游任务中优于现有的生物医学和临床编码器。我们发布了BioClinical ModernBERT的基础版（1.5亿参数）和大版（3.96亿参数），以及训练检查点以支持进一步研究。

</details>


### [54] [Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning](https://arxiv.org/abs/2506.10903)
**中文标题：超越黄金标准：基于LLM评委的认知集成用于形式数学推理**

*Lan Zhang,Marco Valentino,Andre Freitas*

Main category: cs.CL

TL;DR: The paper introduces an epistemically grounded ensemble (EFG) of LLM judges to evaluate autoformalization in formal mathematical reasoning, outperforming coarse-grained methods by aligning better with human assessments.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating autoformalization in formal mathematical reasoning are limited by coarse-grained criteria, making them ineffective for nuanced, advanced domains. Human evaluation is time-consuming and requires expertise.

Method: Proposes an epistemically and formally grounded ensemble (EFG) of LLM judges, evaluating autoformalization based on logical preservation, mathematical consistency, formal validity, and formal quality.

Result: The EFG ensemble correlates more strongly with human assessments than coarse-grained models, especially in evaluating formal qualities, proving it a reliable proxy for autoformalization evaluation.

Conclusion: LLM-as-judges, guided by well-defined atomic properties, offer scalable, interpretable, and reliable support for evaluating formal mathematical reasoning.

摘要: 自动形式化在形式数学推理中起着关键作用，它能够将自然语言陈述自动翻译为形式语言。尽管最近使用大型语言模型（LLM）的进展显示出有希望的结果，但自动评估自动形式化的方法仍未得到充分探索。随着转向更复杂的领域（如高等数学），人工评估需要大量时间和领域专业知识，尤其是当底层陈述和背景知识的复杂性增加时。LLM作为评委为自动化此类评估提供了一种有前景的方法。然而，现有方法通常采用粗粒度和通用的评估标准，限制了其在高级形式数学推理中的有效性，而质量取决于细微的多粒度维度。在本研究中，我们通过引入一种系统化的自动方法来评估自动形式化任务，朝着填补这一空白迈出了一步。所提出的方法基于一种认知和形式上扎根的LLM评委集成（EFG），其标准包括逻辑保持（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ），从而实现了对不同贡献因素的透明评估。我们验证了该框架可作为形式数学领域中自动形式化评估的代理。总体而言，我们的实验表明，EFG集成LLM评委是一种合适的新兴评估代理，与粗粒度模型相比，其与人工评估的相关性更强，尤其是在评估形式质量时。这些发现表明，LLM作为评委，尤其是在一组明确定义的原子属性指导下，可以为评估形式数学推理提供可扩展、可解释且可靠的支持。

</details>


### [55] [Magistral](https://arxiv.org/abs/2506.10910)
**中文标题：Magistral**

*Mistral-AI,:,Abhinav Rastogi,Albert Q. Jiang,Andy Lo,Gabrielle Berrada,Guillaume Lample,Jason Rute,Joep Barmentlo,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Léonard Blier,Lucile Saulnier,Matthieu Dinot,Maxime Darrin,Neha Gupta,Roman Soletskyi,Sagar Vaze,Teven Le Scao,Yihan Wang,Adam Yang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Andy Ehrenberg,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jean-Hadrien Chabran,Jean-Malo Delignon,Joachim Studnia,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Kush Jain,Lingxiao Zhao,Louis Martin,Luyu Gao,Lélio Renard Lavaud,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Maximilian Augustin,Mickaël Seznec,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Romain Sauvestre,Rémi Delacourt,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yunhao Tang*

Main category: cs.CL

TL;DR: Magistral is Mistral's first reasoning model, developed using a scalable RL pipeline. It explores pure RL training for LLMs, introduces a method to enforce reasoning language, and shows RL on text maintains or enhances model capabilities. Magistral Medium and Small are released, with the latter open-sourced.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the limits of pure RL training for LLMs without relying on prior models or implementations, and to demonstrate the effectiveness of RL on text data for maintaining or improving model capabilities.

Method: The authors use a ground-up approach with their own models and infrastructure, focusing on pure RL training. They introduce a method to enforce reasoning language and train Magistral Medium on Mistral Medium 3 using RL alone. Magistral Small includes cold-start data from Magistral Medium.

Result: RL on text maintains or improves multimodal understanding, instruction following, and function calling. Magistral Medium and Small are successfully trained, with the latter open-sourced under Apache 2.0.

Conclusion: The study demonstrates the feasibility and benefits of pure RL training for LLMs, showing that RL on text data preserves or enhances model capabilities. Magistral models are released to support further research.

摘要: 我们介绍了Magistral，这是Mistral的第一个推理模型，以及我们自己的可扩展强化学习（RL）流程。我们没有依赖现有实现或从先前模型中提取的RL痕迹，而是采用了一种从头开始的方法，仅依赖我们自己的模型和基础设施。值得注意的是，我们展示了一个使我们能够探索纯RL训练LLM极限的堆栈，提出了一种强制模型推理语言的简单方法，并表明仅对文本数据进行RL训练可以保留初始检查点的大部分能力。我们发现，对文本进行RL训练可以保持或提升多模态理解、指令遵循和函数调用能力。我们推出了Magistral Medium，它在Mistral Medium 3的基础上仅通过RL训练进行推理，并开源了Magistral Small（Apache 2.0），其中还包括来自Magistral Medium的冷启动数据。

</details>


### [56] [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
**中文标题：通过半非负矩阵分解将MLP激活分解为可解释特征**

*Or Shafran,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: The paper proposes using Semi-Nonnegative Matrix Factorization (SNMF) to decompose MLP activations in LLMs, yielding interpretable features that outperform sparse autoencoders and supervised baselines in causal evaluations.


<details>
  <summary>Details</summary>
Motivation: Current methods like sparse autoencoders (SAEs) for identifying interpretable features in LLMs lack intrinsic interpretability and struggle in causal evaluations. The paper aims to address these limitations by directly decomposing MLP activations using SNMF.

Method: The paper employs semi-nonnegative matrix factorization (SNMF) to decompose MLP activations, ensuring features are sparse linear combinations of co-activated neurons and mapped to their activating inputs for interpretability.

Result: SNMF-derived features outperform SAEs and a supervised baseline (difference-in-means) in causal steering tasks and align with human-interpretable concepts. The analysis also reveals hierarchical structures in MLP activation spaces.

Conclusion: SNMF is a simple and effective tool for identifying interpretable features in LLMs, offering better performance and alignment with human concepts compared to existing methods.

摘要: 机制可解释性的核心目标之一是确定大型语言模型（LLM）中能够因果解释其输出的分析单元。早期研究集中于单个神经元，但神经元通常编码多个概念的证据促使研究方向转向激活空间中的方向。关键问题是如何以无监督方式找到捕捉可解释特征的方向。现有方法依赖于稀疏自编码器（SAE）的字典学习，通常通过残差流激活训练以从头学习方向。然而，SAE在因果评估中表现不佳，且缺乏内在可解释性，因为其学习未明确与模型计算绑定。本文通过半非负矩阵分解（SNMF）直接分解MLP激活来解决这些限制，使得学习到的特征是（a）共激活神经元的稀疏线性组合，（b）映射到其激活输入，从而直接可解释。在Llama 3.1、Gemma 2和GPT-2上的实验表明，SNMF衍生的特征在因果操控上优于SAE和强监督基线（均值差异），并与人类可解释概念一致。进一步分析揭示，特定神经元组合在语义相关特征间被重复使用，暴露了MLP激活空间中的层次结构。这些结果表明，SNMF是一种简单有效的工具，可用于识别可解释特征并剖析LLM中的概念表示。

</details>


### [57] [Dynamic Epistemic Friction in Dialogue](https://arxiv.org/abs/2506.10934)
**中文标题：对话中的动态认知摩擦**

*Timothy Obiso,Kenneth Lai,Abhijnan Nath,Nikhil Krishnaswamy,James Pustejovsky*

Main category: cs.CL

TL;DR: The paper introduces 'dynamic epistemic friction' as resistance to belief updates in dialogues, positioning it within Dynamic Epistemic Logic, and demonstrates its predictive power in collaborative tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the overlooked role of epistemic friction in belief updates during human-AI interactions, highlighting its importance in aligning Large Language Models with human preferences.

Method: The study defines dynamic epistemic friction within Dynamic Epistemic Logic and analyzes its effects in a collaborative task to predict belief updates in dialogues.

Result: The model of epistemic friction effectively predicts belief updates in dialogues and can be refined to handle real-world dialogue complexities.

Conclusion: Dynamic epistemic friction is a crucial factor in belief alignment, offering a framework to enhance human-AI collaboration by addressing resistance to belief updates.

摘要: 近期，大型语言模型（LLMs）与人类偏好的对齐显著提升了其在人机协作场景中的实用性。然而，这些方法往往忽视了“认知摩擦”的关键作用，即在面对新的、冲突的或模糊信息时更新信念所遇到的固有阻力。本文中，我们将动态认知摩擦定义为对认知整合的阻力，其特征是智能体当前信念状态与外部证据支持的新命题之间的错位。我们将其置于动态认知逻辑（Van Benthem和Pacuit，2011）的框架中，其中摩擦在交互过程中表现为非平凡的信念修正。随后，我们通过一项协作任务的分析，展示了这种认知摩擦模型如何有效预测对话中的信念更新，并进一步讨论了如何将信念对齐作为认知阻力或摩擦的度量模型自然地进行复杂化，以适应现实世界对话场景的复杂性。

</details>


### [58] [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training](https://arxiv.org/abs/2506.10952)
**中文标题：Domain2Vec：向量化数据集以无需训练找到最优数据混合**

*Mozhi Zhang,Howe Tissue,Lu Wang,Xipeng Qiu*

Main category: cs.CL

TL;DR: Domain2Vec introduces a method to decompose datasets into meta-domains, enabling training-free identification of optimal data mixtures for language model pretraining, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding the optimal data mixture for language model pretraining without extensive training, leveraging the Distribution Alignment Assumption (DA²).

Method: Domain2Vec decomposes datasets into meta-domains using a classifier, creating domain vectors that represent distributions over these meta-domains, and uses them to align training and validation data distributions.

Result: Domain2Vec reduces computational overhead by 51.5% while achieving the same validation loss and improves downstream task performance by an average of 2.83% under equivalent compute budgets.

Conclusion: Domain2Vec provides an efficient, scalable solution for optimizing data mixtures in language model pretraining, significantly reducing computational costs and enhancing performance.

摘要: 我们提出了Domain2Vec，这是一种新颖的方法，能够将任何数据集分解为多个元域的线性组合，元域是一种新概念，旨在捕捉数据集的关键底层特征。Domain2Vec维护一个元域词汇表，并使用分类器将给定数据集分解为对应于该词汇表分布的域向量。这些域向量能够在无需训练的情况下，根据分布对齐假设（DA²）识别语言模型预训练的最优数据混合。该假设表明，当训练集和验证集的数据分布更对齐时，验证损失会更低。此外，Domain2Vec可以无缝集成到先前的工作中，以建模域向量与语言模型性能之间的关系，大大提高了先前方法的效率和可扩展性。大量实验表明，Domain2Vec能够以最小的计算开销找到提升下游任务性能的数据混合。具体而言，Domain2Vec在Pile-CC上仅使用原始Pile数据集混合计算量的51.5%就达到了相同的验证损失。在相同的计算预算下，Domain2Vec平均将下游性能提高了2.83%。

</details>


### [59] [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960)
**中文标题：ChineseHarm-Bench：一个中文有害内容检测基准**

*Kangwei Liu,Siyuan Cheng,Bozhong Tian,Xiaozhuan Liang,Yuyang Yin,Meng Han,Ningyu Zhang,Bryan Hooi,Xi Chen,Shumin Deng*

Main category: cs.CL

TL;DR: The paper introduces ChineseHarm-Bench, a professionally annotated benchmark for detecting harmful content in Chinese, addressing the scarcity of such resources. It includes six categories of real-world data and a knowledge rule base to assist LLMs. A knowledge-augmented baseline is proposed to enhance smaller models' performance.


<details>
  <summary>Details</summary>
Motivation: Existing harmful content detection resources are mainly focused on English, leaving Chinese datasets scarce and limited. The paper aims to fill this gap by providing a comprehensive benchmark for Chinese content harm detection.

Method: The authors construct a benchmark from real-world data, covering six harmful content categories, and develop a knowledge rule base. They propose a knowledge-augmented baseline combining human-annotated rules and LLM implicit knowledge.

Result: The benchmark and knowledge rule base improve harmful content detection in Chinese. The knowledge-augmented baseline enables smaller models to match state-of-the-art LLM performance.

Conclusion: ChineseHarm-Bench addresses the lack of Chinese harmful content detection resources and demonstrates the effectiveness of integrating expert knowledge with LLMs for improved detection.

摘要: 大型语言模型（LLMs）越来越多地应用于自动化有害内容检测任务，协助审核人员识别政策违规行为，提高内容审查的整体效率和准确性。然而，现有的有害内容检测资源主要集中在英语领域，中文数据集稀缺且范围有限。我们提出了一个全面、专业标注的中文内容有害检测基准，涵盖六个代表性类别，并完全基于真实数据构建。我们的标注过程还产生了一个知识规则库，为LLMs提供显式专家知识以辅助中文有害内容检测。此外，我们提出了一种知识增强的基线方法，结合了人工标注的知识规则和大型语言模型的隐式知识，使较小模型能够达到与最先进LLMs相当的性能。代码和数据可在https://github.com/zjunlp/ChineseHarm-bench获取。

</details>


### [60] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
**中文标题：AutoMind：用于自动化数据科学的自适应知识型智能体**

*Yixin Ou,Yujie Luo,Jingsheng Zheng,Lanning Wei,Shuofei Qiao,Jintian Zhang,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: AutoMind is an adaptive LLM-agent framework for automated data science, integrating expert knowledge, strategic search, and dynamic coding to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-driven data science agents rely on rigid workflows and lack adaptability, limiting their effectiveness in complex tasks. AutoMind aims to address these limitations.

Method: AutoMind introduces (1) a curated expert knowledge base, (2) an agentic knowledgeable tree search algorithm, and (3) a self-adaptive coding strategy to dynamically tailor solutions.

Result: AutoMind outperforms state-of-the-art baselines in automated data science benchmarks, demonstrating superior effectiveness, efficiency, and solution quality.

Conclusion: AutoMind represents a robust step toward fully automated data science by overcoming the limitations of existing frameworks.

摘要: 大型语言模型（LLM）智能体在解决现实世界数据科学问题方面展现出巨大潜力。LLM驱动的数据科学智能体有望实现整个机器学习流程的自动化，但其实际效果仍有限。现有框架依赖于僵化的预定义工作流程和缺乏灵活性的编码策略，因此仅能在相对简单的经典问题上表现出色，而无法捕捉人类从业者在复杂创新任务中积累的经验。本文介绍了AutoMind，一种自适应、知识型的LLM智能体框架，通过三项关键创新克服了这些不足：（1）一个精心策划的专家知识库，使智能体具备领域专家知识；（2）一种基于知识的树搜索算法，策略性地探索可能的解决方案；（3）一种自适应编码策略，根据任务复杂度动态调整代码生成。在两个自动化数据科学基准测试上的评估表明，AutoMind的性能优于现有最先进的基线方法。进一步分析证实了其在有效性、效率和解决方案质量方面的优势，突显AutoMind是实现完全自动化数据科学的高效且稳健的一步。

</details>


### [61] [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/abs/2506.10979)
**中文标题：推理模型在识别和恢复无益思维方面的表现如何？**

*Sohee Yang,Sang-Woo Lee,Nora Kassner,Daniela Gottesman,Sebastian Riedel,Mor Geva*

Main category: cs.CL

TL;DR: The paper investigates reasoning models' ability to identify and recover from unhelpful thoughts, finding they struggle with recovery despite identifying most issues, especially larger models.


<details>
  <summary>Details</summary>
Motivation: To assess reasoning models' self-reevaluation capabilities in identifying and correcting unhelpful thoughts, which is crucial for accurate reasoning and safer AI systems.

Method: The study examines models' performance in handling four types of unhelpful thoughts: rambling, irrelevant, misdirecting, and incorrect thoughts, including injecting these thoughts into reasoning processes.

Result: Models can identify most unhelpful thoughts but fail to recover from them, with larger models performing worse. Smaller models are less distracted by harmful thoughts.

Conclusion: The findings highlight the need for improved self-reevaluation in reasoning models to enhance accuracy and safety.

摘要: 近期的推理模型展现出反思、回溯和自我验证推理的能力，这对于发现错误并得出准确解决方案至关重要。一个自然的问题是模型能否有效地进行这种自我重新评估。我们通过研究推理模型在识别和恢复四种无益思维（无信息的长篇大论、与问题无关的思维、将问题误导为稍不同问题的思维以及导致错误答案的思维）方面的表现来探讨这一问题。我们发现模型能有效识别大多数无益思维，但在这些思维被注入其思考过程时却难以恢复，导致性能显著下降。模型倾向于天真地延续无关思维的推理线，这表明其自我重新评估能力远未达到“元认知”意识。此外，我们观察到非/逆向缩放趋势，即更大的模型在恢复短无关思维时表现更差，即使被指示重新评估推理。我们通过一个使用无关思维注入的越狱实验展示了这些发现的含义，表明最小的模型最不容易被有害响应触发的思维分散注意力。总体而言，我们的发现呼吁改进推理模型的自我重新评估能力，以开发更好的推理和更安全的系统。

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [62] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
**中文标题：使用文本到图像和音频生成模型的多模态电影视频合成**

*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: The paper presents a method for synthesizing 60-second cinematic videos using Stable Diffusion for images, GPT-2 for narrative, and a hybrid audio pipeline, achieving professional-quality results with optimizations for efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: To leverage advances in generative AI for automating cinematic video creation from text inputs, enhancing multimedia production for creative, educational, and industrial applications.

Method: Combines Stable Diffusion for image synthesis, GPT-2 for narrative structuring, and a hybrid audio pipeline (gTTS and YouTube-sourced music) within a five-scene framework, augmented by frame interpolation, post-processing, and synchronization.

Result: Demonstrates outstanding visual quality, narrative coherence, and efficiency, supporting resolutions up to 1024x768 and frame rates of 15-30 FPS.

Conclusion: The method advances text-to-video synthesis, offering a reliable and high-quality solution for diverse applications.

摘要: 生成人工智能的进步改变了多媒体创作，允许从文本输入自动合成电影视频。本研究描述了一种创建60秒电影视频的方法，结合了Stable Diffusion用于高保真图像合成、GPT-2用于叙事结构，以及使用gTTS和YouTube来源音乐的混合音频管道。采用五场景框架，并通过线性帧插值、电影后期处理（如锐化）和音视频同步提升专业质量。该方法在GPU加速的Google Colab环境中使用Python 3.11实现，具有双模式Gradio界面（简单和高级），支持分辨率高达1024x768和帧率15-30 FPS。通过CUDA内存管理和错误处理等优化确保可靠性。实验展示了出色的视觉质量、叙事连贯性和效率，进一步推动了文本到视频合成在创意、教育和工业领域的应用。

</details>


### [63] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/abs/2506.10082)
**中文标题：LoRA-Edit：基于掩码感知LoRA微调的首帧引导可控视频编辑**

*Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: LoRA-Edit introduces a mask-aware LoRA fine-tuning method for controllable video editing, enabling flexible edits while preserving background regions without altering model architecture.


<details>
  <summary>Details</summary>
Motivation: Current video editing methods rely on large-scale pretraining and lack flexibility for specific edits. First-frame-guided editing provides initial control but struggles with subsequent frames.

Method: The paper proposes a mask-based LoRA tuning method that adapts pretrained Image-to-Video models, using spatial masks to guide region-specific learning and reference images for appearance guidance.

Result: Experimental results show superior video editing performance compared to state-of-the-art methods, with efficient and adaptable edits.

Conclusion: LoRA-Edit offers a flexible and efficient solution for video editing by leveraging mask-aware LoRA tuning, balancing control and adaptability.

摘要: 使用扩散模型进行视频编辑在生成高质量编辑视频方面取得了显著成果。然而，现有方法通常依赖于大规模预训练，限制了特定编辑的灵活性。首帧引导编辑提供了对首帧的控制，但对后续帧缺乏灵活性。为此，我们提出了一种基于掩码的LoRA（低秩适应）微调方法，通过调整预训练的图像到视频（I2V）模型实现灵活的视频编辑。我们的方法在保留背景区域的同时，实现了可控的编辑传播。这一解决方案无需改变模型架构，即可提供高效且适应性强的视频编辑。为了更好地引导这一过程，我们引入了额外的参考（如替代视角或代表性场景状态），作为内容展开的视觉锚点。我们通过掩码驱动的LoRA调优策略解决了控制难题，将预训练的I2V模型适配到编辑上下文中。模型需要从两个不同的来源学习：输入视频提供空间结构和运动线索，而参考图像提供外观指导。空间掩码通过动态调节模型的注意力，实现区域特定学习，确保每个区域从适当的来源获取信息。实验结果表明，我们的方法在视频编辑性能上优于现有最先进方法。

</details>


### [64] [DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding](https://arxiv.org/abs/2506.10084)
**中文标题：DeepTraverse：一种受深度优先搜索启发的算法视觉理解网络**

*Bin Guo,John H. L. Hansen*

Main category: cs.CV

TL;DR: DeepTraverse introduces a novel vision architecture inspired by algorithmic search strategies, enabling adaptive and interpretable feature learning, outperforming conventional models in image classification.


<details>
  <summary>Details</summary>
Motivation: Conventional vision backbones lack adaptive, iterative refinement. The paper explores whether principles from classical search algorithms can create more structured and interpretable processing flows in networks.

Method: DeepTraverse uses recursive exploration modules for deepening feature analysis and adaptive calibration modules to dynamically adjust feature salience, combining algorithmic search strategies with deep learning.

Result: DeepTraverse achieves competitive classification accuracy and robust feature discrimination, often outperforming conventional models with similar or larger parameter counts.

Conclusion: Integrating algorithmic priors provides a principled and effective strategy for building efficient, performant, and structured vision backbones.

摘要: 尽管传统视觉主干网络取得了成功，但其特征构建通常通过一系列几乎一致的操作完成，缺乏自适应迭代优化的显式路径。这引发了一个引人深思的问题：经典搜索算法的原理能否在这些网络中注入更具算法性、结构化和逻辑性的处理流程，从而通过更可解释、甚至类似推理的决策过程构建表征？我们提出了DeepTraverse，一种直接受算法搜索策略启发的新型视觉架构，使其能够通过系统阐明和自适应优化的过程学习特征，与传统方法截然不同。DeepTraverse通过两个关键协同组件实现这一点：递归探索模块通过参数共享高效地沿着有前景的表征路径深化特征分析，以及自适应校准模块根据全局上下文动态调整特征显著性。这种算法的相互作用使DeepTraverse能够智能地构建和优化特征模式。在多样化的图像分类基准测试中，DeepTraverse表现出极具竞争力的分类准确性和鲁棒的特征判别能力，通常优于参数数量相似或更多的传统模型。我们的工作表明，整合此类算法先验为构建更高效、性能更强且结构化的视觉主干网络提供了一种有原则且有效的策略。

</details>


### [65] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/abs/2506.10085)
**中文标题：通用任务进度估计的测试时适应方法**

*Christos Ziakas,Alessandra Russo*

Main category: cs.CV

TL;DR: The paper introduces a test-time adaptation method for progress estimation models, enabling online adaptation to test trajectories via self-supervised learning, outperforming state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: To improve progress estimation models' ability to generalize across diverse tasks, environments, and embodiments by adapting to test-time visual and temporal contexts.

Method: A gradient-based meta-learning strategy trains the model on expert trajectories and task descriptions, optimizing a self-supervised objective for test-time adaptation.

Result: The method generalizes well to out-of-distribution tasks and outperforms autoregressive vision-language models in progress estimation.

Conclusion: Test-time adaptation enhances progress estimation by leveraging semantic content over temporal order, demonstrating superior generalization.

摘要: 我们提出了一种测试时适应方法，使进度估计模型能够通过优化学习的自监督目标，在线适应测试轨迹的视觉和时间上下文。为此，我们引入了一种基于梯度的元学习策略，在专家视觉轨迹及其自然语言任务描述上训练模型，使得测试时适应能够依赖语义内容而非时间顺序改进进度估计。我们的测试时适应方法从单一训练环境推广到多样化的分布外任务、环境和体现形式，优于使用自回归视觉语言模型的最先进上下文学习方法。

</details>


### [66] [EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models](https://arxiv.org/abs/2506.10100)
**中文标题：高效VLA：视觉-语言-动作模型的免训练加速与压缩**

*Yantai Yang,Yuhao Wang,Zichen Wen,Luo Zhongwei,Chang Zou,Zhipeng Zhang,Chuan Wen,Linfeng Zhang*

Main category: cs.CV

TL;DR: EfficientVLA is a training-free framework that accelerates and compresses Vision-Language-Action models by exploiting redundancies in language, visual, and action modules, achieving significant speedup and FLOP reduction with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models face high computational and memory demands due to redundancies, limiting practical deployment. Existing solutions address isolated inefficiencies but fail to holistically tackle bottlenecks across the entire pipeline.

Method: EfficientVLA integrates three strategies: pruning redundant language layers, optimizing visual token selection, and caching intermediate features in the action module.

Result: Applied to CogACT, EfficientVLA achieves a 1.93X speedup, reduces FLOPs to 28.9%, and only drops success rate by 0.6% in the SIMPLER benchmark.

Conclusion: EfficientVLA provides a holistic, training-free solution to accelerate VLA models, making them more practical for deployment without significant performance trade-offs.

摘要: 视觉-语言-动作（VLA）模型，尤其是基于扩散架构的模型，在具身智能方面展现出变革性潜力，但由于其固有的和推理时的大量冗余，计算和内存需求极高，严重阻碍了其应用。现有的加速方法通常针对孤立的低效问题，这种零散的解决方案通常无法全面解决整个VLA流程中的多种计算和内存瓶颈，从而限制了实际部署性。我们提出了EfficientVLA，这是一种结构化且免训练的推理加速框架，通过协同利用多方面的冗余，系统性地消除这些障碍。EfficientVLA协同整合了三种针对性策略：（1）通过分析层间冗余，修剪语言模块中功能无关紧要的层；（2）通过任务感知策略优化视觉处理路径，选择一组紧凑且多样化的视觉标记，平衡任务关键性与信息覆盖；（3）通过策略性地缓存和重用关键中间特征，减轻基于扩散迭代的动作头中的时间计算冗余。我们将该方法应用于标准VLA模型CogACT，在SIMPLER基准测试中实现了1.93倍的推理加速，并将FLOPs降低至28.9%，同时成功率仅下降0.6%。

</details>


### [67] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
**中文标题：用于检测野外儿童的手动标注图像-字幕数据集**

*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

TL;DR: The paper introduces ICCWD, a manually annotated image-caption dataset for detecting children in diverse contexts, benchmarking three detectors with a best true positive rate of 75.3%.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a multi-modal dataset for detecting minors in digital content, which is crucial for regulatory compliance and automation tools.

Method: The authors created ICCWD, a dataset of 10,000 image-caption pairs manually labeled for child presence, and benchmarked three detection methods, including a commercial age estimation system.

Result: Child detection proved challenging, with the best method achieving a 75.3% true positive rate.

Conclusion: The release of ICCWD aims to improve minor detection methods across various scenarios.

摘要: 平台和法律对描绘未成年人（定义为18岁以下的个体）的数字内容与其他类型内容的监管方式不同。鉴于需要评估的内容数量庞大，基于机器学习的自动化工具通常用于检测描绘未成年人的内容。据我们所知，目前尚无用于在多模态环境中检测这些识别方法的数据集或基准。为填补这一空白，我们发布了图像-字幕野外儿童数据集（ICCWD），这是一个旨在为检测未成年人描绘工具提供基准的图像-字幕数据集。我们的数据集比以往的儿童图像数据集更丰富，包含各种情境下的儿童图像，包括虚构描绘和部分可见的身体。ICCWD包含10,000个手动标注的图像-字幕对，用于指示图像中是否存在儿童。为展示我们数据集的潜在用途，我们用它来对三种不同的检测器进行基准测试，包括应用于图像的商业年龄估计系统。我们的结果表明，儿童检测是一项具有挑战性的任务，最佳方法的真阳性率为75.3%。我们希望数据集的发布将有助于设计更广泛的场景中更好的未成年人检测方法。

</details>


### [68] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/abs/2506.10119)
**中文标题：使用计算机视觉检测银屑病：CNN与Vision Transformers的比较研究**

*Natanael Lucena,Fábio S. da Silva,Ricardo Rios*

Main category: cs.CV

TL;DR: The paper compares CNNs and Vision Transformers (ViTs) for classifying psoriasis images, finding ViTs superior, especially the DaViT-B model with a 96.4% f1-score.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of CNNs and ViTs in classifying medical images of psoriasis and similar diseases, aiming to identify the most efficient architecture for automated detection.

Method: Pre-trained CNN and ViT models were adapted to a specific dataset of psoriasis and similar disease images, and their performance was compared using predictive metrics.

Result: ViTs outperformed CNNs, with the DaViT-B model achieving the highest f1-score of 96.4%, demonstrating superior efficiency with smaller models.

Conclusion: ViTs, particularly DaViT-B, are recommended for automated psoriasis detection due to their high performance and efficiency, highlighting their potential in medical image classification.

摘要: 本文比较了卷积神经网络（CNN）和Vision Transformers（ViTs）在多分类银屑病及类似疾病图像任务中的表现。基于ImageNet预训练的模型被适配到特定数据集。两种模型均取得了较高的预测指标，但ViTs凭借较小模型下的卓越表现脱颖而出。Dual Attention Vision Transformer-Base（DaViT-B）获得了最佳结果，f1分数为96.4%，被推荐为自动化银屑病检测的最高效架构。本文强化了ViTs在医学图像分类任务中的潜力。

</details>


### [69] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/abs/2506.10128)
**中文标题：ViCrit：一种用于视觉语言模型视觉感知的可验证强化学习代理任务**

*Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang*

Main category: cs.CV

TL;DR: The paper introduces ViCrit, a verifiable reinforcement learning proxy task for VLMs to localize subtle visual hallucinations in captions, improving visual perception across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Extending RL's success in LLMs to VLMs is challenging due to the lack of vision-centric tasks that are both difficult and verifiable. ViCrit addresses this gap.

Method: ViCrit trains VLMs to detect synthetic visual hallucinations in captions, providing binary rewards for pinpointing errors in objects, attributes, counts, or spatial relations.

Result: Models trained with ViCrit show significant improvements on VL benchmarks, including transfer to abstract image reasoning and visual math.

Conclusion: ViCrit is an effective and generalizable objective for enhancing visual perception in VLMs, demonstrated by ViCrit-Bench's diagnostic evaluation.

摘要: 强化学习（RL）在微调大型语言模型（LLMs）方面表现出色，适用于数学推理或代码生成等具有挑战性且易于验证的任务。然而，将这一成功扩展到视觉语言模型（VLMs）的视觉感知中，却因缺乏同时具有挑战性和明确可验证性的视觉中心任务而受阻。为此，我们提出了ViCrit（视觉描述幻觉批评器），这是一种RL代理任务，用于训练VLMs定位人类编写的图像描述段落中注入的微妙合成视觉幻觉。我们从200字的描述开始，注入单个微妙的视觉描述错误（改变对象、属性、数量或空间关系的几个词），并让模型根据图像和修改后的描述定位被破坏的部分。这一方法保留了完整的感知难度，同时提供了易于计算且明确的二进制精确匹配奖励。通过ViCrit任务训练的模型在各种VL基准测试中表现出显著提升。重要的是，这些改进不仅适用于自然图像训练数据，还能迁移到抽象图像推理和视觉数学中，显示出学习感知而非仅记忆所见对象的潜力。为了便于评估，我们还引入了ViCrit-Bench，这是一个类别平衡的诊断基准，系统地探测不同图像领域和错误类型的感知错误。总之，我们的结果表明，细粒度的幻觉批评是增强VLMs视觉感知的有效且可推广的目标。

</details>


### [70] [RoCA: Robust Cross-Domain End-to-End Autonomous Driving](https://arxiv.org/abs/2506.10145)
**中文标题：RoCA：鲁棒的跨领域端到端自动驾驶**

*Rajeev Yasarla,Shizhong Han,Hsin-Pai Cheng,Litian Liu,Shweta Mahajan,Apratim Bhattacharyya,Yunxiao Shi,Risheek Garrepalli,Hong Cai,Fatih Porikli*

Main category: cs.CV

TL;DR: RoCA is a novel framework for robust cross-domain end-to-end autonomous driving, improving generalizability and adaptation without extra inference costs.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end autonomous driving models struggle with cross-domain deployment, and while LLMs offer open-world knowledge, they lack performance guarantees and incur high retraining costs.

Method: RoCA formulates a joint probabilistic distribution over tokens encoding vehicle information, using Gaussian processes to learn basis tokens and trajectories for diverse scenarios, enabling probabilistic trajectory inference.

Result: RoCA significantly outperforms direct finetuning in domain adaptation and improves generalizability without additional inference computation.

Conclusion: RoCA provides a robust solution for cross-domain end-to-end autonomous driving, achieving strong generalization and adaptation performance.

摘要: 端到端（E2E）自动驾驶作为一种新范式，具有巨大潜力，但跨领域（如城市）部署的实际挑战尚未得到充分研究。尽管已有研究利用大型语言模型（LLM）获取开放世界知识，但LLM无法保证跨领域驾驶性能，且领域适应时可能产生高昂的再训练成本。本文提出RoCA，一种鲁棒的跨领域E2E自动驾驶框架。RoCA在E2E流程中联合建模编码自车和周围车辆信息的令牌的概率分布。通过高斯过程（GP）实例化，RoCA学习一组基础令牌及其对应轨迹，覆盖多样驾驶场景。随后，给定任意驾驶场景，RoCA能概率推断未来轨迹。通过在源领域训练中结合RoCA与基础E2E模型，无需额外推理计算即可提升基础模型的泛化能力。此外，RoCA支持在新目标领域上的鲁棒适应，显著优于直接微调。我们在多种跨领域场景中广泛评估RoCA，结果表明其具备强大的领域泛化和适应性能。

</details>


### [71] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
**中文标题：SPARKE：基于RKE分数的扩散模型中可扩展的提示感知多样性引导方法**

*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

TL;DR: SPARKE introduces a scalable, prompt-aware diversity guidance method for diffusion models, reducing computational complexity while improving diversity in generated samples.


<details>
  <summary>Details</summary>
Motivation: Ensuring diversity in prompt-guided diffusion models is challenging, especially across broad semantic spectrums. Existing methods lack scalability and prompt-aware diversity control.

Method: SPARKE uses conditional entropy for diversity guidance, dynamically conditioning on similar prompts. It reduces computational complexity from O(n³) to O(n) via Conditional latent RKE Score Guidance.

Result: SPARKE improves prompt-aware diversity in generated samples without significant computational costs, validated on text-to-image diffusion models.

Conclusion: SPARKE offers an efficient and scalable solution for enhancing diversity in prompt-guided diffusion models, balancing performance and computational feasibility.

摘要: 扩散模型在高保真图像合成和提示引导生成建模中表现出色。然而，确保提示引导扩散模型生成样本的多样性仍然是一个挑战，尤其是在提示涵盖广泛语义范围且需要以提示感知方式评估生成数据的多样性时。近期方法通过多样性度量引入引导以鼓励更多样化的生成。本文扩展了基于多样性度量的方法，提出了可扩展的提示感知Rény核熵多样性引导（SPARKE）方法。SPARKE利用条件熵进行多样性引导，动态地将多样性度量条件化为相似提示，并实现提示感知的多样性控制。尽管基于熵的引导方法增强了提示感知多样性，但其对矩阵熵分数的依赖在大规模生成场景中带来了计算挑战。为此，我们专注于条件潜在RKE分数引导的特殊情况，将熵计算和基于梯度的优化复杂度从一般熵度量的O(n³)降低到O(n)。降低的计算复杂度允许在不同提示下的数千次生成轮次中进行多样性引导采样。我们在多个文本到图像扩散模型上对SPARKE方法进行了数值测试，结果表明该方法在不显著增加计算成本的情况下提高了生成数据的提示感知多样性。代码已在项目页面发布：https://mjalali.github.io/SPARKE

</details>


### [72] [Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context](https://arxiv.org/abs/2506.10174)
**中文标题：通过时间上下文隐式反照率恢复的地表太阳辐射反演**

*Yael Frischholz,Devis Tuia,Michael Lehning*

Main category: cs.CV

TL;DR: The paper proposes an attention-based emulator for surface solar radiation (SSR) retrieval that implicitly learns clear-sky reflectance from satellite image sequences, improving accuracy in mountainous regions with dynamic snow cover.


<details>
  <summary>Details</summary>
Motivation: Traditional SSR retrieval methods fail in mountainous regions due to intermittent snow cover and changing surfaces. The paper aims to develop a model that can implicitly learn surface reflectance dynamics without relying on hand-crafted features like albedo maps or cloud masks.

Method: The authors use an attention-based emulator built on the Temporo-Spatial Vision Transformer. It processes multi-spectral SEVIRI imagery, static topographic features, and solar geometry, trained on HelioMont's SSR estimates over Switzerland.

Result: The model matches the performance of albedo-informed models when given sufficient temporal context, particularly excelling in mountainous regions and improving generalization across diverse terrains.

Conclusion: The proposed method demonstrates the ability to implicitly learn surface reflectance dynamics, offering a robust solution for SSR retrieval in complex terrains without explicit albedo data.

摘要: 从卫星图像中准确反演地表太阳辐射（SSR）的关键在于估算晴空条件下空间传感器观测到的背景反射率。与这一基线的偏差可用于检测云的存在，并指导辐射传输模型推断大气衰减。传统的反演算法通常使用月度统计数据近似背景反射率，假设地表性质相对于大气条件变化缓慢。然而，这种方法在山区失效，因为间歇性积雪和变化的雪面频繁出现。我们提出了一种基于注意力的SSR反演模拟器，能够隐式地从原始卫星图像序列中学习晴空地表反射率。基于时空视觉变换器的方法无需手工特征（如显式反照率图或云掩膜）。该模拟器在瑞士（以复杂地形和动态积雪为特征）使用HelioMont算法的瞬时SSR估计进行训练。输入包括来自Meteosat第二代平台的多光谱SEVIRI图像，辅以静态地形特征和太阳几何信息。目标变量是HelioMont的SSR，以1.7公里的空间分辨率计算为直接和漫射水平辐照度分量的总和。研究表明，当提供足够长的时间上下文时，该模型的性能与基于反照率的模型相当，突显了其内部学习和利用潜在地表反射率动态的能力。地理空间分析表明，这种效果在山区最为显著，并在简单和复杂地形环境中均提高了泛化能力。代码和数据集公开于https://github.com/frischwood/HeMu-dev.git。

</details>


### [73] [Attention, Please! Revisiting Attentive Probing for Masked Image Modeling](https://arxiv.org/abs/2506.10178)
**中文标题：请注意！重新审视掩码图像建模中的注意力探测方法**

*Bill Psomas,Dionysis Christopoulos,Eirini Baltzi,Ioannis Kakogeorgiou,Tilemachos Aravanis,Nikos Komodakis,Konstantinos Karantzalos,Yannis Avrithis,Giorgos Tolias*

Main category: cs.CV

TL;DR: The paper revisits attentive probing for Masked Image Modeling (MIM), introduces Efficient Probing (EP), a multi-query cross-attention mechanism that improves accuracy and efficiency, outperforming linear probing and prior methods across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard linear probing fails to reflect MIM's potential due to distributed patch tokens, motivating the need for efficient attentive probing methods.

Method: The paper introduces Efficient Probing (EP), a multi-query cross-attention mechanism that reduces redundant projections and parameters, achieving faster performance.

Result: EP outperforms linear probing and prior attentive probing methods across seven benchmarks, generalizes beyond MIM, and shows gains in low-shot and layer-wise settings.

Conclusion: EP is a simple yet effective attentive probing method that balances accuracy and efficiency, offering interpretability and strong performance.

摘要: 随着微调（FT）在大规模应用中变得越来越不切实际，探测成为自监督学习（SSL）的首选评估协议。然而，标准的线性探测（LP）由于补丁标记的分布式特性，未能充分反映掩码图像建模（MIM）训练的模型的潜力。这促使了对注意力探测的需求，该方法利用注意力选择性地聚合补丁级特征。尽管其应用日益广泛，注意力探测仍未被充分探索，现有方法存在参数过多和计算效率低下的问题。
  在这项工作中，我们从准确性与效率权衡的角度重新审视了注意力探测。我们对现有方法进行了系统研究，分析了其机制并对其性能进行了基准测试。我们引入了高效探测（EP），这是一种多查询交叉注意力机制，消除了冗余投影，减少了可训练参数的数量，并比传统的多头注意力实现了高达10倍的加速。尽管简单，EP在七个基准测试中优于LP和先前的注意力探测方法，能够很好地推广到MIM以外的多种预训练范式，生成可解释的注意力图，并在低样本和分层设置中实现了显著的性能提升。代码发布于https://github.com/billpsomas/efficient-probing。

</details>


### [74] [Improving Personalized Search with Regularized Low-Rank Parameter Updates](https://arxiv.org/abs/2506.10182)
**中文标题：通过正则化低秩参数更新改进个性化搜索**

*Fiona Ryan,Josef Sivic,Fabian Caba Heilbron,Judy Hoffman,James M. Rehg,Bryan Russell*

Main category: cs.CV

TL;DR: The paper introduces a method for personalized vision-language retrieval by adapting a dual encoder model with regularized low-rank parameter updates, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Personalized vision-language retrieval is challenging due to the need to learn new concepts from few examples while integrating personal and general knowledge. The paper aims to improve this by adapting model parameters effectively.

Method: The approach involves regularized low-rank adaptation of parameters in the language encoder's final layer, combined with strategies for merging multiple personal concepts. A new metric evaluates general knowledge preservation.

Result: The method outperforms prior art by 4%-22% on personalized image retrieval benchmarks (DeepFashion2 and ConCon-Chi), demonstrating superior accuracy.

Conclusion: Regularized low-rank parameter updates effectively adapt vision-language models for personalized retrieval while preserving general knowledge, setting a new benchmark.

摘要: 个性化视觉语言检索旨在从少量示例中识别新概念（例如“我的狗Fido”）。这一任务具有挑战性，因为它不仅需要从少量图像中学习新概念，还需要将个人知识与通用知识结合以在不同上下文中识别概念。本文展示了如何有效调整视觉语言双编码器模型的内部表示以用于个性化视觉语言检索。我们发现，对语言编码器最后一层的一小部分参数进行正则化低秩调整，是识别个人概念并保留通用知识的高效替代方案。此外，我们探索了合并多个学习到的个人概念参数的策略，发现参数相加是有效的。为了评估微调表示中通用知识的保留情况，我们引入了一种基于视觉语言模型（VLM）生成的标题测量图像检索准确性的指标。我们的方法在两个个性化图像检索基准（DeepFashion2和ConCon-Chi）上实现了最先进的准确性，在个性化检索任务中比现有技术高出4%-22%。

</details>


### [75] [ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators](https://arxiv.org/abs/2506.10226)
**中文标题：ScoreMix：通过扩散生成器中的分数组合改进人脸识别**

*Parsa Rahimi,Sebastien Marcel*

Main category: cs.CV

TL;DR: ScoreMix is a data augmentation method using diffusion models to enhance face recognition by mixing scores from different class-conditioned trajectories, improving performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited labeled data in face recognition by leveraging diffusion models to generate synthetic samples that enhance discriminative capabilities.

Method: ScoreMix convexly mixes scores from different class-conditioned trajectories during diffusion sampling, focusing on combining classes distant in the discriminator's embedding space.

Result: The approach significantly improves performance in benchmarks, especially when mixing distant classes, and shows minimal correlation between the generator's condition space and the discriminator's embedding space.

Conclusion: ScoreMix offers a practical solution for improving face recognition without extensive parameter tuning or large datasets, demonstrating the effectiveness of score composition in diffusion models.

摘要: 本文提出ScoreMix，一种新颖而简单的数据增强策略，利用扩散模型的分数组合特性来增强判别器性能，特别是在标记数据有限的情况下。通过在扩散采样过程中对不同类条件轨迹的分数进行凸组合，我们生成了具有挑战性的合成样本，显著提高了所有研究基准中的判别能力。我们系统地研究了混合的类选择策略，发现当组合在判别器嵌入空间中距离较远的类时，性能提升更大，而不是在生成器条件空间中距离较近的类。此外，我们通过实验证明，在标准指标下，生成器学习的条件空间与判别器的嵌入空间之间的相关性很小。我们的方法在没有进行大量参数搜索的情况下实现了显著的性能改进，展示了在训练判别模型时的实际优势，同时有效缓解了大规模数据集收集的问题。论文网站：https://parsa-ra.github.io/scoremix

</details>


### [76] [California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops](https://arxiv.org/abs/2506.10228)
**中文标题：加利福尼亚农作物产量基准：结合卫星图像、气候、蒸散发和土壤数据层对70多种作物进行县级产量预测**

*Hamid Kamangir,Mona Hajiesmaeeli,Mason Earles*

Main category: cs.CV

TL;DR: The paper introduces a comprehensive dataset and deep learning model for forecasting crop yields in California, integrating satellite imagery, climate, evapotranspiration, and soil data, achieving high predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate crop yield forecasting in California is challenging due to complex environmental factors, despite extensive historical data. This study aims to address this gap by integrating diverse data sources and developing a robust predictive model.

Method: The study combines Landsat satellite imagery, climate records, evapotranspiration, and soil data into a multi-modal deep learning model. The model uses stratified feature extraction and a timeseries encoder to capture spatial and temporal dynamics.

Result: The model achieves an R2 score of 0.76 across over 70 crops, demonstrating strong predictive performance for county-level yield forecasting in California.

Conclusion: The benchmark dataset and modeling framework provide a valuable tool for agricultural forecasting, climate adaptation, and precision farming, with publicly available data and code.

摘要: 加利福尼亚是全球农业生产的领导者，占美国总产量的12.5%，是世界第五大食品和棉花供应地。尽管美国农业部国家农业统计服务提供了大量历史产量数据，但由于环境、气候和土壤相关因素的复杂相互作用，准确及时的作物产量预测仍然具有挑战性。本研究引入了一个全面的作物产量基准数据集，涵盖2008年至2022年加利福尼亚所有县的70多种作物。该基准整合了多种数据源，包括Landsat卫星图像、每日气候记录、每月蒸散发和高分辨率土壤属性。为了有效学习这些异构输入，我们开发了一个多模态深度学习模型，专门用于县级、特定作物的产量预测。该模型采用分层特征提取和时间序列编码器来捕捉生长季节的空间和时间动态。静态输入（如土壤特性和作物身份）则用于长期变异性分析。我们的方法在所有作物的未见测试数据集上实现了0.76的总体R2分数，突显了在加利福尼亚多样化农业区域的强大预测性能。这一基准和建模框架为推进农业预测、气候适应和精准农业提供了宝贵的基础。完整的数据集和代码库已在我们的GitHub存储库中公开。

</details>


### [77] [DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos](https://arxiv.org/abs/2506.10242)
**中文标题：DySS：基于动态查询和状态空间学习的高效多摄像头视频3D目标检测**

*Rajeev Yasarla,Shizhong Han,Hong Cai,Fatih Porikli*

Main category: cs.CV

TL;DR: DySS introduces a dynamic query and state-space learning method for efficient 3D object detection from multi-camera videos, achieving superior performance and real-time inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods for camera-based 3D object detection in BEV are either costly (dense features) or inefficient (sparse queries with large numbers). DySS aims to address these limitations by leveraging state-space learning and dynamic queries for better efficiency and performance.

Method: DySS uses a state-space model (SSM) to process sampled features over time, enhanced by auxiliary tasks (future prediction and masked reconstruction). It dynamically updates queries via merge, remove, and split operations to maintain a lean set of detection queries.

Result: DySS achieves 65.31 NDS and 57.4 mAP on nuScenes test split, outperforming state-of-the-art methods, and runs at 33 FPS on the val split.

Conclusion: DySS demonstrates that dynamic queries and state-space learning can significantly improve efficiency and performance in 3D object detection from multi-camera videos.

摘要: 基于摄像头的鸟瞰图（BEV）3D目标检测是自动驾驶中最重要的感知任务之一。早期方法依赖于密集的BEV特征，构建成本高昂。最近的研究探索了基于稀疏查询的检测方法，但仍需要大量查询，且在使用更多视频帧时计算成本较高。本文提出DySS，一种利用状态空间学习和动态查询的新方法。具体而言，DySS通过状态空间模型（SSM）逐步处理时间步长的采样特征，并通过未来预测和掩码重建等辅助任务优化SSM训练。SSM的状态提供了场景的高效且信息丰富的总结。基于状态空间学习特征，DySS通过合并、删除和拆分操作动态更新查询，保持网络中检测查询的精简和有效性。DySS在nuScenes测试集上取得了65.31 NDS和57.4 mAP的优异性能，优于最新技术。在验证集上，DySS达到56.2 NDS和46.2 mAP，并以33 FPS的实时推理速度运行。

</details>


### [78] [HalLoc: Token-level Localization of Hallucinations for Vision Language Models](https://arxiv.org/abs/2506.10286)
**中文标题：HalLoc：视觉语言模型中幻觉的标记级定位**

*Eunkyu Park,Minyeong Kim,Gunhee Kim*

Main category: cs.CV

TL;DR: HalLoc introduces a dataset and baseline model for efficient, probabilistic hallucination detection in vision-language models, enabling graded confidence outputs and low-overhead integration.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in vision-language models undermine reliability, but current detection methods are computationally intensive and lack nuanced outcomes for real-world scenarios.

Method: HalLoc provides a 150K token-level annotated dataset for hallucination detection across VQA, instruction-following, and captioning tasks, alongside a baseline model for low-overhead detection.

Result: The dataset and model enable efficient, probabilistic hallucination detection with graded confidence, improving reliability without sacrificing efficiency.

Conclusion: HalLoc advances hallucination detection by offering a plug-and-play module for vision-language models, enhancing trustworthiness in real-world applications.

摘要: 幻觉对大型视觉语言模型的可靠性构成重大挑战，其检测对于确保关键应用中的准确性至关重要。当前的检测方法通常依赖于计算密集型模型，导致高延迟和资源需求。其确定性结果也无法应对现实场景中幻觉与真实信息界限模糊的情况。为解决这些问题，我们提出了HalLoc，一个为高效概率性幻觉检测设计的数据集。它包含15万个标记级注释样本，涵盖视觉问答（VQA）、指令跟随和图像描述任务中的幻觉类型。该数据集支持开发能够以分级置信度检测幻觉的模型，从而实现更明智的用户交互。此外，我们还引入了一个基于HalLoc训练的基线模型，提供低开销、并发的幻觉检测功能。该模型可无缝集成到现有视觉语言模型中，在保持效率的同时提高可靠性。这一即插即用的幻觉检测模块为增强视觉语言模型在现实应用中的可信度开辟了新途径。HalLoc数据集和代码已公开于：https://github.com/dbsltm/cvpr25_halloc。

</details>


### [79] [Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation](https://arxiv.org/abs/2506.10302)
**中文标题：不确定性感知深度学习在自动化皮肤癌分类中的综合评估**

*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.CV

TL;DR: The paper evaluates deep learning models for skin cancer classification, highlighting CLIP-based vision transformers as top performers and emphasizing the importance of uncertainty quantification (UQ) for reliable medical diagnosis.


<details>
  <summary>Details</summary>
Motivation: To improve skin cancer diagnosis by addressing data scarcity and lack of uncertainty awareness in deep learning models, ensuring both accuracy and reliability in clinical applications.

Method: Benchmarked pre-trained feature extractors (e.g., CLIP, ResNet50, DenseNet121) with traditional classifiers (e.g., SVM, XGBoost), then incorporated UQ techniques (MCD, Ensemble, EMCD) to assess prediction reliability.

Result: CLIP-based models, especially LAION CLIP ViT-H/14 with SVM, performed best. Ensemble methods balanced accuracy and uncertainty handling, while EMCD was more sensitive to uncertain predictions.

Conclusion: Integrating UQ into deep learning enhances both performance and trustworthiness in medical diagnosis, with ensemble methods offering a practical trade-off.

摘要: 准确可靠的皮肤癌诊断对于早期治疗和改善患者预后至关重要。深度学习（DL）模型在自动化皮肤癌分类中显示出潜力，但其性能可能受限于数据稀缺和缺乏不确定性感知。本研究基于HAM10000数据集，通过迁移学习和不确定性量化（UQ）对DL皮肤病变分类进行了综合评估。第一阶段，我们测试了多种预训练特征提取器（如CLIP变体、ResNet50、DenseNet121、VGG16和EfficientNet-V2-Large）与传统分类器（如SVM、XGBoost和逻辑回归）的组合。结果表明，基于CLIP的视觉变换器（尤其是LAION CLIP ViT-H/14与SVM结合）表现最佳。第二阶段，我们通过蒙特卡洛Dropout（MCD）、集成和集成蒙特卡洛Dropout（EMCD）引入UQ，不仅评估预测准确性，还评估模型输出的可靠性。使用不确定性感知指标（如UAcc、USen、USpe和UPre）评估这些模型。结果显示，集成方法在准确性和不确定性处理之间取得了良好平衡，而EMCD对不确定预测更敏感。本研究强调了将UQ整合到基于DL的医学诊断中的重要性，以提升实际临床应用中的性能和可信度。

</details>


### [80] [Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework](https://arxiv.org/abs/2506.10328)
**中文标题：迈向可扩展的SOAP笔记生成：一种弱监督多模态框架**

*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: A weakly supervised multimodal framework is proposed to generate SOAP notes from lesion images and sparse clinical text, reducing manual annotation needs and achieving performance comparable to advanced models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Manual SOAP note generation is labor-intensive and contributes to clinician burnout, especially in skin carcinoma cases, which are prevalent and costly.

Method: The framework uses weakly supervised learning with multimodal inputs (lesion images and sparse clinical text) to generate structured SOAP notes, minimizing reliance on manual annotations.

Result: The method performs comparably to GPT-4o, Claude, and DeepSeek Janus Pro, with novel metrics (MedConceptEval and CCS) validating clinical relevance.

Conclusion: The framework offers a scalable solution for SOAP note generation, reducing clinician burden and the need for large annotated datasets.

摘要: 皮肤癌是全球最常见的癌症形式，每年医疗支出超过80亿美元。在临床环境中，医生使用详细的SOAP（主观、客观、评估和计划）笔记记录患者就诊情况。然而，手动生成这些笔记耗时耗力，加剧了临床医生的职业倦怠。本研究提出了一种弱监督多模态框架，通过有限的输入（包括病变图像和稀疏的临床文本）生成结构化的SOAP笔记。该方法减少了对人工标注的依赖，实现了可扩展且临床可靠的文档生成，同时减轻了临床医生的负担并降低了对大规模标注数据的需求。我们的方法在关键临床相关性指标上表现与GPT-4o、Claude和DeepSeek Janus Pro相当。为评估临床质量，我们引入了两种新指标MedConceptEval和临床一致性评分（CCS），分别用于评估与专家医学概念的语义对齐和输入特征的临床一致性。

</details>


### [81] [Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video](https://arxiv.org/abs/2506.10331)
**中文标题：用户生成的全向视频音视频质量评估数据集与方法研究**

*Fei Zhao,Da Pan,Zelu Qi,Ping Shi*

Main category: cs.CV

TL;DR: The paper introduces a dataset and baseline model for audio-visual quality assessment (AVQA) of user-generated omnidirectional videos (UGC-ODVs), addressing a gap in research due to the growing popularity of the Metaverse.


<details>
  <summary>Details</summary>
Motivation: With the rise of the Metaverse, omnidirectional videos (ODVs) are shifting from professional to user-generated content, yet research on AVQA for UGC-ODVs is limited. This study aims to fill that gap.

Method: The authors constructed a UGC-ODV dataset with 300 videos captured by five individuals using two types of omnidirectional cameras, covering 10 scene types. They conducted subjective AVQA experiments to obtain Mean Opinion Scores (MOSs) and developed a baseline model with video and audio feature extraction and fusion modules.

Result: The baseline model achieved optimal performance on the proposed dataset, demonstrating its effectiveness for UGC-ODV AVQA.

Conclusion: The study provides a valuable dataset and baseline model to advance research in UGC-ODV AVQA, addressing a critical need in the field.

摘要: 随着元宇宙的兴起，全向视频（ODV）逐渐从专业生成内容（PGC）转向用户生成内容（UGC）。然而，关于ODV中音视频质量评估（AVQA）的研究仍然有限。为此，我们构建了一个UGC全向音视频（A/V）内容数据集。视频由五名个人使用两种不同类型的全向相机拍摄，共300个视频，涵盖10种不同的场景类型。在数据集上进行了主观AVQA实验，以获得A/V序列的平均意见分数（MOS）。随后，为了促进UGC-ODV AVQA领域的发展，我们在提出的数据集上构建了一个有效的AVQA基线模型，该基线模型包括视频特征提取模块、音频特征提取模块和音视频融合模块。实验结果表明，我们的模型在提出的数据集上实现了最佳性能。

</details>


### [82] [Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions](https://arxiv.org/abs/2506.10334)
**中文标题：利用视觉语言模型通过面部表情检测学生的学业情绪**

*Deliang Wang,Chao Yang,Gaowei Chen*

Main category: cs.CV

TL;DR: The study explores using Vision-Language Models (VLMs) to detect students' academic emotions from facial expressions in online learning, finding moderate success with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct, especially in recognizing happy and confused expressions.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised machine learning models for analyzing academic emotions struggle with generalization and require extensive data collection and training. VLMs offer a promising zero-shot alternative.

Method: Employed two VLMs (Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct) to analyze 5,000 images of students' facial expressions (confused, distracted, happy, neutral, tired) using zero-shot prompting.

Result: Both models showed moderate performance, with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct. They excelled in detecting happy emotions but failed to identify distracted behavior. Qwen2.5-VL-7B-Instruct performed well in recognizing confused expressions.

Conclusion: VLMs, particularly Qwen2.5-VL-7B-Instruct, show potential for practical applications in academic emotion detection, especially for identifying student confusion.

摘要: 学生的学业情绪显著影响其社会行为和学习表现。传统方法主要依赖监督式机器学习算法来自动准确分析这些情绪，但这些模型往往难以在不同情境中泛化，需要反复进行数据收集、标注和训练。视觉语言模型（VLMs）的出现提供了一种有前景的替代方案，通过零样本提示实现视觉识别任务的泛化，而无需微调。本研究探讨了VLMs在在线学习环境中通过面部表情分析学生学业情绪的潜力。我们采用了两种VLMs（Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct），通过零样本提示分析了5,000张描绘困惑、分心、快乐、中立和疲惫表情的图像。初步结果表明，两种模型在学业面部表情识别中表现出中等性能，其中Qwen2.5-VL-7B-Instruct优于Llama-3.2-11B-Vision-Instruct。值得注意的是，两种模型在识别学生的快乐情绪方面表现优异，但未能检测到分心行为。此外，Qwen2.5-VL-7B-Instruct在识别学生的困惑表情方面表现出相对较高的性能，突显了其在识别引起学生困惑的内容方面的实际应用潜力。

</details>


### [83] [PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting](https://arxiv.org/abs/2506.10335)
**中文标题：PointGS：基于高斯泼溅的点注意力感知稀疏视图合成**

*Lintao Xiang,Hongpei Zheng,Yating Huang,Qijun Yang,Hujun Yin*

Main category: cs.CV

TL;DR: The paper introduces PointGS, a framework for high-quality 3D rendering from sparse views using Gaussian splatting, outperforming NeRF-based methods and improving over existing 3DGS approaches.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian splatting (3DGS) methods require many calibrated views to avoid overfitting and maintain rendering quality. The paper aims to enable high-quality rendering from sparse views by enhancing point-wise feature representation.

Method: The method uses a stereo foundation model for camera pose estimation and dense point cloud reconstruction. It encodes multiscale 2D appearance features into 3D Gaussians and employs a self-attention-based point interaction network to enrich features, decoded into Gaussian parameters via MLPs for rendering.

Result: The method outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to state-of-the-art 3DGS methods.

Conclusion: PointGS enables real-time, high-quality rendering from sparse views, addressing limitations of existing 3DGS methods and setting a new benchmark for few-shot 3D rendering.

摘要: 3D高斯泼溅（3DGS）是一种创新的渲染技术，通过利用显式的3D场景表示，在渲染速度和视觉质量上超越了神经辐射场（NeRF）。现有的3DGS方法需要大量校准视图以生成一致且完整的场景表示。当输入视图有限时，3DGS容易过拟合训练视图，导致渲染质量显著下降。为解决这一限制，我们提出了一种点级特征感知的高斯泼溅框架，能够从稀疏训练视图中实现实时高质量渲染。具体而言，我们首先使用最新的立体基础模型估计准确的相机姿态并重建密集点云以初始化高斯分布。然后，我们通过采样和聚合稀疏输入的多尺度2D外观特征，为每个3D高斯编码颜色属性。为增强点级外观表示，我们设计了一个基于自注意力机制的点交互网络，使每个高斯点能够与其最近邻交互。这些丰富的特征随后通过两个轻量级多层感知器（MLP）解码为高斯参数以进行最终渲染。在多个基准测试上的广泛实验表明，我们的方法显著优于基于NeRF的方法，并在少样本设置下与最先进的3DGS方法相比具有竞争力。

</details>


### [84] [GeoCAD: Local Geometry-Controllable CAD Generation](https://arxiv.org/abs/2506.10337)
**中文标题：GeoCAD：局部几何可控的CAD生成**

*Zhanwei Zhang,Kaiyuan Liu,Junjie Liu,Wenxiao Wang,Binbin Lin,Liang Xie,Chen Shen,Deng Cai*

Main category: cs.CV

TL;DR: GeoCAD introduces a method for local geometry-controllable CAD generation, enabling users to modify specific parts of CAD models while following geometric instructions. It uses complementary captioning and LLMs to achieve high-quality, valid, and consistent results.


<details>
  <summary>Details</summary>
Motivation: Existing CAD generation methods struggle with following textual instructions or focusing on local parts, limiting design efficiency and customization. GeoCAD aims to overcome these challenges by providing a user-friendly, geometry-controllable solution.

Method: GeoCAD employs a complementary captioning strategy (vertex-based and VLLM-based) to annotate local parts. It trains LLMs to predict masked parts using geometric instructions and remaining parts as input, enabling local modifications during inference.

Result: Experiments show GeoCAD excels in generation quality, validity, and text-to-CAD consistency, successfully modifying local parts while adhering to geometric instructions.

Conclusion: GeoCAD offers an effective solution for local geometry-controllable CAD generation, enhancing design flexibility and efficiency.

摘要: 局部几何可控的计算机辅助设计（CAD）生成旨在自动修改CAD模型的局部部分，提高设计效率。同时，它确保新生成的局部部分的形状遵循用户特定的几何指令（例如，等腰直角三角形或切掉一角的矩形）。然而，现有方法在实现这一目标时面临挑战。具体而言，它们要么无法遵循文本指令，要么无法专注于局部部分。为解决这一局限，我们提出了GeoCAD，一种用户友好且局部几何可控的CAD生成方法。具体来说，我们首先提出了一种互补标注策略，为局部部分生成几何指令。该策略包括基于顶点的标注和基于VLLM的标注，分别用于系统标注简单和复杂的部分。通过这种方式，我们总共标注了约221k个不同的局部部分。在训练阶段，给定一个CAD模型，我们随机掩码一个局部部分。然后，利用其几何指令和剩余部分作为输入，提示大型语言模型（LLMs）预测被掩码的部分。在推理阶段，用户可以指定任何局部部分进行修改，同时遵循多种预定义的几何指令。大量实验证明了GeoCAD在生成质量、有效性和文本到CAD一致性方面的有效性。代码将在https://github.com/Zhanwei-Z/GeoCAD提供。

</details>


### [85] [UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models](https://arxiv.org/abs/2506.10342)
**中文标题：UrbanSense：基于视觉大语言模型的定量分析城市街景框架**

*Jun Yin,Jing Zhong,Peilin Li,Pengyu Zeng,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CV

TL;DR: The paper proposes UrbanSense, a vision-language-model-based framework for automated and scalable analysis of urban streetscape styles, validated by high accuracy in capturing stylistic differences.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized and objective methods in urban cultural studies, the study aims to leverage vision-language models for quantitative analysis of urban streetscapes.

Method: Developed UrbanSense, a multimodal framework using vision-language models, and constructed UrbanDiffBench, a dataset of urban streetscape images from different periods and regions.

Result: Over 80% of generated descriptions passed statistical tests, and high Phi scores (0.912 for cities, 0.833 for periods) confirmed the method's effectiveness in capturing stylistic differences.

Conclusion: UrbanSense offers a scientifically grounded approach to quantify urban style evolution, enhancing objectivity in urban form research.

摘要: 由于地理、时间、历史和社会政治因素的影响，不同城市的城市文化和建筑风格差异显著。理解这些差异对于预测城市未来演变至关重要。作为中国历史延续和现代创新的代表案例，北京和深圳为探索城市街景的转变提供了宝贵视角。然而，传统的城市文化研究方法通常依赖专家解读和历史文献，难以在不同背景下标准化。为此，我们提出了一种基于视觉语言模型的多模态研究框架，实现了对城市街景风格差异的自动化和可扩展分析。这种方法增强了城市形态研究的客观性和数据驱动性。本研究的贡献如下：首先，我们构建了UrbanDiffBench，一个包含不同时期和地区建筑图像的精选城市街景数据集。其次，我们开发了UrbanSense，首个基于视觉语言模型的城市街景分析框架，实现了城市风格表示的定量生成和比较。第三，实验结果表明，超过80%的生成描述通过了t检验（p小于0.05）。主观评价中的高Phi分数（城市为0.912，时期为0.833）证实了该方法捕捉细微风格差异的能力。这些结果凸显了该方法在量化和解释城市风格演变方面的潜力，为未来设计提供了科学依据。

</details>


### [86] [RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration](https://arxiv.org/abs/2506.10344)
**中文标题：RealKeyMorph：基于真实世界坐标的关键点用于分辨率无关的图像配准**

*Mina C. Moghadam,Alan Q. Wang,Omer Taub,Martin R. Prince,Mert R. Sabuncu*

Main category: cs.CV

TL;DR: RealKeyMorph (RKM) is a resolution-agnostic image registration method that avoids resampling by using keypoints in real-world coordinates, improving accuracy for medical images with varying resolutions.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning-based registration methods resample images to fixed resolutions, introducing artifacts. RKM aims to avoid this by working directly with raw data in real-world coordinates.

Method: RKM extends KeyMorph by training a network to learn keypoints in real-world coordinates using the scanner's affine matrix, enabling resolution-agnostic registration without resampling.

Result: RKM demonstrates improved performance for registering 2D abdominal MRIs and 3D brain volumes with varying resolutions.

Conclusion: RKM provides a robust, resolution-agnostic solution for medical image registration, avoiding artifacts from resampling.

摘要: 许多实际场景需要对空间分辨率不同的医学图像进行配准，这种差异可能源于像素间距、切片厚度和视野等图像采集参数的不同。然而，所有先前基于机器学习的配准技术都将图像重采样到固定分辨率。这种做法并不理想，因为重采样可能因插值而引入伪影。为此，我们提出了RealKeyMorph（RKM），一种分辨率无关的图像配准方法。RKM是KeyMorph的扩展，KeyMorph是一种通过训练网络学习给定图像对的对应关键点，然后通过闭式关键点匹配步骤推导对齐变换的配准框架。为了避免重采样并直接在原始数据上操作，RKM在扫描仪的真实世界坐标中输出关键点。为此，我们利用了扫描仪（如MRI机器）生成的仿射矩阵，该矩阵编码了从体素坐标到真实世界坐标的映射。通过将关键点转换到真实世界空间并将其整合到训练过程中，RKM有效地使提取的关键点与分辨率无关。在实验中，我们展示了RKM在正交2D腹部MRI堆栈配准任务以及分辨率变化的3D脑数据集上的优势。

</details>


### [87] [Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation](https://arxiv.org/abs/2506.10353)
**中文标题：Motion-R1：基于思维链推理和强化学习的人体运动生成**

*Runqi Ouyang,Haoyun Li,Zhenyuan Zhang,Xiaofeng Wang,Zheng Zhu,Guan Huang,Xingang Wang*

Main category: cs.CV

TL;DR: Motion-R1 introduces a Chain-of-Thought reasoning framework combined with reinforcement learning to improve text-to-motion generation, addressing issues of controllability, consistency, and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-motion generation methods lack deep linguistic understanding and logical reasoning, leading to inconsistent and less controllable motion outputs. Motion-R1 aims to enhance these aspects by integrating structured reasoning.

Method: Motion-R1 decomposes complex textual instructions into structured action paths using Chain-of-Thought reasoning and employs Group Relative Policy Optimization for joint optimization of reasoning and motion synthesis.

Result: Motion-R1 outperforms state-of-the-art methods in scenarios requiring nuanced semantic understanding and long-term coherence, demonstrating competitive performance across benchmarks.

Conclusion: The integration of Chain-of-Thought reasoning and reinforcement learning in Motion-R1 significantly improves motion generation, offering better controllability and semantic alignment.

摘要: 近年来，大型语言模型在自然语言理解和推理方面的进展为文本到运动的生成开辟了新途径。尽管现有方法在语义对齐和运动合成方面取得了显著进展，但它们通常依赖于端到端的映射策略，未能捕捉深层次的语言结构和逻辑推理。因此，生成的运动往往缺乏可控性、一致性和多样性。为解决这些问题，我们提出了Motion-R1，一个统一的运动-语言建模框架，集成了思维链机制。通过将复杂的文本指令显式分解为逻辑结构的动作路径，Motion-R1为运动生成提供了高级语义指导，显著增强了模型对多步骤、长时程和组合丰富指令的解释和执行能力。为训练模型，我们采用了Group Relative Policy Optimization，一种专为大型模型设计的强化学习算法，利用运动质量反馈联合优化推理链和运动合成。在多个基准数据集上的广泛实验表明，Motion-R1在需要细致语义理解和长期时间一致性的场景中，性能优于或与现有最优方法相当。代码、模型和数据将公开提供。

</details>


### [88] [FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device](https://arxiv.org/abs/2506.10361)
**中文标题：FaceLiVT：基于线性视觉变换器与结构重参数化的移动设备人脸识别**

*Novendra Setyawan,Chi-Chia Sun,Mao-Hsiu Hsu,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: FaceLiVT is a lightweight face recognition model combining CNN-Transformer architecture with Multi-Head Linear Attention, achieving high accuracy and speed on mobile devices.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient face recognition model for mobile devices that balances accuracy and computational complexity.

Method: Integrates hybrid CNN-Transformer architecture with Multi-Head Linear Attention (MHLA) and reparameterized token mixer to reduce computational cost.

Result: Outperforms state-of-the-art lightweight models on benchmarks (LFW, CFP-FP, etc.), with 8.6x speedup over EdgeFace and 21.2x over pure ViT models.

Conclusion: FaceLiVT provides a practical, high-performance solution for real-time face recognition on resource-constrained platforms.

摘要: 本文介绍了FaceLiVT，一种轻量级但强大的人脸识别模型，它结合了混合卷积神经网络（CNN）-变换器架构与创新的轻量级多头线性注意力（MHLA）机制。通过将MHLA与重参数化的令牌混合器结合，FaceLiVT在保持竞争力的准确性的同时有效降低了计算复杂度。在包括LFW、CFP-FP、AgeDB-30、IJB-B和IJB-C等具有挑战性的基准测试中进行的广泛评估表明，其性能优于最先进的轻量级模型。MHLA显著提高了推理速度，使FaceLiVT在移动设备上能够以较低的延迟实现高精度。具体而言，FaceLiVT比最近针对边缘设备优化的混合CNN-变换器模型EdgeFace快8.6倍，比纯ViT模型快21.2倍。凭借其平衡的设计，FaceLiVT为资源受限平台上的实时人脸识别提供了高效且实用的解决方案。

</details>


### [89] [FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.10366)
**中文标题：FSATFusion：基于频率-空间注意力Transformer的红外与可见光图像融合方法**

*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui,Yuhan Lyu*

Main category: cs.CV

TL;DR: FSATFusion is a novel end-to-end network for infrared and visible image fusion, using a frequency-spatial attention Transformer to improve feature extraction and fusion performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for infrared and visible image fusion rely on CNNs, which struggle with global context capture, leading to information loss and suboptimal fusion results.

Method: FSATFusion introduces a frequency-spatial attention Transformer (FSAT) module with an improved Transformer module (ITM) to enhance global context extraction and feature discrimination.

Result: FSATFusion outperforms state-of-the-art methods in fusion quality and efficiency, demonstrates strong generalization, and excels in downstream tasks like object detection.

Conclusion: FSATFusion offers superior performance and generalization for infrared and visible image fusion, validated by extensive experiments.

摘要: 红外与可见光图像融合（IVIF）因其在下游应用中的优异表现，受到学术界和工业界的广泛关注。现有的深度学习方法通常利用卷积神经网络提取图像特征，但卷积操作的固有局限性可能导致全局上下文信息丢失，从而限制融合性能。为解决这一问题，我们提出了一种端到端的融合网络——频率-空间注意力Transformer融合网络（FSATFusion）。FSATFusion包含一个频率-空间注意力Transformer（FSAT）模块，旨在有效提取源图像的判别性特征。该FSAT模块包含一个频率-空间注意力机制（FSAM），能够从特征图中提取显著特征。此外，我们还提出了一种改进的Transformer模块（ITM），以增强原始Transformer的全局上下文信息提取能力。通过定性和定量对比实验，我们证明了FSATFusion在融合质量和效率上优于其他最先进方法。此外，我们的网络在未经任何修改的情况下，成功应用于另外两项任务，验证了FSATFusion的出色泛化能力。最后，目标检测实验证明了FSATFusion在下游视觉任务中的优越性。代码已开源：https://github.com/Lmmh058/FSATFusion。

</details>


### [90] [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
**中文标题：从图像滤波的视角重新审视Transformer**

*Laziz U. Abdullaev,Maksim Tkachenko,Tan M. Nguyen*

Main category: cs.CV

TL;DR: The paper develops an image processing framework to interpret self-attention in Transformers, explaining components like positional encoding and residual connections, while also improving model performance.


<details>
  <summary>Details</summary>
Motivation: To establish a theoretical foundation for understanding self-attention in Transformers and enhance interpretability and performance.

Method: Introduces a unifying image processing framework to analyze self-attention and its components, along with architectural modifications inspired by image filtering.

Result: The framework improves interpretability and empirically enhances accuracy, robustness, and long-sequence understanding in language and vision tasks.

Conclusion: The study bridges gaps in understanding self-attention and demonstrates practical benefits of image processing-inspired modifications in Transformers.

摘要: 自注意力机制作为Transformer等最先进深度学习架构的核心，很大程度上是启发式驱动的，且难以解释。因此，建立坚实的理论基础以解释其成功与局限性已成为近年研究的重点。一些研究尝试从图像去噪和非参数回归的角度理解自注意力，但现有框架仍缺乏对自注意力及其变体中各组件的深入机制解释。本研究旨在通过开发一个统一的图像处理框架，不仅解释自注意力计算本身，还阐明位置编码和残差连接等组件的作用，包括后续变体。我们还指出了这两种概念间的潜在差异，并努力弥合这一差距。我们在Transformer中引入了两种独立的架构修改。虽然主要目标是可解释性，但实验表明，受图像处理启发的修改还能显著提高语言和视觉任务中的准确性、抗数据污染和对抗攻击的鲁棒性，以及长序列理解能力。

</details>


### [91] [Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial](https://arxiv.org/abs/2506.10386)
**中文标题：利用6自由度位姿基础模型进行海洋沉积物掩埋测绘**

*Jerry Yan,Chinmay Talegaonkar,Nicholas Antipa,Eric Terrill,Sophia Merrifield*

Main category: cs.CV

TL;DR: PoseIDON, a computer vision pipeline, uses deep foundation models and multiview photogrammetry to estimate 6DoF object pose and seafloor orientation from ROV video, achieving ~10 cm burial depth error.


<details>
  <summary>Details</summary>
Motivation: Accurate burial depth estimation of seafloor objects is challenging due to occlusion, poor visibility, and degradation, yet crucial for ecological risk assessment and pollutant transport analysis.

Method: Combines deep foundation model features with multiview photogrammetry to align CAD models with observed imagery and fit a local planar seafloor approximation.

Result: Validated on 54 objects, achieving ~10 cm mean burial depth error and revealing spatial burial patterns linked to sediment transport.

Conclusion: PoseIDON enables scalable, non-invasive seafloor burial mapping, aiding environmental assessments at contaminated sites.

摘要: 海底人为物体的掩埋状态为局部沉积动力学提供了见解，同时也对评估生态风险、潜在污染物迁移以及危险材料（如弹药）的回收或缓解策略的可行性至关重要。由于部分遮挡、能见度差和物体退化，从遥感图像中准确估计掩埋深度仍然困难。本研究提出了一种名为PoseIDON的计算机视觉流程，结合深度基础模型特征与多视角摄影测量技术，从ROV视频中估计物体的六自由度位姿及周围海底的朝向。掩埋深度通过将物体的CAD模型与观测图像对齐并拟合局部平面海底近似来推断。该方法在圣佩德罗盆地历史海洋倾倒场记录的54个物体（包括桶和弹药）的视频中进行了验证。模型实现了约10厘米的平均掩埋深度误差，并解析了反映底层沉积物迁移过程的空间掩埋模式。这种方法实现了海底掩埋的可扩展、非侵入式测绘，并支持污染场地的环境评估。

</details>


### [92] [DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba](https://arxiv.org/abs/2506.10390)
**中文标题：DART：用于视觉Transformer和Mamba的可微分动态自适应区域分词器**

*Shicheng Yin,Kaixuan Yin,Yang Liu,Weixing Chen,Liang Lin*

Main category: cs.CV

TL;DR: DART introduces a dynamic adaptive region tokenizer for Vision Transformer and Mamba, improving accuracy by 2.1% on ImageNet-1K with 45% FLOPs reduction by focusing on information-rich areas.


<details>
  <summary>Details</summary>
Motivation: Fixed-size patches in Vision Transformer and Mamba often encode excessive background or miss critical details, especially for sparsely distributed objects.

Method: DART uses learnable region scores and differentiable quantile operations to adaptively partition images into varying-sized patches, allocating denser tokens to information-rich areas.

Result: DART improves accuracy by 2.1% on DeiT (ImageNet-1K) with 45% FLOPs reduction, outperforming uniform token density methods.

Conclusion: DART consistently enhances accuracy with minimal computational overhead, offering an efficient alternative for vision tasks.

摘要: 近年来，非卷积模型如视觉Transformer（ViT）和视觉Mamba（Vim）在计算机视觉任务中取得了显著性能。然而，它们依赖固定大小的图像块，常常导致对背景区域的过度编码或对关键局部细节的遗漏，尤其是在信息稀疏分布时。为此，我们提出了一种完全可微分的动态自适应区域分词器（DART），能够根据内容自适应地将图像划分为不同大小的块。DART通过结合可学习的区域评分和分段可微分分位数操作，将更密集的令牌分配给信息丰富的区域。尽管仅增加了约100万（1M）额外参数，DART在DeiT（ImageNet-1K）上提升了2.1%的准确率。与那些通过均匀增加令牌密度来捕捉细粒度细节的方法不同，DART提供了一种更高效的替代方案，实现了45%的FLOPs减少和更优性能。在DeiT、Vim和VideoMamba上的大量实验表明，DART在几乎不增加甚至减少计算开销的情况下持续提升准确率。代码发布于https://github.com/HCPLab-SYSU/DART。

</details>


### [93] [ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion](https://arxiv.org/abs/2506.10391)
**中文标题：ReconMOST：基于观测引导扩散的多层海水温度重建**

*Yuanyi Song,Pumeng Lyu,Ben Fei,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.CV

TL;DR: ReconMOST proposes a data-driven guided diffusion model for multi-layer sea temperature reconstruction, leveraging historical simulation data and sparse observations to achieve accurate results despite missing data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for ocean temperature reconstruction face challenges like sparse data and computational costs, while ML-based approaches are limited to surface or local regions. ReconMOST aims to address these limitations for global, multi-layer reconstruction.

Method: The framework pre-trains an unconditional diffusion model on historical simulation data, then uses sparse observational data to guide the reverse diffusion process for accurate reconstruction, even in data-sparse regions.

Result: The method achieves MSE values of 0.049 (guidance), 0.680 (reconstruction), and 0.633 (total), demonstrating effectiveness and robustness in handling over 92.5% missing data.

Conclusion: ReconMOST extends ML-based reconstruction to global, multi-layer settings, offering high accuracy, spatial resolution, and generalization, validated by experiments on CMIP6 and EN4 data.

摘要: 准确重建海洋温度对于反映全球气候动态和支持海洋气象研究至关重要。传统方法因数据稀疏、算法复杂和计算成本高而面临挑战，而机器学习方法的应用仍局限于海表和局部区域的温度重建，且难以解决云遮挡等问题。为解决这些限制，本文提出ReconMOST，一种数据驱动的引导扩散模型框架，用于多层海水温度重建。具体而言，我们首先利用大量历史数值模拟数据预训练一个无条件扩散模型，使其能够学习物理一致的海洋温度场分布模式。在生成阶段，稀疏但高精度的现场观测数据被用作反向扩散过程的引导点，生成准确的重建结果。重要的是，在缺乏直接观测数据的区域，预训练中学习的物理一致的空间分布模式能够实现隐式引导和物理合理的重建。我们的方法将基于机器学习的海表温度重建扩展到全球多层场景，处理超过92.5%的缺失数据，同时保持重建精度、空间分辨率和优异的泛化能力。我们在CMIP6数值模拟数据上预训练模型，并在CMIP6和EN4分析数据上进行引导重建实验。均方误差（MSE）结果分别为引导0.049、重建0.680和总体0.633，证明了所提框架的有效性和鲁棒性。源代码可在https://github.com/norsheep/ReconMOST获取。

</details>


### [94] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395)
**中文标题：Pisces：一种用于图像理解与生成的自回归基础模型**

*Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie*

Main category: cs.CV

TL;DR: Pisces is an auto-regressive multimodal foundation model that excels in both image understanding and generation through a novel decoupled visual encoding architecture and tailored training techniques.


<details>
  <summary>Details</summary>
Motivation: Unified models often underperform compared to specialized models in image understanding and generation due to differences in visual features and training processes. Pisces aims to bridge this gap.

Method: Pisces uses a decoupled visual encoding architecture and optimized training techniques for multimodal generation, combined with meticulous data curation, pretraining, and finetuning.

Result: Pisces achieves competitive performance in over 20 image understanding benchmarks and robust generative capabilities on GenEval.

Conclusion: Pisces demonstrates the benefits of separate visual encoders and the synergy between image understanding and generation, advancing unified multimodal models.

摘要: 近年来，大型语言模型（LLMs）的进展使得多模态基础模型能够在统一框架内处理图像理解和生成任务。然而，统一模型在这两项任务中的表现往往逊色于专用模型。开发统一模型的一个关键挑战在于图像理解与生成所需的视觉特征存在固有差异，且每种模态所需的训练过程也截然不同。本文提出Pisces，一种自回归多模态基础模型，通过新颖的解耦视觉编码架构和针对多模态生成优化的训练技术来解决这一挑战。结合精细的数据整理、预训练和微调，Pisces在图像理解和生成任务中均表现出色。我们在20多个公开的图像理解基准上评估Pisces，结果显示其在广泛任务中表现优异。此外，在广泛采用的图像生成基准GenEval上，Pisces展现出强大的生成能力。我们的深入分析揭示了图像理解与生成之间的协同关系，以及使用独立视觉编码器的优势，推动了统一多模态模型领域的发展。

</details>


### [95] [It's Not the Target, It's the Background: Rethinking Infrared Small Target Detection via Deep Patch-Free Low-Rank Representations](https://arxiv.org/abs/2506.10425)
**中文标题：目标不重要，背景才是关键：基于深度无块低秩表示的红外小目标检测新思路**

*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: The paper introduces LRRNet, a novel end-to-end framework for infrared small target detection (IRSTD) that leverages the low-rank property of backgrounds, achieving superior performance in accuracy, robustness, and speed.


<details>
  <summary>Details</summary>
Motivation: IRSTD is challenging due to low signal-to-clutter ratios, diverse target morphologies, and weak visual cues. Existing deep learning methods struggle with unstable performance, prompting the need for a more effective approach.

Method: LRRNet adopts a compression--reconstruction--subtraction (CRS) paradigm to model low-rank background representations directly in the image domain, avoiding patch-based processing or explicit matrix decomposition.

Result: LRRNet outperforms 38 state-of-the-art methods in detection accuracy, robustness, and computational efficiency, achieving real-time performance at 82.34 FPS and demonstrating resilience to sensor noise.

Conclusion: The study successfully demonstrates the effectiveness of learning low-rank background structures directly with deep neural networks, offering a robust and efficient solution for IRSTD.

摘要: 红外小目标检测（IRSTD）在复杂背景下仍是一项长期挑战，原因包括低信杂比（SCR）、目标形态多样以及缺乏显著视觉特征。尽管近期深度学习方法试图学习判别性表示，但小目标的内在多变性和弱先验常导致性能不稳定。本文提出了一种新颖的端到端IRSTD框架LRRNet，利用红外图像背景的低秩特性。受杂乱场景物理可压缩性的启发，该方法采用压缩-重建-减除（CRS）范式，直接在图像域中建模结构感知的低秩背景表示，无需依赖基于块的处理或显式矩阵分解。据我们所知，这是首个通过深度神经网络端到端直接学习低秩背景结构的工作。在多个公共数据集上的大量实验表明，LRRNet在检测精度、鲁棒性和计算效率方面优于38种最先进方法，且以平均82.34 FPS的速度实现实时性能。在具有挑战性的NoisySIRST数据集上的评估进一步证实了模型对传感器噪声的鲁棒性。源代码将在论文录用后公开。

</details>


### [96] [MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment](https://arxiv.org/abs/2506.10430)
**中文标题：MF2Summ：基于时间对齐的多模态融合视频摘要方法**

*Shuo wang,Jihao Zhang*

Main category: cs.CV

TL;DR: MF2Summ is a multimodal video summarization model that integrates visual and auditory features using cross-modal attention and temporal alignment, outperforming existing methods on SumMe and TVSum datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional video summarization methods rely on a single modality (visual), missing the semantic richness of videos. MF2Summ aims to leverage multimodal content (visual and auditory) for more effective summarization.

Method: MF2Summ uses a five-stage process: feature extraction (GoogLeNet for visual, SoundNet for auditory), cross-modal attention interaction, feature fusion, segment prediction (importance, location, center-ness), and key shot selection (NMS and KTS algorithm).

Result: MF2Summ improves F1-scores by 1.9% and 0.6% over DSNet on SumMe and TVSum datasets, respectively, and performs competitively against other state-of-the-art methods.

Conclusion: MF2Summ demonstrates the effectiveness of multimodal fusion and temporal alignment in video summarization, achieving superior performance by integrating visual and auditory features.

摘要: 随着在线视频内容的迅速增长，有效的视频摘要技术变得尤为重要。传统方法通常仅依赖单一模态（通常是视觉），难以捕捉视频的全部语义丰富性。本文提出了MF2Summ，一种基于多模态内容理解的新型视频摘要模型，整合了视觉和听觉信息。MF2Summ采用五阶段流程：特征提取、跨模态注意力交互、特征融合、片段预测和关键镜头选择。视觉特征通过预训练的GoogLeNet模型提取，听觉特征则通过SoundNet提取。融合机制的核心包括跨模态Transformer和对齐引导的自注意力Transformer，旨在有效建模模态间依赖和时间对应关系。预测片段的重要性、位置和中心性后，使用非极大值抑制（NMS）和核时间分割（KTS）算法选择关键镜头。在SumMe和TVSum数据集上的实验结果表明，MF2Summ表现出色，F1分数分别比DSNet模型提高了1.9%和0.6%，并优于其他最先进方法。

</details>


### [97] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
**中文标题：面向缺失模态和分布偏移的鲁棒多模态情感识别**

*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

TL;DR: The paper proposes CIDer, a robust Multimodal Emotion Recognition (MER) framework, addressing missing modalities and OOD data through self-distillation and causal inference, achieving efficient performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods struggle with missing modalities and OOD data, often using complex models or excessive parameters. The paper aims to develop a practical and efficient solution.

Method: CIDer combines Model-Specific Self-Distillation (MSSD) for robustness under missing modalities and Model-Agnostic Causal Inference (MACI) for OOD generalization, with modules like WSAM and MCT for efficiency.

Result: CIDer outperforms state-of-the-art methods in handling missing modalities and OOD data, with fewer parameters and faster training.

Conclusion: CIDer provides a robust and efficient solution for MER, addressing both missing modalities and OOD challenges, with potential for broader applications.

摘要: 近年来，多模态情感识别（MER）在应对模态缺失和分布外（OOD）数据时面临挑战。现有方法通常依赖于特定模型或引入过多参数，限制了其实用性。为解决这些问题，我们提出了一种新型鲁棒MER框架——因果推断蒸馏器（CIDer），并引入了一项新任务——随机模态特征缺失（RMFM），以泛化模态缺失的定义。CIDer整合了两个关键组件：模型特定自蒸馏（MSSD）模块和模型无关因果推断（MACI）模块。MSSD通过低层特征、注意力图和高层表示的权重共享自蒸馏方法，增强了RMFM任务下的鲁棒性。此外，词级自对齐注意力模块（WSAM）降低了计算复杂度，而多模态复合变换器（MCT）实现了高效的多模态融合。为应对OOD挑战，MACI采用定制因果图，通过多模态因果模块（MCM）和细粒度反事实文本来缓解标签和语言偏差。值得注意的是，MACI可以独立地以最少的额外参数提升OOD泛化能力。此外，我们还引入了重新划分的MER OOD数据集。实验结果表明，与现有方法相比，CIDer在RMFM和OOD场景下均表现出鲁棒性能，且参数更少、训练更快。本工作的实现已公开于https://github.com/gw-zhong/CIDer。

</details>


### [98] [Rethinking Generative Human Video Coding with Implicit Motion Transformation](https://arxiv.org/abs/2506.10453)
**中文标题：重新思考基于隐式运动变换的生成式人体视频编码**

*Bolin Chen,Ru-Ling Liao,Jie Chen,Yan Ye*

Main category: cs.CV

TL;DR: The paper proposes an Implicit Motion Transformation (IMT) paradigm to improve Generative Human Video Coding (GHVC), addressing challenges in human body video compression by replacing explicit motion guidance with implicit motion features for better reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Traditional generative video codecs using explicit motion fields struggle with human body videos due to complex motion patterns, leading to distortions. The paper aims to overcome this by exploring implicit motion transformation for better compression and reconstruction.

Method: The paper introduces IMT, which transforms compact visual features into implicit motion guidance for human body signal reconstruction, avoiding the limitations of explicit motion-based methods.

Result: Experiments show that IMT improves GHVC, achieving high-efficiency compression and high-fidelity synthesis compared to explicit motion-based approaches.

Conclusion: IMT effectively addresses the limitations of explicit motion in GHVC, offering a promising direction for human body video compression with better reconstruction quality.

摘要: 与传统基于混合的视频编解码器相比，生成式视频编解码器通过将高维信号演化为紧凑的特征表示以实现编码端的比特流紧凑性，并在解码端开发显式运动场作为中间监督以实现高质量重建。这一范式在人脸视频压缩中取得了显著成功。然而，与面部视频相比，人体视频由于其更复杂多样的运动模式而面临更大挑战，即在生成式人体视频编码（GHVC）中使用显式运动指导时，重建结果可能出现严重失真和运动不准确。因此，本文强调了显式运动方法在人体视频压缩中的局限性，并研究了借助隐式运动变换（IMT）提升GHVC性能的方法。具体而言，我们提出将复杂的人体信号表征为紧凑的视觉特征，并将这些特征转化为隐式运动指导以进行信号重建。实验结果表明，所提出的IMT范式有效，能够帮助GHVC实现高效压缩和高保真合成。

</details>


### [99] [Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance](https://arxiv.org/abs/2506.10459)
**中文标题：利用3D结构不变变换和中间特征距离提升高光谱图像分类的对抗迁移性**

*Chun Liu,Bingqian Zhu,Tao Xu,Zheng Zheng,Zheng Li,Wei Yang,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: The paper proposes a method to enhance adversarial transferability for HSI classification by using 3D structure-invariant transformations and intermediate feature distance loss.


<details>
  <summary>Details</summary>
Motivation: HSIs differ from natural images due to their high-dimensional spectral data, and existing adversarial attack methods struggle to fully utilize HSI structural and feature information, limiting transferability.

Method: The method involves random block division in spatial and spectral dimensions, applying transformations to increase diversity, and designing a feature distancing loss targeting intermediate layers to disrupt true class features.

Result: The adversarial examples generated achieve effective transferability on two public HSI datasets and maintain robust performance under defense strategies.

Conclusion: The proposed method successfully enhances adversarial transferability for HSI classification by leveraging structural and feature information.

摘要: 深度神经网络（DNNs）易受对抗攻击的影响，这对基于DNNs的高光谱图像（HSI）分类技术提出了安全挑战。在自然图像领域，已有大量基于迁移的对抗攻击方法被研究。然而，由于高维和丰富的频谱信息，HSIs与自然图像不同。目前关于HSI对抗样本的研究仍然有限，并且在充分利用图像的结构和特征信息方面面临挑战。为解决这些问题，本文提出了一种新方法，以增强HSI分类模型的对抗样本的迁移性。首先，在保持图像结构不变的情况下，该方法在空间和频谱维度上随机将图像划分为块。然后，逐块应用多种变换以增加输入多样性并减轻过拟合。其次，设计了一种针对中间层的特征距离损失，将原始样本的放大特征与对抗样本的特征之间的距离作为主要损失，而输出层预测作为辅助损失。这引导扰动破坏对抗样本中真实类别的特征，有效增强了迁移性。大量实验表明，所提方法生成的对抗样本在两个公开的HSI数据集上实现了对黑盒模型的有效迁移。此外，该方法在防御策略下仍保持强大的攻击性能。

</details>


### [100] [Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization](https://arxiv.org/abs/2506.10463)
**中文标题：初始位置的重要性：神经网络量化中更好的权重初始化研究**

*Stone Yun,Alexander Wong*

Main category: cs.CV

TL;DR: The paper explores how weight initialization impacts neural network quantization robustness, proposing a Graph Hypernetwork (GHN) method (GHN-QAT) to improve quantized accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite advances in DNN quantization techniques, little attention has been paid to how weight initialization affects quantization robustness. The study aims to fill this gap by investigating initialization methods for better quantized performance.

Method: The study examines various weight initializations for CNN building blocks and introduces GHN-QAT, a method using Graph Hypernetworks to predict parameters for quantized DNNs, with finetuning for improved accuracy.

Result: GHN-QAT significantly improves quantized accuracy, even for 4-bit and 2-bit quantization, outperforming random initialization.

Conclusion: GHN-QAT provides a novel approach to quantization-aware weight initialization, with potential for further integration into quantization-aware training.

摘要: 深度神经网络（DNN）量化作为一种快速、高效的推理工具，在限制机器学习（ML）模型推理成本方面具有重要意义。诸如正则化、量化感知训练和量化鲁棒性惩罚等量化特定模型开发技术极大地提升了现代DNN的准确性和鲁棒性。然而，对于量化训练初始条件的改进却鲜有探索。正如随机权重初始化已被证明对浮点模型的测试准确性有显著影响，不同的权重初始化方法也可能影响训练模型的量化鲁棒性。我们进行了一项广泛研究，探讨了不同权重初始化对高效CNN常用构建模块的影响。分析表明，即使CNN架构各异，随机权重初始化器的选择也会显著影响最终的量化鲁棒性。接下来，我们探索了一种新的量化鲁棒CNN初始化方法——使用图超网络（GHN）预测量化DNN的参数。除了证明GHN预测的参数在常规float32预训练后具有量化鲁棒性外，我们还发现，对GHN进行微调以预测量化图的参数（称为GHN-QAT）可以进一步提高CNN的量化准确性。值得注意的是，GHN-QAT在4位量化中显示出显著的准确性提升，在2位量化中甚至优于随机初始化。据我们所知，这是首次对量化感知DNN权重初始化的深入研究。GHN-QAT为量化DNN模型设计提供了一种新方法。未来的研究，如将GHN-QAT初始化的参数用于量化感知训练，可以进一步简化DNN量化流程。

</details>


### [101] [MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models](https://arxiv.org/abs/2506.10465)
**中文标题：MedSeg-R：基于多模态大语言模型的医学图像推理分割**

*Yu Huang,Zelin Peng,Yichen Zhao,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: The paper introduces MedSeg-R, a framework combining multimodal large language models (MLLMs) with segmentation tasks to generate precise masks from complex medical instructions, supported by a new dataset MedSeg-QA.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation models lack reasoning capabilities for complex clinical questions, limiting their diagnostic utility. MLLMs show promise but struggle with precise mask generation.

Method: MedSeg-R uses a global context understanding module to interpret images and instructions, and a pixel-level grounding module to generate segmentation masks and textual responses.

Result: MedSeg-R outperforms benchmarks, achieving high segmentation accuracy and interpretable textual analysis, validated on the MedSeg-QA dataset.

Conclusion: MedSeg-R bridges the gap between reasoning and segmentation in medical images, offering a robust tool for clinical diagnosis.

摘要: 医学图像分割对临床诊断至关重要，但现有模型依赖显式人工指令且缺乏主动推理能力。尽管多模态大语言模型（MLLMs）在医学问答任务中有所改进，但多数方法难以生成精确的分割掩码，限制了其在自动医学诊断中的应用。本文提出医学图像推理分割任务，旨在基于复杂隐式医学指令生成分割掩码。为此，我们提出MedSeg-R，一种端到端框架，利用MLLMs的推理能力解析临床问题，同时生成精确的分割掩码。其核心包括：1）全局上下文理解模块，解析图像并理解复杂医学指令以生成多模态中间标记；2）像素级定位模块，解码标记以生成精确分割掩码和文本响应。此外，我们提出MedSeg-QA，一个专为医学图像推理分割任务设计的大规模数据集，包含10,000多对图像-掩码和多轮对话，通过大语言模型自动标注并经医生审核。实验表明，MedSeg-R在多个基准测试中表现优异，实现了高分割精度，并支持医学图像的可解释文本分析。

</details>


### [102] [LLMs Are Not Yet Ready for Deepfake Image Detection](https://arxiv.org/abs/2506.10474)
**中文标题：大型语言模型尚未准备好用于深度伪造图像检测**

*Shahroz Tariq,David Nguyen,M. A. P. Chamikara,Tingmin Wu,Alsharif Abuadbba,Kristen Moore*

Main category: cs.CV

TL;DR: Current vision-language models (VLMs) like ChatGPT, Claude, Gemini, and Grok show potential but are unreliable for standalone deepfake detection due to limitations in accuracy and reasoning depth.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of VLMs in detecting deepfakes, given their growing use in various domains and the increasing threat of sophisticated deepfakes to media integrity.

Method: A structured zero-shot evaluation of four VLMs on three deepfake types (faceswap, reenactment, synthetic generation) using a benchmark of authentic and manipulated images.

Result: VLMs can generate coherent explanations and detect surface-level anomalies but fail as standalone systems, with issues like overemphasis on stylistic elements and vulnerability to misleading patterns.

Conclusion: While VLMs are not yet reliable for autonomous deepfake detection, their interpretability and contextual analysis strengths suggest potential for hybrid or human-in-the-loop frameworks.

摘要: 深度伪造技术的日益复杂对媒体完整性和公众信任的维护构成了重大挑战。与此同时，视觉语言模型（VLMs）——即具备视觉推理能力的大型语言模型——已成为多个领域的有前景工具，引发了对其在深度伪造检测中适用性的兴趣。本研究对四种主流VLMs（ChatGPT、Claude、Gemini和Grok）进行了结构化零样本评估，重点关注三种主要深度伪造类型：换脸、重演和合成生成。通过精心构建的基准数据集（包含来自不同来源的真实和篡改图像），我们评估了每个模型的分类准确性和推理深度。分析表明，尽管VLMs能够生成连贯的解释并检测表面异常，但它们尚不能作为独立的检测系统可靠使用。我们强调了关键失败模式，例如对风格元素的过度关注以及对误导性视觉模式（如复古美学）的脆弱性。然而，VLMs在可解释性和上下文分析方面表现出优势，表明它们有潜力增强法证工作流程中的人类专业知识。这些见解表明，尽管通用模型目前缺乏自主深度伪造检测所需的可靠性，但它们有望成为混合或人机协作检测框架的重要组成部分。

</details>


### [103] [Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation](https://arxiv.org/abs/2506.10488)
**中文标题：乐谱基准：标准化的光学音乐识别评估**

*Juan C. Martinez-Sevilla,Joan Cerveto-Serrano,Noelia Luna,Greg Chapman,Craig Sapp,David Rizo,Jorge Calvo-Zaragoza*

Main category: cs.CV

TL;DR: The paper introduces the Sheet Music Benchmark (SMB) dataset and the OMR Normalized Edit Distance (OMR-NED) metric to standardize and improve Optical Music Recognition (OMR) evaluation.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized datasets and metrics for OMR research motivated the creation of SMB and OMR-NED to enable clearer comparisons and better error analysis.

Method: The authors developed SMB, a dataset of 685 pages with diverse musical textures, and OMR-NED, a metric for detailed error analysis in OMR. Baseline experiments were conducted using standardized splits of SMB.

Result: The SMB dataset and OMR-NED metric provide a standardized framework for evaluating OMR performance, facilitating clearer comparisons and identifying optimal approaches.

Conclusion: The work fills a gap in OMR evaluation by introducing standardized tools (SMB and OMR-NED), supported by baseline experiments, to advance OMR research.

摘要: 本研究介绍了乐谱基准（SMB），一个包含685页的专门用于评估光学音乐识别（OMR）研究的数据集。SMB涵盖了多种音乐纹理，包括单音、钢琴曲、四重奏等，均采用Common Western Modern Notation并以Humdrum **kern格式编码。同时，我们提出了OMR标准化编辑距离（OMR-NED），这是一种专门用于评估OMR性能的新指标。OMR-NED基于广泛使用的符号错误率（SER），提供了对音符头、横梁、音高、变音记号等关键音乐元素的细粒度错误分析。OMR-NED提供的数值分数便于清晰比较，帮助研究人员和终端用户识别最优的OMR方法。我们的工作填补了OMR评估领域的长期空白，并通过使用标准化的SMB数据集分割进行训练和评估最新方法的基线实验来支持我们的贡献。

</details>


### [104] [Class-Incremental Learning for Honey Botanical Origin Classification with Hyperspectral Images: A Study with Continual Backpropagation](https://arxiv.org/abs/2506.10489)
**中文标题：基于高光谱图像的蜂蜜植物来源分类的类增量学习：一种持续反向传播的研究**

*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: The paper proposes a novel technique combining continual backpropagation (CB) with class-incremental learning (CIL) to improve honey botanical origin classification using hyperspectral images, achieving performance gains of 1-7%.


<details>
  <summary>Details</summary>
Motivation: Honey's botanical origin affects its market value and consumer benefits, but collecting all varieties for training is impractical. CIL techniques are needed to address this challenge.

Method: The study compares multiple CIL algorithms on a honey hyperspectral dataset and introduces CB to reinitialize less-used neurons, enhancing plasticity.

Result: CB improved most CIL methods by 1-7%, demonstrating effectiveness in addressing loss-of-plasticity.

Conclusion: The CB-enhanced CIL approach is a promising solution for incremental learning in honey classification, offering practical benefits for real-world applications.

摘要: 蜂蜜是全球市场中的重要商品。不同植物来源的蜂蜜具有多样化的风味和健康益处，因此市场价值不同。开发准确有效的植物来源区分技术对保护消费者利益至关重要。然而，一次性收集所有蜂蜜品种来训练模型是不现实的。为此，研究人员开发了类增量学习（CIL）技术来解决这一挑战。本研究在真实世界的高光谱蜂蜜数据集上检验并比较了多种CIL算法。同时提出了一种新技术，通过结合持续反向传播（CB）算法来提升类增量学习算法的性能。CB方法通过重新初始化部分较少使用的隐藏神经元，为神经网络注入变异性，从而解决塑性丧失问题。实验表明，CB将大多数CIL方法的性能提高了1-7%。

</details>


### [105] [Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation](https://arxiv.org/abs/2506.10503)
**中文标题：语义定位引导的Segment Anything模型用于参考遥感图像分割**

*Shuyang Li,Shuang Wang,Zhuangzhuang Sun,Jing Xiao*

Main category: cs.CV

TL;DR: The paper proposes PSLG-SAM, a two-stage framework for Reference Remote Sensing Image Segmentation (RRSIS), combining coarse localization and fine segmentation to reduce annotation burden and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current RRSIS methods face challenges like dense annotation requirements and complex scene interpretation. The paper aims to address these issues by simplifying the task into two stages and leveraging the Segment Anything Model (SAM).

Method: PSLG-SAM decomposes RRSIS into coarse localization (using a visual grounding network) and fine segmentation (guided by SAM with clustering-based foreground points and iterative mask optimization). The second stage can be train-free.

Result: PSLG-SAM achieves significant performance improvements on datasets RRSIS-D and RRSIS-M, surpassing state-of-the-art models.

Conclusion: The proposed framework effectively reduces annotation burden and improves segmentation accuracy, demonstrating the potential of combining localization and segmentation for RRSIS tasks.

摘要: 参考遥感图像分割（RRSIS）任务根据文本描述生成图像中指定对象的分割掩码，引起了广泛关注和研究兴趣。当前的RRSIS方法依赖于多模态融合主干和语义分割头，但面临密集标注要求和复杂场景解释等挑战。为解决这些问题，我们提出了一个名为“提示生成的语义定位引导Segment Anything模型”（PSLG-SAM）的框架，将RRSIS任务分解为粗定位和精细分割两个阶段。在粗定位阶段，视觉定位网络粗略定位文本描述的对象。在精细分割阶段，第一阶段的坐标引导Segment Anything模型（SAM），并通过基于聚类的前景点生成器和掩码边界迭代优化策略实现精确分割。值得注意的是，第二阶段可以无需训练，显著减轻RRSIS任务的标注数据负担。此外，将RRSIS任务分解为两个阶段可以专注于特定区域分割，避免复杂场景的干扰。我们还贡献了一个高质量、多类别的手动标注数据集。在两个数据集（RRSIS-D和RRSIS-M）上的实验验证表明，PSLG-SAM实现了显著的性能提升，并超越了现有的最先进模型。我们的代码将公开提供。

</details>


### [106] [J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft](https://arxiv.org/abs/2506.10505)
**中文标题：J-DDL：战斗机表面损伤检测与定位系统**

*Jin Huang,Mingqiang Wei,Zikuan Li,Hangyu Qu,Wei Zhao,Xinyu Bai*

Main category: cs.CV

TL;DR: J-DDL is a smart system for detecting and localizing surface damage on fighter aircraft using 2D images and 3D point clouds, featuring a YOLO-based detection network with innovations like Fasternet blocks, EMA modules, and Inner-CIOU loss.


<details>
  <summary>Details</summary>
Motivation: Manual inspections of fighter aircraft surfaces are inefficient and inconsistent due to large areas and structural complexity, necessitating an automated solution.

Method: J-DDL integrates 2D images and 3D point clouds, using a YOLO-based network with Fasternet blocks, EMA modules, and Inner-CIOU loss for precise damage detection and 3D localization.

Result: The system achieves accurate damage detection and localization, validated by experiments, and introduces the first public dataset for aircraft damage.

Conclusion: J-DDL enhances inspection efficiency and coverage, advancing automated aircraft inspection technologies.

摘要: 确保战斗机的安全性和延长其使用寿命需要进行频繁且彻底的检查。虽然表面缺陷检测对人类检查员是可行的，但由于飞机表面积大、结构复杂以及维护操作的需求，手动方法在可扩展性、效率和一致性方面面临关键限制。我们提出了一种智能战斗机表面损伤检测与定位系统，称为J-DDL。J-DDL集成了通过激光扫描仪和相机组合系统捕获的整个飞机表面的2D图像和3D点云，以实现精确的损伤检测和定位。我们系统的核心是基于YOLO架构的新型损伤检测网络，专门针对2D飞机图像中的表面缺陷识别进行了优化。关键创新包括用于高效特征提取的轻量级Fasternet模块、结合高效多尺度注意力（EMA）模块的优化颈部架构，以及引入新型损失函数Inner-CIOU以提高检测精度。在2D图像中检测到损伤后，系统将识别的异常映射到对应的3D点云上，从而在飞机表面实现准确的3D缺陷定位。我们的J-DDL不仅简化了检查流程，还确保了对大型复杂飞机外表面更全面和详细的覆盖。为了促进该领域的进一步发展，我们开发了首个公开可用的专注于飞机损伤的数据集。实验评估验证了我们框架的有效性，强调了其在显著推进自动飞机检查技术方面的潜力。

</details>


### [107] [CogStream: Context-guided Streaming Video Question Answering](https://arxiv.org/abs/2506.10516)
**中文标题：CogStream：基于上下文引导的流媒体视频问答**

*Zicheng Zhao,Kangyu Wang,Shijie Li,Rui Qian,Weiyao Lin,Huabin Liu*

Main category: cs.CV

TL;DR: The paper introduces CogStream, a task for streaming video reasoning, and proposes CogReasoner, a model that efficiently identifies relevant historical context to answer questions about streaming videos, supported by a densely annotated dataset.


<details>
  <summary>Details</summary>
Motivation: Existing Video Large Language Models (Vid-LLMs) struggle with streaming video reasoning due to computational inefficiency and irrelevant context inclusion, necessitating a focused approach.

Method: The paper presents CogStream, a task simulating real-world streaming video scenarios, and CogReasoner, a model using visual stream compression and historical dialogue retrieval to identify relevant context.

Result: Extensive experiments validate CogReasoner's effectiveness in handling the CogStream task.

Conclusion: The paper successfully addresses streaming video reasoning challenges with CogStream and CogReasoner, supported by a robust dataset.

摘要: 尽管视频大型语言模型（Vid-LLMs）在多模态理解方面取得了进展，但由于其对上下文信息的依赖，流媒体视频推理仍面临挑战。现有范式将所有可用的历史上下文信息输入Vid-LLMs，导致视觉数据处理的计算负担显著增加。此外，无关上下文的引入会分散模型对关键细节的注意力。本文提出了一项具有挑战性的任务，称为“基于上下文引导的流媒体视频推理”（CogStream），该任务模拟现实中的流媒体视频场景，要求模型识别最相关的历史上下文信息以推断当前流媒体问题的答案。为支持CogStream，我们提出了一个密集标注的数据集，包含广泛且层次化的问题-答案对，由半自动流程生成。此外，我们还提出了CogReasoner作为基线模型，它通过利用视觉流压缩和历史对话检索高效地解决了这一任务。大量实验证明了该方法的有效性。代码即将发布。

</details>


### [108] [ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation](https://arxiv.org/abs/2506.10524)
**中文标题：ALBERT：基于双向编码器表示和高级定位的汽车损伤评估**

*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: ALBERT is an advanced instance segmentation model for car damage and part segmentation, using bidirectional encoder representations and localization to classify 26 damage types, 7 fake variants, and segment 61 car parts with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a precise model for automotive damage evaluation that can distinguish real from fake damages and segment car parts for intelligent inspection applications.

Method: ALBERT leverages bidirectional encoder representations and advanced localization mechanisms, trained on a large annotated dataset with 26 damage types, 7 fake variants, and 61 car parts.

Result: The model achieves strong performance in segmentation accuracy and damage classification, suitable for automotive inspection.

Conclusion: ALBERT's capabilities in damage evaluation and part segmentation make it a promising tool for intelligent automotive inspection systems.

摘要: 本文介绍了ALBERT，一种专为全面汽车损伤和部件分割设计的实例分割模型。利用双向编码器表示的能力，ALBERT结合了高级定位机制，能够准确识别和区分真实与虚假损伤，并分割单个汽车部件。该模型在一个大规模、丰富标注的汽车数据集上进行了训练，该数据集将损伤分为26种类型，识别了7种虚假损伤变体，并分割了61个不同的汽车部件。我们的方法在分割准确性和损伤分类方面表现出色，为智能汽车检测和评估应用铺平了道路。

</details>


### [109] [SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance](https://arxiv.org/abs/2506.10528)
**中文标题：SLICK：基于知识增强的汽车保险损伤分割的选择性定位与实例校准**

*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: SLICK is a novel framework for car damage segmentation that uses structural priors and domain knowledge to achieve precise and robust results in automotive insurance workflows.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address real-world challenges in car damage segmentation, such as occlusion, deformation, and noise, by leveraging structural priors and domain knowledge.

Method: SLICK introduces five components: (1) Selective Part Segmentation, (2) Localization-Aware Attention blocks, (3) Instance-Sensitive Refinement head, (4) Cross-Channel Calibration, and (5) a Knowledge Fusion Module.

Result: Experiments show SLICK's superior segmentation performance, robustness, and practical applicability in automotive insurance workflows.

Conclusion: SLICK effectively tackles car damage segmentation challenges, demonstrating high accuracy and generalization for real-world applications.

摘要: 我们提出了SLICK，这是一种新颖的框架，用于精确且稳健的汽车损伤分割，利用结构先验和领域知识解决现实世界中的汽车检测挑战。SLICK包含五个关键组件：(1) 选择性部件分割，通过高分辨率语义主干和结构先验指导，即使在遮挡、变形或油漆脱落情况下也能实现手术级精度；(2) 定位感知注意力块，动态聚焦于损伤区域，增强复杂街景中的细粒度损伤检测；(3) 实例敏感细化头，利用全景线索和形状先验分离重叠或相邻部件，实现精确边界对齐；(4) 跨通道校准，通过多尺度通道注意力放大细微损伤信号（如划痕和凹痕），同时抑制噪声（如反射和贴花）；(5) 知识融合模块，整合合成碰撞数据、部件几何和真实保险数据集，以提高泛化能力并有效处理罕见情况。在大规模汽车数据集上的实验证明了SLICK在分割性能、鲁棒性以及保险和汽车检测工作流程中的实际适用性方面的优越性。

</details>


### [110] [ContextRefine-CLIP for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2025](https://arxiv.org/abs/2506.10550)
**中文标题：ContextRefine-CLIP：用于EPIC-KITCHENS-100多实例检索挑战2025**

*Jing He,Yiqing Wang,Lingling Li,Kexin Zhang,Puhua Chen*

Main category: cs.CV

TL;DR: CR-CLIP introduces a cross-modal attention flow module to refine visual-textual features, achieving state-of-the-art performance on EPIC-KITCHENS-100 without ensemble learning.


<details>
  <summary>Details</summary>
Motivation: To improve cross-modal retrieval by dynamically refining visual and textual features through bidirectional interaction, addressing limitations in existing models.

Method: CR-CLIP enhances AVION with a cross-modal attention flow module for bidirectional feature refinement and uses Symmetric Multi-Similarity Loss for semantic alignment.

Result: Achieves 66.78mAP and 82.08nDCG on EPIC-KITCHENS-100, outperforming baselines.

Conclusion: CR-CLIP is effective for cross-modal retrieval, validated by superior performance on EPIC-KITCHENS-100.

摘要: 本报告介绍了ContextRefine-CLIP（CR-CLIP），一种高效的视觉-文本多实例检索任务模型。该方法基于双编码器AVION，引入跨模态注意力流模块，实现视觉与文本特征的双向动态交互与精炼，生成更具上下文感知的联合表征。针对EPIC-KITCHENS-100等任务提供的软标签相关矩阵，CR-CLIP结合对称多相似性损失，利用精炼特征实现更准确的语义对齐与优化。在不使用集成学习的情况下，CR-CLIP模型在EPIC-KITCHENS-100公开排行榜上达到66.78mAP和82.08nDCG，显著优于基线模型，充分验证了其在跨模态检索中的有效性。代码将在https://github.com/delCayr/ContextRefine-Clip开源发布。

</details>


### [111] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/abs/2506.10559)
**中文标题：从图像到洞察：用通俗语言解释生物多样性监测**

*Yutong Zhou,Masahiro Ryo*

Main category: cs.CV

TL;DR: The paper proposes an end-to-end visual-to-causal framework to explain species habitat preferences using AI, integrating species recognition, causal inference, and human-readable explanations.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in ecological workflows by providing accessible, interpretable causal insights into species habitat preferences for non-specialists.

Method: The framework combines species recognition, global occurrence retrieval, pseudo-absence sampling, climate data extraction, causal structure discovery, and human-readable explanation generation using structured templates and large language models.

Result: Early results demonstrate the framework's potential in generating statistically grounded, human-understandable explanations for species habitat preferences, tested on a bee and a flower species.

Conclusion: The multimodal AI framework shows promise for enhancing biodiversity monitoring by providing clear, causal habitat explanations, supported by ecological modeling practices.

摘要: 解释物种为何生活在特定地点对于理解生态系统和保护生物多样性至关重要。然而，现有的生态工作流程分散且通常对非专业人士难以理解。我们提出了一种端到端的视觉到因果框架，将物种图像转化为关于其栖息地偏好的可解释因果洞察。该系统整合了物种识别、全球分布检索、伪缺失采样和气候数据提取。然后，我们利用现代因果推断方法发现环境特征之间的因果结构，并估计它们对物种分布的影响。最后，我们从结构化模板和大型语言模型中生成具有统计基础、人类可读的因果解释。我们以蜜蜂和花卉物种为例展示了该框架，并报告了正在进行项目的初步结果，展示了多模态人工智能助手在描述物种栖息地方面的潜力，其背后是推荐的生态建模实践。

</details>


### [112] [Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics](https://arxiv.org/abs/2506.10564)
**中文标题：比较分布时的尾部平衡：全面公平指数（CEI）及其在操作人脸生物特征偏差评估中的应用**

*Imanol Solano,Julian Fierrez,Aythami Morales,Alejandro Peña,Ruben Tolosana,Francisco Zamora-Martinez,Javier San Agustin*

Main category: cs.CV

TL;DR: The paper introduces the Comprehensive Equity Index (CEI) to detect subtle demographic biases in face recognition systems by analyzing tail probabilities of score distributions, outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics often fail to detect subtle demographic biases in face recognition systems, particularly in the tails of score distributions. This limitation motivates the development of CEI.

Method: CEI separately analyzes genuine and impostor score distributions, allowing configurable focus on tail probabilities while considering overall distribution shapes. An automated version, CEI^A, is also introduced.

Result: Experiments show CEI's superior ability to detect nuanced biases in state-of-the-art and intentionally biased face recognition systems, outperforming previous methods.

Conclusion: CEI provides a robust and sensitive tool for fairness assessment in face biometrics, with broader applicability for comparing statistical distributions in other domains.

摘要: 高性能人脸识别（FR）系统中的人口统计偏差往往难以通过现有指标检测，尤其是在分数分布的尾部细微差异方面。我们引入了全面公平指数（CEI），这是一种旨在解决这一局限性的新型指标。CEI独特地分别分析真实和冒名顶替分数分布，允许可配置地关注尾部概率，同时考虑整体分布形状。我们的大量实验（评估最先进的FR系统、故意偏置模型和多样化数据集）证实了CEI在检测现有方法无法捕捉的细微偏差方面的卓越能力。此外，我们提出了CEI^A，这是该指标的自动化版本，增强了客观性并简化了实际应用。CEI为操作FR公平性评估提供了强大而敏感的工具。所提出的方法特别针对人脸生物特征中的偏差评估开发，但通常适用于任何对分析分布尾部感兴趣的问题中的统计分布比较。

</details>


### [113] [LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System](https://arxiv.org/abs/2506.10567)
**中文标题：LRSLAM：稠密视觉SLAM系统中符号距离场的低秩表示**

*Hongbeen Park,Minjeong Park,Giljoo Nam,Jinkyu Kim*

Main category: cs.CV

TL;DR: LRSLAM introduces a low-rank tensor decomposition method for dense visual SLAM, improving efficiency and performance over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Dense visual SLAM faces challenges in real-time performance, robustness, and scalability. Neural implicit representations are computationally expensive, and existing tensor decomposition methods like ESLAM struggle with memory growth.

Method: LRSLAM uses low-rank tensor decomposition (Six-axis and CP decompositions) to enhance convergence rates, memory efficiency, and reconstruction/localization quality.

Result: LRSLAM outperforms state-of-the-art methods in parameter efficiency, processing time, and accuracy across diverse indoor RGB-D datasets.

Conclusion: LRSLAM offers a more efficient and scalable solution for dense visual SLAM, balancing performance and resource usage.

摘要: 同步定位与地图构建（SLAM）在自动驾驶、移动机器人和混合现实等多个领域至关重要。稠密视觉SLAM利用RGB-D相机系统具有优势，但在实现实时性能、鲁棒性和大规模场景的可扩展性方面面临挑战。最近采用神经隐式场景表示的方法显示出潜力，但存在高计算成本和内存需求的问题。ESLAM引入了基于平面的张量分解，但仍受限于内存增长。为解决这些问题，我们提出了一种更高效的视觉SLAM模型LRSLAM，利用低秩张量分解方法。我们的方法结合了六轴和CP分解，在收敛速度、内存效率和重建/定位质量上优于现有最先进方法。在多种室内RGB-D数据集上的评估表明，LRSLAM在参数效率、处理时间和准确性方面表现优异，同时保持了重建和定位质量。代码将在发表后公开。

</details>


### [114] [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](https://arxiv.org/abs/2506.10568)
**中文标题：DreamActor-H1：基于运动设计的扩散变换器的高保真人-产品演示视频生成**

*Lizhen Wang,Zhurong Xia,Tianshu Hu,Pengrui Wang,Pengfei Wang,Zerong Zheng,Ming Zhou*

Main category: cs.CV

TL;DR: The paper introduces DreamActor-H1, a Diffusion Transformer-based framework for generating high-fidelity human-product demonstration videos, preserving identities and spatial relationships while enabling realistic interactions.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks struggle to maintain human and product identities or understand spatial relationships, leading to unrealistic videos. The goal is to improve fidelity and natural interactions in human-product demonstration videos.

Method: The proposed method uses a Diffusion Transformer (DiT) with paired human-product reference information and a masked cross-attention mechanism. It employs a 3D body mesh template and product bounding boxes for motion guidance, along with structured text encoding for 3D consistency.

Result: The approach outperforms state-of-the-art techniques in preserving identity integrity and generating realistic motions, validated by a hybrid dataset with extensive augmentation.

Conclusion: DreamActor-H1 effectively addresses the challenges of identity preservation and spatial understanding, offering a robust solution for high-fidelity human-product demonstration videos.

摘要: 在电子商务和数字营销中，生成高保真的人-产品演示视频对于有效的产品展示至关重要。然而，大多数现有框架要么无法同时保留人和产品的身份特征，要么缺乏对人-产品空间关系的理解，导致不真实的表示和不自然的交互。为解决这些问题，我们提出了一种基于扩散变换器（DiT）的框架。我们的方法通过注入配对的人-产品参考信息并利用额外的掩码交叉注意力机制，同时保留人的身份特征和产品特定细节（如标志和纹理）。我们采用3D人体网格模板和产品边界框提供精确的运动指导，实现手势与产品摆放的直观对齐。此外，结构化文本编码用于融入类别级语义，增强帧间小旋转变化时的3D一致性。通过在混合数据集上训练并采用广泛的数据增强策略，我们的方法在保持人和产品身份完整性及生成真实演示动作方面优于现有技术。项目页面：https://submit2025-dream.github.io/DreamActor-H1/。

</details>


### [115] [Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration](https://arxiv.org/abs/2506.10573)
**中文标题：通过病理级别跨模态对齐和相关性探索改进医学视觉表征学习**

*Jun Wang,Lixing Zhu,Xiaohan Yu,Abhir Bhalerao,Yulan He*

Main category: cs.CV

TL;DR: The paper introduces PLACE, a framework for medical visual representation learning that focuses on pathological-level alignment and correlation exploration, achieving state-of-the-art results without additional annotations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of learning medical visual representations from lengthy reports with complex semantics, emphasizing the need for pathological-level consistency often overlooked in prior works.

Method: The proposed PLACE framework includes a Pathological-Level Cross-Modal Alignment (PCMA) approach and a Visual Pathology Observation Extractor to align pathology observations from images and reports. It also uses a proxy task to explore correlations among image patches.

Result: The framework achieves state-of-the-art performance on tasks like classification, retrieval, segmentation, detection, and report generation.

Conclusion: PLACE effectively improves medical visual representation learning by focusing on pathological-level alignment and fine-grained correlation exploration, demonstrating superior performance across multiple tasks.

摘要: 通过联合学习从图像-报告对中学习医学视觉表征，因其潜力缓解医学领域数据稀缺问题而受到越来越多的研究关注。主要挑战源于冗长的报告，这些报告具有复杂的话语关系和语义病理。以往的研究主要集中在实例级或标记级的跨模态对齐，往往忽视了病理级别一致性的重要性。本文提出了一种新颖的框架PLACE，通过病理级别对齐和相关探索丰富细粒度细节，无需额外的人工标注。具体而言，我们提出了一种病理级别跨模态对齐（PCMA）方法，以最大化图像和报告中病理观察的一致性。为此，引入了视觉病理观察提取器，从局部标记中提取视觉病理观察表征。PCMA模块独立于任何外部疾病标注运行，增强了方法的泛化性和鲁棒性。此外，我们设计了一个代理任务，强制模型识别图像块之间的相关性，从而丰富对多种下游任务至关重要的细粒度细节。实验结果表明，我们提出的框架在分类、图像到文本检索、语义分割、目标检测和报告生成等多个下游任务上实现了新的最先进性能。

</details>


### [116] [DanceChat: Large Language Model-Guided Music-to-Dance Generation](https://arxiv.org/abs/2506.10574)
**中文标题：DanceChat：基于大语言模型的音乐到舞蹈生成**

*Qing Wang,Xiaohang Yang,Yilan Dong,Naveen Raj Govindaraj,Gregory Slabaugh,Shanxin Yuan*

Main category: cs.CV

TL;DR: DanceChat uses a Large Language Model (LLM) to guide music-to-dance generation by providing textual motion instructions, improving diversity and alignment with musical styles.


<details>
  <summary>Details</summary>
Motivation: The semantic gap between music and dance, along with the one-to-many mapping problem and scarcity of paired data, limits current music-to-dance generation methods. DanceChat aims to address these challenges by leveraging LLM guidance.

Method: DanceChat consists of three components: (1) an LLM-based pseudo instruction generation module, (2) a multi-modal feature extraction and fusion module, and (3) a diffusion-based motion synthesis module with multi-modal alignment loss.

Result: Experiments on AIST++ and human evaluations show DanceChat outperforms state-of-the-art methods in both quality and diversity.

Conclusion: DanceChat demonstrates the effectiveness of LLM-guided dance generation, offering a novel approach to bridge the music-dance semantic gap and enhance diversity.

摘要: 音乐到舞蹈生成的目标是根据音乐输入合成人类舞蹈动作。尽管近期取得进展，但由于音乐与舞蹈动作之间的语义鸿沟，仍存在重大挑战。音乐仅提供旋律、节奏和情感等抽象线索，而未明确指定物理动作。此外，一首音乐可产生多种合理的舞蹈解释。这种一对多映射需要额外指导，因为仅凭音乐提供的信息有限，难以生成多样化的舞蹈动作。配对的音乐和舞蹈数据稀缺进一步加剧了这一挑战，限制了模型学习多样化舞蹈模式的能力。本文提出DanceChat，一种基于大语言模型（LLM）的音乐到舞蹈生成方法。我们使用LLM作为编舞者，提供文本动作指令，为舞蹈生成提供明确的高层次指导。这种方法超越了仅从音乐中隐式学习的方式，使模型生成的舞蹈更加多样化且更符合音乐风格。我们的方法包括三个部分：（1）基于LLM的伪指令生成模块，根据音乐风格和结构生成文本舞蹈指导；（2）多模态特征提取和融合模块，将音乐、节奏和文本指导整合为共享表示；（3）基于扩散的动作合成模块及多模态对齐损失，确保生成的舞蹈与音乐和文本线索对齐。在AIST++数据集上的大量实验和人工评估表明，DanceChat在质量和数量上均优于现有方法。

</details>


### [117] [Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning](https://arxiv.org/abs/2506.10575)
**中文标题：基于联合提示-适配器学习的多标签图像识别的文本到图像方法**

*Chun-Mei Feng,Kai Yu,Xinxing Xu,Salman Khan,Rick Siow Mong Goh,Wangmeng Zuo,Yong Liu*

Main category: cs.CV

TL;DR: The paper introduces T2I-PAL, a method leveraging text-to-image generation to reduce the modality gap in CLIP for multi-label image recognition, combining prompt tuning and adapter learning for improved performance.


<details>
  <summary>Details</summary>
Motivation: The modality gap between text and image features in CLIP limits performance in text-as-image fine-tuning. The paper aims to address this gap for multi-label image recognition without requiring fully annotated training images.

Method: T2I-PAL uses text-to-image generation models to create diverse images from text captions, reducing the modality gap. It also employs a class-wise heatmap and learnable prototypes for robust local feature representation, combining prompt tuning and adapter learning.

Result: T2I-PAL outperforms state-of-the-art methods by an average of 3.47% on benchmarks like MS-COCO, VOC2007, and NUS-WIDE.

Conclusion: T2I-PAL effectively reduces the modality gap, enhances multi-label recognition, and integrates seamlessly with CLIP, eliminating the need for extensive manual annotations.

摘要: 受益于图像-文本对比学习，预训练的视觉语言模型（如CLIP）可以直接利用文本作为图像（TaI）进行参数高效的微调（PEFT）。尽管CLIP能够使图像特征与相应的文本特征相似，模态差距仍然是一个重要问题，限制了TaI的图像识别性能。以多标签图像识别（MLR）为例，我们提出了一种名为T2I-PAL的新方法，以解决仅使用文本标题进行PEFT时的模态差距问题。T2I-PAL的核心设计是利用预训练的文本到图像生成模型从文本标题生成逼真且多样化的图像，从而减少模态差距。为了进一步增强MLR，T2I-PAL结合了类别热图和可学习原型，聚合局部相似性，使局部视觉特征的表示更加鲁棒和信息丰富。为了更好的PEFT，我们进一步结合了提示调优和适配器学习以提高分类性能。T2I-PAL具有显著优势：它消除了对完全语义标注训练图像的需求，从而减少了手动标注工作量，并保留了CLIP模型的内在模式，可以与任何现有的CLIP框架无缝集成。在多个基准测试（包括MS-COCO、VOC2007和NUS-WIDE）上的广泛实验表明，我们的T2I-PAL可以将识别性能平均提升3.47%，超越当前最先进的方法。

</details>


### [118] [Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres](https://arxiv.org/abs/2506.10576)
**中文标题：几何与不确定性的协调：超球面上的扩散**

*Muskan Dosi,Chiranjeev Chiranjeev,Kartik Thakral,Mayank Vatsa,Richa Singh*

Main category: cs.CV

TL;DR: The paper introduces HyperSphereDiff, a diffusion model that aligns hyperspherical data geometry with directional noise, improving generative performance by preserving angular uncertainty.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models use isotropic Gaussian noise, which is unsuitable for non-Euclidean data like hyperspherical manifolds, leading to loss of angular geometry and suboptimal performance.

Method: HyperSphereDiff incorporates directional noise to align with hyperspherical structures, preserving class geometry and angular uncertainty.

Result: The approach outperforms standard models on object and face datasets, better preserving hyperspherical manifold geometry.

Conclusion: HyperSphereDiff effectively harmonizes geometry and uncertainty, enhancing generative accuracy for hyperspherical data.

摘要: 当代扩散模型是否保留了超球形数据的类别几何结构？标准扩散模型在前向过程中依赖于各向同性高斯噪声，本质上偏向欧几里得空间。然而，许多现实问题涉及非欧几里得分布，如超球面流形，其中类别特定的模式由超锥内的角度几何控制。在欧几里得空间中建模时，这些角度细节会丢失，导致生成性能不佳。为解决这一限制，我们引入了HyperSphereDiff，通过方向性噪声与超球形结构对齐，保留类别几何并有效捕捉角度不确定性。我们从理论和实验上证明，这种方法使生成过程与超球形数据的内在几何对齐，从而产生更准确且几何感知的生成模型。我们在四个物体数据集和两个人脸数据集上评估了我们的框架，结果表明，引入角度不确定性更好地保留了底层的超球面流形。资源见：{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/}

</details>


### [119] [Rethinking Random Masking in Self Distillation on ViT](https://arxiv.org/abs/2506.10582)
**中文标题：重新思考ViT自蒸馏中的随机掩码策略**

*Jihyeon Seong,Hyunkyung Han*

Main category: cs.CV

TL;DR: The paper rethinks random masking in self-distillation for ViTs, proposing asymmetric masking (only on the student's global view) to preserve critical information and improve robustness, validated on mini-ImageNet.


<details>
  <summary>Details</summary>
Motivation: Random masking in self-distillation frameworks like DINO may unintentionally remove important semantic information, prompting the need for more informed masking strategies.

Method: The authors apply random masking only to the student's global view while keeping the student's local views and teacher's global view unmasked, leveraging DINO's multi-view augmentation.

Result: The asymmetric masking approach yields more robust and fine-grained attention maps, enhancing downstream performance on mini-ImageNet.

Conclusion: Asymmetric random masking in self-distillation improves model robustness and performance by preserving critical information while introducing regularization.

摘要: 视觉变换器（ViT）在各种视觉任务中表现出色，尤其是自蒸馏框架（如DINO）对此贡献显著。在这些框架中，随机掩码常被用于提高训练效率和引入正则化。然而，近期研究指出，不加区分的随机掩码可能会无意中消除关键语义信息，这促使了更智能掩码策略的开发。本研究探讨了自蒸馏设置中随机掩码的作用，重点关注DINO框架。具体而言，我们仅对学生的全局视图应用随机掩码，同时保留学生的局部视图和教师的全局视图的原始未掩码形式。这一设计利用DINO的多视图增强方案，在保持干净监督的同时通过掩码输入引入鲁棒性。我们在mini-ImageNet数据集上使用DINO-Tiny评估了该方法，结果表明，在这种非对称设置下的随机掩码能生成更鲁棒和细粒度的注意力图，最终提升下游性能。

</details>


### [120] [Hierarchical Error Assessment of CAD Models for Aircraft Manufacturing-and-Measurement](https://arxiv.org/abs/2506.10594)
**中文标题：飞机制造与测量中CAD模型的分层误差评估**

*Jin Huang,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: A hierarchical error assessment framework (HEA-MM) for aircraft CAD models is proposed, using structured light scanners and multi-level error analysis (global, part, feature) to ensure high-quality manufacturing.


<details>
  <summary>Details</summary>
Motivation: To ensure high quality in aviation equipment by accurately assessing errors in CAD models during manufacturing and measurement.

Method: HEA-MM employs structured light scanners for 3D measurements, registers point clouds with CAD models, and performs error analysis at global, part, and feature levels, including optimization-based primitive refinement and circular hole detection.

Result: Experimental results demonstrate the effectiveness of HEA-MM in accurately assessing errors in aircraft CAD models.

Conclusion: HEA-MM provides a robust framework for hierarchical error assessment, enhancing the quality and reliability of aircraft manufacturing.

摘要: 航空设备最重要的特征是高质量，包括高性能、高稳定性和高可靠性。本文提出了一种新颖的分层误差评估框架（HEA-MM），用于飞机CAD模型在制造与测量平台中的误差分析。HEA-MM采用结构光扫描仪获取制造工件的全面3D测量数据，将测量点云与参考CAD模型配准后，在三个层次（全局、部件和特征）进行误差分析。全局层次评估扫描点云与参考CAD模型的整体偏差；部件层次对点云的基础块进行误差分析，提出了一种基于优化的基元细化方法，通过拆分和合并操作优化粗基元；特征层次针对CAD模型中常见的圆形孔进行误差分析，提出了一种两阶段算法（边缘点检测和圆拟合）以确保圆形特征的准确检测与分析。多种飞机CAD模型的实验结果表明了该方法的有效性。

</details>


### [121] [Semantic-decoupled Spatial Partition Guided Point-supervised Oriented Object Detection](https://arxiv.org/abs/2506.10601)
**中文标题：语义解耦的空间划分引导的点监督定向目标检测**

*Xinyuan Liu,Hang Xu,Yike Ma,Yucheng Zhang,Feng Dai*

Main category: cs.CV

TL;DR: The paper introduces SSP, a framework for point-supervised oriented object detection in remote sensing, combining rule-driven and data-driven approaches to improve sample assignment and instance extraction, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: High-density scenes in remote sensing require labor-intensive annotations for oriented object detection. Point supervision offers a cost-effective solution, but existing methods suffer from rigid designs leading to inadequate sample assignment and instance confusion.

Method: SSP integrates rule-driven prior injection and data-driven label purification. It includes Pixel-level Spatial Partition for sample assignment and Semantic Spatial Partition for box extraction, forming pseudo-labels for downstream detectors.

Result: SSP achieves 45.78% mAP under point supervision, outperforming PointOBB-v2 by 4.10%. With ORCNN and ReDet, it reaches 47.86% and 48.50% mAP, respectively.

Conclusion: SSP effectively addresses challenges in point-supervised oriented object detection, demonstrating superior performance and adaptability with different architectures.

摘要: 近年来遥感技术的进步推动了图像数据的增长，使得定向目标检测快速发展，但高密度场景下的标注工作繁重。点监督的定向目标检测为遥感中的密集场景提供了一种经济高效的解决方案，但现有方法因基于刚性规则的设计而存在样本分配不足和实例混淆的问题。为此，我们提出了SSP（语义解耦的空间划分），一个结合规则驱动先验注入和数据驱动标签净化的统一框架。具体而言，SSP包含两项核心创新：1）基于像素级空间划分的样本分配，通过像素图的空间划分紧凑估计对象尺度的上下界，并挖掘高质量的正样本和难负样本。2）基于语义空间划分的框提取，从语义图调制的空间划分中提取实例，并将其可靠地转换为边界框，形成伪标签以监督下游检测器的学习。在DOTA-v1.0等数据集上的实验表明，SSP在点监督下达到45.78%的mAP，优于SOTA方法PointOBB-v2 4.10%。此外，与ORCNN和ReDet架构集成时，SSP框架分别达到47.86%和48.50%的mAP。代码发布于https://github.com/antxinyuan/ssp。

</details>


### [122] [High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model](https://arxiv.org/abs/2506.10605)
**中文标题：基于预训练潜在扩散模型的高分辨率高效WiFi CSI图像生成**

*Eshan Ramesh,Nishio Takayuki*

Main category: cs.CV

TL;DR: LatentCSI introduces a method to generate high-resolution images from WiFi CSI measurements using a pretrained latent diffusion model, outperforming traditional GAN-based approaches in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Prior methods for generating images from WiFi CSI rely on complex, computationally intensive techniques like GANs, which are inefficient and lack controllability. LatentCSI aims to address these limitations by leveraging pretrained latent diffusion models for efficient and high-quality image synthesis.

Method: LatentCSI uses a lightweight neural network to map WiFi CSI amplitudes into the latent space of a pretrained latent diffusion model (LDM). The LDM's denoising diffusion model is then applied with text-based guidance, and the result is decoded to produce high-resolution images, bypassing pixel-space generation challenges.

Result: LatentCSI outperforms baseline methods in computational efficiency and perceptual quality on two datasets: a custom wide-band CSI dataset and a subset of the MM-Fi dataset. It also offers text-guided controllability.

Conclusion: LatentCSI provides an efficient, high-quality solution for image generation from WiFi CSI, leveraging pretrained LDMs to avoid the pitfalls of traditional methods while enabling text-guided control.

摘要: 我们提出了LatentCSI，一种利用预训练潜在扩散模型（LDM）从WiFi CSI测量中生成物理环境图像的新方法。与以往依赖复杂且计算密集型技术（如GANs）的方法不同，我们的方法采用轻量级神经网络将CSI幅度直接映射到LDM的潜在空间中。随后，我们在潜在表示上应用LDM的去噪扩散模型，并结合基于文本的指导，最后使用LDM的预训练解码器解码，获得高分辨率图像。这一设计绕过了像素空间图像生成的挑战，避免了传统图像到图像流程中通常需要的显式图像编码阶段，从而实现了高效且高质量的图像合成。我们在两个数据集上验证了我们的方法：一个是我们使用现成WiFi设备和相机收集的宽带CSI数据集；另一个是公开可用的MM-Fi数据集的子集。结果表明，LatentCSI在计算效率和感知质量上均优于直接在真实图像上训练的复杂度相当的基线方法，同时通过其独特的文本引导可控性提供了实际优势。

</details>


### [123] [MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling](https://arxiv.org/abs/2506.10609)
**中文标题：MSTAR：基于注意力循环的无框多查询场景文本检索**

*Liang Yin,Xudong Xie,Zhang Li,Xiang Bai,Yuliang Liu*

Main category: cs.CV

TL;DR: MSTAR introduces a box-free approach for multi-query scene text retrieval, using attention recycling and progressive vision embedding to unify diverse query types, outperforming state-of-the-art models while eliminating costly bounding box annotations.


<details>
  <summary>Details</summary>
Motivation: Existing scene text retrieval methods rely on expensive bounding box annotations and struggle to unify diverse query types, limiting their practical applicability.

Method: MSTAR employs progressive vision embedding and attention recycling to dynamically capture multi-grained text representations, harmonizing free-style queries with style-aware instructions and enhancing vision-language alignment via a multi-instance matching module.

Result: MSTAR surpasses previous state-of-the-art models by 6.4% in MAP on Total-Text and by an average of 8.5% on the new MQTR benchmark, while eliminating box annotation costs.

Conclusion: MSTAR provides an efficient, box-free solution for multi-query scene text retrieval, demonstrating superior performance and versatility across diverse datasets.

摘要: 场景文本检索在精确文本定位的辅助下取得了显著进展。然而，现有方法通常需要昂贵的边界框标注进行训练。此外，它们大多采用定制化的检索策略，但难以统一多种查询类型以满足多样化的检索需求。为解决这些问题，我们提出了基于注意力循环的多查询场景文本检索方法（MSTAR），一种无框的场景文本检索方法。它通过渐进式视觉嵌入动态捕捉文本的多粒度表示，并通过风格感知指令协调自由形式的文本查询。此外，还集成了一个多实例匹配模块以增强视觉-语言对齐。我们还构建了多查询文本检索（MQTR）数据集，这是首个用于评估模型多查询场景文本检索能力的基准，包含四种查询类型和16k张图像。大量实验证明，我们的方法在七个公共数据集和MQTR数据集上均表现出优越性。值得注意的是，MSTAR在Total-Text上的MAP指标上以6.4%的优势略微超越先前的最先进模型，同时消除了边界框标注成本。此外，在MQTR基准上，MSTAR平均比先前模型高出8.5%。代码和数据集可在https://github.com/yingift/MSTAR获取。

</details>


### [124] [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://arxiv.org/abs/2506.10612)
**中文标题：TexTailor：通过有效重采样实现定制化的文本对齐纹理**

*Suin Lee,Dae-Shik Kim*

Main category: cs.CV

TL;DR: TexTailor introduces a novel method for generating consistent object textures from text descriptions by addressing viewpoint shifts and geometry-aware camera adjustments, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-texture methods suffer from inconsistent textures across viewpoints due to insufficient integration of synthesized textures and predefined camera positions, limiting texture consistency.

Method: TexTailor uses a resampling scheme to integrate previously synthesized textures during diffusion, fine-tunes a depth-aware diffusion model, and adaptively adjusts camera positions based on object geometry.

Result: Experiments on Objaverse and ShapeNet datasets show TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures.

Conclusion: TexTailor effectively addresses texture inconsistency by integrating resampling and geometry-aware camera adjustments, demonstrating superior performance.

摘要: 我们提出了TexTailor，一种从文本描述生成一致物体纹理的新方法。现有的文本到纹理合成方法利用深度感知扩散模型逐步生成图像并在预定义的多个视角下合成纹理。然而，这些方法由于（1）扩散过程中每个视角下先前合成纹理的整合不足，以及（2）纹理合成过程的自回归性质，导致纹理属性在视角间逐渐偏移。此外，预定义的相机位置选择未考虑物体几何形状，限制了从不同视角合成的纹理信息的有效利用，最终降低了整体纹理一致性。在TexTailor中，我们通过（1）应用一种重采样方案，在扩散过程中重复整合先前合成纹理的信息，以及（2）在这些重采样纹理上微调深度感知扩散模型，解决了这些问题。在此过程中，我们发现仅使用少量训练图像限制了模型生成与条件对齐的高保真图像的原始能力，因此提出了一种性能保持损失来缓解这一问题。此外，我们通过基于物体几何形状自适应调整相机位置，改进了视角一致纹理的合成。在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视角一致纹理方面优于现有方法。TexTailor的源代码可在https://github.com/Adios42/Textailor获取。

</details>


### [125] [Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models](https://arxiv.org/abs/2506.10633)
**中文标题：基于解剖学的弱监督提示调整用于胸部X光潜在扩散模型**

*Konstantinos Vilouras,Ilias Stogiannidis,Junyu Yan,Alison Q. O'Neil,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: The paper proposes a fine-tuning framework to improve multi-modal alignment in pre-trained Latent Diffusion Models for chest X-ray images, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Text-to-image Latent Diffusion Models are underutilized in medical imaging due to limited data and poor alignment between radiology reports and scans. The paper aims to improve this alignment for downstream tasks like phrase grounding.

Method: The authors introduce a fine-tuning framework for pre-trained Latent Diffusion Models to enhance alignment between free-text radiology reports and chest X-ray scans, enabling efficient adaptation for tasks like phrase grounding.

Result: The method achieves state-of-the-art performance on the MS-CXR benchmark and demonstrates robustness on out-of-distribution data (VinDr-CXR).

Conclusion: The proposed framework successfully improves multi-modal alignment in Latent Diffusion Models for medical imaging, offering potential for downstream applications.

摘要: 近年来，潜在扩散模型在文本引导的图像合成中表现出卓越的效果。在自然（RGB）图像领域，最近的研究表明，这类模型可以在极少甚至无监督的情况下适应各种视觉语言下游任务。然而，在医学影像领域，文本到图像的潜在扩散模型仍相对未被充分探索，主要由于数据可用性有限（例如，出于隐私考虑）。在这项工作中，我们以胸部X光模态为例，首先证明标准的文本条件潜在扩散模型尚未学会将自由文本放射学报告中的临床相关信息与扫描图像的对应区域对齐。为解决这一问题，我们提出了一种微调框架，用于改进预训练模型中的多模态对齐，使其能够高效地重新用于下游任务（如短语定位）。我们的方法在标准基准数据集（MS-CXR）上达到了新的最先进水平，同时在分布外数据（VinDr-CXR）上也表现出稳健的性能。我们的代码将公开发布。

</details>


### [126] [Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models](https://arxiv.org/abs/2506.10634)
**中文标题：对称流匹配：基于分数生成模型的统一图像生成、分割与分类**

*Francisco Caetano,Christiaan Viviers,Peter H. N. De With,Fons van der Sommen*

Main category: cs.CV

TL;DR: SymmFlow introduces a unified framework for image generation, segmentation, and classification using a symmetric learning objective, achieving state-of-the-art performance with efficient sampling.


<details>
  <summary>Details</summary>
Motivation: To unify semantic segmentation, classification, and image generation within a single model while ensuring bi-directional consistency and preserving generative diversity.

Method: Symmetrical Flow Matching (SymmFlow) jointly models forward and reverse transformations with a symmetric learning objective, retaining semantic information across flows for efficient one-step tasks.

Result: Achieves FID scores of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with 25 inference steps, and competitive results in segmentation and classification.

Conclusion: SymmFlow provides a versatile and efficient framework for unified generative and discriminative tasks, outperforming previous methods.

摘要: 流匹配已成为学习分布间连续变换的强大框架，支持高保真生成建模。本文提出对称流匹配（SymmFlow），通过对称学习目标将语义分割、分类和图像生成统一于单一模型中。SymmFlow联合建模正向和反向变换，确保双向一致性的同时保留生成多样性。新训练目标显式保留跨流的语义信息，支持高效采样并保持语义结构，实现无需迭代优化的单步分割与分类。与以往严格一对一映射方法不同，SymmFlow支持灵活条件输入，兼容像素级和图像级类别标签。多基准实验表明，SymmFlow在语义图像合成中达到领先水平，仅需25次推理步骤即在CelebAMask-HQ和COCO-Stuff上分别取得11.9和7.0的FID分数，同时在分割和分类任务中表现优异。代码将公开提供。

</details>


### [127] [GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](https://arxiv.org/abs/2506.10639)
**中文标题：GigaVideo-1：通过4 GPU小时的自动反馈微调推进视频生成**

*Xiaoyi Bao,Jindi Lv,Xiaofeng Wang,Zheng Zhu,Xinze Chen,YuKun Zhou,Jiancheng Lv,Xingang Wang,Guan Huang*

Main category: cs.CV

TL;DR: GigaVideo-1 is an efficient fine-tuning framework for video diffusion models that uses automatic feedback to improve video generation quality without human supervision, achieving significant performance gains with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for video diffusion models rely on human annotations and extensive computational resources, limiting their practicality. GigaVideo-1 aims to address these limitations by leveraging automatic feedback for efficient fine-tuning.

Method: GigaVideo-1 employs a prompt-driven data engine to create diverse, weakness-oriented training samples and a reward-guided training strategy that adaptively weights samples using feedback from pre-trained vision-language models with a realism constraint.

Result: GigaVideo-1 improves performance across 17 evaluation dimensions on the VBench-2.0 benchmark, achieving an average gain of about 4% with only 4 GPU-hours of fine-tuning.

Conclusion: GigaVideo-1 demonstrates effectiveness and efficiency in advancing video generation quality without manual annotations or extensive real data, making it a practical solution for fine-tuning video diffusion models.

摘要: 扩散模型的最新进展极大地提升了视频生成质量，但这些模型仍需通过微调来改进特定维度，如实例保留、运动合理性、构图和物理合理性。现有的微调方法通常依赖于人工标注和大规模计算资源，限制了其实用性。本文提出GigaVideo-1，一种高效的微调框架，无需额外人工监督即可提升视频生成质量。GigaVideo-1并非从外部注入大量高质量数据，而是通过自动反馈释放预训练视频扩散模型的潜在能力。具体而言，我们关注微调过程中的两个关键方面：数据和优化。为改进微调数据，我们设计了一个基于提示的数据引擎，构建多样化、针对弱点的训练样本。在优化方面，我们引入了一种奖励引导的训练策略，利用预训练视觉语言模型的反馈和真实性约束自适应地加权样本。我们在VBench-2.0基准上评估GigaVideo-1，以Wan2.1为基线，覆盖17个评估维度。实验表明，GigaVideo-1在几乎所有维度上均能持续提升性能，平均增益约4%，仅需4 GPU小时。无需人工标注和极少真实数据，GigaVideo-1展示了高效性和有效性。代码、模型和数据将公开提供。

</details>


### [128] [PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis](https://arxiv.org/abs/2506.10669)
**中文标题：PiPViT：基于补丁的可视化可解释原型用于视网膜图像分析**

*Marzieh Oghbaie,Teresa Araújoa,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: PiPViT introduces a patch-based prototype model for retinal image analysis, improving interpretability by learning human-understandable prototypes using a vision transformer and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based methods lack consistency with human-understandable biomarkers in medical imaging, and their granular prototypes are less interpretable for clinical use.

Method: PiPViT uses a vision transformer (ViT) to capture long-range dependencies among patches, employs contrastive learning, and processes multi-resolution inputs to learn interpretable prototypes.

Result: PiPViT achieved competitive performance on retinal OCT image classification across four datasets, with prototypes confirmed as clinically relevant.

Conclusion: PiPViT provides transparent explanations for diagnostic decisions, aiding clinicians in understanding outcomes.

摘要: 背景与目标：基于原型的方法通过学习细粒度的部分原型来提高可解释性；然而，它们在输入像素空间中的可视化并不总是与人类可理解的生物标志物一致。此外，著名的基于原型的方法通常学习过于细粒度的原型，这在医学影像中可解释性较差，而生物标志物和病变的存在与范围都至关重要。
方法：为解决这些问题，我们提出了PiPViT（基于补丁的可视化可解释原型），一种本质可解释的原型模型用于图像识别。利用视觉变换器（ViT），PiPViT捕捉补丁之间的长程依赖关系，以学习稳健且人类可解释的原型，仅使用图像级标签近似病变范围。此外，PiPViT受益于对比学习和多分辨率输入处理，从而能够有效定位不同尺度的生物标志物。
结果：我们在四个数据集上评估了PiPViT在视网膜OCT图像分类中的表现，其定量性能与最先进方法相当，同时提供了更有意义的解释。此外，对保留测试集的定量评估证实了学习到的原型在语义和临床上是相关的。我们相信PiPViT可以透明地解释其决策，并帮助临床医生理解诊断结果。GitHub页面：https://github.com/marziehoghbaie/PiPViT

</details>


### [129] [Enhancing Deepfake Detection using SE Block Attention with CNN](https://arxiv.org/abs/2506.10683)
**中文标题：使用SE块注意力与CNN增强Deepfake检测**

*Subhram Dasgupta,Janelle Mason,Xiaohong Yuan,Olusola Odeyomi,Kaushik Roy*

Main category: cs.CV

TL;DR: The paper proposes a lightweight CNN with SE block attention for efficient Deepfake detection, achieving high accuracy with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology poses a significant threat to information authenticity and security, and existing detection models are often resource-intensive. The goal is to develop a lightweight yet effective solution.

Method: The authors integrate a squeeze and excitation (SE) block with a CNN to dynamically recalibrate channel-wise features, emphasizing informative features and suppressing less useful ones.

Result: The model achieves 94.14% classification accuracy and an AUC-ROC score of 0.985 on the Style GAN dataset, demonstrating competitive performance with smaller size.

Conclusion: The proposed lightweight CNN with SE block attention offers an efficient and scalable solution for Deepfake detection, balancing accuracy and resource usage.

摘要: 在数字时代，Deepfake通过先进的人工智能技术制造高度逼真的篡改内容，对信息的真实性和安全性构成严峻挑战。这些复杂的伪造内容超越了传统检测方法的复杂性和真实性。为解决这一问题，我们旨在利用前沿的深度学习方法设计一种创新的Deepfake检测模型。然而，大多数现有的Deepfake检测模型体积庞大，导致存储和内存消耗过高。在本研究中，我们提出了一种轻量级的卷积神经网络（CNN）结合挤压激励块注意力（SE）用于Deepfake检测。SE块模块旨在动态调整通道特征，使网络能够强调信息丰富的特征并抑制无用特征，从而实现更高效的学习模块。该模块与简单的序列模型集成以执行Deepfake检测。该模型体积较小，同时在Deepfake检测任务中与现有模型竞争精度。在Diverse Fake Face Dataset的Style GAN数据集上，该模型的总体分类准确率为94.14%，AUC-ROC得分为0.985。我们提出的方法为以最小计算资源应对Deepfake挑战提供了一条有前景的途径，为数字内容验证开发了高效且可扩展的解决方案。

</details>


### [130] [Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework](https://arxiv.org/abs/2506.10685)
**中文标题：无源对抗验证码：一种双阶段对抗验证码框架**

*Xia Du,Xiaoyuan Liu,Jizhe Zhou,Zheng Lin,Chi-man Pun,Zhe Chen,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: The paper introduces Unsourced Adversarial CAPTCHA (UAC), a framework generating high-fidelity adversarial examples using text prompts, enhancing CAPTCHA diversity and attack efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional CAPTCHAs are vulnerable to deep learning attacks, and existing adversarial methods often distort images, making them hard for humans to interpret. UAC addresses these issues by generating adversarial examples without relying on original images.

Method: UAC uses a Large Language Model (LLM) to guide adversarial example generation. For targeted attacks, the EDICT method optimizes dual latent variables in a diffusion model. For untargeted attacks, BP-UAC employs multimodal gradients and bi-path optimization.

Result: BP-UAC achieves high attack success rates across diverse systems, producing CAPTCHAs that are natural and indistinguishable to both humans and DNNs.

Conclusion: UAC provides a robust framework for generating adversarial CAPTCHAs, improving security and diversity while maintaining human interpretability.

摘要: 随着深度学习的快速发展，传统的验证码方案越来越容易受到基于深度神经网络（DNN）的自动化攻击。现有的对抗攻击方法通常依赖于原始图像特征，导致失真，影响人类理解，并在缺乏初始输入图像的场景中应用受限。为解决这些问题，我们提出了无源对抗验证码（UAC），这是一种新颖的框架，通过攻击者指定的文本提示生成高保真对抗样本。UAC利用大型语言模型（LLM）增强验证码多样性，并支持定向和非定向攻击。对于定向攻击，EDICT方法在扩散模型中优化双潜变量以获得更优图像质量。在非定向攻击中，特别是黑盒场景下，我们引入了双路径无源对抗验证码（BP-UAC），这是一种两步优化策略，采用多模态梯度和双路径优化以实现高效误分类。实验表明，BP-UAC在多种系统中实现了高攻击成功率，生成的验证码对人类和DNN均难以区分。

</details>


### [131] [Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery](https://arxiv.org/abs/2506.10689)
**中文标题：通过多任务和多年龄方法在无约束图像中筛查未成年人的未成年检测**

*Christopher Gaul,Eduardo Fidalgo,Enrique Alegre,Rocío Alaiz Rodríguez,Eri Pérez Corral*

Main category: cs.CV

TL;DR: The paper proposes a multi-task architecture for underage detection in unconstrained images, addressing data imbalance and distribution shifts with a novel loss function and rigorous evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy of automatic screening of minors in unconstrained images, overcoming challenges like data imbalance and distribution shifts.

Method: The method involves a multi-task architecture with a frozen FaRL backbone, a two-layer MLP, and specialized heads for age regression and under-age detection. It uses an α-reweighted focal-style loss and age-balanced mini-batch sampling.

Result: The model reduces age estimation error and improves under-18 detection F2 scores, demonstrating strong generalization under domain shifts.

Conclusion: The proposed multi-task approach effectively addresses underage detection challenges, showing robustness to real-world distribution shifts.

摘要: 准确自动筛查无约束图像中的未成年人需要模型对分布偏移具有鲁棒性，并能应对公开数据中儿童样本不足的问题。为解决这些问题，我们提出了一种多任务架构，基于冻结的FaRL视觉-语言骨干网络，结合一个紧凑的两层MLP，共享特征于一个年龄回归头和四个二元未成年检测头（针对12、15、18和21岁的法律关键年龄阈值）。为应对严重的类别不平衡，我们引入了α加权的焦点式损失和年龄平衡的小批量采样，在随机优化中均衡了12个年龄区间。通过去除边缘案例的年龄间隔，进一步提升了性能。

此外，我们提出了严格的评估标准，即“总体未成年基准”，包含30.3万张清理过的训练图像和11万张测试图像，定义了“ASORES-39k”受限总体测试（去除噪声最大的领域）和“ASWIFT-20k”年龄估计野外观测测试（2万张图像，模拟极端姿态（>45度）、表情和低图像质量的真实世界偏移）。

在清理后的总体数据集上训练并采用重采样和年龄间隔后，我们的多年龄模型“F”在ASORES-39k受限测试中将年龄估计的均方根误差从5.733（仅年龄基线）降低到5.656岁，并将18岁以下检测的F2分数从0.801提升到0.857（假阳性率为1%）。在ASWIFT-20k野外观测数据的域偏移下，相同配置几乎保持了0.99的召回率，同时将F2分数从0.742提升到0.833（相对于仅年龄基线），展示了在分布偏移下的强泛化能力。对于12岁以下和15岁以下任务，F2分数分别从0.666提升到0.955和从0.689提升到0.916。

</details>


### [132] [Continual Hyperbolic Learning of Instances and Classes](https://arxiv.org/abs/2506.10710)
**中文标题：实例与类别的持续双曲学习**

*Melika Ayoughi,Mina Ghadimi Atigh,Mohammad Mahdi Derakhshani,Cees G. M. Snoek,Pascal Mettes,Paul Groth*

Main category: cs.CV

TL;DR: The paper introduces HyperCLIC, a continual learning algorithm using hyperbolic space to handle hierarchical relationships between instances and classes, validated on the EgoObjects dataset.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like robotics require models to simultaneously learn instances and classes, which form a hierarchical structure. Current continual learning methods focus on either instances or classes, not both.

Method: Proposes HyperCLIC, leveraging hyperbolic space for hierarchical data representation, incorporating hyperbolic classification and distillation objectives for continual embedding of hierarchical relations.

Result: HyperCLIC effectively operates at multiple granularities with improved hierarchical generalization, validated on the EgoObjects dataset.

Conclusion: HyperCLIC addresses the challenge of continual learning of both instances and classes by modeling their hierarchical relationships in hyperbolic space, showing superior performance.

摘要: 持续学习传统上专注于对实例或类别进行分类，但现实应用（如机器人和自动驾驶汽车）要求模型同时处理两者。为了模拟现实场景，我们引入了同时持续学习实例和类别的任务。该任务挑战模型随时间适应多级粒度，需要在细粒度实例识别与粗粒度类别泛化之间取得平衡。本文指出，类别和实例自然形成层次结构。为了建模这些层次关系，我们提出了HyperCLIC，一种利用双曲空间的持续学习算法，双曲空间因其能够以低失真和紧凑嵌入表示树状结构而特别适合层次数据。我们的框架结合了双曲分类和蒸馏目标，实现了层次关系的持续嵌入。为了评估多粒度性能，我们引入了持续层次指标。我们在EgoObjects数据集上验证了我们的方法，这是唯一捕捉动态现实环境中层次对象识别复杂性的数据集。实证结果表明，HyperCLIC在多粒度上有效运行，具有改进的层次泛化能力。

</details>


### [133] [Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement](https://arxiv.org/abs/2506.10712)
**中文标题：基于不确定性掩蔽伯努利扩散的伪装目标检测细化**

*Yuqi Shen,Fengyang Xiao,Sujie Hu,Youwei Pang,Yifan Pu,Chengyu Fang,Xiu Li,Chunming He*

Main category: cs.CV

TL;DR: The paper proposes the Uncertainty-Masked Bernoulli Diffusion (UMBD) model, a generative refinement framework for Camouflaged Object Detection (COD), which selectively refines poorly segmented regions using uncertainty-guided masking and achieves significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing COD methods lack effective post-processing refinement, limiting their performance despite progress in segmentation. The paper aims to address this gap by introducing a targeted refinement approach.

Method: UMBD uses an uncertainty-guided masking mechanism to apply Bernoulli diffusion to residual regions with poor segmentation. It integrates the Hybrid Uncertainty Quantification Network (HUQNet) for accurate uncertainty estimation and adaptive guidance during refinement.

Result: UMBD consistently improves performance across COD benchmarks, achieving average gains of 5.5% in MAE and 3.2% in weighted F-measure with modest computational overhead.

Conclusion: The UMBD framework effectively combines discriminative COD models with generative refinement, demonstrating its potential for enhancing COD performance without significant computational cost.

摘要: 伪装目标检测（COD）由于目标与背景之间微妙的视觉差异而具有固有挑战性。尽管现有方法已取得显著进展，但后处理细化的潜力尚未充分挖掘。为解决这一局限，我们提出了不确定性掩蔽伯努利扩散（UMBD）模型，这是首个专为COD设计的生成式细化框架。UMBD引入了一种不确定性引导的掩蔽机制，选择性地对分割质量较差的残差区域应用伯努利扩散，从而实现针对性细化并保留正确分割区域。为支持这一过程，我们设计了混合不确定性量化网络（HUQNet），采用多分支架构并融合多源不确定性以提高估计精度，从而在生成采样过程中提供自适应引导。所提出的UMBD框架可与多种基于编码器-解码器的COD模型无缝集成，结合其判别能力与基于扩散的细化生成优势。在多个COD基准测试上的广泛实验表明，该方法实现了性能的持续提升，仅需适度的计算开销即可平均提升5.5%的MAE和3.2%的加权F-measure。代码将公开。

</details>


### [134] [Deep Learning-based Multi Project InP Wafer Simulation for Unsupervised Surface Defect Detection](https://arxiv.org/abs/2506.10713)
**中文标题：基于深度学习的多项目InP晶圆模拟用于无监督表面缺陷检测**

*Emílio Dolgener Cantú,Rolf Klemens Wittmann,Oliver Abdeen,Patrick Wagner,Wojciech Samek,Moritz Baier,Sebastian Lapuschkin*

Main category: cs.CV

TL;DR: The paper proposes a deep learning method to generate synthetic golden standards from CAD data for unsupervised defect detection in InP wafer manufacturing, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Current defect detection in InP wafer manufacturing is manual due to lack of golden standards, which is inefficient and labor-intensive.

Method: The method uses Deep Neural Networks to simulate photo-realistic InP wafer images from CAD data, evaluating various training objectives and image quality.

Result: The deep-learning-based approach outperforms a baseline decision-tree method, enabling efficient defect detection using simulated golden standards.

Conclusion: The proposed method provides a practical solution for automated defect detection in InP wafer manufacturing, improving efficiency and reducing manual effort.

摘要: 半导体制造中的质量管理通常依赖于已知黄金标准的模板匹配。对于磷化铟（InP）多项目晶圆制造，低生产规模和高设计变异性导致此类黄金标准通常不可用。因此，缺陷检测是手动且劳动密集型的。本研究通过提出一种方法来解决这一挑战，该方法利用深度神经网络从CAD数据生成合成的黄金标准，模拟逼真的InP晶圆图像。我们评估了各种训练目标，并在合成数据和InP晶圆照片上评估了模拟图像的质量。我们的基于深度学习的方法优于基于决策树的基线方法，使得可以从CAD计划中在任何用户定义的晶圆区域使用“模拟黄金芯片”，以实现更高效的缺陷检测。我们将该方法应用于模板匹配过程，以展示其在表面缺陷检测中的实际效用。

</details>


### [135] [IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain](https://arxiv.org/abs/2506.10730)
**中文标题：IQE-CLIP：面向医学领域的实例感知查询嵌入零样本/少样本异常检测**

*Hong Huang,Weixiang Sun,Zhijian Wu,Jingwen Niu,Donghuan Lu,Xian Wu,Yefeng Zheng*

Main category: cs.CV

TL;DR: IQE-CLIP is a novel framework for zero-/few-shot anomaly detection in the medical domain, leveraging instance-aware query embeddings and learnable prompts to outperform existing CLIP-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods for anomaly detection rely on predefined prompts and lack instance-level discrimination, especially in medical tasks. IQE-CLIP aims to address these limitations by integrating visual and textual information for better anomaly detection.

Method: IQE-CLIP introduces class-based and learnable prompting tokens for CLIP adaptation and an instance-aware query module to extract region-level contextual information, generating anomaly-sensitive embeddings.

Result: Experiments on six medical datasets show IQE-CLIP achieves state-of-the-art performance in zero-shot and few-shot anomaly detection.

Conclusion: IQE-CLIP effectively combines textual and visual information for anomaly detection in medical tasks, demonstrating superior performance and adaptability.

摘要: 近年来，视觉语言模型（如CLIP）在零样本和少样本异常检测（ZFSAD）任务中取得了显著进展。然而，现有的大多数基于CLIP的方法假设已知类别信息，并依赖于针对特定场景精心设计的提示。这些文本提示虽然能捕捉文本空间的语义信息，但往往无法在联合嵌入空间中区分正常和异常实例。此外，大多数ZFSAD方法集中在工业领域，对医学任务的探索有限。为解决这些问题，我们提出了IQE-CLIP，一种面向医学领域的ZFSAD新框架。我们证明，结合文本和实例感知视觉信息的查询嵌入能更有效地指示异常。具体而言，我们引入了基于类别和可学习的提示标记，以更好地将CLIP适配到医学场景。此外，我们设计了一个实例感知查询模块，从两种模态中提取区域级上下文信息，从而生成对异常敏感的嵌入。在六个医学数据集上的大量实验表明，IQE-CLIP在零样本和少样本设置下均达到了最先进的性能。代码和数据可在\href{https://github.com/hongh0/IQE-CLIP/}{此链接}获取。

</details>


### [136] [PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework](https://arxiv.org/abs/2506.10741)
**中文标题：PosterCraft：重新思考高质量美学海报生成的统一框架**

*SiXiang Chen,Jianyu Lai,Jialin Gao,Tian Ye,Haoyu Chen,Hengyu Shi,Shitong Shao,Yunlong Lin,Song Fei,Zhaohu Xing,Yeying Jin,Junfeng Luo,Xiaoming Wei,Lei Zhu*

Main category: cs.CV

TL;DR: PosterCraft introduces a unified framework for generating high-quality aesthetic posters by optimizing text rendering, layout coherence, and stylistic harmony through a cascaded workflow and automated data-construction pipelines.


<details>
  <summary>Details</summary>
Motivation: Generating aesthetic posters is complex, requiring precise text rendering and seamless integration of artistic content. Existing modular pipelines and rigid layouts limit creativity and quality.

Method: PosterCraft uses a cascaded workflow: (i) text-rendering optimization, (ii) region-aware fine-tuning, (iii) aesthetic-text reinforcement learning, and (iv) joint vision-language feedback refinement, supported by automated data pipelines.

Result: PosterCraft outperforms open-source baselines in rendering accuracy, layout coherence, and visual appeal, approaching SOTA commercial system quality.

Conclusion: PosterCraft provides a robust, unified framework for high-quality poster generation, leveraging automated data pipelines and a flexible approach to layout and style.

摘要: 生成美学海报比简单的设计图像更具挑战性：它不仅需要精确的文本渲染，还需要无缝整合抽象艺术内容、引人注目的布局和整体风格和谐。为此，我们提出了PosterCraft，一个统一的框架，摒弃了先前的模块化流程和僵化的预定义布局，使模型能够自由探索连贯且视觉吸引人的构图。PosterCraft采用精心设计的级联工作流程来优化高质量美学海报的生成：（i）在我们新引入的Text-Render-2M数据集上进行大规模文本渲染优化；（ii）在HQ-Poster100K上进行区域感知的监督微调；（iii）通过最佳偏好优化进行美学文本强化学习；（iv）联合视觉语言反馈细化。每个阶段都配备了完全自动化的数据构建流程，以满足其特定需求，从而无需复杂的架构修改即可实现稳健训练。通过多项实验评估，PosterCraft在渲染准确性、布局连贯性和整体视觉吸引力方面显著优于开源基线，接近最先进的商业系统质量。我们的代码、模型和数据集可在项目页面找到：https://ephemeral182.github.io/PosterCraft

</details>


### [137] [Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales](https://arxiv.org/abs/2506.10774)
**中文标题：基于笔画的循环放大器：任意超大规模下的图像超分辨率**

*Wenhao Guo,Peng Lu,Xujun Peng,Zhaoran Zhao,Sheng Li*

Main category: cs.CV

TL;DR: The paper introduces Stroke-based Cyclic Amplifier (SbCA), a unified model for ultra-large image super-resolution, addressing performance decline and blurring in existing methods by decomposing images into stroke vectors and iteratively refining details.


<details>
  <summary>Details</summary>
Motivation: Existing Arbitrary-Scale Image Super-Resolution (ASISR) methods perform poorly at ultra-large scales beyond training data, causing blurring and artifacts. The paper aims to solve this by proposing a model that maintains high fidelity even at extreme upsampling factors.

Method: SbCA decomposes images into stroke vectors for magnification and uses a detail completion module to restore missing details. A cyclic strategy iteratively refines the image, trained once for all scales.

Result: SbCA outperforms state-of-the-art methods in ultra-large upsampling (e.g., ×100), producing high-quality, artifact-free images with superior visual fidelity.

Conclusion: The proposed SbCA model effectively addresses distribution drift and blurring in ultra-large super-resolution, delivering high-quality results beyond the capabilities of existing methods.

摘要: 先前的任意尺度图像超分辨率（ASISR）方法在放大因子超出训练数据范围时性能显著下降，导致严重模糊。为解决这一问题，我们提出了一种统一模型——基于笔画的循环放大器（SbCA），用于超大规模放大任务。SbCA的核心是笔画向量放大器，它将图像分解为一系列以矢量图形表示的笔画进行放大。随后，细节补全模块恢复缺失细节，确保高保真图像重建。我们的循环策略通过迭代细化细节实现超大规模放大，仅需一次训练即可适用于所有尺度，同时保持子尺度在训练范围内。该方法有效解决了分布漂移问题，消除了伪影、噪声和模糊，生成高质量的高分辨率超分辨率图像。在合成和真实数据集上的实验验证表明，我们的方法在超大规模放大任务（如×100）中显著优于现有方法，视觉质量远超最先进技术。

</details>


### [138] [SlotPi: Physics-informed Object-centric Reasoning Models](https://arxiv.org/abs/2506.10778)
**中文标题：SlotPi：基于物理知识的对象中心推理模型**

*Jian Li,Wan Han,Ning Lin,Yu-Liang Zhan,Ruizhi Chengze,Haining Wang,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Hao Sun*

Main category: cs.CV

TL;DR: SlotPi is a physics-informed object-centric reasoning model that integrates Hamiltonian principles for dynamic forecasting, validated on diverse datasets including fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: Current object-centric dynamic simulation methods lack integration of physical knowledge and validation across diverse scenarios, especially involving fluids and objects.

Method: SlotPi combines a Hamiltonian-based physical module with a spatio-temporal prediction module for dynamic forecasting.

Result: SlotPi excels in prediction and VQA tasks on benchmark and fluid datasets, demonstrating strong adaptability.

Conclusion: SlotPi's robust performance lays a foundation for advanced world models, addressing gaps in physical knowledge integration and scenario adaptability.

摘要: 通过视觉观察理解和推理受物理定律支配的动态过程，类似于人类在现实世界中的能力，具有重大挑战。目前，模拟人类行为的对象中心动态模拟方法已取得显著进展，但忽略了两个关键方面：1）将物理知识整合到模型中。人类通过观察世界获得物理洞察，并应用这些知识准确推理各种动态场景；2）验证模型在多样化场景中的适应性。现实世界的动态，尤其是涉及流体和物体的动态，要求模型不仅能捕捉物体交互，还能模拟流体流动特性。为解决这些问题，我们提出了SlotPi，一种基于槽的物理知识对象中心推理模型。SlotPi将基于哈密顿原理的物理模块与时空预测模块结合，用于动态预测。实验表明，该模型在基准和流体数据集上的预测和视觉问答（VQA）任务中表现出色。此外，我们创建了一个涵盖物体交互、流体动态和流体-物体交互的真实世界数据集，验证了模型的性能。模型在所有数据集上的稳健表现凸显了其强大的适应性，为开发更先进的世界模型奠定了基础。

</details>


### [139] [Human-Robot Navigation using Event-based Cameras and Reinforcement Learning](https://arxiv.org/abs/2506.10790)
**中文标题：基于事件相机和强化学习的人机导航**

*Ignacio Bugueno-Cordova,Javier Ruiz-del-Solar,Rodrigo Verschae*

Main category: cs.CV

TL;DR: A robot navigation controller using event cameras and reinforcement learning for real-time human-centered navigation and obstacle avoidance, outperforming conventional image-based methods.


<details>
  <summary>Details</summary>
Motivation: Conventional image-based controllers suffer from motion blur, latency, and fixed processing rates. Event cameras offer asynchronous, adaptive visual processing, making them ideal for dynamic navigation tasks.

Method: Combines event-based perception, range sensing, and reinforcement learning (Deep Deterministic Policy Gradient) with an initial imitation learning phase for improved sample efficiency.

Result: Demonstrates robust navigation, pedestrian following, and obstacle avoidance in simulated environments.

Conclusion: The framework successfully integrates event cameras and reinforcement learning for adaptive, real-time navigation, showing promise for practical applications.

摘要: 本研究提出了一种结合事件相机和其他传感器与强化学习的机器人导航控制器，以实现实时以人为中心的导航和避障。与传统的基于图像的控制器（以固定速率运行且存在运动模糊和延迟问题）不同，该方法利用事件相机的异步特性，在灵活的时间间隔内处理视觉信息，从而实现自适应推理和控制。该框架集成了基于事件的感知、额外的距离传感以及通过深度确定性策略梯度进行的策略优化，并通过初始模仿学习阶段提高了样本效率。在模拟环境中取得了令人鼓舞的结果，展示了鲁棒的导航、行人跟随和避障能力。演示视频可在项目网站上查看。

</details>


### [140] [Prompts to Summaries: Zero-Shot Language-Guided Video Summarization](https://arxiv.org/abs/2506.10807)
**中文标题：从提示到摘要：零样本语言引导的视频摘要**

*Mario Barbara,Alaa Maalouf*

Main category: cs.CV

TL;DR: The paper introduces Prompts-to-Summaries, a zero-shot, text-queryable video summarization method that leverages off-the-shelf video-language models and large language models without training data, outperforming unsupervised and matching supervised methods.


<details>
  <summary>Details</summary>
Motivation: The explosive growth of video data necessitates flexible, user-controllable summarization tools that can operate without domain-specific training data and incorporate user intent expressed in natural language.

Method: The pipeline segments raw video into scenes, generates scene-level descriptions via a memory-efficient VidLM prompting scheme, uses an LLM to assign importance scores, and propagates scores to frame-level via consistency and uniqueness metrics.

Result: The method surpasses prior unsupervised methods on SumMe and TVSum and performs competitively on the QFVS benchmark without training data, while also introducing a new dataset, VidSum-Reason.

Conclusion: Pretrained multimodal models, combined with principled prompting and score propagation, provide a powerful foundation for universal, text-queryable video summarization.

摘要: 视频数据的爆炸性增长加剧了对无需领域特定训练数据且能灵活响应用户需求的摘要工具的需求。现有方法要么依赖数据集，限制了泛化能力，要么无法融入用户通过自然语言表达的意图。我们提出了“从提示到摘要”：首个零样本、可通过文本查询的视频摘要工具，它利用现成的视频-语言模型（VidLMs）生成字幕，并通过大型语言模型（LLMs）的评判将其转化为用户引导的摘要，完全无需训练数据，超越了所有无监督方法，并与有监督方法表现相当。我们的流程包括：（i）将原始视频分割为连贯场景，（ii）通过高效内存的批量式VidLM提示方案生成丰富的场景级描述，可扩展至单GPU处理数小时长的视频，（iii）利用LLM作为评判者，在精心设计的提示下为场景分配重要性分数，（iv）通过两个新指标（一致性（时间连贯性）和独特性（新颖性））将分数传播至短片段级别，得到细粒度的帧重要性。在SumMe和TVSum上，我们的无数据方法超越了所有依赖数据的无监督方法。在查询聚焦视频摘要（QFVS）基准测试中，尽管未使用训练数据且竞争对手需要监督帧级重要性，我们的方法仍表现优异。为促进进一步研究，我们发布了VidSum-Reason，一个包含长尾概念和多步推理的新查询驱动数据集；我们的框架在该数据集上取得了稳健的F1分数，并成为首个具有挑战性的基线。总体而言，我们的结果表明，预训练的多模态模型通过原则性提示和分数传播，已为通用、可通过文本查询的视频摘要提供了强大基础。

</details>


### [141] [Unsupervised Deformable Image Registration with Structural Nonparametric Smoothing](https://arxiv.org/abs/2506.10813)
**中文标题：基于结构非参数平滑的无监督可变形图像配准**

*Hang Zhang,Xiang Chen,Renjiu Hu,Rongguang Wang,Jinwei Zhang,Min Liu,Yaonan Wang,Gaolei Li,Xinxing Cheng,Jinming Duan*

Main category: cs.CV

TL;DR: The paper introduces SmoothProper, a neural module for unsupervised deformable image registration (DIR) that addresses challenges like aperture and large displacements by enforcing smoothness and structural consistency without label supervision.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised DIR methods struggle with images containing sparse features amid large smooth regions (e.g., retinal vessels), as they predict deformation fields in a single forward pass without post-training constraints, shifting regularization entirely to network weights.

Method: SmoothProper integrates a duality-based optimization layer with tailored interaction terms to propagate flow signals, enforce smoothness, and preserve structural consistency. It is model-agnostic, adds minimal parameter overhead, and eliminates hyperparameter tuning.

Result: On a retinal vessel dataset with aperture and large-displacement challenges, SmoothProper reduces registration error to 1.88 pixels on 2912x2912 images, outperforming existing unsupervised DIR methods.

Conclusion: SmoothProper is the first unsupervised DIR approach to effectively address aperture and large-displacement challenges, offering a plug-and-play solution that enhances registration accuracy without label supervision.

摘要: 基于学习的可变形图像配准（DIR）通过神经网络摊销传统优化过程，加速了图像对齐。标签监督进一步提高了准确性，实现了对未见扫描图像的高效、精确非线性对齐。然而，对于具有稀疏特征且包含大面积平滑区域的图像（如视网膜血管），现有无监督DIR方法难以解决孔径和大位移的挑战。这是因为神经网络在单次前向传递中预测变形场，导致场在训练后不受约束，并将正则化负担完全转移到网络权重上。为解决这些问题，我们提出了SmoothProper，一种即插即用的神经模块，通过在前向传递中强制平滑性和促进消息传递来优化配准。通过将基于对偶的优化层与定制的交互项相结合，SmoothPrope高效地在空间位置间传播流信号，强制平滑性并保持结构一致性。该模块与模型无关，能够无缝集成到现有配准框架中，且参数开销极小，同时消除了正则化超参数调优的需求。在具有孔径和大位移挑战的视网膜血管数据集上的初步结果表明，我们的方法将配准误差降至1.88像素（2912x2912图像），这是首个有效解决这两大挑战的无监督DIR方法。源代码将在https://github.com/tinymilky/SmoothProper上提供。

</details>


### [142] [Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders](https://arxiv.org/abs/2506.10816)
**中文标题：基于掩码自编码器的遮挡感知3D手物姿态估计**

*Hui Yang,Wei Sun,Jian Liu,Jin Zheng,Jian Xiao,Ajmal Mian*

Main category: cs.CV

TL;DR: The paper proposes HOMAE, an occlusion-aware method for 3D hand-object pose estimation using masked autoencoders, achieving state-of-the-art performance by integrating multi-scale features and combining implicit SDF with explicit point clouds.


<details>
  <summary>Details</summary>
Motivation: Hand-object pose estimation from monocular RGB images is challenging due to severe occlusions. Existing methods lack global structural perception, limiting their effectiveness.

Method: HOMAE uses a target-focused masking strategy to impose structured occlusion, integrates multi-scale features to predict an SDF, and combines implicit SDF with explicit point clouds for robust handling of occlusions.

Result: HOMAE achieves state-of-the-art performance on DexYCB and HO3Dv2 benchmarks.

Conclusion: The proposed method effectively addresses occlusion challenges in hand-object pose estimation by leveraging global context and local geometry.

摘要: 从单目RGB图像中估计手物姿态仍然是一个重大挑战，主要是由于手物交互中固有的严重遮挡。现有方法未能充分探索全局结构感知和推理，限制了其在处理遮挡手物交互中的效果。为解决这一挑战，我们提出了一种基于掩码自编码器的遮挡感知手物姿态估计方法，称为HOMAE。具体而言，我们提出了一种目标聚焦的掩码策略，对手物交互区域施加结构化遮挡，促使模型学习上下文感知特征并推理遮挡结构。我们进一步整合从解码器提取的多尺度特征，预测一个有符号距离场（SDF），捕捉全局上下文和细粒度几何。为增强几何感知，我们将隐式SDF与从SDF导出的显式点云结合，利用两种表示的互补优势。这种融合通过结合SDF的全局上下文和点云提供的精确局部几何，实现了对遮挡区域的更鲁棒处理。在具有挑战性的DexYCB和HO3Dv2基准上的大量实验表明，HOMAE在手物姿态估计中达到了最先进的性能。我们将发布代码和模型。

</details>


### [143] [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821)
**中文标题：VideoDeepResearch：基于代理工具的长视频理解研究**

*Huaying Yuan,Zheng Liu,Junjie Zhou,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CV

TL;DR: VideoDeepResearch introduces an agentic framework for long video understanding (LVU) using a text-only reasoning model and modular tools, outperforming MLLM baselines by significant margins.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with LVU due to complexity and context constraints. The paper challenges the need for extended context MLLMs by proposing a simpler, tool-based approach.

Method: The framework combines a text-only reasoning model with modular tools (multimodal retrievers, visual perceivers) to selectively access and utilize video content.

Result: VideoDeepResearch outperforms MLLM baselines by 9.6%, 6.6%, and 3.9% on MLVU, LVBench, and LongVideoBench, respectively.

Conclusion: Agentic systems with modular tools can effectively address LVU challenges without relying on complex MLLMs.

摘要: 长视频理解（LVU）由于其固有的复杂性和上下文窗口限制，对当前的多模态大语言模型（MLLM）提出了重大挑战。普遍认为解决LVU任务需要具备扩展上下文窗口、强大视觉感知能力和熟练领域知识的基础MLLM。在这项工作中，我们通过引入VideoDeepResearch，一种新颖的代理框架，挑战了这一常见观点。我们的方法仅依赖于纯文本的大推理模型（LRM）与模块化多模态工具包（包括多模态检索器和视觉感知器）的结合，这些工具在实践中易于获取。对于每个LVU任务，系统通过推理制定问题解决策略，同时选择性访问和利用关键视频内容。我们在流行的LVU基准测试（包括MLVU、Video-MME和LVBench）上进行了广泛实验。结果表明，VideoDeepResearch显著优于现有MLLM基线，在MLVU（测试）、LVBench和LongVideoBench上分别超越了之前的最先进水平9.6%、6.6%和3.9%。这些发现凸显了代理系统在克服LVU问题关键挑战中的潜力。

</details>


### [144] [Post-Training Quantization for Video Matting](https://arxiv.org/abs/2506.10840)
**中文标题：视频抠图的后训练量化**

*Tianrui Zhu,Houyuan Chen,Ruihao Gong,Michele Magno,Haotong Qin,Kai Zhang*

Main category: cs.CV

TL;DR: The paper proposes a novel Post-Training Quantization (PTQ) framework for video matting, combining block-reconstruction optimization, global calibration, and optical flow assistance to achieve high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Video matting is computationally intensive, and deploying it on resource-constrained devices is challenging. PTQ is underdeveloped for video matting, facing issues in accuracy and temporal coherence.

Method: The framework includes a two-stage PTQ strategy (block-reconstruction optimization and global calibration) and an Optical Flow Assistance component to enhance temporal coherence.

Result: PTQ4VM achieves state-of-the-art accuracy across bit-widths, with 4-bit quantization performing close to full-precision while saving 8x FLOPs.

Conclusion: The proposed PTQ framework effectively addresses challenges in video matting quantization, achieving high accuracy and efficiency.

摘要: 视频抠图在电影制作和虚拟现实等应用中至关重要，但其计算密集型模型在资源受限设备上的部署面临挑战。量化是模型压缩和加速的关键技术。作为一种高效方法，后训练量化（PTQ）在视频抠图中仍处于起步阶段，面临保持准确性和时间一致性的重大障碍。为解决这些问题，本文提出了一种专为视频抠图模型设计的新颖通用PTQ框架，据我们所知，这是该领域的首次系统性尝试。我们的贡献包括：（1）一种两阶段PTQ策略，结合基于块重建的优化以实现快速、稳定的初始量化和局部依赖捕获，随后通过全局校准量化参数以最小化精度损失。（2）一种统计驱动的全局仿射校准（GAC）方法，使网络能够补偿因忽略BN层效应等因素引起的累积统计失真，甚至将现有PTQ方法在视频抠图任务中的误差降低高达20%。（3）一种光流辅助（OFA）组件，利用帧的时空和语义先验指导PTQ过程，增强模型在复杂场景中区分运动前景的能力，最终在超低位量化下实现接近全精度的性能。全面的定量和视觉结果表明，与现有量化方法相比，PTQ4VM在不同位宽下均实现了最先进的精度性能。我们强调，4位PTQ4VM甚至实现了接近全精度的性能，同时节省了8倍的浮点运算量。

</details>


### [145] [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos](https://arxiv.org/abs/2506.10857)
**中文标题：VRBench：长叙事视频中多步推理的基准测试**

*Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: VRBench is a new benchmark for evaluating multi-step reasoning in long narrative videos, featuring 1,010 videos, 9,468 QA pairs, and 30,292 reasoning steps, with a focus on temporal and procedural validity.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack focus on temporal reasoning and procedural validity in long videos, limiting the assessment of large models' multi-step reasoning capabilities.

Method: VRBench includes a multi-stage filtering process for video curation, a human-AI framework for generating reasoning chains, and a multi-phase evaluation pipeline with MCQ and LLM-guided scoring.

Result: Extensive evaluations of 12 LLMs and 16 VLMs on VRBench provided insights into multi-step reasoning, highlighting model strengths and weaknesses.

Conclusion: VRBench advances the field by offering a comprehensive benchmark for evaluating multi-step reasoning in long videos, with potential for future improvements.

摘要: 我们提出了VRBench，这是首个专为评估大型模型在多步推理能力方面的长叙事视频基准测试，解决了现有评估中忽视时间推理和程序有效性的问题。它包含1,010个长视频（平均时长1.6小时），以及9,468个人工标注的多步问答对和30,292个带时间戳的推理步骤。这些视频通过多阶段筛选过程（包括专家互评）以确保情节连贯性。我们开发了一个人机协作框架，生成连贯的推理链，每条链需要多个时间锚定的步骤，涵盖七种类型（如事件归因、隐式推理）。VRBench设计了一个多阶段评估流程，从结果和过程两个层面评估模型。除了用于最终结果的多选题外，我们还提出了一个基于进度的LLM引导评分指标，从多个维度全面评估推理链的质量。通过对12个LLM和16个VLM在VRBench上的广泛评估，我们进行了深入分析，并提供了推动多步推理领域发展的宝贵见解。

</details>


### [146] [CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation](https://arxiv.org/abs/2506.10890)
**中文标题：CreatiPoster：迈向可编辑和可控的多层图形设计生成**

*Zhao Zhang,Yutao Cheng,Dexiang Hong,Maoke Yang,Gonglei Shi,Lei Ma,Hui Zhang,Jie Shao,Xinglong Wu*

Main category: cs.CV

TL;DR: CreatiPoster is a framework for generating editable, multi-layer graphic designs from natural-language instructions or assets, outperforming existing tools and releasing a 100,000-design corpus.


<details>
  <summary>Details</summary>
Motivation: Current AI tools for graphic design struggle with editability, user asset integration, and professional appeal, while template-based systems are impractical. CreatiPoster aims to democratize high-quality, editable design generation.

Method: CreatiPoster uses a protocol model (RGBA large multimodal model) to generate a JSON specification for each layer (text/asset) and a conditional background model to synthesize coherent backgrounds. It supports diverse applications like canvas editing and multilingual adaptation.

Result: CreatiPoster surpasses leading open-source and proprietary systems, demonstrated by automated metrics. A copyright-free corpus of 100,000 designs is released to aid research.

Conclusion: CreatiPoster advances AI-assisted graphic design by enabling editable, high-quality compositions and supporting diverse applications, fostering democratization in the field.

摘要: 图形设计在商业和个人场景中扮演着关键角色，然而创建高质量、可编辑且美观的图形组合仍然是一项耗时且需要专业技能的任务，尤其是对初学者而言。当前的AI工具虽然自动化了部分工作流程，但在准确整合用户提供的素材、保持可编辑性和实现专业视觉吸引力方面仍有不足。商业系统（如Canva Magic Design）依赖庞大的模板库，但这些模板难以复制。本文介绍了CreatiPoster，一个从可选的自然语言指令或素材生成可编辑多层组合的框架。协议模型（一种RGBA大型多模态模型）首先生成一个JSON规范，详细描述每一层（文本或素材）的精确布局、层次、内容和样式，以及简洁的背景提示。随后，条件背景模型根据这些渲染的前景层合成连贯的背景。我们构建了一个带有自动化指标的图形设计生成基准，并展示CreatiPoster超越了领先的开源方法和专有商业系统。为了推动进一步研究，我们发布了一个包含10万个多层设计的无版权语料库。CreatiPoster支持多种应用，如画布编辑、文本叠加、响应式调整、多语言适配和动态海报，推动了AI辅助图形设计的民主化。项目主页：https://github.com/graphic-design-ai/creatiposter

</details>


### [147] [AIR: Zero-shot Generative Model Adaptation with Iterative Refinement](https://arxiv.org/abs/2506.10895)
**中文标题：AIR：通过迭代优化的零样本生成模型适应**

*Guimeng Liu,Milad Abdollahzadeh,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: The paper introduces AIR, a zero-shot generative model adaptation method that improves image quality by addressing misalignment between text and image offsets in CLIP embedding space, leveraging iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot generative model adaptation methods assume perfect alignment between text and image offsets in CLIP embedding space, leading to degraded image quality. The paper aims to address this limitation by analyzing and mitigating offset misalignment.

Method: The authors propose Adaptation with Iterative Refinement (AIR), which focuses on improving target domain image quality by leveraging insights from offset misalignment studies in NLP. AIR iteratively refines the alignment between text and image offsets.

Result: AIR achieves state-of-the-art performance in qualitative, quantitative, and user studies across 26 experimental setups, demonstrating superior image quality compared to existing methods.

Conclusion: The study highlights the importance of addressing offset misalignment in zero-shot generative model adaptation and validates AIR as an effective solution for improving image quality.

摘要: 零样本生成模型适应（ZSGM）旨在仅通过文本指导，无需目标域的任何样本，将预训练的生成器适应到目标域。近期ZSGM方法的核心是方向性损失，它利用文本指导的形式，在视觉-语言模型（如CLIP）的嵌入空间中对齐图像偏移与文本偏移。这与NLP中的类比推理类似，即通过对齐两对词之间的偏移来识别另一对词中的缺失元素。然而，现有ZSGM方法的一个主要局限是学习目标假设图像偏移与文本偏移在CLIP嵌入空间中完全对齐，导致生成图像质量下降。我们的工作有两个主要贡献。受NLP中偏移不对齐研究的启发，作为第一个贡献，我们进行了一项实证研究，分析了CLIP嵌入空间中文本偏移与图像偏移的不对齐现象，发现偏移不对齐与概念距离相关，即相近概念的不对齐程度较低。作为第二个贡献，我们提出了迭代优化适应（AIR），这是首个基于偏移不对齐新见解的ZSGM方法，专注于提高目标域图像质量。在26个实验设置中，定性、定量和用户研究一致表明，AIR方法达到了最先进的性能。补充实验见附录。

</details>


### [148] [M4V: Multi-Modal Mamba for Text-to-Video Generation](https://arxiv.org/abs/2506.10915)
**中文标题：M4V：用于文本到视频生成的多模态Mamba框架**

*Jiancheng Huang,Gengwei Zhang,Zequn Jie,Siyu Jiao,Yinlong Qian,Ling Chen,Yunchao Wei,Lin Ma*

Main category: cs.CV

TL;DR: M4V introduces a Multi-Modal Mamba framework for efficient text-to-video generation, reducing computational costs by 45% compared to Transformer-based methods while maintaining high-quality output.


<details>
  <summary>Details</summary>
Motivation: Text-to-video generation is computationally demanding with Transformers due to quadratic complexity. Mamba offers linear-time efficiency but lacks multi-modal and spatiotemporal modeling capabilities, which M4V aims to address.

Method: M4V uses a multi-modal diffusion Mamba (MM-DiM) block for integrating multi-modal information and spatiotemporal modeling, along with a reward learning strategy to enhance visual realism.

Result: M4V reduces FLOPs by 45% at 768×1280 resolution and produces high-quality videos, outperforming attention-based methods in efficiency and quality.

Conclusion: M4V provides an efficient and high-quality solution for text-to-video generation, balancing computational cost and performance.

摘要: 文本到视频生成极大地丰富了内容创作，并有望发展为强大的世界模拟器。然而，建模广阔的时空空间在计算上仍然非常昂贵，尤其是使用Transformer时，其序列处理的二次复杂度限制了实际应用。近年来，线性时间序列建模的进展，特别是Mamba架构，提供了更高效的替代方案。然而，其简单设计限制了其在多模态和时空视频生成任务中的直接适用性。为解决这些问题，我们提出了M4V，一种用于文本到视频生成的多模态Mamba框架。具体而言，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态令牌重组设计实现多模态信息和时空建模的无缝集成。因此，在生成768×1280分辨率的视频时，M4V中的Mamba块比基于注意力的替代方案减少了45%的FLOPs。此外，为缓解长上下文自回归生成过程中的视觉质量下降，我们引入了一种奖励学习策略，进一步提升了每帧的视觉真实感。在文本到视频基准测试中的广泛实验表明，M4V能够在显著降低计算成本的同时生成高质量视频。代码和模型将在https://huangjch526.github.io/M4V_project公开提供。

</details>


### [149] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
**中文标题：VINCIE：从视频中解锁上下文图像编辑**

*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: The paper introduces VINCIE, a method for in-context image editing trained directly from videos using a block-causal diffusion transformer, achieving state-of-the-art results on multi-turn editing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing in-context image editing methods rely on task-specific pipelines and expert models, limiting scalability. The paper explores whether such models can be learned directly from videos, leveraging their inherent sequential and multimodal nature.

Method: The authors propose a scalable approach to annotate videos as interleaved multimodal sequences and train a block-causal diffusion transformer on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction.

Result: The model demonstrates strong in-context image editing capabilities, achieving state-of-the-art results on multi-turn editing benchmarks and showing promise in multi-concept composition, story generation, and chain-of-editing applications.

Conclusion: VINCIE successfully learns in-context image editing directly from videos, offering a scalable and versatile approach that outperforms existing methods.

摘要: 上下文图像编辑旨在基于包含文本和先前生成图像的上下文序列修改图像。现有方法通常依赖于任务特定的流程和专家模型（例如分割和修复）来整理训练数据。在这项工作中，我们探讨了是否可以直接从视频中学习上下文图像编辑模型。我们引入了一种可扩展的方法，将视频注释为交错的多模态序列。为了有效地从这些数据中学习，我们设计了一个基于三个代理任务训练的块因果扩散变换器：下一图像预测、当前分割预测和下一分割预测。此外，我们提出了一个新的多轮图像编辑基准，以推动该领域的研究。大量实验表明，我们的模型表现出强大的上下文图像编辑能力，并在两个多轮图像编辑基准上取得了最先进的结果。尽管仅在视频上训练，我们的模型在多概念组合、故事生成和编辑链应用中显示出有前景的能力。

</details>


### [150] [SpectralAR: Spectral Autoregressive Visual Generation](https://arxiv.org/abs/2506.10962)
**中文标题：SpectralAR：基于频谱自回归的视觉生成**

*Yuanhui Huang,Weiliang Chen,Wenzhao Zheng,Yueqi Duan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SpectralAR introduces a spectral autoregressive framework for visual generation, using ordered spectral tokens to achieve causality and efficiency, outperforming existing methods with fewer tokens and parameters.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive visual generation methods use spatial patches, which are inherently parallel and contradict causality. SpectralAR addresses this by leveraging spectral sequences for causality and efficiency.

Method: SpectralAR transforms images into ordered spectral tokens via Nested Spectral Tokenization, representing low to high frequencies, and performs autoregressive generation in a coarse-to-fine manner.

Result: SpectralAR achieves a gFID score of 3.02 on ImageNet-1K with only 64 tokens and 310M parameters, demonstrating superior performance in image reconstruction and generation.

Conclusion: SpectralAR successfully integrates causality and efficiency in autoregressive visual generation by utilizing spectral sequences, offering a scalable and effective alternative to spatial patch-based methods.

摘要: 由于自回归视觉生成在可扩展性和与其他模态的兼容性方面优于扩散模型，其受到越来越多的关注。现有方法大多将视觉序列构建为空间块进行自回归生成。然而，图像块本质上是并行的，与自回归建模的因果性相矛盾。为解决这一问题，我们提出了一种基于频谱的自回归（SpectralAR）视觉生成框架，从频谱角度实现视觉序列的因果性。具体而言，我们首先通过嵌套频谱标记化将图像转换为有序的频谱标记，代表从低频到高频的组件。随后，我们以从粗到细的方式对频谱标记序列进行自回归生成。通过考虑图像的不同细节层次，SpectralAR在不增加额外复杂度的情况下实现了序列因果性和标记效率。我们在ImageNet-1K上进行了大量实验，用于图像重建和自回归生成，SpectralAR仅用64个标记和310M参数就达到了3.02的gFID分数。项目页面：https://huang-yh.github.io/spectralar/。

</details>


### [151] [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
**中文标题：MMMG：一个用于文本到图像推理的大规模、多学科、多层次生成基准**

*Yuxuan Luo,Yuhui Yuan,Junwen Chen,Haonan Cai,Ziyi Yue,Yuwei Yang,Fatima Zohra Daha,Ji Li,Zhouhui Lian*

Main category: cs.CV

TL;DR: The paper introduces MMMG, a benchmark for evaluating text-to-image models' reasoning capabilities in generating knowledge-rich images, revealing significant deficits in current models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating knowledge-rich images that require multimodal reasoning, the authors propose MMMG as a benchmark to evaluate and improve text-to-image models.

Method: MMMG includes 4,456 expert-validated image-prompt pairs across 10 disciplines and 6 educational levels, using a unified Knowledge Graph (KG) representation for evaluation. MMMG-Score combines factual fidelity (graph-edit distance) and visual clarity to assess models.

Result: Evaluations of 16 state-of-the-art models show low reasoning capabilities, with GPT-4o scoring only 50.20. The baseline model FLUX-Reason achieves a score of 34.45.

Conclusion: MMMG highlights the limitations of current models in generating knowledge images and provides a benchmark and baseline to drive future research.

摘要: 本文提出知识图像生成作为一项新任务，并介绍了大规模多学科多层次知识图像生成基准（MMMG），以探究图像生成模型的推理能力。知识图像在人类文明和学习机制中占据核心地位，这一点由双重编码理论和图片优势效应所强调。生成此类图像具有挑战性，需要将世界知识与像素级基础融合为清晰的解释性视觉内容。为进行全面评估，MMMG提供了4,456个经过专家验证的知识图像-提示对，涵盖10个学科、6个教育层次以及多种知识格式（如图表、思维导图等）。为消除评估中的混杂复杂性，我们采用统一的知识图谱（KG）表示。每个KG明确描述了目标图像的核心实体及其依赖关系。我们还引入了MMMG-Score来评估生成的知识图像，该指标结合了事实保真度（通过KG间的图编辑距离衡量）和视觉清晰度评估。对16种最先进的文本到图像生成模型的全面评估揭示了严重的推理缺陷——实体保真度低、关系弱且杂乱，其中GPT-4o的MMMG-Score仅为50.20，凸显了该基准的难度。为促进进一步研究，我们发布了FLUX-Reason（MMMG-Score为34.45），这是一个结合推理大语言模型和扩散模型的有效开源基线，训练于16,000个精选知识图像-提示对。

</details>


### [152] [Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/abs/2506.10967)
**中文标题：超越注意力或相似性：最大化MLLM中令牌剪枝的条件多样性**

*Qizhe Zhang,Mengzhen Liu,Lichen Li,Ming Lu,Yuan Zhang,Junwen Pan,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: The paper introduces CDPruner, a novel visual token pruning method for MLLMs that maximizes conditional diversity using DPP, achieving high efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current token pruning methods in MLLMs either retain redundant tokens (attention-based) or ignore instruction relevance (similarity-based), leading to suboptimal performance.

Method: CDPruner defines conditional similarity between visual tokens and reformulates pruning using DPP to maximize conditional diversity, ensuring the selected subset is both representative and instruction-aligned.

Result: CDPruner reduces FLOPs by 95% and CUDA latency by 78% in LLaVA while maintaining 94% accuracy, outperforming other methods on vision-language benchmarks.

Conclusion: CDPruner is a training-free, model-agnostic solution that effectively balances efficiency and performance in MLLMs by focusing on conditional diversity.

摘要: 在多模态大语言模型（MLLM）中，输入视觉令牌的长度通常远大于文本令牌，导致高推理成本。许多工作试图通过去除冗余视觉令牌来解决这一问题。然而，当前方法要么依赖基于注意力的剪枝（保留大量重复令牌），要么使用基于相似性的剪枝（忽略指令相关性），从而导致性能不佳。本文提出了一种名为CDPruner的新型视觉令牌剪枝方法，通过最大化保留令牌的条件多样性来超越注意力或相似性。我们首先定义了基于指令的视觉令牌之间的条件相似性，然后使用行列式点过程（DPP）重新表述令牌剪枝问题，以最大化所选子集的条件多样性。所提出的CDPruner无需训练且与模型无关，可轻松应用于各种MLLM。在多种MLLM上的广泛实验表明，CDPruner在各种视觉语言基准测试中达到了新的最优水平。通过DPP最大化条件多样性，所选子集更好地代表了输入图像，同时严格遵循用户指令，从而在高剪枝比例下仍保持强大性能。应用于LLaVA时，CDPruner将FLOPs减少了95%，CUDA延迟降低了78%，同时保持了94%的原始准确率。代码可在https://github.com/Theia-4869/CDPruner获取。

</details>


### [153] [GenWorld: Towards Detecting AI-generated Real-world Simulation Videos](https://arxiv.org/abs/2506.10975)
**中文标题：GenWorld：面向检测AI生成的现实世界模拟视频**

*Weiliang Chen,Wenzhao Zheng,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu,Yueqi Duan*

Main category: cs.CV

TL;DR: The paper introduces GenWorld, a high-quality real-world simulation dataset for detecting AI-generated videos, and proposes SpannDetector, a model leveraging multi-view consistency for improved detection performance.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated videos threatens information credibility, but existing detectors lack high-quality real-world datasets, hindering their development.

Method: GenWorld is created with real-world simulation videos from diverse generators and prompts. SpannDetector uses multi-view consistency to detect AI-generated videos.

Result: SpannDetector outperforms existing methods, especially in detecting high-quality videos, showcasing the importance of real-world clues.

Conclusion: GenWorld and SpannDetector advance AI-generated video detection, emphasizing physical plausibility for explainable results.

摘要: 视频生成技术的繁荣威胁了现实世界信息的可信度，并加剧了对AI生成视频检测器的需求。尽管取得了一些进展，但高质量现实世界数据集的缺乏阻碍了可信检测器的发展。本文提出了GenWorld，一个大规模、高质量、现实世界模拟的数据集，用于AI生成视频检测。GenWorld具有以下特点：（1）现实世界模拟：GenWorld专注于复制现实场景的视频，这些视频因其真实性和潜在影响而具有重要意义；（2）高质量：GenWorld采用多种最先进的视频生成模型，提供逼真且高质量的伪造视频；（3）跨提示多样性：GenWorld包含来自不同生成器和多种提示模态（如文本、图像、视频）生成的视频，为学习更具泛化性的取证特征提供了潜力。我们分析了现有方法，发现它们无法检测世界模型（如Cosmos）生成的高质量视频，揭示了忽略现实世界线索的潜在缺陷。为此，我们提出了一个简单而有效的模型SpannDetector，利用多视角一致性作为现实世界AI生成视频检测的强大标准。实验表明，我们的方法取得了优异的结果，突出了基于物理合理性的可解释AI生成视频检测的有前景方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld

</details>


### [154] [QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2506.10977)
**中文标题：QuadricFormer：将场景视为超二次曲面以实现3D语义占用预测**

*Sicheng Zuo,Wenzhao Zheng,Xiaoyong Han,Longchao Yang,Yong Pan,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper proposes QuadricFormer, a method using superquadrics for efficient 3D semantic occupancy prediction, outperforming existing voxel-based and Gaussian-based approaches in both accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D occupancy prediction either use inefficient dense voxel representations or limited ellipsoidal Gaussians, failing to capture diverse object geometries in driving scenes. Superquadrics offer a more expressive and efficient alternative.

Method: The paper introduces a probabilistic superquadric mixture model for occupancy prediction, where each superquadric represents a probability distribution with geometric priors. QuadricFormer, the proposed model, includes a pruning-and-splitting module to optimize superquadric placement in occupied regions.

Result: QuadricFormer achieves state-of-the-art performance on the nuScenes dataset, demonstrating superior efficiency and accuracy in modeling complex driving scenes.

Conclusion: Superquadrics provide an effective and efficient representation for 3D semantic occupancy prediction, addressing the limitations of voxel-based and Gaussian-based methods.

摘要: 3D占用预测对于稳健的自动驾驶系统至关重要，因为它能够全面感知环境结构和语义。现有方法大多采用密集体素表示，忽略了驾驶场景的稀疏性，导致效率低下。近期研究探索了基于稀疏高斯分布的物体中心表示，但其椭球形先验限制了多样化结构的建模。在真实驾驶场景中，物体呈现丰富的几何形状（如长方体、圆柱体和不规则形状），需要密集排列的椭球形高斯分布以实现精确建模，导致表示效率低下。为解决这一问题，我们提出使用几何表达能力强的超二次曲面作为场景基元，通过其固有的形状多样性，用更少的基元高效表示复杂结构。我们开发了一种概率超二次曲面混合模型，将每个超二次曲面解释为具有几何先验的占用概率分布，并通过概率混合计算语义。在此基础上，我们提出了QuadricFormer，一种基于超二次曲面的高效3D占用预测模型，并引入剪枝与分割模块，通过将超二次曲面集中在占用区域进一步提升建模效率。在nuScenes数据集上的大量实验表明，QuadricFormer在保持高效的同时实现了最先进的性能。

</details>


### [155] [Fine-Grained Perturbation Guidance via Attention Head Selection](https://arxiv.org/abs/2506.10978)
**中文标题：通过注意力头选择的细粒度扰动引导**

*Donghoon Ahn,Jiwon Kang,Sanghyun Lee,Minjae Kim,Jaewon Min,Wooseok Jang,Saungwu Lee,Sayak Paul,Susung Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces 'HeadHunter', a framework for fine-grained attention head selection in Diffusion Transformers (DiTs), enabling targeted control over generation quality and visual attributes. It also proposes 'SoftPAG' for tunable perturbation strength, outperforming layer-level methods.


<details>
  <summary>Details</summary>
Motivation: Existing attention perturbation methods lack principled approaches for determining where to apply perturbations in DiTs, especially as quality-relevant computations are distributed across layers. The paper aims to address this by analyzing granularity and enabling targeted control.

Method: Investigates attention perturbation granularity (layer to head level), identifies heads governing specific visual concepts, and proposes 'HeadHunter' for iterative head selection aligned with user objectives. Introduces 'SoftPAG' for continuous perturbation strength tuning.

Result: Demonstrates superior performance in general quality enhancement and style-specific guidance on DiT-based models (Stable Diffusion 3, FLUX.1), mitigating oversmoothing and enabling targeted visual style manipulation.

Conclusion: The work provides the first head-level analysis of attention perturbation in diffusion models, revealing interpretable specialization and practical strategies for effective perturbation design.

摘要: 近年来，扩散模型中的引导方法通过扰动模型构建隐式弱模型，并引导生成远离该模型。在这些方法中，注意力扰动在无条件场景（如无分类器引导不适用时）表现出强大的实证性能。然而，现有的注意力扰动方法缺乏确定扰动应施加位置的原则性方法，尤其是在扩散变换器（DiT）架构中，质量相关计算分布在多个层中。本文研究了注意力扰动的粒度（从层级别到单个注意力头），发现特定头控制不同的视觉概念（如结构、风格和纹理质量）。基于这一发现，我们提出了“HeadHunter”，一个系统性框架，用于迭代选择与用户目标一致的注意力头，从而实现对生成质量和视觉属性的细粒度控制。此外，我们引入了“SoftPAG”，通过将每个选定头的注意力图线性插值到单位矩阵，提供连续调节扰动强度的功能，并抑制伪影。我们的方法不仅缓解了现有层级别扰动的过度平滑问题，还通过组合头选择实现了对特定视觉风格的有针对性操控。我们在现代大规模基于DiT的文本到图像模型（包括Stable Diffusion 3和FLUX.1）上验证了方法的有效性，展示了在通用质量提升和风格特定引导方面的卓越性能。我们的工作首次对扩散模型中的注意力扰动进行了头级别分析，揭示了注意力层内的可解释专业化，并为设计有效的扰动策略提供了实用方法。

</details>


### [156] [InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model](https://arxiv.org/abs/2506.10980)
**中文标题：InstaInpaint：基于掩码大重建模型的即时3D场景修复**

*Junqi You,Chieh Hubert Lin,Weijie Lyu,Zhengbo Zhang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: InstaInpaint is a fast 3D-scene inpainting method that achieves real-time performance (0.4 seconds) using a feed-forward framework and a custom large reconstruction model, outperforming prior methods by 1000x in speed while maintaining high quality.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene inpainting methods are slow and computationally intensive, making them impractical for real-time or interactive applications. The paper aims to address this gap by proposing a faster, efficient solution.

Method: The paper introduces InstaInpaint, a feed-forward framework that generates 3D inpainting from 2D proposals. It uses a self-supervised masked-finetuning strategy to train a custom large reconstruction model (LRM) on large-scale data.

Result: InstaInpaint achieves a 1000x speed-up over prior methods while maintaining state-of-the-art performance on benchmarks. It also generalizes well to applications like object insertion and multi-region inpainting.

Conclusion: InstaInpaint provides a practical, high-speed solution for 3D scene inpainting, enabling real-time interactive operations with superior performance and flexibility.

摘要: 近年来，3D场景重建技术的进步使得虚拟和增强现实中的实时观看成为可能。为了支持交互式操作（如移动或编辑物体）以提升沉浸感，3D场景修复方法被提出用于修复或补全改变的几何结构。然而，现有方法依赖于耗时且计算密集的优化过程，使其难以应用于实时或在线场景。我们提出了InstaInpaint，一种基于参考的前馈框架，可在0.4秒内从2D修复提案生成3D场景修复结果。我们开发了一种自监督的掩码微调策略，用于在大规模数据集上训练定制的大重建模型（LRM）。通过大量实验，我们分析并确定了多个关键设计，以提高泛化能力、纹理一致性和几何正确性。InstaInpaint在保持两个标准基准测试中领先性能的同时，实现了比现有方法快1000倍的速度。此外，我们还展示了InstaInpaint在灵活的下游应用（如物体插入和多区域修复）中的良好泛化能力。更多视频结果请访问我们的项目页面：https://dhmbb2.github.io/InstaInpaint_page/。

</details>


### [157] [SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis](https://arxiv.org/abs/2506.10981)
**中文标题：SceneCompleter：用于生成式新视角合成的密集3D场景补全**

*Weiliang Chen,Jiayi Bi,Yuanhui Huang,Wenzhao Zheng,Yueqi Duan*

Main category: cs.CV

TL;DR: SceneCompleter introduces a novel framework for 3D-consistent generative novel view synthesis by completing dense 3D scenes, combining geometry and appearance through a dual-stream diffusion model and a scene embedder.


<details>
  <summary>Details</summary>
Motivation: Existing methods for novel view synthesis rely on 2D completion followed by 3D recovery, leading to smooth surfaces and distorted geometry due to the difficulty of inferring 3D structure from RGB data alone.

Method: SceneCompleter uses a geometry-appearance dual-stream diffusion model to synthesize novel views in RGBD space and a scene embedder for holistic scene understanding from reference images.

Result: The method achieves superior coherence and plausibility in generative novel view synthesis across diverse datasets by effectively fusing structural and textural information.

Conclusion: SceneCompleter demonstrates a significant improvement in 3D-consistent generative novel view synthesis by addressing the limitations of existing approaches.

摘要: 生成模型在新视角合成（NVS）中通过减少对密集多视角捕获的依赖而受到广泛关注。然而，现有方法通常遵循传统范式，即生成模型先在2D中补全缺失区域，随后通过3D恢复技术重建场景，这往往导致过度平滑的表面和扭曲的几何形状，因为生成模型难以仅从RGB数据推断3D结构。本文提出SceneCompleter，一种通过密集3D场景补全实现3D一致性生成新视角合成的新框架。SceneCompleter通过两个关键组件实现视觉一致性和3D一致的生成场景补全：（1）一种几何-外观双流扩散模型，联合在RGBD空间中合成新视角；（2）一种场景编码器，从参考图像中编码更全面的场景理解。通过有效融合结构和纹理信息，我们的方法在多样数据集上展示了生成新视角合成的优越一致性和合理性。项目页面：https://chen-wl20.github.io/SceneCompleter

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [158] [A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/abs/2506.10130)
**中文标题：关于符号AI与生成式AI中确定性与范围之间基本权衡的猜想**

*Luciano Floridi*

Main category: cs.AI

TL;DR: The paper conjectures a trade-off between provable correctness and broad data-mapping capacity in AI, suggesting that systems with high certainty (symbolic AI) have narrow scope, while broad-capacity systems (generative AI) cannot achieve zero-error performance.


<details>
  <summary>Details</summary>
Motivation: To formalize and make explicit the implicit trade-off between certainty and scope in AI systems, reframing engineering and philosophical expectations for AI.

Method: The article reviews historical motivations, states the conjecture in information-theoretic terms, and contextualizes it within epistemology, formal verification, and philosophy of technology. It analyzes implications using underdetermination, epistemic risk, and moral responsibility.

Result: The conjecture clarifies evaluation standards, governance frameworks, and hybrid system design, emphasizing the need for rigorous verification.

Conclusion: Proving or refuting the conjecture is crucial for the future of trustworthy AI, as it reshapes expectations and design principles.

摘要: 本文提出一个猜想，形式化地描述了人工智能（AI）系统中可证明的正确性与广泛数据映射能力之间的基本权衡。当AI系统被设计为具有演绎上滴水不漏的保证（对其输出无错误性质的确定性证明）——如经典符号AI——其操作领域必须被严格限定和预先结构化。相反，能够输入高维数据以产生丰富信息输出的系统——如当代生成模型——必然放弃了零错误性能的可能性，承担了不可消除的错误或误分类风险。通过将这一先前隐含的权衡明确化并接受严格验证，该猜想显著重构了AI的工程目标和哲学期望。在回顾了这种矛盾的历史动机后，文章以信息论形式陈述了猜想，并将其置于认识论、形式验证和技术哲学的广泛讨论中。随后，文章分析了其影响和后果，借鉴了未确定性、谨慎认识风险及道德责任等概念。讨论阐明了如果猜想成立，它将如何帮助重塑评估标准、治理框架和混合系统设计。结论强调了最终证明或反驳这一不等式对于可信AI未来的重要性。

</details>


### [159] [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/abs/2506.10157)
**中文标题：一位患者，多种情境：通过情境智能扩展医疗AI**

*Michelle M. Li,Ben Y. Reis,Adam Rodman,Tianxi Cai,Noa Dagan,Ran D. Balicer,Joseph Loscalzo,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: The paper proposes context-switching AI for medical applications to dynamically adapt to diverse clinical contexts without retraining, addressing limitations of current models prone to contextual errors.


<details>
  <summary>Details</summary>
Motivation: Current medical AI models struggle with dynamically adjusting to new populations, specialties, or settings, leading to contextual errors. The paper aims to overcome this by proposing context-switching AI.

Method: The paper outlines a vision for AI models that dynamically adapt their reasoning to new specialties, populations, workflows, and clinical roles without requiring retraining.

Result: The envisioned context-switching AI can diagnose, manage, and treat diseases across specialties and regions, improving access to medical care.

Conclusion: Context-switching AI has the potential to revolutionize medical care by dynamically adapting to diverse clinical contexts, reducing errors, and expanding access.

摘要: 医疗基础模型，包括基于临床笔记训练的语言模型、基于医学图像的视觉语言模型以及基于电子健康记录的多模态模型，可以总结临床笔记、回答医学问题并辅助决策。将这些模型适应新的人群、专业或环境通常需要微调、精心设计的提示或从知识库中检索。这可能不切实际，并限制了它们解释不熟悉输入和适应训练中未涵盖的临床情境的能力。因此，模型容易出现情境错误，即预测看似合理但未能考虑关键的特定患者或情境信息。这些错误源于当前模型的一个基本局限：难以在医疗护理的不断变化情境中动态调整其行为。本文展望了医疗AI中的情境切换愿景：模型能够在不重新训练的情况下动态适应新的专业、人群、工作流程和临床角色。我们设想情境切换AI能够跨专业和地区诊断、管理和治疗多种疾病，并扩大医疗服务的可及性。

</details>


### [160] [Correlation vs causation in Alzheimer's disease: an interpretability-driven study](https://arxiv.org/abs/2506.10179)
**中文标题：阿尔茨海默病中的相关性与因果关系：一项以可解释性为导向的研究**

*Hamzah Dabool,Raghad Mustafa*

Main category: cs.AI

TL;DR: The study explores the difference between correlation and causation in Alzheimer's disease (AD) research, using machine learning and interpretability techniques to identify key features influencing AD classification. Results show that strong correlations do not imply causation, underscoring the need for careful data interpretation.


<details>
  <summary>Details</summary>
Motivation: To distinguish between correlation and causation in AD research, as this distinction is crucial for accurate diagnosis, treatment, and identifying true disease drivers.

Method: Combined correlation analysis, machine learning (XGBoost), and interpretability techniques (SHAP values) to analyze clinical, cognitive, genetic, and biomarker features.

Result: Identified key AD-influencing features (e.g., cognitive scores, genetic risk factors) and showed that strong correlations do not necessarily indicate causation.

Conclusion: Careful interpretation of associative data is essential, and integrating feature importance with classical analysis can pave the way for future causal inference studies in AD.

摘要: 理解因果关系与相关性的区别在阿尔茨海默病（AD）研究中至关重要，因为它影响诊断、治疗以及真正疾病驱动因素的识别。本研究结合相关性分析、机器学习分类和模型可解释性技术，探究了临床、认知、遗传和生物标志物特征之间的关系。采用XGBoost算法，我们识别出影响AD分类的关键特征，包括认知评分和遗传风险因素。相关性矩阵揭示了相互关联的变量群，而SHAP（SHapley Additive exPlanations）值则提供了跨疾病阶段的特征贡献详细见解。我们的结果表明，强相关性并不一定意味着因果关系，强调了仔细解释关联数据的必要性。通过将特征重要性和可解释性与经典统计分析相结合，本研究为未来旨在揭示真正病理机制的因果推断研究奠定了基础。最终，区分因果因素与相关标记可以改善阿尔茨海默病的早期诊断和针对性干预。

</details>


### [161] [Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems](https://arxiv.org/abs/2506.10192)
**中文标题：迈向负责任的人工智能：自主系统在安全性、公平性和问责性方面的进展**

*Filip Cano*

Main category: cs.AI

TL;DR: This thesis advances responsible AI by improving safety, fairness, and accountability in autonomous systems through deterministic and probabilistic shielding techniques, fairness shields, and a formal framework for assessing intentional behavior.


<details>
  <summary>Details</summary>
Motivation: As AI systems increasingly impact critical societal domains, ensuring their responsible use—encompassing safety, fairness, transparency, and accountability—has become imperative.

Method: The study extends deterministic shielding for safety, introduces fairness shields for group fairness, and proposes a formal framework for assessing intentional behavior in AI agents.

Result: The methods validate practical deployment in autonomous vehicles, balance fairness with minimal interference, and provide metrics for retrospective analysis of unintended harm.

Conclusion: The contributions collectively advance safer, fairer, and more accountable AI systems, laying groundwork for future trustworthy AI research.

摘要: 随着自主系统日益影响关键社会领域，确保人工智能（AI）的负责任使用变得至关重要。然而，可信AI的概念仍然广泛且多面。本论文在AI系统的安全性、公平性、透明性和问责性方面推进了知识。在安全性方面，我们扩展了经典的确定性屏蔽技术，使其对延迟观察具有弹性，从而能够在实际条件下部署。我们还将确定性和概率性安全屏蔽应用于模拟自动驾驶车辆，以防止与道路使用者发生碰撞，并在真实驾驶模拟器中验证了这些技术的有效性。我们引入了公平性屏蔽，这是一种新颖的后处理方法，用于在有限和周期性时间范围内的顺序决策设置中强制执行群体公平性。通过优化干预成本并严格确保公平性约束，该方法高效地平衡了公平性与最小干扰。在透明性和问责性方面，我们提出了一个正式框架，用于评估概率决策代理中的有意行为，引入了代理和意图商的定量指标。我们利用这些指标提出了意图的回顾性分析，有助于在自主系统造成意外伤害时确定责任。最后，我们通过“反应式决策”框架统一了这些贡献，提供了一个整合先前方法的通用形式化。总的来说，这些进展为更安全、更公平和更负责任的AI系统的实现做出了实际贡献，为未来可信AI的研究奠定了基础。

</details>


### [162] [WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2506.10264)
**中文标题：WGSR-Bench：基于兵棋推演的博弈论战略推理基准测试用于大型语言模型**

*Qiyue Yin,Pei Xu,Qiaozhe Li,Shengda Liu,Shengqi Shen,Tong Wang,Yihong Han,Xiaonan Zhao,Likun Yang,Shiyue Cao,Shiyu Qiu,Yuxuan Liu,Shizhao Yu,Lei Cui,Chengxin Yan,Jie Sun,Xiangquan Tang,Kaiqi Huang*

Main category: cs.AI

TL;DR: WGSR-Bench is a new benchmark for evaluating strategic reasoning in LLMs using wargame scenarios, focusing on multi-agent dynamics, intent inference, and counterfactual reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation for strategic reasoning in LLMs, despite their advancements in other reasoning tasks.

Method: Introduces WGSR-Bench, a benchmark using wargame scenarios to test LLMs on environmental awareness, opponent risk modeling, and policy generation.

Result: Designs an LLM-based wargame agent to comprehensively assess strategic reasoning capabilities.

Conclusion: WGSR-Bench aims to evaluate LLMs' strategic reasoning and advance research in model-driven strategic intelligence.

摘要: 近年来，大型语言模型（LLMs）的突破性进展使得人工智能在推理任务上的表现实现了质的飞跃，尤其是在数学、符号和常识推理方面展现出卓越能力。然而，作为高级人类认知的关键组成部分，战略推理——即在动态环境中评估多主体行为、制定行动计划并调整策略的能力——尚未得到系统性评估或建模。为填补这一空白，本文提出了WGSR-Bench，首个以兵棋推演为评估环境的LLMs战略推理基准测试。兵棋推演作为一种典型的高复杂度战略场景，融合了环境不确定性、对抗动态和非唯一战略选择，使其成为评估LLMs在多主体决策、意图推断和反事实推理方面能力的有效测试平台。WGSR-Bench围绕三个核心任务设计测试样本，即环境态势感知、对手风险建模和政策生成，这些任务构成了核心的S-POE架构，用于系统性评估战略推理的主要能力。最后，设计了一个基于LLM的兵棋推演代理，将这些部分整合以进行全面战略推理评估。通过WGSR-Bench，我们希望评估最先进LLMs在博弈论战略推理中的优势与局限，并推动大型模型驱动的战略智能研究。

</details>


### [163] [Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution](https://arxiv.org/abs/2506.10281)
**中文标题：比蒸汽更接近语言：AI作为新生产力革命的认知引擎**

*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: The paper reframes AI as a cognitive engine driving a new productivity revolution, akin to written language, enhancing human intellect rather than mechanizing tasks. It argues AI's role in amplifying knowledge work and reshaping society, calling for rethinking skills and policies.


<details>
  <summary>Details</summary>
Motivation: To conceptualize AI as a transformative cognitive revolution, distinct from the Industrial Revolution, and explore its impact on productivity, work, and society.

Method: The paper adopts a multidisciplinary approach, combining computer science, economics, and sociology, using historical comparisons and domain examples to illustrate AI's cognitive productivity role.

Result: AI is shown to function as an engine of cognition, amplifying knowledge work and heralding a new productivity paradigm, requiring shifts in skills, organizations, and policies.

Conclusion: AI complements human cognitive abilities, marking a new chapter in productivity evolution, demanding societal and policy adaptations.

摘要: 人工智能（AI）被重新定义为一种认知引擎，推动着一种与工业革命的物理推力截然不同的新型生产力革命。本文提出了一种理论框架，将AI视为类似于书面语言的认知革命——一种对人类智力的变革性增强，而非另一种机械化工具。我们通过比较AI的出现与信息技术的历史飞跃，展示了它如何放大知识工作。来自不同领域的例子证明了AI作为认知任务生产力驱动者的影响。我们采用多学科视角，结合计算机科学的进步、经济学的见解和社会学的观点，探讨AI如何重塑工作和社会。通过概念框架，我们描绘了从手工生产力到认知生产力的转变。我们的核心论点是，AI作为一种认知引擎——类似于人类语言如何革命化知识——预示着一种新的生产力范式。我们讨论了这场革命如何要求对技能、组织和政策进行重新思考。本文在学术严谨性与清晰性之间取得平衡，得出结论：AI的潜力在于补充人类认知能力，标志着生产力演化的新篇章。

</details>


### [164] [The Alignment Trap: Complexity Barriers](https://arxiv.org/abs/2506.10304)
**中文标题：对齐陷阱：复杂性壁垒**

*Jasper Yao*

Main category: cs.AI

TL;DR: The paper identifies fundamental computational complexity barriers in verifying AI safety as system capabilities scale, showing that safety verification becomes exponentially harder and coNP-complete beyond a critical threshold. It formalizes the Capability-Risk Scaling dynamic and proves four core theorems, concluding with a strategic trilemma for AI development.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing challenge of ensuring AI safety as systems become more capable, highlighting the tension between increasing capabilities and the computational intractability of verifying safety.

Method: The method involves formalizing the Capability-Risk Scaling dynamic and proving four core theorems using complexity theory to analyze the relationship between AI expressiveness and safety verification.

Result: The results show that safety verification becomes exponentially harder with system expressiveness, safe policies are extremely rare, no finite alignment techniques can cover all cases, and robust safety properties are measure-zero for neural networks.

Conclusion: The conclusion presents a strategic trilemma: constrain system complexity for verifiable safety, accept unverifiable risks, or develop new safety paradigms beyond verification.

摘要: 我们建立了验证AI安全性在系统能力扩展时的基本计算复杂性壁垒。主要结果表明，对于表达能力EXP$(m)$超过临界阈值$\tau$的AI系统，安全性验证需要指数时间且是coNP完全的。我们形式化了能力-风险缩放（CRS）动态，展示了AI能力的提升如何推动社会安全需求趋向完美，从而与验证复杂性形成不可避免的紧张关系。通过四个核心定理，我们证明了（1）验证复杂性随系统表达能力的提升呈指数增长，（2）安全策略仅占策略空间的$2^{-2^m}$比例，（3）有限的校准技术无法提供全覆盖，（4）鲁棒的安全性质在神经网络中构成测度为零的集合。这些结果刻画了一个“难解性缺口”，即实际安全需求落在计算难解性区域内。最后，我们提出了一个战略三难选择：AI开发必须限制系统复杂性以保持可验证的安全性，接受不可验证的风险以扩展能力，或开发超越验证的全新安全范式。我们的工作首次对AI对齐进行了系统的复杂性理论分析，并确立了任何安全方法必须面对的严格界限。核心定理在Lean4中的形式化验证正在进行中。

</details>


### [165] [A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon](https://arxiv.org/abs/2506.10326)
**中文标题：一个用于在竞争性宝可梦中跨多样化团队策略泛化的基准**

*Cameron Angliss,Jiaxun Cui,Jiaheng Hu,Arrasy Rahman,Peter Stone*

Main category: cs.AI

TL;DR: The paper introduces VGC-Bench, a benchmark for AI agents to generalize across diverse team strategies in Pokémon VGC, highlighting the challenge of adapting to large combinatorial team spaces and providing baselines for future research.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of developing AI agents that can adapt to vastly different strategic landscapes in multi-agent environments, particularly in Pokémon VGC, where team configurations are highly combinatorial and strategies shift dramatically.

Method: The authors introduce VGC-Bench, a benchmark with standardized evaluation protocols, human-play datasets, and baselines including large-language-model agents, behavior cloning, reinforcement learning, and game-theoretic methods like self-play and double oracle.

Result: While methods succeed in single-team settings, they struggle to scale as team size grows, indicating policy generalization across diverse team strategies remains a significant challenge.

Conclusion: Generalization across diverse team strategies in Pokémon VGC is an open challenge, and VGC-Bench provides a foundation for future research with its infrastructure and baselines.

摘要: 开发能够在不重新训练的情况下稳健适应截然不同战略环境的AI代理是多智能体学习的核心挑战。宝可梦视频游戏锦标赛（VGC）是一个拥有约10^139种可能团队配置的领域，远大于Dota或星际争霸。宝可梦VGC中团队构建的高度离散和组合性质导致最优策略根据所操控的团队和对手团队而剧烈变化，使得泛化尤为困难。为了推动这一问题的研究，我们引入了VGC-Bench：一个提供关键基础设施、标准化评估协议、人类游戏数据集以及一系列基线的基准——从大型语言模型代理和行为克隆到强化学习及经验博弈论方法（如自我对弈、虚构对弈和双重预言）。在代理在单一团队配置上训练和评估的限制性设置中，我们的方法能够击败专业VGC选手。我们在逐渐增大的团队集上对所有基线方法进行了广泛评估，发现即使在单一团队设置中表现最佳的算法在团队规模扩大时也面临困难。因此，跨多样化团队策略的策略泛化仍然是社区面临的开放挑战。我们的代码已在https://github.com/cameronangliss/VGC-Bench开源。

</details>


### [166] [Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts](https://arxiv.org/abs/2506.10357)
**中文标题：Optimus-3：迈向具有可扩展任务专家的通用多模态 Minecraft 智能体**

*Zaijing Li,Yuquan Xie,Rui Shao,Gongwei Chen,Weili Guan,Dongmei Jiang,Liqiang Nie*

Main category: cs.AI

TL;DR: The paper introduces Optimus-3, a generalist multimodal agent for Minecraft, addressing challenges like data scarcity, task interference, and visual diversity through scalable data generation, a Mixture-of-Experts architecture, and multimodal reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Building a generalist agent in open-world environments like Minecraft is challenging due to insufficient domain-specific data, task interference, and visual diversity. The paper aims to overcome these challenges.

Method: 1) A knowledge-enhanced data generation pipeline for scalable training data. 2) A Mixture-of-Experts (MoE) architecture with task-level routing to reduce task interference. 3) Multimodal Reasoning-Augmented Reinforcement Learning for handling visual diversity.

Result: Optimus-3 outperforms generalist multimodal large language models and state-of-the-art agents across diverse tasks in Minecraft.

Conclusion: Optimus-3 demonstrates the effectiveness of scalable data generation, MoE architecture, and multimodal reinforcement learning in building generalist agents for complex open-world environments.

摘要: 近年来，基于多模态大语言模型（MLLMs）的智能体在多个领域取得了显著进展。然而，在像 Minecraft 这样的开放世界中构建具备感知、规划、行动、基础和反思等能力的通用智能体仍然面临挑战：领域特定数据不足、异构任务之间的干扰以及开放世界中的视觉多样性。本文通过三个关键贡献解决这些挑战：1）提出了一种知识增强的数据生成管道，为智能体开发提供可扩展且高质量的训练数据；2）为了减少异构任务之间的干扰，引入了具有任务级路由的混合专家（MoE）架构；3）开发了一种多模态推理增强的强化学习方法，以增强智能体在 Minecraft 中处理视觉多样性的推理能力。基于这些创新，我们提出了 Optimus-3，一种适用于 Minecraft 的通用智能体。大量实验结果表明，Optimus-3 在 Minecraft 环境中的广泛任务上超越了通用多模态大语言模型和现有的最先进智能体。项目页面：https://cybertronagent.github.io/Optimus-3.github.io/

</details>


### [167] [NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War](https://arxiv.org/abs/2506.10384)
**中文标题：NeuroPAL：基于神经进化的间断式随时学习在《星际争霸：母巢之战》宏观管理中的应用**

*Jim O'Connor,Yeonghun Lee,Gary B Parker*

Main category: cs.AI

TL;DR: NeuroPAL combines NEAT with Punctuated Anytime Learning (PAL) to improve StarCraft AI macromanagement, achieving faster training and emergent human-like strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional StarCraft AI methods (rule-based or deep learning) lack adaptability and efficiency. NeuroPAL aims to enhance neuroevolution's scalability in complex real-time strategy games.

Method: NeuroPAL integrates NEAT with PAL, alternating low-fidelity training and high-fidelity evaluations to boost sample efficiency and strategy discovery.

Result: PAL accelerates learning by 50% compared to standard NEAT, with agents exhibiting expert human strategies like proxy barracks placement.

Conclusion: Structured evaluation mechanisms like PAL enhance neuroevolution's effectiveness in complex environments like StarCraft.

摘要: 《星际争霸：母巢之战》仍然是人工智能研究中的一个具有挑战性的基准，尤其是在需要长期战略规划的宏观管理领域。传统的星际争霸AI方法依赖于基于规则的系统或有监督的深度学习，但两者在适应性和计算效率方面均存在局限性。本文提出NeuroPAL，一种将增强拓扑神经进化（NEAT）与间断式随时学习（PAL）相结合的神经进化框架，以提高进化训练的效率。通过交替进行频繁的低保真训练和周期性的高保真评估，PAL提升了NEAT的样本效率，使智能体能够在更少的训练迭代中发现有效策略。我们在《星际争霸：母巢之战》的固定地图、单一种族场景中评估NeuroPAL，并将其性能与基于标准NEAT的训练进行比较。结果表明，PAL显著加速了学习过程，使智能体在仅需NEAT一半的训练时间内达到竞技水平。此外，进化后的智能体表现出如代理兵营布置和防御建筑优化等专家级人类玩家常用的涌现行为。这些发现表明，像PAL这样的结构化评估机制可以增强神经进化在复杂即时战略环境中的可扩展性和有效性。

</details>


### [168] [Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills](https://arxiv.org/abs/2506.10387)
**中文标题：Mirage-1：通过分层多模态技能增强和更新GUI代理**

*Yuquan Xie,Zaijing Li,Rui Shao,Gongwei Chen,Kaiwen Zhou,Yinchuan Li,Dongmei Jiang,Liqiang Nie*

Main category: cs.AI

TL;DR: The paper introduces Mirage-1, a GUI agent with a Hierarchical Multimodal Skills (HMS) module and Skill-Augmented Monte Carlo Tree Search (SA-MCTS) to improve long-horizon task performance in online environments. It outperforms previous agents by significant margins on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents struggle with long-horizon tasks due to insufficient knowledge and the gap between offline and online domains. The paper aims to address these issues by mimicking how humans generalize knowledge.

Method: Proposes HMS for hierarchical knowledge abstraction (execution skills, core skills, meta-skills) and SA-MCTS to bridge the domain gap by leveraging offline skills in online exploration.

Result: Mirage-1 outperforms previous agents by 32%, 19%, 15%, and 79% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH benchmarks, respectively.

Conclusion: Mirage-1 demonstrates significant improvements in handling long-horizon tasks in online environments, validated by a new benchmark (AndroidLH) and outperforming existing agents.

摘要: 近年来，利用多模态大语言模型（MLLM）作为GUI代理的研究取得了显著成果。然而，这些代理在在线环境中的长时任务中仍表现不佳，主要原因是知识不足以及离线与在线领域之间的固有差距。本文受人类在开放环境中泛化知识的启发，提出了分层多模态技能（HMS）模块，以解决知识不足的问题。该模块逐步将轨迹抽象为执行技能、核心技能和元技能，为长时任务规划提供分层知识结构。为弥合领域差距，我们提出了技能增强蒙特卡洛树搜索（SA-MCTS）算法，该算法高效利用离线环境中习得的技能，以减少在线树探索中的动作搜索空间。基于HMS，我们提出了Mirage-1，一种多模态、跨平台、即插即用的GUI代理。为验证Mirage-1在真实世界长时任务中的性能，我们构建了一个新的基准测试AndroidLH。实验结果表明，Mirage-1在AndroidWorld、MobileMiniWob++、Mind2Web-Live和AndroidLH上的表现分别优于之前的代理32%、19%、15%和79%。项目页面：https://cybertronagent.github.io/Mirage-1.github.io/

</details>


### [169] [Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges](https://arxiv.org/abs/2506.10408)
**中文标题：通过系统1或系统2推理RAG：面向行业挑战的推理代理检索增强生成综述**

*Jintao Liang,Gang Su,Huifeng Lin,You Wu,Rui Zhao,Ziyue Li*

Main category: cs.AI

TL;DR: The paper surveys Reasoning Agentic RAG, categorizing it into predefined and agentic reasoning systems, and discusses challenges and future directions for enhancing flexibility and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of early RAG systems in complex real-world scenarios requiring dynamic reasoning and multi-modal integration.

Method: The paper reviews and categorizes Reasoning Agentic RAG methods into predefined reasoning (fixed pipelines) and agentic reasoning (autonomous tool use), analyzing their designs and strategies.

Result: The survey highlights the effectiveness of Reasoning Agentic RAG in complex tasks and identifies key research challenges for future improvements.

Conclusion: The paper emphasizes the need for advancing Reasoning Agentic RAG systems to improve flexibility, robustness, and applicability in real-world scenarios.

摘要: 检索增强生成（RAG）作为一种强大的框架，通过将外部检索与语言生成相结合，克服了大型语言模型（LLM）的知识局限性。虽然基于静态管道的早期RAG系统在结构化任务中表现出色，但在需要复杂推理、动态检索和多模态集成的现实场景中表现不佳。为解决这些挑战，领域转向了推理代理RAG，该范式将决策和自适应工具使用直接嵌入检索过程。本文全面综述了推理代理RAG方法，将其分为两大类：预定义推理（遵循固定模块化管道以增强推理）和代理推理（模型在推理过程中自主协调工具交互）。我们分析了两种范式下的代表性技术，涵盖架构设计、推理策略和工具协调。最后，我们讨论了关键研究挑战，并提出了未来方向，以提升推理代理RAG系统的灵活性、鲁棒性和适用性。相关研究已整理至https://github.com/ByebyeMonica/Reasoning-Agentic-RAG。

</details>


### [170] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
**中文标题：处理服务的多维自动扩展：基于代理方法的比较**

*Boris Sedlak,Alireza Furutanpey,Zihang Wang,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.AI

TL;DR: The paper compares four agent-based methods for multi-dimensional autoscaling in edge computing, demonstrating their viability for dynamic resource and configuration adjustments in constrained environments.


<details>
  <summary>Details</summary>
Motivation: Edge computing's strict resource constraints necessitate flexible autoscaling methods that can dynamically adjust both hardware resources and service configurations to meet performance requirements.

Method: The study introduces an agent-based framework comparing four scaling agents (Active Inference, Deep Q Network, Analysis of Structural Knowledge, and Deep Active Inference) using real-world services (YOLOv8 and OpenCV) to evaluate performance.

Result: All agents achieved acceptable SLO performance with distinct convergence patterns: Deep Q Network benefits from pre-training, structural analysis converges quickly, and deep active inference combines theory with scalability.

Conclusion: Multi-dimensional agent-based autoscaling is viable for edge environments, encouraging further research in this direction.

摘要: 边缘计算由于严格的资源限制打破了传统的自动扩展方式，因此需要更灵活的扩展行为，利用多个弹性维度。本文提出了一种基于代理的自动扩展框架，动态调整硬件资源和内部服务配置，以在受限环境中最大化需求满足。我们比较了四种扩展代理：主动推理、深度Q网络、结构知识分析和深度主动推理，使用两个并行运行的实际处理服务：YOLOv8用于视觉识别和OpenCV用于QR码检测。结果显示，所有代理均实现了可接受的SLO性能，但收敛模式各异。深度Q网络受益于预训练，结构分析收敛迅速，而深度主动推理代理结合了理论基础和实际可扩展性优势。我们的研究结果为边缘环境中基于代理的多维自动扩展的可行性提供了证据，并鼓励未来在这一研究方向上的工作。

</details>


### [171] [OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics](https://arxiv.org/abs/2506.10481)
**中文标题：OIBench：用信息学奥赛评测强推理模型**

*Yaoming Zhu,Junxin Wang,Yiyang Li,Lin Qiu,ZongYu Wang,Jun Xu,Xuezhi Cao,Yuhuai Wei,Mingshi Wang,Xunliang Cai,Rong Ma*

Main category: cs.AI

TL;DR: OIBench is a challenging olympiad-level informatics benchmark with 250 curated problems, designed to evaluate and advance algorithmic reasoning in models. It shows current SOTA models outperform humans but still fall short of canonical solutions.


<details>
  <summary>Details</summary>
Motivation: Conventional algorithm benchmarks are saturated, necessitating more challenging benchmarks to guide improvements in algorithmic reasoning.

Method: OIBench is constructed with 250 original problems, covering various programming paradigms and complexities, and includes Time/Space Completion Curves for efficiency analysis and human-model comparisons.

Result: Current SOTA models outperform most human participants in correctness and efficiency but are suboptimal compared to canonical solutions.

Conclusion: OIBench, as an open-source resource, aims to advance code reasoning capabilities in future LLMs.

摘要: 随着模型日益复杂，传统算法基准逐渐饱和，亟需更具挑战性的基准来指导算法推理的未来改进。本文介绍了OIBench，一个高质量、私密且具有挑战性的信息学奥赛级别数据集，包含250道精心设计的原创题目。我们详细阐述了基准的构建方法，确保全面评估各种编程范式和复杂度，并通过实验展示了其抗污染特性。我们提出时间/空间完成曲线以进行更精细的效率分析，并通过高水平参与者评估实现人机直接比较。实验表明，开源模型虽落后于闭源模型，但当前SOTA模型在正确性和效率上已超越大多数人类参与者，但仍未达到标准解决方案的水平。通过将OIBench作为完全开源资源发布（https://huggingface.co/datasets/AGI-Eval/OIBench），我们希望该基准能为未来大语言模型的代码推理能力提升做出贡献。

</details>


### [172] [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
**中文标题：科学家的首次考试：通过感知、理解与推理评估多模态大语言模型的认知能力**

*Yuhao Zhou,Yiheng Wang,Xuming He,Ruoyao Xiao,Zhiwei Li,Qiantai Feng,Zijie Guo,Yuejin Yang,Hao Wu,Wenxuan Huang,Jiaqi Wei,Dan Si,Xiuqi Yao,Jia Bu,Haiwen Huang,Tianfan Fu,Shixiang Tang,Ben Fei,Dongzhan Zhou,Fenghua Ling,Yan Lu,Siqi Sun,Chenhui Li,Guanjie Zheng,Jiancheng Lv,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: The paper introduces the Scientists' First Exam (SFE) benchmark to evaluate Multimodal Large Language Models (MLLMs) on scientific perception, understanding, and reasoning, revealing significant gaps in current models' performance.


<details>
  <summary>Details</summary>
Motivation: Current scientific benchmarks inadequately assess MLLMs' perception and reasoning abilities, limiting their potential in scientific discovery. The SFE benchmark aims to fill this gap.

Method: The SFE benchmark includes 830 expert-verified VQA pairs across three question types and 66 multimodal tasks in five disciplines, testing perception, understanding, and reasoning.

Result: State-of-the-art models GPT-o3 and InternVL-3 scored only 34.08% and 26.52% on SFE, indicating substantial room for improvement.

Conclusion: The SFE benchmark highlights the need for further development of MLLMs in scientific domains to enhance AI-driven discoveries.

摘要: 科学发现日益依赖于基于信息密集型科学数据和领域专业知识的复杂多模态推理。通过专家级科学基准的支持，科学多模态大语言模型（MLLMs）有望在实际工作流程中显著提升这一发现过程。然而，目前的科学基准主要关注评估MLLMs的知识理解能力，导致对其感知和推理能力的评估不足。为解决这一问题，我们提出了“科学家的首次考试”（SFE）基准，旨在通过三个相互关联的层次评估MLLMs的科学认知能力：科学信号感知、科学属性理解和科学比较推理。具体而言，SFE包含830个经过专家验证的视觉问答对，涵盖三种问题类型，分布在五个高价值学科的66项多模态任务中。大量实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的得分仅为34.08%和26.52%，凸显了MLLMs在科学领域仍有巨大改进空间。我们希望SFE的见解能进一步推动AI增强科学发现的发展。

</details>


### [173] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
**中文标题：LogiPlan：用于大语言模型逻辑规划与关系推理的结构化基准**

*Yanan Cai,Ahmed Salem,Besmira Nushi,Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan is a benchmark for evaluating LLMs' logical planning and relational reasoning abilities, featuring tasks like plan generation, consistency detection, and comparison questions. It reveals performance gaps among top models.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capabilities in logical planning and relational reasoning, crucial for applications like network infrastructure and knowledge bases, by providing a dynamic and fine-grained evaluation framework.

Method: LogiPlan includes three tasks (Plan Generation, Consistency Detection, Comparison Question) with adjustable complexity. It evaluates models like GPT-4.5 and Claude 3.7 Sonnet, testing their self-correction abilities.

Result: State-of-the-art models perform well on simpler tasks but struggle with complex configurations, showing performance gaps linked to model scale and architecture.

Conclusion: While reasoning-enhanced models show promise, deeper logical planning remains challenging, highlighting the need for further advancements in LLM capabilities.

摘要: 我们推出了LogiPlan，这是一个新颖的基准测试，旨在评估大语言模型（LLMs）在逻辑规划和复杂关系结构推理方面的能力。逻辑关系推理对于依赖LLMs生成和查询关系图的应用（如网络基础设施、知识库或业务流程模式）至关重要。我们的框架通过控制对象数量、关系数量以及关系链的最小深度，动态调整任务复杂度，从而实现对模型在不同难度级别上的细粒度评估。LogiPlan包含三项互补任务：（1）计划生成，模型需构建满足特定结构约束的有效有向关系图；（2）一致性检测，测试模型识别关系结构中不一致性的能力；（3）比较问题，评估模型在给定图中判断查询关系有效性的能力。此外，我们还通过提示模型验证和优化初始解决方案，评估其自我修正能力。我们对包括DeepSeek R1、Gemini 2.0 Pro、Gemini 2 Flash Thinking、GPT-4.5、GPT-4o、Llama 3.1 405B、O3-mini、O1和Claude 3.7 Sonnet在内的先进模型进行了评估，揭示了与模型规模和架构相关的显著性能差距。分析表明，尽管近期增强推理能力的模型在简单实例上表现良好，但在需要更深层次逻辑规划的复杂配置中仍存在困难。

</details>


### [174] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
**中文标题：Primender序列：一种用于测试符号推理与AI推理的新型数学构造**

*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: The paper introduces the Primender sequence, a hybrid integer sequence combining primality and digit-based conditions, as a benchmark for evaluating LLMs' symbolic reasoning. It tests models on rule inference, hypothesis validation, and sequence generation.


<details>
  <summary>Details</summary>
Motivation: The study aims to provide an interpretable, rule-based testbed for assessing LLMs' ability to infer hidden rules, validate mathematical hypotheses, and generalize symbolic logic at scale.

Method: The Primender sequence is defined by numbers that are primes or have prime suffixes. The study designs prompts to test LLMs on identifying the sequence's rule, validating a hypothesis, and generating terms, using comparative metrics for evaluation.

Result: The paper evaluates multiple LLMs (e.g., ChatGPT, Gemini) on their performance in rule inference, hypothesis validation, and sequence generation, contributing a reproducible methodology for benchmarking symbolic reasoning.

Conclusion: The Primender sequence serves as a novel benchmark for LLMs, bridging number theory, AI, and software engineering, with potential for future extensions in symbolic reasoning tasks.

摘要: 本文介绍了Primender序列，这是一种新颖的整数序列，其定义规则结合了经典素数性质与模数位条件。具体而言，若一个数n是素数或其末尾数字（任意长度）为素数，则该数被包含在序列中。换句话说，序列中的数要么是素数，要么至少有一个素数后缀。该序列展现出确定性但非平凡的结构，融合了数论性质与符号模式。我们提出将Primender序列作为评估大型语言模型（LLM）符号推理能力的基准。本研究的动机源于对可解释、基于规则的测试平台的需求，以评估LLM推断隐藏规则、验证数学假设及规模化推广符号逻辑的能力。探索的一个关键假设是：当Primender序列中的某个数恰好比小于或等于它的最大素数大1时，它与序列中前一个数的差值也为1。我们设计了一个结构化提示与评估框架，以在多个先进LLM（如ChatGPT、Copilot、DeepSeek、Gemini、Grok和LLaMA）上测试该假设。模型的任务包括识别底层规则、验证假设并生成序列的下100,000项。通过规则推断准确率、假设评估、序列有效性及符号解释质量等比较指标评估模型性能。本研究贡献了一种新颖的数学构造和可复现的方法论，用于在符号推理、假设测试和规模化模式推广方面对LLM进行基准测试，从而连接数论、人工智能与软件工程领域。

</details>


### [175] [Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information](https://arxiv.org/abs/2506.10613)
**中文标题：基于数据驱动的大型信息物理系统诊断方法：最小先验信息需求**

*Henrik Sebastian Steude,Alexander Diedrich,Ingo Pill,Lukas Moddemann,Daniel Vranješ,Oliver Niggemann*

Main category: cs.AI

TL;DR: A new diagnostic approach for large cyber-physical systems requires minimal prior knowledge, combining neural network-based anomaly detection with a graph diagnosis algorithm, achieving high accuracy and practical applicability.


<details>
  <summary>Details</summary>
Motivation: Diagnosing complex cyber-physical systems typically demands extensive prior knowledge, which is challenging to obtain. The paper aims to develop a method that works with minimal prior information.

Method: The approach combines a neural network-based symptom generator for subsystem-level anomaly detection and a graph diagnosis algorithm using minimal causal relationship information between subsystems.

Result: Experiments show the method identifies the true causal component in 82% of cases and reduces the search space in 73% of scenarios, with successful real-world application tests.

Conclusion: The method demonstrates strong potential for practical use in large, complex cyber-physical systems with limited prior knowledge.

摘要: 复杂信息物理系统的诊断过程通常需要详细的系统模型或全面的训练数据作为先验知识，但获取这些信息具有显著挑战。为解决这一问题，我们提出了一种新的诊断方法，仅需对子系统关系的基本了解及正常运行数据。该方法结合了基于神经网络的症状生成器（用于子系统级异常检测）和一种新的图诊断算法（利用子系统间的最小因果关系信息，这些信息在实践中通常可用）。通过完全可控的模拟数据集实验，我们的方法在82%的案例中正确识别了真实因果组件，并在73%的场景中有效缩小了搜索空间。在真实世界的安全水处理数据集上的额外测试展示了该方法在实际场景中的潜力。结果表明，我们的方法在大型复杂信息物理系统中具有实际应用潜力，尤其是在先验知识有限的情况下。

</details>


### [176] [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/abs/2506.10674)
**中文标题：TeleMath：大型语言模型在电信数学问题解决中的基准**

*Vincenzo Colle,Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Fadhel Ayed,Merouane Debbah*

Main category: cs.AI

TL;DR: The paper introduces TeleMath, a benchmark dataset for evaluating LLMs in solving telecom-specific mathematical problems, revealing that specialized models outperform general-purpose ones.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of LLMs in handling domain-specific, mathematically intensive tasks in telecommunications, an area largely unexplored.

Method: Developed TeleMath, a dataset of 500 QnA pairs covering telecom topics, using a pipeline starting with expert-crafted problems, and evaluated various LLMs.

Result: Specialized models for mathematical/logical reasoning performed best, while general-purpose models struggled despite their size.

Conclusion: TeleMath highlights the need for domain-specific benchmarks and shows the limitations of general-purpose LLMs in specialized mathematical tasks.

摘要: 人工智能在电信领域的日益普及引发了人们对大型语言模型（LLM）处理领域特定、数学密集型任务能力的兴趣。尽管最近的进展提高了LLM在一般数学推理中的表现，但它们在信号处理、网络优化和性能分析等专业领域的有效性仍未被充分探索。为填补这一空白，我们推出了TeleMath，这是首个专门设计用于评估LLM在电信领域解决数学问题能力的基准数据集。TeleMath包含500个问答对，涵盖了电信领域的广泛主题。本文概述了从专家精选问题种子开始的问答对生成流程。对多种开源LLM的评估显示，TeleMath上的最佳表现由专门为数学或逻辑推理设计的近期模型实现。相比之下，通用模型即使参数规模庞大，也常常难以应对这些挑战。我们已发布数据集和评估代码，以方便结果复现并支持未来研究。

</details>


### [177] [Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL](https://arxiv.org/abs/2506.10678)
**中文标题：基于LLM和SHACL的AutomationML文本约束自动化验证**

*Tom Westermann,Aljosha Köcher,Felix Gehlhoff*

Main category: cs.AI

TL;DR: The paper introduces a pipeline to formalize and validate textual constraints in AutomationML (AML) using LLMs and SHACL, enabling semi-automatic checks without requiring formal method expertise.


<details>
  <summary>Details</summary>
Motivation: Existing AML modeling recommendations are informal and textual, lacking automated validation capabilities within AML itself.

Method: The pipeline maps AML models to OWL ontologies via RML and SPARQL, uses LLMs to translate textual rules into SHACL constraints, and validates these against the AML ontology, with results interpreted in natural language.

Result: The approach successfully demonstrates that complex AML modeling rules can be semi-automatically checked without formal method knowledge.

Conclusion: The proposed pipeline effectively bridges the gap between informal textual constraints and automated validation in AML, enhancing usability.

摘要: AutomationML（AML）支持工程中的标准化数据交换，但现有的AML建模建议通常以非正式和文本形式的约束提出，这些约束无法在AML中自动验证。本文介绍了一种将这些约束形式化并验证的流程。首先，通过RML和SPARQL将AML模型映射到OWL本体。此外，使用大型语言模型（LLM）将文本规则翻译为SHACL约束，并在之前生成的AML本体上进行验证。最后，SHACL验证结果以自然语言自动解释。该方法在一个AML建议样本上进行了演示。结果表明，即使复杂的建模规则也可以半自动检查，而无需用户理解形式化方法或本体技术。

</details>


### [178] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
**中文标题：系统ASPMT2SMT：通过SMT求解器计算ASPMT理论**

*Michael Bartholomew,Joohyung Lee*

Main category: cs.AI

TL;DR: The paper introduces a compiler, aspsmt2smt, that translates ASPMT programs into SMT instances, enabling SMT solvers like z3 to compute stable models, particularly effective for real number computations.


<details>
  <summary>Details</summary>
Motivation: To bridge answer set programming (ASP) and satisfiability modulo theories (SMT) by leveraging the functional stable model semantics, enabling more efficient computation of stable models for ASPMT programs.

Method: The system uses gringo for partial grounding of ASPMT programs and z3 for solving the resulting SMT instances, handling continuous changes via real number computations.

Result: The compiler effectively translates ASPMT programs into SMT instances, demonstrating success in handling real number computations for continuous reasoning.

Conclusion: The aspsmt2smt system successfully integrates ASP and SMT, providing a practical tool for computing stable models in ASPMT programs, especially for continuous domains.

摘要: 答案集编程模理论（ASPMT）是一种基于功能稳定模型语义的答案集编程与可满足性模理论结合的方法。研究表明，ASPMT程序的紧密片段可以转化为SMT实例，从而允许SMT求解器计算ASPMT程序的稳定模型。本文介绍了一个名为aspsmt2smt的编译器，实现了这一转换。该系统使用ASP基础工具gringo和SMT求解器z3。gringo对输入程序进行部分基础化，同时保留一些变量由z3处理。我们展示了该系统能够有效处理实数计算，用于连续变化的推理。

</details>


### [179] [Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering](https://arxiv.org/abs/2506.10753)
**中文标题：模拟之前先思考：符号推理协调神经计算以回答反事实问题**

*Adam Ishay,Zhun Yang,Joohyung Lee,Ilgu Kang,Dongjae Lim*

Main category: cs.AI

TL;DR: The paper introduces a neuro-symbolic method using symbolic reasoning (via Answer Set Programming) to enhance counterfactual question answering in video dynamics, achieving state-of-the-art results on CLEVRER and CRAFT benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing neuro-symbolic models for video dynamics struggle with counterfactual reasoning. The paper aims to improve this by integrating symbolic reasoning about causal relations.

Method: The method defines a causal graph for event relations and uses Answer Set Programming (ASP) to coordinate neural perception and simulation modules. For CRAFT, it employs large language models (e.g., GPT-3.5/4) as dynamics simulators with symbolic-guided prompts.

Result: The approach achieves state-of-the-art performance on CLEVRER and improves counterfactual question answering on CRAFT using symbolic-guided prompts with language models.

Conclusion: Symbolic reasoning enhances neuro-symbolic models for counterfactual reasoning, demonstrating effectiveness on benchmarks and potential for further integration with language models.

摘要: 视频动态的因果和时间推理是一个具有挑战性的问题。尽管结合符号推理与神经感知和预测的神经符号模型显示出潜力，但它们在回答反事实问题时仍存在局限。本文提出了一种方法，通过利用符号推理对事件间因果关系的分析，增强神经符号模型的反事实推理能力。我们定义了因果图来表示这些关系，并使用声明性逻辑编程方法——答案集编程（ASP）来协调感知和模拟模块。我们在CLEVRER和CRAFT两个基准上验证了方法的有效性。我们的改进在CLEVRER挑战中达到了最先进的性能，显著优于现有模型。在CRAFT基准中，我们利用大型预训练语言模型（如GPT-3.5和GPT-4）作为动态模拟器的代理。研究结果表明，通过符号因果推理指导的替代提示，该方法可以进一步提高其在反事实问题上的表现。

</details>


### [180] [OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems](https://arxiv.org/abs/2506.10764)
**中文标题：OPT-BENCH：评估LLM代理在大规模搜索空间优化问题上的表现**

*Xiaozhe Li,Jixuan Chen,Xinyu Fang,Shengyuan Ding,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: OPT-BENCH is a benchmark for evaluating LLM agents on large-scale optimization problems, featuring real-world ML tasks and NP problems. It introduces OPT-Agent, a framework for iterative solution refinement, and shows that historical context improves performance.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' ability in iterative optimization and solution refinement, which remains understudied despite their success in diverse tasks.

Method: OPT-BENCH includes 20 Kaggle ML tasks and 10 NP problems, and introduces OPT-Agent, an end-to-end framework for iterative reasoning and solution improvement using historical feedback.

Result: Experiments on 9 LLMs show that historical context significantly enhances optimization performance in both ML and NP tasks.

Conclusion: OPT-BENCH provides a robust benchmark for advancing LLM-driven optimization, with open-sourced datasets and tools to foster further research.

摘要: 大型语言模型（LLM）在解决多样化任务中展现出卓越能力，但其通过从先前反馈中学习来迭代优化复杂解决方案的能力尚未充分探索。为填补这一空白，我们提出了OPT-BENCH，一个全面的基准测试，旨在评估LLM代理在大规模搜索空间优化问题上的表现。OPT-BENCH包含20个来自Kaggle的真实机器学习任务和10个经典NP问题，为评估LLM代理在迭代推理和解决方案优化方面提供了多样化和挑战性的环境。为实现严格评估，我们引入了OPT-Agent，一个端到端的优化框架，模拟人类在解决复杂问题时的推理过程，通过利用历史反馈生成、验证和迭代改进解决方案。通过对来自6个模型家族的9种最先进LLM进行广泛实验，我们分析了优化迭代次数、温度设置和模型架构对解决方案质量和收敛性的影响。结果表明，结合历史上下文显著提升了ML和NP任务的优化性能。所有数据集、代码和评估工具均已开源，以推动LLM驱动的优化和迭代推理研究的进一步发展。项目页面：https://github.com/OliverLeeXZ/OPT-BENCH。

</details>


### [181] [A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models](https://arxiv.org/abs/2506.10853)
**中文标题：基于MCP增强思维链大语言模型的个体时空活动生成方法研究**

*Yu Zhang,Yang Hu,De Wang*

Main category: cs.AI

TL;DR: The paper proposes a framework combining chain-of-thought reasoning and Model Context Protocol (MCP) to enhance large language models' ability to simulate human spatiotemporal behaviors, validated with high-quality results in urban planning applications.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for simulating human spatiotemporal behavior are computationally expensive and lack generalizability. Large language models (LLMs) show potential but struggle with spatiotemporal reasoning. The paper aims to address these limitations by integrating CoT reasoning with MCP.

Method: The framework combines a five-stage cognitive framework (CoT reasoning) with six MCP tool categories (temporal management, spatial navigation, environmental perception, personal memory, social collaboration, and experience evaluation) to enhance LLMs' spatiotemporal behavior simulation.

Result: Experiments in Shanghai's Lujiazui district showed high similarity with real mobile signaling data, achieving generation quality scores of 7.86 to 8.36. Parallel processing reduced generation time from 1.30 to 0.17 minutes per sample.

Conclusion: The framework advances LLMs' applications in urban computing, providing a practical approach for synthetic mobility data generation and supporting smart city planning and transportation forecasting.

摘要: 人类时空行为模拟对城市规划研究至关重要，但传统的基于规则和统计的方法存在计算成本高、泛化能力有限和可扩展性差的问题。尽管大语言模型（LLMs）作为“世界模拟器”表现出潜力，但在时空推理方面仍面临空间认知有限、物理约束理解不足和群体同质化倾向等挑战。本文提出了一种将思维链（CoT）推理与模型上下文协议（MCP）结合的框架，以增强LLMs模拟与验证数据模式相符的时空行为的能力。该方法通过五阶段认知框架实现类人渐进式推理，并通过六类专用MCP工具（时间管理、空间导航、环境感知、个人记忆、社会协作和经验评估）进行综合数据处理。在上海陆家嘴地区的实验中，框架在1000个生成样本中验证了有效性。结果显示生成结果与真实移动信号数据高度相似，不同基础模型的生成质量得分在7.86至8.36之间。并行处理实验显示效率提升，样本生成时间从1.30分钟降至0.17分钟（从2个进程扩展到12个进程）。该工作为将CoT推理与MCP结合用于城市行为建模提供了贡献，推动了LLMs在城市计算中的应用，并为合成移动数据生成提供了实用方法。该框架为智能城市规划、交通预测和参与式城市设计应用奠定了基础。

</details>


### [182] [GenPlanX. Generation of Plans and Execution](https://arxiv.org/abs/2506.10897)
**中文标题：GenPlanX：计划生成与执行**

*Daniel Borrajo,Giuseppe Canonaco,Tomás de la Rosa,Alfredo Garrachón,Sriram Gopalakrishnan,Simerjot Kaur,Marianela Morales,Sunandita Patra,Alberto Pozanco,Keshav Ramani,Charese Smiley,Pietro Totis,Manuela Veloso*

Main category: cs.AI

TL;DR: GenPlanX integrates LLMs with classical AI planning to interpret natural language tasks and generate executable plans, enhancing productivity in office workflows.


<details>
  <summary>Details</summary>
Motivation: Classical AI planning lacks natural language understanding, while LLMs excel at interpreting human intents. Combining both can improve task automation and human-AI collaboration.

Method: GenPlanX combines LLMs for natural language task interpretation with a classical AI planning engine and an execution framework.

Result: GenPlanX effectively assists with office-related tasks, demonstrating improved workflow efficiency and productivity.

Conclusion: GenPlanX successfully bridges the gap between natural language understanding and AI planning, offering practical benefits for human-AI collaboration.

摘要: 传统的人工智能规划技术为复杂任务生成动作序列，但在使用自然语言描述规划任务时缺乏理解能力。大型语言模型（LLM）的出现为人机交互带来了新的能力。在规划任务中，LLM 尤其擅长解读人类意图等用途。本文介绍了 GenPlanX，它将 LLM 用于基于自然语言的规划任务描述，结合了经典的人工智能规划引擎以及执行与监控框架。我们展示了 GenPlanX 在协助用户完成办公相关任务方面的有效性，突显了其通过无缝的人机协作优化工作流程和提升生产力的潜力。

</details>


### [183] [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/abs/2506.10912)
**中文标题：破解有毒分子：多模态大语言模型是否准备好进行结构级分子解毒？**

*Fei Lin,Ziyang Gong,Cong Wang,Yonglin Tian,Tengchao Zhang,Xue Yang,Gen Luo,Fei-Yue Wang*

Main category: cs.AI

TL;DR: The paper introduces ToxiMol, the first benchmark for MLLMs in molecular toxicity repair, evaluating their ability to generate less toxic molecular alternatives. Results show MLLMs face challenges but exhibit potential in toxicity understanding and molecule editing.


<details>
  <summary>Details</summary>
Motivation: Toxicity is a major cause of drug development failure, yet molecular toxicity repair lacks systematic definition and benchmarking. The paper aims to fill this gap with ToxiMol.

Method: The authors create ToxiMol, a benchmark with 11 tasks and 560 toxic molecules, and propose ToxiEval, an automated evaluation framework integrating toxicity prediction, synthetic accessibility, drug-likeness, and structural similarity.

Result: Current MLLMs show significant challenges in molecular toxicity repair but demonstrate promising capabilities in toxicity understanding and structure-aware editing.

Conclusion: While MLLMs are not yet ready for structure-level molecular detoxification, they exhibit potential, highlighting the need for further research and development.

摘要: 毒性仍然是早期药物开发失败的主要原因。尽管在分子设计和性质预测方面取得了进展，但分子毒性修复任务——生成结构有效且毒性降低的分子替代物——尚未被系统定义或基准测试。为了填补这一空白，我们引入了ToxiMol，这是首个专注于分子毒性修复的通用多模态大语言模型（MLLMs）基准任务。我们构建了一个标准化数据集，涵盖11个主要任务和560个代表性有毒分子，涉及多种机制和粒度。我们设计了一个具有机制感知和任务自适应能力的提示注释流程，并基于专家毒理学知识。同时，我们提出了一个自动化评估框架ToxiEval，它将毒性终点预测、合成可及性、药物相似性和结构相似性整合到一个高通量的修复成功评估链中。我们系统评估了近30种主流通用MLLMs，并设计了多项消融研究以分析关键因素，如评估标准、候选多样性和失败归因。实验结果表明，尽管当前的MLLMs在这一任务上仍面临重大挑战，但它们开始在毒性理解、语义约束遵守和结构感知分子编辑方面展现出有前景的能力。

</details>


### [184] [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/abs/2506.10947)
**中文标题：虚假奖励：重新思考RLVR中的训练信号**

*Rulin Shao,Shuyue Stella Li,Rui Xin,Scott Geng,Yiping Wang,Sewoong Oh,Simon Shaolei Du,Nathan Lambert,Sewon Min,Ranjay Krishna,Yulia Tsvetkov,Hannaneh Hajishirzi,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TL;DR: The paper demonstrates that reinforcement learning with verifiable rewards (RLVR) can improve mathematical reasoning in models like Qwen2.5-Math-7B even with spurious rewards, though results vary across model families.


<details>
  <summary>Details</summary>
Motivation: To explore whether RLVR can elicit strong reasoning in models despite using rewards that lack correlation with correct answers, and to understand the variability in performance across different model families.

Method: The study uses RLVR with various spurious rewards (random, format-based, incorrect labels, etc.) to train models like Qwen2.5-Math-7B, Llama3, and OLMo2, analyzing their performance on mathematical reasoning tasks.

Result: RLVR significantly improved Qwen2.5-Math-7B's performance (up to 27.1% gains) even with spurious rewards, but similar gains were not observed in Llama3 or OLMo2. A distinctive behavior (code reasoning) in Qwen increased from 65% to over 90%.

Conclusion: RLVR may surface useful reasoning representations from pretraining, but its effectiveness varies by model family. Future RLVR research should validate on diverse models, not just a single choice.

摘要: 我们展示了带有可验证奖励的强化学习（RLVR）可以在某些模型中引发强大的数学推理能力，即使使用的奖励与正确答案几乎没有、没有甚至负相关。例如，RLVR将Qwen2.5-Math-7B在MATH-500上的性能提升了21.4%（随机奖励）、13.8%（格式奖励）、24.1%（错误标签）、26.0%（1-shot RL）和27.1%（多数投票）——几乎接近使用真实奖励的29.1%提升。然而，对Qwen有效的虚假奖励在其他模型家族（如Llama3或OLMo2）中往往无法带来提升。特别是，我们发现代码推理（在不实际执行代码的情况下思考代码）是Qwen2.5-Math的一种独特行为，在RLVR后显著增加，从65%上升到超过90%，即使使用虚假奖励。总体而言，我们假设，由于缺乏有用的奖励信号，RLVR可能在某种程度上利用了预训练期间学习的有用推理表示，尽管具体机制仍需未来研究。我们建议未来的RLVR研究应在多样化模型上验证，而非仅依赖单一默认选择，因为我们的研究表明，即使在完全虚假的奖励信号下，Qwen模型也能轻松获得显著的性能提升。

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [185] [Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations](https://arxiv.org/abs/2506.10249)
**中文标题：扩展创造力：理解人类与AI创意关系的概念框架**

*Andrea Gaggioli,Sabrina Bartolotta,Andrea Ubaldi,Katusha Gerardini,Eleonora Diletta Sarcinella,Alice Chirico*

Main category: cs.HC

TL;DR: The paper proposes a framework for understanding how AI enhances human creativity through three modes: Support, Synergy, and Symbiosis, based on AI's autonomy and perceived agency.


<details>
  <summary>Details</summary>
Motivation: To clarify how AI can effectively enhance human creativity by exploring different modes of human-AI interaction in creative processes.

Method: The study adopts a distributed creativity perspective, defining three modes (Support, Synergy, Symbiosis) along dimensions of AI autonomy and perceived agency, and examines their impact on creativity levels.

Result: The framework identifies how each mode (Support, Synergy, Symbiosis) influences creativity, from everyday problem-solving to paradigm-shifting innovation, with implications for theory, ethics, and design.

Conclusion: The proposed framework provides a structured way to understand and design human-AI creative collaborations, highlighting the importance of balancing autonomy and agency for effective enhancement of creativity.

摘要: 人工智能在增强人类创造力方面具有巨大潜力。然而，实现这一愿景需要更清晰地理解如何有效实现这种增强。我们采用分布式创造力的视角，确定了AI在创意过程中发挥作用的三种主要模式：支持模式（AI作为工具）、协同模式（AI与人类以互补方式合作）和共生模式（人类与AI认知高度整合，形成统一的创意系统）。这些模式基于两个关键维度定义：AI系统的技术自主性和其被感知的代理程度。我们探讨了每种配置如何影响不同层次的创造力（从日常问题解决到范式转变的创新），并讨论了理论、伦理和设计方面的意义。

</details>


### [186] [The Role of Generative AI in Facilitating Social Interactions: A Scoping Review](https://arxiv.org/abs/2506.10927)
**中文标题：生成式AI在促进社交互动中的作用：范围综述**

*T. T. J. E. Arets,G. Perugia,M. Houben,W. A. IJsselsteijn*

Main category: cs.HC

TL;DR: This scoping review explores how generative AI (GAI) technologies enhance social interactions, analyzing 30 studies to identify trends, design methodologies, and socio-ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Reduced social connectedness threatens mental health and well-being, yet the impact of GAI on social interactions remains understudied.

Method: A scoping review of 30 studies published since 2020, focusing on GAI applications in storytelling, skills training, reminiscence, learning, music, and conversation.

Result: Key trends include participatory design approaches and diverse application domains, alongside socio-ethical issues like cultural bias and accessibility.

Conclusion: GAI shows promise for personalized social interactions but requires equitable design and inclusive evaluation practices.

摘要: 社交联系的减少对心理健康、预期寿命和整体幸福感构成了日益严重的威胁。生成式AI（GAI）技术，如大型语言模型（LLMs）和图像生成工具，正越来越多地融入旨在提升人类社交体验的应用中。尽管其应用日益广泛，但这些技术如何影响社交互动尚不明确。本范围综述探讨了基于GAI的应用如何设计以促进社交互动，其目标社交形式，以及设计和评估方法。通过对2020年以来30项研究的分析，我们确定了包括讲故事、社会情感技能训练、回忆、协作学习、音乐创作和一般对话在内的关键应用趋势。我们强调了参与式和共同设计方法在促进技术有效使用和社交参与中的作用，同时探讨了文化偏见和可访问性等社会伦理问题。本综述强调了GAI支持动态和个性化互动的潜力，但呼吁更多关注公平设计实践和包容性评估策略。

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [187] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
**中文标题：FastFLUX：通过块级替换和三明治训练修剪FLUX**

*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX is a pruning framework for FLUX models that replaces complex residual branches with linear layers and uses localized fine-tuning to maintain performance while improving inference speed.


<details>
  <summary>Details</summary>
Motivation: FLUX models are highly expressive but suffer from slow inference, high memory usage, and poor deployability due to their large parameter sizes. Existing acceleration methods often degrade performance or require high training costs.

Method: FastFLUX introduces Block-wise Replacement with Linear Layers (BRLL) to replace complex residual branches with lightweight linear layers and Sandwich Training (ST), a localized fine-tuning strategy using LoRA to supervise neighboring blocks.

Result: FastFLUX maintains high image quality while significantly improving inference speed, even with 20% of the hierarchy pruned.

Conclusion: FastFLUX effectively balances performance and efficiency, offering a practical solution for deploying FLUX models in resource-constrained environments.

摘要: 近年来，文本到图像（T2I）生成技术的进步催生了高度表达力的模型，如扩散变换器（DiTs），以FLUX为代表。然而，其庞大的参数量导致推理速度慢、内存占用高且部署性差。现有的加速方法（如单步蒸馏和注意力修剪）通常伴随显著的性能下降和高昂的训练成本。为解决这些问题，我们提出了FastFLUX，一种架构级修剪框架，旨在提升FLUX的推理效率。其核心是块级线性层替换（BRLL）方法，该方法将ResBlock中结构复杂的残差分支替换为轻量级线性层，同时保留原始快捷连接以确保稳定性。此外，我们引入了三明治训练（ST），一种利用LoRA监督相邻块的局部微调策略，以缓解结构替换带来的性能下降。实验表明，FastFLUX在定性和定量评估中均保持了较高的图像质量，同时显著提升了推理速度，即使修剪了20%的层次结构。我们的代码即将发布。

</details>


### [188] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
**中文标题：环境扩散全能：用劣质数据训练优质模型**

*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: The paper introduces Ambient Diffusion Omni, a framework that leverages low-quality and synthetic images to enhance diffusion model training, achieving state-of-the-art results in image quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models rely on curated, high-quality datasets, but the paper argues that lower-quality images, often discarded, contain valuable signal that can improve model performance.

Method: The framework exploits natural image properties (spectral power law decay and locality) to extract signal from mixed-quality data, validated on synthetically corrupted images and applied to achieve top ImageNet FID scores.

Result: Ambient Diffusion Omni achieves state-of-the-art ImageNet FID and improves image quality and diversity in text-to-image generation.

Conclusion: The paper demonstrates that noise can mitigate distribution skew, enabling effective training on mixed-quality data, with theoretical support for the trade-off between biased and unbiased data learning.

摘要: 我们展示了如何利用低质量、合成和分布外图像来提升扩散模型的质量。通常，扩散模型是在从网络和其他来源高度过滤的数据池中精选的数据集上训练的。我们证明，那些常被丢弃的低质量图像具有巨大价值。我们提出了环境扩散全能（Ambient Diffusion Omni），这是一个简单而原则性的框架，用于训练扩散模型，使其能够从所有可用图像中提取信号。我们的框架利用了自然图像的两个特性——频谱功率律衰减和局部性。我们首先通过在合成损坏（高斯模糊、JPEG压缩和运动模糊）的图像上成功训练扩散模型来验证框架。随后，我们利用该框架在ImageNet FID上达到最先进水平，并在文本到图像生成模型中显著提升了图像质量和多样性。核心观点是噪声可以抑制高质量分布与实际观察到的混合分布之间的初始偏差。我们通过分析在扩散时间范围内学习偏置数据与有限无偏数据之间的权衡，为方法提供了严格的理论依据。

</details>


### [189] [Learning-based density-equalizing map](https://arxiv.org/abs/2506.10027)
**中文标题：基于学习的密度均衡映射**

*Yanwen Huang,Lok Ming Lui,Gary P. T. Choi*

Main category: cs.GR

TL;DR: The paper introduces a learning-based density-equalizing mapping framework (LDEM) using deep neural networks, which outperforms traditional methods in accuracy, avoids overlapping artifacts, and seamlessly generalizes from 2D to 3D.


<details>
  <summary>Details</summary>
Motivation: Traditional density-equalizing map (DEM) methods face challenges like limited accuracy, overlapping artifacts, and difficulty in extending from 2D to 3D. The paper aims to address these issues with a learning-based approach.

Method: The authors propose LDEM, a deep neural network framework with a loss function enforcing density uniformity and geometric regularity, using a hierarchical approach for transformation prediction.

Result: LDEM shows superior density-equalizing and bijectivity properties, avoids overlapping artifacts, and generalizes easily from 2D to 3D without structural changes.

Conclusion: The learning-based LDEM framework offers scalable and robust computation of density-equalizing maps, opening new possibilities for practical applications.

摘要: 密度均衡映射（DEM）是一种强大的技术，用于创建形状变形，其面积变化反映底层密度函数。近几十年来，DEM在数据可视化、几何处理和医学影像等领域广泛应用。传统DEM方法主要依赖扩散方程的迭代数值求解器或基于优化的方法，这些方法通常面临精度有限、极端情况下产生重叠伪影以及从2D扩展到3D时需要大量算法重新设计等问题。本文提出了一种基于学习的密度均衡映射框架（LDEM），利用深度神经网络实现。具体而言，我们引入了一种损失函数，强制密度均匀性和几何规律性，并采用分层方法预测粗粒度和密集级别的变换。与现有方法相比，我们的方法在多种简单和复杂密度分布下表现出更优的密度均衡性和双射性，并可轻松应用于具有不同效果的表面重网格化。此外，该方法无需对模型结构或损失函数进行修改即可无缝从2D推广到3D领域。总之，我们的工作为实际应用中可扩展且稳健的密度均衡映射计算开辟了新途径。

</details>


### [190] [Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On](https://arxiv.org/abs/2506.10468)
**中文标题：基于真实人体的低门槛单服装虚拟试穿数据集收集方法**

*Zaiqiang Wu,Yechen Li,Jingyuan Liu,Yuki Shibata,Takayuki Hori,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: The paper proposes a low-barrier method for collecting per-garment datasets using real human bodies, improving virtual try-on accuracy and interaction without expensive robotic mannequins.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods face limitations like high costs of robotic mannequins and misalignment of synthesized garments with human bodies. The paper aims to address these issues by using real human bodies for dataset collection and improving garment alignment.

Method: The approach involves collecting per-garment datasets with real human bodies and introducing a hybrid person representation combining intermediate representation with a simplified DensePose map for better garment alignment and interaction.

Result: Evaluations showed superior image quality and temporal consistency compared to state-of-the-art methods, and user studies confirmed the system's usefulness for garment purchasing decisions.

Conclusion: The proposed method effectively reduces barriers to dataset collection and improves virtual try-on accuracy, demonstrating practical benefits for users.

摘要: 现有的基于图像的虚拟试穿方法通常仅限于正面视角且缺乏实时性能。虽然单服装虚拟试穿方法通过捕获单服装数据集并训练单服装神经网络解决了这些问题，但仍存在实际限制：（1）用于捕获单服装数据集的机器人模特成本过高，难以广泛采用，且无法准确复制自然人体变形；（2）合成的服装常与人体不对齐。为解决这些挑战，我们提出了一种基于真实人体的低门槛单服装数据集收集方法，无需定制机器人模特。我们还引入了一种混合人物表示，通过简化的DensePose图增强现有中间表示，确保合成服装图像与人体准确对齐，并实现无需定制可穿戴设备的人体-服装交互。我们通过定性和定量评估与其他最先进的基于图像的虚拟试穿方法进行比较，并进行消融研究，证明了我们的方法在图像质量和时间一致性方面的优越性。最后，用户研究结果表明，大多数参与者认为我们的虚拟试穿系统有助于服装购买决策。

</details>


### [191] [Edit360: 2D Image Edits to 3D Assets from Any Angle](https://arxiv.org/abs/2506.10507)
**中文标题：Edit360：从任意角度将2D图像编辑扩展到3D资产**

*Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang*

Main category: cs.GR

TL;DR: Edit360 is a tuning-free framework that extends 2D image edits to multi-view consistent 3D assets using video diffusion models, enabling edits from any viewpoint with structural coherence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D asset editing are limited to predetermined viewing angles, restricting flexibility and practical applications. Edit360 aims to overcome this by enabling fine-grained, multi-view consistent edits from any angle.

Method: Edit360 leverages video diffusion models, selecting anchor views for 2D edits and propagating them across 360 degrees using an Anchor-View Editing Propagation mechanism to align multi-view information in latent and attention spaces.

Result: The framework successfully reconstructs high-quality 3D assets with customizable edits, ensuring structural coherence across all viewpoints.

Conclusion: Edit360 provides a flexible and practical solution for extending 2D edits to 3D assets, enabling customizable 3D content creation from any angle.

摘要: 近年来，扩散模型在图像生成和编辑方面取得了显著进展，但将这些能力扩展到3D资产仍然具有挑战性，尤其是对于需要多视角一致性的细粒度编辑。现有方法通常将编辑限制在预定的视角范围内，严重限制了其灵活性和实际应用。我们提出了Edit360，一种无需调整的框架，可将2D修改扩展到多视角一致的3D编辑。基于视频扩散模型，Edit360支持从任意视角进行用户特定的编辑，同时确保所有视角的结构一致性。该框架选择锚定视角进行2D修改，并将编辑传播到整个360度范围内。为此，Edit360引入了一种新颖的锚定视角编辑传播机制，有效在扩散模型的潜在和注意力空间中对齐和合并多视角信息。生成的编辑后的多视角序列有助于重建高质量的3D资产，从而实现可定制的3D内容创作。

</details>


### [192] [Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture](https://arxiv.org/abs/2506.10580)
**中文标题：Transformer IMU校准器：动态身体IMU校准用于惯性动作捕捉**

*Chengxu Zuo,Jiawei Huang,Xiao Jiang,Yuan Yao,Xiangren Shi,Rui Cao,Xinyu Yi,Feng Xu,Shihui Guo,Yipeng Qin*

Main category: cs.GR

TL;DR: The paper introduces a dynamic calibration method for IMUs in motion capture, relaxing static assumptions and enabling real-time estimation of drift and offset matrices using a Transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Current IMU calibration methods rely on restrictive static assumptions, limiting their application. The paper aims to enable dynamic calibration for broader use.

Method: The method uses a Transformer-based model to estimate drift and offset matrices in real time, leveraging synthetic datasets and a calibration trigger based on IMU reading diversity.

Result: The approach achieves implicit IMU calibration and accurate long-term motion capture with sparse IMUs, a first in the field.

Conclusion: The proposed method successfully breaks static calibration constraints, expanding IMU applications in motion capture.

摘要: 本文提出了一种新颖的动态校准方法，用于稀疏惯性动作捕捉系统，首次打破了IMU校准中严格的绝对静态假设（即坐标漂移RG'G和测量偏移RBS在整个运动过程中保持不变），从而显著扩展了其应用场景。具体而言，我们在两个放宽的假设下实现了RG'G和RBS的实时估计：i）矩阵在短时间内变化可忽略；ii）人体运动/IMU读数在该时间窗口内具有多样性。直观上，第一个假设减少了候选矩阵的数量，第二个假设提供了多样化的约束，极大地缩小了解空间，从而可以从IMU读数的短历史中实时准确估计RG'G和RBS。为实现这一点，我们创建了成对的RG'G、RBS矩阵和IMU读数的合成数据集，并使用基于Transformer的模型学习其映射关系。我们还设计了一个基于IMU读数多样性的校准触发器，以确保在应用我们的方法之前满足假设ii）。据我们所知，我们是首个实现隐式IMU校准（即无需显式校准过程即可无缝使用IMU）的团队，也是首个实现使用稀疏IMU进行长期准确动作捕捉的团队。代码和数据集可在https://github.com/ZuoCX1996/TIC获取。

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [193] [An Analysis of Datasets, Metrics and Models in Keyphrase Generation](https://arxiv.org/abs/2506.10346)
**中文标题：关键词生成的数据集、指标与模型分析**

*Florian Boudin,Akiko Aizawa*

Main category: cs.IR

TL;DR: The paper provides a comprehensive analysis of over 50 research papers on keyphrase generation, identifying issues in evaluation practices and dataset similarities, and introduces a new PLM-based model to aid future research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a comprehensive review and analysis of previous work in keyphrase generation, despite continuous efforts in the field.

Method: The method involves reviewing and analyzing over 50 research papers on keyphrase generation, focusing on model architectures, datasets, and evaluation metrics.

Result: The results highlight critical issues in evaluation practices, such as dataset similarities and inconsistent metric calculations, and introduce a new PLM-based model.

Conclusion: The conclusion emphasizes the need for improved evaluation practices and the release of a new model to support future research in keyphrase generation.

摘要: 关键词生成是指生成一组单词或短语以概括文档内容的任务。过去几年中，针对这一任务的持续努力涉及多个研究方向，如模型架构、数据资源和使用场景。然而，由于缺乏对先前工作的回顾与分析，关键词生成的现状仍不明确。本文通过分析50多篇关键词生成的研究论文，填补了这一空白，提供了对近期进展、局限性和开放挑战的全面概述。我们的发现揭示了当前评估实践中的几个关键问题，例如常用基准数据集之间的相似性以及指标计算的不一致性导致性能被高估。此外，我们通过发布一个基于预训练语言模型的关键词生成模型，解决了预训练模型可用性有限的问题，以促进未来研究。

</details>


### [194] [Conversational Search: From Fundamentals to Frontiers in the LLM Era](https://arxiv.org/abs/2506.10635)
**中文标题：对话式搜索：从基础到LLM时代的前沿**

*Fengran Mo,Chuan Meng,Mohammad Aliannejadi,Jian-Yun Nie*

Main category: cs.IR

TL;DR: The paper explores conversational search, focusing on multi-turn interactions and the role of LLMs in advancing intelligent search systems. It connects fundamentals with emerging LLM-driven topics.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between traditional conversational search principles and the transformative potential of LLMs, providing a comprehensive guide for academia and industry.

Method: The tutorial introduces core principles of conversational search and integrates emerging LLM-driven advancements, offering a structured overview for participants.

Result: Participants gain a holistic understanding of conversational search, from fundamentals to LLM-driven innovations, preparing them to contribute to next-gen systems.

Conclusion: The tutorial successfully links foundational knowledge with cutting-edge LLM applications, fostering development in conversational search.

摘要: 对话式搜索支持用户与系统之间的多轮交互，以满足用户的复杂信息需求。在此过程中，系统需理解用户在对话上下文中的搜索意图，并通过灵活的对话界面返回相关信息。近期强大的大语言模型（LLMs）因其指令遵循、内容生成和推理能力吸引了广泛关注，为构建智能对话式搜索系统提供了新的机遇与挑战。本教程旨在介绍对话式搜索中基础与LLM革命性新兴主题之间的联系，面向学术界和工业界的学生、研究人员及从业者。参与者将全面理解对话式搜索的核心原则及LLM驱动的前沿发展，掌握为下一代对话式搜索系统开发所需的知识。

</details>


### [195] [Towards Understanding Bias in Synthetic Data for Evaluation](https://arxiv.org/abs/2506.10301)
**中文标题：理解用于评估的合成数据中的偏差**

*Hossein A. Rahmani,Varsha Ramineni,Nick Craswell,Bhaskar Mitra,Emine Yilmaz*

Main category: cs.IR

TL;DR: The paper investigates biases in synthetic test collections generated by LLMs for IR system evaluation, showing their impact on absolute performance but less so on relative comparisons.


<details>
  <summary>Details</summary>
Motivation: To validate the reliability of synthetic test collections created by LLMs for IR system evaluation and analyze potential biases.

Method: Empirically examines bias in synthetic test collections using LLMs for query and label generation, validated with a linear mixed-effects model.

Result: Bias in synthetic test collections significantly affects absolute system performance evaluation but has less impact on relative performance comparisons.

Conclusion: While synthetic test collections introduce bias, their use for relative system comparisons remains viable, though caution is needed for absolute evaluations.

摘要: 测试集合对于评估信息检索（IR）系统至关重要。为这些集合创建多样化的用户查询具有挑战性，而获取相关性判断（即检索文档与查询的匹配程度）通常成本高昂且资源密集。最近，利用大型语言模型（LLMs）生成合成数据集在各种应用中受到关注。虽然之前的工作使用LLMs生成合成查询或文档以改进排序模型，但利用LLMs创建合成测试集合仍相对较少探索。先前的研究表明，合成测试集合有潜力用于系统评估，但需要更多分析来验证这一观点。本文深入研究了使用LLMs构建的合成测试集合的可靠性，其中LLMs用于生成合成查询、标签或两者。我们特别关注此类测试集合用于评估时可能出现的潜在偏差。我们首先通过实验证明评估结果中存在此类偏差，并分析其对系统评估的影响。进一步通过线性混合效应模型验证了这种偏差的存在。分析表明，尽管合成测试集合在评估结果中引入的偏差对绝对系统性能（如计算绝对性能）影响显著，但其对相对系统性能比较的影响可能较小。代码和数据可在以下地址获取：https://github.com/rahmanidashti/BiasSyntheticData。

</details>


### [196] [Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation](https://arxiv.org/abs/2506.10658)
**中文标题：基于去噪和增强图视图的对比矩阵补全用于鲁棒推荐**

*Narges Nemati,Mostafa Haghir Chehreghani*

Main category: cs.IR

TL;DR: The paper proposes MCCL, a contrastive learning-based matrix completion method for robust recommendations, addressing noise sensitivity and overfitting in GNNs by denoising and aligning graph representations.


<details>
  <summary>Details</summary>
Motivation: Current GNN-based matrix completion methods are sensitive to noisy edges and prone to overfitting, limiting their generalizability in recommender systems.

Method: MCCL extracts local subgraphs, generates two graph representations (denoised via attention and aligned via a variational autoencoder), and uses a mutual learning loss to harmonize them.

Result: MCCL improves RMSE by up to 0.8% and ranking metrics by up to 36% on real-world datasets.

Conclusion: MCCL enhances recommendation robustness by combining denoising and representation alignment, outperforming existing methods.

摘要: 矩阵补全是推荐系统中广泛采用的框架，通过预测用户-物品评分矩阵中的缺失项来全面理解用户偏好。然而，当前基于图神经网络（GNN）的方法由于其固有的消息传递机制，对噪声或无关边高度敏感，且容易过拟合，限制了其泛化能力。为克服这些挑战，我们提出了一种名为对比学习矩阵补全（MCCL）的新方法。该方法首先为每个交互提取局部邻域子图，随后生成两种不同的图表示。第一种表示通过将GNN层与注意力机制结合来强调去噪，而第二种表示通过图变分自编码器将特征分布与标准先验对齐。训练过程中采用互学习损失函数逐步协调这些表示，使模型能够捕捉共同模式并显著提升泛化能力。在多个真实数据集上的广泛实验表明，我们的方法不仅提高了预测分数的数值准确性（RMSE提升高达0.8%），还生成了更优的排名（排名指标提升高达36%）。

</details>


### [197] [Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated Global Context Information](https://arxiv.org/abs/2506.10859)
**中文标题：通过后聚合全局上下文信息实现基于LLM的精确零样本逐点排序**

*Kehan Long,Shasha Li,Chen Xu,Jintao Tang,Ting Wang*

Main category: cs.IR

TL;DR: The paper introduces a novel Global-Consistent Comparative Pointwise Ranking (GCCP) strategy to improve zero-shot document ranking by incorporating global context information, achieving better performance while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Current pointwise ranking methods for zero-shot document ranking are efficient but ignore comparative insights between documents, leading to inconsistent scoring and suboptimal performance. The paper aims to enhance effectiveness while preserving efficiency.

Method: The proposed GCCP strategy uses a query-focused summary of pseudo-relevant candidates as an anchor document to capture global context. Contrastive relevance scores are generated and post-aggregated with existing pointwise methods (PAGC) for improved performance.

Result: Experiments on TREC DL and BEIR benchmarks show that GCCP significantly outperforms previous pointwise methods and achieves competitive performance against more resource-intensive comparative methods.

Conclusion: The GCCP strategy effectively integrates global context into pointwise ranking, improving performance without sacrificing efficiency, and validates the efficacy of the anchor construction approach.

摘要: 最近的进展成功利用大型语言模型（LLM）进行零样本文档排序，探索了多种提示策略。比较方法（如成对和列表方式）虽然效果显著，但计算密集，因此在大规模应用中不太实用。基于评分的逐点方法通过独立且同时生成每个候选文档的相关性分数，展现出更高的效率。然而，这种独立性忽略了文档之间的关键比较洞察，导致评分不一致和性能欠佳。本文旨在通过两项关键创新提高逐点方法的有效性，同时保持其效率：（1）我们提出了一种新颖的全局一致比较逐点排序（GCCP）策略，通过将每个候选文档与锚定文档进行全局参考比较，生成对比性相关性分数。我们策略性地将锚定文档设计为伪相关候选文档的查询聚焦摘要，通过捕获文档比较的全局上下文，作为有效的参考点。（2）这些对比性相关性分数可以与现有的逐点方法高效地进行后聚合（PAGC），以无需训练的方式无缝整合关键的全局上下文信息。在TREC DL和BEIR基准上的大量实验表明，我们的方法显著优于先前的逐点方法，同时保持了相当的效率。我们的方法还与需要更多计算资源的比较方法相比具有竞争力。更多分析进一步验证了我们锚定构建策略的有效性。

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [198] [DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction](https://arxiv.org/abs/2506.10309)
**中文标题：DUN-SRE：具有时空旋转等变性的深度展开网络用于动态MRI重建**

*Yuliang Zhu,Jing Cheng,Qi Xie,Zhuo-Xu Cui,Qingyong Zhu,Yuanyuan Liu,Xin Liu,Jianfeng Ren,Chengbo Wang,Dong Liang*

Main category: eess.IV

TL;DR: The paper introduces DUN-SRE, a deep unrolling network with spatiotemporal rotation equivariance, to improve dynamic MRI reconstruction by modeling both spatial and temporal symmetry priors, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Dynamic MRI exhibits spatial and temporal symmetries, but existing methods fail to fully exploit these priors, especially temporal symmetry, limiting reconstruction quality.

Method: DUN-SRE integrates (2+1)D equivariant convolutions into a deep unrolling framework, combining data consistency and proximal mapping to enforce spatiotemporal symmetry constraints.

Result: Experiments show DUN-SRE outperforms existing methods, particularly in preserving rotation-symmetric structures and generalizing to dynamic MRI tasks.

Conclusion: DUN-SRE effectively models spatiotemporal symmetries, enhancing dynamic MRI reconstruction accuracy and generalization.

摘要: 动态磁共振成像（MRI）表现出变换对称性，包括单个帧内的空间旋转对称性和时间维度上的时间对称性。在重建模型中明确纳入这些对称性先验可以显著提高图像质量，特别是在激进欠采样情况下。最近，等变卷积神经网络（ECNN）在利用空间对称性先验方面显示出巨大潜力。然而，现有的ECNN未能建模时间对称性，这可能是动态MRI重建中最普遍且信息丰富的结构先验。为解决这一问题，我们提出了一种新颖的具有时空旋转等变性的深度展开网络（DUN-SRE）用于动态MRI重建。DUN-SRE通过（2+1）D等变卷积架构建立时空等变性。特别是，它将数据一致性和近端映射模块集成到一个统一的深度展开框架中。这种架构确保了时空旋转对称性约束在整个重建过程中的严格传播，从而能够更物理准确地建模心脏运动动态。此外，还开发了一种高保真群滤波器参数化机制，以在施加对称性约束的同时保持表示精度。在心脏CINE MRI数据集上的综合实验表明，DUN-SRE实现了最先进的性能，特别是在保留旋转对称结构方面，为广泛的动态MRI重建任务提供了强大的泛化能力。

</details>


### [199] [Rethinking Brain Tumor Segmentation from the Frequency Domain Perspective](https://arxiv.org/abs/2506.10142)
**中文标题：从频域视角重新思考脑肿瘤分割**

*Minye Shao,Zeyu Wang,Haoran Duan,Yawen Huang,Bing Zhai,Shizheng Wang,Yang Long,Yefeng Zheng*

Main category: eess.IV

TL;DR: The paper proposes HFF-Net, a frequency-domain approach for brain tumor segmentation, improving performance by decomposing MRI images into low and high-frequency components and dynamically emphasizing critical details.


<details>
  <summary>Details</summary>
Motivation: Current brain tumor segmentation methods struggle with enhancing regions in MRI due to insufficient consideration of MRI-specific features like textures and directional variations.

Method: HFF-Net uses Frequency Domain Decomposition (FDD) to separate MRI images into low and high-frequency components, Adaptive Laplacian Convolution (ALC) to emphasize critical details, and Frequency Domain Cross-Attention (FDCA) for multi-scale feature fusion.

Result: HFF-Net achieves an average 4.48% improvement in mean Dice scores and 7.33% improvement in enhancing tumor segmentation, with maintained computational efficiency.

Conclusion: The frequency-domain perspective significantly enhances brain tumor segmentation, particularly for contrast-enhancing regions, demonstrating clinical applicability.

摘要: 脑肿瘤的精确分割，尤其是对比增强MRI中可见的区域（通过对比剂注射突出显示），对于准确的临床诊断和治疗计划至关重要，但仍具有挑战性。然而，当前方法在分割这些增强脑肿瘤区域时表现出显著的性能下降，主要由于对MRI特异性肿瘤特征（如复杂纹理和方向变化）的考虑不足。为此，我们提出了谐波频率融合网络（HFF-Net），从频域视角重新思考脑肿瘤分割。为了全面表征肿瘤区域，我们开发了频域分解（FDD）模块，将MRI图像分离为低频分量（捕捉平滑的肿瘤轮廓）和高频分量（突出细节纹理和方向边缘）。为了进一步增强对肿瘤边界的敏感性，我们引入了自适应拉普拉斯卷积（ALC）模块，通过动态更新的卷积核自适应强调关键高频细节。为了有效融合多尺度肿瘤特征，我们设计了频域交叉注意力（FDCA），整合语义、位置和切片特定信息。我们通过可视化、理论推理和实验分析进一步验证和解释频域改进。在四个公共数据集上的大量实验表明，HFF-Net在三个主要子区域的平均Dice得分中实现了4.48%（范围2.39%至7.72%）的相对改进，在对比增强肿瘤区域的分割中实现了7.33%（范围5.96%至8.64%）的相对改进，同时保持了良好的计算效率和临床适用性。代码：https://github.com/VinyehShaw/HFF。

</details>


### [200] [Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation](https://arxiv.org/abs/2506.10230)
**中文标题：基于提示引导的潜在扩散与预测类别条件用于3D前列腺MRI生成**

*Emerson P. Grabke,Masoom A. Haider,Babak Taati*

Main category: eess.IV

TL;DR: The paper introduces CCELLA, a novel dual-head conditioning approach for latent diffusion models (LDMs) to generate high-quality 3D prostate MRI images with limited data, outperforming existing methods in performance and accessibility.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current medical LDMs, which rely on short-prompt text encoders, non-medical LDMs, or large data volumes for fine-tuning, the authors propose a more efficient and accessible solution.

Method: The authors propose CCELLA, a dual-head conditioning approach that combines non-medical large language model-encoded text features and pathology classification. They also introduce a joint loss function and a data-efficient LDM training framework.

Result: The method achieves a 3D FID score of 0.025, significantly better than a foundation model's 0.071. Synthetic images improved classifier accuracy from 69% to 74%, and training solely on synthetic images matched real-image performance.

Conclusion: CCELLA enables pathology-conditioned LDM training with limited data, improving performance and accessibility for medical image synthesis.

摘要: 潜在扩散模型（LDM）可以缓解医学影像机器学习开发中的数据稀缺问题。然而，医学LDM训练通常依赖于性能或科学可访问性受限的策略，包括依赖短提示文本编码器、重用非医学LDM或需要大量数据进行微调。我们提出了一种类别条件高效大型语言模型适配器（CCELLA）来解决这些限制。CCELLA是一种新颖的双头条件方法，同时通过交叉注意力将非医学大型语言模型编码的文本特征和通过时间步嵌入的病理分类条件化LDM U-Net。我们还提出了联合损失函数和数据高效的LDM训练框架。这些策略的结合使得在有限数据量和人工数据标注下，能够进行病理条件化的LDM训练，生成高质量的医学影像，从而提升LDM性能和科学可访问性。我们的方法在规模受限的前列腺MRI数据集上实现了3D FID分数0.025，显著优于最近的基础模型（FID 0.071）。在训练前列腺癌预测分类器时，将我们方法生成的合成图像添加到训练数据集中，分类器准确率从69%提高到74%。仅使用我们方法的合成图像训练分类器，其性能与仅使用真实图像训练相当。

</details>


### [201] [Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization](https://arxiv.org/abs/2506.10233)
**中文标题：基于流体驱动异常随机化的条件扩散模型用于脑图像引导异常检测**

*Ana Lawry Aguila,Peirong Liu,Oula Puonti,Juan Eugenio Iglesias*

Main category: eess.IV

TL;DR: The paper introduces a conditional diffusion model framework for anomaly detection in brain MRI, using fluid-driven anomaly randomization to generate synthetic pseudo-pathologies for better reconstruction of healthy images. It outperforms existing methods, including supervised approaches.


<details>
  <summary>Details</summary>
Motivation: Supervised machine learning for pathology detection in brain MRI requires large disease-specific datasets, which are often unavailable for rare diseases. Unsupervised methods, like diffusion models, train on healthy images alone but struggle to accurately reconstruct anomalies or healthy tissue. This work aims to improve anomaly detection by integrating synthetic pseudo-pathologies into the modeling process.

Method: The authors propose a conditional diffusion model framework that uses fluid-driven anomaly randomization to generate realistic synthetic anomalies from auxiliary datasets. These synthetic anomalies guide the reconstruction of healthy images, improving anomaly detection.

Result: The model outperforms variational autoencoders, conditional and unconditional latent diffusion models, and even supervised inpainting methods in detecting pathology, as demonstrated on synthetic and real datasets (e.g., ATLAS).

Conclusion: The proposed framework effectively integrates synthetic anomalies to enhance anomaly detection and healthy image reconstruction, offering a robust solution for scenarios with limited diseased data.

摘要: 监督机器学习在脑MRI中实现了精确的病理检测，但在某些情况下（如罕见疾病）可能缺乏足够的患病数据。基于重建的无监督异常检测方法，尤其是扩散模型，因其仅需健康图像训练而在医学领域广受欢迎。然而，这些方法假设模型无法准确表示或重建异常区域，但实际中常无法重建健康组织或完全去除异常。本文提出了一种新颖的条件扩散模型框架，用于脑MRI中的异常检测和健康图像重建。我们的弱监督方法将合成的伪病理图像整合到建模过程中，以更好地指导健康图像的重建。为生成这些伪病理，我们采用流体驱动异常随机化技术对辅助数据集中的真实病理分割图进行增强，确保合成异常既真实又解剖学合理。我们评估了模型在合成异常数据集和ATLAS数据集中的病理检测能力。实验表明，我们的模型：（i）始终优于变分自编码器、条件及无条件潜在扩散模型；（ii）在多数数据集上超越了需要成对患病/健康图像的监督修复方法。

</details>


### [202] [SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation](https://arxiv.org/abs/2506.10325)
**中文标题：SWDL：基于深度拉普拉斯金字塔的分层差异学习用于半监督3D颅内出血分割**

*Cheng Wang,Siqi Chen,Donghua Mi,Yang Chen,Yudong Zhang,Yinsheng Li*

Main category: eess.IV

TL;DR: SWDL-Net is a semi-supervised learning framework for 3D intracranial hemorrhage segmentation, combining Laplacian pyramid edge sharpening with deep convolutional upsampling to improve lesion detail and boundary segmentation with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: The challenge of obtaining manual annotations for intracranial hemorrhage (ICH) segmentation due to its tedious and costly process motivates the need for semi-supervised learning (SSL) methods to reduce reliance on labeled data.

Method: SWDL-Net integrates Laplacian pyramid for edge sharpening and deep convolutional upsampling for detail precision, using a difference learning mechanism to combine these complementary approaches.

Result: SWDL-Net outperforms state-of-the-art methods on a 271-case ICH dataset and the BHSD benchmark, achieving superior performance with only 2% and 5% labeled data, respectively.

Conclusion: The proposed SWDL-Net framework effectively addresses the scarcity of labeled data in ICH segmentation by leveraging complementary advantages of Laplacian pyramid and deep convolutions, demonstrating significant improvements over existing methods.

摘要: 近年来，医学影像领域的进展使基于深度学习的分割成为主流方法，但其通常需要大量手动标注数据。然而，由于标注过程繁琐且成本高昂，获取颅内出血（ICH）的标注尤为困难。半监督学习（SSL）为解决标注数据稀缺问题提供了一种有前景的解决方案，尤其是在体积医学图像分割中。与主要关注高置信度伪标签或一致性正则化的传统SSL方法不同，我们提出了SWDL-Net，一种新颖的SSL框架，利用拉普拉斯金字塔和深度卷积上采样的互补优势。拉普拉斯金字塔擅长边缘锐化，而深度卷积通过灵活的特征映射增强细节精度。我们的框架通过差异学习机制有效整合了这些互补方法，实现了对病变细节和边界的卓越分割。在包含271例ICH的数据集和公共基准上的大量实验表明，SWDL-Net在仅使用2%标注数据的情况下优于当前最先进的方法。在公开可用的脑出血分割数据集（BHSD）上使用5%标注数据的进一步评估也证实了我们方法的优越性。代码和数据已发布于https://github.com/SIAT-CT-LAB/SWDL。

</details>


### [203] [ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2506.10675)
**中文标题：ConStyX：用于可泛化医学图像分割的内容风格增强**

*Xi Chen,Zhiqiang Shen,Peng Cao,Jinzhu Yang,Osmar R. Zaiane*

Main category: eess.IV

TL;DR: The paper proposes ConStyX, a domain generalization method for medical image segmentation that augments both content and style of training data to improve model robustness across multiple domains, while mitigating negative effects of over-augmentation.


<details>
  <summary>Details</summary>
Motivation: Medical images from multiple domains cause domain shifts, reducing segmentation model performance. Existing domain randomization methods are limited by style-only augmentation and over-augmentation issues.

Method: ConStyX augments both content and style of training data to cover a wider range of domains and mitigates negative effects of over-augmented features during training.

Result: Extensive experiments show ConStyX achieves superior generalization performance across multiple domains.

Conclusion: ConStyX effectively addresses domain shift issues in medical image segmentation by combining content and style augmentation, outperforming existing methods.

摘要: 医学图像通常来自多个领域，导致领域偏移，影响医学图像分割模型的性能。领域泛化（DG）旨在通过训练具有强泛化能力的鲁棒模型来解决这一问题。最近，许多基于领域随机化的DG方法被提出。然而，这些方法存在以下局限性：1）由于仅依赖图像风格扰动，领域随机化的效率受限；2）忽视了过度增强图像对模型训练的负面影响。为解决这些问题，我们提出了一种新颖的基于领域随机化的DG方法，称为内容风格增强（ConStyX），用于可泛化的医学图像分割。具体而言，ConStyX 1）增强训练数据的内容和风格，使增强后的训练数据能更好地覆盖更广泛的数据领域；2）在模型训练中利用良好增强的特征，同时减轻过度增强特征的负面影响。跨多个领域的广泛实验表明，我们的ConStyX实现了卓越的泛化性能。代码可在https://github.com/jwxsp1/ConStyX获取。

</details>


### [204] [Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches](https://arxiv.org/abs/2506.10825)
**中文标题：医学图像分割中的通用模型：与任务特定方法的调查及性能比较**

*Andrea Moglia,Matteo Leccardi,Matteo Cavicchioli,Alice Maccarini,Marco Marcon,Luca Mainardi,Pietro Cerveri*

Main category: eess.IV

TL;DR: This survey explores generalist models in medical image segmentation, comparing their performance with task-specific approaches, highlighting challenges, and suggesting future directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate the potential of generalist models, inspired by large language models, in medical image segmentation, and compare their effectiveness with traditional task-specific methods.

Method: The survey introduces foundational concepts, categorizes various adaptations of the Segment Anything Model (SAM), and analyzes their performance against state-of-the-art task-specific models.

Result: Generalist models show promise but face challenges in regulatory compliance, privacy, and AI trustworthiness, while also offering innovative directions for future research.

Conclusion: The study underscores the need for addressing practical challenges and explores future advancements in synthetic data, fusion techniques, and clinical applications.

摘要: 随着大型语言模型成功的范式转变，利用海量数据预训练并在不同下游任务上微调，通用模型已进入计算机视觉领域。Segment Anything Model（SAM）的引入为自然图像分割树立了里程碑，激发了医学图像分割中多种架构的设计。本调查对医学图像分割中的通用模型进行了全面深入的探讨。我们首先介绍了支撑其发展的基本概念，然后对SAM的不同变体进行了分类，包括零样本、少样本、微调、适配器、最新的SAM 2、仅基于图像的创新模型以及基于文本和图像的模型。我们详细分析了它们在初级研究和文献最佳水平上的表现，并与最先进的任务特定模型进行了严格比较。我们强调了在符合监管框架、隐私和安全法律、预算以及可信人工智能（AI）方面的挑战。最后，我们分享了关于合成数据、早期融合、从自然语言处理通用模型中学到的经验、代理AI和物理AI以及临床转化等未来方向的展望。

</details>


### [205] [Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image Segmentation](https://arxiv.org/abs/2506.10858)
**中文标题：Med-URWKV：基于纯RWKV和ImageNet预训练的医学图像分割方法**

*Zhenhuan Zhou*

Main category: eess.IV

TL;DR: Med-URWKV is a pure RWKV-based medical image segmentation model that leverages ImageNet pre-training, achieving competitive performance without modifications to the RWKV mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing methods (CNNs, Transformers, hybrids) have limitations like restricted receptive fields or high computational costs. RWKV offers linear complexity and strong long-range modeling, but its potential with pre-training for medical tasks is unexplored.

Method: Med-URWKV integrates a pure RWKV architecture into U-Net, using a pre-trained VRWKV encoder from ImageNet for medical image segmentation.

Result: Experiments on seven datasets show Med-URWKV matches or outperforms other RWKV models trained from scratch, validating the effectiveness of pre-training.

Conclusion: Med-URWKV demonstrates the benefits of pre-trained RWKV encoders in medical segmentation, offering a promising direction for future research.

摘要: 医学图像分割是计算机辅助诊断和治疗中的一项基础关键技术。现有方法主要分为三类：基于卷积神经网络（CNN）、基于Transformer以及两者的混合架构。然而，这些方法各有局限性，例如CNN的感受野受限或Transformer因二次复杂度带来的计算开销。最近，Receptance Weighted Key Value（RWKV）模型因其线性计算复杂度和强大的长程建模能力，成为视觉任务的有力替代方案。一些研究已将RWKV应用于医学图像分割任务，并取得了竞争性性能。但这些研究多集中于对Vision-RWKV（VRWKV）机制的修改，并从零开始训练模型，未探索利用预训练VRWKV模型在医学图像分割任务中的潜在优势。本文提出Med-URWKV，一种基于U-Net框架的纯RWKV架构，结合ImageNet预训练，进一步挖掘RWKV在医学图像分割中的潜力。据我们所知，Med-URWKV是医学领域首个可直接复用大规模预训练VRWKV编码器的纯RWKV分割模型。在七个数据集上的实验结果表明，Med-URWKV的分割性能与从零开始训练的其他精心优化的RWKV模型相当甚至更优，验证了预训练VRWKV编码器在提升模型性能方面的有效性。代码将公开。

</details>


### [206] [Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach](https://arxiv.org/abs/2506.10916)
**中文标题：数字病理学中的半自动化质量保证：基于瓦片分类的方法**

*Meredith VandeHaar,M. Clinch,I. Yilmaz,M. A. Rahman,Y. Xiao,F. Dogany,H. M. Alazab,A. Nassar,Z. Akkus,B. Dangott*

Main category: eess.IV

TL;DR: The paper proposes an AI algorithm for semi-automated quality assurance in digital pathology by classifying tiles into artifact types, reducing manual review time.


<details>
  <summary>Details</summary>
Motivation: Current quality assurance in digital pathology is manual and inefficient, with limited use of deep learning despite its potential to improve accuracy and scalability.

Method: The algorithm analyzes tiles from whole slide images, categorizing them into 10 artifact types or background, and creates a map to guide human reviewers. It uses InceptionResNet and a hybrid design of single and multiple instance models.

Result: The study demonstrated the effectiveness of the hybrid model in detecting artifacts like chatter, fold, and pen, optimizing detection for each type.

Conclusion: The hybrid AI approach improves efficiency and accuracy in digital pathology quality assurance, reducing reliance on manual review.

摘要: 质量保证是数字病理学中一个关键但尚未充分探索的领域，即使是微小的伪影也可能产生重大影响。伪影已被证明会对AI诊断模型的性能产生负面影响。目前，训练有素的工作人员在将这些数字化图像发布给病理学家之前会手动检查，这些图像随后用于诊断。传统的图像处理方法为检测数字病理学切片上的伪影提供了基础。然而，当前的工具并未利用深度学习，而深度学习有潜力提高检测的准确性和可扩展性。尽管有这些进步，数字病理学中的质量保证方法仍然有限，为创新提供了空间。
我们提出了一种AI算法，旨在通过分析瓦片并将其分类为10种预定义的伪影类型或背景来筛查数字病理学切片。该算法识别并定位伪影，创建一张突出显示感兴趣区域的地图。通过将人类操作员引导至受伪影影响的特定瓦片，该算法最大限度地减少了手动检查整个切片以发现质量问题所需的时间和精力。
从内部档案和癌症基因组图谱中，选择了133张全切片图像，并使用内部开发的软件ZAPP（Mayo Clinic, Jacksonville, FL）对10种伪影进行了标注。对不同瓦片尺寸和放大倍数的多个模型进行了消融研究，最终选择了InceptionResNet。训练并测试了单伪影模型，随后是一个有限的多实例模型（包含表现良好的伪影：chatter、fold和pen）。根据本研究的结果，我们建议采用一种混合设计进行伪影筛查，结合单伪影二值模型和多实例模型，以优化每种伪影的检测。

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [207] [HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration](https://arxiv.org/abs/2506.10401)
**中文标题：HPCTransCompile：一个用于高性能CUDA转译和LLM初步探索的AI编译器生成数据集**

*Jiaqi Lv,Xufeng He,Yanchen Liu,Xu Dai,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: The paper introduces HPCTransCompile, a framework for generating high-performance CUDA and platform code pairs using AI compiler and optimization technology, addressing the lack of quality datasets for CUDA transpilation. It also proposes HPCTransEval, a benchmark for evaluating LLM performance in CUDA transpilation.


<details>
  <summary>Details</summary>
Motivation: The dominance of CUDA in parallel computing requires performance portability to other platforms, but existing methods for CUDA transpilation face limitations in coverage, generalizability, and development costs. Additionally, LLMs lack high-quality datasets for effective CUDA transpilation.

Method: The paper proposes a framework leveraging AI compiler and automatic optimization technology to generate high-performance CUDA and platform code pairs. It includes a graph-based data augmentation method and introduces HPCTransEval for evaluating LLM performance.

Result: Experiments on CUDA-to-CPU transpilation show the framework significantly improves CUDA transpilation, demonstrating LLMs' potential to address CUDA ecosystem compatibility challenges.

Conclusion: The framework and benchmark enhance CUDA transpilation and highlight LLMs' capability to tackle compatibility issues in the CUDA ecosystem.

摘要: 深度学习的快速发展推动了模型参数和计算需求的指数级增长。NVIDIA GPU及其基于CUDA的软件生态系统为并行计算提供了强大支持，显著缓解了计算瓶颈。同时，由于用户编程习惯的培养和GPU的高性能，CUDA生态系统在并行软件领域确立了主导地位。这种主导地位要求其他硬件平台支持基于CUDA的软件，并具备性能可移植性。然而，由于并行编程范式和硬件架构的差异，将CUDA代码转译到其他平台面临重大挑战。现有方法依赖于语言扩展、领域特定语言（DSL）或编译器，但在工作负载覆盖范围和通用性方面存在局限性。此外，这些方法通常需要高昂的开发成本。最近，LLM在各个垂直领域展现出非凡潜力，尤其是在代码相关任务中。然而，现有LLM在CUDA转译，特别是高性能代码方面的表现仍不理想。这一局限的主要原因在于缺乏高质量的训练数据集。为解决这些挑战，我们提出了一种利用AI编译器和自动优化技术生成高性能CUDA及对应平台代码对的新框架。我们进一步通过基于图的数据增强方法增强了该框架，并引入了HPCTransEval，一个用于评估LLM在CUDA转译中性能的基准。我们以CUDA到CPU的转译为例，在领先的LLM上进行了实验。结果表明，我们的框架显著改善了CUDA转译，凸显了LLM在解决CUDA生态系统兼容性挑战方面的潜力。

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [208] [Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac Substructure Segmentation](https://arxiv.org/abs/2506.10797)
**中文标题：多模态心脏亚结构分割的模态无关图像级联（MAGIC）**

*Nicholas Summerfield,Qisheng He,Alex Kuo,Ahmed I. Ghanem,Simeng Zhu,Chase Ruff,Joshua Pan,Anudeep Kumar,Prashant Nagpal,Jiwei Zhao,Ming Dong,Carri K. Glide-Hurst*

Main category: physics.med-ph

TL;DR: The paper introduces MAGIC, a modality-agnostic deep learning model for multi-modality cardiac substructure segmentation, showing superior performance in 57% of cases compared to other models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalizability in deep learning models for cardiac substructure segmentation across different imaging modalities and overlapping structures, which is crucial for minimizing radiation-induced heart disease in thoracic therapy.

Method: MAGIC uses replicated encoding and decoding branches of an nnU-Net-based U-shaped backbone to segment 20 cardiac substructures from three modalities (Sim-CT, MR-Linac, CCTA). It was trained on 76 cases, validated on 15, and tested on 30, with 12 comparison models.

Result: MAGIC achieved average DSC scores of 0.75 (Sim-CT), 0.68 (MR-Linac), and 0.80 (CCTA), outperforming comparison models in 57% of cases with limited statistical differences.

Conclusion: MAGIC provides an effective, lightweight, and flexible solution for multi-modality cardiac substructure segmentation, simplifying computational requirements for clinical implementation.

摘要: 心脏亚结构在胸部放射治疗计划中至关重要，以最小化辐射诱发心脏病的风险。深度学习（DL）提供了减少勾画负担的高效方法，但缺乏对不同模态和重叠结构的泛化能力。本研究提出并验证了一种模态无关图像级联（MAGIC），用于全面且多模态的心脏亚结构分割。MAGIC通过基于nnU-Net的U形主干网络的复制编码和解码分支实现，保留了单一模型的功能。从模拟CT（Sim-CT）、低场MR-Linac和心脏CT血管造影（CCTA）模态中手动勾画的20个心脏亚结构（心脏、腔室、大血管、瓣膜、冠状动脉和传导节点）用于训练（n=76）、验证（n=15）和测试（n=30）MAGIC。12个比较模型（三个模态中的四个分割子组）进行了等效训练。所有方法在训练效率和参考轮廓上进行了比较，使用Dice相似系数（DSC）和双尾Wilcoxon符号秩检验（阈值p<0.05）。平均DSC分数为Sim-CT 0.75（0.16）、MR-Linac 0.68（0.21）和CCTA 0.80（0.16）。MAGIC在57%的案例中优于比较模型，统计差异有限。MAGIC提供了一种高效且准确的分割解决方案，轻量级且能够在单一模型中分割多模态和重叠结构。MAGIC通过简化计算需求并为临床环境提供无与伦比的灵活性，进一步实现了临床应用。

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [209] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
**中文标题：在π演算中对按值推送调用进行编码**

*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: The paper presents an encoding of Levy's call-by-push-value lambda-calculus (CBPV) in the pi-calculus, proving its soundness and completeness. It uses the internal pi-calculus (pi-i-calculus) for technical advantages and meets Gorla's criteria for good encodings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge CBPV and the pi-calculus, addressing challenges like formalization with de Bruijn indices and ensuring bisimulation congruence, while meeting established encoding criteria.

Method: The method involves defining an encoding of CBPV in the pi-i-calculus, providing informal proofs for soundness and completeness, and comparing it with other pi-calculus variants. A partial Coq formalization is also initiated.

Result: The encoding is proven sound and complete, satisfies Gorla's criteria, and shows similarities to Milner's encoding. The pi-i-calculus simplifies bisimulation and congruence properties.

Conclusion: The paper concludes that the encoding is robust, meets theoretical standards, and lays groundwork for further formalization in Coq.

摘要: 本报告定义了Levy的按值推送调用λ演算（CBPV）在π演算中的编码，并证明了该编码的健全性和完备性。我们提供了健全性、完备性及所需引理的非正式（手工）证明。该编码专门针对内部π演算（π-i-演算），以规避在形式化中使用de Bruijn索引的某些挑战，同时也有助于实现早、晚和开放双模拟的统一，且双模拟在此设定下具有同余性。此外，我们论证了该编码满足Gorla提出的良好编码五项标准，并展示了与Milner编码的相似之处。本文包括CBPV在π-i-演算、异步多态π演算和局部π演算中的编码。我们开始在Coq中对π-i-演算中编码的健全性和完备性进行形式化证明。并非所有用于形式化的引理本身都经过形式化证明，但我们认为这些未证明的引理是合理的，因为它们已通过手工证明或属于基于非正式论证的Coq形式化问题。

</details>


### [210] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
**中文标题：StepProof：自然语言数学证明的逐步验证**

*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: StepProof introduces a step-by-step autoformalization method for verifying natural language mathematical proofs at the sentence level, improving success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing autoformalization methods lack granularity, verifying only complete proofs. StepProof aims to enable finer, sentence-level verification to enhance accuracy and usability.

Method: StepProof breaks down proofs into verifiable subproofs for sentence-level verification, leveraging LLMs for natural language understanding and autoformalization.

Result: StepProof significantly improves proof success rates and efficiency, with minor manual adjustments further enhancing performance.

Conclusion: StepProof addresses the limitations of current autoformalization methods by enabling step-by-step verification, demonstrating its effectiveness in improving proof verification.

摘要: 交互式定理证明器（ITP）是用于将数学证明形式化验证至公理级别的强大工具。然而，它们缺乏自然语言接口仍然是一个重大限制。近年来，大型语言模型（LLM）的进展增强了对自然语言输入的理解，为自动形式化（即将自然语言证明翻译为可验证的形式化证明）铺平了道路。尽管有这些进展，现有的自动形式化方法仅限于验证完整证明，缺乏更细粒度的句子级验证能力。为了填补这一空白，我们提出了StepProof，一种新颖的自动形式化方法，专为逐步验证设计。StepProof将完整证明分解为多个可验证的子证明，实现句子级验证。实验结果表明，与传统方法相比，StepProof显著提高了证明成功率和效率。此外，我们发现对自然语言证明进行少量手动调整，使其更适合逐步验证，进一步提升了StepProof在自动形式化中的表现。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [211] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
**中文标题：利用静态代码分析增强大型语言模型以实现自动化代码质量改进**

*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: The paper integrates LLMs with static code analysis and RAG to automate code quality improvements, reducing issues and enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance code quality and streamline software development by automating issue detection and revision using LLMs and static analysis.

Method: Combines static code analysis for issue detection with LLMs for automated revisions, using RAG for relevance and a custom tool to correct hallucinations.

Result: Significant reduction in code issues, demonstrating the effectiveness of the combined approach.

Conclusion: Integrating LLMs with static analysis and RAG improves code quality and development efficiency.

摘要: 本研究探讨了通过将大型语言模型（如OpenAI的GPT-3.5 Turbo和GPT-4o）集成到软件开发工作流中，实现代码问题检测和自动化修订。一个静态代码分析框架用于检测大型软件项目中的问题，如漏洞、错误和代码异味。提取并组织每个问题的详细信息，以便利用LLMs进行自动化代码修订。通过迭代的提示工程过程，确保提示的结构化输出与项目需求一致。采用检索增强生成（RAG）以提高修订的相关性和精确性，使LLM能够访问并整合实时外部知识。针对LLM幻觉问题（模型生成看似合理但错误的输出），开发了一个自定义的“代码比较应用”，在将更改应用到代码库之前识别并纠正错误。后续静态代码分析扫描显示代码问题显著减少，证明了结合LLMs、静态分析和RAG在提高代码质量、优化软件开发流程以及节省时间和资源方面的有效性。

</details>


### [212] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
**中文标题：利用大型语言模型形式化软件需求**

*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: The paper introduces the VERIFAI project, which aims to automate the generation of formal specifications and ensure traceability of requirements in software development using NLP, ontologies, reuse of artifacts, and large language models.


<details>
  <summary>Details</summary>
Motivation: The project addresses challenges in traceability and verification of formal specifications in software development, aiming to streamline the process from design to implementation and verification.

Method: The project employs Natural Language Processing, ontologies, reuse of existing software artifacts, large language models, and AI to generate and trace formal specifications.

Result: The project explores innovative approaches to automate and improve the traceability and verification of software requirements.

Conclusion: The VERIFAI project presents a promising framework for enhancing the efficiency and accuracy of formal specification generation and traceability in software development.

摘要: 本文简要介绍了我们最近启动的项目VERIFAI：自然语言需求的可追溯性与验证。该项目通过支持自动生成形式化规范以及从初始软件设计阶段到系统实现和验证的需求可追溯性，解决了形式化规范的可追溯性和验证中的挑战。项目中探索的方法包括自然语言处理、使用本体描述软件系统领域、重用类似系统中的现有软件工件（即基于相似性的重用）、大型语言模型用于识别和声明规范，以及使用人工智能指导整个过程。

</details>


### [213] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
**中文标题：用户重视与批评的内容：AI驱动的移动应用用户反馈的大规模分析**

*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: This paper presents a large-scale analysis of user feedback on AI-powered mobile apps, identifying key themes in positive and negative reviews through a validated multi-stage pipeline.


<details>
  <summary>Details</summary>
Motivation: To understand how users perceive and critique AI features in mobile apps, given the lack of comprehensive studies despite the rapid proliferation of such features.

Method: A multi-stage analysis pipeline was developed, including review classification, aspect-sentiment extraction, and clustering, validated using a human-labeled benchmark and large language models (LLMs).

Result: Users focus on productivity, reliability, and personalized assistance in positive feedback, while negative feedback highlights technical failures, pricing, and language limitations. The pipeline extracted over one million aspect-sentiment pairs clustered into 33 topics.

Conclusion: The approach provides a detailed and accurate reflection of user experiences with AI-powered apps, revealing both universal and domain-specific sentiments.

摘要: 人工智能（AI）驱动的功能已迅速普及到包括生产力、教育、娱乐和创意在内的多个领域的移动应用中。然而，用户如何感知、评价和批评这些AI功能仍是一个未被充分探索的领域，主要原因是用户反馈的数量过于庞大。本研究首次对AI驱动的移动应用用户反馈进行了全面的大规模分析，利用了一个包含14个类别、292个AI驱动应用和894K条AI相关评论的Google Play数据集。我们开发并验证了一个多阶段分析流程，从人工标注的基准开始，系统地评估了大型语言模型（LLMs）和提示策略。每个阶段（包括评论分类、方面情感提取和聚类）均经过准确性和一致性的验证。我们的流程能够对用户反馈进行可扩展、高精度的分析，提取出超过一百万条方面情感对，并聚类为18个积极和15个消极用户主题。分析显示，用户始终关注一组狭窄的主题：积极评论强调生产力、可靠性和个性化帮助，而负面反馈则突出技术故障（如扫描和识别）、价格问题以及语言支持的局限性。我们的流程能够同时捕捉同一评论中对某一功能的满意和对另一功能的沮丧。这些细粒度、共现的情感往往被传统方法所忽视，这些方法要么孤立地处理积极和消极反馈，要么依赖粗粒度的分析。因此，我们的方法更真实地反映了用户对AI驱动应用的实际体验。基于类别的分析进一步揭示了普遍满意度驱动因素和特定领域的挫败感。

</details>


### [214] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
**中文标题：SWE-Factory：用于问题解决训练数据和评估基准的自动化工厂**

*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: SWE-Factory is an automated pipeline for generating GitHub issue resolution training data and evaluation benchmarks, addressing labor-intensive challenges with multi-agent systems, standardized grading, and automated validation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for creating large-scale GitHub issue resolution datasets are labor-intensive and challenging, especially in setting up environments, grading, and validation. SWE-Factory aims to automate these processes to accelerate dataset collection.

Method: SWE-Factory integrates three automated components: SWE-Builder (a multi-agent system for environment setup), exit-code-based grading, and automated fail2pass validation.

Result: Experiments on 671 issues across four languages show SWE-Factory constructs valid instances efficiently (e.g., $0.045 per instance with GPT-4.1-mini). Grading achieves 100% accuracy, and validation reaches 0.92 precision and 1.00 recall.

Conclusion: SWE-Factory effectively automates dataset creation for GitHub issue resolution, offering high-quality, cost-efficient solutions for training and evaluating LLMs.

摘要: 为GitHub问题解决任务构建大规模数据集对于训练和评估大型语言模型（LLM）的软件工程能力至关重要。然而，传统的基准创建过程极具挑战性且劳动密集，尤其是在设置评估环境、评分测试结果和验证任务实例的阶段。本文提出SWE-Factory，一种旨在解决这些挑战的自动化流水线。我们的流水线整合了三个核心自动化组件：首先，我们引入SWE-Builder，一个多智能体系统，用于自动化评估环境构建，该系统利用四个专用智能体在协作迭代循环中工作，并通过环境内存池提高效率。其次，我们提出了一种基于退出代码的标准化评分方法，无需手动编写自定义解析器。最后，我们利用这些可靠的退出代码信号自动化fail2pass验证过程。在四种编程语言的671个问题上进行的实验表明，我们的流水线能有效构建有效任务实例；例如，使用GPT-4.1-mini时，SWE-Builder以每实例0.045美元的成本构建了269个有效实例，而使用Gemini-2.5-flash时，以最低成本每实例0.024美元实现了可比性能。我们还证明，基于退出代码的评分与人工检查相比达到100%的准确率，自动化fail2pass验证的精确度为0.92，召回率为1.00。我们希望我们的自动化流水线能加速大规模、高质量的GitHub问题解决数据集的收集，用于训练和评估。我们的代码和数据集发布于https://github.com/DeepSoftwareAnalytics/swe-factory。

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [215] [Equitable Mechanism Design for Facility Location](https://arxiv.org/abs/2506.10460)
**中文标题：设施选址的公平机制设计**

*Toby Walsh*

Main category: cs.GT

TL;DR: The paper explores strategy-proof mechanisms for equitable facility location, focusing on the Gini index and Nash welfare, but proves an impossibility result for bounding the approximation ratio of the optimal Gini index.


<details>
  <summary>Details</summary>
Motivation: To design equitable facility location mechanisms that ensure fairness among agents, measured by the Gini index and Nash welfare, while addressing the limitations of strategy-proof mechanisms.

Method: The study first proves an impossibility result for bounding the Gini index approximation ratio, then proposes analyzing the complemented Gini index and Nash welfare through deterministic and randomized mechanisms.

Result: No strategy-proof mechanism can bound the approximation ratio of the optimal Gini index, but the complemented Gini index and Nash welfare offer viable alternatives for equitable outcomes.

Conclusion: While direct bounding of the Gini index is impossible, focusing on the complemented Gini index and Nash welfare provides practical avenues for equitable facility location design.

摘要: 我们研究了用于设施选址的策略证明机制，旨在最大化代理人之间的公平性。与文献中常见的做法一致，我们使用基尼指数来衡量公平性。首先，我们证明了一个简单但基本的不可能性结果：没有任何策略证明机制能够为一处或多处设施的最优基尼效用指数限定近似比率。因此，我们转而提出计算效用补基尼指数的近似比率，并探讨确定性和随机性机制在此方面的表现。此外，由于纳什福利常被视为平等主义和功利主义结果的折中方案，我们还研究了机制对纳什福利的近似效果。

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [216] [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
**中文标题：Omni-DPO：一种用于大语言模型动态偏好学习的双视角范式**

*Shangpin Peng,Weinong Wang,Zhuotao Tian,Senqiao Yang,Xing Wu,Haotian Xu,Chengquan Zhang,Takashi Isobe,Baotian Hu,Min Zhang*

Main category: cs.LG

TL;DR: Omni-DPO introduces a dual-perspective optimization framework for dynamic preference learning in LLMs, improving data utilization and performance by weighting samples based on inherent quality and model learning dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing DPO-based approaches treat all preference pairs uniformly, ignoring variations in quality and learning utility, leading to suboptimal performance. Omni-DPO addresses this by dynamically weighting samples.

Method: Omni-DPO jointly considers the inherent quality of preference pairs and the model's evolving performance, adaptively weighting samples during training for better data utilization.

Result: Omni-DPO outperforms baselines, with Gemma-2-9b-it beating Claude 3 Opus by 6.7 points on Arena-Hard and excelling in mathematical reasoning tasks.

Conclusion: Omni-DPO is effective and robust, demonstrating superior performance and generalization across tasks, with code and models made available.

摘要: 直接偏好优化（DPO）因其简单高效成为基于人类反馈的强化学习（RLHF）的核心技术。然而，现有的基于DPO的方法通常对所有偏好对一视同仁，忽略了其固有质量和学习效用的关键差异，导致数据利用和性能不佳。为解决这一问题，我们提出了Omni-DPO，一种双视角优化框架，同时考虑（1）每个偏好对的固有质量；（2）模型在这些对上的动态表现。通过根据数据质量和模型学习动态自适应地加权样本，Omni-DPO实现了更有效的训练数据利用和更好的性能。在各种模型和基准测试上的实验结果表明了Omni-DPO的优越性和泛化能力。在文本理解任务中，使用Omni-DPO微调的Gemma-2-9b-it在Arena-Hard基准测试上显著领先Claude 3 Opus达6.7分。在数学推理任务中，Omni-DPO在所有基准测试中均优于基线方法，为我们的方法的有效性和鲁棒性提供了强有力的实证证据。代码和模型将在https://github.com/pspdada/Omni-DPO上提供。

</details>


### [217] [Textual Bayes: Quantifying Uncertainty in LLM-Based Systems](https://arxiv.org/abs/2506.10060)
**中文标题：文本贝叶斯：量化基于大型语言模型系统中的不确定性**

*Brendan Leigh Ross,Noël Vouitsis,Atiyeh Ashari Ghomi,Rasa Hosseinzadeh,Ji Xin,Zhaoyan Liu,Yi Sui,Shiyi Hou,Kin Kwan Leung,Gabriel Loaiza-Ganem,Jesse C. Cresswell*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian approach to quantify uncertainty in LLM-based systems by treating prompts as textual parameters, using a novel MCMC algorithm (MHLP) for inference, and demonstrating improved accuracy and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Accurately quantifying uncertainty in LLMs is challenging due to their black-box nature and sensitivity to prompts, limiting their use in high-stakes domains.

Method: The paper proposes viewing LLM-based systems through a Bayesian lens, interpreting prompts as textual parameters, and introduces MHLP, a novel MCMC algorithm combining prompt optimization with standard MCMC methods.

Result: Empirical results show improvements in predictive accuracy and uncertainty quantification across various benchmarks.

Conclusion: The work provides a viable path to integrate Bayesian methods into LLMs, enhancing reliability and calibration in LLM-based systems.

摘要: 尽管大型语言模型（LLMs）在解决现实世界中的复杂任务方面能力日益增强，但准确量化其不确定性仍是一个关键且未解决的问题，这限制了它们在高风险领域的应用。这一挑战因许多最先进LLMs的闭源、黑盒特性而进一步加剧。此外，基于LLM的系统对绑定它们的提示（prompts）可能非常敏感，而这些提示通常需要大量手动调整（即提示工程）。在本研究中，我们通过贝叶斯视角来解决这些问题。我们将提示视为统计模型中的文本参数，从而能够利用少量训练数据对这些提示进行贝叶斯推断。这一新颖视角不仅能够对模型的文本参数及其下游预测进行原则性的不确定性量化，还能结合以自由文本形式表达的关于这些参数的先验信念。为了进行贝叶斯推断（即使对于研究充分的数据模态也是一个难题），我们引入了“通过LLM提议的Metropolis-Hastings”（MHLP），这是一种结合了提示优化技术与标准MCMC方法的新型马尔可夫链蒙特卡罗（MCMC）算法。MHLP是对现有LLM管道的即插即用修改，包括那些完全依赖闭源模型的管道。实验表明，我们的方法在一系列LLM基准测试和不确定性量化任务中均提高了预测准确性和不确定性量化（UQ）能力。更广泛地说，我们的工作展示了将丰富的贝叶斯方法融入LLM时代的可行路径，为更可靠和校准的基于LLM的系统铺平了道路。

</details>


### [218] [Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach](https://arxiv.org/abs/2506.10102)
**中文标题：基于图的选择性联邦多任务学习方法**

*Ahmed Elbakary,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: A novel federated multi-task learning method uses cross-client similarity and a feature anchor for personalized learning, with graph-based regularization and community detection to ensure efficient and fair collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of personalized learning in federated settings while maintaining communication efficiency and preventing negative collaboration among clients.

Method: Proposes a feature anchor for compact representation, shares lightweight classification heads, and uses dynamic graph-based regularization with community detection to group similar clients.

Result: Outperforms state-of-the-art baselines on heterogeneous datasets, showing superior computation/communication efficiency and fairness.

Conclusion: The method effectively enables personalized learning, ensures positive collaboration, and improves efficiency and fairness in federated multi-task learning.

摘要: 我们提出了一种新颖的联邦多任务学习方法，利用跨客户端相似性为每个客户端实现个性化学习。为了避免将整个模型传输到参数服务器，我们提出了一种通信高效的方案，引入了特征锚点——一个紧凑的向量表示，总结了从客户端本地类别学习到的特征。该特征锚点与服务器共享，以考虑本地客户端的分布。此外，客户端共享分类头（轻量级线性层），并执行基于图的正则化以实现客户端之间的协作。通过将客户端之间的协作建模为动态图并持续更新和优化该图，我们可以应对客户端的任何漂移。为了确保有益的知识转移并防止负面协作，我们采用了一种基于社区检测的方法，将该动态图划分为同质社区，最大化每个社区内任务相似性的总和（表示为图边的权重）。这种机制将协作限制在高度相似的客户端之间，确保积极的交互并保持个性化。在两个异构数据集上的大量实验表明，我们的方法显著优于现有基线。此外，我们展示了该方法在计算和通信效率上的优越性，并促进了客户端之间的公平性。

</details>


### [219] [GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments](https://arxiv.org/abs/2506.10120)
**中文标题：GRAIL：动态感知环境中图主动学习的基准**

*Maryam Khalid,Akane Sano*

Main category: cs.LG

TL;DR: GRAIL is a benchmarking framework for evaluating graph-based active learning (AL) in dynamic environments, introducing metrics for effectiveness, diversity, and user burden.


<details>
  <summary>Details</summary>
Motivation: Existing graph AL methods are evaluated on static datasets and focus only on accuracy, ignoring user-centric factors like diversity and adaptability in dynamic settings.

Method: GRAIL introduces a benchmarking framework with novel metrics to assess AL strategies in dynamic environments, using real-life human sensor data.

Result: Experiments reveal trade-offs between prediction performance and user burden, showing limitations in current AL strategies and the need for balanced approaches.

Conclusion: GRAIL highlights the importance of balancing node importance, query diversity, and network topology for effective AL in dynamic environments.

摘要: 基于图的主动学习（AL）利用图结构高效优先标记查询，降低标签成本和用户负担，适用于健康监测、人类行为分析和传感器网络等应用。通过识别战略位置的节点，图AL在保持模型性能的同时减少数据收集需求，成为动态环境中的有力工具。然而，现有图AL方法通常在静态图数据集上评估，且主要关注预测准确性，忽视了用户中心考量如采样多样性、查询公平性和动态环境的适应性。为填补这一空白，我们提出GRAIL，一种新颖的基准框架，用于评估动态现实环境中的图AL策略。GRAIL引入新指标评估持续有效性、多样性和用户负担，全面评估不同条件下的AL方法。在动态真实人类传感器数据上的广泛实验揭示了预测性能与用户负担之间的权衡，凸显了现有AL策略的局限性。GRAIL证明了平衡节点重要性、查询多样性和网络拓扑的重要性，为动态环境中的图AL解决方案提供了评估机制。

</details>


### [220] [Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning](https://arxiv.org/abs/2506.10137)
**中文标题：行为克隆中组合泛化的自预测表示**

*Daniel Lawson,Adriana Hugessen,Charlotte Cloutier,Glen Berseth,Khimya Khetarpal*

Main category: cs.LG

TL;DR: The paper proposes a method called $\text{BYOL-}\gamma$ augmented GCBC to improve combinatorial generalization in goal-conditioned behavioral cloning by learning temporally consistent state representations, avoiding contrastive samples or TD learning.


<details>
  <summary>Details</summary>
Motivation: Goal-conditioned behavioral cloning (GCBC) struggles with zero-shot generalization to novel state-goal pairs due to lack of temporal consistency in state representations. The paper aims to address this by leveraging successor representations.

Method: The authors introduce $\text{BYOL-}\gamma$ augmented GCBC, a representation learning objective that approximates successor representations without contrastive samples or TD learning, enhancing temporal consistency.

Result: The method achieves competitive performance on tasks requiring combinatorial generalization, demonstrating its effectiveness.

Conclusion: The proposed $\text{BYOL-}\gamma$ augmented GCBC successfully improves combinatorial generalization in GCBC by learning temporally consistent representations, offering a simpler alternative to existing methods.

摘要: 行为克隆（BC）方法通过监督学习（SL）从人类示范中学习策略，在机器人等领域非常有效。目标条件化这些策略使得单个通用策略能够捕捉离线数据集中的多样化行为。尽管目标条件化行为克隆（GCBC）方法在训练任务上表现良好，但它们未必能零样本泛化到需要处理新颖状态-目标对的任务，即组合泛化。这一限制部分归因于BC学习的状态表示缺乏时间一致性；如果时间相关的状态被编码为相似的潜在表示，那么新颖状态-目标对的分布外差距将会减小。因此，在表示空间中鼓励这种时间一致性应有助于组合泛化。后继表示（successor representations）很好地体现了这一特性，它编码了从当前状态访问的未来状态分布。然而，先前学习后继表示的方法依赖于对比样本、时间差分（TD）学习或两者。本文提出了一种简单而有效的表示学习目标——$\text{BYOL-}\gamma$增强的GCBC，它不仅能在有限MDP情况下理论上近似后继表示而无需对比样本或TD学习，而且在一系列需要组合泛化的挑战性任务中表现出色。

</details>


### [221] [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138)
**中文标题：解读学习搜索：在玩推箱子的RNN中找到转移模型和价值函数**

*Mohammad Taufeeque,Aaron David Tucker,Adam Gleave,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: The paper reverse-engineers a convolutional RNN trained to play Sokoban, revealing mechanisms akin to bidirectional search, including directional activations acting as a value function and specialized kernels forming a transition model.


<details>
  <summary>Details</summary>
Motivation: To understand how a model-free reinforcement learning-trained RNN solves Sokoban puzzles and leverages test-time compute, revealing interpretable mechanisms.

Method: Analyzed the RNN's activations and kernels, identifying directional channels as value functions and specialized kernels as transition models, while noting unique aspects like separate box consideration.

Result: The RNN's mechanisms resemble bidirectional search, with directional activations guiding backtracking and pruning, and kernels forming paths, though state representation is not unified.

Conclusion: The RNN's learned mechanisms are interpretable and analogous to classic search components, despite differences like separate box processing and layered representations.

摘要: 我们部分逆向工程了一个通过无模型强化学习训练的卷积循环神经网络（RNN），该网络用于玩推箱子游戏。先前的研究发现，该网络通过更多的测试时间计算解决了更多关卡。我们的分析揭示了与经典双向搜索组件类似的几种机制。对于每个方块，RNN通过特定方向的通道激活表示其计划。这些状态-动作激活类似于价值函数——其大小决定了何时回溯以及哪个计划分支保留下来。专门的核将这些激活（包含计划和价值）向前和向后扩展以形成路径，构成一个转移模型。该算法在某些方面也与经典搜索不同。状态表示并不统一；相反，网络分别考虑每个箱子。每一层都有自己的计划表示和价值函数，增加了搜索深度。远非难以理解，通过无模型训练学习的网络机制可以利用测试时间计算，并以熟悉的术语进行解释。

</details>


### [222] [A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes](https://arxiv.org/abs/2506.10180)
**中文标题：机器学习技术用于糖尿病早期预测的比较研究**

*Mowafaq Salem Alzboon,Mohammad Al-Batah,Muhyeeddin Alqaraleh,Ahmad Abuashour,Ahmad Fuad Bader*

Main category: cs.LG

TL;DR: The study compares machine learning techniques for diabetes prediction, finding Neural Networks and Random Forests most effective with accuracies of 78.57% and 76.30%, respectively.


<details>
  <summary>Details</summary>
Motivation: Diabetes is a growing health concern, and early detection is vital. The paper explores machine learning's potential to improve diabetes prediction.

Method: The study evaluates Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network using the Pima Indians Diabetes dataset.

Result: Neural Networks achieved the highest accuracy (78.57%), followed by Random Forests (76.30%).

Conclusion: Machine learning, particularly Neural Networks and Random Forests, can effectively predict diabetes and serve as early detection tools.

摘要: 在许多国家，糖尿病正成为一个重大的健康问题，早期识别和管理至关重要。使用机器学习算法预测糖尿病已取得令人鼓舞的结果。本研究利用皮马印第安人糖尿病数据集，评估了几种机器学习方法在糖尿病预测中的效果。该数据集包含768名患者的信息，如年龄、BMI和血糖水平。评估的技术包括逻辑回归、决策树、随机森林、k近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络。结果表明，神经网络算法的表现最佳，准确率为78.57%，其次是随机森林方法，准确率为76.30%。研究暗示机器学习算法有助于糖尿病预测，并可作为高效的早期检测工具。

</details>


### [223] [Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition](https://arxiv.org/abs/2506.10184)
**中文标题：利用多层感知器网络优化遗传算法以增强TinyFace识别**

*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh*

Main category: cs.LG

TL;DR: The paper explores optimizing MLP networks using GA and PCA for TinyFace recognition, showing GA's superiority in complex datasets and PCA's benefits in simpler ones.


<details>
  <summary>Details</summary>
Motivation: To enhance TinyFace recognition by optimizing MLP networks through feature selection (GA) and dimensionality reduction (PCA), addressing performance in diverse datasets.

Method: 1) Baseline MLP training, 2) GA-based feature selection, 3) PCA-based dimension reduction, tested on TinyFace, Heart Disease, and Iris datasets.

Result: GA improved accuracy in complex datasets by identifying critical features, while PCA worked better in low-dimensional, noise-free datasets.

Conclusion: Feature selection (GA) and dimensionality reduction (PCA) interdependently enhance MLP performance, offering practical guidelines for machine learning tasks.

摘要: 本研究通过涉及TinyFace、心脏病和Iris三个不同数据集的严格实验过程，对多层感知器（MLP）网络进行了实证研究。研究概述包括三种关键方法：a）使用默认设置进行MLP基线训练，b）基于遗传算法（GA）的特征选择优化，c）基于主成分分析（PCA）的降维。结果显示这些技术对性能的影响：PCA在低维和无噪声数据集中表现出优势，而GA通过准确识别关键特征在复杂数据集中持续提高准确性。比较表明，特征选择和降维在提升MLP性能中具有相互依赖的作用。本研究为特征工程和神经网络参数优化的文献提供了贡献，并为广泛的机器学习任务提供了实用指南。

</details>


### [224] [Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment](https://arxiv.org/abs/2506.10186)
**中文标题：通过旋转对齐实现可扩展的非等变3D分子生成**

*Yuhui Ding,Thomas Hofmann*

Main category: cs.LG

TL;DR: The paper proposes a scalable non-equivariant 3D molecule generation method by learning rotational alignments, achieving performance comparable to equivariant models with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Equivariant diffusion models, while effective, are limited by specialized architectures that hinder scalability and efficiency. This paper aims to relax these constraints while maintaining high performance.

Method: The approach learns sample-dependent SO(3) transformations to align molecules in a latent space, then trains a non-equivariant diffusion model on these aligned representations.

Result: The method outperforms non-equivariant models, matches state-of-the-art equivariant models in quality, and improves training and sampling efficiency.

Conclusion: The proposed method offers a scalable alternative to equivariant models without sacrificing performance, with potential for broader applications.

摘要: 等变扩散模型在3D分子生成中表现出色，这些模型通过使用SE(3)-等变去噪网络结合了3D分子的欧几里得对称性。然而，专门的等变架构限制了扩散模型的可扩展性和效率。本文提出了一种放松此类等变约束的方法。具体而言，我们的方法为每个分子学习一个样本依赖的SO(3)变换，以构建对齐的潜在空间。随后在对齐表示上训练一个非等变扩散模型。实验结果表明，我们的方法显著优于先前报道的非等变模型，其样本质量与最先进的等变扩散模型相当，并提供了更高的训练和采样效率。代码可在https://github.com/skeletondyh/RADM获取。

</details>


### [225] [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
**中文标题：利用元学习检测维基百科上的傀儡账户**

*Luc Raszewski,Christine De Kock*

Main category: cs.LG

TL;DR: The paper proposes using meta-learning to improve sockpuppet detection on Wikipedia by adapting to author-specific behaviors, outperforming pre-trained models in precision.


<details>
  <summary>Details</summary>
Motivation: To enhance the detection of malicious sockpuppets on Wikipedia by addressing the limitations of prior methods, which struggle with adaptability to specific author behaviors and limited text data.

Method: The paper applies meta-learning, a technique that trains models across multiple tasks to optimize rapid adaptation to new sockpuppet-group writing styles.

Result: Meta-learning significantly improves prediction precision compared to pre-trained models, demonstrating better performance in data-scarce settings.

Conclusion: Meta-learning advances sockpuppetry detection by enabling models to adapt quickly to new behaviors, with potential benefits for open editing platforms. The release of a new dataset supports future research.

摘要: 在维基百科上检测恶意傀儡账户对于维护互联网上可靠信息的获取和防止虚假信息的传播至关重要。先前的机器学习方法依赖于风格和元数据特征，但未能优先适应特定作者的行为。因此，这些方法在建模特定傀儡账户群体的行为时效果不佳，尤其是在文本数据有限的情况下。为解决这一问题，我们提出应用元学习，这是一种机器学习技术，旨在通过跨多个任务训练模型来提高在数据稀缺环境中的性能。元学习优化模型，使其能够快速适应新的傀儡账户群体的写作风格。我们的结果表明，与预训练模型相比，元学习显著提高了预测的精确度，标志着在开放编辑平台上打击傀儡账户行为的进步。我们发布了一个新的傀儡账户调查数据集，以促进未来在傀儡账户和元学习领域的研究。

</details>


### [226] [Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms](https://arxiv.org/abs/2506.10212)
**中文标题：心电与心音信号的交叉学习：探索双模态机电心脏波形的共同与独有特性**

*Sajjad Karimi,Amit J. Shah,Gari D. Clifford,Reza Sameni*

Main category: cs.LG

TL;DR: The paper explores the relationship between ECG and PCG signals, using machine learning to reconstruct one from the other, highlighting the superiority of nonlinear models and challenges in cross-subject scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand the distinct and overlapping information in ECG and PCG signals, their potential for mutual reconstruction, and biomarker extraction under varying physiological conditions.

Method: Employed linear and nonlinear machine learning models, including non-causal LSTM networks, on the EPHNOGRAM dataset of simultaneous ECG-PCG recordings during rest and exercise.

Result: Nonlinear models, especially non-causal LSTM, performed best. ECG reconstruction from PCG was easier than the reverse. Exercise and cross-subject scenarios posed challenges, but envelope-based modeling improved generalizability. Clinically relevant ECG biomarkers could be estimated from PCG.

Conclusion: The study enhances understanding of ECG and PCG relationships, with potential applications in multimodal cardiac monitoring technologies.

摘要: 同步心电图（ECG）和心音图（PCG）通过分别捕捉心脏的电活动和机械活动，提供了心脏功能的多模态视角。然而，这些信号的独特和重叠信息内容，以及它们在相互重建和生物标志物提取方面的潜力，尤其是在不同生理条件和个体间，尚未完全理解。
在本研究中，我们利用EPHNOGRAM数据集中的静息和运动状态下的同步ECG-PCG记录，系统地研究了ECG和PCG的共同与独有特性。我们采用了一系列线性和非线性机器学习模型，包括非因果LSTM网络，以从另一种模态重建每种模态，并分析了因果关系、生理状态和跨受试者变异性的影响。结果表明，非线性模型，特别是非因果LSTM，提供了更优的重建性能，其中从PCG重建ECG比反向重建更为可行。运动和跨受试者场景带来了显著挑战，但基于包络的建模利用瞬时振幅特征显著提高了跨模态学习的跨受试者泛化能力。此外，我们还证明，临床相关的ECG生物标志物（如基准点和QT间期）可以在跨受试者设置下从PCG中估计。
这些发现增进了我们对机电心脏模态之间关系的理解，包括波形特性和心脏事件的时间，为新型多模态心脏监测技术提供了潜在应用。

</details>


### [227] [Provably Learning from Language Feedback](https://arxiv.org/abs/2506.10341)
**中文标题：可证明地从语言反馈中学习**

*Wanqiao Xu,Allen Nie,Ruijie Zheng,Aditya Modi,Adith Swaminathan,Ching-An Cheng*

Main category: cs.LG

TL;DR: The paper formalizes Learning from Language Feedback (LLF), introduces a complexity measure called transfer eluder dimension, and presents a no-regret algorithm (HELiX) for solving LLF problems with performance guarantees.


<details>
  <summary>Details</summary>
Motivation: To provide a principled framework for learning from language feedback, addressing the lack of theoretical grounding in existing empirical demonstrations.

Method: The paper formalizes LLF, introduces transfer eluder dimension as a complexity measure, and develops the HELiX algorithm for no-regret learning from language feedback.

Result: The transfer eluder dimension captures the learning complexity of LLF, and HELiX performs well empirically, even when standard LLM prompting fails.

Conclusion: The work lays the foundation for principled interactive learning from language feedback, with theoretical and empirical validation.

摘要: 通过观察和语言反馈进行交互式学习是一个日益受到关注的领域，这得益于大型语言模型（LLM）代理的出现。尽管已有令人印象深刻的实证演示，但目前仍缺乏对这些决策问题的原则性框架。本文中，我们形式化了“从语言反馈中学习”（LLF）问题，提出了足够的假设以在潜在奖励的情况下实现学习，并引入了“转移规避维度”作为复杂性度量来表征LLF问题的难度。我们表明，转移规避维度捕捉了反馈中信息改变LLF问题学习复杂性的直觉。我们展示了在某些情况下，从丰富的语言反馈中学习可以比从奖励中学习快指数级。我们开发了一种无遗憾算法HELiX，它通过顺序交互可证明地解决LLF问题，其性能保证与问题的转移规避维度成比例。在多个实证领域中，我们表明HELiX即使在重复提示LLM不可靠的情况下也能表现良好。我们的贡献标志着设计基于通用语言反馈的原则性交互式学习算法的第一步。

</details>


### [228] [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
**中文标题：我们能否从大语言模型中推断出训练数据的机密属性？**

*Penguin Huang,Chhavi Yadav,Ruihan Wu,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: The paper investigates whether sensitive properties of training data can be inferred from large language models (LLMs) and introduces PropInfer, a benchmark for evaluating such attacks, demonstrating their success.


<details>
  <summary>Details</summary>
Motivation: To explore the vulnerability of LLMs in revealing confidential dataset-level properties, such as patient demographics or disease prevalence, during fine-tuning, which has not been studied before.

Method: Introduces PropInfer, a benchmark task for property inference in LLMs, and proposes two tailored attacks: a prompt-based generation attack and a shadow-model attack using word frequency signals.

Result: Empirical evaluations show the attacks are successful, revealing a previously unrecognized vulnerability in LLMs.

Conclusion: LLMs are susceptible to property inference attacks, highlighting a new security concern in their deployment for sensitive applications.

摘要: 大型语言模型（LLMs）越来越多地在特定领域的数据集上进行微调，以支持医疗、金融和法律等领域的应用。这些微调数据集通常具有敏感和机密的属性（如患者人口统计或疾病流行率），这些属性本不应被泄露。虽然先前的工作研究了判别模型（如图像分类模型）和生成模型（如图像数据的GANs）的属性推断攻击，但这些攻击是否适用于LLMs尚不清楚。在这项工作中，我们引入了PropInfer，一个用于评估LLMs在问答和聊天完成两种微调范式下属性推断的基准任务。基于ChatDoctor数据集，我们的基准包括多种属性类型和任务配置。我们还提出了两种定制攻击：基于提示的生成攻击和利用词频信号的影子模型攻击。对多个预训练LLMs的实证评估表明，我们的攻击是成功的，揭示了LLMs中一个先前未被识别的漏洞。

</details>


### [229] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
**中文标题：LaMAGIC2：基于语言模型的模拟拓扑生成的高级电路公式**

*Chen-Chia Chang,Wan-Hsuan Lin,Yikang Shen,Yiran Chen,Xin Zhang*

Main category: cs.LG

TL;DR: LaMAGIC2 introduces a succinct float-input canonical formulation (SFCI) for language model-based analog topology generation, improving efficiency and precision over prior methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for analog topology design are inefficient and lack precision sensitivity, necessitating a more robust framework.

Method: LaMAGIC2 uses an identifier-based representation (SFCI) to reduce token length complexity and enhance numeric precision sensitivity.

Result: LaMAGIC2 achieves 34% higher success rates under tight tolerances and 10X lower MSEs, with improved transferability for larger circuits.

Conclusion: LaMAGIC2 establishes a robust framework for analog topology generation, outperforming prior methods in efficiency and precision.

摘要: 现代应用对模拟拓扑设计的定制化需求使得自动化设计变得至关重要，但目前的方法需要大量人工操作。现有最先进的工作采用序列到序列的方法，并通过监督微调语言模型来生成用户指定的拓扑。然而，其电路公式效率低下，原因是其令牌长度复杂度为O(|V|²)，且对数值输入的精度敏感性较低。本文提出了LaMAGIC2，一种基于语言模型的模拟拓扑生成的简洁浮点输入规范公式（SFCI）。SFCI通过基于标识符的表示改进组件类型识别，将令牌长度复杂度降低至O(|V|)，并增强数值精度敏感性，从而在严格容差下实现更好的性能。实验表明，与先前方法相比，LaMAGIC2在0.01的严格容差下实现了34%更高的成功率，且均方误差降低了10倍。LaMAGIC2在顶点较多的电路中还表现出更好的可迁移性，最高提升58.5%。这些进展使LaMAGIC2成为模拟拓扑生成的强大框架。

</details>


### [230] [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/abs/2506.10378)
**中文标题：通过因果表示学习发现语言模型的层次化潜在能力**

*Jikai Jin,Vasilis Syrgkanis,Sham Kakade,Hanlin Zhang*

Main category: cs.LG

TL;DR: The paper proposes a causal representation learning framework to identify latent capability factors in language models, revealing a hierarchical causal structure among general problem-solving, instruction-following, and mathematical reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Faithful evaluation of language model capabilities is hindered by methodological challenges like confounding effects and high computational costs. The study aims to uncover latent capabilities causally.

Method: A causal representation learning framework models benchmark performance as linear transformations of latent capability factors, controlling for base model variations as confounders.

Result: A three-node linear causal structure explains performance variations, showing a clear hierarchy from general problem-solving to instruction-following and mathematical reasoning.

Conclusion: The study highlights the importance of controlling base model variations to uncover causal relationships among latent capabilities, offering insights beyond numerical rankings.

摘要: 对语言模型能力的忠实评估对于指导模型开发至关重要。然而，该领域的严格因果评估面临重大方法学挑战，包括复杂的混杂效应和与广泛重新训练相关的高计算成本。为解决这些挑战，我们提出了一个因果表示学习框架，其中观察到的基准性能被建模为少数潜在能力因素的线性变换。关键的是，这些潜在因素在适当控制基础模型作为共同混杂因素后被识别为因果相关的。将这种方法应用于包含超过1500个模型在Open LLM Leaderboard六个基准上评估的综合数据集，我们确定了一个简洁的三节点线性因果结构，可靠地解释了观察到的性能变化。进一步解释这一因果结构提供了超越简单数值排名的实质性科学见解：具体而言，我们揭示了一个明确的因果方向，从一般问题解决能力开始，通过指令遵循熟练度，最终达到数学推理能力。我们的结果强调了在评估过程中仔细控制基础模型变化的关键作用，这是准确揭示潜在模型能力之间因果关系的必要步骤。

</details>


### [231] [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
**中文标题：Time-IMM：一种用于不规则多模态多元时间序列的数据集与基准**

*Ching Chang,Jeehyun Hwang,Yidan Shi,Haixin Wang,Wen-Chih Peng,Tien-Fu Chen,Wei Wang*

Main category: cs.LG

TL;DR: Time-IMM is a dataset and benchmark for irregular multimodal time series, addressing real-world data challenges like varying sampling rates and missingness. IMM-TSF, the benchmark library, improves forecasting by modeling multimodality explicitly.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks assume clean, regular time series, while real-world data is irregular and multimodal. Time-IMM bridges this gap by providing a dataset and tools for realistic analysis.

Method: Time-IMM categorizes nine types of irregularity and introduces IMM-TSF, a benchmark with fusion modules for asynchronous integration and evaluation.

Result: Explicitly modeling multimodality on irregular data significantly improves forecasting performance.

Conclusion: Time-IMM and IMM-TSF advance time series analysis under real-world conditions, providing tools for better forecasting.

摘要: 现实应用中的时间序列数据（如医疗、气候建模和金融）通常是不规则、多模态且混乱的，具有不同的采样率、异步模态和普遍缺失。然而，现有基准通常假设数据是干净、规则采样的单模态数据，导致研究与实际部署之间存在显著差距。我们提出了Time-IMM，这是一个专门设计用于捕捉多模态多元时间序列中因果驱动的不规则性的数据集。Time-IMM代表了九种不同类型的时间序列不规则性，分为触发型、约束型和人为型机制。作为数据集的补充，我们引入了IMM-TSF，一个用于不规则多模态时间序列预测的基准库，支持异步集成和现实评估。IMM-TSF包括专门的融合模块，如时间戳到文本的融合模块和多模态融合模块，支持基于最近平均和基于注意力的集成策略。实证结果表明，显式建模多模态不规则时间序列数据能显著提升预测性能。Time-IMM和IMM-TSF为在现实条件下推进时间序列分析提供了基础。数据集可在https://www.kaggle.com/datasets/blacksnail789521/time-imm/data公开获取，基准库可在https://anonymous.4open.science/r/IMMTSF_NeurIPS2025访问。

</details>


### [232] [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/abs/2506.10617)
**中文标题：基于深度学习的重叠心电图图像数字化方法及开源Python代码**

*Reza Karbasi,Masoud Rahimi,Abdol-Hossein Vahabie,Hadi Moradi*

Main category: cs.LG

TL;DR: The paper proposes a two-stage deep learning pipeline to accurately digitize overlapping ECG images, achieving superior performance over baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of digitizing paper-based ECG recordings, especially those with overlapping signals, which is under-addressed in existing methods.

Method: A U-Net based segmentation network isolates the primary ECG trace, followed by an adaptive grid detection module to convert the mask into a time-series signal.

Result: The U-Net achieved an IoU of 0.87, and the digitization method outperformed baselines with lower MSE (0.0010 vs. 0.0015) and higher Pearson Correlation (0.9644 vs. 0.9366) for non-overlapping signals, and similar improvements for overlapping signals.

Conclusion: The proposed method significantly enhances digitization accuracy, especially for overlapping signals, and provides a reliable foundation for converting analog ECG records into digital data.

摘要: 本文解决了纸质心电图（ECG）记录准确数字化的持续挑战，特别关注了信号重叠导致的单导联受损问题——这是现有方法中常见但未充分解决的问题。我们提出了一种两阶段流程来克服这一限制。第一阶段采用基于U-Net的分割网络，通过训练包含重叠信号和自定义数据增强的数据集，准确分离主要ECG轨迹。第二阶段通过自适应网格检测模块将精炼的二进制掩码转换为时间序列信号，提高了对不同ECG格式和尺度的适应性。实验结果表明，我们的方法在细粒度分割任务中U-Net架构的IoU达到0.87。重要的是，与现有基线技术相比，我们的数字化方法在非重叠和重叠ECG样本中均表现出更优性能。对于非重叠信号，我们的方法实现了均方误差（MSE）0.0010和皮尔逊相关系数（rho）0.9644，而基线分别为0.0015和0.9366。对于信号重叠样本，我们的方法MSE为0.0029，rho为0.9641，显著优于基线的0.0178和0.8676。这项工作展示了一种有效策略，显著提高了数字化准确性，尤其是在信号重叠情况下，为将模拟ECG记录可靠转换为可分析的数字化数据奠定了坚实基础，适用于现代研究和临床应用。实现代码已公开于GitHub仓库：https://github.com/masoudrahimi39/ECG-code。

</details>


### [233] [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/abs/2506.10751)
**中文标题：Neural在ArchEHR-QA 2025中的应用：基于证据的临床问答的代理提示优化**

*Sai Prasanna Teja Reddy Bogireddy,Abrar Majeedi,Viswanatha Reddy Gajjala,Zhuoyan Xu,Siddhant Rai,Vaishnav Potlapalli*

Main category: cs.LG

TL;DR: The paper introduces Neural, a method for clinical QA that decouples evidence retrieval and answer synthesis, using prompt optimization to achieve high performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To improve automated QA over EHRs by addressing the challenges of precise evidence retrieval and faithful answer generation with limited supervision.

Method: Decouples the task into evidence identification and answer synthesis, uses DSPy's MIPROv2 for prompt optimization, and employs self-consistency voting to enhance evidence recall.

Result: Achieved an overall score of 51.5 on the test set, outperforming zero-shot and few-shot prompting by over 20 and 10 points, respectively.

Conclusion: Data-driven prompt optimization is a cost-effective alternative to model fine-tuning for clinical QA, enhancing AI reliability in healthcare.

摘要: 电子健康记录（EHR）上的自动问答（QA）可以为临床医生和患者填补关键信息缺口，但其需要精确的证据检索和有限监督下的忠实答案生成。本文介绍了Neural，这是BioNLP 2025 ArchEHR-QA共享任务中基于证据的临床QA的亚军方法。我们的方法将任务分解为（1）句子级证据识别和（2）带有明确引用的答案合成。在每个阶段，我们使用DSPy的MIPROv2优化器自动探索提示空间，并在开发集上联合调整指令和少量示例。自我一致性投票方案进一步提高了证据召回率而不牺牲精确度。在隐藏测试集上，我们的方法获得了51.5的总分，位居第二，同时分别比标准的零样本和少样本提示高出20分和10分以上。这些结果表明，数据驱动的提示优化是高风险临床QA中模型微调的经济高效替代方案，提升了医疗保健中AI助手的可靠性。

</details>


### [234] [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
**中文标题：扩散对偶**

*Subham Sekhar Sahoo,Justin Deschenaux,Aaron Gokaslan,Guanghan Wang,Justin Chiu,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: The paper introduces Duo, a method that improves uniform-state discrete diffusion models by leveraging insights from Gaussian diffusion, achieving faster training and sampling while outperforming autoregressive models on some benchmarks.


<details>
  <summary>Details</summary>
Motivation: Uniform-state discrete diffusion models are promising for fast text generation but underperform compared to autoregressive and masked diffusion models. The paper aims to bridge this gap by utilizing properties of Gaussian diffusion.

Method: The method, Duo, transfers techniques from Gaussian diffusion to discrete diffusion. It includes a curriculum learning strategy guided by Gaussian processes to speed up training and Discrete Consistency Distillation to enable few-step generation.

Result: Models trained with Duo surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Sampling is accelerated by two orders of magnitude through Discrete Consistency Distillation.

Conclusion: Duo successfully narrows the performance gap between uniform-state discrete diffusion models and autoregressive models, demonstrating the potential of leveraging Gaussian diffusion insights in discrete settings.

摘要: 均匀状态离散扩散模型因其固有的自我纠正能力而有望实现快速文本生成，但其性能通常不及自回归模型和掩码扩散模型。本研究通过利用一个关键发现来缩小这一性能差距：均匀状态扩散过程自然源于底层的高斯扩散。我们的方法Duo将高斯扩散的强大技术迁移到离散扩散中，以改进训练和采样。首先，我们引入了一种由高斯过程指导的课程学习策略，通过减少方差将训练速度提高了一倍。采用课程学习训练的模型在7个基准测试中的3个上超越了自回归模型的零样本困惑度。其次，我们提出了离散一致性蒸馏，将连续设置中的一致性蒸馏技术适配到离散场景中。该算法通过将采样速度提高两个数量级，实现了扩散语言模型中的少步生成。代码和模型检查点可在项目页面获取：http://s-sahoo.github.io/duo

</details>


### [235] [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)
**中文标题：通过可解释性在现实场景中稳健提升大型语言模型的公平性**

*Adam Karvonen,Samuel Marks*

Main category: cs.LG

TL;DR: The paper reveals that simple anti-bias prompts fail in realistic hiring scenarios, introduces internal bias mitigation to robustly reduce biases in LLMs, and shows its effectiveness across multiple models.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in high-stakes hiring, but existing bias mitigation methods fail in realistic settings, necessitating a more robust approach.

Method: The study identifies and neutralizes sensitive attribute directions in model activations using internal bias mitigation, applying affine concept editing at inference time.

Result: Internal bias mitigation reduces biases to very low levels (typically under 1%) across commercial and open-source models while maintaining performance.

Conclusion: Practitioners should adopt realistic evaluations and internal mitigation strategies for equitable LLM deployment in hiring.

摘要: 大型语言模型（LLMs）越来越多地用于高风险的招聘应用，直接影响人们的职业和生活。虽然先前研究表明简单的反偏见提示可以在受控评估中消除人口统计偏见，但我们发现这些方法在引入现实情境细节时会失效。我们通过内部偏见缓解来解决这些问题：通过识别并中和模型激活中的敏感属性方向，我们在所有测试场景中实现了稳健的偏见减少。在领先的商业模型（GPT-4o、Claude 4 Sonnet、Gemini 2.5 Flash）和开源模型（Gemma-2 27B、Gemma-3、Mistral-24B）中，我们发现添加现实情境（如公司名称、公开职业页面的文化描述和选择性招聘限制）会引发显著的种族和性别偏见（面试率差异高达12%）。这些偏见在所有测试模型和场景中一致表现为偏向黑人候选人和女性候选人。此外，模型可以从细微线索（如大学背景）推断人口统计信息并产生偏见，而这些偏见甚至在检查模型的思维链推理时也不可见。为解决这些局限性，我们的内部偏见缓解方法识别种族和性别相关方向，并在推理时应用仿射概念编辑。尽管使用简单合成数据集的方向，干预方法仍能稳健地推广，将偏见持续降至极低水平（通常低于1%，始终低于2.5%），同时基本保持模型性能。我们的研究结果表明，部署LLMs进行招聘的从业者应采用更现实的评估方法，并考虑内部缓解策略以实现公平结果。

</details>


### [236] [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/abs/2506.10946)
**中文标题：GUARD：基于数据归因的大语言模型引导性遗忘与保留**

*Evelyn Ma,Duo Zhou,Peizhi Niu,Huiting Zhou,Huan Zhang,Olgica Milenkovic,S. Rasoul Etesami*

Main category: cs.LG

TL;DR: GUARD is a novel framework for guided unlearning in large language models (LLMs) that uses data attribution to mitigate unintended forgetting while preserving valuable information. It introduces a lightweight proxy metric and adaptive unlearning weights, achieving significant retention improvements without compromising unlearning effectiveness.


<details>
  <summary>Details</summary>
Motivation: Unlearning in LLMs is crucial for regulatory compliance, copyright protection, and privacy, but existing methods often degrade model utility by inadvertently removing valuable data. The paper addresses this by exploring data-level factors to improve retention during unlearning.

Method: GUARD introduces a lightweight proxy data attribution metric to quantify alignment between forget and retain sets. It then designs an unlearning objective with adaptive, nonuniform weights inversely proportional to proxy scores, reallocating unlearning power to minimize retention loss.

Result: GUARD significantly enhances retention while maintaining unlearning performance, reducing utility sacrifice by up to 194.92% in terms of Truth Ratio when forgetting 10% of training data, as demonstrated on the TOFU benchmark.

Conclusion: GUARD provides a computationally efficient and effective solution for LLM unlearning, balancing retention and forgetting through data attribution and adaptive weighting, with rigorous theoretical guarantees and empirical validation.

摘要: 由于法规遵从、版权保护和隐私问题，大语言模型（LLM）的遗忘变得越来越重要。然而，LLM遗忘的一个关键挑战是无意遗忘，即删除特定数据时无意中损害了模型的效用及其对有价值、期望信息的保留。尽管先前的工作主要集中在架构创新上，但数据级因素对遗忘性能的影响仍未得到充分探索。因此，现有方法在遗忘高影响力数据时往往导致保留性能下降。为解决这一问题，我们提出了GUARD——一种基于数据归因的引导性遗忘与保留框架。GUARD的核心是一种专为LLM遗忘设计的轻量级代理数据归因指标，用于量化遗忘集与保留集之间的“对齐”关系，同时保持计算效率。在此基础上，我们设计了一种新颖的遗忘目标，为样本分配自适应、非均匀的遗忘权重，与代理归因分数成反比。通过这种遗忘能力的重新分配，GUARD减轻了保留中的无意损失。我们提供了严格的理论保证，证明GUARD在保持与先前方法相当的遗忘指标的同时，显著提高了保留性能。在TOFU基准测试中，针对多种LLM架构的广泛实验表明，GUARD在确保有效遗忘的同时，显著改善了效用保留。值得注意的是，在遗忘10%训练数据时，GUARD将保留集的效用损失（以Truth Ratio衡量）降低了高达194.92%。

</details>


### [237] [PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation](https://arxiv.org/abs/2506.10351)
**中文标题：PhysioWave：一种用于生理信号表示的多尺度小波变换器**

*Yanlong Chen,Mattia Orlandi,Pierangelo Maria Rapa,Simone Benatti,Luca Benini,Yawei Li*

Main category: cs.LG

TL;DR: The paper introduces PhysioWave, a wavelet-based approach for analyzing physiological signals, addressing noise and non-stationarity. It includes pretrained models for EMG and ECG, and a multi-modal framework integrating EEG, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Physiological signals are often noisy and non-stationary, making traditional analysis methods ineffective. The paper aims to develop a robust wavelet-based approach to capture multi-scale features and improve signal representation.

Method: The method involves a wavelet-based architecture for multi-scale time-frequency analysis, pretrained models for EMG and ECG, and a unified multi-modal framework integrating EEG with learnable weighted fusion.

Result: The approach achieves superior performance in downstream tasks, outperforming existing methods in handling low SNR, inter-subject variability, and device mismatch.

Conclusion: The proposed wavelet-based architecture and multi-modal design advance physiological signal analysis, with potential applications in health monitoring and diagnostics.

摘要: 生理信号常受到运动伪影、基线漂移和其他低信噪比干扰的影响，这对分析提出了重大挑战。此外，这些信号表现出强烈的非平稳性，具有尖锐的峰值和持续演变的突变，使得传统的时域或滤波方法难以表示。为解决这些问题，本文提出了一种基于小波的生理信号分析方法，旨在捕捉各种生理信号中的多尺度时频特征。利用这一技术，首次引入了针对EMG和ECG的两个大规模预训练模型，在下游任务中实现了卓越性能并设定了新的基准。此外，通过整合预训练的EEG模型，构建了一个统一的多模态框架，其中每个模态通过其专用分支引导并通过可学习的加权融合进行融合。这一设计有效地解决了低信噪比、高受试者间变异性和设备不匹配等挑战，在多模态任务中优于现有方法。所提出的小波架构为多样生理信号的分析奠定了坚实基础，而多模态设计则指向下一代生理信号处理，对可穿戴健康监测、临床诊断和更广泛的生物医学应用具有潜在影响。

</details>


### [238] [Build the web for agents, not agents for the web](https://arxiv.org/abs/2506.10953)
**中文标题：为代理构建网络，而非为网络构建代理**

*Xing Han Lù,Gaurav Kamath,Marius Mosbach,Siva Reddy*

Main category: cs.LG

TL;DR: The paper proposes a shift from adapting AI web agents to human-designed interfaces to creating Agentic Web Interfaces (AWI) optimized for AI capabilities, outlining six design principles for safety, efficiency, and standardization.


<details>
  <summary>Details</summary>
Motivation: Current web agents struggle with human-designed interfaces, leading to inefficiencies and limitations. The paper advocates for a new paradigm where interfaces are specifically designed for AI agents to improve performance and reliability.

Method: Introduces the concept of Agentic Web Interface (AWI) and establishes six guiding principles for its design, focusing on safety, efficiency, and standardization.

Result: The proposed AWI framework aims to overcome limitations of existing interfaces, enabling more efficient and transparent web agent interactions.

Conclusion: A shift to agent-centric web interfaces is necessary for advancing web agent capabilities, requiring collaboration within the ML community.

摘要: 近年来，大型语言模型（LLMs）及其多模态对应模型的进步引发了人们对开发网络代理——能够在网络环境中自主导航并完成任务的人工智能系统——的极大兴趣。尽管在自动化复杂网络交互方面具有巨大潜力，但由于人类设计的界面与LLM能力之间的根本性不匹配，当前方法面临重大挑战。现有方法在处理庞大的DOM树、依赖附加信息的截图或通过API交互完全绕过用户界面时，均难以应对网络输入的固有复杂性。本立场文件主张网络代理研究的范式转变：与其迫使网络代理适应为人类设计的界面，不如开发一种专门为代理能力优化的新交互范式。为此，我们引入了代理网络界面（AWI）的概念，这是一种专为代理导航网站而设计的界面。我们确立了AWI设计的六项指导原则，强调安全性、效率和标准化，以兼顾所有主要利益相关者的利益。这一重构旨在克服现有界面的根本性限制，为更高效、可靠和透明的网络代理设计铺平道路，这将是更广泛的机器学习社区共同参与的合作努力。

</details>


### [239] [Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation](https://arxiv.org/abs/2506.10403)
**中文标题：是时候废除LLM作为评判者：程序是评估的未来**

*Tzu-Heng Huang,Harit Vishwakarma,Frederic Sala*

Main category: cs.LG

TL;DR: The paper proposes PAJAMA, a program-based alternative to using LLMs as judges for evaluating model responses, addressing high costs, biases, and inflexibility by synthesizing executable programs for local use.


<details>
  <summary>Details</summary>
Motivation: Current use of LLMs as judges for evaluating responses is costly, unreliable, inflexible, and biased, necessitating a more efficient and adaptable solution.

Method: Introduces PAJAMA, which uses LLMs to synthesize executable judging programs that can be stored and run locally, offering cost savings, interpretability, and adaptability.

Result: PAJAMA improves judgment consistency by 15.83%, reduces biased responses by 23.7%, and outperforms LLM-as-a-judge on benchmarks like CHAT-HARD, with significantly lower costs.

Conclusion: Program-based judges like PAJAMA provide a superior, cost-effective, and bias-mitigated alternative to traditional LLM-as-a-judge methods.

摘要: 大型语言模型（LLM）被广泛用于评估LLM生成和回答的质量，但这带来了重大挑战：高昂的API成本、不确定的可靠性、不灵活的流程以及固有的偏见。为解决这些问题，我们提出了PAJAMA（Program-As-a-Judge for Automated Model Assessment），这是一种新的替代方案，它使用LLM合成可执行的评判程序，而非直接评分回答。这些合成的程序可以本地存储和运行，成本降低数个数量级，同时提供可解释和可审计的评判逻辑，且易于调整。基于程序的评判者减少了偏见，与基于Qwen2.5-14B的LLM评判者相比，评判一致性提高了15.83%，偏见回答平均减少了23.7%。当程序评判被提炼为模型时，PAJAMA在RewardBench的CHAT-HARD子集上优于LLM评判者，在Prometheus和JudgeLM数据集上分别超出指标2.19%和8.67%，且成本降低了三个数量级。

</details>


### [240] [Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors](https://arxiv.org/abs/2506.10146)
**中文标题：平衡双曲嵌入是自然的分布外检测器**

*Tejaswi Kasarla,Max van Spengler,Pascal Mettes*

Main category: cs.LG

TL;DR: The paper proposes Balanced Hyperbolic Learning, a method using hierarchical hyperbolic embeddings to improve out-of-distribution detection, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of out-of-distribution recognition in deep learning by leveraging hyperbolic embeddings for better discrimination between in- and out-of-distribution samples.

Method: The authors introduce Balanced Hyperbolic Learning, which optimizes hierarchical distortion and balance in hyperbolic class embeddings, using them as prototypes for classification and extending scoring functions to hyperbolic space.

Result: Empirical evaluations show the method outperforms existing out-of-distribution approaches, other hyperbolic methods, and state-of-the-art contrastive methods across 13 datasets.

Conclusion: Hierarchical hyperbolic embeddings are effective for out-of-distribution detection and enable hierarchical generalization.

摘要: 分布外识别是深度学习中的一个重要且经过深入研究的问题，其目标是过滤掉不属于网络训练分布的样本。本文的结论很简单：良好的分层双曲嵌入更适合区分分布内和分布外样本。我们引入了平衡双曲学习，提出了一种双曲类嵌入算法，联合优化了分层失真和浅层与深层子层次之间的平衡。然后，我们将这些类嵌入用作双曲原型，对分布内数据进行分类。我们还概述了如何将现有的分布外评分函数推广到双曲原型上。在13个数据集和13个评分函数上的实证评估表明，我们的双曲嵌入在相同数据和相同骨干网络训练下优于现有的分布外方法。我们还表明，我们的双曲嵌入优于其他双曲方法，击败了最先进的对比方法，并天然支持分层分布外泛化。

</details>


### [241] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/abs/2506.10177)
**中文标题：基于扩散生成模型的确定性采样中的几何规律性**

*Defang Chen,Zhenyu Zhou,Can Wang,Siwei Lyu*

Main category: cs.LG

TL;DR: The paper uncovers a geometric regularity in deterministic sampling trajectories of diffusion-based generative models, showing they lie in low-dimensional subspaces with a consistent 'boomerang' shape. It proposes a dynamic programming-based scheme to optimize sampling time schedules, improving image generation with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the geometric properties of deterministic sampling trajectories in diffusion-based generative models, which are currently not well-characterized, and leverage these insights to enhance sampling efficiency.

Method: The authors analyze the deterministic sampling dynamics of diffusion models, revealing low-dimensional subspace properties and a consistent trajectory shape. They propose a dynamic programming-based scheme to align sampling time schedules with the discovered trajectory structure.

Result: The discovered trajectory regularity allows for optimized sampling schedules, leading to superior image generation performance, especially with fewer function evaluations (5-10).

Conclusion: The findings highlight a fundamental geometric regularity in diffusion model sampling, enabling practical improvements in sampling efficiency and performance with minimal modifications to existing methods.

摘要: 基于扩散的生成模型利用随机微分方程（SDEs）及其等效的概率流常微分方程（ODEs），在复杂的高维数据分布与易于处理的先验分布之间建立平滑转换。本文揭示了确定性采样动力学中一个显著的几何规律性：每条模拟采样轨迹都位于一个极低维的子空间中，且所有轨迹无论模型架构、应用条件或生成内容如何，都呈现几乎相同的“回旋镖”形状。我们描述了这些轨迹的几个有趣特性，特别是在基于核估计数据建模的闭式解下。我们还通过提出一种基于动态规划的方案，将采样时间表与底层轨迹结构更好地对齐，展示了所发现轨迹规律性的实际应用。这一简单策略仅需对现有基于ODE的数值求解器进行最小修改，计算开销可忽略不计，并在仅需5~10次函数评估的区域中实现了更优的图像生成性能。

</details>


### [242] [Size-adaptive Hypothesis Testing for Fairness](https://arxiv.org/abs/2506.10586)
**中文标题：公平性的大小自适应假设检验**

*Antonio Ferrara,Francesco Cozzi,Alan Perotti,André Panisson,Francesco Bonchi*

Main category: cs.LG

TL;DR: The paper introduces a size-adaptive hypothesis-testing framework for fairness assessment, addressing statistical challenges in small and large demographic subgroups with Bayesian and Wald test approaches.


<details>
  <summary>Details</summary>
Motivation: Current fairness assessments are statistically brittle, ignoring sampling error and treating small subgroups the same as large ones, especially problematic in intersectional analyses with sparse data.

Method: A unified framework combining Wald tests for large subgroups (using Central-Limit results) and Bayesian Dirichlet-multinomial estimators for small subgroups, with Monte-Carlo credible intervals.

Result: The method provides interpretable, statistically rigorous decisions under varying data availability, validated on benchmark datasets.

Conclusion: The proposed framework offers a robust, evidence-based approach to fairness assessment, adaptable to subgroup sizes and intersectionality.

摘要: 确定算法决策系统是否歧视特定人口群体通常涉及将公平性指标的单一估计值与预定义阈值进行比较。这种做法在统计上是脆弱的：它忽略了抽样误差，并将小人口子群体与大群体同等对待。这一问题在交叉分析中尤为严重，其中多个敏感属性被联合考虑，导致更多更小的群体。随着这些群体变得更加细分，代表它们的数据变得过于稀疏，无法进行可靠估计，公平性指标产生过宽的置信区间，无法对潜在的不公平待遇得出有意义的结论。
  本文介绍了一种统一的大小自适应假设检验框架，将公平性评估转化为基于证据的统计决策。我们的贡献有两点。(i) 对于足够大的子群体，我们证明了统计奇偶差异的中心极限定理，从而得出解析置信区间和Wald检验，其I类（假阳性）误差保证在水平α。(ii) 对于长尾的小交叉群体，我们推导了一个完全贝叶斯的Dirichlet-多项式估计器；蒙特卡洛可信区间针对任何样本量进行校准，并随着数据的增加自然收敛于Wald区间。我们在基准数据集上实证验证了我们的方法，展示了我们的检验如何在数据可用性和交叉性不同的情况下提供可解释、统计严格的决策。

</details>


### [243] [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://arxiv.org/abs/2506.10629)
**中文标题：基于技能的迁移适应：信息几何、解耦与无监督强化学习的新目标**

*Yucheng Yang,Tianyi Zhou,Qiang He,Lei Han,Mykola Pechenizkiy,Meng Fang*

Main category: cs.LG

TL;DR: The paper analyzes the limitations of Mutual Information Skill Learning (MISL) in unsupervised reinforcement learning (URL) and proposes a new disentanglement metric (LSEPIN) and a Wasserstein distance-based objective (WSEP) to improve skill diversity and separability for better downstream task adaptation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of theoretical analysis in MISL for URL, particularly regarding how well learned skills can initialize downstream tasks, and proposes new methods to ensure skill diversity and separability.

Method: The authors introduce a disentanglement metric (LSEPIN) and an information-geometric analysis connecting it to task adaptation cost. They replace KL divergence with Wasserstein distance for better geometric properties, leading to a new objective (WSEP) and an algorithm (PWSEP).

Result: The proposed WSEP objective and PWSEP algorithm theoretically improve downstream task adaptation and discover more optimal initial policies compared to MISL.

Conclusion: The paper concludes that the proposed methods (LSEPIN, WSEP, and PWSEP) enhance skill learning for URL by ensuring better diversity and separability, leading to improved downstream task performance.

摘要: 无监督强化学习（URL）旨在为未见的下游任务学习通用技能。互信息技能学习（MISL）通过最大化状态与技能之间的互信息来解决URL问题，但缺乏足够的理论分析，例如其学习到的技能如何初始化下游任务的策略。本文的新理论分析表明，学习技能的多样性和可分离性对下游任务适应至关重要，但MISL并不一定保证这些特性。为了补充MISL，我们提出了一种新的解耦度量LSEPIN。此外，我们建立了LSEPIN与下游任务适应成本之间的信息几何联系。为了更好的几何性质，我们研究了一种新策略，用Wasserstein距离替代信息几何中的KL散度。我们将几何分析扩展到这一策略，从而提出了一个新的技能学习目标WSEP。理论证明WSEP有助于下游任务适应，并且比MISL能够发现更多的下游任务初始策略。最后，我们提出了另一种基于Wasserstein距离的算法PWSEP，理论上可以发现所有最优初始策略。

</details>


### [244] [Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs](https://arxiv.org/abs/2506.10630)
**中文标题：时间序列预测作为推理：一种基于强化大语言模型的慢思考方法**

*Yucong Luo,Yitong Zhou,Mingyue Cheng,Jiahao Wang,Daoyu Wang,Tingyue Pan,Jintao Zhang*

Main category: cs.LG

TL;DR: The paper proposes Time-R1, a two-stage reinforcement fine-tuning framework for LLMs to enhance multi-step reasoning in time series forecasting, addressing limitations of fast-thinking methods and prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods lack explicit reasoning processes, while LLMs with slow-thinking capabilities offer potential but face limitations like high computational costs and privacy risks. The paper aims to train LLMs for better time series reasoning.

Method: Time-R1 uses a two-stage approach: supervised fine-tuning for adaptation and reinforcement learning for generalization, with a multi-objective reward and GRIP for optimizing reasoning paths.

Result: Experiments show Time-R1 significantly improves forecasting performance across diverse datasets.

Conclusion: Time-R1 effectively enhances LLMs' reasoning for time series forecasting, outperforming traditional methods.

摘要: 为了推动时间序列预测（TSF）的发展，已有多种方法被提出以提高预测准确性，从统计技术发展到数据驱动的深度学习架构。尽管这些方法有效，但大多数仍遵循快速思考范式——依赖于提取历史模式并将其映射到未来值作为核心建模理念，缺乏包含中间时间序列推理的明确思考过程。同时，新兴的慢思考大语言模型（如OpenAI-o1）显示出卓越的多步推理能力，为解决这些问题提供了另一种途径。然而，仅靠提示工程存在一些局限性，包括高计算成本、隐私风险以及对深入领域特定时间序列推理的有限能力。为解决这些局限性，一种更有前景的方法是训练大语言模型以发展慢思考能力并获取强大的时间序列推理技能。为此，我们提出了Time-R1，一个两阶段强化微调框架，旨在增强大语言模型在时间序列预测中的多步推理能力。具体而言，第一阶段进行监督微调以实现预热适应，而第二阶段采用强化学习以提高模型的泛化能力。特别是，我们设计了一个专门用于时间序列预测的细粒度多目标奖励，并引入了GRIP（基于组的相对重要性策略优化），利用非均匀采样进一步鼓励和优化模型对有效推理路径的探索。实验表明，Time-R1在多样数据集上显著提高了预测性能。

</details>


### [245] [Data Shifts Hurt CoT: A Theoretical Study](https://arxiv.org/abs/2506.10647)
**中文标题：数据偏移损害思维链：一项理论研究**

*Lang Yin,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.LG

TL;DR: The paper studies how data shifts (distribution shifts and data poisoning) harm the effectiveness of Chain of Thought (CoT) in solving the $k$-parity problem, revealing that CoT can lead to worse performance than direct prediction.


<details>
  <summary>Details</summary>
Motivation: Previous works on CoT rely on ideal assumptions (identical training/testing distributions and clean data), which don't hold in real-world scenarios. The paper aims to rigorously study the harm caused by data shifts.

Method: Focuses on the $k$-parity problem, analyzing the joint impact of distribution shifts and data poisoning on models trained with CoT decomposition.

Result: CoT can degrade performance on learning parity compared to direct prediction, and the paper provides a mechanistic explanation for this phenomenon.

Conclusion: Data shifts significantly harm CoT's effectiveness, highlighting the need for robustness in real-world applications.

摘要: 思维链（CoT）已被应用于各种大型语言模型（LLM），并被证明能有效提高输出质量。近期研究表明，Transformer在表达能力上存在绝对上限，因此无法解决许多计算难题。然而，借助CoT，Transformer被证明能有效解决一些难题，例如$k$-奇偶问题。但这些研究依赖于两个关键假设：（1）训练和测试分布相同，（2）训练数据无污染且推理步骤正确。然而，现实中这些假设并不总是成立。尽管数据偏移的风险已引起关注，但据我们所知，本研究首次严格分析了此类偏移的具体危害。聚焦于$k$-奇偶问题，本文研究了两种数据偏移（分布偏移和数据投毒）对通过成熟CoT分解训练的模型质量的联合影响。除了揭示一个令人惊讶的现象——CoT在学习奇偶问题上的表现比直接预测更差外，我们的技术结果还对此影响的机制原因提供了严格而全面的解释。

</details>


### [246] [Saturation Self-Organizing Map](https://arxiv.org/abs/2506.10680)
**中文标题：饱和自组织映射**

*Igor Urbanik,Paweł Gajewski*

Main category: cs.LG

TL;DR: The paper introduces Saturation Self-Organizing Maps (SatSOM), an extension of SOMs designed to mitigate catastrophic forgetting in continual learning by gradually reducing learning rates and neighborhood radii for well-trained neurons.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of catastrophic forgetting in Self-Organizing Maps (SOMs) during continual learning, which limits their effectiveness in sequential tasks.

Method: SatSOM incorporates a saturation mechanism that reduces the learning rate and neighborhood radius of neurons as they accumulate information, thereby freezing well-trained neurons and redirecting learning to underutilized areas.

Result: The proposed SatSOM improves knowledge retention in continual learning scenarios by effectively managing neuron utilization and learning dynamics.

Conclusion: SatSOM successfully extends SOMs to better handle continual learning by preventing catastrophic forgetting through its saturation mechanism.

摘要: 持续学习对神经系统提出了根本性挑战，因为它们在面对顺序任务时常常遭受灾难性遗忘。自组织映射（SOMs）尽管具有可解释性和高效性，但也无法避免这一问题。本文提出了饱和自组织映射（SatSOM），这是SOMs的一种扩展，旨在改善持续学习场景中的知识保留。SatSOM引入了一种新颖的饱和机制，随着神经元积累信息，逐渐降低其学习率和邻域半径。这有效地冻结了训练良好的神经元，并将学习重定向到映射中未充分利用的区域。

</details>


### [247] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/abs/2506.10632)
**中文标题：生成模型中潜在空间的Hessian几何**

*Alexander Lobashev,Dmitry Guskov,Maria Larchenko,Mikhail Tamm*

Main category: cs.LG

TL;DR: The paper introduces a method to analyze latent space geometry in generative models by reconstructing the Fisher information metric, revealing fractal phase transitions and nonlinear geodesic behavior at phase boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the complex geometry of latent spaces in generative models, particularly focusing on phase transitions and thermodynamic properties, which are not well-explored in existing literature.

Method: The method approximates the posterior distribution of latent variables to learn the log-partition function, defining the Fisher metric for exponential families. It is validated on Ising and TASEP models and applied to diffusion models.

Result: The method outperforms baselines in reconstructing thermodynamic quantities and reveals fractal phase transitions in diffusion models, with nonlinear geodesics at phase boundaries.

Conclusion: The findings provide new insights into the latent space structure of diffusion models, highlighting connections to phase transitions and nonlinear behavior.

摘要: 本文提出了一种分析生成模型潜在空间几何的新方法，包括统计物理模型和扩散模型，通过重建Fisher信息度量。该方法近似给定生成样本的潜在变量的后验分布，并利用其学习对数配分函数，从而定义指数族的Fisher度量。提供了理论收敛保证，并在Ising和TASEP模型上验证了该方法，在重建热力学量方面优于现有基线。应用于扩散模型时，该方法揭示了潜在空间中相变的分形结构，表现为Fisher度量的突变。我们证明，虽然测地线插值在单个相内近似线性，但在相边界处这种线性关系被破坏，扩散模型表现出关于潜在空间的发散Lipschitz常数。这些发现为扩散模型潜在空间的复杂结构及其与相变等现象的联系提供了新见解。源代码可在https://github.com/alobashev/hessian-geometry-of-diffusion-models获取。

</details>


### [248] [ConTextTab: A Semantics-Aware Tabular In-Context Learner](https://arxiv.org/abs/2506.10707)
**中文标题：ConTextTab：一种语义感知的表格上下文学习模型**

*Marco Spinaci,Marek Polewczyk,Maximilian Schambach,Sam Thelin*

Main category: cs.LG

TL;DR: ConTextTab combines semantic understanding with tabular in-context learning, outperforming SOTA models by leveraging real-world data and specialized embeddings.


<details>
  <summary>Details</summary>
Motivation: Current tabular ICL models either lack semantic understanding (trained on synthetic data) or struggle with context limitations (LLM-based). ConTextTab aims to merge the strengths of both approaches.

Method: ConTextTab integrates semantic understanding into a table-native ICL framework using specialized embeddings for different data modalities and training on large-scale real-world tabular data.

Result: The model achieves competitive performance across benchmarks and sets a new SOTA on the semantically rich CARTE benchmark.

Conclusion: ConTextTab successfully bridges the gap between semantic-rich and table-native ICL models, demonstrating superior performance on diverse tasks.

摘要: 表格上下文学习（ICL）最近在多项表格预测任务中取得了最先进的性能。此前仅适用于小型表格的分类问题，而TabPFN和TabICL等最新进展已将其扩展到更大的数据集。尽管当前表格原生ICL架构在结构上高效且适应表格数据结构，但由于仅基于合成数据训练，未能充分利用真实表格数据中的丰富语义和世界知识。另一方面，基于预训练大型语言模型（如TabuLa-8B）的表格ICL模型虽然具备深度语义理解和世界知识，但由于固有架构限制，仅能利用少量上下文。为了结合两者的优势，我们提出了ConTextTab，将语义理解和对齐整合到表格原生ICL框架中。通过为不同数据模态使用专用嵌入并在大规模真实表格数据上训练，我们的模型在广泛基准测试中与SOTA竞争，同时在语义丰富的CARTE基准上树立了新标准。

</details>


### [249] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/abs/2506.10955)
**中文标题：Error**

*Aayush Karan,Kulin Shah,Sitan Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

摘要: Error

</details>


### [250] [Efficiency Robustness of Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.10831)
**中文标题：动态深度学习系统的效率鲁棒性**

*Ravishka Rathnasuriya,Tingxi Li,Zexin Xu,Zihe Song,Mirazul Haque,Simin Chen,Wei Yang*

Main category: cs.LG

TL;DR: The paper explores efficiency robustness in Dynamic Deep Learning Systems (DDLSs), categorizing efficiency attacks and analyzing adversarial strategies, while highlighting limitations of current defenses and the need for new mitigation approaches.


<details>
  <summary>Details</summary>
Motivation: DDLSs improve efficiency by adapting inference computation, but this dynamic behavior introduces new attack surfaces, particularly efficiency adversarial attacks that degrade system performance. The paper aims to systematically study these attacks and their impact.

Method: The paper presents a comprehensive taxonomy of efficiency attacks on DDLSs, categorizing them into three dynamic behaviors: attacks on dynamic computations per inference, dynamic inference iterations, and dynamic output production. It evaluates adversarial strategies and existing defenses.

Result: The study identifies key challenges in securing DDLSs against efficiency attacks and demonstrates the limitations of current defense mechanisms, emphasizing the need for novel mitigation strategies.

Conclusion: Future adaptive DDLSs require new defense approaches to counter efficiency attacks, as existing mechanisms are insufficient.

摘要: 深度学习系统（DLSs）越来越多地部署在实时应用中，包括资源受限的环境（如移动和物联网设备）。为了解决效率挑战，动态深度学习系统（DDLSs）根据输入复杂性调整推理计算，减少开销。尽管这种动态行为提高了效率，但也引入了新的攻击面。特别是效率对抗攻击利用这些动态机制降低系统性能。本文系统地探讨了DDLSs的效率鲁棒性，首次提出了效率攻击的综合分类法。我们基于三种动态行为对这些攻击进行分类：（i）针对每次推理动态计算的攻击，（ii）针对动态推理迭代的攻击，以及（iii）针对下游任务动态输出生成的攻击。通过深入评估，我们分析了针对DDLSs效率的对抗策略，并确定了保护这些系统的关键挑战。此外，我们研究了现有的防御机制，展示了它们对日益流行的效率攻击的局限性，以及需要新的缓解策略来保护未来的自适应DDLSs。

</details>


### [251] [Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods](https://arxiv.org/abs/2506.10959)
**中文标题：理解结构化流形上的上下文学习：将注意力机制与核方法联系起来**

*Zhaiming Shen,Alexander Hsu,Rongjie Lai,Wenjing Liao*

Main category: cs.LG

TL;DR: The paper theoretically analyzes in-context learning (ICL) for regression of Hölder functions on manifolds, linking attention mechanisms to kernel methods and deriving generalization error bounds.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of ICL, especially for structured geometric data, by exploring its connection to kernel methods and manifolds.

Method: Establishes a connection between attention mechanisms and kernel methods, then derives generalization error bounds for ICL on manifolds, focusing on prompt length and training tasks.

Result: Transformers achieve minimax regression rates for Hölder functions on manifolds, with error scaling exponentially with intrinsic manifold dimension, not ambient space.

Conclusion: The study provides foundational insights into the role of geometry in ICL and tools for analyzing nonlinear ICL models.

摘要: 尽管上下文学习（ICL）在自然语言和视觉领域取得了显著成功，但其理论理解——尤其是在结构化几何数据背景下——仍未得到探索。本研究首次对ICL在流形上Hölder函数回归的理论进行了探讨。通过建立注意力机制与经典核方法之间的新联系，我们推导出了基于提示长度和训练任务数量的泛化误差界。当观察到足够数量的训练任务时，变换器能够实现流形上Hölder函数的最小最大回归率，其误差随流形的内蕴维度而非环境空间维度呈指数级变化。我们的结果还揭示了泛化误差如何随训练任务数量变化，为变换器作为上下文算法学习器的复杂性提供了新的见解。这些发现为理解几何在ICL中的作用提供了基础性见解，并为研究非线性模型的ICL提供了新工具。

</details>


### [252] [Farseer: A Refined Scaling Law in Large Language Models](https://arxiv.org/abs/2506.10972)
**中文标题：Farseer：大型语言模型中的精细化缩放定律**

*Houyi Li,Wenzhen Zheng,Qiufeng Wang,Zhenyu Ding,Haoying Wang,Zili Wang,Shijie Xuyang,Ning Ding,Shuigeng Zhou,Xiangyu Zhang,Daxin Jiang*

Main category: cs.LG

TL;DR: Farseer introduces a refined scaling law for Large Language Models (LLMs) that improves predictive accuracy and reduces extrapolation error by 433%, enabling reliable evaluation of training strategies and optimal compute allocation.


<details>
  <summary>Details</summary>
Motivation: The high cost of training LLMs creates a scaling gap where small-scale insights fail to transfer to production systems, hindering innovation. Farseer aims to bridge this gap with a more accurate scaling law.

Method: Farseer constructs a model loss surface $L(N,D)$ to better fit empirical data, outperforming prior laws like Chinchilla's. It validates the approach by training 1,000 LLMs across diverse scales and configurations.

Result: Farseer reduces extrapolation error by 433% compared to Chinchilla's law, providing robust predictions for LLM performance across all $(N,D)$ settings and insights into optimal compute allocation.

Conclusion: Farseer's refined scaling law enhances predictive accuracy and generalizability, enabling confident extrapolation from small-scale studies to large-scale LLM training. All data and models are open-sourced to support further research.

摘要: 训练大型语言模型（LLM）的成本极高，这导致了一个关键的缩放鸿沟：小规模实验的见解往往无法转移到资源密集的生产系统中，从而阻碍了高效创新。为了弥合这一鸿沟，我们提出了Farseer，一种新颖且精细化的缩放定律，能够在不同规模下提供更高的预测准确性。通过系统构建模型损失表面$L(N,D)$，Farseer比之前的定律（如Chinchilla定律）显著更好地拟合了经验数据。我们的方法产生了准确、稳健且高度可泛化的预测，展示了出色的外推能力，将Chinchilla定律的外推误差降低了433%。这使得我们能够可靠地评估所有$(N,D)$设置下的竞争性训练策略，从而自信地将小规模消融研究的结论外推以预测大规模性能。此外，Farseer为计算资源的最优分配提供了新的见解，更好地反映了现代LLM训练的复杂需求。为了验证我们的方法，我们训练了约1,000个不同规模和配置的LLM，消耗了约300万NVIDIA H100 GPU小时。我们将所有模型、数据、结果和日志全面开源在https://github.com/Farseer-Scaling-Law/Farseer，以促进进一步研究。

</details>


### [253] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/abs/2506.10973)
**中文标题：将神经网络架构扩展到函数空间以进行算子学习的原理性方法**

*Julius Berner,Miguel Liu-Schiaffini,Jean Kossaifi,Valentin Duruisseaux,Boris Bonev,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: The paper proposes a principled method to extend neural networks to function spaces for operator learning, enabling applications in scientific problems like PDEs. It provides a recipe for converting existing neural architectures into neural operators with minimal modifications.


<details>
  <summary>Details</summary>
Motivation: Deep learning has excelled in finite-dimensional spaces (e.g., computer vision), but scientific problems often involve infinite-dimensional function spaces (e.g., PDEs). Neural operators bridge this gap, but prior work lacks direct extensions of existing neural architectures. This paper aims to provide practical principles for such extensions.

Method: The paper identifies key principles for constructing mappings between function spaces and proposes a recipe to convert popular neural architectures (e.g., CNNs, Transformers) into neural operators with minimal changes.

Result: The proposed method enables neural operators to leverage existing neural architectures, making them practical for scientific applications like learning PDE solution operators.

Conclusion: The paper provides a practical guide for extending neural networks to function spaces, facilitating the use of neural operators in scientific problems.

摘要: 许多科学问题（如连续时间动力系统和偏微分方程描述的问题）自然地在函数空间中表述。虽然函数空间通常是无限维的，但深度学习主要在计算机视觉和自然语言处理等有限维空间映射的应用中取得进展。这种数据性质的根本差异限制了神经网络在科学应用中的成功程度。神经算子是一种将神经网络推广到函数空间映射的原理性方法，为深度学习在科学问题中的变革性影响提供了途径。例如，神经算子可以学习整个偏微分方程类的解算子，如具有不同边界条件、系数函数和几何形状的物理系统。深度学习成功的一个关键因素是通过大量实证测试对神经架构进行精心设计。将这些神经架构转化为神经算子，使算子学习能够享受同样的实证优化。然而，先前的神经算子架构通常作为独立模型引入，并未直接作为现有神经网络架构的扩展。本文识别并提炼了构建无限维函数空间之间映射的实用实现的关键原则。利用这些原则，我们提出了一种将几种流行的神经架构转换为神经算子的方法，只需最小修改。本文旨在指导实践者完成这一过程，并详细说明使神经算子实际工作的步骤。我们的代码可在 https://github.com/neuraloperator/NNs-to-NOs 找到。

</details>


### [254] [Rethinking Losses for Diffusion Bridge Samplers](https://arxiv.org/abs/2506.10982)
**中文标题：重新思考扩散桥采样器的损失函数**

*Sebastian Sanokowski,Lukas Gruber,Christoph Bartmann,Sepp Hochreiter,Sebastian Lehner*

Main category: cs.LG

TL;DR: The paper argues that the reverse Kullback-Leibler loss with the log-derivative trick (rKL-LD) is superior to the Log Variance (LV) loss for diffusion bridge samplers, both conceptually and empirically, requiring less hyperparameter tuning and providing more stable training.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that the LV loss, while effective in certain contexts, lacks a clear theoretical foundation for diffusion bridges and learned diffusion coefficients, unlike the rKL loss. The paper aims to address this gap and improve sampling performance.

Method: The method involves analyzing the theoretical equivalence of LV and rKL losses, highlighting their differences in diffusion bridge contexts, and empirically comparing their performance using the rKL-LD loss.

Result: Experimental results show that samplers trained with the rKL-LD loss outperform those using the LV loss, achieving better performance with less hyperparameter optimization and more stable training.

Conclusion: The conclusion advocates for the use of the rKL-LD loss in diffusion bridge samplers due to its theoretical soundness, superior performance, and practical advantages.

摘要: 扩散桥是一类有前景的深度学习方法，用于从未归一化分布中采样。近期研究表明，在使用重参数化技巧计算rKL梯度时，对数方差（LV）损失始终优于反向Kullback-Leibler（rKL）损失。然而，对于具有不可学习前向过程的扩散采样器，当结合对数导数技巧时，LV损失与rKL损失的梯度等价性在扩散桥或学习扩散系数的情况下不再成立。基于这一发现，我们认为对于扩散桥，LV损失并不像rKL损失那样可以通过数据处理不等式得到理论支持。分析表明，使用对数导数技巧的rKL损失（rKL-LD）不仅避免了这些概念问题，而且在性能上始终优于LV损失。在多个具有挑战性的基准测试中，使用rKL-LD损失训练的采样器表现更优。从实践角度看，rKL-LD需要更少的超参数优化，且训练过程更稳定。

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [255] [AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation](https://arxiv.org/abs/2506.10540)
**中文标题：AniMaker：基于MCTS驱动片段生成的多智能体自动动画叙事**

*Haoyuan Shi,Yunxin Li,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang*

Main category: cs.MA

TL;DR: AniMaker is a multi-agent framework for generating coherent storytelling animations from text, using MCTS-driven clip generation and specialized agents to ensure consistency and quality.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods struggle with coherent multi-scene storytelling due to rigid clip generation and instability, leading to disjointed narratives and pacing issues.

Method: AniMaker employs specialized agents (Director, Photography, Reviewer, Post-Production) with MCTS-Gen for efficient clip generation and AniEval for multi-shot animation evaluation.

Result: AniMaker outperforms existing methods in quality (measured by VBench and AniEval) and efficiency, improving multi-candidate generation for storytelling animations.

Conclusion: AniMaker advances AI-generated storytelling by ensuring coherence and quality, moving closer to production standards.

摘要: 尽管视频生成模型发展迅速，但生成跨多场景和多角色的连贯叙事视频仍具挑战性。现有方法通常将预生成的关键帧机械转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型的不稳定性意味着单个低质量片段会显著降低整个动画的逻辑连贯性和视觉连续性。为克服这些障碍，我们提出了AniMaker，一个多智能体框架，支持高效的多候选片段生成和叙事感知片段选择，从而仅从文本输入创建全局一致且故事连贯的动画。该框架围绕多个专用智能体构建，包括用于故事板生成的导演智能体、用于视频片段生成的摄影智能体、用于评估的评审智能体，以及用于编辑和配音的后期制作智能体。AniMaker的核心技术包括摄影智能体中的MCTS-Gen，这是一种受蒙特卡洛树搜索（MCTS）启发的策略，能智能导航候选空间以生成高质量片段并优化资源使用；以及评审智能体中的AniEval，这是首个专为多镜头动画评估设计的框架，通过考虑每个片段的前后片段背景，评估故事级一致性、动作完成度和动画特定特征。实验表明，AniMaker在VBench和我们提出的AniEval框架等流行指标上均表现出优越质量，同时显著提高了多候选生成的效率，使AI生成的叙事动画更接近生产标准。

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [256] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/abs/2506.10001)
**中文标题：基于语义通信的云-边-端协同元宇宙服务架构**

*Yuxuan Li,Sheng Jinag,Bizhu Wang*

Main category: cs.MM

TL;DR: The paper proposes a semantic communication-enabled architecture (SC-CEE-Meta) to improve metaverse services by reducing latency and enhancing image quality through cloud-edge-end collaboration.


<details>
  <summary>Details</summary>
Motivation: The metaverse faces challenges like high data transmission demands and poor wireless channel quality, leading to latency and degraded user experience.

Method: The SC-CEE-Meta architecture includes VR video semantic transmission, video synthesis, and 3D scene reconstruction modules, leveraging semantic communication to reduce data transmission and improve efficiency.

Result: The architecture reduces wireless transmission delay by 96.05% and improves image quality by 43.99% under poor channel conditions, as tested on Meta Quest Pro.

Conclusion: The SC-CEE-Meta architecture effectively addresses metaverse service challenges by optimizing data transmission and enhancing user experience through semantic communication and cloud-edge-end collaboration.

摘要: 随着技术进步和对新视听体验的追求增强，元宇宙获得了越来越多的热情。然而，由于高分辨率虚拟场景等大量数据需要在云平台和VR设备之间传输，它面临着实际障碍。具体而言，VR设备的无线传输因带宽不足而受到阻碍，导致速度和延迟问题。同时，信道质量差会导致数据错误并恶化用户体验。为解决这一问题，我们提出了基于语义通信的云-边-端协同沉浸式元宇宙服务（SC-CEE-Meta）架构，包括VR视频语义传输、视频合成和3D虚拟场景重建三个模块。通过在VR设备和边缘服务器上部署语义模块并发送关键语义信息而非专注于比特级重建，可以降低延迟、解决资源与带宽冲突，并更好地抵御信道干扰。此外，云端部署视频合成和3D场景重建预处理，而边缘设备托管3D重建渲染模块，共同提供沉浸式服务。在Meta Quest Pro上验证，SC-CEE-Meta可以在信道条件差的情况下将无线传输延迟降低96.05%，并将图像质量提升43.99%。

</details>


### [257] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
**中文标题：EQ-TAA：基于扩散的事故视频合成的等变交通事故预测**

*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: The paper proposes EQ-TAA, a method for traffic accident anticipation using a diffusion-based model to synthesize accident videos, addressing data bias and improving prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current traffic accident anticipation methods suffer from data bias and annotation challenges due to the uncertain and fast-evolving nature of traffic scenes. The paper aims to overcome these issues by generating synthetic accident videos to enhance model training.

Method: The authors introduce an Attentive Video Diffusion (AVD) model to synthesize accident video clips from normal ones, preserving content and style. EQ-TAA uses an equivariant triple loss with generated pseudo-normal and pseudo-accident clips for training.

Result: Experiments show competitive performance of AVD and EQ-TAA compared to state-of-the-art methods, validating the effectiveness of synthetic data in improving accident anticipation.

Conclusion: The proposed EQ-TAA framework, leveraging synthetic accident videos, effectively addresses data bias and improves traffic accident anticipation performance without requiring additional annotations.

摘要: 交通场景中的交通事故预测（TAA）是实现未来零死亡的一项具有挑战性的问题。当前方法通常将TAA视为需要费力标注事故持续时间的监督学习任务。然而，交通场景固有的长尾性、不确定性和快速演变特性导致事故的真实因果部分难以识别，且容易被数据偏差主导，从而产生背景混淆问题。为此，我们提出了一种注意力视频扩散（AVD）模型，通过生成行车记录仪视频中的因果部分（即从正常片段到事故片段）来合成额外的事故视频片段。AVD旨在基于事故或无事故文本提示生成因果视频帧，同时在视频生成后保留帧的风格和内容以用于TAA。这种方法可以使用从各种驾驶场景收集的数据集进行训练，无需任何额外标注。此外，AVD通过等变三重损失促进了一种等变TAA（EQ-TAA），该损失针对锚定无事故视频片段以及生成的对比伪正常和伪事故片段对。大量实验已用于评估AVD和EQ-TAA的性能，并获得了与最先进方法相比具有竞争力的性能。

</details>


### [258] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/abs/2506.10004)
**中文标题：沉浸式多媒体通信：扩展现实流媒体的最新进展**

*Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: The paper surveys state-of-the-art XR streaming, covering definitions, hardware, traffic characteristics, quality of experience factors, optimization methods, and current applications, while highlighting challenges for future development.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of XR streaming advancements, addressing its unique requirements and challenges to enhance user experience and guide future research.

Method: The paper defines XR, introduces hardware and interaction methods, analyzes traffic characteristics, explores quality of experience factors, and presents optimization techniques based on visual attention.

Result: The survey identifies key elements for improving XR streaming efficiency and user satisfaction, along with current applications and unresolved challenges.

Conclusion: XR streaming is evolving rapidly, with significant potential for revolutionizing content consumption, but requires further research to address technical and experiential challenges.

摘要: 扩展现实（XR）正在迅速发展，并有望彻底改变内容的创建和消费方式。在XR中，用户通过整合多种感官输入来形成对虚拟环境的统一感知。本文综述了XR流媒体的最新进展，重点关注多种范式。首先，我们定义了XR并介绍了各种XR头显及其多模态交互方法，以提供基础理解。接着，我们分析了XR流量的特征，以突出其独特的数据传输需求。我们还探讨了影响XR系统体验质量的因素，旨在确定提升用户满意度的关键要素。随后，我们提出了基于视觉注意力的XR流媒体优化方法，以提高效率和性能。最后，我们研究了当前的应用并强调了挑战，为XR的持续和未来发展提供见解。

</details>


### [259] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
**中文标题：通过动态双向重建实现灵活多模态输入的HER2表达预测**

*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: The paper proposes a dynamic bimodal framework for HER2 prediction in breast cancer, enabling flexible single- or dual-modality analysis with improved accuracy and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Current HER2 assessment models analyze H&E or IHC images separately, missing the clinical benefit of their combined interpretation. The complexity and cost of acquiring both modalities simultaneously limit practical use.

Method: The framework includes a dynamic branch selector, a bidirectional cross-modal GAN for feature-space reconstruction, and a hybrid training protocol. It adapts to single- or dual-modality inputs.

Result: The model improves single-modality H&E accuracy from 71.44% to 94.25%, achieves 95.09% dual-modality accuracy, and maintains 90.28% reliability with IHC-only inputs. Cross-modal reconstruction enhances F1-scores.

Conclusion: The framework democratizes precise HER2 assessment by reducing reliance on synchronized acquisition, benefiting resource-limited settings.

摘要: 目前乳腺癌的HER2评估模型主要单独分析H&E或IHC图像，尽管临床依赖它们的协同解释。然而，同时获取这两种模态常因工作流程复杂性和成本限制而受阻。我们提出了一种自适应双模态框架，通过三项创新实现灵活的单/双模态HER2预测：1）动态分支选择器，根据输入完整性激活单模态重建或双模态联合推断；2）双向跨模态GAN，执行缺失模态的上下文感知特征空间重建；3）结合对抗学习和多任务优化的混合训练协议。该架构将单模态H&E预测准确率从71.44%提升至94.25%，同时实现95.09%的双模态准确率，并在仅使用IHC输入时保持90.28%的可靠性。框架的“双模态优先，单模态兼容”设计在不需同步采集的情况下提供接近双模态的性能，尤其通过减少IHC基础设施成本使资源有限的环境受益。实验验证显示，与H&E/IHC基线相比，准确率分别提高了22.81%/12.90%，跨模态重建将F1分数提升至0.9609（HE到IHC）和0.9251（IHC到HE）。通过动态将输入路由至重建增强或原生融合路径，系统减轻了因数据缺失导致的性能下降，同时保持了计算效率（轻量级变体参数减少78.55%）。这种弹性架构展示了在多样化医疗环境中普及精确HER2评估的巨大潜力。

</details>


### [260] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
**中文标题：基于统一多模态空间中扩散的可控表达性3D面部动画**

*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: A diffusion-based framework for expressive 3D facial animation that leverages multimodal signals (text, audio, emotion labels) and an attention-based latent diffusion model to improve emotion similarity and motion diversity.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of single-modal control signals and deterministic regression-based mapping in audio-driven emotional 3D facial animation, which restrict expressiveness and flexibility.

Method: Introduces a FLAME-centered multimodal emotion binding strategy and an attention-based latent diffusion model with content-aware attention and emotion-guided layers.

Result: Outperforms existing methods with a 21.6% improvement in emotion similarity while maintaining natural facial dynamics.

Conclusion: The proposed framework enables flexible and expressive 3D facial animation by leveraging multimodal signals and diffusion-based modeling.

摘要: 音频驱动的情绪化3D面部动画面临两大挑战：(1) 依赖单模态控制信号（视频、文本或情绪标签）而未能利用其互补优势以实现全面的情绪操控；(2) 确定性回归映射限制了情绪表达和非语言行为的随机性，从而限制了合成动画的表现力。为解决这些问题，我们提出了一种基于扩散的可控表达性3D面部动画框架。我们的方法包含两大创新：(1) 一种以FLAME为中心的多模态情绪绑定策略，通过对比学习对齐文本、音频和情绪标签等多样化模态，实现从多信号源的灵活情绪控制；(2) 一种基于注意力的潜在扩散模型，具有内容感知注意力和情绪引导层，在保持时间连贯性和自然面部动态的同时丰富了运动多样性。大量实验表明，我们的方法在多数指标上优于现有方法，情绪相似性提高了21.6%，同时保持了生理上合理的面部动态。项目页面：https://kangweiiliu.github.io/Control_3D_Animation。

</details>


### [261] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/abs/2506.10008)
**中文标题：视觉叙事推理的结构化图表示：漫画的分层框架**

*Yi-Chun Chen*

Main category: cs.MM

TL;DR: The paper introduces a hierarchical knowledge graph framework for analyzing visual narratives like comics, decomposing content into multiple levels and integrating semantic, spatial, and temporal relationships. It demonstrates high precision in tasks like action retrieval and dialogue tracing.


<details>
  <summary>Details</summary>
Motivation: To address the need for structured understanding of visual narratives, especially in multimodal media like comics, by capturing complex relationships between visual and textual elements.

Method: Proposes a hierarchical framework that decomposes narratives into macro-level story arcs and fine-grained event segments, represented through integrated knowledge graphs. Multimodal graphs link visual and textual components at the panel level.

Result: Applied to the Manga109 dataset, the framework achieved high precision and recall in tasks such as action retrieval, dialogue tracing, character appearance mapping, and panel timeline reconstruction.

Conclusion: The framework provides a scalable foundation for narrative-based content analysis, interactive storytelling, and multimodal reasoning in visual media.

摘要: 本文提出了一种分层知识图谱框架，用于结构化理解视觉叙事，特别是如漫画等多模态媒体。该方法将叙事内容分解为多个层次，从宏观故事线到细粒度事件片段，并通过集成知识图谱捕捉语义、空间和时间关系。在面板层面，我们构建了多模态图，将视觉元素（如角色、物体和动作）与相应的文本组件（如对话和字幕）联系起来。这些图在叙事层次上集成，支持对故事结构、角色连续性和事件进展的推理。我们将该方法应用于手动标注的Manga109数据集子集，并展示了其在多样化叙事任务中的符号推理能力，包括动作检索、对话追踪、角色出场映射和面板时间线重建。评估结果显示，该框架在各项任务中具有高精度和高召回率，验证了其一致性和可解释性。这项工作为基于叙事的视觉媒体内容分析、交互式叙事和多模态推理提供了可扩展的基础。

</details>


### [262] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/abs/2506.10011)
**中文标题：WDMIR：基于小波驱动的多模态意图识别**

*Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu*

Main category: cs.MM

TL;DR: The paper introduces WDMIR, a wavelet-driven framework for multimodal intent recognition, enhancing accuracy by analyzing non-verbal cues in the frequency domain.


<details>
  <summary>Details</summary>
Motivation: Existing intent recognition methods focus on text analysis, neglecting non-verbal cues. WDMIR aims to bridge this gap by leveraging frequency-domain analysis for richer semantic understanding.

Method: Proposes a wavelet-driven fusion module for synchronized video-audio feature decomposition and a cross-modal interaction mechanism for progressive feature enhancement from bimodal to trimodal integration.

Result: Achieves state-of-the-art performance with a 1.13% accuracy improvement on MIntRec. The wavelet-driven module boosts recognition accuracy by 0.41% for subtle emotional cues.

Conclusion: WDMIR effectively integrates verbal and non-verbal information, demonstrating superior performance in multimodal intent recognition.

摘要: 多模态意图识别（MIR）旨在通过整合视频、音频和文本模态中的言语和非言语信息来准确解读用户意图。现有方法侧重于文本分析，往往忽略了非言语线索中丰富的语义内容。本文提出了一种新颖的基于小波驱动的多模态意图识别（WDMIR）框架，通过对非言语信息进行频域分析来增强意图理解。具体而言，我们提出：（1）一个小波驱动的融合模块，在频域中对视频-音频特征进行同步分解和整合，实现对时间动态的细粒度分析；（2）一种跨模态交互机制，促进从双模态到三模态的渐进特征增强，有效弥合言语和非言语信息之间的语义鸿沟。在MIntRec上的大量实验表明，我们的方法实现了最先进的性能，准确率比之前的方法提高了1.13%。消融研究进一步验证了小波驱动融合模块显著提升了从非言语源中提取语义信息的能力，在分析细微情感线索时识别准确率提高了0.41%。

</details>


### [263] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
**中文标题：多模态大型语言模型综述**

*Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker*

Main category: cs.MM

TL;DR: The survey explores the evolution of Multimodal Large Language Models (MLLMs), categorizing six generative modalities and analyzing foundational techniques enabling cross-modal capabilities. It highlights architectural trends, synergies, and unresolved challenges in MLLM development.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a unified perspective on the rapid advancements in MLLMs, which now span diverse output modalities beyond text, and to identify key techniques and challenges in achieving cross-modal capabilities.

Method: The method involves categorizing six primary generative modalities and examining foundational techniques like SSL, MoE, RLHF, and CoT prompting. It analyzes key models, architectural trends, and emergent cross-modal synergies.

Result: The survey identifies architectural innovations (e.g., transformers, diffusion models) enabling cross-modal transfer and modular specialization, while highlighting open challenges in evaluation, modularity, and structured reasoning.

Conclusion: The conclusion emphasizes the need for more general-purpose, adaptive, and interpretable multimodal systems, offering a unified view of MLLM development and critical future directions.

摘要: 多模态大型语言模型（MLLMs）已迅速超越文本生成，通过将语言与其他感官模态在统一架构下整合，现涵盖图像、音乐、视频、人体运动和3D对象等多种输出模态。本综述分类了六种主要生成模态，并探讨了自监督学习（SSL）、专家混合（MoE）、人类反馈强化学习（RLHF）和思维链（CoT）提示等基础技术如何实现跨模态能力。我们分析了关键模型、架构趋势和新兴的跨模态协同效应，同时强调了可转移技术和未解决的挑战。诸如变压器和扩散模型等架构创新支撑了这一融合，实现了跨模态转移和模块化专业化。我们突出了新兴的协同模式，并指出了评估、模块化和结构化推理方面的开放挑战。本综述提供了对MLLM发展的统一视角，并确定了实现更通用、自适应和可解释的多模态系统的关键路径。

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [264] [Semi-Tensor-Product Based Convolutional Neural Networks](https://arxiv.org/abs/2506.10407)
**中文标题：基于半张量积的卷积神经网络**

*Daizhan Cheng*

Main category: eess.SY

TL;DR: The paper introduces a new convolutional product (CP) based on semi-tensor product (STP) and domain-based CP, avoiding padding-related issues, and applies it to STP-based CNNs for image and signal identification.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional inner products and padding in CNNs by leveraging STP and domain-based CP for more flexible and efficient convolutional operations.

Method: Proposes a new CP combining domain-based CP with STP, eliminating padding, and develops an STP-based CNN for image and third-order signal identification.

Result: The STP-based CNN avoids junk information from padding and is successfully applied to image and signal identification tasks.

Conclusion: The proposed STP-based CP and CNN offer a padding-free, efficient alternative for convolutional operations in neural networks.

摘要: 向量的半张量积（STP）是传统向量内积的推广，允许因子向量具有不同维度。本文提出了一种基于域的卷积积（CP）。通过将基于域的CP与向量的STP结合，提出了一种新的CP。由于没有零填充或其他填充，可以避免由填充引起的垃圾信息。利用它，开发了基于STP的卷积神经网络（CNN），并考虑了其在图像和第三阶信号识别中的应用。

</details>


### [265] [Energy Aware Camera Location Search Algorithm for Increasing Precision of Observation in Automated Manufacturing](https://arxiv.org/abs/2506.10251)
**中文标题：提高自动化制造观测精度的能量感知相机位置搜索算法**

*Rongfei Li,Francis Assadian*

Main category: eess.SY

TL;DR: The paper proposes an energy-aware algorithm to optimize camera placement in automated manufacturing, minimizing image noise and improving observation precision with limited energy.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on control and observation architectures but overlooks the impact of camera location on image quality in eye-to-hand configurations. Environmental noise varies by location, affecting estimation accuracy.

Method: The algorithm explores the camera workspace to find optimal or suboptimal locations with minimal noise, using adaptive search policies and image averaging for efficiency. It operates within energy constraints.

Result: Simulations show the algorithm successfully improves observation precision in automated manufacturing, achieving desirable accuracy without filtering high-frequency image information.

Conclusion: The algorithm effectively balances energy constraints and observation precision, offering a practical solution for optimizing camera placement in manufacturing environments.

摘要: 视觉伺服技术已在自动化制造任务中得到广泛应用，尤其在工具姿态对齐方面。为获取工具的全局视图，多数应用采用眼到手配置或眼到手/眼在手协作配置。现有研究多关注控制和观测架构的开发，但鲜有探讨眼到手配置中相机位置的重要性。制造环境中，相机估计质量因观测位置不同而差异显著，环境条件综合作用导致单张图像在不同位置的噪声水平不同。本文提出一种相机移动策略算法，探索相机工作空间并搜索图像噪声水平最小的最优位置。同时，该算法确保相机在有限能量下最终停留在已搜索的次优位置（若最优位置不可达）。与简单暴力搜索不同，该算法通过学习环境自适应调整搜索策略，使相机更高效地探索空间。借助图像平均技术，该算法仅用单相机即可在眼到手配置中实现理想的观测精度，无需滤除原始图像中的高频信息。通过自动化制造应用的仿真，结果表明该算法在有限能量下成功提高了观测精度。

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [266] [Ground Reaction Force Estimation via Time-aware Knowledge Distillation](https://arxiv.org/abs/2506.10265)
**中文标题：基于时间感知知识蒸馏的地面反作用力估计**

*Eun Som Jeon,Sinjini Mitra,Jisoo Lee,Omik M. Save,Ankita Shukla,Hyunglae Lee,Pavan Turaga*

Main category: eess.SP

TL;DR: The paper proposes a Time-aware Knowledge Distillation framework to improve GRF estimation from wearable insole sensors, outperforming current baselines by leveraging temporal and similarity features.


<details>
  <summary>Details</summary>
Motivation: Traditional GRF measurement methods like instrumented treadmills are costly and lack portability, while wearable insole sensors are noisy and less accurate. The paper aims to address these limitations.

Method: The Time-aware Knowledge Distillation framework captures complementary relationships and sequential properties between features in a mini-batch during distillation.

Result: The framework outperforms current baselines in GRF estimation, as validated by comparisons with treadmill measurements.

Conclusion: The proposed framework provides a lightweight, accurate solution for GRF estimation from wearable sensors, addressing portability and cost issues.

摘要: 可穿戴传感器的人体步态分析已广泛应用于日常生活保健、康复、物理治疗及临床诊断与监测等领域。地面反作用力（GRF）尤其提供了关于身体在运动过程中与地面交互的关键信息。尽管仪器化跑步机被广泛用作测量步行中GRF的金标准，但其缺乏便携性和高昂成本使其在许多应用中不切实际。作为一种替代方案，低成本、便携式的可穿戴鞋垫传感器被用于测量GRF；然而，这些传感器易受噪声和干扰影响，且准确性低于跑步机测量。为解决这些问题，我们提出了一种基于时间感知知识蒸馏的框架，用于从鞋垫传感器数据中估计GRF。该框架在知识蒸馏过程中利用小批量内的相似性和时间特征，有效捕捉了特征之间的互补关系以及目标与输入数据的序列特性。通过将鞋垫传感器数据的GRF估计与仪器化跑步机的测量结果进行比较，评估了通过该框架蒸馏的轻量级模型的性能。实证结果表明，时间感知知识蒸馏在可穿戴传感器数据的GRF估计中优于当前基线方法。

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [267] [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020)
**中文标题：从威胁到工具：利用拒绝感知注入攻击实现安全对齐**

*Kyubyung Chae,Hyunbin Jin,Taesup Kim*

Main category: cs.CR

TL;DR: The paper introduces RAAI, a training-free method to repurpose LLM attack techniques for safety alignment, significantly increasing harmful response rates and improving model robustness through synthetic data.


<details>
  <summary>Details</summary>
Motivation: Current methods for safety alignment of LLMs are costly and time-consuming, relying on human-labeled data or complex synthetic data generation. The paper aims to simplify and improve this process by leveraging attack techniques.

Method: The authors propose Refusal-Aware Adaptive Injection (RAAI), which detects refusal signals in LLMs and injects predefined phrases to generate harmful completions. This synthetic data is then used for fine-tuning.

Result: RAAI increased harmful response rates from 2.15% to 61.04% on average across benchmarks. Fine-tuning with RAAI-generated data improved model robustness against harmful prompts without compromising general capabilities.

Conclusion: The study demonstrates that LLM attack methodologies can be repurposed as practical tools for scalable and controllable safety alignment.

摘要: 安全对齐大型语言模型（LLM）通常需要大量人工标注的偏好数据，这一过程既昂贵又耗时。虽然合成数据提供了一种有前景的替代方案，但当前方法往往依赖于复杂的迭代提示或辅助模型。为解决这一问题，我们引入了拒绝感知自适应注入（RAAI），这是一种简单、无需训练且模型无关的框架，通过重新利用LLM攻击技术实现。RAAI通过检测内部拒绝信号并自适应注入预定义短语，以引出有害但流畅的补全。实验表明，RAAI有效破解了LLM，将有害响应率从基线2.15%提升至平均61.04%（在四个基准测试中）。关键的是，使用RAAI生成的合成数据对LLM进行微调，可以增强模型对有害提示的鲁棒性，同时保留在标准任务（如MMLU和ARC）上的通用能力。这项工作展示了如何将LLM攻击方法重新定义为可扩展且可控的安全对齐工具。

</details>


### [268] [LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges](https://arxiv.org/abs/2506.10022)
**中文标题：大型语言模型陷入交火：恶意请求与越狱挑战**

*Haoyang Li,Huan Gao,Zhiyuan Zhao,Zhiyu Lin,Junyu Gao,Xuelong Li*

Main category: cs.CR

TL;DR: The paper introduces MalwareBench, a benchmark dataset to evaluate LLMs' vulnerability to jailbreak attacks in malicious code generation, revealing their limited ability to reject such threats.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored susceptibility of LLMs to jailbreak attacks in code generation, highlighting security gaps.

Method: Developed MalwareBench with 3,520 jailbreaking prompts based on 320 malicious code requirements, testing 11 jailbreak methods and 29 code categories.

Result: Mainstream LLMs struggle to reject malicious requests (60.93% rejection rate), which drops to 39.92% with combined jailbreak methods.

Conclusion: LLMs' code security capabilities remain a significant challenge, necessitating further research and improvements.

摘要: 大型语言模型（LLMs）的广泛应用引发了对其安全性的担忧，尤其是其易受越狱攻击的脆弱性，这些攻击利用精心设计的提示生成恶意输出。尽管已有研究探讨了LLMs的一般安全能力，但其在代码生成中对越狱攻击的特定易感性仍未被充分研究。为填补这一空白，我们提出了MalwareBench，一个包含3,520个恶意代码生成越狱提示的基准数据集，旨在评估LLMs对此类威胁的鲁棒性。MalwareBench基于320个手工制作的恶意代码生成需求，涵盖11种越狱方法和29种代码功能类别。实验表明，主流LLMs在拒绝恶意代码生成需求方面能力有限，而多种越狱方法的结合进一步降低了模型的安全能力：具体而言，恶意内容的平均拒绝率为60.93%，结合越狱攻击算法后降至39.92%。我们的工作表明，LLMs的代码安全能力仍面临重大挑战。

</details>


### [269] [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024)
**中文标题：私有记忆编辑：将记忆转化为防御手段以增强大型语言模型的数据隐私**

*Elena Sofia Ruzzetti,Giancarlo A. Xompero,Davide Venditti,Fabio Massimo Zanzotto*

Main category: cs.CR

TL;DR: The paper introduces Private Memorization Editing (PME), a method to prevent private data leakage in LLMs by detecting and editing memorized PII, turning memorization into a defense strategy.


<details>
  <summary>Details</summary>
Motivation: LLMs may memorize Personally Identifiable Information (PII), posing privacy risks. The paper aims to leverage this memorization ability to defend against privacy attacks.

Method: PME detects memorized PII and edits the model's knowledge to mitigate memorization, without affecting the model's performance.

Result: PME effectively reduces leaked PII, sometimes reducing privacy attack accuracy to zero, while maintaining model functionality.

Conclusion: PME transforms LLM memorization into a privacy defense, robustly preventing PII leakage without compromising model performance.

摘要: 大型语言模型（LLMs）会记忆大量未受控制的数据，其中可能包含不应存储或泄露的个人身份信息（PII）。本文提出私有记忆编辑（PME）方法，通过将LLMs的记忆能力转化为隐私防御策略，防止私有数据泄露。虽然针对LLMs的攻击利用了其训练数据的先验知识，但我们的方法旨在利用同类知识增强模型的鲁棒性。我们检测记忆的PII，并通过编辑模型对训练数据的知识来减轻PII的记忆。实验证明，该方法不影响底层语言模型，同时使其更抗隐私训练数据提取攻击。PME能在多种配置下有效减少泄露的PII数量，某些情况下甚至将隐私攻击的准确率降至零。

</details>


### [270] [Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks](https://arxiv.org/abs/2506.10029)
**中文标题：ChatGPT与Gemini的安全性与对齐性实证评估：通过越狱实验的漏洞比较分析**

*Rafaël Nouailles*

Main category: cs.CR

TL;DR: The paper compares the security and alignment of ChatGPT and Gemini, analyzing jailbreak techniques and vulnerabilities through experiments.


<details>
  <summary>Details</summary>
Motivation: To address cybersecurity challenges posed by LLMs, such as prompt injection and jailbreaking, by evaluating and comparing the security of ChatGPT and Gemini.

Method: Comparative analysis of ChatGPT and Gemini's security levels, including a taxonomy of jailbreak techniques and experimental validation.

Result: Identifies vulnerabilities in both models, highlighting differences in their susceptibility to jailbreak attacks and other security risks.

Conclusion: The study underscores the need for improved security measures in LLMs to mitigate risks like jailbreaking and misinformation.

摘要: 大型语言模型（LLMs）正在改变数字应用，特别是在文本生成、图像创作、信息检索和代码开发方面。OpenAI于2022年11月推出的ChatGPT迅速成为标杆，促使谷歌的Gemini等竞争对手的出现。然而，这些技术进步带来了新的网络安全挑战，包括提示注入攻击、规避监管措施（越狱）、错误信息传播（幻觉）以及与深度伪造相关的风险。本文对ChatGPT和Gemini的安全性和对齐性进行了比较分析，并提出了与实验相关的越狱技术分类。

</details>


### [271] [Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment](https://arxiv.org/abs/2506.10030)
**中文标题：在RAG即服务环境中保护多模态知识版权**

*Tianyu Chen,Jian Lou,Wenjie Wang*

Main category: cs.CR

TL;DR: AQUA is the first watermark framework for protecting image knowledge in Multimodal RAG systems, using acronym-based triggers and spatial cues to ensure robust and stealthy copyright tracing.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods in RAG only protect textual knowledge, leaving image knowledge vulnerable, necessitating a solution for multimodal copyright protection.

Method: AQUA embeds semantic signals into synthetic images via acronym-based triggers and spatial relationship cues, ensuring watermark survival through indirect propagation.

Result: Experiments show AQUA is efficient, effective, and imperceptible, enabling robust and reliable copyright tracing in diverse models and datasets.

Conclusion: AQUA fills a critical gap in multimodal RAG protection by safeguarding image knowledge copyright with stealthy and reliable watermarking.

摘要: 随着检索增强生成（RAG）演变为共享知识库的服务导向平台（RAG即服务），保护贡献数据的版权变得至关重要。现有的RAG水印方法仅关注文本知识，而图像知识未受保护。本文提出AQUA，首个用于多模态RAG系统中图像知识保护的水印框架。AQUA通过两种互补方法将语义信号嵌入合成图像：基于缩写的触发器和空间关系线索。这些技术确保水印信号在从图像检索器到文本生成器的间接传播中存活，高效、有效且不可察觉。跨多种模型和数据集的实验表明，AQUA实现了稳健、隐蔽且可靠的版权追踪，填补了多模态RAG保护的关键空白。

</details>


### [272] [GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)
**中文标题：GenBreak：利用大型语言模型对文本到图像生成器进行红队测试**

*Zilong Wang,Xiang Zheng,Xiaosen Wang,Bo Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: GenBreak is a framework that fine-tunes a large language model (LLM) to craft adversarial prompts for testing the safety of text-to-image (T2I) generators, revealing vulnerabilities in commercial models.


<details>
  <summary>Details</summary>
Motivation: Current red-teaming and adversarial attack methods for T2I models either produce easily detectable toxic images or fail to generate genuinely harmful outputs, leaving a gap in evaluating the safety of defended T2I models.

Method: GenBreak fine-tunes an LLM using supervised learning on curated datasets and reinforcement learning via interaction with a surrogate T2I model, integrating multiple reward signals to craft adversarial prompts that enhance evasion and toxicity.

Result: The crafted prompts effectively bypass safety filters in commercial T2I generators, demonstrating practical and concerning safety weaknesses.

Conclusion: GenBreak provides a reliable tool for evaluating T2I model safety by systematically uncovering vulnerabilities, highlighting the need for improved defenses.

摘要: 文本到图像（T2I）模型（如Stable Diffusion）发展迅速，广泛应用于内容创作。然而，这些模型可能被滥用于生成有害内容（如裸体或暴力），带来重大安全风险。尽管大多数平台采用内容审核系统，但潜在漏洞仍可能被恶意利用。目前针对T2I模型的红队测试和对抗攻击研究存在明显局限：一些研究成功生成高度有害图像，但使用的对抗提示容易被安全过滤器检测和拦截；另一些研究则专注于绕过安全机制，但未能生成真正有害的输出，忽视了高风险提示的发现。因此，目前缺乏可靠工具来评估防御性T2I模型的安全性。为解决这一问题，我们提出GenBreak框架，通过微调红队大型语言模型（LLM）系统性地探索T2I生成器的潜在漏洞。我们的方法结合了监督微调（基于精选数据集）和强化学习（通过与代理T2I模型交互），通过整合多种奖励信号，指导LLM生成既能增强规避能力又能提高图像毒性的对抗提示，同时保持语义连贯性和多样性。这些提示在针对商业T2I生成器的黑盒攻击中表现出强大的有效性，揭示了实际且令人担忧的安全弱点。

</details>


### [273] [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171)
**中文标题：LLM代理的披露审计**

*Saswat Das,Jameson Sandler,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: The paper proposes CMPL, an auditing framework to detect privacy risks in LLM agents through multi-turn interactions, revealing vulnerabilities missed by single-turn defenses.


<details>
  <summary>Details</summary>
Motivation: LLM agents handle sensitive data, increasing unauthorized disclosure risks; existing defenses focus on single-turn interactions, missing latent vulnerabilities in multi-turn scenarios.

Method: The CMPL framework uses iterative probing to stress-test agents, simulating realistic multi-turn interactions to systematically uncover privacy risks.

Result: CMPL successfully identifies privacy risks across diverse domains and safety configurations, outperforming single-turn defenses.

Conclusion: CMPL is an effective diagnostic tool for conversational privacy, offering quantifiable risk metrics and an open benchmark for future evaluations.

摘要: 大型语言模型代理已开始作为个人助理、客服机器人和临床助手出现。尽管这些应用带来了显著的操作优势，但它们也需要持续访问敏感数据，从而增加了未经授权披露的可能性。本研究提出了一种对话隐私的审计框架，用于量化和审计这些风险。所提出的对话操纵隐私泄露（CMPL）框架是一种迭代探测策略，旨在对执行严格隐私指令的代理进行压力测试。CMPL不仅关注单一披露事件，还模拟现实的多轮交互，系统地揭示潜在漏洞。我们在多样化领域、数据模态和安全配置上的评估表明，该审计框架能够揭示现有单轮防御无法阻止的隐私风险。除了将CMPL作为一种诊断工具引入外，本文还提供了（1）基于可量化风险指标的审计程序，（2）一个开放的基准，用于评估跨代理实现的对话隐私。

</details>


### [274] [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
**中文标题：提示攻击揭示遗忘方法中的表面知识移除**

*Yeonwoo Jang,Shariqah Hossain,Ashwin Sreevatsa,Diogo Cruz*

Main category: cs.CR

TL;DR: The paper reveals that some machine unlearning methods are vulnerable to prompt attacks, failing to truly remove knowledge despite appearing effective. It evaluates eight techniques, showing varying robustness, and emphasizes the need for better evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: To challenge the effectiveness of machine unlearning methods by demonstrating their vulnerability to prompt attacks and to highlight the gap between superficial output suppression and true knowledge removal.

Method: Systematically evaluated eight unlearning techniques across three model families using output-based, logit-based, and probe analysis to assess knowledge retrieval under prompt attacks.

Result: Some methods (e.g., RMU, TAR) showed robust unlearning, while others (e.g., ELM) were vulnerable, with specific attacks recovering up to 57.3% accuracy. Logit analysis confirmed unlearned models do not hide knowledge by formatting changes.

Conclusion: The study challenges assumptions about unlearning effectiveness, advocating for improved evaluation frameworks to distinguish true knowledge removal from superficial suppression.

摘要: 在这项工作中，我们表明，某些机器遗忘方法在面对简单的提示攻击时可能会失效。我们系统地评估了三种模型家族中的八种遗忘技术，并采用基于输出、基于逻辑和探针分析的方法，以确定所谓的遗忘知识在多大程度上可以被检索。尽管像RMU和TAR这样的方法表现出强大的遗忘能力，但ELM仍然容易受到特定提示攻击的影响（例如，原始提示中的印地语填充文本恢复了57.3%的准确率）。我们的逻辑分析还证实，遗忘模型通常不会通过修改答案的格式来隐藏知识，因为输出和逻辑准确性之间的相关性很强。这些结果挑战了关于遗忘有效性的普遍假设，并强调了需要能够可靠地区分真正知识移除和表面输出抑制的评估框架。我们还公开提供了我们的评估框架，以便轻松评估提示技术以检索遗忘知识。

</details>


### [275] [SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks](https://arxiv.org/abs/2506.10424)
**中文标题：SOFT：选择性数据混淆保护LLM微调免受成员推理攻击**

*Kaiyuan Zhang,Siyuan Cheng,Hanxi Guo,Yuetian Chen,Zian Su,Shengwei An,Yuntao Du,Charles Fleming,Ashish Kundu,Xiangyu Zhang,Ninghui Li*

Main category: cs.CR

TL;DR: The paper introduces SOFT, a defense technique to protect fine-tuned LLMs from membership inference attacks by selectively obfuscating data, balancing privacy and utility.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs often involves sensitive data, making them vulnerable to membership inference attacks (MIAs), which exploit loss reduction during training to reveal membership information.

Method: SOFT (Selective data Obfuscation in LLM Fine-Tuning) uses influential data selection with an adjustable parameter to obfuscate sensitive data while preserving model utility.

Result: Experiments across six domains and multiple LLM architectures show SOFT effectively reduces privacy risks while maintaining competitive performance.

Conclusion: SOFT provides a practical and scalable solution to protect sensitive information in fine-tuned LLMs against MIAs.

摘要: 大型语言模型（LLMs）取得了显著成功并被广泛应用于各种场景。然而，微调这些模型通常涉及私有或敏感信息，引发了严重的隐私问题。本研究首次全面评估了微调LLMs对成员推理攻击（MIAs）的脆弱性。实证分析表明，MIAs利用微调过程中的损失减少，高效地揭示了成员信息。这些发现促使我们开发防御方法。我们提出了SOFT（选择性数据混淆在LLM微调中），这是一种新颖的防御技术，通过利用可调参数的影响数据选择来减轻隐私泄露，同时平衡效用保留和隐私保护。我们的广泛实验覆盖了六个不同领域和多种LLM架构及规模。结果显示，SOFT在保持模型性能的同时有效降低了隐私风险，为保护微调LLMs中的敏感信息提供了实用且可扩展的解决方案。

</details>


### [276] [Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications](https://arxiv.org/abs/2506.10467)
**中文标题：多智能体LLM系统的规范与评估——原型及网络安全应用**

*Felix Härer*

Main category: cs.CR

TL;DR: The paper explores the potential of multi-agent LLM systems by specifying and evaluating their architecture and prototype, particularly in cybersecurity applications.


<details>
  <summary>Details</summary>
Motivation: To address the lack of defined specifications for multi-agent LLM systems and explore their combined potential in specific domains like cybersecurity.

Method: Extends a system architecture and prototype from previous research, introduces a specification for multi-agent LLM systems, and evaluates them through cybersecurity tasks.

Result: The system successfully completed cybersecurity tasks, demonstrating feasibility in question answering, server security, and network security using OpenAI and DeepSeek LLMs.

Conclusion: The study highlights the potential of multi-agent LLM systems for complex tasks and the need for systematic specifications and evaluations.

摘要: 近期LLM的进展表明其在新型应用中的潜力，例如通过OpenAI和DeepSeek最新模型的推理能力。为了将这些模型应用于文本生成之外的特定领域，可以采用基于LLM的多智能体方法，通过结合推理技术、代码生成和软件执行来解决复杂任务。应用可能会利用这些能力以及专业LLM智能体的知识。然而，尽管对LLM、推理技术和应用进行了许多单独评估，但它们的联合规范和组合应用尚未得到充分探索。需要为多智能体LLM系统定义规范，以探索其潜力及其对特定应用的适用性，从而对LLM、推理技术及相关方面进行系统评估。本文报告了通过多智能体系统对这些方面进行规范和评估的探索性研究结果。系统架构和原型从先前研究中扩展而来，并引入了多智能体系统的规范。涉及网络安全任务的测试案例表明了架构和评估方法的可行性。特别是，结果显示了对OpenAI和DeepSeek的LLM智能体在问题回答、服务器安全和网络安全任务中的正确完成情况的评估。

</details>


### [277] [Secure Data Access in Cloud Environments Using Quantum Cryptography](https://arxiv.org/abs/2506.10028)
**中文标题：在云环境中使用量子密码学实现安全数据访问**

*S. Vasavi Venkata Lakshmi,Ziaul Haque Choudhury*

Main category: cs.CR

TL;DR: The paper proposes using quantum cryptography (QKD, BB84, and QOTP) to secure data in cloud environments, ensuring protection against future quantum computer threats.


<details>
  <summary>Details</summary>
Motivation: Traditional data security methods may become ineffective with the advent of quantum computers, necessitating quantum-based solutions for cloud data protection.

Method: The study employs Quantum Key Distribution (QKD) with the BB84 protocol to generate secure keys and Quantum One Time Pad (QOTP) for encryption and decryption.

Result: The combination of QKD, BB84, and QOTP provides a robust defense against hackers, even those with quantum computing capabilities.

Conclusion: Quantum cryptography offers a future-proof solution for securing cloud data, making cloud computing safer for users.

摘要: 云计算使得数据的存储和访问变得更加便捷，但如何确保数据安全却是一个重大挑战。传统的安全方法在未来强大的量子计算机出现时可能不再有效。为了解决这一问题，本研究采用量子密码学来保护云环境中的数据。量子密钥分发（QKD）通过利用量子粒子（如光子）传输信息来生成安全密钥。具体而言，我们使用了BB84协议，这是一种简单可靠的方法，可以生成无法被窃取且无法被检测到的安全密钥。为了保护数据，我们采用量子一次性密码本（QOTP）进行加密和解密，确保数据的完全隐私。本研究展示了这些量子方法如何应用于云系统，以提供强大的防御能力，即使黑客拥有量子计算机。QKD、BB84和QOTP的结合为云中存储或共享的数据提供了一种安全可靠的方式。通过量子密码学，本文提供了一种确保当前和未来数据安全的方法，使云计算对用户的数据存储更加安全可靠。

</details>


### [278] [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
**中文标题：SoK：评估大型语言模型的越狱防护机制**

*Xunguang Wang,Zhenlan Ji,Wenxuan Wang,Zongjie Li,Daoyuan Wu,Shuai Wang*

Main category: cs.CR

TL;DR: This paper provides a comprehensive analysis of jailbreak guardrails for LLMs, proposing a taxonomy and evaluation framework to assess their effectiveness and guide future research.


<details>
  <summary>Details</summary>
Motivation: The deployment of LLMs has exposed vulnerabilities to jailbreak attacks, and existing guardrails lack a unified taxonomy and evaluation framework, prompting the need for a holistic analysis.

Method: The authors propose a multi-dimensional taxonomy for guardrails and introduce a Security-Efficiency-Utility framework to evaluate their effectiveness through extensive analysis and experiments.

Result: The study identifies strengths and limitations of existing guardrails, explores their universality across attack types, and provides insights for optimizing defense combinations.

Conclusion: The work offers a structured foundation for future research, aiming to guide the principled advancement and deployment of robust LLM guardrails.

摘要: 大型语言模型（LLMs）取得了显著进展，但其部署暴露了关键漏洞，尤其是绕过安全机制的越狱攻击。防护机制——监控和控制LLM交互的外部防御手段——已成为一种有前景的解决方案。然而，当前LLM防护机制的格局较为分散，缺乏统一的分类法和全面的评估框架。在这篇系统化知识（SoK）论文中，我们首次对LLM的越狱防护机制进行了全面分析。我们提出了一种新颖的多维度分类法，将防护机制划分为六个关键维度，并引入了一个安全-效率-实用性的评估框架来评估其实际效果。通过广泛的分析和实验，我们确定了现有防护机制方法的优势和局限性，探讨了它们对不同攻击类型的普适性，并为优化防御组合提供了见解。我们的工作为未来的研究和开发提供了结构化基础，旨在指导稳健LLM防护机制的原则性发展和部署。代码可在https://github.com/xunguangwang/SoK4JailbreakGuardrails获取。

</details>


### [279] [TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks](https://arxiv.org/abs/2506.10722)
**中文标题：TED-LaST：针对自适应攻击的鲁棒后门防御**

*Xiaoxing Mo,Yuxuan Cheng,Nan Sun,Leo Yu Zhang,Wei Luo,Shang Gao*

Main category: cs.CR

TL;DR: The paper introduces TED-LaST, a robust defense strategy against adaptive backdoor attacks in DNNs, enhancing traditional TED with label-supervised dynamics tracking and adaptive layer emphasis. It effectively counters sophisticated attacks like Adap-Blend and Adapt-Patch.


<details>
  <summary>Details</summary>
Motivation: Existing TED-based defenses are vulnerable to adaptive backdoor attacks that distort topological representations. The paper aims to enhance TED's robustness against such attacks.

Method: TED-LaST introduces label-supervised dynamics tracking and adaptive layer emphasis to detect stealthy threats, even in cases of topological inseparability. It also reviews and classifies adaptive attack strategies.

Result: Experiments on datasets (CIFAR-10, GTSRB, ImageNet100) and models (ResNet20, ResNet101) show TED-LaST effectively counters advanced backdoor attacks, setting a new benchmark for robust detection.

Conclusion: TED-LaST significantly improves DNN security against evolving backdoor threats, providing a robust defense framework.

摘要: 深度神经网络（DNN）易受后门攻击，攻击者在训练过程中植入隐藏触发器以恶意控制模型行为。拓扑演化动力学（TED）最近成为检测DNN后门攻击的有力工具。然而，TED可能受到自适应后门攻击的威胁，这些攻击会扭曲网络层间的拓扑表示分布。为解决这一局限，我们提出TED-LaST（针对洗涤、慢释放和目标映射攻击策略的拓扑演化动力学），这是一种增强TED对自适应攻击鲁棒性的新型防御策略。TED-LaST引入了两项关键创新：标签监督的动态跟踪和自适应层强调。这些增强功能能够识别传统TED防御无法检测的隐蔽威胁，即使在拓扑空间不可分离和轻微拓扑扰动的情况下。我们回顾并分类了最先进自适应攻击中的数据投毒技巧，并提出了一种带有目标映射的增强自适应攻击，可以动态转移恶意任务并充分利用自适应攻击的隐蔽性。我们在多个数据集（CIFAR-10、GTSRB和ImageNet100）和模型架构（ResNet20、ResNet101）上的综合实验表明，TED-LaST有效对抗了Adap-Blend、Adapt-Patch等复杂后门以及提出的增强自适应攻击。TED-LaST为鲁棒后门检测设立了新标杆，显著提升了DNN对抗不断演变的威胁的安全性。

</details>


### [280] [ME: Trigger Element Combination Backdoor Attack on Copyright Infringement](https://arxiv.org/abs/2506.10776)
**中文标题：ME：针对版权侵权的触发元素组合后门攻击**

*Feiyu Yang,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.CR

TL;DR: The paper introduces a Multi-Element (ME) attack method to enhance the SilentBadDiffusion (SBD) backdoor attack on generative diffusion models, using more poisonous elements per sample and Discrete Cosine Transform (DCT) for stealth. It also provides new datasets for research, achieving better attack performance than benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods like SBD have limited data resources and suboptimal performance with few poisoning samples. The paper aims to improve attack effectiveness by introducing more poisonous elements and new datasets.

Method: The ME attack method increases poisonous visual-text elements per sample and uses DCT for stealth. New datasets are introduced to support research in this area.

Result: The ME method achieved higher Copyright Infringement Rates (CIR) and lower First Attack Epochs (FAE) compared to benchmarks, even with low subsampling ratios.

Conclusion: The ME method and new datasets significantly improve the effectiveness of backdoor attacks on generative diffusion models, outperforming existing methods.

摘要: 生成扩散模型（如Stable Diffusion）复制训练数据的能力可能被攻击者利用，通过重复的毒化图像-文本对发起版权侵权攻击。SilentBadDiffusion（SBD）是最近提出的一种方法，在文本到图像任务中表现出色。然而，该领域可行数据资源有限，部分因版权或内容不当问题受限；现有数据集中并非所有图像都适合攻击方法；此外，SBD在少量毒化样本下的攻击效果不理想。本文提出了适用于SBD等攻击研究的新数据集，并基于SBD提出多元素（ME）攻击方法，通过增加每个毒化样本的视觉-文本毒化元素数量以增强攻击能力，同时引入离散余弦变换（DCT）保持隐蔽性。在两个新数据集上，我们获得的版权侵权率（CIR）/首次攻击周期（FAE）分别为16.78%/39.50和51.20%/23.60，接近甚至优于基准Pokemon和Mijourney数据集。在低采样率（5%，6个毒化样本）条件下，ME和DCT的CIR/FAE为0.23%/84.00和12.73%/65.50，均优于原始SBD（完全无法攻击）。

</details>


### [281] [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949)
**中文标题：利用轻量级序列监视器监测大语言模型中的分解攻击**

*Chen Yueh-Han,Nitish Joshi,Yulin Chen,Maksym Andriushchenko,Rico Angell,He He*

Main category: cs.CR

TL;DR: The paper proposes a lightweight sequential monitor to defend against decomposition attacks in LLMs, achieving a 93% defense success rate while reducing costs and latency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety defenses fail under decomposition attacks, where malicious goals are split into benign subtasks to bypass detection. Current methods lack the ability to reason about long-range intent.

Method: The authors introduce a lightweight sequential monitoring framework that cumulatively evaluates subtasks in real time, using a curated dataset to validate their approach.

Result: The monitor achieves a 93% defense success rate, outperforms reasoning models, and remains robust against random task injection while reducing costs by 90% and latency by 50%.

Conclusion: Lightweight sequential monitors are highly effective in mitigating decomposition attacks and are practical for deployment.

摘要: 当前的大语言模型（LLM）安全防御在分解攻击下失效，即恶意目标被分解为规避拒绝的良性子任务。现有浅层安全对齐技术的局限性在于仅能检测即时提示中的危害，而无法推断长期意图，导致对看似良性指令序列中逐渐显现的恶意意图视而不见。为此，我们提出添加一个外部监视器，以更高粒度观察对话。为便于研究分解攻击的监测，我们构建了迄今为止最大且最多样化的数据集，涵盖问答、文本到图像和代理任务。通过在先进LLM上测试验证，我们的数据集在GPT-4o上平均攻击成功率达87%，证实分解攻击广泛有效。此外，我们发现随机任务可注入分解子任务中以进一步混淆恶意意图。为实时防御，我们提出一种轻量级序列监视框架，累积评估每个子任务。研究表明，经过精心提示设计的轻量级监视器防御成功率达93%，优于o3 mini等推理模型作为监视器。此外，其对随机任务注入保持稳健，并将成本降低90%，延迟减少50%。我们的结果表明，轻量级序列监视器在缓解分解攻击方面极为有效，且适合实际部署。

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [282] [A multi-scale loss formulation for learning a probabilistic model with proper score optimisation](https://arxiv.org/abs/2506.10868)
**中文标题：用于学习概率模型的多尺度损失公式与适当评分优化**

*Simon Lang,Martin Leutbecher,Pedro Maciel*

Main category: physics.ao-ph

TL;DR: The paper evaluates a multi-scale loss formulation for training probabilistic weather forecasting models, showing improved small-scale variability without compromising forecast skill.


<details>
  <summary>Details</summary>
Motivation: To enhance probabilistic weather forecasting models by better capturing small-scale variability through a multi-scale loss formulation.

Method: Tested the multi-scale loss in AIFS-CRPS, a model trained by optimizing the almost fair continuous ranked probability score (afCRPS).

Result: The multi-scale loss improved small-scale variability without negatively affecting forecast skill.

Conclusion: The findings suggest promising directions for scale-aware training in probabilistic weather forecasting models.

摘要: 我们评估了多尺度损失公式对训练概率机器学习天气预报模型的影响。多尺度损失在欧洲中期天气预报中心（ECMWF）开发的机器学习天气预报模型AIFS-CRPS中进行了测试。AIFS-CRPS通过直接优化几乎公平连续排名概率评分（afCRPS）进行训练。多尺度损失更好地约束了小尺度变异性，同时未对预报技能产生负面影响。这为未来在尺度感知模型训练中的工作开辟了有前景的方向。

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [283] [Leveraging LLMs for Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10093)
**中文标题：利用大型语言模型实现精准农业中的任务规划**

*Marcos Abel Zuzuárregui,Stefano Carpin*

Main category: cs.RO

TL;DR: The paper presents an end-to-end system using LLMs like ChatGPT to enable non-technical users to assign complex data collection tasks to autonomous robots in precision agriculture via natural language, addressing challenges in spatial reasoning and routing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify mission planning for autonomous robots in precision agriculture, as end users often lack technical expertise, making it difficult to adapt robots for diverse tasks.

Method: The method involves leveraging LLMs to translate natural language instructions into mission plans encoded with an IEEE task specification standard, executed via ROS2 nodes that interface with existing ROS libraries.

Result: Results demonstrate the system's ability to handle complex tasks while highlighting LLMs' limitations in spatial reasoning and routing, which the proposed implementation addresses.

Conclusion: The conclusion is that LLMs can effectively bridge the gap between non-technical users and autonomous robots in precision agriculture, though challenges remain in spatial reasoning and routing.

摘要: 机器人和人工智能在精准农业中具有巨大潜力。尽管机器人系统已成功应用于多种任务，但如何使其适应多样化任务仍具挑战性，尤其是终端用户通常缺乏技术专长。本文提出了一种端到端系统，利用大型语言模型（如ChatGPT），使用户能够通过自然语言指令为自主机器人分配复杂的数据收集任务。为提高可重用性，任务计划采用现有的IEEE任务规范标准进行编码，并通过ROS2节点在机器人上执行，这些节点将高层任务描述与现有ROS库连接起来。通过大量实验，我们突出了大型语言模型在此背景下的优势与局限性，特别是在空间推理和解决复杂路径规划问题方面，并展示了所提实现如何克服这些问题。

</details>


### [284] [One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10106)
**中文标题：一体适用：基于大语言模型的精准农业异构任务规划**

*Marcos Abel Zuzuárregui,Mustafa Melih Toslak,Stefano Carpin*

Main category: cs.RO

TL;DR: The paper introduces a natural language robotic mission planner using LLMs to simplify control of heterogeneous robots in precision agriculture, enabling non-technical users to create complex missions without coding.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and steep learning curves of AI tools in precision agriculture, making robotic automation accessible to non-technical users.

Method: Leverages large language models (LLMs) and predefined primitives to translate human language into executable descriptions for diverse robotic platforms.

Result: The system successfully supports complex agricultural missions across different robotic platforms, including manipulation and computer vision tasks.

Conclusion: The work advances accessibility of robotic automation in precision agriculture for non-technical users.

摘要: 人工智能正在改变精准农业，为农民提供了简化日常操作的新工具。尽管这些技术进步承诺提高效率，但它们通常带来额外的复杂性和陡峭的学习曲线，这对非技术用户尤其具有挑战性，因为他们需要在技术采用和现有工作负担之间取得平衡。本文提出了一种自然语言（NL）机器人任务规划器，使非专业人员能够通过通用界面控制异构机器人。通过利用大语言模型（LLM）和预定义原语，我们的架构将人类语言无缝翻译为可由不同机器人平台执行的中间描述。借助该系统，用户无需编写任何代码即可制定复杂的农业任务。在本文中，我们通过涉及机器人操作和计算机视觉任务的新实验扩展了之前为轮式机器人任务规划设计的系统。我们的结果表明，该架构既足够通用以支持多样化的机器人，又足够强大以执行复杂的任务请求。这项工作标志着在使精准农业中的机器人自动化对非技术用户更加可及方面迈出了重要一步。

</details>


### [285] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
**中文标题：利用视觉语言模型的导航框架**

*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: The paper proposes a modular navigation framework using vision-language models (VLMs) to decouple understanding from action planning, aiming for efficient and adaptable navigation without extensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The challenge in Vision-and-Language Navigation (VLN) lies in balancing multimodal understanding with computational efficiency, especially with large VLMs. The paper aims to address this by proposing a modular framework.

Method: The framework integrates a frozen vision-language model (Qwen2.5-VL-7B-Instruct) with lightweight planning logic, using prompt engineering, structured history management, and a two-frame visual input strategy.

Result: Initial evaluation on the Room-to-Room benchmark shows challenges in generalizing to unseen environments but demonstrates the potential for scalable and efficient navigation systems.

Conclusion: The modular approach provides a foundation for future improvements, suggesting enhanced environmental priors and expanded multimodal input integration as promising directions.

摘要: 视觉与语言导航（VLN）在具身AI中提出了一个复杂挑战，要求智能体解释自然语言指令并在视觉丰富的陌生环境中导航。大型视觉语言模型（LVLM）如CLIP和Flamingo的最新进展显著提升了多模态理解能力，但也带来了计算成本和实时部署的新挑战。本项目提出了一种模块化、即插即用的导航框架，将视觉语言理解与动作规划解耦。通过集成冻结的视觉语言模型Qwen2.5-VL-7B-Instruct与轻量级规划逻辑，我们旨在实现灵活、快速且适应性强的导航，而无需大量模型微调。我们的框架利用提示工程、结构化历史管理和双帧视觉输入策略，增强了导航步骤间的决策连续性。我们在VLN-CE设置下使用Matterport3D数据集和Habitat-Lab模拟环境对Room-to-Room基准进行了系统评估。尽管初步结果显示在严格评估设置下对未见环境的泛化能力存在挑战，但我们的模块化方法为可扩展和高效的导航系统奠定了基础，突出了通过增强环境先验和扩展多模态输入整合的未来改进方向。

</details>


### [286] [Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving](https://arxiv.org/abs/2506.10317)
**中文标题：利用语言和道路设计手册为自动驾驶地图重建提供信息**

*Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya,Dhaval Bhanderi*

Main category: cs.RO

TL;DR: The paper proposes enhancing lane-topology prediction for autonomous driving by integrating natural language conventions, road design manuals, and structured metadata into the SMERF model, improving detection and association of lanes and traffic elements.


<details>
  <summary>Details</summary>
Motivation: Lane-topology prediction is crucial for autonomous navigation, and existing methods can benefit from incorporating structured road metadata and design conventions encoded in natural language.

Method: The authors augment the SMERF model by combining road metadata from OSM maps, lane-width priors from road design manuals, and road centerline encodings. The method is evaluated on complex intersection scenarios using topology-aware metrics.

Result: The approach shows improved performance in lane and traffic element detection and their association, demonstrating generalization across diverse topologies and conditions.

Conclusion: The integration of language-based conventions and road design manuals enhances the accuracy and scalability of lane-topology prediction for autonomous driving.

摘要: 车道拓扑预测是安全可靠自动驾驶导航的关键组成部分。对道路环境的准确理解有助于这一任务。我们注意到，这些信息通常遵循自然语言编码的惯例，通过反映道路结构的设计规范和捕捉道路功能的道路名称。我们以轻量级的方式将这些信息增强到SMERF（一种基于地图先验的在线车道拓扑预测模型）中，通过结合OSM地图中的结构化道路元数据、道路设计手册中的车道宽度先验以及道路中心线编码。我们在两种地理多样化的复杂交叉口场景中评估了我们的方法。我们的方法在车道和交通元素检测及其关联方面均显示出改进。我们使用四种拓扑感知指标全面评估模型性能。这些结果表明了我们的方法在不同拓扑和条件下的泛化和扩展能力。

</details>


### [287] [EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence](https://arxiv.org/abs/2506.10600)
**中文标题：EmbodiedGen：面向具身智能的生成式3D世界引擎**

*Wang Xinjie,Liu Liu,Cao Yu,Wu Ruiqi,Qin Wenkang,Wang Dehui,Sui Wei,Su Zhizhong*

Main category: cs.RO

TL;DR: EmbodiedGen is a generative 3D world engine designed to create scalable, high-quality, and interactive 3D assets for embodied intelligence tasks, addressing the limitations of traditional manually created assets.


<details>
  <summary>Details</summary>
Motivation: Current embodied intelligence tasks rely on manually created 3D assets, which are costly and lack realism, hindering scalability. EmbodiedGen aims to provide a low-cost, scalable solution for generating diverse and realistic 3D worlds.

Method: EmbodiedGen uses generative AI to create high-quality, controllable 3D assets in URDF format. It includes modules like Image-to-3D, Text-to-3D, and Scene Generation for diverse and interactive world creation.

Result: The platform generates photorealistic 3D assets with accurate physical properties, supporting downstream tasks in training and evaluation of embodied intelligence.

Conclusion: EmbodiedGen offers a scalable and cost-effective solution for 3D world generation, leveraging generative AI to enhance embodied intelligence research.

摘要: 构建物理真实且比例准确的模拟3D世界对于具身智能任务的训练和评估至关重要。3D数据资产的多样性、真实性、低成本可及性和经济性对于实现具身AI的泛化和可扩展性至关重要。然而，目前大多数具身智能任务仍严重依赖传统的手工创建和标注的3D计算机图形资产，这些资产生产成本高且真实性有限。这些限制显著阻碍了数据驱动方法的可扩展性。我们提出了EmbodiedGen，一个用于交互式3D世界生成的基础平台。它能够以低成本规模化生成高质量、可控且逼真的3D资产，这些资产具有准确的物理属性和真实世界比例，并以统一机器人描述格式（URDF）呈现。这些资产可直接导入各种物理模拟引擎进行细粒度物理控制，支持训练和评估中的下游任务。EmbodiedGen是一个易于使用、功能齐全的工具包，由六个关键模块组成：图像到3D、文本到3D、纹理生成、关节对象生成、场景生成和布局生成。EmbodiedGen生成由生成式3D资产组成的多样化和交互式3D世界，利用生成式AI应对具身智能相关研究中的泛化和评估挑战。代码可在https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html获取。

</details>


### [288] [Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop](https://arxiv.org/abs/2506.10968)
**中文标题：《Eye, Robot: 通过BC-RL感知-动作循环学习观察以行动》**

*Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: The paper introduces EyeRobot, a robotic system with gaze behavior trained via a BC-RL loop to achieve hand-eye coordination for task completion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to mimic human active gaze behavior for task completion in robotics, enabling efficient manipulation in large workspaces with a single camera.

Method: The method involves training a gaze policy using reinforcement learning (RL) and a hand policy via behavioral cloning (BC), with data from teleoperated demonstrations and a 360 camera. The BC-RL loop jointly trains hand and eye agents.

Result: EyeRobot demonstrates effective hand-eye coordination, stable fixation, and improved object tracking, successfully completing manipulation tasks in panoramic workspaces.

Conclusion: The conclusion highlights EyeRobot's ability to facilitate manipulation in large workspaces through emergent hand-eye coordination, validated by experiments on five tasks.

摘要: 人类并非被动观察视觉世界，而是通过主动观察来行动。基于这一原则，我们提出了EyeRobot，一种具有从任务需求中自然产生的注视行为的机器人系统。我们开发了一个可以自由旋转观察周围环境的机械眼球，并通过强化学习训练注视策略来控制它。为此，我们首先收集了与360度摄像头配对的远程操作演示数据。这些数据被导入支持渲染任意眼球视点的仿真环境，从而在机器人演示的基础上实现眼球注视的模拟。接着，我们引入BC-RL循环来联合训练手和眼：手（BC）代理通过渲染的眼球观察进行训练，而眼（RL）代理在手产生正确动作预测时获得奖励。通过这种方式，手眼协调能力随着眼睛注视任务完成所需区域而自然产生。EyeRobot采用了一种受中央凹启发的策略架构，以较小的计算预算实现高分辨率，同时我们发现这还能带来更稳定的注视和更强的物体跟踪及干扰忽略能力。我们在五个需要机器人手臂周围弧形区域操作的泛光工作空间任务上评估了EyeRobot。实验结果表明，EyeRobot表现出的手眼协调行为能有效促进单摄像头下大工作空间的操控。更多视频请访问项目网站：https://www.eyerobot.net/

</details>


### [289] [Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding](https://arxiv.org/abs/2506.10756)
**中文标题：基于开放词汇目标理解的无人机视觉语言导航**

*Yuhang Zhang,Haosheng Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: The paper introduces VLFly, a framework for UAVs to perform language-guided navigation using continuous velocity commands from monocular camera inputs, integrating LLM and VLM for robust open-vocabulary goal understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in VLN, such as poor generalization to new environments and reliance on fixed action spaces, by developing a UAV-specific solution that operates without localization sensors.

Method: VLFly uses an LLM-based instruction encoder, a VLM-powered goal retriever, and a waypoint planner to generate trajectories from egocentric observations, enabling real-time UAV control.

Result: VLFly outperforms baselines in diverse simulations and real-world tasks, demonstrating robust open-vocabulary goal understanding and generalized navigation even with abstract language.

Conclusion: VLFly successfully addresses VLN challenges for UAVs, offering a scalable and sensor-efficient solution for language-guided navigation.

摘要: 视觉与语言导航（VLN）是自主机器人领域的一项长期挑战，旨在使智能体能够在复杂环境中遵循人类指令进行导航。该领域仍存在两个关键瓶颈：对分布外环境的泛化能力不足以及对固定离散动作空间的依赖。为解决这些问题，我们提出了视觉语言飞行（VLFly），这是一个专为无人机（UAV）设计的框架，用于执行语言引导的飞行任务。VLFly无需定位或主动测距传感器，仅通过机载单目摄像头捕获的自我中心观察输出连续速度指令。VLFly集成了三个模块：基于大型语言模型（LLM）的指令编码器，将高级语言重新表述为结构化提示；由视觉语言模型（VLM）驱动的目标检索器，通过视觉语言相似性将这些提示与目标图像匹配；以及路径点规划器，生成可执行轨迹以实现实时无人机控制。VLFly在多种模拟环境中进行评估，无需额外微调，始终优于所有基线方法。此外，在室内和室外环境中进行的直接和间接指令下的真实世界VLN任务表明，VLFly实现了稳健的开放词汇目标理解和泛化导航能力，即使面对抽象语言输入也能胜任。

</details>


### [290] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
**中文标题：基于数据驱动的机器人附肢与颗粒材料动态交互预测**

*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: A data-driven approach combining dimension reduction, surrogate modeling, and data assimilation significantly reduces computational time while maintaining accuracy in predicting robot-granular terrain interactions, aiding robot navigation in complex terrains.


<details>
  <summary>Details</summary>
Motivation: To improve robot navigation in granular terrains by developing a computationally efficient and accurate data-driven model that integrates offline simulation and sparse experimental data.

Method: The approach integrates dimension reduction (Sequentially Truncated Higher-Order Singular Value Decomposition), surrogate modeling (Gaussian Process), and data assimilation (Reduced Order Particle Filter) to predict interactions.

Result: The method achieves orders of magnitude reduction in computational time with comparable accuracy to high-fidelity simulations and outperforms simulations in long-horizon predictions when combined with experimental data.

Conclusion: The data-driven approach is effective for predicting robot-granular terrain interactions, offering computational efficiency and accuracy, and holds potential for broader applications beyond specific cases.

摘要: 提出并采用了一种替代的数据驱动建模方法，以获取机器人运动与颗粒地形在特定长度尺度上交互的基本见解。该方法基于维度缩减（顺序截断高阶奇异值分解）、替代建模（高斯过程）和数据同化技术（降阶粒子滤波）的集成。该方法可在线使用，并基于离线数据，包括高保真仿真数据和稀疏实验数据的离线收集。结果表明，与基于物理的高保真仿真相比，所提出的数据驱动建模方法可以显著减少计算时间。仅以仿真数据为输入时，数据驱动预测技术生成的预测结果与仿真结果具有相当的准确性。当同时输入仿真数据和稀疏物理实验测量数据时，嵌入数据同化技术的数据驱动方法在长期预测方面可能优于仅依赖高保真仿真。此外，数据驱动建模方法还能重现基于物理仿真恢复的最大阻力缩放关系，表明其具有超越个案的一般预测能力。这些结果有望帮助机器人在未知和复杂地形中的导航与探索，无论是在线还是离线阶段。

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [291] [FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](https://arxiv.org/abs/2506.10789)
**中文标题：法西斯测量仪：在线新法西斯主义话语分类器**

*Rudy Alexandro Garrido Veliz,Martin Semmann,Chris Biemann,Seid Muhie Yimam*

Main category: cs.CY

TL;DR: The paper introduces a neo-fascist coding scheme for digital discourse, develops classification models using NLP, and highlights the prevalence of neo-fascist rhetoric online.


<details>
  <summary>Details</summary>
Motivation: To address the growing threat of neo-fascism in Western societies by developing tools to detect and analyze its discourse online.

Method: Developed a neo-fascist coding scheme, collected data from neo-fascist forums, crowdsourced annotations, and fine-tuned SLMs and LLMs for classification.

Result: Successfully created the first classification models for neo-fascist discourse, revealing its prevalence in targeted forums.

Conclusion: Continued research and action against neo-fascist discourse are essential for protecting democratic societies.

摘要: 新法西斯主义是一种政治和社会意识形态，过去十年在美国及其他西方社会中显著增长。它对民主及其针对的少数群体构成严重威胁，需要采取积极行动以防止其升级。本研究首次提出了一种针对美国社会背景下数字话语的新法西斯主义编码方案，由政治学研究人员监督。我们的工作填补了自然语言处理（NLP）与政治学在这一现象之间的空白。此外，为了测试编码方案，我们收集了大量来自知名新法西斯主义团体（如Iron March和Stormfront.org论坛）的互联网活动，并将指南应用于收集到的帖子子集。通过众包，我们标注了一千条被标记为新法西斯主义或非新法西斯主义的帖子。利用这些标注数据，我们对小型语言模型（SLMs）和大型语言模型（LLMs）进行了微调和测试，获得了首批新法西斯主义话语分类模型。我们发现此类论坛中新法西斯主义言论的普遍性使其成为未来研究的理想目标。在进行NLP研究时，社会背景是新法西斯主义言论的关键考量因素。最后，为了民主社会的福祉，必须继续并加强对这类政治运动的抵制工作。免责声明：本研究专注于检测文本中的新法西斯主义内容，类似于其他仇恨言论分析，不对个人或组织进行标记。

</details>


### [292] [LLM-Driven Personalized Answer Generation and Evaluation](https://arxiv.org/abs/2506.10829)
**中文标题：基于大语言模型的个性化答案生成与评估**

*Mohammadreza Molavi,Mohammadreza Tavakoli,Mohammad Moein,Abdolali Faraji,Gábor Kismihók*

Main category: cs.CY

TL;DR: The paper explores using LLMs to generate personalized answers for online learners, showing that examples improve response quality.


<details>
  <summary>Details</summary>
Motivation: To enhance online learning by providing personalized answers tailored to individual learners' questions, reducing educator workload.

Method: Developed a framework and dataset using StackExchange, tested LLMs in 0-shot, 1-shot, and few-shot scenarios, and evaluated answers via BERTScore, LLM, and human assessments.

Result: LLMs performed better when given examples of desired answers, significantly improving personalized response quality.

Conclusion: LLMs can effectively generate personalized answers for learners, especially when provided with contextual examples.

摘要: 在线学习因其灵活性和可访问性而迅速发展。个性化，即适应个体学习者的需求，对于提升学习体验至关重要，尤其是在线环境中。个性化的一个关键方面是为学习者提供针对其具体问题的定制答案。因此，本文探讨了大语言模型（LLMs）在生成个性化答案方面的潜力，以增强学习者的参与度并减轻教育工作者的负担。为了评估LLMs在此背景下的有效性，我们利用StackExchange平台在语言学习和编程两个领域进行了全面研究。我们开发了一个框架和数据集用于验证自动生成的个性化答案。随后，我们使用不同的策略（包括零样本、单样本和少样本场景）生成了个性化答案。生成的答案通过三种方法进行评估：1. BERTScore，2. LLM评估，以及3. 人工评估。我们的研究结果表明，为LLMs提供期望答案的示例（来自学习者或类似学习者）可以显著提升LLMs为个体学习者定制回答的能力。

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [293] [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/abs/2506.10274)
**中文标题：离散音频标记：不仅仅是一篇综述！**

*Pooneh Mousavi,Gallil Maimon,Adel Moumen,Darius Petermann,Jiatong Shi,Haibin Wu,Haici Yang,Anastasia Kuznetsova,Artem Ploujnikov,Ricard Marxer,Bhuvana Ramabhadran,Benjamin Elizalde,Loren Lugosch,Jinyu Li,Cem Subakan,Phil Woodland,Minje Kim,Hung-yi Lee,Shinji Watanabe,Yossi Adi,Mirco Ravanelli*

Main category: cs.SD

TL;DR: The paper systematically reviews and benchmarks discrete audio tokenizers across speech, music, and general audio domains, proposing a taxonomy and evaluating performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing studies on discrete audio tokens lack a unified comparison across domains and tasks, prompting the need for a comprehensive review and benchmark.

Method: The paper proposes a taxonomy of tokenization approaches, evaluates tokenizers on reconstruction, downstream performance, and acoustic language modeling, and conducts ablation studies.

Result: The study identifies key limitations and trade-offs, providing insights for future research in discrete audio tokenization.

Conclusion: The paper offers a systematic framework and practical guidance for advancing research in discrete audio tokenization.

摘要: 离散音频标记是一种紧凑的表示形式，旨在保留感知质量、语音内容和说话者特征，同时实现高效的存储和推理，并在多样化的下游任务中表现出竞争力。它们为连续特征提供了实用的替代方案，使语音和音频能够集成到现代大型语言模型（LLMs）中。随着基于标记的音频处理兴趣的增长，出现了多种标记化方法，一些综述也回顾了该领域的最新进展。然而，现有研究往往专注于特定领域或任务，缺乏对各种基准的统一比较。本文对离散音频标记器进行了系统性的回顾和基准测试，涵盖语音、音乐和通用音频三个领域。我们提出了一种基于编码器-解码器、量化技术、训练范式、流式处理和应用领域的标记化方法分类法。我们在重建、下游性能和声学语言建模等多个基准上评估了标记器，并通过受控消融研究分析了权衡。我们的研究结果突出了关键限制、实际考虑和开放挑战，为这一快速发展领域的未来研究提供了见解和指导。更多信息，包括主要结果和标记器数据库，请访问我们的网站：https://poonehmousavi.github.io/dates-website/。

</details>


### [294] [Fine-Grained control over Music Generation with Activation Steering](https://arxiv.org/abs/2506.10225)
**中文标题：通过激活引导实现音乐生成的细粒度控制**

*Dipanshu Panda,Jayden Koshy Joe,Harshith M R,Swathi Narashiman,Pranay Mathur,Anish Veerakumar,Aniruddh Krishna,Keerthiharan A*

Main category: cs.SD

TL;DR: A method for fine-grained control over music generation by steering activations in MusicGen, enabling timbre, style, and genre adjustments through inference-time interventions.


<details>
  <summary>Details</summary>
Motivation: To achieve precise control over music generation, allowing for modifications like timbre transfer, style transfer, and genre fusion without retraining the model.

Method: Uses inference-time interventions on MusicGen, steering residual stream or attention layer activations via linear probes, framed as a regression task for better directional information preservation.

Result: Improved performance in fine-grained control, combining global text prompts with local activation steering for versatile music generation.

Conclusion: The method successfully provides both global and local control over music generation, enhancing flexibility and precision in music synthesis.

摘要: 我们提出了一种通过推理时干预对自回归生成音乐变换器MusicGen进行细粒度控制的方法。我们的方法通过使用线性探针训练的权重引导残差流，或以类似方式引导注意力层激活，实现了音色转换、风格转换和流派融合。我们发现将其建模为回归任务能提升性能，假设均方误差能更好地保留激活空间中的有意义方向信息。结合MusicGen中文本提示提供的全局条件，我们的方法实现了对音乐生成的全局和局部控制。展示我们方法的音频样本可在演示页面获取。

</details>


### [295] [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
**中文标题：PAL：通过大语言模型探测音频编码器——音频编码器到大语言模型的信息传递研究**

*Tony Alex,Wish Suharitdamrong,Sara Atito,Armin Mustafa,Philip J. B. Jackson,Imran Razzak,Muhammad Awais*

Main category: cs.SD

TL;DR: The paper investigates how architectural design choices in audio-LLMs affect the transfer of semantic information from audio encoders to LLMs, proposing modifications that improve performance by 10-60% over baseline.


<details>
  <summary>Details</summary>
Motivation: To explore the under-studied mechanisms of efficient semantic representation transfer from audio encoders to LLMs in Audio-LLMs, focusing on optimizing cross-modal interactions.

Method: The study modifies a standard audio-LLM architecture, testing hypotheses about delayed audio integration, probing via attention submodules, and ensemble audio encoders, using a dataset of 5.6 million audio-text pairs.

Result: Proposed modifications, including delayed audio integration and ensemble encoders, improved performance by 10-60% over baseline, validating the approach.

Conclusion: Optimizing architectural design choices in audio-LLMs significantly enhances cross-modal information transfer, with delayed integration and ensemble encoders proving particularly effective.

摘要: 将音频感知能力整合到大语言模型（LLMs）中，推动了音频-LLMs领域的重大进展。尽管面向应用的开发（特别是在为特定能力如音频推理策划训练数据方面）进展迅速，但从音频编码器到LLMs的丰富语义表征高效传递的底层机制仍未被充分探索。我们将有效的音频-LLM交互概念化为LLM能够熟练探测音频编码器表征以满足文本查询的能力。本文系统地研究了架构设计选择如何影响这一过程。从一个标准的Pengi/LLaVA风格的音频-LLM架构出发，我们提出并评估了几种基于机制可解释性研究和LLM操作原则的修改。实验表明：（1）延迟音频集成直到LLM的初始层建立文本上下文，增强了其探测音频表征以获取相关信息的能力；（2）LLM可以仅通过其注意力子模块熟练探测音频表征，无需传播到其前馈网络（FFN）子模块；（3）高效集成的多样化音频编码器提供了更丰富、互补的表征，从而拓宽了LLM探测更广泛音频信息的能力。所有假设均在一个包含560万音频-文本对的数据集上使用相同的三阶段训练课程进行评估，确保对比的受控性。最终架构整合了所有提出的修改，相对基线实现了10%至60%的性能提升，验证了我们在音频-LLMs中优化跨模态信息传递的方法。项目页面：https://ta012.github.io/PAL/

</details>


### [296] [BNMusic: Blending Environmental Noises into Personalized Music](https://arxiv.org/abs/2506.10754)
**中文标题：BNMusic：将环境噪音融入个性化音乐**

*Chi Zuo,Martin B. Møller,Pablo Martínez-Nuevo,Huayang Huang,Yu Wu,Ye Zhu*

Main category: cs.SD

TL;DR: BNMusic blends environmental noises into personalized music using a two-stage framework to reduce noise noticeability and improve acoustic experiences.


<details>
  <summary>Details</summary>
Motivation: Traditional acoustic masking often requires excessive volume to align sounds. BNMusic aims to blend noises into music more effectively using cross-modal generation.

Method: A two-stage framework: (1) synthesizes music encapsulating noise essence via mel-spectrogram, (2) adaptively amplifies music to enhance blending while preserving quality.

Result: Experiments on MusicBench, EPIC-SOUNDS, and ESC-50 show effective noise blending with rhythmically aligned and enjoyable music.

Conclusion: BNMusic successfully reduces noise noticeability and enhances acoustic experiences through personalized music blending.

摘要: 在受到环境噪音干扰时，传统的声学掩蔽技术试图用其他主导但较少干扰的声音掩盖噪音。然而，主导声音与噪音之间的不匹配（如节奏不一致）通常需要过度增加音量才能实现有效掩蔽。受跨模态生成最新进展的启发，本文提出了一种替代声学掩蔽的方法，旨在通过将噪音融入基于用户提供的文本提示生成的个性化音乐中来降低噪音的显著程度。遵循使用梅尔频谱表示的音乐生成范式，我们提出了一个将噪音融入个性化音乐（BNMusic）的两阶段框架。第一阶段合成一段完整的音乐，其梅尔频谱表示封装了噪音的音乐本质。第二阶段，我们自适应地放大生成的音乐片段，以进一步降低噪音感知并增强融合效果，同时保持听觉质量。在MusicBench、EPIC-SOUNDS和ESC-50上的综合评估实验证明了我们框架的有效性，展示了将环境噪音与节奏对齐、自适应放大且令人愉悦的音乐片段融合的能力，从而最小化噪音的显著程度，提升整体声学体验。

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [297] [Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence](https://arxiv.org/abs/2506.10925)
**中文标题：自主无线空间网络的代理语义控制：通过MCP驱动的分布式智能扩展Space-O-RAN**

*Eduardo Baena,Paolo Testolina,Michele Polese,Sergi Aliaga,Andrew Benincasa,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia*

Main category: cs.NI

TL;DR: The paper proposes extending Space-O-RAN with a semantic agentic layer using MCP and A2A protocols to enable context-aware decision-making for autonomous wireless networks in lunar operations.


<details>
  <summary>Details</summary>
Motivation: Lunar surface operations require autonomous, robust, and adaptive wireless communication systems. Current Space-O-RAN lacks semantic integration and dynamic decision-making capabilities.

Method: Introduces a semantic agentic layer with MCP and A2A protocols for distributed cognitive agents in rovers, landers, and base stations, enabling context-aware coordination and reasoning.

Result: The extension allows for delay-adaptive reasoning, bandwidth-aware semantic compression, and interaction with MCP servers for telemetry and mission constraint analysis.

Conclusion: The proposed semantic agentic layer enhances Space-O-RAN's adaptability and autonomy for lunar wireless networks.

摘要: 月球表面作业对无线通信系统提出了严格要求，包括自主性、抗干扰能力以及对环境和任务驱动的适应性。尽管Space-O-RAN提供了符合3GPP标准的分布式编排模型，但其决策逻辑仅限于静态策略且缺乏语义集成。我们提出了一种新颖的扩展，通过模型上下文协议（MCP）和代理间（A2A）通信协议实现语义代理层，支持跨实时、近实时和非实时控制层的上下文感知决策。部署在漫游车、着陆器和月球基站中的分布式认知代理实现了无线感知协调策略，包括延迟自适应推理和带宽感知语义压缩，同时与多个MCP服务器交互以分析遥测数据、运动规划和任务约束。

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [298] [Learning Chaotic Dynamics with Neuromorphic Network Dynamics](https://arxiv.org/abs/2506.10773)
**中文标题：利用神经形态网络动力学学习混沌动力学**

*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: The paper explores using a neuromorphic network with memristive elements to model chaotic dynamics, finding optimal conditions for learning through input voltage manipulation.


<details>
  <summary>Details</summary>
Motivation: To understand how neuromorphic networks, as dynamical systems, can learn and model complex chaotic dynamics using their inherent physics.

Method: Simulated a neuromorphic network with memristive components in a reservoir computing framework to predict chaotic time series by manipulating input voltages and electrodes.

Result: Optimal learning occurred when input voltages maximized memristive dynamics coverage, while increased electrode coverage suppressed less useful nonlinear responses.

Conclusion: The study offers insights into optimizing neuromorphic networks for learning complex dynamics using external controls.

摘要: 本研究探讨了如何利用神经形态网络（本身是一种动力学系统）学习和建模动力学系统。研究中使用的神经形态网络基于一个由忆阻元件组成的复杂电路，这些元件对输入电信号产生神经突触非线性响应。为了确定如何利用底层系统的物理特性进行计算，该神经形态网络在自主预测多元混沌时间序列的任务中进行了模拟和评估，采用了储备池计算框架。通过仅操纵输入电极和电压，发现当输入电压最大化忆阻元件内部动力学探索整个忆阻模型动态范围时，非线性动力学响应达到最优。此外，增加输入电极的网络覆盖范围会抑制不利于学习的其他非线性响应。这些结果为如何通过仅调整外部控制参数来优化实际神经形态网络设备以学习复杂动力学系统提供了宝贵见解。

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [299] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
**中文标题：从工具调用到符号思维：LLMs在持久Lisp元编程循环中的应用**

*Jordi de la Torre*

Main category: cs.PL

TL;DR: The paper introduces a novel architecture combining LLMs with a persistent Lisp environment, enabling dynamic tool creation and reflective programming through a live REPL.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between neural language generation and symbolic programming by allowing LLMs to interactively define and evolve tools in a stateful environment.

Method: Proposes embedding Lisp expressions within LLM generation, intercepted via middleware, to facilitate dynamic tool creation and reflective programming in a live REPL.

Result: A design framework and architectural principles for integrating symbolic programming with neural language generation in interactive AI systems.

Conclusion: The proposed architecture successfully merges symbolic and neural approaches, paving the way for more interactive and adaptable AI systems.

摘要: 我们提出了一种新颖的架构，将大型语言模型（LLMs）与一个持久的、交互式的Lisp环境相结合。这种设置使LLMs能够通过与实时REPL的程序化交互来定义、调用和演化其自身工具。通过在生成中嵌入Lisp表达式并通过中间件层拦截它们，该系统实现了有状态的外部内存、反射式编程和动态工具创建。我们提出了一个设计框架和架构原则，以指导未来将符号编程与神经语言生成相结合的交互式AI系统的实现。

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [300] [AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](https://arxiv.org/abs/2506.10312)
**中文标题：AC/DC：基于大语言模型的对话延续音频理解**

*Yusuke Fujita,Tomoya Mizumoto,Atsushi Kojima,Lianbo Liu,Yui Sudo*

Main category: eess.AS

TL;DR: The paper introduces an LLM-based audio comprehension model that uses dialogue continuation to improve caption understanding and zero-shot instruction-following without multitask training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the caption variation problem in audio comprehension by leveraging LLMs' dialogue continuation ability, enabling better understanding of captions beyond surface-level words.

Method: The method trains a model to generate responses as if input captions triggered a dialogue, rather than directly generating captions, enhancing comprehension through dialogue continuation.

Result: Experiments on AudioCaps, WavCaps, and Clotho datasets show the model's ability to follow unseen instructions in zero-shot scenarios, outperforming traditional approaches.

Conclusion: The conclusion highlights that dialogue continuation training effectively captures caption meaning and enables zero-shot instruction-following, even without multitask training.

摘要: 我们提出了一种基于指令跟随的音频理解模型，利用大语言模型（LLMs）的对话延续能力。该方法不直接生成训练数据中的目标字幕，而是训练模型产生响应，仿佛输入字幕触发了对话。这种对话延续训练缓解了字幕变异性问题。通过学习延续对话，模型能够捕捉字幕的深层含义，而不仅仅是表面词汇。因此，我们的模型无需多任务指令调优，仅通过音频字幕数据集训练即可实现零样本指令跟随能力。在AudioCaps、WavCaps和Clotho数据集上的实验，以及AudioBench音频场景问答测试，证明了模型能够遵循各种未见过的指令。

</details>


### [301] [RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding](https://arxiv.org/abs/2506.10289)
**中文标题：RT-VC：基于语音发音编码的实时零样本语音转换**

*Yisi Liu,Chenyang Wang,Hanjo Kim,Raniya Khan,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: RT-VC is a real-time zero-shot voice conversion system using articulatory features and DDSP to achieve ultra-low latency (61.4 ms) while maintaining high-quality performance.


<details>
  <summary>Details</summary>
Motivation: Voice conversion is crucial for applications like assistive communication and entertainment, but existing methods often suffer from latency issues. RT-VC aims to provide real-time, high-quality voice conversion with minimal delay.

Method: RT-VC uses an articulatory feature space to separate content and speaker characteristics, combined with differentiable digital signal processing (DDSP) for efficient vocoding, reducing latency.

Result: RT-VC achieves a CPU latency of 61.4 ms (13.3% reduction compared to SOTA) while maintaining comparable synthesis quality.

Conclusion: RT-VC demonstrates that articulatory features and DDSP can enable real-time, high-quality voice conversion with significantly reduced latency.

摘要: 语音转换已成为从辅助通信到娱乐等众多应用中的关键技术。本文提出了RT-VC，一种零样本实时语音转换系统，具有超低延迟和高质量性能。我们的方法利用发音特征空间自然解耦内容和说话人特征，从而实现更鲁棒和可解释的语音转换。此外，通过集成可微分数字信号处理（DDSP），直接从发音特征进行高效声码合成，显著降低了转换延迟。实验评估表明，在保持与当前最先进方法相当的合成质量的同时，RT-VC的CPU延迟为61.4毫秒，延迟降低了13.3%。

</details>


### [302] [Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes](https://arxiv.org/abs/2506.10653)
**中文标题：基于熵最小化和说话人编码的语音识别器鲁棒无监督自适应**

*Rogier C. van Dalen,Shucong Zhang,Titouan Parcollet,Sourav Bhattacharya*

Main category: eess.AS

TL;DR: The paper proposes a robust adaptation method for speech recognisers using entropy minimisation and speaker codes, achieving significant improvements in word error rates with minimal adaptation data.


<details>
  <summary>Details</summary>
Motivation: Speech recognisers often underperform in new environments due to limited adaptation data, which is typically unlabelled. The paper aims to make adaptation robust even with just one minute of data.

Method: The method introduces a novel loss function based on conditional entropy over multiple hypotheses (instead of a single pseudo-label) and uses a compact 'speaker code' to characterise speakers efficiently.

Result: On a noise-augmented dataset, the method achieved a 20% relative improvement in word error rate with one minute of data, increasing to 29% with 10 minutes.

Conclusion: The proposed combination of entropy minimisation and speaker codes effectively enhances the robustness of speech recogniser adaptation to new speakers with minimal data.

摘要: 语音识别器通常在特定环境下表现最佳，需要适应新环境才能发挥良好性能。对于新说话人的适应，通常数据量过少且无标签，导致微调不够鲁棒。本文提出了一种组合方法，使仅需一分钟数据的适应变得鲁棒。首先，本文提出了一种新的损失函数，即基于完整假设的条件熵，而非仅依赖于单一易错的假设或“伪标签”。使用多假设使适应对初始识别错误更具鲁棒性。其次，一种“说话人编码”以向量形式表征说话人，其长度足够短，仅需少量数据即可估计。在远场噪声增强版的Common Voice数据集上，所提方案在一分钟适应数据上实现了20%的词错误率相对改进，十分钟数据上提升至29%。

</details>
