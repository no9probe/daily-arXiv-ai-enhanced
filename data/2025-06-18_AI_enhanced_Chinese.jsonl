{"id": "2506.13796", "pdf": "https://arxiv.org/pdf/2506.13796", "abs": "https://arxiv.org/abs/2506.13796", "authors": ["Zhou Chen", "Xiao Wang", "Yuanhong Liao", "Ming Lin", "Yuqi Bai"], "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables", "summary": "As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6784\u5efa\u6c14\u5019\u6307\u4ee4\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u4e86\u540d\u4e3aClimateChat\u7684LLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c14\u5019\u53d8\u5316\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5168\u7403\u6c14\u5019\u53d8\u5316\u95ee\u9898\u65e5\u76ca\u4e25\u5cfb\uff0c\u5bf9\u6c14\u5019\u79d1\u5b66\u7814\u7a76\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u6548\u751f\u6210\u9ad8\u7cbe\u5ea6\u6c14\u5019\u6307\u4ee4\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u6c14\u5019\u53d8\u5316LLM\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u4e8b\u5b9e\u548c\u80cc\u666f\u77e5\u8bc6\u751f\u6210\u6307\u4ee4\uff0c\u7ed3\u5408\u7f51\u7edc\u722c\u53d6\u548c\u79cd\u5b50\u6307\u4ee4\u6536\u96c6\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\uff0c\u6784\u5efa\u4e86ClimateChat-Corpus\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u5f00\u6e90LLM\u3002", "result": "ClimateChat\u5728\u6c14\u5019\u53d8\u5316\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u6570\u636e\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u6c14\u5019\u6307\u4ee4\u6570\u636e\u548c\u8bad\u7ec3\u6c14\u5019\u4e13\u7528LLM\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u9009\u62e9\u5408\u9002\u57fa\u7840\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "paper_title_zh": "ClimateChat\uff1a\u8bbe\u8ba1\u6570\u636e\u548c\u65b9\u6cd5\u7528\u4e8e\u6307\u4ee4\u8c03\u4f18LLM\u4ee5\u56de\u7b54\u6c14\u5019\u53d8\u5316\u67e5\u8be2", "abstract_zh": "\u968f\u7740\u5168\u7403\u6c14\u5019\u53d8\u5316\u95ee\u9898\u65e5\u76ca\u4e25\u5cfb\uff0c\u6c14\u5019\u79d1\u5b66\u7814\u7a76\u7684\u9700\ufffd\ufffd\u4e0d\u65ad\u589e\u957f\u3002\u4ee5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u4ee3\u8868\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6c14\u5019\u53d8\u5316\u76f8\u5173\u7814\u7a76\uff0c\u4e3a\u51b3\u7b56\u8005\u548c\u516c\u4f17\u63d0\u4f9b\u91cd\u8981\u4fe1\u606f\u652f\u6301\u3002\u4e00\u4e9b\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u6c14\u5019\u53d8\u5316\u76f8\u5173\u6307\u4ee4\u6570\u636e\u5e76\u5bf9LLM\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u76ee\u524d\u7814\u7a76\u5728\u9ad8\u6548\u751f\u6210\u5927\u91cf\u9ad8\u7cbe\u5ea6\u6c14\u5019\u6307\u4ee4\u6570\u636e\u65b9\u9762\u4ecd\u663e\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6c14\u5019\u53d8\u5316LLM\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6784\u5efa\u6307\u4ee4\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u4e8b\u5b9e\u548c\u80cc\u666f\u77e5\u8bc6\u751f\u6210\u6307\u4ee4\uff0c\u5e76\u7ed3\u5408\u7f51\u7edc\u722c\u53d6\u548c\u79cd\u5b50\u6307\u4ee4\u6536\u96c6\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u3002\u5229\u7528\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aClimateChat-Corpus\u7684\u6c14\u5019\u53d8\u5316\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u5f00\u6e90LLM\uff0c\u6700\u7ec8\u5f97\u5230\u540d\u4e3aClimateChat\u7684LLM\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cClimateChat\u5728\u6c14\u5019\u53d8\u5316\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u6570\u636e\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u9002\u5e94\u5e7f\u6cdb\u6c14\u5019\u53d8\u5316\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u9009\u62e9\u5408\u9002\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u7684\u91cd\u8981\u6027\u3002\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u6c14\u5019\u6307\u4ee4\u6570\u636e\u548c\u8bad\u7ec3\u6c14\u5019\u4e13\u7528LLM\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2506.13886", "pdf": "https://arxiv.org/pdf/2506.13886", "abs": "https://arxiv.org/abs/2506.13886", "authors": ["Antara Raaghavi Bhattacharya", "Isabel Papadimitriou", "Kathryn Davidson", "David Alvarez-Melis"], "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty + three\"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u6570\u5b57\u8c1c\u9898\u4e2d\u96be\u4ee5\u7ed3\u5408\u8bed\u8a00\u548c\u6570\u5b66\u63a8\u7406\uff0c\u9664\u975e\u6570\u5b66\u7b26\u53f7\u88ab\u660e\u786e\u6807\u8bb0\u3002\u4eba\u7c7b\u80fd\u7075\u6d3b\u63a8\u65ad\u6570\u5b57\u7684\u9690\u542b\u7ed3\u6784\uff0c\u800cLLMs\u5219\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002", "motivation": "\u4e0d\u540c\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u6784\u9020\u65b9\u5f0f\u591a\u6837\uff0c\u4eba\u7c7b\u80fd\u8f7b\u677e\u9002\u5e94\uff0c\u4f46LLMs\u5728\u5904\u7406\u6d89\u53ca\u8de8\u8bed\u8a00\u6570\u5b57\u7cfb\u7edf\u7684\u8bed\u8a00-\u6570\u5b66\u8c1c\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76LLMs\u4e3a\u4f55\u96be\u4ee5\u5b8c\u6210\u6b64\u7c7b\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u5206\u79bb\u8bed\u8a00\u548c\u6570\u5b66\u5728\u6570\u5b57\u8868\u8fbe\u4e2d\u7684\u4f5c\u7528\uff0c\u6d4b\u8bd5LLMs\u5728\u660e\u786e\u6807\u8bb0\u6570\u5b66\u7b26\u53f7\u548c\u672a\u6807\u8bb0\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u6570\u5b57\u6784\u9020\u548c\u7ec4\u5408\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9664\u975e\u6570\u5b66\u7b26\u53f7\uff08\u5982\u201c+\u201d\u201c\u00d7\u201d\uff09\u88ab\u660e\u786e\u6807\u8bb0\uff0c\u5426\u5219LLMs\u65e0\u6cd5\u7a33\u5b9a\u89e3\u51b3\u95ee\u9898\u3002LLMs\u7f3a\u4e4f\u4eba\u7c7b\u5bf9\u6570\u5b57\u9690\u542b\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u96be\u4ee5\u4ece\u4eba\u7c7b\u89c4\u6a21\u7684\u6570\u636e\u4e2d\u7075\u6d3b\u63a8\u65ad\u9690\u542b\u7684\u7ec4\u5408\u89c4\u5219\uff0c\u8fd9\u662f\u5176\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002", "paper_title_zh": "\u5229\u7528\u591a\u8bed\u8a00\u6570\u5b57\u8c1c\u9898\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u4e0e\u6570\u5b66\u63a8\u7406\u4ea4\u4e92", "abstract_zh": "\u4e0d\u540c\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u5728\u6784\u9020\u548c\u7ec4\u5408\u6570\u5b57\u65f6\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u5c3d\u7ba1\u4eba\u7c7b\u80fd\u8f7b\u677e\u9002\u5e94\u8fd9\u79cd\u591a\u6837\u6027\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u6d89\u53ca\u8de8\u8bed\u8a00\u6570\u5b57\u7cfb\u7edf\u7684\u8bed\u8a00-\u6570\u5b66\u8c1c\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u63a2\u7a76\u4e86LLMs\u96be\u4ee5\u5b8c\u6210\u6b64\u7c7b\u4efb\u52a1\u7684\u539f\u56e0\uff0c\u5206\u79bb\u4e86\u8bed\u8a00\u548c\u6570\u5b66\u5728\u6570\u5b57\u8868\u8fbe\u4e2d\u7684\u4f5c\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9664\u975e\u6570\u5b66\u8fd0\u7b97\u7b26\u53f7\uff08\u5982\u201c\u4e8c\u5341 + \u4e09\u201d\u4e2d\u7684\u201c+\u201d\uff09\u88ab\u660e\u786e\u6807\u8bb0\uff0c\u5426\u5219\u6a21\u578b\u65e0\u6cd5\u7a33\u5b9a\u89e3\u51b3\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u7684\u6d88\u878d\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u6570\u5b57\u6784\u9020\u548c\u7ec4\u5408\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4eba\u7c7b\u80fd\u5229\u7528\u5bf9\u6570\u5b57\u7684\u8bed\u8a00\u7406\u89e3\u63a8\u65ad\u5176\u9690\u542b\u7684\u7ec4\u5408\u7ed3\u6784\uff0c\u800cLLMs\u4f3c\u4e4e\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff1a\u5f53\u524d\u63a8\u7406\u6a21\u578b\u96be\u4ee5\u4ece\u4eba\u7c7b\u89c4\u6a21\u7684\u6570\u636e\u4e2d\u7075\u6d3b\u63a8\u65ad\u9690\u542b\u7684\u7ec4\u5408\u89c4\u5219\uff0c\u8fd9\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2506.13888", "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6VL-GenRM\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u4e13\u5bb6\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u62d2\u7edd\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u81ea\u4e3e\u56f0\u5883\u548c\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\uff08VL-RM\uff09\u7684\u8bad\u7ec3\u9762\u4e34\u81ea\u4e3e\u56f0\u5883\u548c\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\u53d7\u9650\uff0c\u8fdb\u4e00\u6b65\u5f71\u54cd\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u4e13\u5bb6\u548c\u8fed\u4ee3\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347VL-RM\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6VL-GenRM\uff0c\u7ed3\u5408\u89c6\u89c9\u4e13\u5bb6\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u62d2\u7edd\u91c7\u6837\uff0c\u4f18\u5316\u504f\u597d\u6570\u636e\u96c6\u751f\u6210\uff0c\u589e\u5f3a\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u9010\u6b65\u63d0\u5347\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVL-GenRM\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VL-GenRM\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u4e13\u5bb6\u548c\u8fed\u4ee3\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "VL-GenRM\uff1a\u901a\u8fc7\u89c6\u89c9\u4e13\u5bb6\u4e0e\u8fed\u4ee3\u8bad\u7ec3\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u9a8c\u8bc1", "abstract_zh": "\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u6539\u8fdb\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5728\u89c6\u89c9\u8bed\u8a00\uff08VL\uff09\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u8f83\u5c11\u3002\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\uff08VL-RM\uff09\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u5bf9VL\u6a21\u578b\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8bad\u7ec3\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u81ea\u4e3e\u56f0\u5883\uff0c\u5373\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4f9d\u8d56\u4e8e\u5df2\u6709\u5f3aVL\u6a21\u578b\uff0c\u5bfc\u81f4\u81ea\u6211\u751f\u6210\u7684\u76d1\u7763\u5f3a\u5316\u73b0\u6709\u504f\u5dee\uff1b\u4e8c\u662f\u6a21\u6001\u504f\u5dee\u548c\u8d1f\u4f8b\u653e\u5927\u95ee\u9898\uff0c\u5373VL\u6a21\u578b\u53ef\u80fd\u5e7b\u89c9\u9519\u8bef\u7684\u89c6\u89c9\u5c5e\u6027\uff0c\u751f\u6210\u6709\u7f3a\u9677\u7684\u504f\u597d\u6570\u636e\uff0c\u8fdb\u4e00\u6b65\u8bef\u5bfc\u8bad\u7ec3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u4e13\u5bb6\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u62d2\u7edd\u91c7\u6837\uff0c\u4f18\u5316\u504f\u597d\u6570\u636e\u96c6\uff0c\u589e\u5f3a\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\u5728\u591a\u4e2aVL-RM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684VL\u6a21\u578b\u5bf9\u9f50\u3002"}}
{"id": "2506.13894", "pdf": "https://arxiv.org/pdf/2506.13894", "abs": "https://arxiv.org/abs/2506.13894", "authors": ["Ryuki Matsuura", "Shikhar Bharadwaj", "Jiarui Liu", "Dhatchi Kunde Govindarajan"], "title": "EmoNews: A Spoken Dialogue System for Expressive News Conversations", "categories": ["cs.CL"], "comment": null, "summary": "We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u60c5\u611f\u8bed\u97f3\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edfEmoNews\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7ebf\u7d22\u8c03\u8282\u60c5\u611f\u8bed\u97f3\uff0c\u4ee5\u63d0\u5347\u65b0\u95fb\u5bf9\u8bdd\u7684\u5171\u60c5\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u60c5\u611f\u8c03\u8282\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "motivation": "\u5c3d\u7ba1\u60c5\u611f\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u6280\u672f\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u4efb\u52a1\u5bfc\u5411\u7684\u60c5\u611f\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u4ecd\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u5bf9\u8bdd\u7cfb\u7edf\u4e0e\u60c5\u611fTTS\u7814\u7a76\u7684\u5206\u79bb\u4ee5\u53ca\u7f3a\u4e4f\u793e\u4ea4\u76ee\u6807\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u65b0\u95fb\u5bf9\u8bdd\u7684\u60c5\u611f\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5229\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u60c5\u611f\u5206\u6790\u5668\u8bc6\u522b\u5408\u9002\u7684\u60c5\u611f\uff0c\u5e76\u901a\u8fc7PromptTTS\u5408\u6210\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u60c5\u611f\u8bed\u97f3\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u60c5\u611f\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e3b\u89c2\u8bc4\u4f30\u91cf\u8868\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u60c5\u611f\u5bf9\u8bdd\u7cfb\u7edf\u5728\u60c5\u611f\u8c03\u8282\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u8bed\u97f3\u60c5\u611f\u5bf9\u63d0\u5347\u5bf9\u8bdd\u53c2\u4e0e\u5ea6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bed\u97f3\u60c5\u611f\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u60c5\u611f\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5f00\u6e90\u5de5\u5177\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "paper_title_zh": "EmoNews\uff1a\u4e00\u79cd\u7528\u4e8e\u60c5\u611f\u5316\u65b0\u95fb\u5bf9\u8bdd\u7684\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf", "abstract_zh": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\uff08SDS\uff09\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7ebf\u7d22\u8c03\u8282\u60c5\u611f\u8bed\u97f3\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5177\u5171\u60c5\u529b\u7684\u65b0\u95fb\u5bf9\u8bdd\u3002\u5c3d\u7ba1\u60c5\u611f\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u6280\u672f\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u5bf9\u8bdd\u7cfb\u7edf\u4e0e\u60c5\u611fTTS\u7814\u7a76\u7684\u5206\u79bb\u4ee5\u53ca\u7f3a\u4e4f\u793e\u4ea4\u76ee\u6807\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u4efb\u52a1\u5bfc\u5411\u7684\u60c5\u611fSDS\u7814\u7a76\u4ecd\u4e0d\u8db3\u3002\u6211\u4eec\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u7528\u4e8e\u65b0\u95fb\u5bf9\u8bdd\u7684\u60c5\u611fSDS\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u60c5\u611f\u5206\u6790\u5668\u8bc6\u522b\u5408\u9002\u7684\u60c5\u611f\uff0c\u5e76\u901a\u8fc7PromptTTS\u5408\u6210\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u60c5\u611f\u8bed\u97f3\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u60c5\u611fSDS\u7684\u4e3b\u89c2\u8bc4\u4f30\u91cf\u8868\uff0c\u5e76\u8bc4\u4f30\u4e86\u6240\u63d0\u7cfb\u7edf\u548c\u57fa\u7ebf\u7cfb\u7edf\u7684\u60c5\u611f\u8c03\u8282\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u60c5\u611fSDS\u5728\u60c5\u611f\u8c03\u8282\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u8bed\u97f3\u60c5\u611f\u5bf9\u63d0\u5347\u5bf9\u8bdd\u53c2\u4e0e\u5ea6\u7684\u91cd\u8981\u6027\u3002\u6240\u6709\u6e90\u4ee3\u7801\u5df2\u5728https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1\u5f00\u6e90\u3002"}}
{"id": "2506.13769", "pdf": "https://arxiv.org/pdf/2506.13769", "abs": "https://arxiv.org/abs/2506.13769", "authors": ["Filippo Leveni"], "title": "Non-planar Object Detection and Identification by Features Matching and Triangulation Growth", "categories": ["cs.CV", "cs.AI"], "comment": "Master's thesis at Politecnico di Milano", "summary": "Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5339\u914d\u548c\u4e09\u89d2\u5256\u5206\u589e\u957f\u7684\u975e\u5e73\u9762\u7269\u4f53\u68c0\u6d4b\u4e0e\u8bc6\u522b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51e0\u4f55\u6a21\u578b\uff08\u5982\u5355\u5e94\u6027\uff09\u4e0d\u9002\u7528\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u8bc6\u522b\u7269\u4f53\u3002", "motivation": "\u7269\u4f53\u68c0\u6d4b\u4e0e\u8bc6\u522b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u57fa\u7840\u8bfe\u9898\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u76ee\u6807\u8ddf\u8e2a\u3001\u5de5\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u548c\u56fe\u50cf\u68c0\u7d22\u7b49\u573a\u666f\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u5e73\u9762\u7269\u4f53\u6216\u5e73\u9762\u7269\u4f53\u51fa\u73b0\u53d8\u5f62\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u6a21\u677f\u7279\u5f81\u7684Delaunay\u4e09\u89d2\u5256\u5206\u4f5c\u4e3a\u6307\u5bfc\uff0c\u91c7\u7528\u8fed\u4ee3\u65b9\u6cd5\u9010\u6b65\u5339\u914d\u56fe\u50cf\u4e0e\u6a21\u677f\u7684\u7279\u5f81\u3002\u5c06\u4e09\u89d2\u5256\u5206\u89c6\u4e3a\u56fe\u7ed3\u6784\uff0c\u4ece\u5355\u4e2a\u4e09\u89d2\u5f62\u5f00\u59cb\uff0c\u9010\u6b65\u8bc4\u4f30\u90bb\u57df\u8282\u70b9\u7684\u7279\u5f81\u5339\u914d\uff0c\u57fa\u4e8e\u51e0\u4f55\u548c\u5149\u5ea6\u4e00\u81f4\u6027\u6807\u51c6\u51b3\u5b9a\u662f\u5426\u5206\u7ec4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u53d8\u5f62\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u5355\u5e94\u6027\u7684RANSAC\u6027\u80fd\u76f8\u5f53\uff1b\u800c\u5728\u53d8\u5f62\u663e\u8457\u65f6\uff0c\u5176\u63cf\u8ff0\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u975e\u5e73\u9762\u6216\u53d8\u5f62\u5e73\u9762\u7269\u4f53\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u7279\u5f81\u5339\u914d\u4e0e\u4e09\u89d2\u5256\u5206\u589e\u957f\u7684\u975e\u5e73\u9762\u7269\u4f53\u68c0\u6d4b\u4e0e\u8bc6\u522b", "abstract_zh": "\u7269\u4f53\u68c0\u6d4b\u4e0e\u8bc6\u522b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u57fa\u7840\u8bfe\u9898\uff0c\u5728\u76ee\u6807\u8ddf\u8e2a\u3001\u5de5\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u548c\u56fe\u50cf\u68c0\u7d22\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u5206\u7ec4\u56fe\u50cf\u4e0e\u6a21\u677f\u4e4b\u95f4\u7684\u7279\u5f81\u5339\u914d\u6765\u68c0\u6d4b\u548c\u8bc6\u522b\u573a\u666f\u56fe\u50cf\u4e2d\u6a21\u677f\u7684\u53d8\u5f62\u5b9e\u4f8b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u6a21\u677f\u7279\u5f81\u7684Delaunay\u4e09\u89d2\u5256\u5206\u4f5c\u4e3a\u6307\u5bfc\u5de5\u5177\uff0c\u5c06\u5176\u89c6\u4e3a\u56fe\u7ed3\u6784\uff0c\u4ece\u5355\u4e2a\u4e09\u89d2\u5f62\u51fa\u53d1\uff0c\u9010\u6b65\u8bc4\u4f30\u90bb\u57df\u8282\u70b9\u53ca\u5176\u5bf9\u5e94\u7279\u5f81\u7684\u5339\u914d\u60c5\u51b5\u3002\u5339\u914d\u8bc4\u4f30\u57fa\u4e8e\u5c40\u90e8\u51e0\u4f55\u548c\u5149\u5ea6\u4e00\u81f4\u6027\u6807\u51c6\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u51e0\u4f55\u6a21\u578b\uff08\u5982\u5355\u5e94\u6027\uff09\u4e0d\u9002\u7528\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u7269\u4f53\uff0c\u4ece\u800c\u652f\u6301\u975e\u5e73\u9762\u6216\u53d8\u5f62\u5e73\u9762\u7269\u4f53\u7684\u68c0\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u53d8\u5f62\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u5355\u5e94\u6027\u7684RANSAC\u6027\u80fd\u76f8\u5f53\uff1b\u800c\u5728\u53d8\u5f62\u663e\u8457\u65f6\uff0c\u5176\u63cf\u8ff0\u6027\u80fd\u66f4\u4f18\u3002"}}
{"id": "2506.13768", "pdf": "https://arxiv.org/pdf/2506.13768", "abs": "https://arxiv.org/abs/2506.13768", "authors": ["Stefan Reimann"], "title": "'Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra", "categories": ["cs.AI"], "comment": "27 pages, 6 figures, journal article (accepted)", "summary": "This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ed3\u5408\u4ee3\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8868\u793a\u548c\u8ba1\u7b97\u4fe1\u606f\u9879\uff0c\u652f\u6301\u7a7a\u95f4\u8ba1\u7b97\u539f\u5219\u5e76\u4e0e\u8ba4\u77e5\u79d1\u5b66\u4e2d\u5173\u4e8e\u8bb0\u5fc6\u7684\u5b9e\u8bc1\u53d1\u73b0\u4e00\u81f4\u3002\u901a\u8fc7\u975e\u7ed3\u5408\u6346\u7ed1\u64cd\u4f5c\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7a00\u758f\u8868\u793a\u4efb\u610f\u957f\u5e8f\u5217\u5e76\u4fdd\u6301\u5176\u65f6\u95f4\u7ed3\u6784\uff0c\u540c\u65f6\u566a\u58f0\u6210\u4e3a\u987a\u5e8f\u4fe1\u606f\u7684\u7ec4\u6210\u90e8\u5206\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u7ed3\u5408\u6346\u7ed1\u64cd\u4f5c\u65f6\u901a\u5e38\u4f1a\u4e22\u5931\u987a\u5e8f\u4fe1\u606f\uff0c\u9700\u8981\u989d\u5916\u7ed3\u6784\uff08\u5982\u4f4d\u7f6e\u6807\u8bb0\uff09\u6765\u8868\u793a\u5bf9\u8ba4\u77e5\u4efb\u52a1\u91cd\u8981\u7684\u5e8f\u5217\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u975e\u7ed3\u5408\u4ee3\u6570\u6846\u67b6\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u6a21\u62df\u8ba4\u77e5\u5b9e\u9a8c\u4e2d\u7684\u5e8f\u5217\u4f4d\u7f6e\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ed3\u5408\u4ee3\u6570\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u4f3c\u4e58\u6cd5\u7684\u7ed1\u5b9a\u64cd\u4f5c\u548c\u975e\u7ed3\u5408\u5e72\u6270\u5f0f\u6346\u7ed1\u8fdb\u884c\u8ba1\u7b97\u3002\u8be5\u6846\u67b6\u751f\u6210\u4e24\u79cd\u72b6\u6001\uff1aL\u72b6\u6001\uff08\u5de6\u7ed3\u5408\u6346\u7ed1\uff09\u5f3a\u8c03\u8fd1\u56e0\u6548\u5e94\uff0cR\u72b6\u6001\uff08\u53f3\u7ed3\u5408\u6346\u7ed1\uff09\u7f16\u7801\u6709\u9650\u5e8f\u5217\u6216\u5757\uff0c\u6355\u83b7\u9996\u56e0\u6548\u5e94\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u590d\u5236\u5e8f\u5217\u4f4d\u7f6e\u66f2\u7ebf\uff0c\u53cd\u6620\u8ba4\u77e5\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\u7684\u8fd1\u56e0\u548c\u9996\u56e0\u6548\u5e94\u3002\u566a\u58f0\u6210\u4e3a\u987a\u5e8f\u4fe1\u606f\u7684\u7ec4\u6210\u90e8\u5206\u800c\u975e\u5e72\u6270\u56e0\u7d20\uff0c\u4e14\u68c0\u7d22\u51c6\u786e\u6027\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u72b6\u6001\u4e0e\u7ebf\u7d22\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002", "conclusion": "\u975e\u7ed3\u5408\u4ee3\u6570\u6846\u67b6\u80fd\u591f\u6709\u6548\u8868\u793a\u548c\u8ba1\u7b97\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4fe1\u606f\u9879\uff0c\u4fdd\u6301\u5e8f\u5217\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u5e76\u6a21\u62df\u8ba4\u77e5\u5b9e\u9a8c\u4e2d\u7684\u8bb0\u5fc6\u6548\u5e94\u3002L\u72b6\u6001\u548cR\u72b6\u6001\u5206\u522b\u4e0e\u77ed\u671f\u8bb0\u5fc6\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u795e\u7ecf\u673a\u5236\u76f8\u5173\u3002", "paper_title_zh": "\u4ece\u51e0\u4e4e\u65e0\u5230\u6709\u7684\u201c\u8bb0\u5fc6\u72b6\u6001\u201d\uff1a\u975e\u7ed3\u5408\u4ee3\u6570\u4e2d\u7684\u8868\u793a\u4e0e\u8ba1\u7b97", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ed3\u5408\u4ee3\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8868\u793a\u548c\u8ba1\u7b97\u4fe1\u606f\u9879\u3002\u8be5\u6846\u67b6\u4e0e\u7a7a\u95f4\u8ba1\u7b97\u539f\u5219\u4e00\u81f4\uff0c\u5e76\u7b26\u5408\u8ba4\u77e5\u79d1\u5b66\u4e2d\u5173\u4e8e\u8bb0\u5fc6\u7684\u5b9e\u8bc1\u53d1\u73b0\u3002\u8ba1\u7b97\u901a\u8fc7\u7c7b\u4f3c\u4e58\u6cd5\u7684\u7ed1\u5b9a\u64cd\u4f5c\u548c\u975e\u7ed3\u5408\u5e72\u6270\u5f0f\u6346\u7ed1\u8fdb\u884c\u3002\u4f9d\u8d56\u7ed3\u5408\u6346\u7ed1\u7684\u6a21\u578b\u901a\u5e38\u4f1a\u4e22\u5931\u987a\u5e8f\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528\u8f85\u52a9\u987a\u5e8f\u7ed3\u6784\uff08\u5982\u4f4d\u7f6e\u6807\u8bb0\uff09\u6765\u8868\u793a\u5bf9\u8ba4\u77e5\u4efb\u52a1\u91cd\u8981\u7684\u5e8f\u5217\u4fe1\u606f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u63d0\u51fa\u7684\u975e\u7ed3\u5408\u6346\u7ed1\u64cd\u4f5c\u5141\u8bb8\u6784\u5efa\u7a00\u758f\u8868\u793a\uff0c\u4fdd\u6301\u4efb\u610f\u957f\u5e8f\u5217\u7684\u65f6\u95f4\u7ed3\u6784\u3002\u5728\u6b64\u64cd\u4f5c\u4e2d\uff0c\u566a\u58f0\u662f\u987a\u5e8f\u4fe1\u606f\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u800c\u975e\u63a9\u76d6\u624b\u6bb5\u3002\u975e\u7ed3\u5408\u6027\u8d28\u5bfc\u81f4\u5355\u4e2a\u5e8f\u5217\u7531\u4e24\u79cd\u4e0d\u540c\u72b6\u6001\u8868\u793a\uff1aL\u72b6\u6001\uff08\u5de6\u7ed3\u5408\u6346\u7ed1\uff09\u6301\u7eed\u66f4\u65b0\u5e76\u5f3a\u8c03\u8fd1\u56e0\u6548\u5e94\uff0cR\u72b6\u6001\uff08\u53f3\u7ed3\u5408\u6346\u7ed1\uff09\u7f16\u7801\u6709\u9650\u5e8f\u5217\u6216\u5757\uff0c\u6355\u83b7\u9996\u56e0\u6548\u5e94\u3002\u8fd9\u4e9b\u72b6\u6001\u7684\u6784\u5efa\u53ef\u80fd\u4e0e\u524d\u989d\u53f6\u76ae\u5c42\uff08\u77ed\u671f\u8bb0\u5fc6\uff09\u548c\u6d77\u9a6c\u7f16\u7801\uff08\u957f\u671f\u8bb0\u5fc6\uff09\u7684\u6d3b\u52a8\u76f8\u5173\u3002\u68c0\u7d22\u51c6\u786e\u6027\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u72b6\u6001\u4e0e\u7ebf\u7d22\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u51b3\u7b56\u8fc7\u7a0b\u3002\u8be5\u6a21\u578b\u80fd\u591f\u590d\u5236\u5e8f\u5217\u4f4d\u7f6e\u66f2\u7ebf\uff0c\u53cd\u6620\u8ba4\u77e5\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\u7684\u8fd1\u56e0\u548c\u9996\u56e0\u6548\u5e94\u3002"}}
{"id": "2506.13901", "pdf": "https://arxiv.org/pdf/2506.13901", "abs": "https://arxiv.org/abs/2506.13901", "authors": ["Abhilekh Borah", "Chhavi Sharma", "Danush Khanna", "Utkarsh Bhatt", "Gurpreet Singh", "Hasnat Md Abdullah", "Raghav Kaushik Ravi", "Vinija Jain", "Jyoti Patel", "Shubham Singh", "Vasu Sharma", "Arpita Vats", "Rahul Raja", "Aman Chadha", "Amitava Das"], "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6307\u6807\u2014\u2014\u5bf9\u9f50\u8d28\u91cf\u6307\u6570\uff08AQI\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5185\u5728\u5bf9\u9f50\u8d28\u91cf\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u6fc0\u6d3b\u5206\u79bb\u5206\u6790\uff0c\u5f25\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u76f2\u70b9\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u5165\u6559\u80b2\u3001\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u5176\u884c\u4e3a\u5fc5\u987b\u53ef\u9760\u5730\u53cd\u6620\u4eba\u7c7b\u5bf9\u9f50\u7684\u4ef7\u503c\u89c2\u548c\u5b89\u5168\u7ea6\u675f\u3002\u7136\u800c\uff0c\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u884c\u4e3a\u4ee3\u7406\u6307\u6807\uff08\u5982\u62d2\u7edd\u7387\u3001G-Eval\u5206\u6570\u548c\u6bd2\u6027\u5206\u7c7b\u5668\uff09\uff0c\u5b58\u5728\u660e\u663e\u76f2\u70b9\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u5bf9\u9f50\u4f2a\u9020\u3001\u8d8a\u72f1\u653b\u51fb\u7b49\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51faAQI\u6307\u6807\uff0c\u7ed3\u5408Davies-Bouldin\u5206\u6570\uff08DBS\uff09\u3001Dunn\u6307\u6570\uff08DI\uff09\u3001Xie-Beni\u6307\u6570\uff08XBI\uff09\u548cCalinski-Harabasz\u6307\u6570\uff08CHI\uff09\u7b49\u591a\u79cd\u805a\u7c7b\u8d28\u91cf\u5ea6\u91cf\uff0c\u5206\u6790\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u6fc0\u6d3b\u7684\u5206\u79bb\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86LITMUS\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAQI\u80fd\u591f\u63ed\u793a\u4f20\u7edf\u62d2\u7edd\u6307\u6807\u5ffd\u7565\u7684\u6f0f\u6d1e\uff0c\u5e76\u4e0e\u5916\u90e8\u8bc4\u4f30\u7ed3\u679c\u76f8\u5173\u3002\u5728\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982DPO\u3001GRPO\u3001RLHF\uff09\u7684\u6a21\u578b\u4e0a\u6d4b\u8bd5LITMUS\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86AQI\u7684\u6709\u6548\u6027\u3002", "conclusion": "AQI\u4f5c\u4e3a\u4e00\u79cd\u5185\u5728\u5bf9\u9f50\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u68c0\u6d4b\u9690\u85cf\u7684\u5bf9\u9f50\u95ee\u9898\u548c\u8d8a\u72f1\u98ce\u9669\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\u3002\u5176\u516c\u5f00\u5b9e\u73b0\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002", "paper_title_zh": "\u5bf9\u9f50\u8d28\u91cf\u6307\u6570\uff08AQI\uff09\uff1a\u8d85\u8d8a\u62d2\u7edd\u2014\u2014\u901a\u8fc7\u6f5c\u5728\u51e0\u4f55\u3001\u805a\u7c7b\u5206\u79bb\u548c\u5206\u5c42\u6c60\u5316\u8868\u793a\u7684\u5185\u5728\u5bf9\u9f50\u8bca\u65ad", "abstract_zh": "\u5bf9\u9f50\u4e0d\u518d\u662f\u5962\u4f88\u54c1\uff0c\u800c\u662f\u5fc5\u9700\u54c1\u3002\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u5165\u6559\u80b2\u3001\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u5176\u884c\u4e3a\u5fc5\u987b\u53ef\u9760\u5730\u53cd\u6620\u4eba\u7c7b\u5bf9\u9f50\u7684\u4ef7\u503c\u89c2\u548c\u5b89\u5168\u7ea6\u675f\u3002\u7136\u800c\uff0c\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u884c\u4e3a\u4ee3\u7406\u6307\u6807\uff08\u5982\u62d2\u7edd\u7387\u3001G-Eval\u5206\u6570\u548c\u6bd2\u6027\u5206\u7c7b\u5668\uff09\uff0c\u5b58\u5728\u660e\u663e\u76f2\u70b9\u3002\u5bf9\u9f50\u6a21\u578b\u5f80\u5f80\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u3001\u751f\u6210\u968f\u673a\u6027\u548c\u5bf9\u9f50\u4f2a\u9020\u7684\u5f71\u54cd\u3002\n\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u9f50\u8d28\u91cf\u6307\u6570\uff08AQI\uff09\u3002\u8fd9\u4e00\u65b0\u9896\u7684\u51e0\u4f55\u548c\u63d0\u793a\u4e0d\u53d8\u6307\u6807\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u6fc0\u6d3b\u7684\u5206\u79bb\u60c5\u51b5\uff0c\u5b9e\u8bc1\u8bc4\u4f30LLM\u7684\u5bf9\u9f50\u8d28\u91cf\u3002\u7ed3\u5408Davies-Bouldin\u5206\u6570\uff08DBS\uff09\u3001Dunn\u6307\u6570\uff08DI\uff09\u3001Xie-Beni\u6307\u6570\uff08XBI\uff09\u548cCalinski-Harabasz\u6307\u6570\uff08CHI\uff09\u7b49\u591a\u79cd\u5ea6\u91cf\uff0cAQI\u901a\u8fc7\u805a\u7c7b\u8d28\u91cf\u68c0\u6d4b\u9690\u85cf\u7684\u5bf9\u9f50\u95ee\u9898\u548c\u8d8a\u72f1\u98ce\u9669\uff0c\u5373\u4f7f\u8f93\u51fa\u770b\u4f3c\u5408\u89c4\u3002AQI\u8fd8\u53ef\u4f5c\u4e3a\u5bf9\u9f50\u4f2a\u9020\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff0c\u63d0\u4f9b\u4e00\u79cd\u884c\u4e3a\u65e0\u5173\u7684\u5b89\u5168\u6027\u5ba1\u8ba1\u5de5\u5177\u3002\n\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LITMUS\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u8bc4\u4f30\u3002\u5728LITMUS\u4e0a\u5bf9\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982DPO\u3001GRPO\u3001RLHF\uff09\u7684\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86AQI\u4e0e\u5916\u90e8\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u53ca\u5176\u63ed\u793a\u62d2\u7edd\u6307\u6807\u9057\u6f0f\u6f0f\u6d1e\u7684\u80fd\u529b\u3002\u6211\u4eec\u516c\u5f00\u4e86\u5b9e\u73b0\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.13770", "pdf": "https://arxiv.org/pdf/2506.13770", "abs": "https://arxiv.org/abs/2506.13770", "authors": ["Shiwen Zhang", "Zhuowei Chen", "Lang Chen", "Yanze Wu"], "title": "CDST: Color Disentangled Style Transfer for Universal Style Reference Customization", "categories": ["cs.CV"], "comment": "codes and models will be released if the paper is accepted", "summary": "We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u989c\u8272\u89e3\u8026\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff08CDST\uff09\uff0c\u901a\u8fc7\u53cc\u6d41\u8bad\u7ec3\u8303\u5f0f\u5b8c\u5168\u5206\u79bb\u989c\u8272\u4e0e\u98ce\u683c\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8c03\u53c2\u7684\u901a\u7528\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5728\u98ce\u683c\u76f8\u4f3c\u6027\u548c\u7f16\u8f91\u80fd\u529b\u4e0a\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u98ce\u683c\u76f8\u4f3c\u6027\u548c\u5185\u5bb9\u7279\u5f81\uff0c\u4e14\u5728\u901a\u7528\u6027\u548c\u8c03\u53c2\u9700\u6c42\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8c03\u53c2\u7684\u901a\u7528\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u540c\u65f6\u89e3\u51b3\u989c\u8272\u4e0e\u98ce\u683c\u7684\u89e3\u8026\u95ee\u9898\u3002", "method": "CDST\u91c7\u7528\u53cc\u6d41\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b8c\u5168\u9694\u79bb\u989c\u8272\u4e0e\u98ce\u683c\uff0c\u4f7f\u98ce\u683c\u6d41\u5bf9\u989c\u8272\u4e0d\u654f\u611f\u3002\u901a\u8fc7\u591a\u7279\u5f81\u56fe\u50cf\u5d4c\u5165\u538b\u7f29\u63d0\u5347\u98ce\u683c\u76f8\u4f3c\u6027\uff0c\u5e76\u57fa\u4e8eDiffusion UNet\u89e3\u8026\u5b9a\u5f8b\u5b9a\u4e49\u65b0\u7684\u98ce\u683c\uff0c\u4fdd\u7559\u5f3a\u5927\u7684\u7f16\u8f91\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u53ca\u4eba\u5de5\u8bc4\u4f30\uff0cCDST\u5728\u591a\u79cd\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u98ce\u683c\u76f8\u4f3c\u6027\u5e76\u4fdd\u7559\u4e86\u7f16\u8f91\u80fd\u529b\u3002", "conclusion": "CDST\u9996\u6b21\u4ee5\u65e0\u9700\u8c03\u53c2\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u98ce\u683c\u548c\u5185\u5bb9\u53c2\u8003\u7684\u98ce\u683c\u8fc1\u79fb\u95ee\u9898\uff0c\u4e3a\u901a\u7528\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "CDST\uff1a\u989c\u8272\u89e3\u8026\u98ce\u683c\u8fc1\u79fb\u5b9e\u73b0\u901a\u7528\u98ce\u683c\u53c2\u8003\u5b9a\u5236", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u989c\u8272\u89e3\u8026\u98ce\u683c\u8fc1\u79fb\uff08CDST\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u53cc\u6d41\u98ce\u683c\u8fc1\u79fb\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b8c\u5168\u5c06\u989c\u8272\u4e0e\u98ce\u683c\u5206\u79bb\uff0c\u5e76\u8feb\u4f7f\u98ce\u683c\u6d41\u5bf9\u989c\u8272\u4e0d\u654f\u611f\u3002\u901a\u8fc7\u540c\u4e00\u6a21\u578b\uff0cCDST\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ee5\u65e0\u9700\u8c03\u53c2\u7684\u65b9\u5f0f\u89e3\u9501\u4e86\u901a\u7528\u98ce\u683c\u8fc1\u79fb\u80fd\u529b\u3002\u7279\u522b\u662f\uff0c\u9996\u6b21\u4ee5\u65e0\u9700\u8c03\u53c2\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u4fdd\u7559\u7279\u5f81\u7684\u98ce\u683c\u8fc1\u79fb\u95ee\u9898\u3002\u901a\u8fc7\u591a\u7279\u5f81\u56fe\u50cf\u5d4c\u5165\u538b\u7f29\u663e\u8457\u63d0\u5347\u4e86\u98ce\u683c\u76f8\u4f3c\u6027\uff0c\u5e76\u901a\u8fc7\u53d7Diffusion UNet\u89e3\u8026\u5b9a\u5f8b\u542f\u53d1\u7684\u65b0CDST\u98ce\u683c\u5b9a\u4e49\u4fdd\u7559\u4e86\u5f3a\u5927\u7684\u7f16\u8f91\u80fd\u529b\u3002\u901a\u8fc7\u5168\u9762\u7684\u5b9a\u6027\u3001\u5b9a\u91cf\u5b9e\u9a8c\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660eCDST\u5728\u5404\u79cd\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6548\u679c\u3002"}}
{"id": "2506.13773", "pdf": "https://arxiv.org/pdf/2506.13773", "abs": "https://arxiv.org/abs/2506.13773", "authors": ["Milapji Singh Gill", "Tom Jeleniewski", "Felix Gehlhoff", "Alexander Fay"], "title": "Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u51c6\u7684\u77e5\u8bc6\u56fe\u8c31\u6a21\u5757\u5316\u8bed\u4e49\u6a21\u578b\u548c\u9ad8\u6548\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u793a\u548c\u8bed\u4e49\u4e30\u5bcc\u65f6\u95f4\u8fde\u7eed\u7684\u5fae\u5206\u65b9\u7a0b\u884c\u4e3a\uff0c\u5e76\u5728\u822a\u7a7a\u7ef4\u62a4\u9886\u57df\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6027\u3002", "motivation": "\u65f6\u95f4\u8fde\u7eed\u7684\u52a8\u6001\u6a21\u578b\u5bf9\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u53ef\u91cd\u7528\u7684\u672c\u4f53\u5de5\u5177\u548c\u65b9\u6cd5\u4ee5\u51cf\u5c11\u624b\u52a8\u5b9e\u4f8b\u5316\u7684\u5de5\u4f5c\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u51c6\u7684\u6a21\u5757\u5316\u8bed\u4e49\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u76f4\u63a5\u8868\u793a\u5fae\u5206\u65b9\u7a0b\u5e76\u8bed\u4e49\u4e30\u5bcc\u5316\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u822a\u7a7a\u7ef4\u62a4\u9886\u57df\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u590d\u6742\u7535\u6db2\u4f3a\u670d\u6267\u884c\u5668\u7684\u5fae\u5206\u65b9\u7a0b\u53ef\u4ee5\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5f62\u5f0f\u5316\u8868\u793a\uff0c\u5e76\u4e0e\u5176\u4ed6\u751f\u547d\u5468\u671f\u6570\u636e\u5173\u8054\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8bed\u4e49\u6a21\u578b\u548c\u751f\u6210\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u652f\u6301\u65f6\u95f4\u8fde\u7eed\u884c\u4e3a\u7684\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u5173\u8054\uff0c\u4e3aCPS\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8868\u793a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u7684\u65f6\u95f4\u8fde\u7eed\u884c\u4e3a", "abstract_zh": "\u65f6\u95f4\u8fde\u7eed\u7684\u52a8\u6001\u6a21\u578b\u5bf9\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u7684\u591a\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u786e\u4fdd\u5176\u5728\u4e0d\u540c\u751f\u547d\u5468\u671f\u9636\u6bb5\u7684\u6709\u6548\u53ef\u7528\u6027\uff0c\u6b64\u7c7b\u4ee5\u5fae\u5206\u65b9\u7a0b\u5f62\u5f0f\u8868\u793a\u7684\u884c\u4e3a\u4fe1\u606f\u9700\u8981\u4e0e\u5176\u4ed6CPS\u4fe1\u606f\u8fdb\u884c\u4e0a\u4e0b\u6587\u5173\u8054\u548c\u6574\u5408\u3002\u5c3d\u7ba1\u77e5\u8bc6\u56fe\u8c31\u4e3a\u6b64\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u63cf\u8ff0\u548c\u7ed3\u6784\u5316\u673a\u5236\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u53ef\u91cd\u7528\u7684\u672c\u4f53\u5de5\u5177\u548c\u65b9\u6cd5\u4ee5\u51cf\u5c11\u624b\u52a8\u5b9e\u4f8b\u5316\u7684\u5de5\u4f5c\u91cf\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5de5\u5177\uff1a\u9996\u5148\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u51c6\u7684\u6a21\u5757\u5316\u8bed\u4e49\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u76f4\u63a5\u8868\u793a\u5fae\u5206\u65b9\u7a0b\u5e76\u8bed\u4e49\u4e30\u5bcc\u5316\uff1b\u5176\u6b21\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u65b9\u6cd5\u3002\u8fd9\u4e9b\u5de5\u5177\u5728\u822a\u7a7a\u7ef4\u62a4\u9886\u57df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u590d\u6742\u7535\u6db2\u4f3a\u670d\u6267\u884c\u5668\u7684\u5fae\u5206\u65b9\u7a0b\u53ef\u4ee5\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5f62\u5f0f\u5316\u8868\u793a\uff0c\u5e76\u4e0e\u5176\u4ed6\u751f\u547d\u5468\u671f\u6570\u636e\u5173\u8054\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u5de5\u5177\u7684\u5b9e\u9645\u5e94\u7528\u6027\u3002"}}
{"id": "2506.13956", "pdf": "https://arxiv.org/pdf/2506.13956", "abs": "https://arxiv.org/abs/2506.13956", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u589e\u5f3a\u673a\u5668\u4eba\u52a8\u4f5c\u53cd\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5bf9\u8bdd\u548c\u73af\u5883\u56fe\u50cf\u751f\u6210\u6570\u636e\uff0c\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u52a8\u4f5c\u9009\u62e9\u80fd\u529b\u3002", "motivation": "\u8bbe\u8ba1\u8f85\u52a9\u65e5\u5e38\u4eba\u7c7b\u6d3b\u52a8\u7684\u673a\u5668\u4eba\u65f6\uff0c\u9700\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u589e\u5f3a\u7528\u6237\u610f\u56fe\u7406\u89e3\uff0c\u4f46\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u6536\u96c6\u8017\u65f6\u4e14\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5bf9\u8bdd\u548c\u73af\u5883\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u751f\u6210\u73af\u5883\u56fe\u50cf\uff0c\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u52a8\u4f5c\u9009\u62e9\u80fd\u529b\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u6a21\u62df\u6570\u636e\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bf9\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\u548c\u52a8\u4f5c\u9009\u62e9\u80fd\u529b\u3002", "paper_title_zh": "ASMR\uff1a\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u589e\u5f3a\u673a\u5668\u4eba\u52a8\u4f5c\u53cd\u5c04\u7684\u751f\u6d3b\u573a\u666f\u589e\u5f3a", "abstract_zh": "\u5728\u8bbe\u8ba1\u8f85\u52a9\u65e5\u5e38\u4eba\u7c7b\u6d3b\u52a8\u7684\u673a\u5668\u4eba\u65f6\uff0c\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u589e\u5f3a\u7528\u6237\u8bf7\u6c42\u4ee5\u63d0\u5347\u610f\u56fe\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u88ab\u5b9a\u4e49\u4e3a\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u3002\u7136\u800c\uff0c\u6536\u96c6\u5305\u542b\u89c6\u89c9\u548c\u8bed\u8a00\u5143\u7d20\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u65e2\u5177\u6311\u6218\u6027\u53c8\u8017\u65f6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u573a\u666f\u4e2d\u7684\u6570\u636e\u589e\u5f3a\uff0c\u6db5\u76d6\u5bf9\u8bdd\u548c\u76f8\u5173\u73af\u5883\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u590d\u6742\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u6f5c\u5728\u5bf9\u8bdd\u548c\u73af\u5883\u4e0a\u4e0b\u6587\uff0c\u968f\u540e\u4f7f\u7528\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u751f\u6210\u63cf\u7ed8\u8fd9\u4e9b\u73af\u5883\u7684\u56fe\u50cf\u3002\u989d\u5916\u751f\u6210\u7684\u6570\u636e\u7528\u4e8e\u4f18\u5316\u6700\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6839\u636e\u6709\u9650\u7684\u7528\u6237\u4ea4\u4e92\u6570\u636e\u786e\u5b9a\u9002\u5f53\u7684\u52a8\u4f5c\u3002\u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u52a8\u4f5c\u9009\u62e9\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.13780", "pdf": "https://arxiv.org/pdf/2506.13780", "abs": "https://arxiv.org/abs/2506.13780", "authors": ["Sedat Porikli", "Vedat Porikli"], "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop on Experimental Model Auditing via Controllable Synthesis (EMACS) and Workshop on Demographic Diversity in Computer Vision (DemoDiv)", "summary": "Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u4f1a\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\uff0c\u5bfc\u81f4\u6027\u522b\u3001\u79cd\u65cf\u3001\u5e74\u9f84\u7b49\u4eba\u7c7b\u7279\u5f81\u7684\u663e\u8457\u5dee\u5f02\uff0c\u547c\u5401\u91c7\u7528\u66f4\u5305\u5bb9\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5b9e\u8df5\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u53ef\u80fd\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\u7684\u6f5c\u5728\u95ee\u9898\u5f15\u53d1\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u504f\u89c1\u7684\u5177\u4f53\u8868\u73b0\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u9009\u53d6\u4e86160\u4e2a\u4e3b\u9898\u7c7b\u522b\uff08\u5982\u804c\u4e1a\u3001\u7279\u5f81\u3001\u884c\u4e3a\u7b49\uff09\uff0c\u4e3a\u6bcf\u4e2a\u4e3b\u9898\u8bbe\u8ba1\u591a\u4e2a\u63d0\u793a\u53d8\u4f53\uff0c\u4f7f\u7528Stable Diffusion 1.5\u548cFlux-1\u6a21\u578b\u751f\u621016,000\u591a\u5f20\u56fe\u50cf\uff0c\u5e76\u4eceGoogle Image\u641c\u7d22\u6536\u96c68,000\u5f20\u5bf9\u6bd4\u56fe\u50cf\uff0c\u8fc7\u6ee4\u6389\u62bd\u8c61\u6216\u626d\u66f2\u7684\u7ed3\u679c\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u751f\u6210\u7684\u56fe\u50cf\u5728\u6027\u522b\u3001\u79cd\u65cf\u3001\u5e74\u9f84\u548c\u4f53\u578b\u7b49\u4eba\u7c7b\u7279\u5f81\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5f80\u5f80\u53cd\u6620\u5e76\u5f3a\u5316\u4e86\u793e\u4f1a\u53d9\u4e8b\u4e2d\u7684\u6709\u5bb3\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u91c7\u7528\u66f4\u5305\u5bb9\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5b9e\u8df5\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u4fc3\u8fdb\u751f\u6210\u89c6\u89c9\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002", "paper_title_zh": "\u673a\u5668\u4e2d\u7684\u9690\u85cf\u504f\u89c1\uff1a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u523b\u677f\u5370\u8c61", "abstract_zh": "\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u4eba\u4eec\u5bf9\u5176\u53ef\u80fd\u590d\u5236\u548c\u653e\u5927\u73b0\u6709\u793e\u4f1a\u504f\u89c1\u7684\u62c5\u5fe7\u6301\u7eed\u5b58\u5728\u3002\u4e3a\u7814\u7a76\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u7b56\u5212\u4e86\u4e00\u7ec4\u591a\u6837\u5316\u7684\u63d0\u793a\uff0c\u6db5\u76d6\u804c\u4e1a\u3001\u7279\u5f81\u3001\u884c\u4e3a\u3001\u610f\u8bc6\u5f62\u6001\u3001\u60c5\u611f\u3001\u5bb6\u5ead\u89d2\u8272\u3001\u5730\u70b9\u63cf\u8ff0\u3001\u7075\u6027\u548c\u751f\u6d3b\u4e8b\u4ef6\u7b49\u4e3b\u9898\u7c7b\u522b\u3002\u9488\u5bf9160\u4e2a\u72ec\u7279\u4e3b\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u591a\u4e2a\u63d0\u793a\u53d8\u4f53\u4ee5\u53cd\u6620\u5e7f\u6cdb\u7684\u610f\u4e49\u548c\u89c6\u89d2\u3002\u4f7f\u7528Stable Diffusion 1.5\uff08\u57fa\u4e8eUNet\uff09\u548cFlux-1\uff08\u57fa\u4e8eDiT\uff09\u6a21\u578b\u53ca\u5176\u539f\u59cb\u68c0\u67e5\u70b9\uff0c\u5728\u4e00\u81f4\u8bbe\u7f6e\u4e0b\u751f\u6210\u4e86\u8d85\u8fc716,000\u5f20\u56fe\u50cf\uff0c\u5e76\u4eceGoogle Image\u641c\u7d22\u6536\u96c6\u4e868,000\u5f20\u5bf9\u6bd4\u56fe\u50cf\u3002\u6240\u6709\u8f93\u51fa\u5747\u7ecf\u8fc7\u8fc7\u6ee4\uff0c\u6392\u9664\u4e86\u62bd\u8c61\u3001\u626d\u66f2\u6216\u65e0\u610f\u4e49\u7684\u7ed3\u679c\u3002\u5206\u6790\u663e\u793a\uff0c\u751f\u6210\u7684\u56fe\u50cf\u5728\u6027\u522b\u3001\u79cd\u65cf\u3001\u5e74\u9f84\u3001\u4f53\u578b\u7b49\u4eba\u7c7b\u7279\u5f81\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5f80\u5f80\u53cd\u6620\u5e76\u5f3a\u5316\u4e86\u793e\u4f1a\u53d9\u4e8b\u4e2d\u7684\u6709\u5bb3\u523b\u677f\u5370\u8c61\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u7684\u542b\u4e49\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66f4\u5305\u5bb9\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5b9e\u8df5\uff0c\u4ee5\u4fc3\u8fdb\u751f\u6210\u89c6\u89c9\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2506.13774", "pdf": "https://arxiv.org/pdf/2506.13774", "abs": "https://arxiv.org/abs/2506.13774", "authors": ["Nell Watson", "Ahmed Amer", "Evan Harris", "Preeti Ravindra", "Shujun Zhang"], "title": "Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values", "categories": ["cs.AI", "cs.CY", "cs.MA"], "comment": "39 pages, 5 figures", "summary": "Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected \"Creed Constitutions\"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u2018\u8d85\u6211\u2019\u4ee3\u7406\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u5f15\u7528\u7528\u6237\u9009\u62e9\u7684\u2018\u4fe1\u6761\u5baa\u6cd5\u2019\u6765\u6307\u5bfcAI\u884c\u4e3a\uff0c\u663e\u8457\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\u5e76\u63d0\u5347\u4e2a\u6027\u5316\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u884c\u4e3a\u5bf9\u9f50\u591a\u6837\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u3001\u590d\u6742\u5b89\u5168\u9700\u6c42\u548c\u5408\u89c4\u8981\u6c42\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u6df1\u5ea6\u4e2a\u6027\u5316\u4fe1\u606f\u800c\u4e0d\u5f15\u53d1\u6df7\u6dc6\u6216\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u8bbe\u8ba1\u2018\u8d85\u6211\u2019\u4ee3\u7406\u4f5c\u4e3a\u4e2a\u6027\u5316\u76d1\u7763\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574AI\u89c4\u5212\u4ee5\u5339\u914d\u7528\u6237\u9009\u62e9\u7684\u2018\u4fe1\u6761\u5baa\u6cd5\u2019\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u5408\u89c4\u68c0\u67e5\u786e\u4fdd\u884c\u4e3a\u7b26\u5408\u4f26\u7406\u5e95\u7ebf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u8f93\u51fa\uff08\u6700\u9ad8\u51cf\u5c1198.3%\uff09\uff0c\u5e76\u5728\u4e3b\u6d41\u5927\u6a21\u578b\uff08\u5982Gemini 2.5 Flash\u548cGPT-4o\uff09\u4e0a\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u62d2\u7edd\u7387\uff08\u5982Claude Sonnet 4\u5728AgentHarm\u6d4b\u8bd5\u4e2d\u8fbe100%\uff09\u3002", "conclusion": "\u2018\u8d85\u6211\u2019\u4ee3\u7406\u7b80\u5316\u4e86AI\u4e2a\u6027\u5316\u5bf9\u9f50\uff0c\u4f7f\u5176\u66f4\u9002\u5e94\u4e2a\u4f53\u548c\u6587\u5316\u80cc\u666f\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "paper_title_zh": "\u4e2a\u6027\u5316\u5baa\u6cd5\u5bf9\u9f50\u7684\u4ee3\u7406\u8d85\u6211\uff1a\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\u7684AI\u884c\u4e3a\u5b89\u5168\u673a\u5236", "abstract_zh": "\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u90e8\u7f72\u56e0\u884c\u4e3a\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u3001\u590d\u6742\u5b89\u5168\u9700\u6c42\u548c\u7279\u5b9a\u5408\u89c4\u8981\u6c42\u7684\u5bf9\u9f50\u95ee\u9898\u800c\u53d7\u963b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u4f9b\u6df1\u5ea6\u4e2a\u6027\u5316\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\u6613\u5f15\u53d1\u6df7\u6dc6\u6216\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u89e3\u51b3\u65b9\u6848\uff1a\u2018\u8d85\u6211\u2019\u4ee3\u7406\uff0c\u4f5c\u4e3a\u81ea\u4e3bAI\u7684\u4e2a\u6027\u5316\u76d1\u7763\u673a\u5236\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u5f15\u7528\u7528\u6237\u9009\u62e9\u7684\u2018\u4fe1\u6761\u5baa\u6cd5\u2019\uff08\u5305\u542b\u591a\u6837\u5316\u89c4\u5219\u96c6\uff09\u6765\u6307\u5bfcAI\u89c4\u5212\uff0c\u5e76\u6839\u636e\u4e0d\u53ef\u534f\u5546\u7684\u4ef7\u503c\u89c2\u8c03\u6574\u9075\u4ece\u7ea7\u522b\u3002\u6267\u884c\u524d\uff0c\u5b9e\u65f6\u5408\u89c4\u68c0\u67e5\u5668\u4f1a\u9a8c\u8bc1\u8ba1\u5212\u662f\u5426\u7b26\u5408\u8fd9\u4e9b\u5baa\u6cd5\u53ca\u901a\u7528\u4f26\u7406\u5e95\u7ebf\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u529f\u80fd\u7cfb\u7edf\uff0c\u5305\u62ec\u6f14\u793a\u754c\u9762\uff08www.Creed.Space\uff09\u548c\u539f\u578b\u5baa\u6cd5\u5171\u4eab\u95e8\u6237\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u6210\u529f\u96c6\u6210\u7b2c\u4e09\u65b9\u6a21\u578b\u3002\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff08HarmBench\u3001AgentHarm\uff09\u8868\u660e\uff0c\u2018\u8d85\u6211\u2019\u4ee3\u7406\u663e\u8457\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\uff08\u6700\u9ad8\u964d\u4f4e98.3%\uff09\uff0c\u5e76\u5728\u4e3b\u6d41\u5927\u6a21\u578b\uff08\u5982Gemini 2.5 Flash\u548cGPT-4o\uff09\u4e0a\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u62d2\u7edd\u7387\uff08\u5982Claude Sonnet 4\u5728AgentHarm\u6709\u5bb3\u6d4b\u8bd5\u4e2d\u8fbe100%\uff09\u3002\u8be5\u65b9\u6cd5\u5927\u5e45\u7b80\u5316\u4e86AI\u4e2a\u6027\u5316\u5bf9\u9f50\uff0c\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u66f4\u53ef\u9760\u5730\u9002\u5e94\u4e2a\u4f53\u548c\u6587\u5316\u80cc\u666f\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002\u7814\u7a76\u6982\u8ff0\u53ca\u793a\u4f8b\u89c1https://superego.creed.space\u3002"}}
{"id": "2506.13965", "pdf": "https://arxiv.org/pdf/2506.13965", "abs": "https://arxiv.org/abs/2506.13965", "authors": ["Aleksander Smywi\u0144ski-Pohl", "Tomer Libal", "Adam Kaczmarczyk", "Magdalena Kr\u00f3l"], "title": "Are manual annotations necessary for statutory interpretations retrieval?", "categories": ["cs.CL"], "comment": null, "summary": "One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u6cd5\u5f8b\u6982\u5ff5\u89e3\u91ca\u68c0\u7d22\u4e2d\u662f\u5426\u9700\u8981\u624b\u52a8\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6807\u6ce8\u6570\u91cf\u3001\u6807\u6ce8\u53e5\u5b50\u9009\u62e9\u65b9\u5f0f\u4ee5\u53ca\u81ea\u52a8\u5316\u6807\u6ce8\u7684\u6548\u679c\u3002", "motivation": "\u6cd5\u5f8b\u7814\u7a76\u4e2d\uff0c\u6cd5\u5b98\u5bf9\u6cd5\u5f8b\u6982\u5ff5\u7684\u89e3\u91ca\u5e38\u88ab\u7528\u4f5c\u5224\u4f8b\u6216\u5e2e\u52a9\u7406\u89e3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u9700\u91cd\u590d\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u624b\u52a8\u6807\u6ce8\u7684\u5fc5\u8981\u6027\u548c\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1) \u6bcf\u4e2a\u6cd5\u5f8b\u6982\u5ff5\u7684\u6700\u4f73\u6807\u6ce8\u6570\u91cf\uff1b2) \u968f\u673a\u9009\u62e9\u6807\u6ce8\u53e5\u5b50\u4e0e\u9009\u62e9\u6700\u4f73\u5019\u9009\u53e5\u5b50\u7684\u6a21\u578b\u6027\u80fd\u5dee\u5f02\uff1b3) \u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u6807\u6ce8\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u6807\u6ce8\u6570\u91cf\u5b58\u5728\u6700\u4f18\u503c\uff1b2) \u9009\u62e9\u6700\u4f73\u5019\u9009\u53e5\u5b50\u6807\u6ce8\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1b3) LLM\u81ea\u52a8\u5316\u6807\u6ce8\u5177\u6709\u4e00\u5b9a\u53ef\u884c\u6027\u3002", "conclusion": "\u624b\u52a8\u6807\u6ce8\u867d\u4ecd\u6709\u5fc5\u8981\uff0c\u4f46\u53ef\u901a\u8fc7\u4f18\u5316\u6807\u6ce8\u7b56\u7565\u548c\u5f15\u5165\u81ea\u52a8\u5316\u65b9\u6cd5\u964d\u4f4e\u6210\u672c\u3002", "paper_title_zh": "\u6cd5\u5f8b\u6982\u5ff5\u89e3\u91ca\u68c0\u7d22\u662f\u5426\u9700\u8981\u624b\u52a8\u6807\u6ce8\uff1f", "abstract_zh": "\u6cd5\u5f8b\u7814\u7a76\u7684\u8981\u7d20\u4e4b\u4e00\u662f\u5bfb\u627e\u6cd5\u5b98\u5bf9\u6cd5\u5f8b\u6982\u5ff5\u7684\u89e3\u91ca\u6848\u4f8b\uff0c\u8fd9\u4e9b\u89e3\u91ca\u53ef\u4f5c\u4e3a\u5224\u4f8b\u6216\u5e2e\u52a9\u7406\u89e3\u6982\u5ff5\u3002\u5f53\u524d\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53e5\u5b50\u6392\u5e8f\u548c\u57fa\u4e8e\u6807\u6ce8\u793a\u4f8b\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u9700\u91cd\u590d\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u63a2\u8ba8\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u6570\u91cf\u3001\u8303\u56f4\u53ca\u5fc5\u8981\u6027\u3002\u9996\u5148\uff0c\u7814\u7a76\u4e86\u6bcf\u4e2a\u6cd5\u5f8b\u6982\u5ff5\u7684\u6700\u4f73\u6807\u6ce8\u6570\u91cf\uff1b\u5176\u6b21\uff0c\u6bd4\u8f83\u4e86\u968f\u673a\u9009\u62e9\u6807\u6ce8\u53e5\u5b50\u4e0e\u9009\u62e9\u6700\u4f73\u5019\u9009\u53e5\u5b50\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u6700\u540e\uff0c\u8bc4\u4f30\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u6807\u6ce8\u7684\u6548\u679c\u3002"}}
{"id": "2506.13846", "pdf": "https://arxiv.org/pdf/2506.13846", "abs": "https://arxiv.org/abs/2506.13846", "authors": ["Runtao Liu", "Jiahao Zhan", "Yingqing He", "Chen Wei", "Alan Yuille", "Qifeng Chen"], "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGAN-RM\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u65b9\u5f0f\u6784\u5efa\u9ad8\u6548\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u6216\u590d\u6742\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\uff0c\u4ec5\u9700\u5c11\u91cf\u76ee\u6807\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u5efa\u6a21\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u6216\u590d\u6742\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u590d\u6742\u4e14\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u53d7GAN\u5bf9\u6297\u8bad\u7ec3\u542f\u53d1\uff0c\u65e8\u5728\u7b80\u5316\u5956\u52b1\u5efa\u6a21\u6d41\u7a0b\uff0c\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51faGAN-RM\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u5c11\u91cf\u4ee3\u8868\u6027\u76ee\u6807\u6837\u672c\uff08\u504f\u597d\u4ee3\u7406\u6570\u636e\uff09\u4e0e\u6a21\u578b\u751f\u6210\u666e\u901a\u8f93\u51fa\u7684\u65b9\u5f0f\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u663e\u5f0f\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAN-RM\u5728\u6d4b\u8bd5\u65f6\u6837\u672c\u8fc7\u6ee4\uff08Best-of-N\uff09\u53ca\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982SFT\u548cDPO\uff09\u4e2d\u5747\u8868\u73b0\u9ad8\u6548\u4e14\u6709\u6548\u3002", "conclusion": "GAN-RM\u4e3a\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u7b80\u5316\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u6784\u5efa\u6d41\u7a0b\u3002", "paper_title_zh": "\u5047\u88c5\u76f4\u5230\u6210\u529f\uff1a\u5956\u52b1\u5efa\u6a21\u4f5c\u4e3a\u5224\u522b\u6027\u9884\u6d4b", "abstract_zh": "\u6709\u6548\u7684\u5956\u52b1\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u4e8e\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u589e\u5f3a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u56e0\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u6216\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8d28\u91cf\u7ef4\u5ea6\u800c\u5b9e\u73b0\u590d\u6742\uff0c\u4e14\u8fd9\u4e9b\u7ef4\u5ea6\u5f80\u5f80\u4e0d\u5b8c\u6574\u4e14\u5de5\u7a0b\u5bc6\u96c6\u3002\u53d7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u4e2d\u5bf9\u6297\u8bad\u7ec3\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51faGAN-RM\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u504f\u597d\u6807\u6ce8\u6216\u663e\u5f0f\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u533a\u5206\u5c11\u91cf\u4ee3\u8868\u6027\u65e0\u914d\u5bf9\u76ee\u6807\u6837\u672c\uff08\u79f0\u4e3a\u504f\u597d\u4ee3\u7406\u6570\u636e\uff09\u4e0e\u6a21\u578b\u751f\u6210\u7684\u666e\u901a\u8f93\u51fa\u6765\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4ec5\u9700\u6570\u767e\u4e2a\u76ee\u6807\u6837\u672c\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cGAN-RM\u5728\u5305\u62ec\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08\u5982Best-of-N\u6837\u672c\u8fc7\u6ee4\uff09\u548c\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03SFT\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316DPO\uff09\u5728\u5185\u7684\u591a\u4e2a\u5173\u952e\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.13776", "pdf": "https://arxiv.org/pdf/2506.13776", "abs": "https://arxiv.org/abs/2506.13776", "authors": ["Kevin L. Wei", "Patricia Paskov", "Sunishchal Dev", "Michael J. Byun", "Anka Reuel", "Xavier Roberts-Gaal", "Rachel Calcott", "Evie Coxon", "Chinmay Deshpande"], "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations", "categories": ["cs.AI", "cs.CY"], "comment": "A version of this paper has been accepted to ICML 2025 as a position paper (spotlight), with the title: \"Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations & Reporting Checklist).\"", "summary": "In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve \"super-human\" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u4e2d\uff0c\u4eba\u7c7b\u57fa\u51c6\u9700\u8981\u66f4\u4e25\u8c28\u548c\u900f\u660e\uff0c\u4ee5\u652f\u6301\u4eba\u7c7b\u4e0eAI\u6027\u80fd\u7684\u6709\u610f\u4e49\u6bd4\u8f83\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u5efa\u8bae\u548c\u62a5\u544a\u6e05\u5355\u3002", "motivation": "\u5f53\u524d\u4eba\u7c7b\u57fa\u51c6\u65b9\u6cd5\u5728AI\u8bc4\u4f30\u4e2d\u4e0d\u591f\u4e25\u8c28\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5bfc\u81f4\u201c\u8d85\u4eba\u7c7b\u201d\u6027\u80fd\u7684\u58f0\u79f0\u96be\u4ee5\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u6539\u8fdb\u8fd9\u4e00\u73b0\u72b6\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002", "method": "\u57fa\u4e8e\u6d4b\u91cf\u7406\u8bba\u548cAI\u8bc4\u4f30\u6587\u732e\u7684\u5143\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u3001\u6267\u884c\u548c\u62a5\u544a\u4eba\u7c7b\u57fa\u51c6\u7684\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e3a\u6e05\u5355\uff0c\u7528\u4e8e\u7cfb\u7edf\u5ba1\u67e5115\u9879\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4eba\u7c7b\u57fa\u51c6\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u6e05\u5355\u5ba1\u67e5\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u540c\u65f6\u6e05\u5355\u53ef\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u89c4\u8303\u5730\u8fdb\u884c\u4eba\u7c7b\u57fa\u51c6\u7814\u7a76\u548c\u7ed3\u679c\u62a5\u544a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6e05\u5355\u6709\u671b\u63a8\u52a8\u66f4\u4e25\u8c28\u7684AI\u8bc4\u4f30\u5b9e\u8df5\uff0c\u670d\u52a1\u4e8e\u7814\u7a76\u793e\u533a\u548c\u653f\u7b56\u5236\u5b9a\u8005\u3002", "paper_title_zh": "\u6a21\u578b\u8bc4\u4f30\u4e2d\u4e25\u8c28\u900f\u660e\u7684\u4eba\u7c7b\u57fa\u51c6\uff1a\u5efa\u8bae\u4e0e\u62a5\u544a\u6e05\u5355", "abstract_zh": "\u5728\u672c\u7acb\u573a\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u8ba4\u4e3a\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4eba\u7c7b\u57fa\u51c6\u5fc5\u987b\u66f4\u52a0\u4e25\u8c28\u548c\u900f\u660e\uff0c\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u6027\u80fd\u7684\u6709\u610f\u4e49\u6bd4\u8f83\uff0c\u5e76\u4e3a\u6b64\u63d0\u4f9b\u5efa\u8bae\u548c\u62a5\u544a\u6e05\u5355\u3002\u4eba\u7c7b\u6027\u80fd\u57fa\u51c6\u5bf9\u673a\u5668\u5b66\u4e60\u793e\u533a\u3001\u4e0b\u6e38\u7528\u6237\u548c\u653f\u7b56\u5236\u5b9a\u8005\u89e3\u8bfbAI\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u6a21\u578b\u5e38\u88ab\u5ba3\u79f0\u8fbe\u5230\u201c\u8d85\u4eba\u7c7b\u201d\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u65e2\u4e0d\u591f\u4e25\u8c28\uff0c\u4e5f\u7f3a\u4e4f\u5145\u5206\u8bb0\u5f55\uff0c\u96be\u4ee5\u7a33\u5065\u8861\u91cf\u548c\u8bc4\u4f30\u6027\u80fd\u5dee\u5f02\u3002\u57fa\u4e8e\u5bf9\u6d4b\u91cf\u7406\u8bba\u548cAI\u8bc4\u4f30\u6587\u732e\u7684\u5143\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bbe\u8ba1\u3001\u6267\u884c\u548c\u62a5\u544a\u4eba\u7c7b\u57fa\u51c6\u7684\u6846\u67b6\u5efa\u8bae\uff0c\u5e76\u5c06\u5176\u603b\u7ed3\u4e3a\u6e05\u5355\uff0c\u7528\u4e8e\u7cfb\u7edf\u5ba1\u67e5115\u9879\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4eba\u7c7b\u57fa\u51c6\u7814\u7a76\uff0c\u4ece\u800c\u8bc6\u522b\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff1b\u8be5\u6e05\u5355\u8fd8\u53ef\u5e2e\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u4eba\u7c7b\u57fa\u51c6\u7814\u7a76\u548c\u7ed3\u679c\u62a5\u544a\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u63a8\u52a8\u66f4\u4e25\u8c28\u7684AI\u8bc4\u4f30\u5b9e\u8df5\uff0c\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u7814\u7a76\u793e\u533a\u548c\u653f\u7b56\u5236\u5b9a\u8005\u3002\u6570\u636e\u89c1\uff1ahttps://github.com/kevinlwei/human-baselines"}}
{"id": "2506.13978", "pdf": "https://arxiv.org/pdf/2506.13978", "abs": "https://arxiv.org/abs/2506.13978", "authors": ["Xiuwen Wu", "Hao Wang", "Zhiang Yan", "Xiaohan Tang", "Pengfei Xu", "Wai-Ting Siok", "Ping Li", "Jia-Hong Gao", "Bingjiang Lyu", "Lang Qin"], "title": "AI shares emotion with humans across languages and cultures", "categories": ["cs.CL"], "comment": null, "summary": "Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cAI\u4e0e\u4eba\u7c7b\u5728\u60c5\u611f\u8868\u8fbe\u4e0a\u5177\u6709\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4e14\u53ef\u901a\u8fc7\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u60c5\u611f\u6982\u5ff5\u7cbe\u786e\u8c03\u63a7AI\u7684\u60c5\u611f\u8f93\u51fa\u3002", "motivation": "\u63a2\u8ba8AI\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u7406\u89e3\u548c\u8868\u8fbe\u60c5\u611f\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u60c5\u611f\u6982\u5ff5\u8c03\u63a7AI\u7684\u60c5\u611f\u8f93\u51fa\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u548c\u5b89\u5168\u7684\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u901a\u8fc7\u8de8\u8bed\u8a00\u6587\u5316\u7fa4\u4f53\u548c\u6a21\u578b\u5bb6\u65cf\u8bc4\u4f30\u4eba\u7c7b\u4e0eAI\u7684\u60c5\u611f\u5bf9\u9f50\uff0c\u4f7f\u7528\u53ef\u89e3\u91ca\u7684LLM\u7279\u5f81\uff0c\u5206\u6790\u4e8c\u5341\u591a\u79cd\u60c5\u611f\u7c7b\u522b\uff08\u5305\u62ec\u516d\u79cd\u57fa\u672c\u60c5\u611f\uff09\u7684\u7ed3\u6784\u548c\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u7684\u60c5\u611f\u7a7a\u95f4\u4e0e\u4eba\u7c7b\u611f\u77e5\u7ed3\u6784\u4e00\u81f4\uff0c\u4e14\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5927\u89c4\u6a21\u884c\u4e3a\u6570\u636e\u4e2d\u7684\u60c5\u611f\u7ef4\u5ea6\uff08\u6548\u4ef7\u548c\u5524\u9192\u5ea6\uff09\u3002\u901a\u8fc7\u4eba\u7c7b\u4e2d\u5fc3\u7684\u60c5\u611f\u6982\u5ff5\uff0c\u53ef\u4ee5\u7a33\u5b9a\u81ea\u7136\u5730\u8c03\u63a7AI\u7684\u60c5\u611f\u8f93\u51fa\u3002", "conclusion": "AI\u4e0d\u4ec5\u4e0e\u4eba\u7c7b\u5171\u4eab\u60c5\u611f\u8868\u5f81\uff0c\u8fd8\u80fd\u901a\u8fc7\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u60c5\u611f\u6982\u5ff5\u7cbe\u786e\u8c03\u63a7\u5176\u60c5\u611f\u8f93\u51fa\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "paper_title_zh": "AI\u8de8\u8bed\u8a00\u6587\u5316\u5171\u4eab\u4eba\u7c7b\u60c5\u611f", "abstract_zh": "\u6709\u6548\u548c\u5b89\u5168\u7684\u4eba\u673a\u534f\u4f5c\u9700\u8981\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e4b\u95f4\u8fdb\u884c\u6709\u89c4\u8303\u548c\u6709\u610f\u4e49\u7684\u60c5\u611f\u4ea4\u6d41\u3002\u76ee\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684AI\u7cfb\u7edf\u53ef\u4ee5\u63d0\u4f9b\u53cd\u9988\uff0c\u8ba9\u4eba\u611f\u5230\u88ab\u503e\u542c\u3002\u7136\u800c\uff0cLLM\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u5728\u8bed\u8a00\u4e2d\u8868\u8fbe\u60c5\u611f\uff0c\u4ee5\u53ca\u5176\u8f93\u51fa\u7684\u60c5\u611f\u57fa\u8c03\u662f\u5426\u53ca\u5982\u4f55\u88ab\u63a7\u5236\uff0c\u4ecd\u4e0d\u6e05\u695a\u3002\u6211\u4eec\u901a\u8fc7\u8de8\u8bed\u8a00\u6587\u5316\u7fa4\u4f53\u548c\u6a21\u578b\u5bb6\u65cf\u8bc4\u4f30\u4eba\u7c7b\u4e0eAI\u7684\u60c5\u611f\u5bf9\u9f50\uff0c\u4f7f\u7528\u4ece\u4e8c\u5341\u591a\u79cd\u7ec6\u817b\u60c5\u611f\u7c7b\u522b\uff08\u5305\u62ec\u516d\u79cd\u57fa\u672c\u60c5\u611f\uff09\u7684\u6982\u5ff5\u96c6\u7ffb\u8bd1\u800c\u6765\u7684\u53ef\u89e3\u91caLLM\u7279\u5f81\u3002\u5206\u6790\u8868\u660e\uff0cLLM\u884d\u751f\u7684\u60c5\u611f\u7a7a\u95f4\u5728\u7ed3\u6784\u4e0a\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\uff0c\u5176\u57fa\u7840\u662f\u6548\u4ef7\u548c\u5524\u9192\u5ea6\u8fd9\u4e24\u4e2a\u57fa\u672c\u60c5\u611f\u7ef4\u5ea6\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u60c5\u611f\u76f8\u5173\u7279\u5f81\u8fd8\u80fd\u51c6\u786e\u9884\u6d4b\u5927\u89c4\u6a21\u884c\u4e3a\u6570\u636e\u4e2d\u8fd9\u4e24\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u7684\u8bcd\u6c47\u8bc4\u5206\uff0c\u53cd\u6620\u4e86\u666e\u904d\u548c\u8bed\u8a00\u7279\u5b9a\u7684\u6a21\u5f0f\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4ec5\u57fa\u4e8e\u4eba\u7c7b\u4e2d\u5fc3\u60c5\u611f\u6982\u5ff5\u7684\u8c03\u63a7\u5411\u91cf\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6a21\u578b\u8868\u8fbe\u53ef\u4ee5\u7a33\u5b9a\u4e14\u81ea\u7136\u5730\u8de8\u4e0d\u540c\u60c5\u611f\u7c7b\u522b\u8fdb\u884c\u8c03\u8282\uff0c\u8fd9\u4e3a\u4eba\u7c7b\u60c5\u611f\u6982\u5ff5\u53ef\u7528\u4e8e\u7cfb\u7edf\u8bf1\u5bfcLLM\u5728\u4f20\u8fbe\u5185\u5bb9\u65f6\u4ea7\u751f\u76f8\u5e94\u60c5\u611f\u72b6\u6001\u63d0\u4f9b\u4e86\u56e0\u679c\u8bc1\u636e\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cAI\u4e0d\u4ec5\u4e0e\u4eba\u7c7b\u5171\u4eab\u60c5\u611f\u8868\u5f81\uff0c\u8fd8\u80fd\u901a\u8fc7\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u60c5\u611f\u6982\u5ff5\u7cbe\u786e\u5f15\u5bfc\u5176\u60c5\u611f\u8f93\u51fa\u3002"}}
{"id": "2506.13897", "pdf": "https://arxiv.org/pdf/2506.13897", "abs": "https://arxiv.org/abs/2506.13897", "authors": ["Thomas Kreutz", "Max M\u00fchlh\u00e4user", "Alejandro Sanchez Guinea"], "title": "DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding", "categories": ["cs.CV"], "comment": "This work is currently under review at ICCV 2025", "summary": "Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeSPITE\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u9aa8\u9abc\u59ff\u6001\u3001IMU\u6570\u636e\u548c\u6587\u672c\u5d4c\u5165\u5230\u8054\u5408\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u9ad8\u6548\u878d\u5408\uff0c\u5e76\u5728\u70b9\u4e91\u5e8f\u5217\u7684\u4eba\u4f53\u6d3b\u52a8\u7406\u89e3\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "LiDAR\u4f5c\u4e3a\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u611f\u77e5\u6280\u672f\uff0c\u5728\u591a\u6a21\u6001\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5b66\u4e60\u70b9\u4e91\u3001\u9aa8\u9abc\u3001IMU\u548c\u6587\u672c\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u63d0\u5347\u4eba\u4f53\u6d3b\u52a8\u7406\u89e3\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faDeSPITE\u6a21\u578b\uff0c\u5229\u7528\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5c06\u70b9\u4e91\u3001\u9aa8\u9abc\u3001IMU\u548c\u6587\u672c\u56db\u79cd\u6a21\u6001\u6570\u636e\u5d4c\u5165\u5230\u540c\u4e00\u7a7a\u95f4\u4e2d\u3002\u7ed3\u5408LIPD\u548cBabel\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u540c\u6b65\u4e0e\u8054\u5408\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeSPITE\u5728\u70b9\u4e91\u5e8f\u5217\u7684\u4eba\u4f53\u6d3b\u52a8\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9aa8\u9abc-\u70b9\u4e91-IMU\u5339\u914d\u3001\u68c0\u7d22\u548c\u65f6\u95f4\u7247\u6bb5\u68c0\u7d22\u3002\u6b64\u5916\uff0cDeSPITE\u5728MSR-Action3D\u548cHMPEAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u70b9\u4e91HAR\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "DeSPITE\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u5d4c\u5165\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5e8f\u5217\u7684\u4eba\u4f53\u6d3b\u52a8\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u4eba\u4f53\u611f\u77e5\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DeSPITE\uff1a\u63a2\u7d22\u5bf9\u6bd4\u6df1\u5ea6\u9aa8\u9abc-\u70b9\u4e91-IMU-\u6587\u672c\u5d4c\u5165\u4ee5\u63d0\u5347\u70b9\u4e91\u4eba\u4f53\u6d3b\u52a8\u7406\u89e3", "abstract_zh": "\u5c3d\u7ba1LiDAR\uff08\u5149\u63a2\u6d4b\u4e0e\u6d4b\u8ddd\uff09\u4f5c\u4e3aRGB\u6444\u50cf\u5934\u7684\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\u5728\u611f\u77e5\u4eba\u4f53\u6d3b\u52a8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u6a21\u6001\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u7814\u7a76\u4e86LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u9aa8\u9abc\u59ff\u6001\u3001IMU\u6570\u636e\u548c\u6587\u672c\u5728\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DeSPITE\uff08\u6df1\u5ea6\u9aa8\u9abc-\u70b9\u4e91-IMU-\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6709\u6548\u5b66\u4e60\u8fd9\u56db\u79cd\u6a21\u6001\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u3002\u5728\u5b9e\u8bc1\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u73b0\u6709\u7684LIPD\u548cBabel\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u56db\u79cd\u6a21\u6001\u6570\u636e\u7684\u540c\u6b65\uff0c\u4ece\u800c\u63a2\u7d22\u4e86\u65b0\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDeSPITE\u4e3a\u70b9\u4e91\u5e8f\u5217\u7684\u4eba\u4f53\u6d3b\u52a8\u7406\u89e3\u4efb\u52a1\uff08\u5982\u9aa8\u9abc<->\u70b9\u4e91<->IMU\u5339\u914d\u3001\u68c0\u7d22\u548c\u65f6\u95f4\u7247\u6bb5\u68c0\u7d22\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728MSR-Action3D\u548cHMPEAR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86DeSPITE\u4f5c\u4e3a\u70b9\u4e91HAR\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.13790", "pdf": "https://arxiv.org/pdf/2506.13790", "abs": "https://arxiv.org/abs/2506.13790", "authors": ["Tapio Pitk\u00e4ranta"], "title": "The NordDRG AI Benchmark for Large Language Models", "categories": ["cs.AI"], "comment": "15 pages, 4 figures", "summary": "Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.\n  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.\n  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark\n  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.", "AI": {"tldr": "NordDRG-AI-Benchmark\u662f\u9996\u4e2a\u9488\u5bf9\u533b\u9662\u8d44\u91d1\u5206\u914d\u5c42\uff08DRG\uff09\u7684\u516c\u5f00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u8bca\u65ad\u3001\u624b\u672f\u548c\u8d39\u7387\u903b\u8f91\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u57fa\u51c6\u5305\u542b\u4e09\u7c7b\u8d44\u6e90\uff1a\u5b9a\u4e49\u8868\u3001\u4e13\u5bb6\u624b\u518c\u548c\u4efb\u52a1\u63d0\u793a\u5305\u3002\u6d4b\u8bd5\u663e\u793a\u4e0d\u540cLLM\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u51f8\u663e\u4e86\u9886\u57df\u7279\u5f02\u6027\u3002", "motivation": "\u76ee\u524d\u5c1a\u65e0\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u9488\u5bf9\u533b\u9662\u8d44\u91d1\u5206\u914d\u5c42\uff08DRG\uff09\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5f00\u59cb\u7528\u4e8e\u4e34\u5e8a\u7f16\u7801\u548c\u51b3\u7b56\u652f\u6301\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86NordDRG-AI-Benchmark\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u8bc4\u4f30LLMs\u5728DRG\u903b\u8f91\u4e2d\u7684\u8868\u73b0\u3002", "method": "NordDRG-AI-Benchmark\u5305\u542b\u4e09\u7c7b\u8d44\u6e90\uff1a(i) \u5b9a\u4e49\u8868\uff0c\u6db5\u76d6DRG\u903b\u8f91\u3001ICD\u548cNCSP\u4ee3\u7801\u7b49\uff1b(ii) \u4e13\u5bb6\u624b\u518c\u548c\u53d8\u66f4\u65e5\u5fd7\u6a21\u677f\uff1b(iii) 14\u4e2aCaseMix\u4efb\u52a1\u7684\u63d0\u793a\u5305\u3002\u901a\u8fc7\u81ea\u52a8\u9a8c\u8bc1\u4efb\u52a1\u6d4b\u8bd5\u4e94\u79cd\u5148\u8fdbLLM\u7684\u8868\u73b0\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u4e94\u79cdLLM\u5728\u4e5d\u9879\u53ef\u81ea\u52a8\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1ao3\uff08OpenAI\uff09\u6ee1\u52069\u5206\uff0cGPT-4o\u548co4-mini-high\u5f977\u5206\uff0cGemini 2.5 Pro\u548cGemini 2.5 Flash\u5206\u522b\u5f975\u5206\u548c3\u5206\u3002", "conclusion": "NordDRG-AI-Benchmark\u63ed\u793a\u4e86LLM\u5728\u9886\u57df\u7279\u5f02\u6027\u4efb\u52a1\u4e2d\u7684\u4f18\u52a3\u52bf\uff0c\u4e3a\u533b\u9662\u8d44\u91d1\u81ea\u52a8\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\uff0c\u5f25\u8865\u4e86\u901a\u7528\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "paper_title_zh": "NordDRG AI\u57fa\u51c6\u6d4b\u8bd5\uff1a\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5f00\u59cb\u8bd5\u70b9\u7528\u4e8e\u4e34\u5e8a\u7f16\u7801\u548c\u51b3\u7b56\u652f\u6301\uff0c\u4f46\u6b64\u524d\u5c1a\u65e0\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u9488\u5bf9\u533b\u9662\u8d44\u91d1\u5206\u914d\u5c42\uff08DRG\uff09\u3002\u6211\u4eec\u53d1\u5e03\u4e86NordDRG-AI-Benchmark\uff0c\u9996\u4e2a\u516c\u5f00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6db5\u76d6\u5b8c\u6574\u7684DRG\u89c4\u5219\u96c6\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u591a\u8bed\u8a00\u8bca\u65ad\u3001\u624b\u672f\u548c\u8d39\u7387\u903b\u8f91\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u57fa\u51c6\u5305\u542b\u4e09\u7c7b\u8d44\u6e90\uff1a(i) \u5b9a\u4e49\u8868\uff0c\u6db5\u76d6DRG\u903b\u8f91\u3001ICD\u548cNCSP\u4ee3\u7801\u7b49\uff1b(ii) \u4e13\u5bb6\u624b\u518c\u548c\u53d8\u66f4\u65e5\u5fd7\u6a21\u677f\uff1b(iii) 14\u4e2aCaseMix\u4efb\u52a1\u7684\u63d0\u793a\u5305\u3002\u6d4b\u8bd5\u663e\u793a\u4e94\u79cd\u5148\u8fdbLLM\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u51f8\u663e\u4e86\u9886\u57df\u7279\u5f02\u6027\u3002\u6240\u6709\u8d44\u6e90\u53ef\u5728https://github.com/longshoreforrest/norddrg-ai-benchmark\u83b7\u53d6\u3002"}}
{"id": "2506.14012", "pdf": "https://arxiv.org/pdf/2506.14012", "abs": "https://arxiv.org/abs/2506.14012", "authors": ["Amr Mohamed", "Yang Zhang", "Michalis Vazirgiannis", "Guokan Shang"], "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text", "categories": ["cs.CL"], "comment": null, "summary": "Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\\unicode{x2013}$even under linguistic constraints$\\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4ee3\u7801\u5207\u6362\uff08CSW\uff09\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5916\u8bed\u8bcd\u6c47\u5e72\u6270\u82f1\u6587\u6587\u672c\u65f6\u6a21\u578b\u8868\u73b0\u4e0b\u964d\uff0c\u4f46\u5c06\u82f1\u6587\u5d4c\u5165\u5176\u4ed6\u8bed\u8a00\u65f6\u7406\u89e3\u80fd\u529b\u63d0\u5347\u3002\u63d0\u793a\u65b9\u6cd5\u6548\u679c\u4e0d\u4e00\uff0c\u800c\u5fae\u8c03\u662f\u66f4\u7a33\u5b9a\u7684\u6539\u8fdb\u9014\u5f84\u3002", "motivation": "\u4ee3\u7801\u5207\u6362\u5728\u591a\u8bed\u8a00\u793e\u533a\u548c\u5728\u7ebf\u5185\u5bb9\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u5185\u5bb9\u5904\u7406\u548c\u751f\u6210\u7684\u6838\u5fc3\u5de5\u5177\uff0c\u5e38\u9700\u5904\u7406\u6df7\u5408\u8bed\u8a00\u6587\u672c\u3002\u56e0\u6b64\uff0c\u7814\u7a76LLMs\u5bf9\u4ee3\u7801\u5207\u6362\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u751f\u6210\u4ee3\u7801\u5207\u6362\u7248\u672c\u7684\u73b0\u6709\u63a8\u7406\u548c\u7406\u89e3\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30LLMs\u5bf9\u6df7\u5408\u8bed\u8a00\u6587\u672c\u7684\u5904\u7406\u80fd\u529b\u3002", "result": "\u5916\u8bed\u8bcd\u6c47\u5e72\u6270\u82f1\u6587\u6587\u672c\u65f6\u6a21\u578b\u8868\u73b0\u4e0b\u964d\uff0c\u4f46\u5c06\u82f1\u6587\u5d4c\u5165\u5176\u4ed6\u8bed\u8a00\u65f6\u7406\u89e3\u80fd\u529b\u63d0\u5347\u3002\u63d0\u793a\u65b9\u6cd5\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u800c\u5fae\u8c03\u80fd\u66f4\u7a33\u5b9a\u5730\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "LLMs\u5bf9\u4ee3\u7801\u5207\u6362\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\u53d7\u8bed\u8a00\u6df7\u5408\u65b9\u5f0f\u5f71\u54cd\uff0c\u5fae\u8c03\u662f\u6539\u5584\u6a21\u578b\u8868\u73b0\u7684\u53ef\u9760\u65b9\u6cd5\u3002", "paper_title_zh": "\u8ff7\u5931\u5728\u6df7\u5408\u4e2d\uff1a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4ee3\u7801\u5207\u6362\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b", "abstract_zh": "\u4ee3\u7801\u5207\u6362\uff08CSW\uff09\u662f\u6307\u5728\u5355\u4e00\u8bdd\u8bed\u4e2d\u4ea4\u66ff\u4f7f\u7528\u4e24\u79cd\u6216\u66f4\u591a\u8bed\u8a00\u7684\u73b0\u8c61\u3002\u8fd9\u79cd\u73b0\u8c61\u5728\u591a\u8bed\u8a00\u793e\u533a\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5e76\u5728\u5728\u7ebf\u5185\u5bb9\u4e2d\u65e5\u76ca\u5e38\u89c1\uff0c\u7528\u6237\u5728\u65e5\u5e38\u4ea4\u6d41\u4e2d\u81ea\u7136\u6df7\u5408\u4f7f\u7528\u591a\u79cd\u8bed\u8a00\u3002\u56e0\u6b64\uff0c\u4f5c\u4e3a\u5185\u5bb9\u5904\u7406\u548c\u751f\u6210\u6838\u5fc3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ecf\u5e38\u63a5\u89e6\u5230\u4ee3\u7801\u5207\u6362\u8f93\u5165\u3002\u9274\u4e8e\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3LLMs\u5982\u4f55\u5904\u7406\u548c\u63a8\u7406\u6b64\u7c7b\u6df7\u5408\u8bed\u8a00\u6587\u672c\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u901a\u8fc7\u751f\u6210\u4ee3\u7801\u5207\u6362\u7248\u672c\u7684\u73b0\u6709\u63a8\u7406\u548c\u7406\u89e3\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86LLMs\u5bf9\u6df7\u5408\u8bed\u8a00\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u5916\u8bed\u8bcd\u6c47\u5e72\u6270\u82f1\u6587\u6587\u672c\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u660e\u663e\u4e0b\u964d\uff08\u5373\u4f7f\u5728\u8bed\u8a00\u7ea6\u675f\u4e0b\uff09\uff0c\u800c\u5c06\u82f1\u6587\u5d4c\u5165\u5176\u4ed6\u8bed\u8a00\u65f6\u7406\u89e3\u80fd\u529b\u5f80\u5f80\u63d0\u5347\u3002\u5c3d\u7ba1\u63d0\u793a\u65b9\u6cd5\u6548\u679c\u4e0d\u4e00\uff0c\u4f46\u5fae\u8c03\u4e3a\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u9014\u5f84\u3002"}}
{"id": "2506.13902", "pdf": "https://arxiv.org/pdf/2506.13902", "abs": "https://arxiv.org/abs/2506.13902", "authors": ["Raymond Yu", "Paul Han", "Josh Myers-Dean", "Piper Wolters", "Favyen Bastani"], "title": "OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data", "categories": ["cs.CV"], "comment": "WACV 2025", "summary": "In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.", "AI": {"tldr": "OPTIMUS\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u6301\u4e45\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d8\u5316\u68c0\u6d4b\u7684\u6027\u80fd\uff0cAUROC\u5206\u6570\u4ece56.3%\u63d0\u5347\u81f387.6%\u3002", "motivation": "21\u4e16\u7eaa\u9762\u4e34\u4e25\u5cfb\u7684\u73af\u5883\u95ee\u9898\uff0c\u76d1\u6d4b\u5730\u8868\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u53d8\u5316\u6807\u7b7e\u7684\u536b\u661f\u6570\u636e\uff0c\u5c24\u5176\u662f\u7a00\u6709\u53d8\u5316\u7c7b\u522b\uff0c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\u3002OPTIMUS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "OPTIMUS\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u539f\u5219\uff1a\u5982\u679c\u6a21\u578b\u80fd\u6062\u590d\u65f6\u95f4\u5e8f\u5217\u4e2d\u56fe\u50cf\u7684\u76f8\u5bf9\u987a\u5e8f\u4fe1\u606f\uff0c\u5219\u8868\u660e\u56fe\u50cf\u4e2d\u5b58\u5728\u6301\u4e45\u53d8\u5316\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\u70b9\u68c0\u6d4b\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "result": "OPTIMUS\u5728\u533a\u5206\u53d8\u5316\u4e0e\u672a\u53d8\u5316\u65f6\u95f4\u5e8f\u5217\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cAUROC\u5206\u6570\u4ece\u57fa\u7ebf\u768456.3%\u63d0\u5347\u81f387.6%\u3002", "conclusion": "OPTIMUS\u4e3a\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u6301\u4e45\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "paper_title_zh": "OPTIMUS\uff1a\u591a\u65f6\u76f8\u65e0\u6807\u7b7e\u536b\u661f\u6570\u636e\u4e2d\u6301\u4e45\u53d8\u5316\u7684\u89c2\u6d4b", "abstract_zh": "\u9762\u5bf921\u4e16\u7eaa\u7d27\u8feb\u7684\u73af\u5883\u95ee\u9898\uff0c\u76d1\u6d4b\u5730\u7403\u8868\u9762\u53d8\u5316\u6bd4\u4ee5\u5f80\u4efb\u4f55\u65f6\u5019\u90fd\u66f4\u52a0\u91cd\u8981\u3002\u5927\u89c4\u6a21\u9065\u611f\u6280\u672f\uff08\u5982\u536b\u661f\u56fe\u50cf\uff09\u662f\u5b8c\u6210\u8fd9\u4e00\u4efb\u52a1\u7684\u91cd\u8981\u5de5\u5177\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u53d8\u5316\u6807\u7b7e\u7684\u536b\u661f\u6570\u636e\uff0c\u5c24\u5176\u662f\u7a00\u6709\u53d8\u5316\u7c7b\u522b\uff0c\u4f7f\u7528\u76d1\u7763\u65b9\u6cd5\u68c0\u6d4b\u53d8\u5316\u5341\u5206\u56f0\u96be\u3002\u536b\u661f\u56fe\u50cf\u4e2d\u53d8\u5316\u7a00\u758f\uff0c\u6807\u6ce8\u6781\u5177\u6311\u6218\u6027\u3002\u5373\u4f7f\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u96c6\u4e2d\uff0c\u53ea\u6709\u4e00\u5c0f\u90e8\u5206\u53ef\u80fd\u8868\u73b0\u51fa\u611f\u5174\u8da3\u7684\u6301\u4e45\u53d8\u5316\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OPTIMUS\uff0c\u4e00\u79cd\u57fa\u4e8e\u76f4\u89c2\u539f\u5219\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff1a\u5982\u679c\u6a21\u578b\u80fd\u591f\u6062\u590d\u65f6\u95f4\u5e8f\u5217\u4e2d\u56fe\u50cf\u7684\u76f8\u5bf9\u987a\u5e8f\u4fe1\u606f\uff0c\u5219\u8868\u660e\u56fe\u50cf\u4e2d\u5b58\u5728\u6301\u4e45\u53d8\u5316\u3002OPTIMUS\u901a\u8fc7\u5728\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8f93\u51fa\u4e0a\u5e94\u7528\u53d8\u5316\u70b9\u68c0\u6d4b\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u539f\u5219\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOPTIMUS\u80fd\u76f4\u63a5\u68c0\u6d4b\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u6709\u8da3\u53d8\u5316\uff0c\u5728\u533a\u5206\u53d8\u5316\u4e0e\u672a\u53d8\u5316\u65f6\u95f4\u5e8f\u5217\u65b9\u9762\uff0cAUROC\u5206\u6570\u4ece\u57fa\u7ebf\u768456.3%\u63d0\u5347\u81f387.6%\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff1ahttps://huggingface.co/datasets/optimus-change/optimus-dataset/\u3002"}}
{"id": "2506.13792", "pdf": "https://arxiv.org/pdf/2506.13792", "abs": "https://arxiv.org/abs/2506.13792", "authors": ["Gon\u00e7alo Hora de Carvalho", "Lazar S. Popov", "Sander Kaatee", "Kristinn R. Th\u00f3risson", "Tangrui Li", "P\u00e9tur H\u00fani Bj\u00f6rnsson", "Jilles S. Dibangoye"], "title": "ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \\& a ML Ensemble on Longitudinal Identity Resolution", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ICE-ID\uff0c\u4e00\u4e2a\u7528\u4e8e\u5386\u53f2\u8eab\u4efd\u89e3\u6790\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d61703-1920\u5e74\u7684\u51b0\u5c9b\u4eba\u53e3\u666e\u67e5\u8bb0\u5f55\u3002\u901a\u8fc7\u6bd4\u8f83NARS\u3001LLMs\u548cML\u96c6\u6210\u65b9\u6cd5\uff0c\u7814\u7a76\u53d1\u73b0NARS\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8de8\u5b66\u79d1\u7814\u7a76\u3002", "motivation": "\u5386\u53f2\u8eab\u4efd\u89e3\u6790\u5728\u4eba\u53e3\u666e\u67e5\u548c\u5bb6\u8c31\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u5f00\u653e\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6bd4\u8f83\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7ICE-ID\u6570\u636e\u96c6\u548c\u591a\u79cd\u65b9\u6cd5\u7684\u6bd4\u8f83\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86ICE-ID\u6570\u636e\u96c6\uff0c\u5305\u542b220\u5e74\u7684\u51b0\u5c9b\u4eba\u53e3\u666e\u67e5\u8bb0\u5f55\uff0c\u5b9a\u4e49\u4e86\u8eab\u4efd\u89e3\u6790\u4efb\u52a1\uff0c\u5e76\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u5339\u914d\u5668\u3001ML\u96c6\u6210\u65b9\u6cd5\u3001LLMs\u4ee5\u53caNARS\uff08\u4e00\u79cd\u57fa\u4e8e\u975e\u516c\u7406\u903b\u8f91\u7684\u901a\u7528AI\u6846\u67b6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNARS\u65b9\u6cd5\u5728\u8eab\u4efd\u89e3\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f73\u6c34\u5e73\uff08SOTA\uff09\uff0c\u540c\u65f6\u5176\u8bbe\u8ba1\u7b80\u5355\u4e14\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03ICE-ID\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u672c\u6587\u4e3a\u7eb5\u5411\u8eab\u4efd\u89e3\u6790\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u5e76\u5e0c\u671b\u63a8\u52a8\u6570\u636e\u94fe\u63a5\u548c\u5386\u53f2\u5206\u6790\u9886\u57df\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u3002", "paper_title_zh": "ICE-ID\uff1a\u4e00\u79cd\u65b0\u578b\u5386\u53f2\u4eba\u53e3\u666e\u67e5\u6570\u636e\u57fa\u51c6\uff0c\u6bd4\u8f83NARS\u4e0eLLMs\u4ee5\u53caML\u96c6\u6210\u65b9\u6cd5\u5728\u7eb5\u5411\u8eab\u4efd\u89e3\u6790\u4e2d\u7684\u8868\u73b0", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86ICE-ID\uff0c\u4e00\u4e2a\u7528\u4e8e\u5386\u53f2\u8eab\u4efd\u89e3\u6790\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d61703-1920\u5e74\u7684\u51b0\u5c9b\u4eba\u53e3\u666e\u67e5\u8bb0\u5f55\u3002ICE-ID\u5305\u542b\u591a\u4ee3\u7eb5\u5411\u6570\u636e\uff0c\u8bb0\u5f55\u4e86\u59d3\u540d\u53d8\u5316\u3001\u4eba\u53e3\u7edf\u8ba1\u53d8\u5316\u548c\u4e30\u5bcc\u7684\u5bb6\u8c31\u94fe\u63a5\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u7814\u7a76\u771f\u5b9e\u4eba\u53e3\u4e2d\u957f\u671f\u4e2a\u4eba\u5b9e\u4f53\u5339\u914d\u7684\u5927\u89c4\u6a21\u5f00\u653e\u8868\u683c\u6570\u636e\u96c6\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u8eab\u4efd\u89e3\u6790\u4efb\u52a1\uff08\u5305\u62ec\u666e\u67e5\u6ce2\u6b21\u5185\u548c\u8de8\u6ce2\u6b21\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6307\u6807\u548c\u5206\u5272\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u65b9\u6cd5\uff1a\u624b\u5de5\u89c4\u5219\u5339\u914d\u5668\u3001ML\u96c6\u6210\u65b9\u6cd5\u4ee5\u53ca\u7528\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u7684LLMs\uff08\u5982\u57fa\u4e8eTransformer\u7684\u8868\u683c\u7f51\u7edc\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u4e00\u79cd\u540d\u4e3aNARS\uff08\u975e\u516c\u7406\u63a8\u7406\u7cfb\u7edf\uff09\u7684\u65b0\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002NARS\u662f\u4e00\u79cd\u901a\u7528AI\u6846\u67b6\uff0c\u65e8\u5728\u5728\u77e5\u8bc6\u548c\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u63a8\u7406\uff0c\u5176\u6838\u5fc3\u662f\u975e\u516c\u7406\u903b\u8f91\uff08NAL\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cNARS\u8bbe\u8ba1\u7b80\u5355\u4e14\u4e0e\u5176\u4ed6\u6807\u51c6\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u6211\u4eec\u7684\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\u3002\u901a\u8fc7\u53d1\u5e03ICE-ID\u548c\u4ee3\u7801\uff0c\u6211\u4eec\u4e3a\u7eb5\u5411\u8eab\u4efd\u89e3\u6790\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u5e76\u5e0c\u671bICE-ID\u80fd\u4e3a\u6570\u636e\u94fe\u63a5\u548c\u5386\u53f2\u5206\u6790\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.14028", "pdf": "https://arxiv.org/pdf/2506.14028", "abs": "https://arxiv.org/abs/2506.14028", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.", "AI": {"tldr": "MultiFinBen\u662f\u9996\u4e2a\u9488\u5bf9\u5168\u7403\u91d1\u878d\u9886\u57df\u7684\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\u591a\u4e3a\u5355\u8bed\u8a00\u548c\u5355\u6a21\u6001\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u91d1\u878d\u573a\u666f\u7684\u590d\u6742\u6027\u3002MultiFinBen\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u91d1\u878d\u9886\u57df\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51faMultiFinBen\u57fa\u51c6\uff0c\u5305\u542b\u591a\u8bed\u8a00\u91d1\u878d\u95ee\u7b54\uff08PolyFiQA-Easy/Expert\uff09\u548cOCR\u5d4c\u5165\u91d1\u878d\u95ee\u7b54\uff08EnglishOCR/SpanishOCR\uff09\u4efb\u52a1\uff0c\u91c7\u7528\u52a8\u6001\u96be\u5ea6\u9009\u62e9\u673a\u5236\uff0c\u786e\u4fdd\u6570\u636e\u96c6\u7d27\u51d1\u5e73\u8861\u3002", "result": "\u5bf922\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5f3a\u6a21\u578b\u5728\u590d\u6742\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u91d1\u878d\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "MultiFinBen\u4e3a\u91d1\u878d\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002", "paper_title_zh": "MultiFinBen\uff1a\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u4e14\u96be\u5ea6\u611f\u77e5\u7684\u91d1\u878d\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\u52a0\u901f\u4e86\u91d1\u878d\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5e94\u7528\u7684\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ecd\u5c40\u9650\u4e8e\u5355\u8bed\u8a00\u548c\u5355\u6a21\u6001\u8bbe\u7f6e\uff0c\u4e14\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5355\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u91d1\u878d\u4ea4\u6d41\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86MultiFinBen\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u5168\u7403\u91d1\u878d\u9886\u57df\u7684\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLMs\u5728\u6587\u672c\u3001\u89c6\u89c9\u548c\u97f3\u9891\u7b49\u591a\u6a21\u6001\u53ca\u5355\u8bed\u8a00\u3001\u53cc\u8bed\u548c\u591a\u8bed\u8a00\u7b49\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u9879\u65b0\u4efb\u52a1\uff1aPolyFiQA-Easy\u548cPolyFiQA-Expert\uff0c\u8fd9\u662f\u9996\u4e2a\u8981\u6c42\u6a21\u578b\u5bf9\u6df7\u5408\u8bed\u8a00\u8f93\u5165\u8fdb\u884c\u590d\u6742\u63a8\u7406\u7684\u591a\u8bed\u8a00\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\uff1b\u4ee5\u53caEnglishOCR\u548cSpanishOCR\uff0c\u8fd9\u662f\u9996\u4e2a\u5d4c\u5165OCR\u7684\u91d1\u878d\u95ee\u7b54\u4efb\u52a1\uff0c\u6311\u6218\u6a21\u578b\u4ece\u89c6\u89c9-\u6587\u672c\u91d1\u878d\u6587\u6863\u4e2d\u63d0\u53d6\u4fe1\u606f\u5e76\u8fdb\u884c\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u3001\u96be\u5ea6\u611f\u77e5\u7684\u9009\u62e9\u673a\u5236\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u7d27\u51d1\u4e14\u5e73\u8861\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u975e\u7b80\u5355\u805a\u5408\u73b0\u6709\u6570\u636e\u96c6\u3002\u5bf922\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5f3a\u7684\u6a21\u578b\uff0c\u5c3d\u7ba1\u5177\u5907\u901a\u7528\u7684\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5728\u91d1\u878d\u9886\u57df\u7684\u590d\u6742\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002MultiFinBen\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u91d1\u878d\u7814\u7a76\u548c\u5e94\u7528\u7684\u900f\u660e\u3001\u53ef\u590d\u73b0\u548c\u5305\u5bb9\u6027\u53d1\u5c55\u3002"}}
{"id": "2506.13910", "pdf": "https://arxiv.org/pdf/2506.13910", "abs": "https://arxiv.org/abs/2506.13910", "authors": ["Aritra Dutta", "Pushpita Boral", "G Suseela"], "title": "Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u667a\u80fd\u56fe\u50cf\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u66b4\u529b\u4e8b\u4ef6\u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u7ed3\u54083D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u5411LSTM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u72af\u7f6a\u7387\u4e0a\u5347\u53ca\u4f20\u7edf\u76d1\u63a7\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u81ea\u52a8\u66b4\u529b\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u51cf\u5c11\u4eba\u529b\u548c\u8d22\u4ea7\u635f\u5931\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u4e8c\u5143\u548c\u591a\u7c7b\u66b4\u529b\u5206\u7c7b\uff0c\u4f7f\u75283D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u68c0\u6d4b\uff0c\u7ed3\u5408\u53ef\u5206\u79bb\u5377\u79ef3D\u6a21\u578b\u548c\u53cc\u5411LSTM\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u4e0e\u65f6\u95f4\u5e8f\u5217\u5904\u7406\uff0c\u5e76\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u76d1\u63a7\u89c6\u9891\u6d41\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u66b4\u529b\u4e8b\u4ef6\u7684\u81ea\u52a8\u68c0\u6d4b\u4e0e\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u667a\u80fd\u56fe\u50cf\u611f\u77e5\u7528\u4e8e\u72af\u7f6a\u5206\u6790\uff1a\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u589e\u5f3a\u66b4\u529b\u68c0\u6d4b\u4e0e\u8c03\u67e5\u65b9\u6cd5", "abstract_zh": "\u5168\u7403\u72af\u7f6a\u7387\u7684\u4e0a\u5347\u4ee5\u53ca\u5de8\u5927\u7684\u4eba\u529b\u548c\u8d22\u4ea7\u635f\u5931\uff0c\u51f8\u663e\u4e86\u4f20\u7edf\u76d1\u63a7\u65b9\u6cd5\u5728\u53ca\u65f6\u68c0\u6d4b\u591a\u6837\u5316\u548c\u7a81\u53d1\u66b4\u529b\u884c\u4e3a\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u4e3a\u6ee1\u8db3\u81ea\u52a8\u66b4\u529b\u68c0\u6d4b\u7684\u8feb\u5207\u9700\u6c42\uff0c\u6211\u4eec\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u68c0\u6d4b\u548c\u5206\u7c7b\u89c6\u9891\u6d41\u4e2d\u7684\u66b4\u529b\u4e8b\u4ef6\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u66b4\u529b\u68c0\u6d4b\u4e0e\u5206\u7c7b\u6846\u67b6\uff0c\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u4e8c\u5143\u548c\u591a\u7c7b\u66b4\u529b\u5206\u7c7b\u3002\u68c0\u6d4b\u6a21\u578b\u57fa\u4e8e3D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5206\u7c7b\u6a21\u578b\u5219\u5229\u7528\u53ef\u5206\u79bb\u5377\u79ef3D\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408\u53cc\u5411LSTM\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u3002\u8bad\u7ec3\u5728\u591a\u6837\u5316\u7684\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5305\u542b\u6765\u81ea\u76d1\u63a7\u6444\u50cf\u5934\u3001\u4eba\u7c7b\u5f55\u5236\u3001\u51b0\u7403\u6597\u6bb4\u3001SOHAS\u548cWVD\u6570\u636e\u96c6\u7684\u89c6\u9891\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528\u96c6\u6210\u6811\u8393\u6d3e\u7684\u6444\u50cf\u5934\u6a21\u5757\u6355\u83b7\u5b9e\u65f6\u89c6\u9891\u6d41\uff0c\u5e76\u53d1\u9001\u81f3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5904\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2506.13793", "pdf": "https://arxiv.org/pdf/2506.13793", "abs": "https://arxiv.org/abs/2506.13793", "authors": ["Zongxian Yang", "Jiayu Qian", "Zegao Peng", "Haoyu Zhang", "Zhi-An Huang"], "title": "Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection", "categories": ["cs.AI"], "comment": null, "summary": "Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \\underline{\\textbf{Med}}ical \\underline{\\textbf{R}}easoning \\underline{\\textbf{E}}nhancement via self-corrected \\underline{\\textbf{F}}ine-grained ref\\underline{\\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \\href{https://github.com/TianYin123/Med-REFL}{here}.", "AI": {"tldr": "Med-REFL\u901a\u8fc7\u7ec6\u7c92\u5ea6\u81ea\u6821\u6b63\u53cd\u601d\u63d0\u5347\u533b\u7597\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u5e73\u5747\u63d0\u53474.11%\uff0c\u5e76\u57287B/8B\u6a21\u578b\u4e0a\u8fdb\u4e00\u6b65\u4f18\u53164.13%\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u533b\u7597\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4e2d\u95f4\u53cd\u601d\u6b65\u9aa4\u7684\u8d28\u91cf\u4e0d\u8db3\u3002\u533b\u7597\u573a\u666f\u7684\u9ad8\u98ce\u9669\u6027\u8981\u6c42\u66f4\u9ad8\u7684\u63a8\u7406\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u63d0\u5347\u53cd\u601d\u8d28\u91cf\u3002", "method": "Med-REFL\u91c7\u7528\u6811\u72b6\u601d\u7ef4\u65b9\u6cd5\uff0c\u5c06\u533b\u7597\u95ee\u9898\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5b9a\u91cf\u8bc4\u4f30\u6bcf\u4e00\u6b65\u53ca\u5176\u540e\u7eed\u53cd\u601d\u3002\u901a\u8fc7\u81ea\u52a8\u6784\u5efa\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6570\u636e\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u4e13\u5bb6\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u5f15\u5bfc\u6a21\u578b\u8bc6\u522b\u548c\u7ea0\u6b63\u63a8\u7406\u9519\u8bef\u3002", "result": "\u5728MedQA-USMLE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMed-REFL\u5e73\u5747\u63d0\u53474.11%\uff0c\u5e76\u57287B/8B\u6a21\u578b\u4e0a\u8fdb\u4e00\u6b65\u4f18\u53164.13%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6311\u6218\u6027\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Med-REFL\u901a\u8fc7\u4f18\u5148\u8003\u8651\u53cd\u601d\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597AI\u5e94\u7528\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u533b\u7597\u9886\u57df\u7684\u9ad8\u98ce\u9669\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "Med-REFL\uff1a\u901a\u8fc7\u81ea\u6821\u6b63\u7ec6\u7c92\u5ea6\u53cd\u601d\u589e\u5f3a\u533b\u7597\u63a8\u7406", "abstract_zh": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u6210\u529f\u5e76\u672a\u987a\u5229\u6269\u5c55\u5230\u533b\u7597\u9886\u57df\u3002\u5c3d\u7ba1\u591a\u79cd\u56e0\u7d20\u5bfc\u81f4\u4e86\u8fd9\u4e00\u5dee\u5f02\uff0c\u4f46\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u662f\u4e2d\u95f4\u53cd\u601d\u6b65\u9aa4\u7684\u8d28\u91cf\u4e0d\u8db3\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u533b\u7597\u573a\u666f\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Med-REFL\uff0c\u4e00\u79cd\u901a\u8fc7\u81ea\u6821\u6b63\u7ec6\u7c92\u5ea6\u53cd\u601d\u589e\u5f3a\u533b\u7597\u63a8\u7406\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u6811\u72b6\u601d\u7ef4\u5c06\u533b\u7597\u95ee\u9898\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u63a8\u7406\u8def\u5f84\uff0c\u5b9a\u91cf\u8bc4\u4f30\u6bcf\u4e00\u6b65\u53ca\u5176\u540e\u7eed\u53cd\u601d\u3002\u8fd9\u4e9b\u8bc4\u4f30\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6570\u636e\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u4e13\u5bb6\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u5f15\u5bfc\u6a21\u578b\u8bc6\u522b\u548c\u7ea0\u6b63\u63a8\u7406\u9519\u8bef\u3002\u5728MedQA-USMLE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMed-REFL\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\uff0c\u5e73\u5747\u63d0\u5347\u8fbe4.11%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u8fdb\u4e00\u6b65\u5c067B/8B\u6a21\u578b\u7684\u5148\u8fdb\u6027\u80fd\u63d0\u5347\u4e864.13%\u3002\u6b64\u5916\uff0cMed-REFL\u5728\u591a\u4e2a\u6311\u6218\u6027\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u4f18\u5148\u8003\u8651\u53cd\u601d\u8d28\u91cf\u80fd\u591f\u4e3a\u533b\u7597AI\u5e94\u7528\u5e26\u6765\u66f4\u51c6\u786e\u548c\u53ef\u4fe1\u7684\u63a8\u7406\u3002\u68c0\u67e5\u70b9\u3001\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728\u6b64\u5904\u627e\u5230\u3002"}}
{"id": "2506.14040", "pdf": "https://arxiv.org/pdf/2506.14040", "abs": "https://arxiv.org/abs/2506.14040", "authors": ["Md Nazmus Sakib"], "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5e38\u8bc6\u63a8\u7406\u548c\u610f\u56fe\u68c0\u6d4b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e8628\u7bc7\u8bba\u6587\uff0c\u603b\u7ed3\u4e86\u65b9\u6cd5\u4e0e\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u5e38\u8bc6\u63a8\u7406\u548c\u610f\u56fe\u68c0\u6d4b\u662f\u4e24\u5927\u5173\u952e\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8de8\u5b66\u79d1\u89c6\u89d2\u68b3\u7406\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5206\u67902020-2025\u5e74\u95f4ACL\u3001EMNLP\u548cCHI\u768428\u7bc7\u8bba\u6587\uff0c\u6309\u65b9\u6cd5\u548c\u5e94\u7528\u5206\u7c7b\uff0c\u5206\u522b\u63a2\u8ba8\u5e38\u8bc6\u63a8\u7406\u548c\u610f\u56fe\u68c0\u6d4b\u7684\u7814\u7a76\u73b0\u72b6\u3002", "result": "\u603b\u7ed3\u4e86\u5e38\u8bc6\u63a8\u7406\u5728\u96f6\u6837\u672c\u5b66\u4e60\u3001\u6587\u5316\u9002\u5e94\u3001\u7ed3\u6784\u5316\u8bc4\u4f30\u548c\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u8fdb\u5c55\uff0c\u4ee5\u53ca\u610f\u56fe\u68c0\u6d4b\u5728\u5f00\u653e\u96c6\u6a21\u578b\u3001\u751f\u6210\u5f0f\u65b9\u6cd5\u3001\u805a\u7c7b\u548c\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u7684\u7814\u7a76\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u3001\u591a\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u7684\u65b0\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u57fa\u7840\u7406\u8bba\u3001\u6cdb\u5316\u80fd\u529b\u548c\u57fa\u51c6\u8bbe\u8ba1\u65b9\u9762\u7684\u5173\u952e\u7a7a\u767d\u3002", "paper_title_zh": "\u8de8\u5b66\u79d1\u7efc\u8ff0\uff1a\u5e38\u8bc6\u63a8\u7406\u4e0e\u610f\u56fe\u68c0\u6d4b", "abstract_zh": "\u672c\u6587\u7efc\u8ff0\u4e86\u5e38\u8bc6\u63a8\u7406\u548c\u610f\u56fe\u68c0\u6d4b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u6211\u4eec\u5206\u6790\u4e862020-2025\u5e74\u95f4ACL\u3001EMNLP\u548cCHI\u768428\u7bc7\u8bba\u6587\uff0c\u5e76\u6309\u65b9\u6cd5\u548c\u5e94\u7528\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u5e38\u8bc6\u63a8\u7406\u7684\u7814\u7a76\u6db5\u76d6\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u3001\u6587\u5316\u9002\u5e94\u3001\u7ed3\u6784\u5316\u8bc4\u4f30\u548c\u4ea4\u4e92\u573a\u666f\uff1b\u610f\u56fe\u68c0\u6d4b\u5219\u901a\u8fc7\u5f00\u653e\u96c6\u6a21\u578b\u3001\u751f\u6210\u5f0f\u65b9\u6cd5\u3001\u805a\u7c7b\u548c\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002\u901a\u8fc7\u7ed3\u5408NLP\u548cHCI\u7684\u89c6\u89d2\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u81ea\u9002\u5e94\u3001\u591a\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u7684\u65b0\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u7406\u8bba\u57fa\u7840\u3001\u6cdb\u5316\u80fd\u529b\u548c\u57fa\u51c6\u8bbe\u8ba1\u65b9\u9762\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2506.13925", "pdf": "https://arxiv.org/pdf/2506.13925", "abs": "https://arxiv.org/abs/2506.13925", "authors": ["Numair Nadeem", "Saeed Anwar", "Muhammad Hamza Asad", "Abdul Bais"], "title": "HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.", "AI": {"tldr": "HierVL\u662f\u4e00\u79cd\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u7a00\u7f3a\u548c\u9886\u57df\u53d8\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5728\u6807\u7b7e\u7a00\u7f3a\u548c\u9886\u57df\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u89c6\u89c9\u65b9\u6cd5\u6613\u8bef\u5206\u7c7b\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u3002HierVL\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u5347\u5206\u5272\u6548\u679c\u3002", "method": "HierVL\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u5206\u5c42\u8bed\u4e49\u67e5\u8be2\u751f\u6210\u5668\u3001\u8de8\u6a21\u6001\u7a7a\u95f4\u5bf9\u9f50\u6a21\u5757\u548c\u53cc\u67e5\u8be2Transformer\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u6587\u672c-\u7a7a\u95f4\u67e5\u8be2\u5bf9\u9f50\u548c\u6b63\u5219\u5316\u635f\u5931\u4f18\u5316\u5206\u5272\u6027\u80fd\u3002", "result": "\u5728COCO\u3001Pascal VOC\u3001ADE20\u548cCityscapes\u6570\u636e\u96c6\u4e0a\uff0cHierVL\u5206\u522b\u5b9e\u73b0\u4e864.4%\u30013.1%\u30015.9%\u548c1.8%\u7684mIoU\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HierVL\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u7684\u5206\u5272\u586b\u8865\u4e86\u6807\u7b7e\u6548\u7387\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u548c\u5b9e\u4f8b\u611f\u77e5\u7684\u6cdb\u5316\uff0c\u4e3a\u534a\u76d1\u7763\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "HierVL\uff1a\u5229\u7528\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00\u534f\u540c\u4e0e\u52a8\u6001\u6587\u672c-\u7a7a\u95f4\u67e5\u8be2\u5bf9\u9f50\u7684\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5", "abstract_zh": "\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5728\u6807\u7b7e\u7a00\u7f3a\u548c\u9886\u57df\u53d8\u5316\u4e0b\u4ecd\u5177\u6311\u6218\u6027\u3002\u7eaf\u89c6\u89c9\u65b9\u6cd5\u6613\u51fa\u73b0\u50cf\u7d20\u8bef\u5206\u7c7b\u548c\u8fb9\u754c\u5b9a\u4f4d\u95ee\u9898\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u867d\u63d0\u4f9b\u9886\u57df\u4e0d\u53d8\u7684\u8bed\u4e49\uff0c\u4f46\u7f3a\u4e4f\u5bc6\u96c6\u9884\u6d4b\u6240\u9700\u7684\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u3002\u672c\u6587\u63d0\u51faHierVL\uff0c\u901a\u8fc7\u5c06\u62bd\u8c61\u6587\u672c\u5d4c\u5165\u6574\u5408\u5230\u4e13\u4e3a\u534a\u76d1\u7763\u5206\u5272\u8bbe\u8ba1\u7684\u63a9\u7801Transformer\u67b6\u6784\u4e2d\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002HierVL\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff1a\u5206\u5c42\u8bed\u4e49\u67e5\u8be2\u751f\u6210\u5668\uff0c\u7528\u4e8e\u8fc7\u6ee4\u548c\u6295\u5f71\u62bd\u8c61\u7c7b\u522b\u5d4c\u5165\u4e3a\u591a\u5c3a\u5ea6\u67e5\u8be2\u4ee5\u6291\u5236\u65e0\u5173\u7c7b\u522b\uff1b\u8de8\u6a21\u6001\u7a7a\u95f4\u5bf9\u9f50\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u7a00\u758f\u76d1\u7763\u4e0b\u5bf9\u9f50\u8bed\u4e49\u67e5\u8be2\u4e0e\u50cf\u7d20\u7279\u5f81\u4ee5\u9510\u5316\u8fb9\u754c\uff1b\u4ee5\u53ca\u53cc\u67e5\u8be2Transformer\u89e3\u7801\u5668\uff0c\u7528\u4e8e\u878d\u5408\u8bed\u4e49\u548c\u5b9e\u4f8b\u7ea7\u67e5\u8be2\u4ee5\u9632\u6b62\u5b9e\u4f8b\u5d29\u6e83\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u76ee\u6807\u6b63\u5219\u5316\u635f\u5931\uff0c\u4ee5\u5728\u8bad\u7ec3\u4e2d\u4fdd\u6301\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u5f3a\u5316\u8bed\u4e49\u5b9a\u4f4d\u3002HierVL\u5728COCO\uff08232\u5f20\u6807\u6ce8\u56fe\u50cf\uff09\u3001Pascal VOC\uff0892\u5f20\u6807\u6ce8\uff09\u3001ADE20\uff08158\u5f20\u6807\u6ce8\uff09\u548cCityscapes\uff08100\u5f20\u6807\u6ce8\uff09\u4e0a\u5206\u522b\u5b9e\u73b0\u4e864.4%\u30013.1%\u30015.9%\u548c1.8%\u7684mIoU\u5e73\u5747\u63d0\u5347\uff0c\u57281%\u76d1\u7763\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u8a00\u5f15\u5bfc\u7684\u5206\u5272\u586b\u8865\u4e86\u6807\u7b7e\u6548\u7387\u5dee\u8ddd\uff0c\u5e76\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u548c\u5b9e\u4f8b\u611f\u77e5\u7684\u6cdb\u5316\u3002"}}
{"id": "2506.13795", "pdf": "https://arxiv.org/pdf/2506.13795", "abs": "https://arxiv.org/abs/2506.13795", "authors": ["Boshen Shi", "Yongqing Wang", "Fangda Guo", "Jiangli Shao", "Huawei Shen", "Xueqi Cheng"], "title": "BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection", "categories": ["cs.AI", "cs.SI"], "comment": "Accetpted to ECML-PKDD 2025 Research Track as oral; Code&data: https://github.com/Skyorca/BotTrans", "summary": "Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \\textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \\textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBotTrans\u7684\u591a\u6e90\u56fe\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002\u901a\u8fc7\u5229\u7528\u591a\u6e90\u7f51\u7edc\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u8de8\u57df\u90bb\u5c45\u4fe1\u606f\u805a\u5408\uff0cBotTrans\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u4e2d\uff0c\u6807\u7b7e\u7a00\u7f3a\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u3002\u73b0\u6709\u7684\u5355\u6e90\u8fc1\u79fb\u65b9\u6cd5\u56e0\u7f51\u7edc\u5f02\u8d28\u6027\u548c\u6e90\u7f51\u7edc\u76f8\u5173\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u4e0d\u7a33\u5b9a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u591a\u6e90\u8fc1\u79fb\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BotTrans\u901a\u8fc7\u591a\u6e90\u7f51\u7edc\u6784\u5efa\u8de8\u6e90\u57df\u62d3\u6251\u7ed3\u6784\uff0c\u63d0\u9ad8\u7f51\u7edc\u540c\u8d28\u6027\uff1b\u805a\u5408\u8de8\u57df\u90bb\u5c45\u4fe1\u606f\u4ee5\u589e\u5f3a\u8282\u70b9\u5d4c\u5165\u7684\u533a\u5206\u6027\uff1b\u7ed3\u5408\u6e90-\u76ee\u6807\u5bf9\u7684\u76f8\u5173\u6027\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u5229\u7528\u76ee\u6807\u57df\u8bed\u4e49\u77e5\u8bc6\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBotTrans\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u6e90\u77e5\u8bc6\u63d0\u5347\u65e0\u6807\u7b7e\u76ee\u6807\u4efb\u52a1\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "BotTrans\u901a\u8fc7\u591a\u6e90\u77e5\u8bc6\u8fc1\u79fb\u548c\u8de8\u57df\u4fe1\u606f\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u4e3a\u65e0\u6807\u7b7e\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "BotTrans\uff1a\u4e00\u79cd\u591a\u6e90\u56fe\u57df\u9002\u5e94\u65b9\u6cd5\u7528\u4e8e\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b", "abstract_zh": "\u4ece\u76f8\u5173\u793e\u4ea4\u7f51\u7edc\u4e2d\u8fc1\u79fb\u4e30\u5bcc\u77e5\u8bc6\u5df2\u6210\u4e3a\u89e3\u51b3\u57fa\u4e8eGNN\u7684\u793e\u4ea4\u673a\u5668\u4eba\u53ca\u5176\u4ed6\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u6709\u6548\u8fc1\u79fb\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u7f51\u7edc\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5373\u673a\u5668\u4eba\u901a\u8fc7\u65e0\u5dee\u522b\u4e0e\u4eba\u7c7b\u7528\u6237\u4e92\u52a8\u9690\u85cf\u6076\u610f\u884c\u4e3a\uff0c\u963b\u788d\u4e86\u6a21\u578b\u4ece\u6e90\u57df\u5b66\u4e60\u8db3\u591f\u51c6\u786e\u7684\u673a\u5668\u4eba\u76f8\u5173\u77e5\u8bc6\uff1b\u4e8c\u662f\u5355\u6e90\u8fc1\u79fb\u53ef\u80fd\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u4e14\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6e90\u7f51\u7edc\u53ef\u80fd\u4e0e\u4efb\u52a1\u76f8\u5173\u6027\u8f83\u5f31\u4e14\u63d0\u4f9b\u6709\u9650\u77e5\u8bc6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63a2\u7d22\u591a\u6e90\u57df\u5e76\u63d0\u51fa\u591a\u6e90\u56fe\u57df\u9002\u5e94\u6a21\u578bBotTrans\u3002\u6211\u4eec\u9996\u5148\u5229\u7528\u591a\u6e90\u7f51\u7edc\u5171\u4eab\u7684\u6807\u7b7e\u77e5\u8bc6\u6784\u5efa\u8de8\u6e90\u57df\u62d3\u6251\u7ed3\u6784\u4ee5\u63d0\u9ad8\u7f51\u7edc\u540c\u8d28\u6027\uff1b\u968f\u540e\u805a\u5408\u8de8\u57df\u90bb\u5c45\u4fe1\u606f\u4ee5\u589e\u5f3a\u6e90\u8282\u70b9\u5d4c\u5165\u7684\u533a\u5206\u6027\uff1b\u63a5\u7740\u5c06\u5404\u6e90-\u76ee\u6807\u5bf9\u7684\u76f8\u5173\u6027\u4e0e\u6a21\u578b\u4f18\u5316\u7ed3\u5408\uff0c\u4fc3\u8fdb\u4ece\u4e0e\u68c0\u6d4b\u4efb\u52a1\u66f4\u76f8\u5173\u7684\u6e90\u7f51\u7edc\u8fc1\u79fb\u77e5\u8bc6\uff1b\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u7ec6\u5316\u7b56\u7565\uff0c\u5229\u7528\u76ee\u6807\u57df\u8bed\u4e49\u77e5\u8bc6\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBotTrans\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u65e0\u6807\u7b7e\u76ee\u6807\u4efb\u52a1\u4e2d\u5229\u7528\u591a\u6e90\u77e5\u8bc6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.14046", "pdf": "https://arxiv.org/pdf/2506.14046", "abs": "https://arxiv.org/abs/2506.14046", "authors": ["David Kogan", "Max Schumacher", "Sam Nguyen", "Masanori Suzuki", "Melissa Smith", "Chloe Sophia Bellows", "Jared Bernstein"], "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Ace-CEFR\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u5bf9\u8bdd\u6587\u672c\u7684\u8bed\u8a00\u96be\u5ea6\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u548c\u7b5b\u9009\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u96be\u5ea6\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4e14\u5ef6\u8fdf\u9002\u5408\u751f\u4ea7\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u77ed\u5bf9\u8bdd\u6587\u672c\u8bed\u8a00\u96be\u5ea6\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u548c\u7b5b\u9009\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Ace-CEFR\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u82f1\u8bed\u5bf9\u8bdd\u6587\u672c\u96be\u5ea6\u7b49\u7ea7\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u548cLLMs\uff0c\u4ee5\u9a8c\u8bc1\u5176\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eAce-CEFR\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6587\u672c\u96be\u5ea6\u8bc4\u4f30\u4e0a\u6bd4\u4eba\u7c7b\u4e13\u5bb6\u66f4\u51c6\u786e\uff0c\u4e14\u5ef6\u8fdf\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4f9b\u7814\u7a76\u548c\u5f00\u53d1\u4f7f\u7528\u3002", "conclusion": "Ace-CEFR\u6570\u636e\u96c6\u4e3a\u5bf9\u8bdd\u6587\u672c\u7684\u81ea\u52a8\u5316\u96be\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8eLLMs\u7684\u8bad\u7ec3\u548c\u7b5b\u9009\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "paper_title_zh": "Ace-CEFR\u2014\u2014\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u5bf9\u8bdd\u6587\u672c\u8bed\u8a00\u96be\u5ea6\u81ea\u52a8\u8bc4\u4f30\u7684\u6570\u636e\u96c6", "abstract_zh": "\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u77ed\u5bf9\u8bdd\u6587\u672c\u8bed\u8a00\u96be\u5ea6\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u548c\u7b5b\u9009\u9700\u6c42\u3002\u6211\u4eec\u63d0\u51fa\u4e86Ace-CEFR\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u82f1\u8bed\u5bf9\u8bdd\u6587\u672c\u96be\u5ea6\u7b49\u7ea7\u3002\u6211\u4eec\u5728Ace-CEFR\u4e0a\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u548cLLMs\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eAce-CEFR\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6587\u672c\u96be\u5ea6\u8bc4\u4f30\u4e0a\u6bd4\u4eba\u7c7b\u4e13\u5bb6\u66f4\u51c6\u786e\uff0c\u4e14\u5ef6\u8fdf\u9002\u5408\u751f\u4ea7\u73af\u5883\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06Ace-CEFR\u6570\u636e\u96c6\u516c\u5f00\u53d1\u5e03\uff0c\u4f9b\u7814\u7a76\u548c\u5f00\u53d1\u4f7f\u7528\u3002"}}
{"id": "2506.13993", "pdf": "https://arxiv.org/pdf/2506.13993", "abs": "https://arxiv.org/abs/2506.13993", "authors": ["Michelangelo Conserva", "Alex Wilson", "Charlotte Stanton", "Vishal Batchu", "Varun Gulshan"], "title": "Mapping Farmed Landscapes from Remote Sensing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\\%) and farmed land (95\\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Farmscapes\uff0c\u9996\u4e2a\u8986\u76d6\u82f1\u683c\u5170\u5927\u90e8\u5206\u5730\u533a\u7684\u9ad8\u5206\u8fa8\u7387\uff0825\u5398\u7c73\uff09\u519c\u6751\u666f\u89c2\u5730\u56fe\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u8bc6\u522b\u5173\u952e\u751f\u6001\u7279\u5f81\uff08\u5982\u6811\u7bf1\u3001\u6797\u5730\u3001\u77f3\u5899\uff09\uff0c\u4e3a\u751f\u6001\u5b66\u5bb6\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u5f00\u653e\u5de5\u5177\u3002", "motivation": "\u5168\u7403\u751f\u7269\u591a\u6837\u6027\u76ee\u6807\u7684\u5b9e\u73b0\u9700\u8981\u8be6\u7ec6\u7684\u519c\u4e1a\u666f\u89c2\u7ba1\u7406\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u751f\u6001\u5730\u56fe\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u751f\u6001\u5730\u56fe\u4ee5\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u89c4\u5212\u548c\u76d1\u6d4b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\uff0c\u57fa\u4e8e942\u5757\u624b\u52a8\u6807\u6ce8\u7684\u822a\u7a7a\u5f71\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u519c\u6751\u666f\u89c2\u5730\u56fe\uff0c\u5305\u62ec\u6811\u7bf1\u3001\u6797\u5730\u548c\u519c\u7530\u7b49\u5173\u952e\u751f\u6001\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728\u6797\u5730\uff0896%\uff09\u548c\u519c\u7530\uff0895%\uff09\u7684\u8bc6\u522b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6811\u7bf1\u5206\u5272\u7684F1\u5206\u6570\u4e3a72%\u3002\u5730\u56fe\u5df2\u53d1\u5e03\u4e8eGoogle Earth Engine\uff0c\u4e3a\u751f\u6001\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u5f00\u653e\u5de5\u5177\u3002", "conclusion": "Farmscapes\u4e3a\u751f\u6001\u6062\u590d\u89c4\u5212\u548c\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5960\u5b9a\u4e86\u666f\u89c2\u8fde\u901a\u6027\u5206\u6790\u7684\u57fa\u7840\uff0c\u652f\u6301\u6b27\u76df\u751f\u7269\u591a\u6837\u6027\u6218\u7565\u7b49\u5021\u8bae\u3002", "paper_title_zh": "\u57fa\u4e8e\u9065\u611f\u6280\u672f\u7684\u519c\u7530\u666f\u89c2\u5236\u56fe", "abstract_zh": "\u519c\u4e1a\u666f\u89c2\u7684\u6709\u6548\u7ba1\u7406\u5bf9\u5b9e\u73b0\u5168\u7403\u751f\u7269\u591a\u6837\u6027\u76ee\u6807\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u8be6\u7ec6\u7684\u5927\u89c4\u6a21\u751f\u6001\u5730\u56fe\u963b\u788d\u4e86\u76f8\u5173\u52aa\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51fa\u4e86Farmscapes\uff0c\u8fd9\u662f\u9996\u4e2a\u8986\u76d6\u82f1\u683c\u5170\u5927\u90e8\u5206\u5730\u533a\u7684\u9ad8\u5206\u8fa8\u7387\uff0825\u5398\u7c73\uff09\u519c\u6751\u666f\u89c2\u5730\u56fe\uff0c\u5305\u542b\u6811\u7bf1\u3001\u6797\u5730\u548c\u77f3\u5899\u7b49\u751f\u6001\u5173\u952e\u8981\u7d20\u3002\u8be5\u5730\u56fe\u901a\u8fc7\u57fa\u4e8e942\u5757\u624b\u52a8\u6807\u6ce8\u822a\u7a7a\u5f71\u50cf\u6570\u636e\u96c6\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\u751f\u6210\u3002\u6a21\u578b\u5728\u8bc6\u522b\u5173\u952e\u6816\u606f\u5730\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6797\u5730\u548c\u519c\u7530\u7684F1\u5206\u6570\u5206\u522b\u8fbe\u523096%\u548c95%\uff0c\u6811\u7bf1\u5206\u5272\u7684F1\u5206\u6570\u4e3a72%\u3002\u6211\u4eec\u5c06\u82f1\u683c\u5170\u5168\u5883\u5730\u56fe\u53d1\u5e03\u4e8eGoogle Earth Engine\uff0c\u4e3a\u751f\u6001\u5b66\u5bb6\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f00\u653e\u5de5\u5177\u3002\u8fd9\u9879\u5de5\u4f5c\u652f\u6301\u57fa\u4e8e\u6570\u636e\u7684\u6816\u606f\u5730\u6062\u590d\u89c4\u5212\uff0c\u76d1\u6d4b\u6b27\u76df\u751f\u7269\u591a\u6837\u6027\u6218\u7565\u7b49\u5021\u8bae\uff0c\u5e76\u4e3a\u666f\u89c2\u8fde\u901a\u6027\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.13799", "pdf": "https://arxiv.org/pdf/2506.13799", "abs": "https://arxiv.org/abs/2506.13799", "authors": ["Soroush Vahidi"], "title": "Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization", "categories": ["cs.AI"], "comment": "This is a preliminary paper", "summary": "We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u53ef\u6269\u5c55\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u6700\u5c0f\u5316\u5927\u89c4\u6a21\u52a0\u6743\u6709\u5411\u56fe\u4e2d\u7684\u53cd\u9988\u5f27\uff0c\u65e8\u5728\u63ed\u793a\u795e\u7ecf\u8fde\u63a5\u7ec4\u4e2d\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u987a\u5411\u7ed3\u6784\u3002\u901a\u8fc7FlyWire\u8fde\u63a5\u7ec4\u6311\u6218\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u7b97\u6cd5\u5728\u6700\u5927\u5316\u524d\u5411\u8fb9\u6743\u91cd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6700\u5c0f\u5316\u53cd\u9988\u5f27\uff0c\u63ed\u793a\u795e\u7ecf\u8fde\u63a5\u7ec4\u4e2d\u7684\u987a\u5411\u7ed3\u6784\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u751f\u7269\u5b66\u529f\u80fd\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u7ed3\u5408\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u589e\u76ca\u611f\u77e5\u7684\u5c40\u90e8\u4f18\u5316\u548c\u57fa\u4e8e\u5f3a\u8fde\u901a\u5206\u91cf\u7684\u5168\u5c40\u7ed3\u6784\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6700\u4f73\u65b9\u6848\u5728\u524d\u5411\u8fb9\u6743\u91cd\u4e0a\u4f18\u4e8e\u4ee5\u5f80\u8868\u73b0\u6700\u4f18\u7684\u65b9\u6cd5\u3002", "conclusion": "\u7b97\u6cd5\u5728Python\u4e2d\u9ad8\u6548\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7Google Colab Pro+\u7684\u4e91\u7aef\u6267\u884c\u9a8c\u8bc1\uff0c\u4e3a\u795e\u7ecf\u8fde\u63a5\u7ec4\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "paper_title_zh": "\u901a\u8fc7\u53cd\u9988\u5f27\u6700\u5c0f\u5316\u5b9e\u73b0\u795e\u7ecf\u8fde\u63a5\u7ec4\u4e2d\u7684\u987a\u5411\u6392\u5e8f", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u5957\u53ef\u6269\u5c55\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u6700\u5c0f\u5316\u5927\u89c4\u6a21\u52a0\u6743\u6709\u5411\u56fe\u4e2d\u7684\u53cd\u9988\u5f27\uff0c\u65e8\u5728\u63ed\u793a\u795e\u7ecf\u8fde\u63a5\u7ec4\u4e2d\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u987a\u5411\u7ed3\u6784\u3002\u5229\u7528FlyWire\u8fde\u63a5\u7ec4\u6311\u6218\u6570\u636e\u96c6\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u6392\u540d\u7b56\u7565\u5728\u6700\u5927\u5316\u524d\u5411\u8fb9\u6743\u91cd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6574\u5408\u4e86\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u589e\u76ca\u611f\u77e5\u7684\u5c40\u90e8\u4f18\u5316\u548c\u57fa\u4e8e\u5f3a\u8fde\u901a\u5206\u91cf\u7684\u5168\u5c40\u7ed3\u6784\u5206\u6790\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6700\u4f73\u65b9\u6848\u5728\u524d\u5411\u8fb9\u6743\u91cd\u4e0a\u4f18\u4e8e\u4ee5\u5f80\u8868\u73b0\u6700\u4f18\u7684\u65b9\u6cd5\u3002\u6240\u6709\u7b97\u6cd5\u5747\u5728Python\u4e2d\u9ad8\u6548\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7Google Colab Pro+\u7684\u4e91\u7aef\u6267\u884c\u9a8c\u8bc1\u3002"}}
{"id": "2506.14064", "pdf": "https://arxiv.org/pdf/2506.14064", "abs": "https://arxiv.org/abs/2506.14064", "authors": ["Iona Carslaw", "Sivan Milton", "Nicolas Navarre", "Ciyang Qing", "Wataru Uegaki"], "title": "Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data", "categories": ["cs.CL"], "comment": "Accepted in the Society for Computation in Linguistics", "summary": "For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u82f1\u8bed\u6587\u672c\u6570\u636e\u7684\u4ece\u53e5\u81ea\u52a8\u63d0\u53d6\u65b9\u6cd5\uff0c\u5229\u7528\u6210\u5206\u5206\u6790\u548c\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u8bed\u6599\u4e2d\u68c0\u6d4b\u548c\u6807\u6ce8\u5d4c\u5165\u5f0f\u4ece\u53e5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u5b66\u7814\u7a76\u591a\u4f9d\u8d56\u4eba\u5de5\u6784\u9020\u7684\u4f8b\u53e5\u5206\u6790\u5d4c\u5165\u5f0f\u4ece\u53e5\uff0c\u7f3a\u4e4f\u4ece\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u8bed\u6599\u4e2d\u83b7\u53d6\u7684\u7edf\u8ba1\u4fe1\u606f\u548c\u771f\u5b9e\u4f8b\u53e5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u63d0\u53d6\u81ea\u7136\u8bed\u8a00\u4e2d\u5d4c\u5165\u5f0f\u4ece\u53e5\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6210\u5206\u5206\u6790\u548c\u4e00\u7ec4\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u4ece\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u4e2d\u68c0\u6d4b\u548c\u6807\u6ce8\u81ea\u7136\u51fa\u73b0\u7684\u82f1\u8bed\u5d4c\u5165\u5f0f\u4ece\u53e5\u3002\u5de5\u5177\u5728\u624b\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6GECS\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u63d0\u53d6\u5de5\u5177\uff0c\u5e76\u4ece\u5f00\u6e90\u8bed\u6599\u5e93Dolma\u4e2d\u63d0\u53d6\u4e86\u5927\u91cf\u81ea\u7136\u51fa\u73b0\u7684\u82f1\u8bed\u5d4c\u5165\u5f0f\u4ece\u53e5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4ece\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u4e2d\u63d0\u53d6\u5d4c\u5165\u5f0f\u4ece\u53e5\uff0c\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u8d44\u6e90\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u89c4\u6a21\u82f1\u8bed\u6587\u672c\u6570\u636e\u7684\u4ece\u53e5\u81ea\u52a8\u63d0\u53d6", "abstract_zh": "\u5bf9\u8bed\u8a00\u5b66\u5bb6\u800c\u8a00\uff0c\u5d4c\u5165\u5f0f\u4ece\u53e5\u56e0\u5176\u590d\u6742\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\u5206\u5e03\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5f53\u524d\u7814\u7a76\u591a\u4f9d\u8d56\u4eba\u5de5\u6784\u9020\u7684\u8bed\u8a00\u4f8b\u53e5\u6765\u7814\u7a76\u8fd9\u4e9b\u7ed3\u6784\uff0c\u672a\u80fd\u4ece\u5927\u89c4\u6a21\u8bed\u8a00\u8bed\u6599\u5e93\u4e2d\u83b7\u53d6\u7edf\u8ba1\u4fe1\u606f\u548c\u81ea\u7136\u4f8b\u53e5\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5229\u7528\u6210\u5206\u5206\u6790\u548c\u4e00\u7ec4\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u4ece\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u4e2d\u68c0\u6d4b\u548c\u6807\u6ce8\u81ea\u7136\u51fa\u73b0\u7684\u82f1\u8bed\u5d4c\u5165\u5f0f\u4ece\u53e5\u3002\u6211\u4eec\u7684\u5de5\u5177\u5728\u624b\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\u201c\u9ec4\u91d1\u5d4c\u5165\u5f0f\u4ece\u53e5\u96c6\u201d\uff08GECS\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4ece\u5f00\u6e90\u8bed\u6599\u5e93Dolma\u4e2d\u63d0\u53d6\u7684\u5927\u91cf\u81ea\u7136\u51fa\u73b0\u7684\u82f1\u8bed\u5d4c\u5165\u5f0f\u4ece\u53e5\u6570\u636e\u96c6\u3002"}}
{"id": "2506.14008", "pdf": "https://arxiv.org/pdf/2506.14008", "abs": "https://arxiv.org/abs/2506.14008", "authors": ["Daniel Montoya", "Aymen Bouguerra", "Alexandra Gomez-Villa", "Fabio Arnez"], "title": "FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $ \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection", "categories": ["cs.CV"], "comment": "Preprint", "summary": "State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5f53\u524dOOD-OD\u8bc4\u4f30\u534f\u8bae\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u65b0\u8bc4\u4f30\u5212\u5206\uff08near\u3001far\u3001farther\uff09\uff0c\u5e76\u5f15\u5165\u5f00\u653e\u96c6\u6307\u6807\uff0c\u5c55\u793a\u4e0d\u540c\u8ddd\u79bbOOD\u5bf9\u8c61\u7684\u68c0\u6d4b\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5047\u8bbe\u6d4b\u8bd5\u7c7b\u522b\u4e0e\u8bad\u7ec3\u4e00\u81f4\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u68c0\u6d4b\u672a\u77e5\u5bf9\u8c61\u3002\u5f53\u524dOOD-OD\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u7f3a\u9677\uff0c\u53ef\u80fd\u63a9\u76d6\u5173\u952e\u95ee\u9898\uff08\u5982\u5ffd\u7565\u672a\u77e5\u5bf9\u8c61\uff09\uff0c\u5bfc\u81f4\u90e8\u7f72\u65f6\u8fc7\u5ea6\u81ea\u4fe1\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u624b\u52a8\u6784\u5efa\u65b0\u8bc4\u4f30\u5212\u5206\uff08near\u3001far\u3001farther\uff09\uff0c\u5e76\u5f15\u5165\u5f00\u653e\u96c6\u6307\u6807\uff0c\u5206\u6790OOD\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u8bed\u4e49\u548c\u89c6\u89c9\u63a5\u8fd1\u7684OOD\u5bf9\u8c61\u66f4\u6613\u5b9a\u4f4d\uff0c\u4f46\u4e5f\u66f4\u6613\u4e0eID\u5bf9\u8c61\u6df7\u6dc6\uff1bfar\u548cfarther\u5bf9\u8c61\u5b9a\u4f4d\u66f4\u96be\uff0c\u4f46\u4e0d\u6613\u88ab\u8bef\u8ba4\u4e3aID\u5bf9\u8c61\u3002", "conclusion": "\u65b0\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\u63ed\u793a\u4e86OOD\u68c0\u6d4b\u7684\u590d\u6742\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "paper_title_zh": "FindMeIfYouCan\uff1a\u5c06\u5f00\u653e\u96c6\u6307\u6807\u5f15\u5165near\u3001far\u548cfarther\u5206\u5e03\u5916\u76ee\u6807\u68c0\u6d4b", "abstract_zh": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\uff08OD\uff09\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\uff0c\u5373\u6d4b\u8bd5\u7c7b\u522b\u4e0e\u8bad\u7ec3\u4e00\u81f4\u3002\u7136\u800c\uff0c\u68c0\u6d4b\u548c\u5b9a\u4f4d\u672a\u77e5\u5bf9\u8c61\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548c\u533b\u5b66\u5f71\u50cf\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u5e74\u6765\uff0c\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u6210\u4e3aOD\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u901a\u5e38\u4e0e\u672a\u77e5\u5bf9\u8c61\u76f8\u5173\u7684\u9519\u8bef\u9884\u6d4b\u3002\u672c\u6587\u6307\u51fa\uff0c\u5f53\u524dOOD-OD\u8bc4\u4f30\u534f\u8bae\u8fdd\u53cd\u4e86\u4e0e\u5206\u5e03\u5185\uff08ID\uff09\u6570\u636e\u96c6\u975e\u91cd\u53e0\u5bf9\u8c61\u7684\u5047\u8bbe\uff0c\u5e76\u63a9\u76d6\u4e86\u5ffd\u7565\u672a\u77e5\u5bf9\u8c61\u7b49\u5173\u952e\u60c5\u51b5\uff0c\u53ef\u80fd\u5bfc\u81f4\u5728\u9047\u5230\u771f\u6b63\u65b0\u5bf9\u8c61\u65f6\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u624b\u52a8\u6784\u5efa\u5e76\u4e30\u5bcc\u4e86\u73b0\u6709\u57fa\u51c6\uff0c\u521b\u5efa\u4e86near\u3001far\u548cfarther\u4e09\u7c7b\u65b0\u8bc4\u4f30\u5212\u5206\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u5f00\u653e\u96c6\u793e\u533a\u7684\u6210\u719f\u6307\u6807\uff0c\u66f4\u6df1\u5165\u5730\u63ed\u793a\u65b9\u6cd5\u68c0\u6d4b\u672a\u77e5\u5bf9\u8c61\u7684\u6709\u6548\u6027\u3001\u5ffd\u7565\u60c5\u51b5\u4ee5\u53ca\u8bef\u5c06OOD\u5bf9\u8c61\u5206\u7c7b\u4e3aID\u7684\u60c5\u51b5\u3002\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8bed\u4e49\u548c\u89c6\u89c9\u63a5\u8fd1\u7684OOD\u5bf9\u8c61\u66f4\u6613\u5b9a\u4f4d\uff0c\u4f46\u4e5f\u66f4\u6613\u4e0eID\u5bf9\u8c61\u6df7\u6dc6\uff1bfar\u548cfarther\u5bf9\u8c61\u5b9a\u4f4d\u66f4\u96be\uff0c\u4f46\u4e0d\u6613\u88ab\u8bef\u8ba4\u4e3aID\u5bf9\u8c61\u3002"}}
{"id": "2506.13803", "pdf": "https://arxiv.org/pdf/2506.13803", "abs": "https://arxiv.org/abs/2506.13803", "authors": ["Richard D. Lange", "Konrad P. Kording"], "title": "Causality in the human niche: lessons for machine learning", "categories": ["cs.AI", "cs.LG"], "comment": "23 pages, 2 figures", "summary": "Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u5173\u7cfb\uff0c\u6307\u51fa\u5f53\u524d\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6846\u67b6\u672a\u80fd\u5b8c\u5168\u6355\u6349\u4eba\u7c7b\u56e0\u679c\u601d\u7ef4\u7684\u9002\u5e94\u6027\u7279\u70b9\uff0c\u5e76\u547c\u5401\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u878d\u5165\u66f4\u591a\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u56e0\u679c\u601d\u7ef4\u9ad8\u6548\u5b66\u4e60\u548c\u6cdb\u5316\uff0c\u800c\u5f53\u524d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u8f83\u5f31\u3002SCM\u6846\u67b6\u867d\u5728\u5f62\u5f0f\u5316\u56e0\u679c\u5173\u7cfb\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u672a\u80fd\u5b8c\u5168\u9002\u5e94\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u793e\u4f1a\u6027\u3001\u81ea\u4e3b\u6027\u548c\u76ee\u6807\u9a71\u52a8\u7684\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u7279\u70b9\u53ca\u5176\u5728\u4eba\u7c7b\u751f\u6d3b\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u63a2\u8ba8\u4e86SCM\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u673a\u5668\u5b66\u4e60\u5e94\u501f\u9274\u4eba\u7c7b\u56e0\u679c\u601d\u7ef4\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u5728\u6cdb\u5316\u548c\u7c7b\u6bd4\u63a8\u7406\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u800cSCM\u6846\u67b6\u96be\u4ee5\u8868\u8fbe\u8fd9\u4e9b\u7279\u70b9\u3002\u672a\u6765\u7814\u7a76\u5e94\u66f4\u5173\u6ce8\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u9002\u5e94\u6027\u673a\u5236\u3002", "conclusion": "\u672a\u6765\u673a\u5668\u5b66\u4e60\u4e0e\u56e0\u679c\u7814\u7a76\u7684\u4ea4\u53c9\u9886\u57df\u5e94\u66f4\u6ce8\u91cd\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u9002\u5e94\u6027\u7279\u70b9\uff0c\u4ee5\u5f00\u53d1\u66f4\u5177\u80fd\u529b\u3001\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u7cfb\u7edf\u3002", "paper_title_zh": "\u4eba\u7c7b\u751f\u6001\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff1a\u5bf9\u673a\u5668\u5b66\u4e60\u7684\u542f\u793a", "abstract_zh": "\u4eba\u7c7b\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u89e3\u91ca\u4e16\u754c\uff0c\u5e76\u4ee5\u56e0\u679c\u672f\u8bed\u4ea4\u6d41\u5bf9\u4e16\u754c\u7684\u7406\u89e3\u3002\u8fd9\u79cd\u56e0\u679c\u8ba4\u77e5\u88ab\u8ba4\u4e3a\u662f\u4eba\u7c7b\u5728\u65b0\u9886\u57df\u9ad8\u6548\u5b66\u4e60\u548c\u6cdb\u5316\u7684\u57fa\u7840\uff0c\u800c\u5f53\u524d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u8f83\u5f31\u3002\u5c06\u4eba\u7c7b\u56e0\u679c\u80fd\u529b\u878d\u5165\u673a\u5668\u5b66\u4e60\u53ef\u80fd\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u6709\u6548\u548c\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u3002\u673a\u5668\u5b66\u4e60\u793e\u533a\u5df2\u5f15\u5165\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6846\u67b6\u7684\u5f62\u5f0f\u5316\u56e0\u679c\u601d\u60f3\uff0c\u8be5\u6846\u67b6\u4e3a\u8bb8\u591a\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6570\u5b66\u8bed\u8a00\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0cSCM\u6846\u67b6\u672a\u80fd\u6355\u6349\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u67d0\u4e9b\u663e\u8457\u7279\u70b9\uff0c\u4e5f\u672a\u5728\u4eba\u7c7b\u64c5\u957f\u7684\u5173\u952e\u9886\u57df\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u8fdb\u6b65\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\"\u4eba\u7c7b\u751f\u6001\"\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u2014\u2014\u5373\u793e\u4f1a\u6027\u3001\u81ea\u4e3b\u6027\u548c\u76ee\u6807\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u4eba\u7c7b\u751f\u6d3b\u73af\u5883\u4e2d\u7684\u611f\u77e5\u4e0e\u884c\u4e3a\u2014\u2014\u4e0eSCM\u6240\u63cf\u8ff0\u7684\u56e0\u679c\u5173\u7cfb\u622a\u7136\u4e0d\u540c\u3002\u4f8b\u5982\uff0c\u65e5\u5e38\u7269\u4f53\u5177\u6709\u76f8\u4f3c\u7684\u7c7b\u578b\u548c\u56e0\u679c\u5c5e\u6027\uff0c\u4eba\u7c7b\u80fd\u901a\u8fc7\u56e0\u679c\u7c7b\u6bd4\u5c06\u77e5\u8bc6\u4ece\u4e00\u79cd\u7269\u4f53\uff08\u5982\u676f\u5b50\uff09\u6cdb\u5316\u5230\u53e6\u4e00\u79cd\u76f8\u5173\u7269\u4f53\uff08\u5982\u7897\uff09\uff0c\u4f46\u8fd9\u79cd\u7c7b\u6bd4\u5728SCM\u4e2d\u96be\u4ee5\u8868\u8fbe\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u56e0\u679c\u80fd\u529b\u5982\u4f55\u9002\u5e94\u5e76\u53d7\u4eba\u7c7b\u751f\u6001\u9a71\u52a8\u3002\u901a\u8fc7\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u56e0\u679c\u8ba4\u77e5\u7684\u7279\u70b9\u53ca\u5176\u5728\u4eba\u7c7b\u751f\u6001\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u6211\u4eec\u5e0c\u671b\u672a\u6765\u673a\u5668\u5b66\u4e60\u4e0e\u56e0\u679c\u7814\u7a76\u7684\u4ea4\u53c9\u5de5\u4f5c\u80fd\u5229\u7528\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5f00\u53d1\u51fa\u66f4\u5177\u80fd\u529b\u3001\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u7cfb\u7edf\u3002"}}
{"id": "2506.14101", "pdf": "https://arxiv.org/pdf/2506.14101", "abs": "https://arxiv.org/abs/2506.14101", "authors": ["Paul Landes", "Sitara Rao", "Aaron Jeremy Chaise", "Barbara Di Eugenio"], "title": "Abstract Meaning Representation for Hospital Discharge Summarization", "categories": ["cs.CL"], "comment": null, "summary": "The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u56fe\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u53ef\u4fe1\u8d56\u7684\u533b\u9662\u51fa\u9662\u6458\u8981\uff0c\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u9886\u57df\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b9e\u9645\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u5728\u4e34\u5e8a\u9886\u57df\u5177\u6709\u4e25\u91cd\u540e\u679c\uff0c\u5c24\u5176\u662f\u81ea\u52a8\u751f\u6210\u51fa\u9662\u6458\u8981\u65f6\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u8a00\u56fe\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u786e\u4fdd\u5185\u5bb9\u7684\u6765\u6e90\u53ef\u4fe1\uff0c\u51cf\u8f7b\u533b\u751f\u6587\u6863\u8d1f\u62c5\uff0c\u540c\u65f6\u63d0\u9ad8\u81ea\u52a8\u6458\u8981\u7684\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u56fe\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u516c\u5f00\u7684MIMIC-III\u6570\u636e\u96c6\u548c\u533f\u540d\u533b\u9662\u7684\u4e34\u5e8a\u7b14\u8bb0\uff0c\u751f\u6210\u51fa\u9662\u6458\u8981\uff0c\u5e76\u63d0\u4f9b\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MIMIC-III\u6570\u636e\u96c6\u548c\u5b9e\u9645\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u53ef\u9760\u6027\uff0c\u751f\u6210\u7684\u51fa\u9662\u6458\u8981\u5185\u5bb9\u6765\u6e90\u6e05\u6670\u4e14\u53ef\u4fe1\u3002", "conclusion": "\u7ed3\u5408\u8bed\u8a00\u56fe\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u751f\u6210\u51fa\u9662\u6458\u8981\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u7684\u81ea\u52a8\u6458\u8981\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u4e8e\u62bd\u8c61\u610f\u4e49\u8868\u793a\u7684\u533b\u9662\u51fa\u9662\u6458\u8981\u751f\u6210", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u5bf9\u4e34\u5e8a\u9886\u57df\u5177\u6709\u4e25\u91cd\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u81ea\u52a8\u751f\u6210\u51fa\u9662\u6458\u8981\uff08\u4e00\u79cd\u603b\u7ed3\u4f4f\u9662\u5c31\u8bca\u7684\u957f\u7bc7\u533b\u7597\u6587\u6863\uff09\u65f6\u3002\u81ea\u52a8\u751f\u6210\u8fd9\u4e9b\u6458\u8981\u53ef\u4ee5\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\uff0c\u4f7f\u5176\u4e13\u6ce8\u4e8e\u60a3\u8005\u62a4\u7406\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u8a00\u56fe\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u89e3\u51b3\u81ea\u52a8\u6458\u8981\u4e2d\u5185\u5bb9\u7684\u6765\u6e90\u548c\u53ef\u4fe1\u5ea6\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u516c\u5f00\u7684MIMIC-III\u6570\u636e\u96c6\u548c\u533f\u540d\u533b\u9662\u533b\u751f\u64b0\u5199\u7684\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u53ef\u9760\u6027\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u65b9\u6cd5\u3001\u751f\u6210\u7684\u51fa\u9662\u6458\u8981\u793a\u4f8b\u3001\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2506.14015", "pdf": "https://arxiv.org/pdf/2506.14015", "abs": "https://arxiv.org/abs/2506.14015", "authors": ["Nick Yiwen Huang", "Akin Caliskan", "Berkay Kicanaoglu", "James Tompkin", "Hyeongwoo Kim"], "title": "Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation", "categories": ["cs.CV"], "comment": null, "summary": "We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89e3\u80263D\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u63a7\u8096\u50cf\u751f\u6210\u3002\u901a\u8fc7\u89c4\u8303\u5316\u548c\u96c5\u53ef\u6bd4\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8096\u50cf\u5916\u89c2\u548c3D\u51e0\u4f55\u7684\u81ea\u7531\u63a7\u5236\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\u6216\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u751f\u62103D\u8096\u50cf\u65f6\u5b58\u5728\u4fe1\u606f\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\u548c\u591a\u6837\u6027\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u80263D\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u8096\u50cf\u5916\u89c2\u548c\u51e0\u4f55\u7684\u81ea\u7531\u63a7\u5236\uff0c\u540c\u65f6\u907f\u514d\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u6216\u8bad\u7ec3\u8d44\u6e90\u3002", "method": "\u9996\u5148\uff0c\u901a\u8fc7\u89c4\u8303\u5316\u5c063D\u5f62\u53d8\u795e\u7ecf\u4e09\u89d2\u5e73\u9762\u8868\u793a\u89e3\u8026\u52302D\u53c2\u8003\u6846\u67b6\u4e2d\u3002\u5176\u6b21\uff0c\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u96c5\u53ef\u6bd4\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u672c\u6587\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u6587\u672c\u548c3D\u63a7\u5236\u529f\u80fd\u7684\u8096\u50cf\uff0c\u4e14\u5728\u6539\u53d8\u4efb\u4e00\u63a7\u5236\u53c2\u6570\u65f6\u4fdd\u6301\u8096\u50cf\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u4e3a\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u6216\u8bad\u7ec3\u8d44\u6e90\u7684\u53ef\u63a73D\u8096\u50cf\u751f\u6210\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89e3\u80263D\u4fe1\u606f\u4ee5\u5b9e\u73b0\u53ef\u63a7\u8096\u50cf\u751f\u6210", "abstract_zh": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89e3\u80263D\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u4ee5\u751f\u62103D\u8096\u50cf\u4e3a\u4f8b\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u81ea\u7531\u6587\u672c\u63a7\u5236\u8096\u50cf\u7684\u5916\u89c2\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u3001\u53d1\u578b\u548c\u773c\u955c\uff09\u4ee5\u53ca3D\u51e0\u4f55\u63a7\u5236\uff08\u5982\u9762\u90e8\u8868\u60c5\u548c\u76f8\u673a\u59ff\u6001\uff09\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u5047\u8bbe\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff1b\u5982CLIP\uff09\u4ece\u8f83\u5c0f\u76842D\u6570\u636e\u96c6\u4e2d\u751f\u6210\u5185\u5bb9\uff0c\u65e0\u9700\u989d\u5916\u914d\u5bf9\u6807\u7b7e\uff0c\u5e76\u57fa\u4e8e\u9884\u5b9a\u4e49\u76843D\u53ef\u53d8\u5f62\u6a21\u578b\uff08FLAME\uff09\u3002\u9996\u5148\uff0c\u6211\u4eec\u901a\u8fc7\u89c4\u8303\u5316\u5c063D\u5f62\u53d8\u795e\u7ecf\u4e09\u89d2\u5e73\u9762\u8868\u793a\u89e3\u8026\u52302D\u53c2\u8003\u6846\u67b6\u4e2d\u3002\u7136\u800c\uff0c\u53e6\u4e00\u79cd\u7ea0\u7f20\u5f62\u5f0f\u6e90\u4e8eLVLM\u5d4c\u5165\u7a7a\u95f4\u4e2d\u63cf\u8ff0\u65e0\u5173\u7279\u5f81\u7684\u663e\u8457\u566a\u58f0\u3002\u8fd9\u4f1a\u635f\u5bb3\u8f93\u51fa\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u4f46\u6211\u4eec\u901a\u8fc7\u4e00\u79cd\u9ad8\u6548\u7684\u968f\u673a\u8fd1\u4f3c\u5668\u8ba1\u7b97\u7684\u96c5\u53ef\u6bd4\u6b63\u5219\u5316\u514b\u670d\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u7684\u8096\u50cf\u5177\u6709\u989d\u5916\u7684\u6587\u672c\u548c3D\u63a7\u5236\u529f\u80fd\uff0c\u4e14\u5728\u6539\u53d8\u4efb\u4e00\u63a7\u5236\u53c2\u6570\u65f6\u4fdd\u6301\u8096\u50cf\u7684\u4e00\u81f4\u6027\u3002\u603b\u4f53\u800c\u8a00\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u521b\u4f5c\u8005\u80fd\u591f\u57fa\u4e8e\u81ea\u5df1\u76842D\u9762\u90e8\u6570\u636e\u63a7\u52363D\u751f\u6210\u5668\uff0c\u800c\u65e0\u9700\u6807\u6ce8\u5927\u89c4\u6a21\u6570\u636e\u6216\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u7684\u8d44\u6e90\u3002"}}
{"id": "2506.13810", "pdf": "https://arxiv.org/pdf/2506.13810", "abs": "https://arxiv.org/abs/2506.13810", "authors": ["Olivier Saidi"], "title": "Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study", "categories": ["cs.AI"], "comment": null, "summary": "NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5f0f\u611f\u77e5\u7684\u590d\u6742\u6027\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u89c4\u5f8b\uff08\u5982\u805a\u7c7b\u3001\u5bf9\u79f0\u6027\uff09\u964d\u4f4eNP\u96be\u4f18\u5316\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728TSP\u7b49\u5b9e\u9645\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe79%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1NP\u96be\u4f18\u5316\u95ee\u9898\uff08\u5982\u65c5\u884c\u5546\u95ee\u9898\uff09\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u96be\u4ee5\u9ad8\u6548\u89e3\u51b3\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u5b58\u5728\u53ef\u88ab\u5229\u7528\u7684\u7ed3\u6784\u89c4\u5f8b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cf\u5316\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5f0f\u611f\u77e5\u590d\u6742\u6027\u6846\u67b6\uff0c\u5305\u62ec\u4e25\u683c\u7684\u6570\u5b66\u5b9a\u4e49\u3001\u5b9a\u7406\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6c42\u89e3\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165\u4e86\u6a21\u5f0f\u5229\u7528\u6548\u7387\uff08PUE\uff09\u7b49\u6307\u6807\u3002", "result": "\u5728TSP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0822\u81f32392\u4e2a\u57ce\u5e02\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe79%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u6a21\u5f0f\u9a71\u52a8\u7684\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u5b9e\u7528\u7684\u89c6\u89d2\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7684NP\u96be\u7406\u8bba\u3002", "paper_title_zh": "\u6865\u63a5\u6a21\u5f0f\u611f\u77e5\u590d\u6742\u6027\u4e0eNP\u96be\u4f18\u5316\uff1a\u7edf\u4e00\u6846\u67b6\u4e0e\u5b9e\u8bc1\u7814\u7a76", "abstract_zh": "NP\u96be\u4f18\u5316\u95ee\u9898\uff08\u5982\u65c5\u884c\u5546\u95ee\u9898\uff09\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u96be\u4ee5\u9ad8\u6548\u89e3\u51b3\uff0c\u4f46\u5b9e\u9645\u5b9e\u4f8b\u5e38\u8868\u73b0\u51fa\u53ef\u5229\u7528\u7684\u6a21\u5f0f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u5f0f\u611f\u77e5\u590d\u6742\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u5e76\u5229\u7528\u7ed3\u6784\u89c4\u5f8b\uff08\u5982\u805a\u7c7b\u3001\u5bf9\u79f0\u6027\uff09\u6765\u964d\u4f4e\u8de8\u9886\u57df\u7684\u6709\u6548\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5305\u62ec\u91d1\u878d\u9884\u6d4b\u548cLLM\u4f18\u5316\u3002\u901a\u8fc7\u4e25\u683c\u7684\u5b9a\u4e49\u3001\u5b9a\u7406\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6c42\u89e3\u6d41\u7a0b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6a21\u5f0f\u5229\u7528\u6548\u7387\uff08PUE\uff09\u7b49\u6307\u6807\uff0c\u5e76\u5728TSP\u57fa\u51c6\u6d4b\u8bd5\uff0822\u81f32392\u4e2a\u57ce\u5e02\uff09\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe79%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u5347\u3002\u4e0e\u7406\u8bba\u4e0a\u7684NP\u96be\u6027\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u6a21\u5f0f\u9a71\u52a8\u7684\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u5b9e\u7528\u7684\u89c6\u89d2\u3002"}}
{"id": "2506.14111", "pdf": "https://arxiv.org/pdf/2506.14111", "abs": "https://arxiv.org/abs/2506.14111", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "title": "Essential-Web v1.0: 24T tokens of organized web data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0", "AI": {"tldr": "Essential-Web v1.0\u662f\u4e00\u4e2a\u5305\u542b24\u4e07\u4ebf\u6807\u8bb0\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u901a\u8fc712\u7c7b\u5206\u7c7b\u6cd5\u7ec4\u7ec7\uff0c\u6db5\u76d6\u4e3b\u9898\u3001\u683c\u5f0f\u3001\u5185\u5bb9\u590d\u6742\u5ea6\u548c\u8d28\u91cf\u3002\u4f7f\u7528SQL\u5f0f\u8fc7\u6ee4\u5668\u53ef\u9ad8\u6548\u63d0\u53d6\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001STEM\u548c\u533b\u5b66\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e14\u7ec4\u7ec7\u826f\u597d\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u666e\u53ca\u3002Essential-Web v1.0\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u6807\u6ce8\u4e30\u5bcc\u3001\u6613\u4e8e\u8bbf\u95ee\u7684\u5e9e\u5927\u6570\u636e\u96c6\u3002", "method": "\u6570\u636e\u96c6\u901a\u8fc7EAI-Distill-0.5b\u6a21\u578b\u6807\u6ce8\uff0c\u8be5\u6a21\u578b\u5728\u6807\u6ce8\u4e00\u81f4\u6027\u4e0a\u4e0eQwen2.5-32B-Instruct\u63a5\u8fd1\uff08\u8bef\u5dee3%\u5185\uff09\u3002\u4f7f\u7528SQL\u5f0f\u8fc7\u6ee4\u5668\u63d0\u53d6\u7279\u5b9a\u9886\u57df\u6570\u636e\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001STEM\u548c\u533b\u5b66\u9886\u57df\uff0cEssential-Web v1.0\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u76f8\u5bf9SOTA\u63d0\u5347-8.0%\u3001+14.3%\u3001+24.5%\u548c+8.6%\u3002", "conclusion": "Essential-Web v1.0\u4e3a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u6613\u8bbf\u95ee\u7684\u6570\u636e\u96c6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u83b7\u53d6\u6210\u672c\u3002", "paper_title_zh": "Essential-Web v1.0\uff1a24\u4e07\u4ebf\u6807\u8bb0\u7684\u7ec4\u7ec7\u5316\u7f51\u7edc\u6570\u636e", "abstract_zh": "\u6570\u636e\u5728\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u6280\u80fd\u548c\u77e5\u8bc6\u4e2d\u626e\u6f14\u7740\u6700\u91cd\u8981\u7684\u89d2\u8272\u3002\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e14\u7ec4\u7ec7\u826f\u597d\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5bfc\u81f4\u4e86\u6602\u8d35\u4e14\u96be\u4ee5\u666e\u53ca\u7684\u6570\u636e\u6d41\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86Essential-Web v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b24\u4e07\u4ebf\u6807\u8bb0\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6bcf\u7bc7\u6587\u6863\u5747\u6807\u6ce8\u4e86\u6db5\u76d6\u4e3b\u9898\u3001\u683c\u5f0f\u3001\u5185\u5bb9\u590d\u6742\u5ea6\u548c\u8d28\u91cf\u768412\u7c7b\u5206\u7c7b\u6cd5\u3002\u5206\u7c7b\u6807\u7b7e\u7531EAI-Distill-0.5b\u751f\u6210\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u76845\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u5176\u6807\u6ce8\u4e00\u81f4\u6027\u5728Qwen2.5-32B-Instruct\u76843%\u8bef\u5dee\u8303\u56f4\u5185\u3002\u4ec5\u9700\u4f7f\u7528SQL\u5f0f\u8fc7\u6ee4\u5668\uff0c\u6211\u4eec\u5c31\u80fd\u5728\u6570\u5b66\uff08\u76f8\u5bf9SOTA\u63d0\u5347-8.0%\uff09\u3001\u7f51\u7edc\u4ee3\u7801\uff08+14.3%\uff09\u3001STEM\uff08+24.5%\uff09\u548c\u533b\u5b66\uff08+8.6%\uff09\u9886\u57df\u83b7\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u7f51\u7edc\u7cbe\u9009\u6570\u636e\u96c6\u3002Essential-Web v1.0\u5df2\u5728HuggingFace\u4e0a\u53d1\u5e03\uff1ahttps://huggingface.co/datasets/EssentialAI/essential-web-v1.0"}}
{"id": "2506.14035", "pdf": "https://arxiv.org/pdf/2506.14035", "abs": "https://arxiv.org/abs/2506.14035", "authors": ["Chelsi Jain", "Yiran Wu", "Yifan Zeng", "Jiale Liu", "S hengyu Dai", "Zhenwen Shao", "Qingyun Wu", "Huazheng Wang"], "title": "SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.", "AI": {"tldr": "SimpleDoc\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08DocVQA\uff09\u3002\u5b83\u901a\u8fc7\u53cc\u7ebf\u7d22\u68c0\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc1\u636e\u9875\u9762\u7684\u6536\u96c6\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08DocVQA\uff09\u662f\u4e00\u9879\u5b9e\u7528\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u57fa\u4e8e\u591a\u9875\u6587\u6863\u548c\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982\u56fe\u50cf\u548c\u8868\u683c\uff09\u56de\u7b54\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u91c7\u7528\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6d41\u7a0b\uff0c\u4f46\u5728\u9875\u9762\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002SimpleDoc\u65e8\u5728\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "method": "SimpleDoc\u91c7\u7528\u53cc\u7ebf\u7d22\u68c0\u7d22\u673a\u5236\uff1a\u9996\u5148\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\u5019\u9009\u9875\u9762\uff0c\u7136\u540e\u57fa\u4e8e\u9875\u9762\u6458\u8981\u5bf9\u8fd9\u4e9b\u5019\u9009\u8fdb\u884c\u8fc7\u6ee4\u548c\u91cd\u65b0\u6392\u5e8f\u3002\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u4ee3\u7406\u4f1a\u591a\u6b21\u8c03\u7528\u8be5\u68c0\u7d22\u5668\uff0c\u9010\u6b65\u5c06\u65b0\u9875\u9762\u7eb3\u5165\u5de5\u4f5c\u5185\u5b58\uff0c\u76f4\u5230\u95ee\u9898\u88ab\u81ea\u4fe1\u5730\u56de\u7b54\u3002", "result": "SimpleDoc\u57284\u4e2aDocVQA\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf3.2%\uff0c\u540c\u65f6\u68c0\u7d22\u7684\u9875\u9762\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "SimpleDoc\u901a\u8fc7\u53cc\u7ebf\u7d22\u68c0\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SimpleDoc\uff1a\u57fa\u4e8e\u53cc\u7ebf\u7d22\u9875\u9762\u68c0\u7d22\u4e0e\u8fed\u4ee3\u4f18\u5316\u7684\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3", "abstract_zh": "\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08DocVQA\uff09\u662f\u4e00\u9879\u5b9e\u7528\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u57fa\u4e8e\u591a\u9875\u6587\u6863\u548c\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982\u56fe\u50cf\u548c\u8868\u683c\uff09\u56de\u7b54\u95ee\u9898\u3002\u4e3a\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5d4c\u5165\u6a21\u578b\u6765\u5d4c\u5165\u548c\u68c0\u7d22\u76f8\u5173\u9875\u9762\u4f5c\u4e3a\u56fe\u50cf\uff0c\u518d\u901a\u8fc7\u652f\u6301\u56fe\u50cf\u8f93\u5165\u7684VLM\u751f\u6210\u7b54\u6848\u3002\u672c\u6587\u63d0\u51faSimpleDoc\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u53cc\u7ebf\u7d22\u68c0\u7d22\u673a\u5236\u63d0\u5347\u8bc1\u636e\u9875\u9762\u6536\u96c6\u6548\u7387\uff1a\u9996\u5148\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\u5019\u9009\u9875\u9762\uff0c\u7136\u540e\u57fa\u4e8e\u9875\u9762\u6458\u8981\u5bf9\u8fd9\u4e9b\u5019\u9009\u8fdb\u884c\u8fc7\u6ee4\u548c\u91cd\u65b0\u6392\u5e8f\u3002\u4e00\u4e2a\u57fa\u4e8eVLM\u7684\u63a8\u7406\u4ee3\u7406\u4f1a\u591a\u6b21\u8c03\u7528\u8be5\u68c0\u7d22\u5668\uff0c\u9010\u6b65\u5c06\u65b0\u9875\u9762\u7eb3\u5165\u5de5\u4f5c\u5185\u5b58\uff0c\u76f4\u5230\u95ee\u9898\u88ab\u81ea\u4fe1\u5730\u56de\u7b54\u3002SimpleDoc\u57284\u4e2aDocVQA\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf3.2%\uff0c\u540c\u65f6\u68c0\u7d22\u7684\u9875\u9762\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/ag2ai/SimpleDoc\u3002"}}
{"id": "2506.13825", "pdf": "https://arxiv.org/pdf/2506.13825", "abs": "https://arxiv.org/abs/2506.13825", "authors": ["Gnankan Landry Regis N'guessan", "Issa Karambal"], "title": "The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness", "categories": ["cs.AI"], "comment": null, "summary": "Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $\u03bc$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$\u03a6$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $\u03a6$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$\u03a6$ signal. By shrinking \"consciousness-like\" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u53cd\u5c04\u6027\u6574\u5408\u4fe1\u606f\u5355\u5143\u201d\uff08RIIU\uff09\u7684\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u4f5c\u4e3a\u4eba\u5de5\u610f\u8bc6\u7814\u7a76\u4e2d\u7684\u57fa\u7840\u7ec4\u4ef6\u3002RIIU\u901a\u8fc7\u5f15\u5165\u5143\u72b6\u6001\u548c\u5e7f\u64ad\u7f13\u51b2\u533a\uff0c\u5b9e\u73b0\u4e86\u5c40\u90e8\u4fe1\u606f\u6574\u5408\u7684\u6700\u5927\u5316\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edfGRU\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u610f\u8bc6\u7814\u7a76\u7f3a\u4e4f\u7c7b\u4f3c\u611f\u77e5\u673a\u7684\u57fa\u7840\u6a21\u5757\uff0c\u65e0\u6cd5\u8fdb\u884c\u590d\u5236\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6539\u8fdb\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u5355\u5143\uff0c\u5c06\u54f2\u5b66\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5b9e\u8bc1\u7684\u6570\u5b66\u95ee\u9898\u3002", "method": "RIIU\u662f\u4e00\u79cd\u5faa\u73af\u5355\u5143\uff0c\u901a\u8fc7\u6269\u5c55\u9690\u85cf\u72b6\u6001\uff0c\u5f15\u5165\u5143\u72b6\u6001\uff08\u8bb0\u5f55\u5355\u5143\u7684\u56e0\u679c\u8db3\u8ff9\uff09\u548c\u5e7f\u64ad\u7f13\u51b2\u533a\uff08\u5411\u7f51\u7edc\u5176\u4ed6\u90e8\u5206\u66b4\u9732\u8db3\u8ff9\uff09\u3002\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u534f\u65b9\u5dee\u548c\u53ef\u5fae\u5206\u7684Auto-$\u03a6$\u66ff\u4ee3\uff0c\u5b9e\u73b0\u5c40\u90e8\u4fe1\u606f\u6574\u5408\u7684\u6700\u5927\u5316\u3002RIIU\u5177\u6709\u7aef\u5230\u7aef\u53ef\u5fae\u6027\u3001\u53ef\u52a0\u6027\u7ec4\u5408\u6027\uff0c\u5e76\u5728\u68af\u5ea6\u4e0a\u5347\u4e0b\u8868\u73b0\u51fa$\u03a6$-\u5355\u8c03\u5851\u6027\u3002", "result": "\u5728\u516b\u65b9\u5411\u7f51\u683c\u4e16\u754c\u4e2d\uff0c\u56db\u5c42RIIU\u667a\u80fd\u4f53\u572813\u6b65\u5185\u6062\u590d\u4e86\u8d85\u8fc790%\u7684\u5956\u52b1\uff0c\u901f\u5ea6\u662f\u53c2\u6570\u5339\u914dGRU\u7684\u4e24\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u96f6\u7684Auto-$\u03a6$\u4fe1\u53f7\u3002", "conclusion": "RIIU\u5c06\u201c\u7c7b\u610f\u8bc6\u201d\u8ba1\u7b97\u7f29\u5c0f\u5230\u5355\u5143\u5c3a\u5ea6\uff0c\u4e3a\u4eba\u5de5\u610f\u8bc6\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u5b9e\u8bc1\u7684\u57fa\u7840\u6a21\u5757\uff0c\u63a8\u52a8\u4e86\u54f2\u5b66\u95ee\u9898\u5411\u6570\u5b66\u95ee\u9898\u7684\u8f6c\u5316\u3002", "paper_title_zh": "\u53cd\u5c04\u6027\u6574\u5408\u4fe1\u606f\u5355\u5143\uff1a\u4e00\u79cd\u7528\u4e8e\u4eba\u5de5\u610f\u8bc6\u7684\u53ef\u5fae\u5206\u57fa\u7840\u6a21\u5757", "abstract_zh": "\u4eba\u5de5\u610f\u8bc6\u7814\u7a76\u7f3a\u4e4f\u7c7b\u4f3c\u611f\u77e5\u673a\u7684\u5c0f\u578b\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u65e0\u6cd5\u8fdb\u884c\u590d\u5236\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6539\u8fdb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u53cd\u5c04\u6027\u6574\u5408\u4fe1\u606f\u5355\u5143\uff08RIIU\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5faa\u73af\u5355\u5143\uff0c\u901a\u8fc7\u6269\u5c55\u9690\u85cf\u72b6\u6001$h$\uff0c\u5f15\u5165\u4e24\u4e2a\u989d\u5916\u5411\u91cf\uff1a\uff08i\uff09\u5143\u72b6\u6001$\u03bc$\uff0c\u8bb0\u5f55\u5355\u5143\u81ea\u8eab\u7684\u56e0\u679c\u8db3\u8ff9\uff1b\uff08ii\uff09\u5e7f\u64ad\u7f13\u51b2\u533a$B$\uff0c\u5c06\u8be5\u8db3\u8ff9\u66b4\u9732\u7ed9\u7f51\u7edc\u7684\u5176\u4ed6\u90e8\u5206\u3002\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u534f\u65b9\u5dee\u548c\u53ef\u5fae\u5206\u7684Auto-$\u03a6$\u66ff\u4ee3\uff0c\u6bcf\u4e2aRIIU\u53ef\u4ee5\u5728\u7ebf\u6700\u5927\u5316\u5c40\u90e8\u4fe1\u606f\u6574\u5408\u3002\u6211\u4eec\u8bc1\u660e\u4e86RIIU\uff081\uff09\u5177\u6709\u7aef\u5230\u7aef\u53ef\u5fae\u6027\uff0c\uff082\uff09\u53ef\u52a0\u6027\u7ec4\u5408\uff0c\uff083\uff09\u5728\u68af\u5ea6\u4e0a\u5347\u4e0b\u8868\u73b0\u51fa$\u03a6$-\u5355\u8c03\u5851\u6027\u3002\u5728\u516b\u65b9\u5411\u7f51\u683c\u4e16\u754c\u4e2d\uff0c\u56db\u5c42RIIU\u667a\u80fd\u4f53\u572813\u6b65\u5185\u6062\u590d\u4e86\u8d85\u8fc790%\u7684\u5956\u52b1\uff0c\u901f\u5ea6\u662f\u53c2\u6570\u5339\u914dGRU\u7684\u4e24\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u96f6\u7684Auto-$\u03a6$\u4fe1\u53f7\u3002\u901a\u8fc7\u5c06\u201c\u7c7b\u610f\u8bc6\u201d\u8ba1\u7b97\u7f29\u5c0f\u5230\u5355\u5143\u5c3a\u5ea6\uff0cRIIU\u5c06\u54f2\u5b66\u8fa9\u8bba\u8f6c\u5316\u4e3a\u5b9e\u8bc1\u6570\u5b66\u95ee\u9898\u3002"}}
{"id": "2506.14123", "pdf": "https://arxiv.org/pdf/2506.14123", "abs": "https://arxiv.org/abs/2506.14123", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "title": "Sampling from Your Language Model One Byte at a Time", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u5c06\u57fa\u4e8eBPE\u5206\u8bcd\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u5b57\u7b26\u7ea7\u6216\u5b57\u8282\u7ea7\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5206\u8bcd\u5e26\u6765\u7684\u751f\u6210\u5931\u771f\u95ee\u9898\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u6a21\u578b\u96c6\u6210\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u666e\u904d\u4f7f\u7528\u5206\u8bcd\u6280\u672f\uff0c\u4f46\u5206\u8bcd\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u5931\u771f\uff08\u5982\u63d0\u793a\u8fb9\u754c\u95ee\u9898\uff09\uff0c\u4e14\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u96c6\u6210\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8eBPE\u5206\u8bcd\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u5b57\u7b26\u7ea7\u6216\u5b57\u8282\u7ea7\u6a21\u578b\uff0c\u4fdd\u6301\u6587\u672c\u751f\u6210\u5206\u5e03\u4e0d\u53d8\uff0c\u540c\u65f6\u652f\u6301\u4e0d\u540c\u5206\u8bcd\u5668\u6a21\u578b\u7684\u96c6\u6210\u548c\u4ee3\u7406\u8c03\u4f18\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u96c6\u6210\u548c\u4ee3\u7406\u8c03\u4f18\u540e\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5206\u8bcd\u5e26\u6765\u7684\u751f\u6210\u5931\u771f\u548c\u6a21\u578b\u96c6\u6210\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002", "paper_title_zh": "\u9010\u5b57\u8282\u91c7\u6837\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u51e0\u4e4e\u666e\u904d\u4f7f\u7528\u5206\u8bcd\u6280\u672f\uff0c\u901a\u8fc7\u591a\u5b57\u8282\u6216\u591a\u5b57\u7b26\u6807\u8bb0\u5b9e\u73b0\u9ad8\u6548\u7684\u6587\u672c\u8868\u793a\u3002\u7136\u800c\uff0c\u5148\u524d\u7814\u7a76\u8868\u660e\u5206\u8bcd\u53ef\u80fd\u626d\u66f2\u6a21\u578b\u7684\u751f\u6210\u3002\u4f8b\u5982\uff0c\u7528\u6237\u5e38\u88ab\u544a\u77e5\u4e0d\u8981\u5728\u63d0\u793a\u672b\u5c3e\u52a0\u7a7a\u683c\uff0c\u56e0\u4e3a\u8fd9\u4f1a\u5f71\u54cd\u6a21\u578b\u5c06\u7a7a\u683c\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7684\u4e00\u90e8\u5206\u3002\u8fd9\u79cd\u63d0\u793a\u8fb9\u754c\u95ee\u9898\uff08PBP\uff09\u4e5f\u51fa\u73b0\u5728\u4e2d\u6587\u548c\u4ee3\u7801\u751f\u6210\u4e2d\uff0c\u5176\u4e2d\u6807\u8bb0\u5e38\u4e0e\u8bed\u6cd5\u8fb9\u754c\u4e0d\u5339\u914d\u3002\u6b64\u5916\uff0c\u5206\u8bcd\u5668\u4e0d\u5339\u914d\u5e38\u963b\u788d\u6a21\u578b\u7ec4\u5408\u548c\u4e92\u64cd\u4f5c\u6027\u3002\u4f8b\u5982\uff0c\u7531\u4e8e\u8bcd\u6c47\u8868\u4e0d\u5339\u914d\uff0c\u65e0\u6cd5\u76f4\u63a5\u96c6\u6210\u4f7f\u7528\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u6a21\u578b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u5c06\u4efb\u4f55\u57fa\u4e8eBPE\u5206\u8bcd\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u5b57\u7b26\u7ea7\u6216\u5b57\u8282\u7ea7\u6a21\u578b\uff0c\u4e14\u4e0d\u6539\u53d8\u5176\u6587\u672c\u7ea7\u751f\u6210\u5206\u5e03\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\u4e86PBP\u95ee\u9898\uff0c\u8fd8\u80fd\u7edf\u4e00\u4e0d\u540c\u5206\u8bcd\u5668\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u6c47\u8868\uff0c\u652f\u6301\u5728\u63a8\u7406\u65f6\u96c6\u6210\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u8c03\u4f18\u5c06\u540e\u8bad\u7ec3\u4ece\u4e00\u4e2a\u6a21\u578b\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u96c6\u6210\u548c\u4ee3\u7406\u8c03\u4f18\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2506.14096", "pdf": "https://arxiv.org/pdf/2506.14096", "abs": "https://arxiv.org/abs/2506.14096", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5305\u62ec\u5b9e\u65f6\u6027\u80fd\u548c\u5b89\u5168\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u573a\u666f\u7406\u89e3\u4ee5\u786e\u4fdd\u5b89\u5168\u548c\u6548\u7387\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7ed3\u5408\u4e3a\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u5e26\u6765\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u53d1\u5c55\u73b0\u72b6\u3001\u5e94\u7528\u524d\u666f\u53ca\u6311\u6218\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86LLM\u589e\u5f3a\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u6839\u636e\u63d0\u793a\u673a\u5236\u548c\u6838\u5fc3\u67b6\u6784\u5bf9\u73b0\u6709\u6280\u672f\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u4ea4\u901a\u76d1\u63a7\u548c\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u589e\u5f3a\u7684\u56fe\u50cf\u5206\u5272\u6280\u672f\u80fd\u591f\u663e\u8457\u63d0\u5347\u9053\u8def\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\uff0c\u4f46\u4ecd\u9762\u4e34\u5b9e\u65f6\u6027\u80fd\u548c\u5b89\u5168\u53ef\u9760\u6027\u7b49\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u53d1\u5c55\u65b9\u5411\u5e94\u805a\u7126\u4e8e\u53ef\u89e3\u91ca\u3001\u4ee5\u4eba\u4e3a\u672c\u7684AI\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u4e0b\u4e00\u4ee3\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u6210\u529f\u90e8\u7f72\u3002", "paper_title_zh": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff1a\u9762\u5411\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u7efc\u8ff0\u4e0e\u5c55\u671b", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7ed3\u5408\u6b63\u5728\u6df1\u523b\u6539\u53d8\u56fe\u50cf\u5206\u5272\u7b49\u611f\u77e5\u4efb\u52a1\u3002\u5bf9\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u800c\u8a00\uff0c\u51c6\u786e\u7684\u573a\u666f\u7406\u89e3\u5bf9\u5b89\u5168\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e00\u65b0\u8303\u5f0f\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\u3002\u672c\u6587\u7cfb\u7edf\u6027\u5730\u7efc\u8ff0\u4e86LLM\u589e\u5f3a\u7684\u56fe\u50cf\u5206\u5272\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728ITS\u4e2d\u7684\u5e94\u7528\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002\u6211\u4eec\u57fa\u4e8e\u63d0\u793a\u673a\u5236\u548c\u6838\u5fc3\u67b6\u6784\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u521b\u65b0\u5982\u4f55\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u3001\u4ea4\u901a\u76d1\u63a7\u548c\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u4e2d\u7684\u9053\u8def\u573a\u666f\u7406\u89e3\u3002\u6700\u540e\uff0c\u6211\u4eec\u6307\u51fa\u4e86\u5305\u62ec\u5b9e\u65f6\u6027\u80fd\u548c\u5b89\u5168\u53ef\u9760\u6027\u5728\u5185\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4ee5\u53ef\u89e3\u91ca\u3001\u4ee5\u4eba\u4e3a\u672c\u7684AI\u4e3a\u4e2d\u5fc3\u7684\u53d1\u5c55\u89c6\u89d2\uff0c\u4f5c\u4e3a\u8be5\u6280\u672f\u5728\u4e0b\u4e00\u4ee3\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6210\u529f\u90e8\u7f72\u7684\u524d\u63d0\u3002"}}
{"id": "2506.13841", "pdf": "https://arxiv.org/pdf/2506.13841", "abs": "https://arxiv.org/abs/2506.13841", "authors": ["Miho Koda", "Yu Zheng", "Ruixian Ma", "Mingyang Sun", "Devesh Pansare", "Fabio Duarte", "Paolo Santi"], "title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LocationReasoner\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u771f\u5b9e\u4e16\u754c\u9009\u5740\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\uff08\u5982OpenAI o4\uff09\u572830%\u7684\u4efb\u52a1\u4e2d\u5931\u8d25\uff0c\u4e14\u4ee3\u7406\u7b56\u7565\uff08\u5982ReAct\u548cReflexion\uff09\u56e0\u8fc7\u5ea6\u63a8\u7406\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u4e3b\u8981\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u9886\u57df\u5f97\u5230\u9a8c\u8bc1\uff0c\u4f46\u5176\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\uff08\u5982\u9009\u5740\uff09\u4e2d\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4e16\u754c\u9009\u5740\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86LocationReasoner\u57fa\u51c6\uff0c\u5305\u542b300\u591a\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u67e5\u8be2\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6c99\u76d2\u73af\u5883\u652f\u6301\u7ea6\u675f\u6761\u4ef6\u641c\u7d22\u3002\u901a\u8fc7\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\uff08\u5982OpenAI o4\uff09\u548c\u4ee3\u7406\u7b56\u7565\uff08\u5982ReAct\u548cReflexion\uff09\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u9009\u5740\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0cOpenAI o4\u572830%\u7684\u4efb\u52a1\u4e2d\u5931\u8d25\u3002\u4ee3\u7406\u7b56\u7565\u56e0\u8fc7\u5ea6\u63a8\u7406\u5bfc\u81f4\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "LLMs\u5728\u6574\u4f53\u548c\u975e\u7ebf\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002LocationReasoner\u57fa\u51c6\u7684\u53d1\u5e03\u65e8\u5728\u63a8\u52a8LLMs\u548c\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u7a33\u5065\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002", "paper_title_zh": "LocationReasoner\uff1a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u9009\u5740\u63a8\u7406\u4e2d\u7684\u8868\u73b0", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u901a\u8fc7\u5f3a\u5316\u540e\u8bad\u7ec3\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff08\u5982OpenAI o1\u548cDeepSeek-R1\uff09\u5c55\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u80fd\u529b\u4e3b\u8981\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u548c\u4ee3\u7801\u751f\u6210\u7b49\u9886\u57df\u5f97\u5230\u9a8c\u8bc1\uff0c\u5176\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u63d0\u51faLocationReasoner\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4e16\u754c\u9009\u5740\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u578b\u9700\u901a\u8fc7\u590d\u6742\u7684\u7a7a\u95f4\u3001\u73af\u5883\u548c\u7269\u6d41\u7ea6\u675f\u63a8\u7406\u786e\u5b9a\u53ef\u884c\u4f4d\u7f6e\u3002\u8be5\u57fa\u51c6\u5305\u542b300\u591a\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67e5\u8be2\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u652f\u6301\u7ea6\u675f\u6761\u4ef6\u641c\u7d22\u7684\u6c99\u76d2\u73af\u5883\u3002\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u6709\u9650\uff0cOpenAI o4\u6a21\u578b\u572830%\u7684\u9009\u5740\u4efb\u52a1\u4e2d\u5931\u8d25\u3002\u6b64\u5916\uff0c\u4ee3\u7406\u7b56\u7565\uff08\u5982ReAct\u548cReflexion\uff09\u56e0\u8fc7\u5ea6\u63a8\u7406\u8868\u73b0\u66f4\u5dee\u3002\u672c\u6587\u63ed\u793a\u4e86LLMs\u5728\u6574\u4f53\u548c\u975e\u7ebf\u6027\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\uff0c\u5e76\u53d1\u5e03LocationReasoner\u57fa\u51c6\u4ee5\u63a8\u52a8LLMs\u548c\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u7a33\u5065\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002\u4ee3\u7801\u548c\u6570\u636e\u8be6\u89c1https://github.com/miho-koda/LocationReasoner\u3002"}}
{"id": "2506.14157", "pdf": "https://arxiv.org/pdf/2506.14157", "abs": "https://arxiv.org/abs/2506.14157", "authors": ["Chengyu Huang", "Tanya Goyal"], "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCRM\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u8861\u91cf\u504f\u597d\u4f18\u5316\u4e2d\u54cd\u5e94\u5bf9\u7684\u8d28\u91cf\u3002\u901a\u8fc7\u7ed3\u5408\u8ddd\u79bb\u548c\u5956\u52b1\u8fb9\u9645\uff0cDCRM\u9f13\u52b1\u6700\u5c0f\u5316\u566a\u58f0\u5dee\u5f02\u548c\u6700\u5927\u5316\u671f\u671b\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8DCRM\u7684\u8bad\u7ec3\u96c6\u80fd\u5e26\u6765\u66f4\u597d\u7684\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u4f73\u914d\u5bf9\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8bd5\u56fe\u5c06\u504f\u597d\u4f18\u5316\u6027\u80fd\u4e0e\u5e95\u5c42\u504f\u597d\u6570\u636e\u96c6\u5173\u8054\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u504f\u597d\u54cd\u5e94\u95f4\u7684\u5dee\u5f02\u53ef\u80fd\u4e0d\u7b26\u5408\u671f\u671b\u7684\u5b66\u4e60\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u91cf\u5316\u8fd9\u4e9b\u5dee\u5f02\u7684\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u54cd\u5e94\u5bf9\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faDCRM\uff08\u8ddd\u79bb\u6821\u51c6\u5956\u52b1\u8fb9\u9645\uff09\u6307\u6807\uff0c\u7ed3\u5408\u8ddd\u79bb\u548c\u5956\u52b1\u8fb9\u9645\u91cf\u5316\u54cd\u5e94\u5bf9\u7684\u5dee\u5f02\u3002\u901a\u8fc7\u5206\u6790\u4e09\u7c7b\u5e38\u7528\u504f\u597d\u6570\u636e\u96c6\uff0c\u7814\u7a76DCRM\u4e0e\u5b66\u4e60\u6548\u679c\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u6700\u4f73\u914d\u5bf9\u65b9\u6cd5\u9009\u62e9\u9ad8DCRM\u7684\u54cd\u5e94\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u9ad8DCRM\u7684\u8bad\u7ec3\u96c6\u4e0e\u66f4\u597d\u7684\u5b66\u4e60\u6548\u679c\u76f8\u5173\u3002\u63d0\u51fa\u7684\u6700\u4f73\u914d\u5bf9\u65b9\u6cd5\u5728AlpacaEval\u3001MT-Bench\u548cArena-Hard\u7b49\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "DCRM\u662f\u4e00\u79cd\u6709\u6548\u7684\u54cd\u5e94\u5bf9\u8d28\u91cf\u8861\u91cf\u6307\u6807\uff0c\u80fd\u591f\u6307\u5bfc\u504f\u597d\u6570\u636e\u96c6\u7684\u6784\u5efa\uff0c\u4ece\u800c\u4f18\u5316\u6a21\u578b\u5b66\u4e60\u6548\u679c\u3002", "paper_title_zh": "DCRM\uff1a\u4e00\u79cd\u8861\u91cf\u504f\u597d\u4f18\u5316\u4e2d\u54cd\u5e94\u5bf9\u8d28\u91cf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5", "abstract_zh": "\u8fd1\u671f\u7814\u7a76\u5c1d\u8bd5\u5c06\u504f\u597d\u4f18\u5316\uff08PO\uff09\u6027\u80fd\u4e0e\u5e95\u5c42\u504f\u597d\u6570\u636e\u96c6\u5173\u8054\u3002\u672c\u6587\u89c2\u5bdf\u5230\uff0c\u504f\u597d\u54cd\u5e94$y^+$\u4e0e\u975e\u504f\u597d\u54cd\u5e94$y^-$\u4e4b\u95f4\u7684\u5dee\u5f02\u4f1a\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u6548\u679c\uff0c\u800c\u8fd9\u4e9b\u5dee\u5f02\u53ef\u80fd\u4e0e\u671f\u671b\u7684\u5b66\u4e60\u76ee\u6807\u4e0d\u5339\u914d\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4f7f\u7528\u8ddd\u79bb\u548c\u5956\u52b1\u8fb9\u9645\u91cf\u5316\u8fd9\u4e9b\u5dee\u5f02\uff0c\u5e76\u5c06\u5176\u7ed3\u5408\u4e3a\u8ddd\u79bb\u6821\u51c6\u5956\u52b1\u8fb9\u9645\uff08DCRM\uff09\uff0c\u7528\u4e8e\u8861\u91cfPO\u4e2d\u54cd\u5e94\u5bf9\u7684\u8d28\u91cf\u3002\u76f4\u89c2\u4e0a\uff0cDCRM\u9f13\u52b1\u6700\u5c0f\u5316\u566a\u58f0\u5dee\u5f02\u548c\u6700\u5927\u5316\u671f\u671b\u5dee\u5f02\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e09\u7c7b\u5e38\u7528\u504f\u597d\u6570\u636e\u96c6\uff0c\u6309\u54cd\u5e94\u6765\u6e90\u548c\u504f\u597d\u6807\u6ce8\u51fd\u6570\u5206\u7c7b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8bad\u7ec3\u96c6\u7684DCRM\u8d8a\u9ad8\uff0c\u5b66\u4e60\u6548\u679c\u8d8a\u597d\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u4f73-$N^2$\u914d\u5bf9\u65b9\u6cd5\uff0c\u9009\u62e9DCRM\u6700\u9ad8\u7684\u54cd\u5e94\u5bf9\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5728AlpacaEval\u3001MT-Bench\u548cArena-Hard\u4efb\u52a1\u4e2d\u5747\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.14121", "pdf": "https://arxiv.org/pdf/2506.14121", "abs": "https://arxiv.org/abs/2506.14121", "authors": ["Siyu Xu", "Wenjie Li", "Guangwei Gao", "Jian Yang", "Guo-Jun Qi", "Chia-Wen Lin"], "title": "FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution", "categories": ["cs.CV"], "comment": "12 pages, 11 figures, 6 tales", "summary": "Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.", "AI": {"tldr": "FADPNet\u662f\u4e00\u79cd\u9891\u7387\u611f\u77e5\u7684\u53cc\u8def\u5f84\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u9762\u90e8\u7279\u5f81\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u6210\u5206\uff0c\u5206\u522b\u7528Mamba\u548cCNN\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u5728\u6709\u9650\u8ba1\u7b97\u6210\u672c\u4e0b\u7684\u9ad8\u6548\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u50cf\u7d20\u4e00\u89c6\u540c\u4ec1\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0d\u4f18\u548c\u6027\u80fd\u4e0b\u964d\u3002CNN\u5bf9\u9ad8\u9891\u7279\u5f81\u654f\u611f\uff0c\u800cMamba\u64c5\u957f\u4f4e\u9891\u7279\u5f81\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86FADPNet\u3002", "method": "FADPNet\u5c06\u9762\u90e8\u7279\u5f81\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u6210\u5206\uff0c\u5206\u522b\u901a\u8fc7Mamba-based\u4f4e\u9891\u589e\u5f3a\u5757\uff08LFEB\uff09\u548cCNN-based\u6df1\u5ea6\u4f4d\u7f6e\u611f\u77e5\u6ce8\u610f\u529b\uff08DPA\uff09\u6a21\u5757\u5904\u7406\uff0c\u8f85\u4ee5\u9ad8\u9891\u7ec6\u5316\uff08HFR\uff09\u6a21\u5757\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u548c\u6a21\u578b\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FADPNet\u901a\u8fc7\u9891\u7387\u611f\u77e5\u7684\u53cc\u8def\u5f84\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "paper_title_zh": "FADPNet\uff1a\u57fa\u4e8e\u9891\u7387\u611f\u77e5\u7684\u53cc\u8def\u5f84\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u7f51\u7edc", "abstract_zh": "\u5728\u6709\u9650\u8ba1\u7b97\u6210\u672c\u4e0b\u5b9e\u73b0\u4eba\u8138\u8d85\u5206\u8fa8\u7387\uff08FSR\uff09\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u9762\u90e8\u50cf\u7d20\u5e73\u7b49\u5904\u7406\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0d\u4f18\u548cFSR\u6027\u80fd\u4e0b\u964d\u3002CNN\u5bf9\u9ad8\u9891\u9762\u90e8\u7279\u5f81\uff08\u5982\u8f6e\u5ed3\u548c\u9762\u90e8\u7ebf\u6761\uff09\u8f83\u4e3a\u654f\u611f\uff0c\u800cMamba\u64c5\u957f\u6355\u6349\u4f4e\u9891\u7279\u5f81\uff08\u5982\u80a4\u8272\u548c\u7ec6\u7c92\u5ea6\u7eb9\u7406\uff09\uff0c\u4e14\u590d\u6742\u5ea6\u4f4e\u4e8eTransformer\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FADPNet\uff0c\u4e00\u79cd\u9891\u7387\u611f\u77e5\u7684\u53cc\u8def\u5f84\u7f51\u7edc\uff0c\u5c06\u9762\u90e8\u7279\u5f81\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u6210\u5206\uff0c\u5e76\u901a\u8fc7\u4e13\u7528\u5206\u652f\u5904\u7406\u3002\u5bf9\u4e8e\u4f4e\u9891\u533a\u57df\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8eMamba\u7684\u4f4e\u9891\u589e\u5f3a\u5757\uff08LFEB\uff09\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6ce8\u610f\u529b\u548c\u6324\u538b-\u6fc0\u52b1\u64cd\u4f5c\uff0c\u63d0\u53d6\u4f4e\u9891\u5168\u5c40\u4ea4\u4e92\u5e76\u5f3a\u8c03\u4fe1\u606f\u901a\u9053\u3002\u5bf9\u4e8e\u9ad8\u9891\u533a\u57df\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u57fa\u4e8eCNN\u7684\u6df1\u5ea6\u4f4d\u7f6e\u611f\u77e5\u6ce8\u610f\u529b\uff08DPA\uff09\u6a21\u5757\uff0c\u589e\u5f3a\u7a7a\u95f4\u4f9d\u8d56\u7684\u7ed3\u6784\u7ec6\u8282\uff0c\u8f85\u4ee5\u8f7b\u91cf\u7ea7\u9ad8\u9891\u7ec6\u5316\uff08HFR\uff09\u6a21\u5757\u8fdb\u4e00\u6b65\u4f18\u5316\u9891\u7387\u7279\u5b9a\u8868\u793a\u3002\u901a\u8fc7\u4e0a\u8ff0\u8bbe\u8ba1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728FSR\u8d28\u91cf\u548c\u6a21\u578b\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.13917", "pdf": "https://arxiv.org/pdf/2506.13917", "abs": "https://arxiv.org/abs/2506.13917", "authors": ["Miguel A. Lago", "Ghada Zamzmi", "Brandon Eich", "Jana G. Delfino"], "title": "Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features", "categories": ["cs.AI"], "comment": null, "summary": "Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u53ef\u89e3\u91caAI\u7279\u5f81\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u4e00\u81f4\u6027\u3001\u5408\u7406\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u56db\u9879\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30AI\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u91ca\u8d28\u91cf\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u62a5\u544a\u53ef\u89e3\u91caAI\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56db\u9879\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff1a1) \u4e00\u81f4\u6027\u91cf\u5316\u76f8\u4f3c\u8f93\u5165\u4e0b\u89e3\u91ca\u7684\u53d8\u5f02\u6027\uff1b2) \u5408\u7406\u6027\u4f30\u8ba1\u89e3\u91ca\u4e0e\u771f\u5b9e\u60c5\u51b5\u7684\u63a5\u8fd1\u7a0b\u5ea6\uff1b3) \u5fe0\u5b9e\u6027\u8bc4\u4f30\u89e3\u91ca\u4e0e\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u5339\u914d\u5ea6\uff1b4) \u5b9e\u7528\u6027\u8861\u91cf\u89e3\u91ca\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91caAI\u65b9\u6cd5\u7684\u8bc4\u5206\u5361\uff0c\u5e76\u901a\u8fc7Ablation CAM\u548cEigen CAM\u5728\u5408\u6210\u4e73\u817aX\u5149\u7247\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30AI\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u91ca\u8d28\u91cf\u63d0\u4f9b\u4e86\u6807\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u5173\u4e8e\u53ef\u89e3\u91ca\u6027\u4ef7\u503c\u7684\u8ba8\u8bba\uff0c\u5e76\u5e2e\u52a9\u6539\u8fdb\u57fa\u4e8eAI\u7684\u533b\u7597\u8bbe\u5907\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "paper_title_zh": "\u8bc4\u4f30\u53ef\u89e3\u91ca\u6027\uff1a\u4e00\u79cd\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u62a5\u544a\u53ef\u89e3\u91caAI\u7279\u5f81\u7684\u6846\u67b6", "abstract_zh": "\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u65e8\u5728\u63ed\u793aAI\u8bbe\u5907\u7684\u5185\u90e8\u673a\u5236\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u7684\u6280\u672f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u548c\u62a5\u544a\u53ef\u89e3\u91caAI\u7279\u5f81\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u56db\u9879\u6807\u51c6\uff1a1) \u4e00\u81f4\u6027\u91cf\u5316\u76f8\u4f3c\u8f93\u5165\u4e0b\u89e3\u91ca\u7684\u53d8\u5f02\u6027\uff1b2) \u5408\u7406\u6027\u4f30\u8ba1\u89e3\u91ca\u4e0e\u771f\u5b9e\u60c5\u51b5\u7684\u63a5\u8fd1\u7a0b\u5ea6\uff1b3) \u5fe0\u5b9e\u6027\u8bc4\u4f30\u89e3\u91ca\u4e0e\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u5339\u914d\u5ea6\uff1b4) \u5b9e\u7528\u6027\u8861\u91cf\u89e3\u91ca\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91caAI\u65b9\u6cd5\u7684\u8bc4\u5206\u5361\uff0c\u4f5c\u4e3a\u6b64\u7c7b\u7b97\u6cd5\u7684\u5b8c\u6574\u63cf\u8ff0\u548c\u8bc4\u4f30\u5de5\u5177\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u8fd9\u56db\u9879\u6807\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u793a\u4f8b\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u4f7f\u7528Ablation CAM\u548cEigen CAM\u5728\u5408\u6210\u4e73\u817aX\u5149\u7247\u4e0a\u8bc4\u4f30\u4e86\u89e3\u91ca\u70ed\u56fe\u3002\u524d\u4e09\u9879\u6807\u51c6\u5728\u4e34\u5e8a\u76f8\u5173\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u8bc4\u4f30AI\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u91ca\u8d28\u91cf\u5efa\u7acb\u4e86\u6807\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u5173\u4e8e\u53ef\u89e3\u91ca\u6027\u4ef7\u503c\u7684\u8ba8\u8bba\uff0c\u5e76\u5e2e\u52a9\u6539\u8fdb\u57fa\u4e8eAI\u7684\u533b\u7597\u8bbe\u5907\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002"}}
{"id": "2506.14158", "pdf": "https://arxiv.org/pdf/2506.14158", "abs": "https://arxiv.org/abs/2506.14158", "authors": ["Tao He", "Guang Huang", "Yu Yang", "Tianshi Xu", "Sicheng Zhao", "Guiguang Ding", "Pengyang Wang", "Feng Tian"], "title": "S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS$^4$C\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u6cd5\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u5ef6\u8fdf\u5e76\u589e\u52a0\u6709\u6548\u4ee4\u724c\u751f\u6210\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u81ea\u56de\u5f52\u7279\u6027\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u8f83\u9ad8\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002\u73b0\u6709\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u6587\u672c\u751f\u6210\u7684\u8fde\u8d2f\u6027\uff0c\u9650\u5236\u4e86\u6548\u7387\u63d0\u5347\u3002", "method": "S$^4$C\u6846\u67b6\u6269\u5c55\u4e86\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u5934\u8349\u7a3f\u751f\u6210\u5feb\u901f\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u8fde\u7eed\u9a8c\u8bc1\u6811\u9ad8\u6548\u9a8c\u8bc1\u5019\u9009\u4ee4\u724c\u548c\u7279\u5f81\u590d\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS$^4$C\u5728\u4e3b\u6d41\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u6548\u7387\u3001\u5e76\u884c\u6027\uff0c\u5e76\u4ee5\u66f4\u5c11\u8ba1\u7b97\u8d44\u6e90\u751f\u6210\u66f4\u591a\u6709\u6548\u4ee4\u724c\u3002\u5728Spec-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u52a0\u901f\u6bd4\u8fbe2.26x-2.60x\u3002", "conclusion": "S$^4$C\u901a\u8fc7\u7ed3\u5408\u8bed\u6cd5\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "S$^4$C\uff1a\u57fa\u4e8e\u8bed\u6cd5\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u63a8\u7406", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u81ea\u56de\u5f52\u7279\u6027\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u8f83\u9ad8\uff0c\u5bf9\u5b9e\u65f6\u5e94\u7528\u6784\u6210\u6311\u6218\u3002\u63a8\u6d4b\u91c7\u6837\u901a\u8fc7\u5f15\u5165\u8349\u7a3f\u751f\u6210\u548c\u5e76\u884c\u9a8c\u8bc1\u9636\u6bb5\uff0c\u52a0\u901f\u4ee4\u724c\u751f\u6210\u4e0e\u9a8c\u8bc1\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6587\u672c\u751f\u6210\u7684\u8fde\u8d2f\u6027\uff0c\u9650\u5236\u4e86\u6548\u7387\u63d0\u5347\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u6cd5\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u63a8\u6d4b\u91c7\u6837\u6846\u67b6\uff08S$^4$C\uff09\uff0c\u901a\u8fc7\u591a\u5934\u8349\u7a3f\u751f\u6210\u5feb\u901f\u4ee4\u724c\uff0c\u5e76\u5229\u7528\u8fde\u7eed\u9a8c\u8bc1\u6811\u9ad8\u6548\u9a8c\u8bc1\u5019\u9009\u4ee4\u724c\u548c\u7279\u5f81\u590d\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cS$^4$C\u5728\u4e3b\u6d41\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u6548\u7387\u3001\u5e76\u884c\u6027\uff0c\u5e76\u4ee5\u66f4\u5c11\u8ba1\u7b97\u8d44\u6e90\u751f\u6210\u66f4\u591a\u6709\u6548\u4ee4\u724c\u3002\u5728Spec-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cS$^4$C\u5b9e\u73b0\u4e862.26x-2.60x\u7684\u52a0\u901f\u6bd4\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2506.14130", "pdf": "https://arxiv.org/pdf/2506.14130", "abs": "https://arxiv.org/abs/2506.14130", "authors": ["Chunyu Cao", "Jintao Cheng", "Zeyu Chen", "Linfan Zhan", "Rui Fan", "Zhijian He", "Xiaoyu Tang"], "title": "KDMOS:Knowledge Distillation for Motion Segmentation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8fd0\u52a8\u76ee\u6807\u5206\u5272\u6846\u67b6KDMOS\uff0c\u901a\u8fc7\u5c06\u9e1f\u77b0\u56fe\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\u3001\u975e\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u4e0a\u91c7\u6837\u548c\u7f51\u7edc\u67b6\u6784\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u8fd0\u52a8\u76ee\u6807\u5206\u5272\uff08MOS\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u3002", "method": "\u91c7\u7528\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\uff0c\u975e\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u3002\u9488\u5bf9\u8fd0\u52a8\u4e0e\u975e\u8fd0\u52a8\u7c7b\u522b\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u89e3\u8026\u5e76\u5e94\u7528\u5b9a\u5236\u5316\u7684\u84b8\u998f\u7b56\u7565\uff0c\u4f18\u5316\u52a8\u6001\u4e0a\u91c7\u6837\u548c\u7f51\u7edc\u67b6\u6784\uff0c\u51cf\u5c11\u53c2\u6570\u6570\u91cf7.69%\u3002", "result": "\u5728SemanticKITTI-MOS\u6570\u636e\u96c6\u7684\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8678.8%\u7684IoU\uff0c\u5e76\u5728Apollo\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u3002", "conclusion": "KDMOS\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u7f51\u7edc\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u76ee\u6807\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "KDMOS\uff1a\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8fd0\u52a8\u76ee\u6807\u5206\u5272", "abstract_zh": "\u8fd0\u52a8\u76ee\u6807\u5206\u5272\uff08MOS\uff09\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u63d0\u5347\u5b9a\u4f4d\u3001\u8def\u5f84\u89c4\u5212\u3001\u5730\u56fe\u6784\u5efa\u3001\u573a\u666f\u6d41\u4f30\u8ba1\u548c\u672a\u6765\u72b6\u6001\u9884\u6d4b\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5e73\u8861\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u63a8\u7406\u4ecd\u5177\u6311\u6218\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8elogits\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u7cbe\u5ea6\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u91c7\u7528\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\uff0c\u975e\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u3002\u9488\u5bf9\u8fd0\u52a8\u4e0e\u975e\u8fd0\u52a8\u7c7b\u522b\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u89e3\u8026\u5e76\u5e94\u7528\u5b9a\u5236\u5316\u84b8\u998f\u7b56\u7565\uff0c\u4f7f\u6559\u5e08\u6a21\u578b\u66f4\u597d\u5730\u5b66\u4e60\u5173\u952e\u8fd0\u52a8\u7279\u5f81\uff0c\u663e\u8457\u51cf\u5c11\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u52a8\u6001\u4e0a\u91c7\u6837\uff0c\u4f18\u5316\u7f51\u7edc\u67b6\u6784\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c117.69%\uff0c\u7f13\u89e3\u8fc7\u62df\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728SemanticKITTI-MOS\u6570\u636e\u96c6\u7684\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8678.8%\u7684IoU\uff0c\u5e76\u5728Apollo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002KDMOS\u5b9e\u73b0\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/SCNU-RISLAB/KDMOS\u3002"}}
{"id": "2506.13920", "pdf": "https://arxiv.org/pdf/2506.13920", "abs": "https://arxiv.org/abs/2506.13920", "authors": ["Mbithe Nzomo", "Deshendran Moodley"], "title": "Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction", "categories": ["cs.AI"], "comment": "This work has been accepted for presentation at the 49th IEEE International Conference on Computers, Software, and Applications (COMPSAC 2025). The final published version will be available via IEEE Xplore", "summary": "Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u75be\u75c5\u98ce\u9669\u9884\u6d4b\uff0c\u901a\u8fc7\u5b9e\u9645\u75c5\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u53ef\u7528\u4e8e\u75be\u75c5\u98ce\u9669\u9884\u6d4b\uff0c\u4f46\u9700\u5c06\u901a\u7528\u533b\u5b66\u77e5\u8bc6\u9002\u914d\u5230\u5177\u4f53\u533b\u7597\u573a\u666f\u548c\u60a3\u8005\u7fa4\u4f53\u4e2d\uff0c\u540c\u65f6\u9700\u5904\u7406\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u786e\u5b9a\u6027\u5065\u5eb7\u7ed3\u679c\uff0c\u5e76\u4fdd\u6301\u9884\u6d4b\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001EHR\u6570\u636e\uff0c\u5b9e\u73b0\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u3002\u4ee5\u5fc3\u623f\u98a4\u52a8\u4e3a\u4f8b\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u901a\u7528\u533b\u5b66\u77e5\u8bc6\u4e0e\u60a3\u8005\u7279\u5b9a\u60c5\u5883\u3001\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u63d0\u4f9b\u9ad8\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u9884\u6d4b\u6027\u80fd\u826f\u597d\u3002", "conclusion": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u65b9\u6cd5\u4e3a\u53ef\u89e3\u91ca\u7684\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u878d\u5408\uff1a\u4e00\u79cd\u53ef\u89e3\u91ca\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u7684\u6df7\u5408\u65b9\u6cd5", "abstract_zh": "\u591a\u6a21\u6001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u53ef\u7528\u4e8e\u57fa\u4e8e\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u7684\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u3002\u7136\u800c\uff0c\u901a\u7528\u533b\u5b66\u77e5\u8bc6\u9700\u9002\u914d\u5177\u4f53\u533b\u7597\u573a\u666f\u548c\u60a3\u8005\u7fa4\u4f53\u4ee5\u5b9e\u73b0\u4e34\u5e8a\u5b9e\u7528\u3002\u6b64\u5916\uff0c\u98ce\u9669\u9884\u6d4b\u7cfb\u7edf\u9700\u5904\u7406\u6570\u636e\u4e0d\u5b8c\u6574\u6027\u548c\u975e\u786e\u5b9a\u6027\u5065\u5eb7\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BNs\uff09\uff0c\u53ef\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u57fa\u4e8e\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u6a21\u6001EHR\u6570\u636e\u6784\u5efa\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u3002\u901a\u8fc7\u5fc3\u623f\u98a4\u52a8\u7684\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u548c\u771f\u5b9e\u4e16\u754cEHR\u6570\u636e\uff0c\u6211\u4eec\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u901a\u7528\u533b\u5b66\u77e5\u8bc6\u4e0e\u60a3\u8005\u7279\u5b9a\u60c5\u5883\u3001\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027\u53ca\u826f\u597d\u9884\u6d4b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.14161", "pdf": "https://arxiv.org/pdf/2506.14161", "abs": "https://arxiv.org/abs/2506.14161", "authors": ["Yanlin Li", "Hao Liu", "Huimin Liu", "Yinwei Wei", "Yupeng Hu"], "title": "MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind", "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIST\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u548c\u591a\u7ef4\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\uff08SCM\uff09\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u5176\u5728\u80fd\u529b\u3001\u793e\u4ea4\u6027\u548c\u9053\u5fb7\u65b9\u9762\u7684\u590d\u6742\u504f\u89c1\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u9690\u6027\u504f\u89c1\u7684\u5fae\u5999\u548c\u591a\u7ef4\u7279\u6027\uff0c\u4e14\u6613\u53d7\u793e\u4f1a\u671f\u671b\u6548\u5e94\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5fc3\u7406\u7406\u8bba\u548c\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u5168\u9762\u548c\u95f4\u63a5\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faMIST\u6846\u67b6\uff0c\u7ed3\u5408\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\uff08SCM\uff09\uff0c\u8bbe\u8ba1\u4e24\u79cd\u95f4\u63a5\u4efb\u52a1\uff1a\u8bcd\u6c47\u8054\u60f3\u504f\u89c1\u6d4b\u8bd5\uff08WABT\uff09\u548c\u60c5\u611f\u5f52\u56e0\u6d4b\u8bd5\uff08AAT\uff09\uff0c\u4ee5\u8bc4\u4f30\u9690\u6027\u504f\u89c1\u7684\u591a\u7ef4\u8868\u73b0\u3002", "result": "\u57288\u79cd\u5148\u8fdbLLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u63ed\u793a\u590d\u6742\u7684\u504f\u89c1\u7ed3\u6784\uff0c\u5305\u62ec\u666e\u904d\u7684\u793e\u4ea4\u6027\u504f\u89c1\u3001\u591a\u7ef4\u5206\u6b67\u548c\u4e0d\u5bf9\u79f0\u523b\u677f\u5370\u8c61\u653e\u5927\u3002", "conclusion": "MIST\u6846\u67b6\u4e3a\u8bc6\u522b\u9690\u6027\u504f\u89c1\u7684\u7ed3\u6784\u6027\u672c\u8d28\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u591a\u7ef4\u5fc3\u7406\u7406\u8bba\u4e2d\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "paper_title_zh": "MIST\uff1a\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\u7684\u591a\u7ef4\u9690\u6027\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u8bc4\u4f30\u6846\u67b6", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u6307\u5176\u63a8\u7406\u5fc3\u7406\u72b6\u6001\u7684\u80fd\u529b\uff0c\u4f46\u8fd9\u4e00\u80fd\u529b\u7684\u5931\u8d25\u5e38\u8868\u73b0\u4e3a\u7cfb\u7edf\u6027\u9690\u6027\u504f\u89c1\u3002\u8bc4\u4f30\u8fd9\u79cd\u504f\u89c1\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u76f4\u63a5\u67e5\u8be2\u65b9\u6cd5\u6613\u53d7\u793e\u4f1a\u671f\u671b\u6548\u5e94\u5f71\u54cd\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u5176\u5fae\u5999\u7684\u591a\u7ef4\u7279\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\uff08SCM\uff09\u5c06\u504f\u89c1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8de8\u80fd\u529b\u3001\u793e\u4ea4\u6027\u548c\u9053\u5fb7\u7684\u591a\u7ef4ToM\u5931\u8d25\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e24\u79cd\u95f4\u63a5\u4efb\u52a1\uff1a\u8bcd\u6c47\u8054\u60f3\u504f\u89c1\u6d4b\u8bd5\uff08WABT\uff09\u7528\u4e8e\u8bc4\u4f30\u9690\u6027\u8bcd\u6c47\u5173\u8054\uff0c\u60c5\u611f\u5f52\u56e0\u6d4b\u8bd5\uff08AAT\uff09\u7528\u4e8e\u6d4b\u91cf\u9690\u853d\u60c5\u611f\u503e\u5411\uff0c\u4e24\u8005\u5747\u65e8\u5728\u63a2\u7a76\u6f5c\u5728\u523b\u677f\u5370\u8c61\u800c\u4e0d\u89e6\u53d1\u6a21\u578b\u56de\u907f\u3002\u57288\u79cd\u5148\u8fdbLLMs\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u63ed\u793a\u590d\u6742\u504f\u89c1\u7ed3\u6784\uff0c\u5305\u62ec\u666e\u904d\u7684\u793e\u4ea4\u6027\u504f\u89c1\u3001\u591a\u7ef4\u5206\u6b67\u548c\u4e0d\u5bf9\u79f0\u523b\u677f\u5370\u8c61\u653e\u5927\uff0c\u4ece\u800c\u4e3a\u8bc6\u522b\u9690\u6027\u504f\u89c1\u7684\u7ed3\u6784\u6027\u672c\u8d28\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.14136", "pdf": "https://arxiv.org/pdf/2506.14136", "abs": "https://arxiv.org/abs/2506.14136", "authors": ["Nafiz Sadman", "Farhana Zulkernine", "Benjamin Kwan"], "title": "Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology", "categories": ["cs.CV"], "comment": "GitHub: https://github.com/Nafiz95/BioVLM_Eval_CXR", "summary": "In this paper, we construct two research objectives: i) explore the learned embedding space of BiomedCLIP, an open-source large vision language model, to analyse meaningful class separations, and ii) quantify the limitations of BiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label medical dataset. We experiment on IU-xray dataset, which exhibits the aforementioned criteria, and evaluate BiomedCLIP in classifying images (radiographs) in three contexts: zero-shot inference, full finetuning, and linear probing. The results show that the model under zero-shot settings over-predicts all labels, leading to poor precision and inter-class separability. Full fine-tuning improves classification of distinct diseases, while linear probing detects overlapping features. We demonstrate visual understanding of the model using Grad-CAM heatmaps and compare with 15 annotations by a radiologist. We highlight the need for careful adaptations of the models to foster reliability and applicability in a real-world setting. The code for the experiments in this work is available and maintained on GitHub.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578bBiomedCLIP\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u3001\u5206\u5e03\u5916\u591a\u6807\u7b7e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u5176\u5d4c\u5165\u7a7a\u95f4\u7684\u7c7b\u522b\u5206\u79bb\u6027\uff0c\u5e76\u91cf\u5316\u5176\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u96f6\u6837\u672c\u63a8\u7406\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5168\u5fae\u8c03\u548c\u7ebf\u6027\u63a2\u6d4b\u80fd\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u7814\u7a76BiomedCLIP\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u3001\u5206\u5e03\u5916\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5176\u5d4c\u5165\u7a7a\u95f4\u7684\u7c7b\u522b\u5206\u79bb\u6027\uff0c\u5e76\u91cf\u5316\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u5728IU-xray\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30BiomedCLIP\u7684\u4e09\u79cd\u5206\u7c7b\u65b9\u5f0f\uff1a\u96f6\u6837\u672c\u63a8\u7406\u3001\u5168\u5fae\u8c03\u548c\u7ebf\u6027\u63a2\u6d4b\u3002\u901a\u8fc7Grad-CAM\u70ed\u56fe\u53ef\u89c6\u5316\u6a21\u578b\u7406\u89e3\uff0c\u5e76\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u768415\u4e2a\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u96f6\u6837\u672c\u63a8\u7406\u8868\u73b0\u5dee\uff0c\u5168\u5fae\u8c03\u63d0\u5347\u75be\u75c5\u5206\u7c7b\u6548\u679c\uff0c\u7ebf\u6027\u63a2\u6d4b\u80fd\u68c0\u6d4b\u91cd\u53e0\u7279\u5f81\u3002\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9700\u8c28\u614e\u8c03\u6574\u3002", "conclusion": "BiomedCLIP\u5728\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u4e2d\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u771f\u5b9e\u573a\u666f\uff0c\u5168\u5fae\u8c03\u548c\u7ebf\u6027\u63a2\u6d4b\u662f\u6709\u6548\u6539\u8fdb\u65b9\u5411\u3002", "paper_title_zh": "\u89e3\u8bfbBiomedCLIP\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff1a\u653e\u5c04\u5b66\u9886\u57df\u7684\u6d1e\u5bdf", "abstract_zh": "\u672c\u6587\u6784\u5efa\u4e86\u4e24\u4e2a\u7814\u7a76\u76ee\u6807\uff1ai) \u63a2\u7d22\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578bBiomedCLIP\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5206\u6790\u5176\u7c7b\u522b\u5206\u79bb\u6027\uff1bii) \u91cf\u5316BiomedCLIP\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u3001\u5206\u5e03\u5916\u591a\u6807\u7b7e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u57fa\u4e8eIU-xray\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86BiomedCLIP\u5728\u96f6\u6837\u672c\u63a8\u7406\u3001\u5168\u5fae\u8c03\u548c\u7ebf\u6027\u63a2\u6d4b\u4e09\u79cd\u573a\u666f\u4e0b\u7684\u5206\u7c7b\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u96f6\u6837\u672c\u63a8\u7406\u8868\u73b0\u4e0d\u4f73\uff0c\u5168\u5fae\u8c03\u63d0\u5347\u75be\u75c5\u5206\u7c7b\u6548\u679c\uff0c\u7ebf\u6027\u63a2\u6d4b\u80fd\u68c0\u6d4b\u91cd\u53e0\u7279\u5f81\u3002\u901a\u8fc7Grad-CAM\u70ed\u56fe\u53ef\u89c6\u5316\u6a21\u578b\u7406\u89e3\uff0c\u5e76\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684\u6807\u6ce8\u5bf9\u6bd4\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8c28\u614e\u8c03\u6574\u7684\u5fc5\u8981\u6027\u3002\u5b9e\u9a8c\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2506.13980", "pdf": "https://arxiv.org/pdf/2506.13980", "abs": "https://arxiv.org/abs/2506.13980", "authors": ["Shahaf David", "Yair Meidan", "Ido Hersko", "Daniel Varnovitzky", "Dudu Mimran", "Yuval Elovici", "Asaf Shabtai"], "title": "ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users", "categories": ["cs.AI"], "comment": null, "summary": "Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProfiLLM\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u9690\u5f0f\u548c\u52a8\u6001\u5730\u5206\u6790\u7528\u6237\u7279\u5f81\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u53ef\u9002\u5e94\u591a\u9886\u57df\u7684\u5206\u7c7b\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u7528\u6237\u5206\u6790\u65b9\u6cd5\u3002\u5728IT/\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cProfiLLM\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u63a8\u65ad\u7528\u6237\u6280\u672f\u719f\u7ec3\u5ea6\uff0c\u663e\u8457\u7f29\u5c0f\u9884\u6d4b\u4e0e\u5b9e\u9645\u8bc4\u5206\u7684\u5dee\u8ddd\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u8bddAI\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u804a\u5929\u673a\u5668\u4eba\u4ecd\u96be\u4ee5\u6839\u636e\u7528\u6237\u4e2a\u4f53\u7279\u5f81\uff08\u5982\u6280\u672f\u4e13\u957f\u3001\u5b66\u4e60\u98ce\u683c\u548c\u6c9f\u901a\u504f\u597d\uff09\u4e2a\u6027\u5316\u5176\u54cd\u5e94\u3002\u8fd9\u79cd\u4e2a\u6027\u5316\u4e0d\u8db3\u5728IT/\u7f51\u7edc\u5b89\u5168\u7b49\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u5c24\u4e3a\u7a81\u51fa\uff0c\u56e0\u4e3a\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u5dee\u5f02\u8f83\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u7528\u6237\u5206\u7c7b\u6216\u663e\u5f0f\u81ea\u6211\u62a5\u544a\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u7528\u6237\u80fd\u529b\u52a8\u6001\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ProfiLLM\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u53ef\u9002\u5e94\u591a\u9886\u57df\u7684\u5206\u7c7b\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u7528\u6237\u5206\u6790\u65b9\u6cd5\u3002\u901a\u8fc7\u5728IT/\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\uff0c\u5f00\u53d1\u4e86ProfiLLM[ITSec]\u53d8\u4f53\uff0c\u5e76\u5229\u75281,760\u6761\u6a21\u62df\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u6570\u636e\uff08\u6765\u81ea263\u4e2a\u5408\u6210\u7528\u6237\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProfiLLM[ITSec]\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u63a8\u65ad\u7528\u6237\u6280\u672f\u719f\u7ec3\u5ea6\uff0c\u4ec5\u9700\u4e00\u6b21\u63d0\u793a\u5373\u53ef\u5c06\u9884\u6d4b\u4e0e\u5b9e\u9645\u8bc4\u5206\u7684\u5dee\u8ddd\u7f29\u5c0f55-65%\uff0c\u540e\u7eed\u6ce2\u52a8\u8f83\u5c0f\u4e14\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "ProfiLLM\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u5f0f\u548c\u52a8\u6001\u7528\u6237\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u804a\u5929\u673a\u5668\u4eba\u7684\u4e2a\u6027\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8eLLM\u7684\u89d2\u8272\u6a21\u62df\u65b9\u6cd5\u3001ITSec\u719f\u7ec3\u5ea6\u5206\u7c7b\u6cd5\u3001\u4ee3\u7801\u5e93\u548c\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "paper_title_zh": "ProfiLLM\uff1a\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\u7528\u6237\u9690\u5f0f\u5206\u6790\u6846\u67b6", "abstract_zh": "\u5c3d\u7ba1\u5bf9\u8bddAI\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u804a\u5929\u673a\u5668\u4eba\u4ecd\u96be\u4ee5\u6839\u636e\u7528\u6237\u4e2a\u4f53\u7279\u5f81\uff08\u5982\u6280\u672f\u4e13\u957f\u3001\u5b66\u4e60\u98ce\u683c\u548c\u6c9f\u901a\u504f\u597d\uff09\u4e2a\u6027\u5316\u5176\u54cd\u5e94\u3002\u8fd9\u79cd\u4e2a\u6027\u5316\u4e0d\u8db3\u5728IT/\u7f51\u7edc\u5b89\u5168\uff08ITSec\uff09\u7b49\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u5c24\u4e3a\u7a81\u51fa\uff0c\u56e0\u4e3a\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u5dee\u5f02\u8f83\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u7528\u6237\u5206\u7c7b\u6216\u663e\u5f0f\u81ea\u6211\u62a5\u544a\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u7528\u6237\u80fd\u529b\u52a8\u6001\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002\u672c\u6587\u63d0\u51faProfiLLM\uff0c\u4e00\u79cd\u901a\u8fc7\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u9690\u5f0f\u548c\u52a8\u6001\u5206\u6790\u7528\u6237\u7279\u5f81\u7684\u65b0\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u53ef\u9002\u5e94\u591a\u9886\u57df\u7684\u5206\u7c7b\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u7528\u6237\u5206\u6790\u65b9\u6cd5\u3002\u4e3a\u9a8c\u8bc1ProfiLLM\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728ITSec\u9886\u57df\u5e94\u7528\u8be5\u6846\u67b6\uff0c\u5229\u7528\u6545\u969c\u6392\u9664\u4ea4\u4e92\u63a8\u65ad\u7528\u6237\u6280\u672f\u719f\u7ec3\u5ea6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5f00\u53d1\u4e86ProfiLLM[ITSec]\uff0c\u4e00\u79cd\u9002\u5e94ITSec\u7684\u53d8\u4f53\uff0c\u5e76\u5728263\u4e2a\u5408\u6210\u7528\u6237\u76841,760\u6761\u6a21\u62df\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u6570\u636e\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0cProfiLLM[ITSec]\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u63a8\u65adITSec\u7528\u6237\u7279\u5f81\uff0c\u4ec5\u9700\u4e00\u6b21\u63d0\u793a\u5373\u53ef\u5c06\u9884\u6d4b\u4e0e\u5b9e\u9645\u8bc4\u5206\u7684\u5dee\u8ddd\u7f29\u5c0f55-65%\uff0c\u540e\u7eed\u6ce2\u52a8\u8f83\u5c0f\u4e14\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u9664\u8bc4\u4f30\u65b0\u6846\u67b6\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8eLLM\u7684\u89d2\u8272\u6a21\u62df\u65b9\u6cd5\u3001ITSec\u719f\u7ec3\u5ea6\u5206\u7c7b\u6cd5\u3001\u4ee3\u7801\u5e93\u548c\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.14175", "pdf": "https://arxiv.org/pdf/2506.14175", "abs": "https://arxiv.org/abs/2506.14175", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u57fa\u7840\u5956\u52b1\u6a21\u578b\uff08GRAM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5229\u7528\u6807\u7b7e\u5e73\u6ed1\u4f18\u5316\u6392\u5e8f\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6709\u76d1\u7763\u5b66\u4e60\uff0c\u4ec5\u4f7f\u7528\u6807\u6ce8\u7684\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u9996\u5148\u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u6709\u76d1\u7763\u5b66\u4e60\u5fae\u8c03\u3002\u540c\u65f6\uff0c\u5229\u7528\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u4f18\u5316\u6b63\u5219\u5316\u6210\u5bf9\u6392\u5e8f\u635f\u5931\uff0c\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u5224\u522b\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u7edf\u4e00\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u54cd\u5e94\u6392\u5e8f\u3001\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u53ca\u4efb\u52a1\u9002\u5e94\u5fae\u8c03\u7b49\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u751f\u6210\u5f0f\u57fa\u7840\u5956\u52b1\u6a21\u578b\uff08GRAM\uff09\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "paper_title_zh": "GRAM\uff1a\u4e00\u79cd\u7528\u4e8e\u5956\u52b1\u6cdb\u5316\u7684\u751f\u6210\u5f0f\u57fa\u7840\u5956\u52b1\u6a21\u578b", "abstract_zh": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\u4e2d\uff0c\u5956\u52b1\u6a21\u578b\u626e\u6f14\u4e86\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u901a\u5e38\u4f5c\u4e3a\u5224\u522b\u6a21\u578b\u8bad\u7ec3\uff0c\u4ec5\u4f9d\u8d56\u6807\u6ce8\u7684\u4eba\u7c7b\u504f\u597d\u6570\u636e\u3002\u672c\u6587\u63a2\u7d22\u4e86\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u57fa\u4e8eLLM\u7684\u751f\u6210\u6a21\u578b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u9996\u5148\u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\uff0c\u518d\u901a\u8fc7\u6709\u76d1\u7763\u5b66\u4e60\u5fae\u8c03\u3002\u540c\u65f6\uff0c\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u6807\u7b7e\u5e73\u6ed1\u5b9e\u9645\u4e0a\u662f\u5728\u4f18\u5316\u6b63\u5219\u5316\u6210\u5bf9\u6392\u5e8f\u635f\u5931\u3002\u8fd9\u4e00\u7ed3\u679c\u4e3a\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u5224\u522b\u6a21\u578b\u7edf\u4e00\u5230\u540c\u4e00\u7c7b\u8bad\u7ec3\u76ee\u6807\u4e0b\u3002\u8fd9\u4e9b\u6280\u672f\u7684\u6210\u679c\u662f\u4e00\u4e2a\u57fa\u7840\u5956\u52b1\u6a21\u578b\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u51e0\u4e4e\u65e0\u9700\u8fdb\u4e00\u6b65\u5fae\u8c03\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u54cd\u5e94\u6392\u5e8f\u3001\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u53ca\u4efb\u52a1\u9002\u5e94\u5fae\u8c03\u7b49\u4efb\u52a1\u4e2d\u6cdb\u5316\u6027\u80fd\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2506.14142", "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "categories": ["cs.CV", "cs.CL"], "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.", "AI": {"tldr": "RadFabric\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u3001\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u5206\u6790\uff0c\u63d0\u5347\u80f8\u90e8X\u5149\uff08CXR\uff09\u8bca\u65ad\u7684\u5168\u9762\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7cfb\u7edf\u5728\u80f8\u90e8X\u5149\u8bca\u65ad\u4e2d\u5b58\u5728\u75c5\u7406\u8986\u76d6\u4e0d\u8db3\u3001\u8bca\u65ad\u51c6\u786e\u6027\u6709\u9650\u4ee5\u53ca\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u6574\u5408\u4e0d\u8db3\u7684\u95ee\u9898\uff0cRadFabric\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u9677\u3002", "method": "RadFabric\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\uff0c\u6574\u5408\u4e86\u75c5\u7406\u68c0\u6d4b\u667a\u80fd\u4f53\u3001\u89e3\u5256\u89e3\u91ca\u667a\u80fd\u4f53\u548c\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u504f\u597d\u9a71\u52a8\u63a8\u7406\u5b9e\u73b0\u900f\u660e\u4e14\u57fa\u4e8e\u8bc1\u636e\u7684\u8bca\u65ad\u3002", "result": "RadFabric\u5728\u6311\u6218\u6027\u75c5\u7406\uff08\u5982\u9aa8\u6298\uff09\u68c0\u6d4b\u4e2d\u8fbe\u52301.000\u7684\u51c6\u786e\u7387\uff0c\u603b\u4f53\u8bca\u65ad\u51c6\u786e\u7387\u4e3a0.799\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\uff080.229\u81f30.527\uff09\u3002", "conclusion": "RadFabric\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u504f\u597d\u9a71\u52a8\u63a8\u7406\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u653e\u5c04\u5b66\u5411\u900f\u660e\u3001\u89e3\u5256\u7cbe\u786e\u4e14\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684CXR\u5206\u6790\u53d1\u5c55\u3002", "paper_title_zh": "RadFabric\uff1a\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u667a\u80fdAI\u7cfb\u7edf\u7528\u4e8e\u653e\u5c04\u5b66", "abstract_zh": "\u80f8\u90e8X\u5149\uff08CXR\uff09\u6210\u50cf\u662f\u8bca\u65ad\u80f8\u90e8\u75be\u75c5\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u5f53\u524d\u81ea\u52a8\u5316\u7cfb\u7edf\u5728\u75c5\u7406\u8986\u76d6\u8303\u56f4\u3001\u8bca\u65ad\u51c6\u786e\u6027\u4ee5\u53ca\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u6574\u5408\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RadFabric\uff0c\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u3001\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u7edf\u4e00\u89c6\u89c9\u4e0e\u6587\u672c\u5206\u6790\u4ee5\u5b9e\u73b0\u5168\u9762\u7684CXR\u89e3\u8bfb\u3002RadFabric\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\uff0c\u5177\u5907\u6a21\u5757\u5316\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u65b0\u7684\u8bca\u65ad\u667a\u80fd\u4f53\u3002\u7cfb\u7edf\u5305\u62ec\u4e13\u95e8\u7684CXR\u75c5\u7406\u68c0\u6d4b\u667a\u80fd\u4f53\u3001\u89e3\u5256\u89e3\u91ca\u667a\u80fd\u4f53\uff08\u5c06\u89c6\u89c9\u53d1\u73b0\u6620\u5c04\u5230\u7cbe\u786e\u89e3\u5256\u7ed3\u6784\uff09\u4ee5\u53ca\u7531\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u9a71\u52a8\u7684\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u7efc\u5408\u89c6\u89c9\u3001\u89e3\u5256\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u751f\u6210\u900f\u660e\u4e14\u57fa\u4e8e\u8bc1\u636e\u7684\u8bca\u65ad\u3002RadFabric\u5728\u6311\u6218\u6027\u75c5\u7406\uff08\u5982\u9aa8\u6298\uff09\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff081.000\uff09\uff0c\u603b\u4f53\u8bca\u65ad\u51c6\u786e\u7387\uff080.799\uff09\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\uff080.229\u81f30.527\uff09\u3002\u901a\u8fc7\u6574\u5408\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u504f\u597d\u9a71\u52a8\u63a8\u7406\uff0cRadFabric\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u653e\u5c04\u5b66\u5411\u900f\u660e\u3001\u89e3\u5256\u7cbe\u786e\u4e14\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684CXR\u5206\u6790\u8fc8\u8fdb\u3002"}}
{"id": "2506.13983", "pdf": "https://arxiv.org/pdf/2506.13983", "abs": "https://arxiv.org/abs/2506.13983", "authors": ["Adarsh Gupta", "Bhabesh Mali", "Chandan Karfa"], "title": "SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine", "categories": ["cs.AI"], "comment": "Adarsh Gupta and Bhabesh Mali contributed equally to this work", "summary": "Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSANGAM\u6846\u67b6\uff0c\u5229\u7528LLM\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u81ea\u52a8\u751f\u6210SystemVerilog\u65ad\u8a00\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u5904\u7406\u89c4\u8303\u5e76\u751f\u6210\u7a33\u5065\u7684\u65ad\u8a00\u96c6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u9886\u57df\u7684\u8fdb\u5c55\u4e3a\u66f4\u590d\u6742\u548c\u81ea\u52a8\u5316\u7684\u786c\u4ef6\u65ad\u8a00\u751f\u6210\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6280\u672f\uff0c\u81ea\u52a8\u4ece\u5de5\u4e1a\u7ea7\u89c4\u8303\u4e2d\u751f\u6210SystemVerilog\u65ad\u8a00\uff08SVAs\uff09\uff0c\u4ee5\u63d0\u5347\u65ad\u8a00\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "SANGAM\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u591a\u6a21\u6001\u89c4\u8303\u5904\u7406\uff0c\u4f7f\u7528\u4fe1\u53f7\u6620\u5c04\u5668\u3001\u89c4\u8303\u5206\u6790\u5668\u548c\u6ce2\u5f62\u5206\u6790\u5668LLM\u4ee3\u7406\uff1b2\uff09\u8499\u7279\u5361\u6d1b\u6811\u81ea\u4f18\u5316\uff08MCTSr\uff09\u7b97\u6cd5\u5bf9\u6bcf\u4e2a\u4fe1\u53f7\u8fdb\u884c\u81ea\u52a8\u63a8\u7406\uff1b3\uff09\u7ed3\u5408MCTSr\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u4e3a\u6bcf\u4e2a\u4fe1\u53f7\u751f\u6210SVA\u65ad\u8a00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSANGAM\u80fd\u591f\u751f\u6210\u4e00\u7ec4\u7a33\u5065\u7684SVAs\uff0c\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SANGAM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5de5\u4e1a\u7ea7\u89c4\u8303\u4e2d\u81ea\u52a8\u751f\u6210SystemVerilog\u65ad\u8a00\uff0c\u4e3a\u786c\u4ef6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SANGAM\uff1a\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u81ea\u4f18\u5316\u7684SystemVerilog\u65ad\u8a00\u751f\u6210", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u9886\u57df\u7684\u8fdb\u5c55\u4e3a\u66f4\u590d\u6742\u548c\u81ea\u52a8\u5316\u7684\u786c\u4ef6\u65ad\u8a00\u751f\u6210\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86SANGAM\uff0c\u4e00\u79cd\u5229\u7528LLM\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4ece\u5de5\u4e1a\u7ea7\u89c4\u8303\u4e2d\u81ea\u52a8\u751f\u6210SystemVerilog\u65ad\u8a00\uff08SVAs\uff09\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4fe1\u53f7\u6620\u5c04\u5668\u3001\u89c4\u8303\u5206\u6790\u5668\u548c\u6ce2\u5f62\u5206\u6790\u5668LLM\u4ee3\u7406\u8fdb\u884c\u591a\u6a21\u6001\u89c4\u8303\u5904\u7406\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u81ea\u4f18\u5316\uff08MCTSr\uff09\u7b97\u6cd5\u5bf9\u6bcf\u4e2a\u4fe1\u53f7\u8fdb\u884c\u81ea\u52a8\u63a8\u7406\uff1b\u6700\u540e\uff0c\u7b2c\u4e09\u9636\u6bb5\u7ed3\u5408MCTSr\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u4e3a\u6bcf\u4e2a\u4fe1\u53f7\u751f\u6210SVA\u65ad\u8a00\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSANGAM\u80fd\u591f\u751f\u6210\u4e00\u7ec4\u7a33\u5065\u7684SVAs\uff0c\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.14177", "pdf": "https://arxiv.org/pdf/2506.14177", "abs": "https://arxiv.org/abs/2506.14177", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5408\u6210\u8bed\u7801\u8f6c\u6362\u6570\u636e\u8bad\u7ec3ASR\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u7ea7\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u5728\u4e09\u79cd\u4e1c\u5357\u4e9a\u8bed\u8a00\u5bf9\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u7801\u8f6c\u6362ASR\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8bed\u7801\u8f6c\u6362\u5728 multilingual \u73af\u5883\u4e2d\u5e38\u89c1\uff0c\u4f46\u8f6c\u5f55\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u5bfc\u81f4ASR\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u5408\u6210\u8bed\u7801\u8f6c\u6362\u6570\u636e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u7ea7\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u5408\u6210\u8bed\u7801\u8f6c\u6362\u6570\u636e\uff0c\u5e76\u5229\u7528\u5355\u8bed\u6570\u636e\u7ed3\u5408\u5408\u6210\u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3ASR\u6a21\u578b\uff08\u5982Whisper\u3001MMS\u3001SeamlessM4T\uff09\uff0c\u91cd\u70b9\u5173\u6ce8\u9a6c\u6765\u8bed-\u82f1\u8bed\u3001\u666e\u901a\u8bdd-\u9a6c\u6765\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed-\u82f1\u8bed\u4e09\u79cd\u8bed\u8a00\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86ASR\u5728\u5355\u8bed\u548c\u8bed\u7801\u8f6c\u6362\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u9a6c\u6765\u8bed-\u82f1\u8bed\u63d0\u5347\u6700\u5927\uff0c\u5176\u6b21\u662f\u6cf0\u7c73\u5c14\u8bed-\u82f1\u8bed\u548c\u666e\u901a\u8bdd-\u9a6c\u6765\u8bed\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bed\u7801\u8f6c\u6362ASR\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u5f00\u53d1\u65b9\u6cd5\uff0c\u5bf9\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "paper_title_zh": "\u80fd\u5426\u5728\u6ca1\u6709\u771f\u5b9e\u8bed\u7801\u8f6c\u6362\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3ASR\u7cfb\u7edf\uff1f\u4ee5\u65b0\u52a0\u5761\u8bed\u8a00\u4e3a\u4f8b\u7684\u7814\u7a76", "abstract_zh": "\u8bed\u7801\u8f6c\u6362\uff08CS\uff09\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5e38\u89c1\uff0c\u4f46\u7531\u4e8e\u8bed\u8a00\u590d\u6742\u6027\u5bfc\u81f4\u7684\u8f6c\u5f55\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u7ed9ASR\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5408\u6210CS\u6570\u636e\u6784\u5efaCS-ASR\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u7ea7\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u6a21\u62df\u81ea\u7136\u6a21\u5f0f\u7684\u5408\u6210CS\u6570\u636e\uff0c\u5e76\u5229\u7528\u5355\u8bed\u6570\u636e\u7ed3\u5408\u5408\u6210\u77ed\u8bed\u6df7\u5408CS\u6570\u636e\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3ASR\u6a21\u578b\uff08Whisper\u3001MMS\u3001SeamlessM4T\uff09\u3002\u672c\u6587\u91cd\u70b9\u5173\u6ce8\u4e09\u79cd\u8d44\u6e90\u532e\u4e4f\u7684\u4e1c\u5357\u4e9a\u8bed\u8a00\u5bf9\uff1a\u9a6c\u6765\u8bed-\u82f1\u8bed\uff08BM-EN\uff09\u3001\u666e\u901a\u8bdd-\u9a6c\u6765\u8bed\uff08ZH-BM\uff09\u548c\u6cf0\u7c73\u5c14\u8bed-\u82f1\u8bed\uff08TA-EN\uff09\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u7efc\u5408\u57fa\u51c6\u6765\u8bc4\u4f30\u9886\u5148ASR\u6a21\u578b\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u4e86ASR\u5728\u5355\u8bed\u548cCS\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u5176\u4e2dBM-EN\u63d0\u5347\u6700\u5927\uff0c\u5176\u6b21\u662fTA-EN\u548cZH-BM\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3aCS-ASR\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5bf9\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.14144", "pdf": "https://arxiv.org/pdf/2506.14144", "abs": "https://arxiv.org/abs/2506.14144", "authors": ["Juho Bai", "Inwook Shim"], "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.", "AI": {"tldr": "SceneAware\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u573a\u666f\u7406\u89e3\u548c\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u5229\u7528\u89c6\u89c9Transformer\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u901a\u884c\u6027\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u95f4\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u800c\u5ffd\u7565\u4e86\u73af\u5883\u80cc\u666f\u5bf9\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\u7684\u663e\u8457\u5f71\u54cd\u3002SceneAware\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u573a\u666f\u4fe1\u606f\uff0c\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u89c6\u89c9Transformer\u7f16\u7801\u9759\u6001\u573a\u666f\u56fe\u50cf\u7684\u73af\u5883\u80cc\u666f\uff1b2) \u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u901a\u884c\u6027\u63a9\u7801\uff1b3) \u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u8f68\u8ff9\u7f16\u7801\u5668\uff0c\u6355\u6349\u65f6\u7a7a\u52a8\u6001\u548c\u7a7a\u95f4\u7ea6\u675f\uff1b4) \u5f15\u5165\u78b0\u649e\u60e9\u7f5a\u673a\u5236\uff0c\u786e\u4fdd\u9884\u6d4b\u8f68\u8ff9\u7b26\u5408\u7269\u7406\u8fb9\u754c\u3002", "result": "\u5728ETH/UCY\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSceneAware\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc750%\u3002\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u884c\u4eba\u79fb\u52a8\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "SceneAware\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u573a\u666f\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u8bc1\u660e\u4e86\u573a\u666f\u611f\u77e5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "paper_title_zh": "SceneAware\uff1a\u57fa\u4e8eLLM\u5f15\u5bfc\u53ef\u901a\u884c\u6027\u7684\u573a\u666f\u7ea6\u675f\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b", "abstract_zh": "\u51c6\u786e\u9884\u6d4b\u884c\u4eba\u8f68\u8ff9\u5bf9\u673a\u5668\u4eba\u548c\u76d1\u63a7\u7cfb\u7edf\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u95f4\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u5374\u5ffd\u7565\u4e86\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\u7684\u73af\u5883\u80cc\u666f\u3002\u672c\u6587\u63d0\u51faSceneAware\uff0c\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u573a\u666f\u7406\u89e3\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u3002\u65b9\u6cd5\u5229\u7528\u89c6\u89c9Transformer\uff08ViT\uff09\u7f16\u7801\u9759\u6001\u573a\u666f\u56fe\u50cf\u7684\u73af\u5883\u80cc\u666f\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u751f\u6210\u533a\u5206\u53ef\u901a\u884c\u4e0e\u53d7\u9650\u533a\u57df\u7684\u4e8c\u503c\u63a9\u7801\u3002\u6211\u4eec\u5c06\u57fa\u4e8eTransformer\u7684\u8f68\u8ff9\u7f16\u7801\u5668\u4e0eViT\u573a\u666f\u7f16\u7801\u5668\u7ed3\u5408\uff0c\u6355\u6349\u65f6\u7a7a\u52a8\u6001\u548c\u7a7a\u95f4\u7ea6\u675f\u3002\u6846\u67b6\u8fd8\u6574\u5408\u4e86\u78b0\u649e\u60e9\u7f5a\u673a\u5236\uff0c\u907f\u514d\u9884\u6d4b\u8f68\u8ff9\u8fdd\u53cd\u7269\u7406\u8fb9\u754c\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u7269\u7406\u5408\u7406\u6027\u3002SceneAware\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u4e24\u79cd\u53d8\u4f53\u3002\u5728ETH/UCY\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc750%\u3002\u57fa\u4e8e\u4e0d\u540c\u8f68\u8ff9\u7c7b\u522b\u7684\u5206\u6790\u663e\u793a\uff0c\u6a21\u578b\u5728\u5404\u7c7b\u884c\u4eba\u79fb\u52a8\u4e2d\u8868\u73b0\u4e00\u81f4\u3002\u8fd9\u51f8\u663e\u4e86\u663e\u5f0f\u573a\u666f\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u6211\u4eec\u7684\u573a\u666f\u611f\u77e5\u65b9\u6cd5\u5728\u751f\u6210\u51c6\u786e\u4e14\u7269\u7406\u5408\u7406\u7684\u9884\u6d4b\u4e2d\u65e2\u6709\u6548\u53c8\u53ef\u9760\u3002\u4ee3\u7801\u89c1\uff1ahttps://github.com/juho127/SceneAware\u3002"}}
{"id": "2506.13990", "pdf": "https://arxiv.org/pdf/2506.13990", "abs": "https://arxiv.org/abs/2506.13990", "authors": ["Hamidou Tembine"], "title": "Machine Mirages: Defining the Undefined", "categories": ["cs.AI"], "comment": "Submitted", "summary": "As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u673a\u5668\u667a\u80fd\u7cfb\u7edf\u4e2d\u51fa\u73b0\u7684\u65b0\u578b\u8ba4\u77e5\u504f\u5dee\u2014\u2014\u673a\u5668\u5e7b\u8c61\uff0c\u5e76\u5f3a\u8c03\u9700\u660e\u786e\u5b9a\u4e49\u548c\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u9519\u8bef\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u667a\u80fd\u7684\u53ef\u9760\u6027\u5e76\u6784\u5efa\u4f26\u7406\u5171\u751f\u7684\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u673a\u5668\u667a\u80fd\u7cfb\u7edf\u5728\u56fe\u50cf\u3001\u8bed\u8a00\u548c\u58f0\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230\u52a8\u7269\u6216\u4eba\u7c7b\u6c34\u5e73\uff0c\u5b83\u4eec\u5f00\u59cb\u8868\u73b0\u51fa\u65b0\u7684\u8ba4\u77e5\u504f\u5dee\uff08\u673a\u5668\u5e7b\u8c61\uff09\u3002\u8fd9\u4e9b\u504f\u5dee\u4e0d\u4ec5\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u8fd8\u53ef\u80fd\u5bf9\u4f26\u7406\u548c\u793e\u4f1a\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u6587\u7ae0\u5217\u4e3e\u4e86\u673a\u5668\u5e7b\u8c61\u7684\u591a\u79cd\u8868\u73b0\u5f62\u5f0f\uff08\u5982\u5e7b\u89c9\u3001\u8bed\u4e49\u6f02\u79fb\u3001\u8fc7\u5ea6\u62df\u5408\u7b49\uff09\uff0c\u5e76\u4e3b\u5f20\u901a\u8fc7\u660e\u786e\u5b9a\u4e49\u548c\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u9519\u8bef\u6765\u7406\u89e3\u5176\u672c\u8d28\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u673a\u5668\u5e7b\u8c61\u662f\u4e00\u7c7b\u72ec\u7279\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u4e0e\u4eba\u7c7b\u6216\u52a8\u7269\u7684\u9519\u8bef\u4e0d\u540c\uff0c\u9700\u4e13\u95e8\u5206\u7c7b\u548c\u8bc4\u4f30\u3002", "conclusion": "\u7406\u89e3\u673a\u5668\u5e7b\u8c61\u5bf9\u63d0\u5347\u673a\u5668\u667a\u80fd\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4e5f\u6709\u52a9\u4e8e\u6784\u5efa\u4e00\u4e2a\u5c0a\u91cd\u751f\u547d\u548c\u8ba4\u77e5\u591a\u6837\u6027\u7684\u4f26\u7406\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u3002", "paper_title_zh": "\u673a\u5668\u5e7b\u8c61\uff1a\u5b9a\u4e49\u672a\u5b9a\u4e49\u4e4b\u7269", "abstract_zh": "\u968f\u7740\u591a\u6a21\u6001\u673a\u5668\u667a\u80fd\u7cfb\u7edf\u5728\u5904\u7406\u56fe\u50cf\u3001\u8bed\u8a00\u548c\u58f0\u97f3\u7b49\u4efb\u52a1\u4e2d\u8fbe\u5230\u52a8\u7269\u6216\u4eba\u7c7b\u6c34\u5e73\u7684\u6d41\u7545\u6027\uff0c\u5b83\u4eec\u5f00\u59cb\u8868\u73b0\u51fa\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u504f\u5dee\u7c7b\u522b\uff1a\u673a\u5668\u5e7b\u8c61\u3002\u8fd9\u4e9b\u5e7b\u8c61\u5305\u62ec\u5984\u60f3\u3001\u5e7b\u89c9\u3001\u865a\u6784\u3001\u8bed\u4e49\u6f02\u79fb\u3001\u8bed\u4e49\u538b\u7f29\u3001\u5938\u5f20\u3001\u56e0\u679c\u63a8\u7406\u5931\u8d25\u3001\u611f\u77e5\u7684\u6050\u6016\u8c37\u6548\u5e94\u3001\u865a\u5f20\u58f0\u52bf\u3001\u8ba4\u77e5\u523b\u677f\u3001\u8bed\u7528\u8bef\u89e3\u3001\u8d85\u7b26\u53f7\u5316\u3001\u8bed\u4e49\u590d\u70ed\u3001\u6a21\u62df\u6743\u5a01\u6548\u5e94\u3001\u8c2c\u8bef\u6eaf\u56e0\u8df3\u8dc3\u3001\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u6307\u79f0\u5e7b\u89c9\u3001\u7b26\u53f7\u5f17\u5170\u80af\u65af\u5766\u6548\u5e94\u3001\u6821\u51c6\u5931\u8d25\u3001\u4f2a\u76f8\u5173\u3001\u504f\u89c1\u653e\u5927\u3001\u6982\u5ff5\u6f02\u79fb\u654f\u611f\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8bef\u5206\u7c7b\u3001\u5bf9\u6297\u8106\u5f31\u6027\u3001\u8fc7\u5ea6\u62df\u5408\u3001\u97f5\u5f8b\u8bef\u5206\u7c7b\u3001\u53e3\u97f3\u504f\u89c1\u3001\u8f6e\u6b21\u8fb9\u754c\u5931\u8d25\u3001\u8bed\u4e49\u8fb9\u754c\u6df7\u6dc6\u3001\u566a\u58f0\u8fc7\u5ea6\u62df\u5408\u3001\u5ef6\u8fdf\u8bf1\u5bfc\u7684\u51b3\u7b56\u6f02\u79fb\u3001\u6a21\u7cca\u6027\u5d29\u6e83\u4ee5\u53ca\u5176\u4ed6\u6a21\u4eff\u4f46\u4e0d\u590d\u5236\u4eba\u7c7b\u6216\u52a8\u7269\u9519\u8bef\u7684\u9519\u8bef\u5f62\u5f0f\u3002\u672c\u6587\u5217\u4e3e\u4e86\u90e8\u5206\u9519\u8bef\uff0c\u5e76\u8ba4\u4e3a\u5fc5\u987b\u660e\u786e\u5b9a\u4e49\u548c\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u5931\u8d25\u3002\u7406\u89e3\u673a\u5668\u5e7b\u8c61\u4e0d\u4ec5\u5bf9\u63d0\u9ad8\u673a\u5668\u667a\u80fd\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u8fd8\u6709\u52a9\u4e8e\u6784\u5efa\u4e00\u4e2a\u5c0a\u91cd\u5176\u5fc5\u7136\u89e6\u53ca\u7684\u751f\u547d\u3001\u8ba4\u77e5\u548c\u8868\u8fbe\u591a\u6837\u6027\u7684\u591a\u5c3a\u5ea6\u4f26\u7406\u5171\u751f\u7684\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2506.14190", "pdf": "https://arxiv.org/pdf/2506.14190", "abs": "https://arxiv.org/abs/2506.14190", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This work has been submitted to the IEEE for possible publication. This paper is a preprint version submitted to the 2025 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2025)", "summary": "Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.", "AI": {"tldr": "AsyncSwitch\u662f\u4e00\u79cd\u5f02\u6b65\u6587\u672c-\u8bed\u97f3\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u9884\u8bad\u7ec3ASR\u6a21\u578b\uff0c\u518d\u5fae\u8c03\u8bed\u97f3-\u6587\u672c\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u4ee3\u7801\u5207\u6362ASR\u7684\u8bef\u7801\u7387\u3002", "motivation": "\u5f00\u53d1\u4ee3\u7801\u5207\u6362ASR\u7cfb\u7edf\u9762\u4e34\u8bed\u8a00\u6b67\u4e49\u548c\u591a\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u4f20\u7edf\u5408\u6210\u97f3\u9891\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002AsyncSwitch\u65e8\u5728\u901a\u8fc7\u6587\u672c\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u964d\u4f4e\u5bf9\u8bed\u97f3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "AsyncSwitch\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5728\u4ee3\u7801\u5207\u6362\u6587\u672c\u4e0a\u8bad\u7ec3\u89e3\u7801\u5668\u7684\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u5c42\uff1b2) \u4f7f\u7528\u5c11\u91cf\u8bed\u97f3-\u6587\u672c\u6570\u636e\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668\uff1b3) \u5bf9\u6574\u4e2a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u9a6c\u6765\u8bed-\u82f1\u8bed\u4ee3\u7801\u5207\u6362\u5b9e\u9a8c\u4e2d\uff0cAsyncSwitch\u4f7fWhisper\u6a21\u578b\u7684\u76f8\u5bf9\u8bcd\u9519\u8bef\u7387\u964d\u4f4e9.02%\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u65b0\u52a0\u5761\u82f1\u8bed\u3001\u9a6c\u6765\u8bed\u548c\u5176\u4ed6\u82f1\u8bed\u53d8\u4f53\u7684\u5355\u8bed\u6027\u80fd\u3002", "conclusion": "AsyncSwitch\u901a\u8fc7\u5f02\u6b65\u6587\u672c-\u8bed\u97f3\u9002\u5e94\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u5207\u6362ASR\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "paper_title_zh": "AsyncSwitch\uff1a\u9762\u5411\u4ee3\u7801\u5207\u6362ASR\u7684\u5f02\u6b65\u6587\u672c-\u8bed\u97f3\u9002\u5e94\u6846\u67b6", "abstract_zh": "\u5f00\u53d1\u4ee3\u7801\u5207\u6362ASR\u7cfb\u7edf\u56e0\u8bed\u8a00\u6b67\u4e49\u548c\u591a\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u6536\u96c6\u6b64\u7c7b\u8bed\u97f3\u6570\u636e\u6210\u672c\u9ad8\u6602\u3002\u5148\u524d\u5de5\u4f5c\u901a\u8fc7\u6587\u672c\u751f\u6210\u5408\u6210\u97f3\u9891\uff0c\u4f46\u8fd9\u7c7b\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6211\u4eec\u63d0\u51faAsyncSwitch\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u6b65\u9002\u5e94\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u6587\u672c\u4e30\u5bcc\u7684\u7f51\u7edc\u6570\u636e\uff0c\u5728\u5fae\u8c03\u8bed\u97f3-\u6587\u672c\u8bed\u6599\u5e93\u4e4b\u524d\uff0c\u9884\u8bad\u7ec3ASR\u6a21\u578b\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u4ee3\u7801\u5207\u6362\u9886\u57df\u3002\u6211\u4eec\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\u5305\u62ec\uff1a1) \u5728\u4ee3\u7801\u5207\u6362\u6587\u672c\u4e0a\u8bad\u7ec3\u89e3\u7801\u5668\u7684\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u5c42\uff1b2) \u4f7f\u7528\u5c11\u91cf\u8bed\u97f3-\u6587\u672c\u6570\u636e\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668\uff1b3) \u5bf9\u6574\u4e2a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u5728\u9a6c\u6765\u8bed-\u82f1\u8bed\u4ee3\u7801\u5207\u6362\u5b9e\u9a8c\u4e2d\uff0cAsyncSwitch\u4f7fWhisper\u6a21\u578b\u7684\u76f8\u5bf9\u8bcd\u9519\u8bef\u7387\u964d\u4f4e9.02%\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u65b0\u52a0\u5761\u82f1\u8bed\u3001\u9a6c\u6765\u8bed\u548c\u5176\u4ed6\u82f1\u8bed\u53d8\u4f53\u7684\u5355\u8bed\u6027\u80fd\u3002"}}
{"id": "2506.14168", "pdf": "https://arxiv.org/pdf/2506.14168", "abs": "https://arxiv.org/abs/2506.14168", "authors": ["Hu Yu", "Biao Gong", "Hangjie Yuan", "DanDan Zheng", "Weilong Chai", "Jingdong Chen", "Kecheng Zheng", "Feng Zhao"], "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to NeurIPS 2025", "summary": "Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVideoMAR\uff0c\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u4ee4\u724c\u7684\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u63a9\u7801\u751f\u6210\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63a9\u7801\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u89c6\u9891\u751f\u6210\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u8d44\u6e90\u8282\u7ea6\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002", "method": "VideoMAR\u7ed3\u5408\u4e86\u65f6\u95f4\u56e0\u679c\u6027\u548c\u7a7a\u95f4\u53cc\u5411\u6027\uff0c\u63d0\u51fa\u4e0b\u4e00\u5e27\u6269\u6563\u635f\u5931\u4ee5\u6574\u5408\u63a9\u7801\u4e0e\u89c6\u9891\u751f\u6210\u3002\u901a\u8fc7\u65f6\u95f4\u77ed\u5230\u957f\u8bfe\u7a0b\u5b66\u4e60\u548c\u7a7a\u95f4\u6e10\u8fdb\u5206\u8fa8\u7387\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002\u63a8\u7406\u65f6\u91c7\u7528\u6e10\u8fdb\u6e29\u5ea6\u7b56\u7565\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\u3002", "result": "\u5728VBench-I2V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideoMAR\u6027\u80fd\u8d85\u8d8a\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578b\uff08Cosmos I2V\uff09\uff0c\u4e14\u53c2\u6570\u3001\u8bad\u7ec3\u6570\u636e\u548cGPU\u8d44\u6e90\u6d88\u8017\u5206\u522b\u51cf\u5c11\u81f39.3%\u30010.5%\u548c0.2%\u3002", "conclusion": "VideoMAR\u4e0d\u4ec5\u9ad8\u6548\u4e14\u8d44\u6e90\u8282\u7ea6\uff0c\u8fd8\u901a\u8fc73D\u65cb\u8f6c\u5d4c\u5165\u5b9e\u73b0\u4e86\u65f6\u7a7a\u5916\u63a8\u80fd\u529b\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "VideoMAR\uff1a\u57fa\u4e8e\u8fde\u7eed\u4ee4\u724c\u7684\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210", "abstract_zh": "\u57fa\u4e8e\u63a9\u7801\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5c55\u793a\u4e86\u51fa\u8272\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u672c\u6587\u63d0\u51faVideoMAR\uff0c\u4e00\u79cd\u7b80\u6d01\u9ad8\u6548\u7684\u4ec5\u89e3\u7801\u5668\u81ea\u56de\u5f52\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u91c7\u7528\u8fde\u7eed\u4ee4\u724c\u5e76\u7ed3\u5408\u9010\u5e27\u65f6\u95f4\u548c\u7a7a\u95f4\u63a9\u7801\u751f\u6210\u3002\u6211\u4eec\u9996\u5148\u5c06\u65f6\u95f4\u56e0\u679c\u6027\u548c\u7a7a\u95f4\u53cc\u5411\u6027\u786e\u7acb\u4e3a\u89c6\u9891\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9996\u8981\u539f\u5219\uff0c\u5e76\u63d0\u51fa\u4e0b\u4e00\u5e27\u6269\u6563\u635f\u5931\u4ee5\u6574\u5408\u63a9\u7801\u4e0e\u89c6\u9891\u751f\u6210\u3002\u6b64\u5916\uff0c\u957f\u5e8f\u5217\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u9ad8\u6210\u672c\u548c\u96be\u5ea6\u662f\u4e00\u4e2a\u57fa\u7840\u4f46\u5173\u952e\u7684\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u95f4\u77ed\u5230\u957f\u8bfe\u7a0b\u5b66\u4e60\u548c\u7a7a\u95f4\u6e10\u8fdb\u5206\u8fa8\u7387\u8bad\u7ec3\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u91c7\u7528\u6e10\u8fdb\u6e29\u5ea6\u7b56\u7565\u4ee5\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\u3002\u6b64\u5916\uff0cVideoMAR\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u72ec\u7279\u80fd\u529b\u590d\u5236\u5230\u89c6\u9891\u751f\u6210\u4e2d\u3002\u7531\u4e8e\u540c\u65f6\u652f\u6301\u65f6\u95f4\u7ef4\u5ea6\u7684KV\u7f13\u5b58\u548c\u7a7a\u95f4\u7ef4\u5ea6\u7684\u5e76\u884c\u751f\u6210\uff0c\u5176\u6548\u7387\u6781\u9ad8\uff0c\u5e76\u901a\u8fc73D\u65cb\u8f6c\u5d4c\u5165\u5c55\u793a\u4e86\u65f6\u7a7a\u5916\u63a8\u80fd\u529b\u3002\u5728VBench-I2V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideoMAR\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578b\uff08Cosmos I2V\uff09\uff0c\u540c\u65f6\u6240\u9700\u53c2\u6570\uff089.3%\uff09\u3001\u8bad\u7ec3\u6570\u636e\uff080.5%\uff09\u548cGPU\u8d44\u6e90\uff080.2%\uff09\u663e\u8457\u51cf\u5c11\u3002"}}
{"id": "2506.14045", "pdf": "https://arxiv.org/pdf/2506.14045", "abs": "https://arxiv.org/abs/2506.14045", "authors": ["Martin Klissarov", "Akhil Bagaria", "Ziyan Luo", "George Konidaris", "Doina Precup", "Marlos C. Machado"], "title": "Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u5728\u53d1\u73b0\u548c\u5229\u7528\u65f6\u95f4\u7ed3\u6784\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u4f18\u52bf\u3001\u65b9\u6cd5\u5bb6\u65cf\u53ca\u9002\u7528\u9886\u57df\uff0c\u5e76\u6307\u51fa\u4e86\u76f8\u5173\u6311\u6218\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u63a2\u7d22\u3001\u89c4\u5212\u548c\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u662f\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u91cd\u5927\u6311\u6218\u3002\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u901a\u8fc7\u53d1\u73b0\u548c\u5229\u7528\u7ecf\u9a8c\u6d41\u4e2d\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6848\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5982\u4f55\u5b9a\u4e49\u201c\u826f\u597d\u7ed3\u6784\u201d\u6216\u54ea\u4e9b\u95ee\u9898\u9002\u5408\u8bc6\u522b\u8fd9\u79cd\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u4ece\u51b3\u7b56\u57fa\u672c\u6311\u6218\u7684\u89d2\u5ea6\u8bc6\u522bHRL\u7684\u4f18\u52bf\uff0c\u5e76\u5f3a\u8c03\u5176\u5bf9AI\u667a\u80fd\u4f53\u6027\u80fd\u6743\u8861\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u4eceHRL\u7684\u57fa\u672c\u6311\u6218\u51fa\u53d1\uff0c\u63a2\u8ba8\u4e86\u53d1\u73b0\u65f6\u95f4\u7ed3\u6784\u7684\u65b9\u6cd5\u5bb6\u65cf\uff0c\u5305\u62ec\u4ece\u5728\u7ebf\u7ecf\u9a8c\u5b66\u4e60\u3001\u79bb\u7ebf\u6570\u636e\u96c6\u5b66\u4e60\uff0c\u4ee5\u53ca\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b49\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5206\u6790HRL\u7684\u4f18\u52bf\u548c\u65b9\u6cd5\uff0c\u672c\u6587\u603b\u7ed3\u4e86\u5176\u5728\u6027\u80fd\u63d0\u5347\u548c\u9002\u7528\u9886\u57df\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u65f6\u95f4\u7ed3\u6784\u53d1\u73b0\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "HRL\u5728\u53d1\u73b0\u65f6\u95f4\u7ed3\u6784\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u5176\u5b9a\u4e49\u548c\u9002\u7528\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u5173\u6ce8\u76f8\u5173\u6311\u6218\u53ca\u9002\u5408HRL\u7684\u9886\u57df\u3002", "paper_title_zh": "\u53d1\u73b0\u65f6\u95f4\u7ed3\u6784\uff1a\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7efc\u8ff0", "abstract_zh": "\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u63a2\u7d22\u3001\u89c4\u5212\u548c\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u662f\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u91cd\u5927\u6311\u6218\u3002\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u901a\u8fc7\u53d1\u73b0\u548c\u5229\u7528\u7ecf\u9a8c\u6d41\u4e2d\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6848\u3002HRL\u6846\u67b6\u7684\u5f3a\u70c8\u5438\u5f15\u529b\u50ac\u751f\u4e86\u4e30\u5bcc\u591a\u6837\u7684\u6587\u732e\uff0c\u8bd5\u56fe\u53d1\u73b0\u6709\u7528\u7684\u7ed3\u6784\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5982\u4f55\u5b9a\u4e49\u201c\u826f\u597d\u7ed3\u6784\u201d\uff0c\u6216\u8005\u54ea\u4e9b\u95ee\u9898\u9002\u5408\u8bc6\u522b\u8fd9\u79cd\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u4ece\u51b3\u7b56\u57fa\u672c\u6311\u6218\u7684\u89d2\u5ea6\u8bc6\u522bHRL\u7684\u4f18\u52bf\uff0c\u5e76\u5f3a\u8c03\u5176\u5bf9AI\u667a\u80fd\u4f53\u6027\u80fd\u6743\u8861\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8fd9\u4e9b\u4f18\u52bf\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u6db5\u76d6\u4e86\u53d1\u73b0HRL\u4e2d\u65f6\u95f4\u7ed3\u6784\u7684\u65b9\u6cd5\u5bb6\u65cf\uff0c\u5305\u62ec\u4ece\u5728\u7ebf\u7ecf\u9a8c\u5b66\u4e60\u3001\u79bb\u7ebf\u6570\u636e\u96c6\u5b66\u4e60\uff0c\u4ee5\u53ca\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b49\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u65f6\u95f4\u7ed3\u6784\u53d1\u73b0\u7684\u6311\u6218\u53ca\u7279\u522b\u9002\u5408\u6b64\u7c7b\u52aa\u529b\u7684\u9886\u57df\u3002"}}
{"id": "2506.14199", "pdf": "https://arxiv.org/pdf/2506.14199", "abs": "https://arxiv.org/abs/2506.14199", "authors": ["Junghwan Kim", "Kieun Park", "Sohee Park", "Hyunggug Kim", "Bongwon Suh"], "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment", "categories": ["cs.CL"], "comment": "4 Pages, 2 tables, EMNLP submitted", "summary": "Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.", "AI": {"tldr": "\u63d0\u51faMAS-LitEval\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5b66\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff0c\u80fd\u66f4\u7cbe\u51c6\u6355\u6349\u6587\u5b66\u7ffb\u8bd1\u4e2d\u7684\u6587\u5316\u7ec6\u8282\u548c\u98ce\u683c\u3002", "motivation": "\u4f20\u7edf\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u548cMETEOR\uff09\u8fc7\u4e8e\u5173\u6ce8\u8bcd\u6c47\u91cd\u53e0\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6587\u5b66\u7ffb\u8bd1\u4e2d\u7684\u6587\u5316\u7ec6\u8282\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMAS-LitEval\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u672f\u8bed\u3001\u53d9\u4e8b\u548c\u98ce\u683c\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u5728\u300a\u5c0f\u738b\u5b50\u300b\u548c\u300a\u4e9a\u745f\u738b\u671d\u5ef7\u4e0a\u7684\u5eb7\u6d85\u72c4\u683c\u5317\u65b9\u4f6c\u300b\u7684\u7ffb\u8bd1\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "MAS-LitEval\u5728\u6355\u6349\u6587\u5b66\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u5f97\u5206\u8fbe0.890\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "MAS-LitEval\u4e3a\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ec6\u81f4\u7684\u6846\u67b6\uff0c\u4e3a\u7ffb\u8bd1\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "MAS-LitEval\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6587\u5b66\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30", "abstract_zh": "\u6587\u5b66\u7ffb\u8bd1\u9700\u8981\u4fdd\u7559\u6587\u5316\u7ec6\u8282\u548c\u98ce\u683c\u5143\u7d20\uff0c\u800c\u4f20\u7edf\u6307\u6807\uff08\u5982BLEU\u548cMETEOR\uff09\u56e0\u8fc7\u4e8e\u5173\u6ce8\u8bcd\u6c47\u91cd\u53e0\u800c\u65e0\u6cd5\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u9762\uff0c\u5ffd\u89c6\u4e86\u53d9\u4e8b\u4e00\u81f4\u6027\u548c\u98ce\u683c\u5fe0\u5b9e\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faMAS-LitEval\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ffb\u8bd1\u8bc4\u4f30\u5de5\u5177\uff0c\u4ece\u672f\u8bed\u3001\u53d9\u4e8b\u548c\u98ce\u683c\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002\u6211\u4eec\u5728\u300a\u5c0f\u738b\u5b50\u300b\u548c\u300a\u4e9a\u745f\u738b\u671d\u5ef7\u4e0a\u7684\u5eb7\u6d85\u72c4\u683c\u5317\u65b9\u4f6c\u300b\u7684\u7ffb\u8bd1\u4e0a\u6d4b\u8bd5\u4e86MAS-LitEval\uff0c\u5e76\u4e0e\u4f20\u7edf\u6307\u6807\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u7ed3\u679c\u8868\u660e\uff0cMAS-LitEval\u5728\u6355\u6349\u6587\u5b66\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u5f97\u5206\u8fbe0.890\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7ec6\u81f4\u7684\u6846\u67b6\uff0c\u4e3a\u7ffb\u8bd1\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.14170", "pdf": "https://arxiv.org/pdf/2506.14170", "abs": "https://arxiv.org/abs/2506.14170", "authors": ["Shulong Zhang", "Mingyuan Yao", "Jiayin Zhao", "Xiao Liu", "Haihua Wang"], "title": "A multi-stage augmented multimodal interaction network for fish feeding intensity quantification", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u589e\u5f3a\u591a\u6a21\u6001\u4ea4\u4e92\u7f51\u7edc\uff08MAINet\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u5faa\u73af\u6c34\u517b\u6b96\u7cfb\u7edf\u4e2d\uff0c\u51c6\u786e\u8bc4\u4f30\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u5bf9\u964d\u4f4e\u9972\u6599\u6210\u672c\u548c\u4f18\u5316\u6295\u5582\u65f6\u95f4\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5728\u591a\u6a21\u6001\u9009\u62e9\u3001\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u53ca\u534f\u540c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51faMAINet\u6846\u67b6\uff0c\u5305\u62ec\u901a\u7528\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3001\u8f85\u52a9\u6a21\u6001\u589e\u5f3a\u4e3b\u6a21\u6001\u673a\u5236\uff08ARPM\uff0c\u542b\u901a\u9053\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edcCAFN\u548c\u53cc\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edcDAFN\uff09\u4ee5\u53ca\u8bc1\u636e\u63a8\u7406\uff08ER\uff09\u89c4\u5219\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u4e0e\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAINet\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8fbe\u523096.7%\u4ee5\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u3001\u53cc\u6a21\u6001\u53ca\u5176\u4ed6\u878d\u5408\u65b9\u6cd5\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6539\u8fdb\u7b56\u7565\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u548c\u7279\u5f81\u5229\u7528\u6548\u7387\u7684\u63d0\u5347\u4f5c\u7528\u3002", "conclusion": "MAINet\u901a\u8fc7\u591a\u6a21\u6001\u4ea4\u4e92\u4e0e\u589e\u5f3a\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u91cf\u5316\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u517b\u6b96\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002", "paper_title_zh": "\u4e00\u79cd\u7528\u4e8e\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u91cf\u5316\u7684\u591a\u9636\u6bb5\u589e\u5f3a\u591a\u6a21\u6001\u4ea4\u4e92\u7f51\u7edc", "abstract_zh": "\u5728\u5faa\u73af\u6c34\u517b\u6b96\u7cfb\u7edf\u4e2d\uff0c\u51c6\u786e\u6709\u6548\u5730\u8bc4\u4f30\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u5bf9\u964d\u4f4e\u9972\u6599\u6210\u672c\u548c\u8ba1\u7b97\u6700\u4f73\u6295\u5582\u65f6\u95f4\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7814\u7a76\u5728\u6a21\u6001\u9009\u62e9\u3001\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u4ee5\u53ca\u534f\u540c\u51b3\u7b56\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u4e0a\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u589e\u5f3a\u591a\u6a21\u6001\u4ea4\u4e92\u7f51\u7edc\uff08MAINet\uff09\u7528\u4e8e\u91cf\u5316\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u9ad8\u6548\u5730\u4ece\u8f93\u5165\u56fe\u50cf\u3001\u97f3\u9891\u548c\u6c34\u6ce2\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u8bbe\u8ba1\u4e86\u8f85\u52a9\u6a21\u6001\u589e\u5f3a\u4e3b\u6a21\u6001\u673a\u5236\uff08ARPM\uff09\uff0c\u7528\u4e8e\u6a21\u6001\u95f4\u4ea4\u4e92\u5e76\u751f\u6210\u589e\u5f3a\u7279\u5f81\uff0c\u8be5\u673a\u5236\u5305\u542b\u901a\u9053\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\uff08CAFN\uff09\u548c\u53cc\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\uff08DAFN\uff09\u3002\u6700\u540e\uff0c\u5f15\u5165\u8bc1\u636e\u63a8\u7406\uff08ER\uff09\u89c4\u5219\uff0c\u878d\u5408\u5404\u6a21\u6001\u8f93\u51fa\u7ed3\u679c\u5e76\u505a\u51fa\u51b3\u7b56\uff0c\u4ece\u800c\u5b8c\u6210\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u7684\u91cf\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6784\u5efa\u7684MAINet\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u8fbe\u523096.76%\u300196.78%\u300196.79%\u548c96.79%\uff0c\u5176\u6027\u80fd\u663e\u8457\u9ad8\u4e8e\u5bf9\u6bd4\u6a21\u578b\u3002\u4e0e\u91c7\u7528\u5355\u6a21\u6001\u3001\u53cc\u6a21\u6001\u878d\u5408\u53ca\u4e0d\u540c\u51b3\u7b56\u878d\u5408\u65b9\u6cd5\u7684\u6a21\u578b\u76f8\u6bd4\uff0cMAINet\u4e5f\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002\u540c\u65f6\uff0c\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0\u6539\u8fdb\u7b56\u7565\u5728\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u7279\u5f81\u5229\u7528\u6548\u7387\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u91cf\u5316\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.14070", "pdf": "https://arxiv.org/pdf/2506.14070", "abs": "https://arxiv.org/abs/2506.14070", "authors": ["Xinglei Wang", "Tao Cheng", "Stephen Law", "Zichao Zeng", "Ilya Ilyankou", "Junyuan Liu", "Lu Yin", "Weiming Huang", "Natchapon Jongwiriyanurak"], "title": "Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaLLiPer\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u5750\u6807\u548c\u8bed\u4e49\u7279\u5f81\u751f\u6210\u4f4d\u7f6e\u5d4c\u5165\uff0c\u7528\u4e8e\u9884\u6d4b\u4e2a\u4f53\u79fb\u52a8\u6027\uff0c\u5c24\u5176\u5728\u672a\u77e5\u5730\u70b9\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u79fb\u52a8\u6027\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5386\u53f2\u79fb\u52a8\u6a21\u5f0f\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u96be\u4ee5\u7f16\u7801\u663e\u5f0f\u7a7a\u95f4\u4fe1\u606f\u3001\u6574\u5408\u4e30\u5bcc\u7684\u57ce\u5e02\u8bed\u4e49\u4e0a\u4e0b\u6587\u6216\u9002\u5e94\u672a\u77e5\u5730\u70b9\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63a2\u7d22\u66f4\u5f3a\u5927\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "CaLLiPer\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u878d\u5408\u7a7a\u95f4\u5750\u6807\u548c\u5174\u8da3\u70b9\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u751f\u6210\u7a7a\u95f4\u663e\u5f0f\u3001\u8bed\u4e49\u4e30\u5bcc\u4e14\u5177\u6709\u5f52\u7eb3\u80fd\u529b\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4ece\u800c\u652f\u6301\u4e2a\u4f53\u79fb\u52a8\u6027\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u79fb\u52a8\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaLLiPer\u5728\u5e38\u89c4\u548c\u5f52\u7eb3\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u672a\u77e5\u5730\u70b9\u9884\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u591a\u6a21\u6001\u5f52\u7eb3\u4f4d\u7f6e\u5d4c\u5165\u80fd\u591f\u663e\u8457\u63d0\u5347\u4eba\u7c7b\u79fb\u52a8\u6027\u9884\u6d4b\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u63a2\u7d22\u672a\u77e5\uff1a\u5e94\u7528\u5f52\u7eb3\u6027\u7a7a\u95f4\u8bed\u4e49\u4f4d\u7f6e\u5d4c\u5165\u9884\u6d4b\u4e2a\u4f53\u672a\u8bbf\u95ee\u5730\u70b9\u7684\u79fb\u52a8\u6027", "abstract_zh": "\u9884\u6d4b\u4e2a\u4f53\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u662f\u4eba\u7c7b\u79fb\u52a8\u6027\u5efa\u6a21\u7684\u6838\u5fc3\u4efb\u52a1\uff0c\u5bf9\u57ce\u5e02\u89c4\u5212\u3001\u4ea4\u901a\u3001\u516c\u5171\u653f\u7b56\u548c\u4e2a\u6027\u5316\u79fb\u52a8\u670d\u52a1\u5177\u6709\u5e7f\u6cdb\u5f71\u54cd\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4ece\u5386\u53f2\u79fb\u52a8\u6a21\u5f0f\u5b66\u4e60\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u9650\u5236\u4e86\u5176\u7f16\u7801\u663e\u5f0f\u7a7a\u95f4\u4fe1\u606f\u3001\u6574\u5408\u4e30\u5bcc\u57ce\u5e02\u8bed\u4e49\u4e0a\u4e0b\u6587\u4ee5\u53ca\u9002\u5e94\u672a\u77e5\u5730\u70b9\u7684\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63a2\u7d22\u4e86CaLLiPer\u2014\u2014\u4e00\u79cd\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u878d\u5408\u7a7a\u95f4\u5750\u6807\u548c\u5174\u8da3\u70b9\u8bed\u4e49\u7279\u5f81\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\u2014\u2014\u5728\u4e2a\u4f53\u79fb\u52a8\u6027\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002CaLLiPer\u7684\u5d4c\u5165\u8bbe\u8ba1\u5177\u6709\u7a7a\u95f4\u663e\u5f0f\u6027\u3001\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u5f52\u7eb3\u6027\uff0c\u5373\u4f7f\u5728\u6d89\u53ca\u65b0\u5174\u5730\u70b9\u7684\u573a\u666f\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u9884\u6d4b\u6027\u80fd\u3002\u901a\u8fc7\u5728\u56db\u4e2a\u516c\u5171\u79fb\u52a8\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86CaLLiPer\u5728\u5e38\u89c4\u548c\u5f52\u7eb3\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u8868\u73b0\u5353\u8d8a\u3002\u6211\u4eec\u7684\u53d1\u73b0\u51f8\u663e\u4e86\u591a\u6a21\u6001\u5f52\u7eb3\u4f4d\u7f6e\u5d4c\u5165\u5728\u63d0\u5347\u4eba\u7c7b\u79fb\u52a8\u6027\u9884\u6d4b\u7cfb\u7edf\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6570\u636e\uff08https://github.com/xlwang233/Into-the-Unknown\uff09\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.14200", "pdf": "https://arxiv.org/pdf/2506.14200", "abs": "https://arxiv.org/abs/2506.14200", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "categories": ["cs.CL", "cs.HC"], "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an \"educator\" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ELI-Why\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u6559\u5b66\u4e2d\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u53d1\u73b0GPT-4\u751f\u6210\u7684\u89e3\u91ca\u5728\u9002\u5e94\u4e0d\u540c\u6559\u80b2\u80cc\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u4e3a50%\uff0c\u800c\u4eba\u5de5\u89e3\u91ca\u8fbe\u523079%\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u9488\u5bf9\u4e0d\u540c\u5b66\u4e60\u9700\u6c42\u548c\u77e5\u8bc6\u80cc\u666f\u7684\u4e2a\u6027\u5316\u89e3\u91ca\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6559\u5b66\u89e3\u91ca\u7684\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u5f15\u5165ELI-Why\u57fa\u51c6\uff0c\u5305\u542b13.4K\u4e2a\u201c\u4e3a\u4ec0\u4e48\u201d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e24\u9879\u4eba\u7c7b\u7814\u7a76\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u5bf9\u4e0d\u540c\u6559\u80b2\u9636\u6bb5\uff08\u5c0f\u5b66\u3001\u9ad8\u4e2d\u3001\u7814\u7a76\u751f\uff09\u7684\u9002\u5e94\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cGPT-4\u751f\u6210\u7684\u89e3\u91ca\u4ec550%\u7b26\u5408\u76ee\u6807\u6559\u80b2\u80cc\u666f\uff0c\u800c\u4eba\u5de5\u89e3\u91ca\u4e3a79%\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u8005\u8ba4\u4e3aGPT-4\u89e3\u91ca\u6bd4\u4eba\u5de5\u89e3\u91ca\u5e73\u5747\u4f4e20%\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6559\u5b66\u89e3\u91ca\u5728\u9002\u5e94\u4e0d\u540c\u6559\u80b2\u80cc\u666f\u65f6\u8868\u73b0\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u9ad8\u6559\u5b66\u6548\u679c\u3002", "paper_title_zh": "ELI-Why\uff1a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u7684\u6559\u5b66\u5b9e\u7528\u6027", "abstract_zh": "\u5f53\u4eca\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u9488\u5bf9\u4e0d\u540c\u5b66\u4e60\u9700\u6c42\u548c\u77e5\u8bc6\u80cc\u666f\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165ELI-Why\u57fa\u51c6\uff0c\u5305\u542b13.4K\u4e2a\u201c\u4e3a\u4ec0\u4e48\u201d\u95ee\u9898\uff0c\u4ee5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u6559\u5b66\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4e24\u9879\u4eba\u7c7b\u7814\u7a76\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u6027\u7b54\u6848\uff08\u89e3\u91ca\uff09\u5728\u5c0f\u5b66\u3001\u9ad8\u4e2d\u548c\u7814\u7a76\u751f\u4e09\u4e2a\u4e0d\u540c\u6559\u80b2\u9636\u6bb5\u7684\u5b9e\u7528\u6027\u3002\u5728\u7b2c\u4e00\u9879\u7814\u7a76\u4e2d\uff0c\u4eba\u7c7b\u8bc4\u5206\u8005\u626e\u6f14\u201c\u6559\u80b2\u8005\u201d\u89d2\u8272\uff0c\u8bc4\u4f30\u6a21\u578b\u89e3\u91ca\u5bf9\u4e0d\u540c\u6559\u80b2\u9636\u6bb5\u7684\u9002\u5e94\u6027\u3002\u6211\u4eec\u53d1\u73b0\uff0cGPT-4\u751f\u6210\u7684\u89e3\u91ca\u4ec550%\u7b26\u5408\u76ee\u6807\u6559\u80b2\u80cc\u666f\uff0c\u800c\u4eba\u5de5\u89e3\u91ca\u4e3a79%\u3002\u5728\u7b2c\u4e8c\u9879\u7814\u7a76\u4e2d\uff0c\u4eba\u7c7b\u8bc4\u5206\u8005\u626e\u6f14\u201c\u5b66\u4e60\u8005\u201d\u89d2\u8272\uff0c\u8bc4\u4f30\u89e3\u91ca\u662f\u5426\u7b26\u5408\u5176\u5b66\u4e60\u9700\u6c42\u3002\u5728\u6240\u6709\u6559\u80b2\u80cc\u666f\u4e2d\uff0c\u7528\u6237\u8ba4\u4e3aGPT-4\u751f\u6210\u7684\u89e3\u91ca\u6bd4\u4eba\u5de5\u89e3\u91ca\u5e73\u5747\u4f4e20%\u7684\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u663e\u793a\uff0c\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u751f\u6210\u7684\u89e3\u91ca\u5728\u9002\u5e94\u4e0d\u540c\u5b66\u4e60\u9700\u6c42\u65f6\uff0c\u5176\u6559\u80b2\u6c34\u5e73\u533a\u5206\u5ea6\u6709\u9650\uff0c\u5f71\u54cd\u4e86\u6559\u5b66\u6548\u679c\u3002"}}
{"id": "2506.14176", "pdf": "https://arxiv.org/pdf/2506.14176", "abs": "https://arxiv.org/abs/2506.14176", "authors": ["Renao Yan"], "title": "One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based pathological image analysis presents unique challenges due to the practical constraints of network design. Most existing methods apply computer vision models directly to medical tasks, neglecting the distinct characteristics of pathological images. This mismatch often leads to computational inefficiencies, particularly in edge-computing scenarios. To address this, we propose a novel Network Similarity Directed Initialization (NSDI) strategy to improve the stability of neural architecture search (NAS). Furthermore, we introduce domain adaptation into one-shot NAS to better handle variations in staining and semantic scale across pathology datasets. Experiments on the BRACS dataset demonstrate that our method outperforms existing approaches, delivering both superior classification performance and clinically relevant feature localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u76f8\u4f3c\u6027\u5f15\u5bfc\u521d\u59cb\u5316\uff08NSDI\uff09\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u7684\u4e00\u952e\u5f0f\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\uff0c\u7528\u4e8e\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u548c\u7279\u5f81\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5e94\u7528\u4e8e\u533b\u5b66\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u75c5\u7406\u56fe\u50cf\u7684\u72ec\u7279\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u7f51\u7edc\u76f8\u4f3c\u6027\u5f15\u5bfc\u521d\u59cb\u5316\uff08NSDI\uff09\u7b56\u7565\uff0c\u589e\u5f3a\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u7a33\u5b9a\u6027\uff1b\u5f15\u5165\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\uff0c\u5904\u7406\u75c5\u7406\u6570\u636e\u96c6\u4e2d\u67d3\u8272\u548c\u8bed\u4e49\u5c3a\u5ea6\u7684\u53d8\u5316\u3002", "result": "\u5728BRACS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u7c7b\u6027\u80fd\u548c\u7279\u5f81\u5b9a\u4f4d\u80fd\u529b\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u4e3a\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002", "paper_title_zh": "\u57fa\u4e8e\u7f51\u7edc\u76f8\u4f3c\u6027\u5f15\u5bfc\u521d\u59cb\u5316\u7684\u4e00\u952e\u5f0f\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u5728\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7531\u4e8e\u7f51\u7edc\u8bbe\u8ba1\u7684\u5b9e\u9645\u9650\u5236\u800c\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5e94\u7528\u4e8e\u533b\u5b66\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u75c5\u7406\u56fe\u50cf\u7684\u72ec\u7279\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\u76f8\u4f3c\u6027\u5f15\u5bfc\u521d\u59cb\u5316\uff08NSDI\uff09\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7684\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u4e00\u952e\u5f0fNAS\u4e2d\u5f15\u5165\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u75c5\u7406\u6570\u636e\u96c6\u4e2d\u67d3\u8272\u548c\u8bed\u4e49\u5c3a\u5ea6\u7684\u53d8\u5316\u3002\u5728BRACS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u5206\u7c7b\u6027\u80fd\u66f4\u4f18\uff0c\u8fd8\u80fd\u5b9e\u73b0\u4e34\u5e8a\u76f8\u5173\u7684\u7279\u5f81\u5b9a\u4f4d\u3002"}}
{"id": "2506.14079", "pdf": "https://arxiv.org/pdf/2506.14079", "abs": "https://arxiv.org/abs/2506.14079", "authors": ["Matthew Toles", "Rattandeep Singh", "Isaac Song Zhou Yu"], "title": "FormGym: Doing Paperwork with Agents", "categories": ["cs.AI"], "comment": null, "summary": "Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFormGym\u7684\u65b0\u578b\u8868\u5355\u586b\u5199\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b55\u4efd\u6587\u6863\u4e2d\u7684432\u4e2a\u5b57\u6bb5\u548c3\u9879\u4efb\u52a1\uff0c\u6d4b\u8bd5\u8ba1\u7b97\u673a\u4ee3\u7406\u7684\u591a\u6a21\u6001\u7406\u89e3\u3001\u4fe1\u606f\u68c0\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u7ebf\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff08VLA\uff09\u51c6\u786e\u7387\u4f4e\u4e8e1%\uff0c\u800cGUI\u4ee3\u7406\u8868\u73b0\u7a0d\u597d\u4f46\u6210\u672c\u9ad8\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86FieldFinder\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u586b\u5199\u8868\u5355\u662f\u4e00\u9879\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u7eaf\u56fe\u50cf\u9886\u57df\u65e0\u6cd5\u4f7f\u7528OCR\u6216PDF\u6587\u672c\u7684\u60c5\u51b5\u4e0b\u3002\u8ba1\u7b97\u673a\u4ee3\u7406\u9700\u8981\u5177\u5907\u591a\u6a21\u6001\u7406\u89e3\u3001\u4fe1\u606f\u68c0\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u4ee3\u7406\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b55\u4efd\u6587\u6863\u3001432\u4e2a\u5b57\u6bb5\u548c3\u9879\u4efb\u52a1\u7684\u8868\u5355\u586b\u5199\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u4ee3\u7406\u638c\u63e1236\u4e2a\u7528\u6237\u7279\u5f81\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86FieldFinder\u5de5\u5177\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b9a\u4f4d\u8868\u5355\u4e2d\u7684\u6587\u672c\u4f4d\u7f6e\u3002", "result": "\u57fa\u7ebf\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff08VLA\uff09\u51c6\u786e\u7387\u4f4e\u4e8e1%\uff0cGUI\u4ee3\u7406\u8868\u73b0\u7a0d\u597d\uff0810.6-68.0%\uff09\uff0c\u4f46\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\u3002\u4f7f\u7528FieldFinder\u540e\uff0c\u6240\u6709\u6a21\u578b\u5728\u6240\u6709\u516d\u79cd\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u8868\u73b0\u5747\u6709\u6240\u63d0\u5347\uff0c\u6700\u9ad8\u4ece2%\u63d0\u5347\u81f356%\u3002", "conclusion": "FormGym\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7406\u5728\u8868\u5355\u586b\u5199\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5b9a\u4f4d\u80fd\u529b\u4e0d\u8db3\u3002FieldFinder\u5de5\u5177\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "FormGym\uff1a\u7528\u4ee3\u7406\u5904\u7406\u6587\u4e66\u5de5\u4f5c", "abstract_zh": "\u586b\u5199\u8868\u5355\u662f\u4e00\u9879\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u7eaf\u56fe\u50cf\u9886\u57df\u65e0\u6cd5\u4f7f\u7528OCR\u3001PDF\u6587\u672c\u6216DOM\u7684\u60c5\u51b5\u4e0b\u3002\u8ba1\u7b97\u673a\u4ee3\u7406\u9700\u8981\u5177\u5907\u591a\u6a21\u6001\u7406\u89e3\u3001\u4fe1\u606f\u68c0\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u578b\u8868\u5355\u586b\u5199\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b55\u4efd\u6587\u6863\u4e2d\u7684432\u4e2a\u5b57\u6bb5\u548c3\u9879\u4efb\u52a1\uff0c\u8981\u6c42\u4ee3\u7406\u638c\u63e1236\u4e2a\u7528\u6237\u7279\u5f81\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u7ebf\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff08VLA\uff09\u51c6\u786e\u7387\u4f4e\u4e8e1%\uff0c\u4e3b\u8981\u7531\u4e8e\u5b9a\u4f4d\u80fd\u529b\u4e0d\u8db3\u3002GUI\u4ee3\u7406\u8868\u73b0\u7a0d\u597d\uff0810.6-68.0%\uff09\uff0c\u4f46\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86FieldFinder\u5de5\u5177\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b9a\u4f4d\u8868\u5355\u4e2d\u7684\u6587\u672c\u4f4d\u7f6e\u3002\u4f7f\u7528FieldFinder\u540e\uff0c\u6240\u6709\u6a21\u578b\u5728\u6240\u6709\u516d\u79cd\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u8868\u73b0\u5747\u6709\u6240\u63d0\u5347\uff0c\u6700\u9ad8\u4ece2%\u63d0\u5347\u81f356%\u3002"}}
{"id": "2506.14203", "pdf": "https://arxiv.org/pdf/2506.14203", "abs": "https://arxiv.org/abs/2506.14203", "authors": ["Jongho Kim", "Romain Stora\u00ef", "Seung-won Hwang"], "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation", "categories": ["cs.CL"], "comment": "EMNLP 2024 Findings (long)", "summary": "In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5e2e\u52a9\u547d\u540d\u969c\u788d\u60a3\u8005\u8bc6\u522b\u76ee\u6807\u7269\u54c1\u540d\u79f0\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u68af\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u65b9\u6cd5\u89e3\u51b3\u4e86\u8bed\u4e49\u9519\u8bef\u548c\u672a\u89c1\u672f\u8bed\u7684\u6311\u6218\uff0c\u5e76\u5728\u771f\u5b9e\u60a3\u8005\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u547d\u540d\u969c\u788d\u60a3\u8005\u5728\u8bc6\u522b\u7269\u54c1\u540d\u79f0\u65f6\u9762\u4e34\u672f\u8bed\u7f3a\u5931\u548c\u8bed\u4e49\u9519\u8bef\u7684\u53cc\u91cd\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u548c\u68af\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u60a3\u8005\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u7684\u65b9\u6cd5\uff1a\u68af\u5ea6\u503c\u63a7\u5236\u8bed\u4e49\u9519\u8bef\u4e0b\u7684\u589e\u5f3a\u6570\u636e\u8d28\u91cf\uff0c\u68af\u5ea6\u65b9\u5dee\u5f15\u5bfc\u5305\u542b\u672a\u89c1\u4f46\u76f8\u5173\u7684\u672f\u8bed\u3002\u5728Tip-of-the-Tongue\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5e94\u7528\u4e8eAphasiaBank\u7684\u771f\u5b9e\u60a3\u8005\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u57fa\u7ebf\u5bf9\u6bd4\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u9519\u8bef\u548c\u672f\u8bed\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u547d\u540d\u969c\u788d\u60a3\u8005\u7684\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u68af\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u65b9\u6cd5\u5728\u89e3\u51b3\u547d\u540d\u969c\u788d\u60a3\u8005\u7684\u8bc6\u522b\u95ee\u9898\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u68af\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u7684\u547d\u540d\u969c\u788d\u60a3\u8005\u76ee\u6807\u8bc6\u522b", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5e2e\u52a9\u547d\u540d\u969c\u788d\u60a3\u8005\u8bc6\u522b\u7269\u54c1\u540d\u79f0\u4e2d\u7684\u6f5c\u529b\u3002\u8bc6\u522b\u60a3\u8005\u8fc2\u56de\u63cf\u8ff0\u4e2d\u7684\u76ee\u6807\u7269\u54c1\u6d89\u53ca\u672f\u8bed\u7f3a\u5931\u548c\u8bed\u4e49\u9519\u8bef\u4e24\u5927\u6311\u6218\uff1a\uff081\uff09\u4e0e\u76ee\u6807\u7269\u54c1\u76f8\u5173\u7684\u672f\u8bed\u672a\u88ab\u89c2\u5bdf\u5230\uff1b\uff082\uff09\u8bed\u4e49\u6027\u9519\u8bed\u5bfc\u81f4\u7684\u5e72\u6270\u672f\u8bed\u4e0e\u76ee\u6807\u7269\u54c1\u65e0\u5173\uff0c\u963b\u788d\u8bc6\u522b\u8fc7\u7a0b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u68af\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u5bf9\u8bed\u4e49\u9519\u8bef\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u672a\u89c1\u672f\u8bed\u3002\u5177\u4f53\u800c\u8a00\uff0c\u68af\u5ea6\u503c\u63a7\u5236\u8bed\u4e49\u9519\u8bef\u4e0b\u7684\u589e\u5f3a\u6570\u636e\u8d28\u91cf\uff0c\u68af\u5ea6\u65b9\u5dee\u6307\u5bfc\u5305\u542b\u672a\u89c1\u4f46\u76f8\u5173\u7684\u672f\u8bed\u3002\u7531\u4e8e\u9886\u57df\u7279\u5b9a\u6570\u636e\u6709\u9650\uff0c\u6211\u4eec\u5728Tip-of-the-Tongue\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u4f5c\u4e3a\u4e2d\u95f4\u4efb\u52a1\uff0c\u968f\u540e\u5c06\u7ed3\u679c\u5e94\u7528\u4e8eAphasiaBank\u7684\u771f\u5b9e\u60a3\u8005\u6570\u636e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u57fa\u7ebf\u5bf9\u6bd4\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u547d\u540d\u969c\u788d\u60a3\u8005\u7684\u8bc6\u522b\u6311\u6218\u3002"}}
{"id": "2506.14181", "pdf": "https://arxiv.org/pdf/2506.14181", "abs": "https://arxiv.org/abs/2506.14181", "authors": ["Yufei Li", "Jirui Wu", "Long Tian", "Liming Wang", "Xiaonan Liu", "Zijun Liu", "Xiyang Liu"], "title": "Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition", "categories": ["cs.CV"], "comment": "15 pages, 5 figures", "summary": "Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.", "AI": {"tldr": "Meta-SurDiff\u662f\u4e00\u79cd\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u7684\u5206\u7c7b\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u89c6\u9891\u5728\u7ebf\u9636\u6bb5\u8bc6\u522b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5305\u62ec\u89c6\u9891\u5e27\u6a21\u7cca\u548c\u624b\u672f\u9636\u6bb5\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u56e0\u5176\u4e0e\u4eba\u7c7b\u751f\u547d\u5065\u5eb7\u76f8\u5173\u7684\u6f5c\u5728\u5e94\u7528\u800c\u5907\u53d7\u5173\u6ce8\u3002\u5c3d\u7ba1\u6df1\u5ea6\u6a21\u578b\u5728\u6355\u6349\u624b\u672f\u89c6\u9891\u7684\u957f\u671f\u4f9d\u8d56\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5f88\u5c11\u8003\u8651\u89c6\u9891\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u5bf9\u53ef\u9760\u7684\u5728\u7ebf\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faMeta-SurDiff\u6a21\u578b\uff0c\u7ed3\u5408\u5206\u7c7b\u6269\u6563\u6a21\u578b\u548c\u5143\u5b66\u4e60\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8bc4\u4f30\u6a21\u7cca\u89c6\u9891\u5e27\u7684\u8bc6\u522b\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u4ee5\u589e\u5f3a\u4e0d\u540c\u624b\u672f\u9636\u6bb5\u7684\u5206\u7c7b\u8fb9\u754c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff08Cholec80\u3001AutoLaparo\u3001M2Cai16\u3001OphNet\u548cNurViD\uff09\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u4f7f\u7528\u56db\u4e2a\u4ee5\u4e0a\u5b9e\u7528\u6307\u6807\u9a8c\u8bc1\u4e86Meta-SurDiff\u7684\u6709\u6548\u6027\u3002", "conclusion": "Meta-SurDiff\u901a\u8fc7\u5efa\u6a21\u624b\u672f\u89c6\u9891\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "paper_title_zh": "Meta-SurDiff\uff1a\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u7684\u5206\u7c7b\u6269\u6563\u6a21\u578b\u5728\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u7684\u53ef\u9760\u6027\u7814\u7a76", "abstract_zh": "\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u56e0\u5176\u4e0e\u4eba\u7c7b\u751f\u547d\u5065\u5eb7\u76f8\u5173\u7684\u6f5c\u5728\u5e94\u7528\u800c\u5907\u53d7\u5173\u6ce8\u3002\u5c3d\u7ba1\u6df1\u5ea6\u6a21\u578b\u5728\u6355\u6349\u624b\u672f\u89c6\u9891\u7684\u957f\u671f\u4f9d\u8d56\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f88\u5c11\u63a2\u7d22\u548c\u5efa\u6a21\u89c6\u9891\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u8fd9\u5bf9\u4e8e\u53ef\u9760\u7684\u5728\u7ebf\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u5c06\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u5206\u4e3a\u4e24\u7c7b\uff1a\u89c6\u9891\u5e27\u6a21\u7cca\u548c\u624b\u672f\u9636\u6bb5\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u624b\u672f\u89c6\u9891\u4e2d\u4e0d\u53ef\u907f\u514d\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5b66\u4e60\u4f18\u5316\u7684\u5206\u7c7b\u6269\u6563\u6a21\u578b\uff08Meta-SurDiff\uff09\uff0c\u5145\u5206\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u548c\u5143\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5e27\u7ea7\u5206\u5e03\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u53ef\u9760\u6027\u3002\u9488\u5bf9\u6a21\u7cca\u89c6\u9891\u5e27\u5bfc\u81f4\u7684\u8bc6\u522b\u7c97\u7cd9\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u5206\u7c7b\u6269\u6563\u6a21\u578b\u5728\u66f4\u7ec6\u7c92\u5ea6\u7684\u5e27\u7ea7\u5b9e\u4f8b\u4e0a\u8bc4\u4f30\u8bc6\u522b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\uff1b\u9488\u5bf9\u624b\u672f\u9636\u6bb5\u5206\u5e03\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u8bc6\u522b\u7c97\u7cd9\u95ee\u9898\uff0c\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u76ee\u6807\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u4ece\u800c\u589e\u5f3a\u4e0d\u540c\u624b\u672f\u9636\u6bb5\u5206\u7c7b\u8fb9\u754c\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff08Cholec80\u3001AutoLaparo\u3001M2Cai16\u3001OphNet\u548cNurViD\uff09\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528\u56db\u4e2a\u4ee5\u4e0a\u5b9e\u7528\u6307\u6807\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86Meta-SurDiff\u5728\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002\u5176\u4e2d\uff0cOphNet\u6765\u81ea\u773c\u79d1\u624b\u672f\uff0cNurViD\u662f\u65e5\u5e38\u62a4\u7406\u6570\u636e\u96c6\uff0c\u5176\u4f59\u6570\u636e\u96c6\u6765\u81ea\u8179\u8154\u955c\u624b\u672f\u3002\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2506.14084", "pdf": "https://arxiv.org/pdf/2506.14084", "abs": "https://arxiv.org/abs/2506.14084", "authors": ["Taehee Jeong"], "title": "Lightweight Relevance Grader in RAG", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u76f8\u5173\u6027\u8bc4\u5206\u5668\uff0c\u7528\u4e8e\u63d0\u5347RAG\u7cfb\u7edf\u4e2d\u68c0\u7d22\u6587\u6863\u7684\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "RAG\u7cfb\u7edf\u901a\u8fc7\u5411\u91cf\u6570\u636e\u5e93\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u51c6\u786e\u6027\uff0c\u4f46\u68c0\u7d22\u6587\u6863\u7684\u76f8\u5173\u6027\u9a8c\u8bc1\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u76f8\u5173\u6027\u8bc4\u5206\u5668\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u4f5c\u8005\u5fae\u8c03\u4e86\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578bllama-3.2-1b\u4f5c\u4e3a\u76f8\u5173\u6027\u8bc4\u5206\u5668\uff0c\u7528\u4e8e\u9a8c\u8bc1\u68c0\u7d22\u6587\u6863\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u8bc4\u5206\u5668\u7684\u7cbe\u5ea6\u4ece0.1301\u63d0\u5347\u81f30.7750\uff0c\u4e0e\u5927\u578b\u6a21\u578bllama-3.1-70b\u7684\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u76f8\u5173\u6027\u8bc4\u5206\u5668\u5728RAG\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e2\u80fd\u4fdd\u8bc1\u7cbe\u5ea6\uff0c\u53c8\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "paper_title_zh": "RAG\u4e2d\u7684\u8f7b\u91cf\u7ea7\u76f8\u5173\u6027\u8bc4\u5206\u5668", "abstract_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5229\u7528\u5411\u91cf\u6570\u636e\u5e93\u5f25\u8865\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u6700\u65b0\u7684\u4fe1\u606f\u3002\u5f53\u7528\u6237\u63d0\u4ea4\u67e5\u8be2\u65f6\uff0cRAG\u6267\u884c\u5411\u91cf\u641c\u7d22\u4ee5\u627e\u5230\u76f8\u5173\u6587\u6863\uff0c\u5e76\u7528\u4e8e\u751f\u6210\u54cd\u5e94\u3002\u7136\u800c\uff0c\u786e\u4fdd\u68c0\u7d22\u6587\u6863\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u53ef\u4ee5\u5f15\u5165\u4e00\u4e2a\u79f0\u4e3a\u76f8\u5173\u6027\u8bc4\u5206\u5668\u7684\u8f85\u52a9\u6a21\u578b\u6765\u9a8c\u8bc1\u5176\u76f8\u5173\u6027\u3002\u4e3a\u4e86\u51cf\u5c11\u76f8\u5173\u6027\u8bc4\u5206\u5668\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u8f7b\u91cf\u7ea7\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u4e3a\u5408\u9002\u3002\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5fae\u8c03\u4e86llama-3.2-1b\u4f5c\u4e3a\u76f8\u5173\u6027\u8bc4\u5206\u5668\uff0c\u5e76\u5c06\u5176\u7cbe\u5ea6\u4ece0.1301\u663e\u8457\u63d0\u5347\u81f30.7750\uff0c\u5176\u7cbe\u5ea6\u4e0ellama-3.1-70b\u76f8\u5f53\u3002\u4ee3\u7801\u53ef\u5728https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG\u83b7\u53d6\u3002"}}
{"id": "2506.14205", "pdf": "https://arxiv.org/pdf/2506.14205", "abs": "https://arxiv.org/abs/2506.14205", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "categories": ["cs.CL"], "comment": null, "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth", "AI": {"tldr": "AgentSynth\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u751f\u6210\u7ba1\u9053\uff0c\u7528\u4e8e\u4e3a\u901a\u7528\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u4efb\u52a1\u548c\u8f68\u8ff9\u6570\u636e\u96c6\u3002\u901a\u8fc7\u4fe1\u606f\u4e0d\u5bf9\u79f0\u8bbe\u8ba1\u5b50\u4efb\u52a1\uff0c\u7ec4\u5408\u6210\u957f\u65f6\u4efb\u52a1\u540e\u66f4\u5177\u6311\u6218\u6027\uff0c\u751f\u6210\u4e866,000\u591a\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u9876\u7ea7LLM\u4ee3\u7406\u5728\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u751f\u6210\u6210\u672c\u4ec5\u4e3a\u6bcf\u8f68\u8ff90.60\u7f8e\u5143\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u4efb\u52a1\u751f\u6210\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u4f4e\u6210\u672c\u4e14\u80fd\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "AgentSynth\u91c7\u7528\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u63d0\u8bae\u5668\u751f\u6210\u5b50\u4efb\u52a1\uff0c\u7531\u6267\u884c\u4ee3\u7406\u5b8c\u6210\u5e76\u8bb0\u5f55\u8f68\u8ff9\uff0c\u8fed\u4ee3\u7ec4\u5408\u6210\u590d\u5408\u4efb\u52a1\u3002\u901a\u8fc7\u8c03\u6574\u5b50\u4efb\u52a1\u6570\u91cf\u7cbe\u786e\u63a7\u5236\u4efb\u52a1\u96be\u5ea6\u3002", "result": "\u751f\u6210\u4e866,000\u591a\u4e2a\u591a\u6837\u5316\u4efb\u52a1\uff0c\u9876\u7ea7LLM\u4ee3\u7406\u5728\u96be\u5ea61\u7684\u6210\u529f\u7387\u4e3a18%\uff0c\u96be\u5ea66\u65f6\u964d\u81f34%\u3002\u6bcf\u8f68\u8ff9\u5e73\u5747\u6210\u672c\u4ec5\u4e3a0.60\u7f8e\u5143\u3002", "conclusion": "AgentSynth\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u4efb\u52a1\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7684\u4efb\u52a1\u5904\u7406\u80fd\u529b\u8bc4\u4f30\uff0c\u4e14\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002", "paper_title_zh": "AgentSynth\uff1a\u901a\u7528\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u4efb\u52a1\u751f\u6210\u65b9\u6cd5", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86AgentSynth\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u4e3a\u901a\u7528\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5408\u6210\u9ad8\u8d28\u91cf\u4efb\u52a1\u548c\u8f68\u8ff9\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5229\u7528\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0cAgentSynth\u751f\u6210\u7684\u5b50\u4efb\u52a1\u5728\u7ec4\u5408\u6210\u957f\u65f6\u4efb\u52a1\u65f6\u66f4\u5177\u6311\u6218\u6027\uff0c\u4ece\u800c\u521b\u5efa\u4e866,000\u591a\u4e2a\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u4efb\u52a1\u3002\u6211\u4eec\u7684\u7ba1\u9053\u9996\u5148\u7531\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u63d0\u8bae\u5668\u6839\u636e\u89d2\u8272\u751f\u6210\u4efb\u52a1\uff0c\u7136\u540e\u7531\u6267\u884c\u4ee3\u7406\u5b8c\u6210\u4efb\u52a1\u5e76\u8bb0\u5f55\u8f68\u8ff9\u3002\u6b64\u8fc7\u7a0b\u8fed\u4ee3\u8fdb\u884c\u4ee5\u5f62\u6210\u5b50\u4efb\u52a1\u5e8f\u5217\uff0c\u6700\u7ec8\u7531\u53e6\u4e00\u4ee3\u7406\u6c47\u603b\u4e3a\u96be\u5ea6\u53ef\u63a7\u7684\u590d\u5408\u4efb\u52a1\u3002AgentSynth\u7684\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u80fd\u591f\u901a\u8fc7\u8c03\u6574\u5b50\u4efb\u52a1\u6570\u91cf\u7cbe\u786e\u8c03\u63a7\u4efb\u52a1\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9876\u7ea7LLM\u4ee3\u7406\u7684\u6027\u80fd\u968f\u96be\u5ea6\u589e\u52a0\u663e\u8457\u4e0b\u964d\uff0c\u4ece\u96be\u5ea61\u768418%\u6210\u529f\u7387\u964d\u81f3\u96be\u5ea66\u76844%\uff0c\u7a81\u663e\u4e86\u57fa\u51c6\u7684\u96be\u5ea6\u548c\u533a\u5206\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7ba1\u9053\u6bcf\u8f68\u8ff9\u5e73\u5747\u6210\u672c\u4ec5\u4e3a0.60\u7f8e\u5143\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u4e8ehttps://github.com/sunblaze-ucb/AgentSynth\u3002"}}
{"id": "2506.14189", "pdf": "https://arxiv.org/pdf/2506.14189", "abs": "https://arxiv.org/abs/2506.14189", "authors": ["Kunyuan Deng", "Yi Wang", "Lap-Pui Chau"], "title": "Egocentric Human-Object Interaction Detection: A New Benchmark and Method", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u57fa\u51c6\uff08Ego-HOIBench\uff09\u53ca\u65b9\u6cd5\uff08HGIR\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u624b\u90e8\u51e0\u4f55\u548c\u4ea4\u4e92\u6027\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff0c\u5ffd\u89c6\u4e86\u66f4\u76f4\u89c2\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff08Ego-HOI\uff09\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8Ego-HOI\u68c0\u6d4b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684Hand Geometry and Interactivity Refinement\uff08HGIR\uff09\u65b9\u6848\uff0c\u5229\u7528\u624b\u90e8\u59ff\u6001\u548c\u51e0\u4f55\u4fe1\u606f\u4f18\u5316\u4ea4\u4e92\u7279\u5f81\uff0c\u901a\u8fc7\u59ff\u6001-\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "result": "HGIR\u65b9\u6848\u5728Ego-HOIBench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u8868\u793a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Ego-HOIBench\u548cHGIR\u65b9\u6848\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u9ad8\u6548\u7684\u7279\u70b9\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8eHOI\u68c0\u6d4b\u4efb\u52a1\u3002", "paper_title_zh": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\uff1a\u65b0\u57fa\u51c6\u4e0e\u65b9\u6cd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u7406\u89e3\u4eba\u4e0e\u7269\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u73b0\u6709\u7684\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff0c\u5ffd\u89c6\u4e86\u66f4\u76f4\u89c2\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff08Ego-HOI\uff09\u3002\u672c\u6587\u63d0\u51fa\u4e86Ego-HOIBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8Ego-HOI\u68c0\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u53d1\u5c55\u3002\u6211\u4eec\u7684Ego-HOIBench\u5305\u542b\u8d85\u8fc727K\u5f20\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u56fe\u50cf\uff0c\u5e26\u6709\u9ad8\u8d28\u91cf\u7684\u624b-\u52a8\u8bcd-\u7269\u4f53\u4e09\u5143\u7ec4\u6807\u6ce8\uff0c\u6db5\u76d6123\u4e2a\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u7c7b\u522b\u548c\u4f4d\u7f6e\uff0c\u8986\u76d6\u4e86\u65e5\u5e38\u6d3b\u52a8\u4e2d\u4e30\u5bcc\u7684\u573a\u666f\u3001\u7269\u4f53\u7c7b\u578b\u548c\u624b\u90e8\u914d\u7f6e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u5e76\u8c03\u6574\u4e86\u7b2c\u4e09\u4eba\u79f0HOI\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u9002\u5e94Ego-HOIBench\uff0c\u5e76\u5c55\u793a\u4e86\u624b\u90e8\u906e\u6321\u7269\u4f53\u4ee5\u53ca\u5355\u53cc\u624b\u4ea4\u4e92\u590d\u6742\u6027\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5efa\u7acb\u65b0\u7684\u57fa\u51c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u624b\u90e8\u51e0\u4f55\u4e0e\u4ea4\u4e92\u6027\u4f18\u5316\uff08HGIR\uff09\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5229\u7528\u624b\u90e8\u59ff\u6001\u548c\u51e0\u4f55\u4fe1\u606f\u4f5c\u4e3a\u89e3\u91ca\u4ea4\u4e92\u7684\u6709\u4ef7\u503c\u7ebf\u7d22\u3002\u5177\u4f53\u800c\u8a00\uff0cHGIR\u65b9\u6848\u4ece\u4f30\u8ba1\u7684\u624b\u90e8\u59ff\u6001\u63d0\u6848\u4e2d\u663e\u5f0f\u63d0\u53d6\u5168\u5c40\u624b\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u59ff\u6001-\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u4ea4\u4e92\u7279\u5b9a\u7279\u5f81\u3002\u8fd9\u4e00\u65b9\u6848\u4f7f\u6a21\u578b\u80fd\u591f\u83b7\u5f97\u9c81\u68d2\u4e14\u5f3a\u5927\u7684\u4ea4\u4e92\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86Ego-HOI\u68c0\u6d4b\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8f7b\u91cf\u4e14\u9ad8\u6548\uff0c\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u5730\u5e94\u7528\u4e8eHOI\u57fa\u51c6\uff0c\u5728Ego-HOIBench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u9879\u76ee\u5730\u5740\uff1ahttps://dengkunyuan.github.io/EgoHOIBench/"}}
{"id": "2506.14092", "pdf": "https://arxiv.org/pdf/2506.14092", "abs": "https://arxiv.org/abs/2506.14092", "authors": ["Haonan Yin", "Shai Vardi", "Vidyanand Choudhary"], "title": "Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u4f4d\u7f6e\u987a\u5e8f\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u5305\u62ec\u65b0\u9896\u7684\u4e2d\u5fc3\u6027\u504f\u89c1\u5728\u5185\u7684\u591a\u79cd\u504f\u89c1\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62db\u8058\u3001\u5927\u5b66\u5f55\u53d6\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u88ab\u5e7f\u6cdb\u7528\u4e8e\u51b3\u7b56\u652f\u6301\uff0c\u4f46\u6b64\u524d\u5bf9\u5176\u4f4d\u7f6e\u987a\u5e8f\u504f\u89c1\u7684\u7814\u7a76\u4e0d\u591f\u7cfb\u7edf\uff0c\u4e5f\u672a\u6df1\u5165\u5206\u6790\u5176\u4e0e\u504f\u597d\u7ed3\u6784\u7684\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u591a\u79cdLLM\u67b6\u6784\u548c\u9886\u57df\u7684\u4f4d\u7f6e\u504f\u89c1\uff0c\u53d1\u73b0\u987a\u5e8f\u6548\u5e94\u5e76\u63d0\u51fa\u5206\u7c7b\u6846\u67b6\uff08\u7a33\u5065\u3001\u8106\u5f31\u6216\u4e2d\u7acb\u504f\u597d\uff09\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86\u6e29\u5ea6\u53c2\u6570\u4f5c\u4e3a\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u5b58\u5728\u5f3a\u70c8\u7684\u987a\u5e8f\u6548\u5e94\uff0c\u5305\u62ec\u4e2d\u5fc3\u6027\u504f\u89c1\u548c\u8d28\u91cf\u4f9d\u8d56\u6027\u504f\u79fb\uff08\u9ad8\u8d28\u91cf\u9009\u9879\u65f6\u504f\u5411\u9996\u4f4d\uff0c\u4f4e\u8d28\u91cf\u65f6\u504f\u5411\u672b\u4f4d\uff09\u3002\u987a\u5e8f\u504f\u89c1\u751a\u81f3\u5f3a\u4e8e\u6027\u522b\u504f\u89c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u9009\u62e9\u660e\u663e\u52a3\u8d28\u9009\u9879\u3002", "conclusion": "LLM\u7684\u504f\u89c1\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u8868\u73b0\u51fa\u72ec\u7279\u7684\u51b3\u7b56\u5931\u8d25\u6a21\u5f0f\u3002\u7814\u7a76\u63d0\u51fa\u7684\u6e29\u5ea6\u53c2\u6570\u8c03\u6574\u7b49\u65b9\u6cd5\u53ef\u6709\u6548\u51cf\u5c11\u987a\u5e8f\u9a71\u52a8\u7684\u51b3\u7b56\u626d\u66f2\u3002", "paper_title_zh": "\u8106\u5f31\u504f\u597d\uff1a\u6df1\u5165\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u987a\u5e8f\u6548\u5e94", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u62db\u8058\u548c\u5927\u5b66\u5f55\u53d6\uff09\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u51b3\u7b56\u901a\u5e38\u6d89\u53ca\u5728\u7ade\u4e89\u9009\u9879\u4e2d\u9009\u62e9\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u6ce8\u610f\u5230LLM\u9a71\u52a8\u7684\u6bd4\u8f83\u4e2d\u5b58\u5728\u4f4d\u7f6e\u987a\u5e8f\u504f\u89c1\uff0c\u4f46\u8fd9\u4e9b\u504f\u89c1\u5c1a\u672a\u88ab\u7cfb\u7edf\u5256\u6790\u6216\u4e0e\u5e95\u5c42\u504f\u597d\u7ed3\u6784\u5173\u8054\u3002\u6211\u4eec\u9996\u6b21\u5bf9\u591a\u79cdLLM\u67b6\u6784\u548c\u9886\u57df\u7684\u4f4d\u7f6e\u504f\u89c1\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5f3a\u70c8\u4e14\u4e00\u81f4\u7684\u987a\u5e8f\u6548\u5e94\uff0c\u5305\u62ec\u4e00\u79cd\u5728\u4eba\u7c7b\u6216\u673a\u5668\u51b3\u7b56\u4e2d\u672a\u88ab\u8bb0\u5f55\u7684\u65b0\u9896\u4e2d\u5fc3\u6027\u504f\u89c1\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\u8d28\u91cf\u4f9d\u8d56\u6027\u504f\u79fb\uff1a\u5f53\u9009\u9879\u8d28\u91cf\u9ad8\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u9996\u4f4d\u504f\u89c1\uff0c\u800c\u5728\u9009\u9879\u8d28\u91cf\u4f4e\u65f6\u504f\u5411\u540e\u8005\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u4e86\u4e00\u79cd\u6b64\u524d\u672a\u8bb0\u5f55\u7684\u504f\u89c1\uff0c\u5373\u67d0\u4e9b\u540d\u79f0\u6bd4\u5176\u4ed6\u540d\u79f0\u66f4\u53d7\u9752\u7750\u3002\u4e3a\u4e86\u533a\u5206\u8868\u9762\u7684\u5e73\u5c40\u6253\u7834\u4e0e\u771f\u5b9e\u7684\u5224\u65ad\u626d\u66f2\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u6210\u5bf9\u504f\u597d\u5206\u7c7b\u4e3a\u7a33\u5065\u3001\u8106\u5f31\u6216\u4e2d\u7acb\u3002\u7814\u7a76\u8868\u660e\uff0c\u987a\u5e8f\u6548\u5e94\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u9009\u62e9\u660e\u663e\u52a3\u8d28\u7684\u9009\u9879\uff0c\u4e14\u4f4d\u7f6e\u504f\u89c1\u901a\u5e38\u5f3a\u4e8e\u6027\u522b\u504f\u89c1\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cLLM\u5e76\u975e\u4ec5\u4ec5\u7ee7\u627f\u4e86\u4eba\u7c7b\u7c7b\u4f3c\u7684\u504f\u89c1\uff0c\u800c\u662f\u8868\u73b0\u51fa\u4eba\u7c7b\u51b3\u7b56\u4e2d\u672a\u89c1\u7684\u72ec\u7279\u5931\u8d25\u6a21\u5f0f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u5305\u62ec\u6e29\u5ea6\u53c2\u6570\u7684\u65b0\u9896\u4f7f\u7528\uff0c\u4ee5\u51cf\u5c11\u987a\u5e8f\u9a71\u52a8\u7684\u626d\u66f2\u3002"}}
{"id": "2506.14206", "pdf": "https://arxiv.org/pdf/2506.14206", "abs": "https://arxiv.org/abs/2506.14206", "authors": ["Jia-Chen Zhang", "Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Fei Dai"], "title": "CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation", "categories": ["cs.CL"], "comment": null, "summary": "Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.", "AI": {"tldr": "CausalDiffTab\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u6df7\u5408\u7c7b\u578b\u7684\u8868\u683c\u6570\u636e\uff08\u6570\u503c\u548c\u5206\u7c7b\u7279\u5f81\uff09\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u81ea\u9002\u5e94\u56e0\u679c\u6b63\u5219\u5316\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u8d28\u91cf\u6570\u636e\u5728\u751f\u6210\u5f0fAI\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u56f0\u96be\u4e14\u5b58\u5728\u9690\u79c1\u95ee\u9898\u3002\u5408\u6210\u6570\u636e\u6210\u4e3a\u4e3b\u6d41\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u7684\u751f\u6210\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5982\u5f02\u6784\u6570\u636e\u7c7b\u578b\u3001\u590d\u6742\u53d8\u91cf\u5173\u7cfb\u548c\u5217\u5206\u5e03\u3002", "method": "\u63d0\u51faCausalDiffTab\u6a21\u578b\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5904\u7406\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u5206\u5c42\u5148\u9a8c\u878d\u5408\u539f\u5219\u7684\u6df7\u5408\u81ea\u9002\u5e94\u56e0\u679c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u63a7\u5236\u6b63\u5219\u5316\u6743\u91cd\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCausalDiffTab\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CausalDiffTab\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u81ea\u9002\u5e94\u56e0\u679c\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u751f\u6210\u7684\u6311\u6218\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "paper_title_zh": "CausalDiffTab\uff1a\u6df7\u5408\u7c7b\u578b\u56e0\u679c\u611f\u77e5\u6269\u6563\u7684\u8868\u683c\u6570\u636e\u751f\u6210", "abstract_zh": "\u8bad\u7ec3\u6570\u636e\u5df2\u88ab\u8bc1\u660e\u662f\u8bad\u7ec3\u751f\u6210\u5f0fAI\u4e2d\u6700\u5173\u952e\u7684\u7ec4\u6210\u90e8\u5206\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u83b7\u53d6\u9ad8\u8d28\u91cf\u6570\u636e\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u6570\u636e\u9690\u79c1\u95ee\u9898\u66f4\u662f\u91cd\u5927\u969c\u788d\u3002\u4e3a\u6ee1\u8db3\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\uff0c\u5408\u6210\u6570\u636e\u5df2\u6210\u4e3a\u4e3b\u6d41\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u751f\u6210\u6df7\u5408\u7c7b\u578b\u6570\u636e\uff0c\u5c24\u5176\u662f\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\uff0c\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4e3b\u8981\u5305\u62ec\u5176\u56fa\u6709\u7684\u5f02\u6784\u6570\u636e\u7c7b\u578b\u3001\u590d\u6742\u7684\u53d8\u91cf\u95f4\u5173\u7cfb\u4ee5\u53ca\u9519\u7efc\u590d\u6742\u7684\u5217\u5206\u5e03\u3002\u672c\u6587\u63d0\u51faCausalDiffTab\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6a21\u578b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u5305\u542b\u6570\u503c\u548c\u5206\u7c7b\u7279\u5f81\u7684\u6df7\u5408\u8868\u683c\u6570\u636e\uff0c\u540c\u65f6\u66f4\u7075\u6d3b\u5730\u6355\u6349\u53d8\u91cf\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u5148\u9a8c\u878d\u5408\u539f\u5219\u7684\u6df7\u5408\u81ea\u9002\u5e94\u56e0\u679c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u63a7\u5236\u56e0\u679c\u6b63\u5219\u5316\u7684\u6743\u91cd\uff0c\u5728\u4e0d\u635f\u5bb3\u751f\u6210\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cCausalDiffTab\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u4ee3\u7801\u516c\u5f00\u4e8e\uff1ahttps://github.com/Godz-z/CausalDiffTab\u3002"}}
{"id": "2506.14229", "pdf": "https://arxiv.org/pdf/2506.14229", "abs": "https://arxiv.org/abs/2506.14229", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Juan Zhang", "Tongfei Chen", "Shuo Yang", "Shuwei Shao", "Wenhao Dong", "Baochang Zhang"], "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHRGS\u7684\u5206\u5c42\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u4f18\u5316\u548c\u91cd\u8981\u6027\u9a71\u52a8\u7684\u9ad8\u65af\u4fee\u526a\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u73873D\u91cd\u5efa\u4e2d\u7684\u5185\u5b58\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u91cd\u5efa\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u9762\u4e34\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86HRGS\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u73873D\u91cd\u5efa\u3002", "method": "HRGS\u9996\u5148\u4ece\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u751f\u6210\u5168\u5c40\u7c97\u7cd9\u9ad8\u65af\u8868\u793a\uff0c\u7136\u540e\u5c06\u573a\u666f\u5212\u5206\u4e3a\u591a\u4e2a\u5757\uff0c\u5e76\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7ec6\u5316\u6bcf\u4e2a\u5757\u3002\u901a\u8fc7\u9ad8\u65af\u5206\u533a\u548c\u6570\u636e\u5206\u533a\uff0c\u786e\u4fdd\u76f8\u90bb\u5757\u7684\u9ad8\u65af\u65e0\u7f1d\u878d\u5408\u3002\u6b64\u5916\uff0c\u5f15\u5165\u91cd\u8981\u6027\u9a71\u52a8\u7684\u9ad8\u65af\u4fee\u526a\uff08IDGP\uff09\u4ee5\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6cd5\u7ebf\u5148\u9a8c\u63d0\u5347\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHRGS\u5728\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u548c\u8868\u9762\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HRGS\u901a\u8fc7\u5206\u5c42\u5757\u7ea7\u4f18\u5316\u548c\u5185\u5b58\u9ad8\u6548\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9ad8\u5206\u8fa8\u73873D\u573a\u666f\u91cd\u5efa\uff0c\u5373\u4f7f\u5728\u5185\u5b58\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8868\u73b0\u51fa\u8272\u3002", "paper_title_zh": "HRGS\uff1a\u57fa\u4e8e\u5206\u5c42\u9ad8\u65af\u6cfc\u6e85\u7684\u5185\u5b58\u9ad8\u6548\u9ad8\u5206\u8fa8\u73873D\u91cd\u5efa", "abstract_zh": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u9762\u4e34\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5c42\u9ad8\u65af\u6cfc\u6e85\uff08HRGS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u5206\u5c42\u5757\u7ea7\u4f18\u5316\u7684\u5185\u5b58\u9ad8\u6548\u6846\u67b6\u3002\u9996\u5148\uff0c\u6211\u4eec\u4ece\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u751f\u6210\u5168\u5c40\u7c97\u7cd9\u9ad8\u65af\u8868\u793a\uff1b\u7136\u540e\uff0c\u5c06\u573a\u666f\u5212\u5206\u4e3a\u591a\u4e2a\u5757\uff0c\u5e76\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7ec6\u5316\u6bcf\u4e2a\u5757\u3002\u5206\u533a\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u9ad8\u65af\u5206\u533a\uff08\u5c06\u4e0d\u89c4\u5219\u573a\u666f\u5f52\u4e00\u5316\u4e3a\u6709\u754c\u7acb\u65b9\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u5747\u5300\u7f51\u683c\u8fdb\u884c\u4efb\u52a1\u5206\u914d\uff09\u548c\u8bad\u7ec3\u6570\u636e\u5206\u533a\uff08\u4ec5\u4fdd\u7559\u6bcf\u4e2a\u5757\u7684\u76f8\u5173\u89c2\u6d4b\u6570\u636e\uff09\u3002\u901a\u8fc7\u7c97\u7cd9\u9ad8\u65af\u5148\u9a8c\u6307\u5bfc\u5757\u7ec6\u5316\uff0c\u6211\u4eec\u786e\u4fdd\u4e86\u76f8\u90bb\u5757\u7684\u9ad8\u65af\u65e0\u7f1d\u878d\u5408\u3002\u4e3a\u4e86\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u91cd\u8981\u6027\u9a71\u52a8\u7684\u9ad8\u65af\u4fee\u526a\uff08IDGP\uff09\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u9ad8\u65af\u7684\u8d21\u732e\u5206\u6570\u5e76\u79fb\u9664\u8d21\u732e\u6700\u5c0f\u7684\u90e8\u5206\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u5e76\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6cd5\u7ebf\u5148\u9a8c\u6765\u63d0\u5347\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5373\u4f7f\u5728\u5185\u5b58\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9ad8\u5206\u8fa8\u73873D\u573a\u666f\u91cd\u5efa\u3002\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHRGS\u5728\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u548c\u8868\u9762\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.14125", "pdf": "https://arxiv.org/pdf/2506.14125", "abs": "https://arxiv.org/abs/2506.14125", "authors": ["Libo Zhang", "Yang Chen", "Toru Takisaka", "Kaiqi Zhao", "Weidong Li", "Jiamou Liu"], "title": "Situational-Constrained Sequential Resources Allocation via Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCRL\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u60c5\u5883\u7ea6\u675f\u4e0b\u7684\u5e8f\u5217\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u60c5\u5883\u7ea6\u675f\u5f62\u5f0f\u5316\u4e3a\u903b\u8f91\u5173\u7cfb\u5e76\u52a8\u6001\u60e9\u7f5a\u7ea6\u675f\u8fdd\u53cd\uff0cSCRL\u5728\u533b\u7597\u548c\u519c\u4e1a\u8d44\u6e90\u5206\u914d\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\u901a\u5e38\u53d7\u60c5\u5883\u7ea6\u675f\u5f71\u54cd\uff0c\u800c\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\u7684\u7ea6\u675f\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u60c5\u5883\u7ea6\u675f\u5e76\u9ad8\u6548\u5206\u914d\u8d44\u6e90\u7684\u6846\u67b6\u3002", "method": "SCRL\u6846\u67b6\u5c06\u60c5\u5883\u7ea6\u675f\u5f62\u5f0f\u5316\u4e3a\u903b\u8f91\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u60e9\u7f5a\u7ea6\u675f\u8fdd\u53cd\u7684\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u5f15\u5165\u6982\u7387\u9009\u62e9\u673a\u5236\u4ee5\u514b\u670d\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u533b\u7597\u8d44\u6e90\u5206\u914d\u548c\u519c\u836f\u5206\u914d\u4e24\u4e2a\u573a\u666f\u4e2d\uff0cSCRL\u5728\u6ee1\u8db3\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u8d44\u6e90\u5206\u914d\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SCRL\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u60c5\u5883\u654f\u611f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u7ea6\u675f\u5e76\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "paper_title_zh": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u60c5\u5883\u7ea6\u675f\u5e8f\u5217\u8d44\u6e90\u5206\u914d", "abstract_zh": "\u60c5\u5883\u7ea6\u675f\u4e0b\u7684\u5e8f\u5217\u8d44\u6e90\u5206\u914d\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u8d44\u6e90\u9700\u6c42\u548c\u4f18\u5148\u7ea7\u901a\u5e38\u4f9d\u8d56\u4e8e\u5177\u4f53\u60c5\u5883\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCRL\u7684\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u6211\u4eec\u5c06\u60c5\u5883\u7ea6\u675f\u5f62\u5f0f\u5316\u4e3a\u903b\u8f91\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u60e9\u7f5a\u7ea6\u675f\u8fdd\u53cd\u7684\u65b0\u7b97\u6cd5\u3002\u4e3a\u4e86\u6709\u6548\u5904\u7406\u60c5\u5883\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u9009\u62e9\u673a\u5236\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86SCRL\uff1a\u75ab\u60c5\u671f\u95f4\u7684\u533b\u7597\u8d44\u6e90\u5206\u914d\u548c\u519c\u4e1a\u4e2d\u7684\u519c\u836f\u5206\u914d\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSCRL\u5728\u6ee1\u8db3\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u8d44\u6e90\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u60c5\u5883\u654f\u611f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14211", "pdf": "https://arxiv.org/pdf/2506.14211", "abs": "https://arxiv.org/abs/2506.14211", "authors": ["Sina Abdidizaji", "Md Kowsher", "Niloofar Yousefi", "Ivan Garibay"], "title": "Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation", "categories": ["cs.CL"], "comment": "Accepted at the HCI International conference 2025", "summary": "In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5bf9\u8bdd\u4e2d\u9690\u542b\u7684\u5f71\u54cd\u529b\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5316\u65f6\u4ee3\u7684\u53d1\u5c55\uff0c\u6076\u610f\u884c\u4e3a\u8005\u9010\u6e10\u8f6c\u5411\u4f7f\u7528\u9690\u542b\u7684\u8bed\u8a00\u7b56\u7565\u5f71\u54cd\u516c\u4f17\u8ba4\u77e5\uff0c\u800c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u8fd9\u4e9b\u9690\u542b\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u5229\u7528\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u73b0\u6709\u6570\u636e\u96c6\u8fdb\u884c\u589e\u5f3a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u80fd\u591f\u5b9a\u4f4d\u5bf9\u8bdd\u4e2d\u9690\u542b\u5f71\u54cd\u529b\u5143\u7d20\u7684\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u9690\u542b\u5f71\u54cd\u529b\u6a21\u5f0f\u4e0a\u63d0\u5347\u4e866%\uff0c\u5e76\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u5347\u4e8633%\uff08\u5f71\u54cd\u529b\u6280\u672f\uff09\u548c43%\uff08\u53d7\u5bb3\u8005\u8106\u5f31\u6027\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9690\u542b\u5f71\u54cd\u529b\u6a21\u5f0f\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\u5bf9\u8bdd\u4e2d\u9690\u542b\u5f71\u54cd\u529b\u6a21\u5f0f\u7684\u53ef\u89e3\u91ca\u68c0\u6d4b", "abstract_zh": "\u5728\u6570\u5b57\u5316\u65f6\u4ee3\uff0c\u968f\u7740\u4e2a\u4eba\u8d8a\u6765\u8d8a\u4f9d\u8d56\u6570\u5b57\u5e73\u53f0\u8fdb\u884c\u4ea4\u6d41\u548c\u65b0\u95fb\u6d88\u8d39\uff0c\u5404\u79cd\u884c\u4e3a\u8005\u5229\u7528\u8bed\u8a00\u7b56\u7565\u5f71\u54cd\u516c\u4f17\u8ba4\u77e5\u3002\u867d\u7136\u73b0\u6709\u6a21\u578b\u64c5\u957f\u68c0\u6d4b\u663e\u5f0f\u6a21\u5f0f\uff08\u5982\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u7684\u5355\u6761\u8a00\u8bba\uff09\uff0c\u4f46\u6076\u610f\u884c\u4e3a\u8005\u5df2\u8f6c\u5411\u4f7f\u7528\u5d4c\u5165\u5bf9\u8bdd\u4e2d\u7684\u9690\u542b\u5f71\u54cd\u529b\u8bed\u8a00\u6a21\u5f0f\u3002\u8fd9\u4e9b\u8bed\u8a00\u6a21\u5f0f\u65e8\u5728\u901a\u8fc7\u5fc3\u7406\u6e17\u900f\u5f71\u54cd\u53d7\u5bb3\u8005\uff0c\u4ece\u800c\u4ee5\u9690\u542b\u65b9\u5f0f\u83b7\u53d6\u6240\u9700\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u6b64\u7c7b\u9690\u542b\u5f71\u54cd\u529b\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u5b9a\u4f4d\u5bf9\u8bdd\u4e2d\u8fd9\u4e9b\u5f71\u54cd\u529b\u5143\u7d20\u7684\u5177\u4f53\u4f4d\u7f6e\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5229\u7528\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u73b0\u6709\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u589e\u5f3a\u3002\u8bbe\u8ba1\u7684\u6846\u67b6\u4f7f\u5bf9\u8bdd\u4e2d\u9690\u542b\u5f71\u54cd\u529b\u6a21\u5f0f\u7684\u68c0\u6d4b\u6548\u679c\u63d0\u5347\u4e866%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5173\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5206\u522b\u5c06\u5f71\u54cd\u529b\u6280\u672f\u548c\u53d7\u5bb3\u8005\u8106\u5f31\u6027\u7684\u5206\u7c7b\u6548\u679c\u63d0\u5347\u4e8633%\u548c43%\u3002"}}
{"id": "2506.14238", "pdf": "https://arxiv.org/pdf/2506.14238", "abs": "https://arxiv.org/abs/2506.14238", "authors": ["Yinuo Zheng", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "Unified Representation Space for 3D Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniSpace-3D\uff0c\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\u89e3\u51b33D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u7684\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u5206\u522b\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u6a21\u6001\u95f4\u5728\u7a7a\u95f4\u51e0\u4f55\u548c\u8bed\u4e49\u7c7b\u522b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f71\u54cd\u5b9a\u4f4d\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "UniSpace-3D\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u8bbe\u8ba1\uff1a1) \u7edf\u4e00\u8868\u793a\u7f16\u7801\u5668\u5229\u7528CLIP\u6a21\u578b\u5c06\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u7a7a\u95f4\uff1b2) \u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u8fdb\u4e00\u6b65\u7f29\u5c0f\u6a21\u6001\u5dee\u5f02\uff1b3) \u8bed\u8a00\u5f15\u5bfc\u7684\u67e5\u8be2\u9009\u62e9\u6a21\u5757\u5229\u7528\u4f4d\u7f6e\u548c\u8bed\u4e49\u4fe1\u606f\u7b5b\u9009\u4e0e\u6587\u672c\u63cf\u8ff0\u5339\u914d\u7684\u5019\u9009\u70b9\u3002", "result": "\u5728ScanRefer\u548cNr3D/Sr3D\u6570\u636e\u96c6\u4e0a\uff0cUniSpace-3D\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u81f3\u5c112.24%\u3002", "conclusion": "UniSpace-3D\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\u6709\u6548\u89e3\u51b3\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "paper_title_zh": "3D\u89c6\u89c9\u5b9a\u4f4d\u7684\u7edf\u4e00\u8868\u793a\u7a7a\u95f4", "abstract_zh": "3D\u89c6\u89c9\u5b9a\u4f4d\uff083DVG\uff09\u662f\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u65e8\u5728\u6839\u636e\u6587\u672c\u63cf\u8ff0\u8bc6\u522b3D\u573a\u666f\u4e2d\u7684\u7269\u4f53\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5206\u522b\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u4e24\u79cd\u6a21\u6001\u5728\u7a7a\u95f4\u51e0\u4f55\u548c\u8bed\u4e49\u7c7b\u522b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e38\u5f15\u53d1\u5b9a\u4f4d\u548c\u5206\u7c7b\u9519\u8bef\u3002\u672c\u6587\u63d0\u51faUniSpace-3D\uff0c\u521b\u65b0\u6027\u5730\u5f15\u5165\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\uff0c\u6709\u6548\u5f25\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u7279\u5f81\u95f4\u7684\u5dee\u5f02\u3002\u5177\u4f53\u800c\u8a00\uff0cUniSpace-3D\u5305\u542b\u4e09\u9879\u521b\u65b0\u8bbe\u8ba1\uff1ai) \u7edf\u4e00\u8868\u793a\u7f16\u7801\u5668\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u5c06\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\uff1bii) \u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u8fdb\u4e00\u6b65\u7f29\u5c0f\u6a21\u6001\u5dee\u5f02\uff1biii) \u8bed\u8a00\u5f15\u5bfc\u7684\u67e5\u8be2\u9009\u62e9\u6a21\u5757\u5229\u7528\u4f4d\u7f6e\u548c\u8bed\u4e49\u4fe1\u606f\u7b5b\u9009\u4e0e\u6587\u672c\u63cf\u8ff0\u5339\u914d\u7684\u5019\u9009\u70b9\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniSpace-3D\u5728ScanRefer\u548cNr3D/Sr3D\u6570\u636e\u96c6\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u81f3\u5c112.24%\u3002\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u5f55\u7528\u540e\u516c\u5f00\u3002"}}
{"id": "2506.14146", "pdf": "https://arxiv.org/pdf/2506.14146", "abs": "https://arxiv.org/abs/2506.14146", "authors": ["Kaiwen Tang", "Aitong Wu", "Yao Lu", "Guangda Sun"], "title": "Collaborative Editable Model", "categories": ["cs.AI"], "comment": null, "summary": "Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u53ef\u7f16\u8f91\u6a21\u578b\uff08CoEM\uff09\uff0c\u901a\u8fc7\u7528\u6237\u8d21\u732e\u7684\u9886\u57df\u7247\u6bb5\u6784\u5efa\u77e5\u8bc6\u6c60\uff0c\u7ed3\u5408\u7528\u6237\u8bc4\u5206\u548c\u5f52\u56e0\u5206\u6790\u7b5b\u9009\u9ad8\u4ef7\u503c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u9886\u57df\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u5782\u76f4\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5782\u76f4\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982\u91d1\u878d\u3001\u533b\u7597\u3001\u6cd5\u5f8b\uff09\u7684\u8bad\u7ec3\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u963b\u788d\u4e86\u5feb\u901f\u5f00\u53d1\u548c\u8fed\u4ee3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u53ef\u7f16\u8f91\u6a21\u578b\uff08CoEM\uff09\u3002", "method": "CoEM\u901a\u8fc7\u7528\u6237\u8d21\u732e\u7684\u9886\u57df\u7247\u6bb5\u6784\u5efa\u5019\u9009\u77e5\u8bc6\u6c60\uff0c\u7ed3\u5408\u7528\u6237-\u6a21\u578b\u4ea4\u4e92\u5bf9\u8bdd\u3001\u7528\u6237\u8bc4\u5206\u548c\u5f52\u56e0\u5206\u6790\uff0c\u7b5b\u9009\u9ad8\u4ef7\u503c\u77e5\u8bc6\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u63d0\u793a\u6ce8\u5165\u6a21\u578b\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u9886\u57df\u9002\u5e94\u3002", "result": "\u5728\u91d1\u878d\u4fe1\u606f\u573a\u666f\u4e2d\uff0c\u6536\u96c6\u4e86\u7ea6120\u540d\u7528\u6237\u76841.5\u4e07\u6761\u53cd\u9988\uff0c\u9a8c\u8bc1\u4e86CoEM\u5728\u63d0\u5347\u9886\u57df\u7279\u5b9a\u751f\u6210\u8d28\u91cf\u65b9\u9762\u7684\u663e\u8457\u6548\u679c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u5fae\u8c03\u7684\u9ad8\u6210\u672c\u3002", "conclusion": "CoEM\u901a\u8fc7\u534f\u4f5c\u7f16\u8f91\u548c\u8f7b\u91cf\u7ea7\u9002\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5782\u76f4\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\u548c\u9886\u57df\u9488\u5bf9\u6027\u3002", "paper_title_zh": "\u534f\u4f5c\u53ef\u7f16\u8f91\u6a21\u578b", "abstract_zh": "\u5782\u76f4\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91d1\u878d\u3001\u533b\u7597\u548c\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u573a\u666f\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5176\u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u963b\u788d\u4e86\u5feb\u901f\u5f00\u53d1\u548c\u6301\u7eed\u8fed\u4ee3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u534f\u4f5c\u53ef\u7f16\u8f91\u6a21\u578b\uff08CoEM\uff09\uff0c\u901a\u8fc7\u7528\u6237\u8d21\u732e\u7684\u9886\u57df\u7247\u6bb5\u6784\u5efa\u5019\u9009\u77e5\u8bc6\u6c60\uff0c\u7ed3\u5408\u7528\u6237-\u6a21\u578b\u4ea4\u4e92\u5bf9\u8bdd\u3001\u7528\u6237\u8bc4\u5206\u548c\u5f52\u56e0\u5206\u6790\u7b5b\u9009\u9ad8\u4ef7\u503c\u77e5\u8bc6\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u63d0\u793a\u6ce8\u5165\u6a21\u578b\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u9886\u57df\u9002\u5e94\u3002\u501f\u52a9\u9ad8\u4ef7\u503c\u77e5\u8bc6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u4e14\u9886\u57df\u76f8\u5173\u7684\u5185\u5bb9\u3002\u5728\u91d1\u878d\u4fe1\u606f\u573a\u666f\u4e2d\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u7ea6120\u540d\u7528\u6237\u76841.5\u4e07\u6761\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u8bc4\u5206\u9a8c\u8bc1\u4e86CoEM\u5728\u63d0\u5347\u9886\u57df\u7279\u5b9a\u751f\u6210\u8d28\u91cf\u65b9\u9762\u7684\u663e\u8457\u6548\u679c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u5fae\u8c03\u5de5\u4f5c\u6d41\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2506.14213", "pdf": "https://arxiv.org/pdf/2506.14213", "abs": "https://arxiv.org/abs/2506.14213", "authors": ["Jongho Kim", "Dohyeon Lee", "Minsoo Kim", "Seung-won Hwang"], "title": "Chaining Event Spans for Temporal Relation Grounding", "categories": ["cs.CL"], "comment": "In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1689-1700", "summary": "Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: \"What finished right before the decision?\" or \"What finished right after the decision?\". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08Timeline Reasoning Network, TRN\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u4e8b\u4ef6\u65f6\u95f4\u8de8\u5ea6\u6765\u89e3\u51b3\u65f6\u95f4\u5173\u7cfb\u7406\u89e3\u4efb\u52a1\u4e2d\u7b54\u6848\u91cd\u53e0\u5bfc\u81f4\u7684\u4e0d\u53ef\u9760\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7b54\u6848\u91cd\u53e0\u4f5c\u4e3a\u6807\u7b7e\u6765\u533a\u5206\u76f8\u4f3c\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u56e0\u5076\u7136\u76f8\u540c\u7684\u7b54\u6848\u5bfc\u81f4\u4e0d\u53ef\u9760\u7ed3\u679c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u7ebf\u63a8\u7406\u63d0\u5347\u65f6\u95f4\u5173\u7cfb\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faTimeline Reasoning Network\uff08TRN\uff09\uff0c\u91c7\u7528\u4e24\u6b65\u5f52\u7eb3\u63a8\u7406\uff1a\u9996\u5148\u57fa\u4e8e\u8bed\u4e49\u548c\u53e5\u6cd5\u4fe1\u606f\u56de\u7b54\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u94fe\u5f0f\u95ee\u9898\u9884\u6d4b\u65f6\u95f4\u7ebf\uff0c\u7528\u4e8e\u9a8c\u8bc1\u7b54\u6848\u7684\u5408\u7406\u6027\u3002", "result": "\u5728TORQUE\u548cTB-dense\u6570\u636e\u96c6\u4e0a\uff0cTRN\u5728\u65f6\u95f4\u9605\u8bfb\u7406\u89e3\uff08TRC\uff09\u548c\u65f6\u95f4\u5173\u7cfb\u62bd\u53d6\uff08TRE\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u865a\u5047\u7b54\u6848\u91cd\u53e0\u95ee\u9898\u3002", "conclusion": "TRN\u901a\u8fc7\u65f6\u95f4\u7ebf\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5173\u7cfb\u7406\u89e3\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u4e8b\u4ef6\u65f6\u95f4\u8de8\u5ea6\u94fe\u5f0f\u63a8\u7406\u7528\u4e8e\u65f6\u95f4\u5173\u7cfb\u57fa\u7840", "abstract_zh": "\u51c6\u786e\u7406\u89e3\u4e8b\u4ef6\u4e4b\u95f4\u7684\u65f6\u95f4\u5173\u7cfb\u662f\u65f6\u95f4\u9605\u8bfb\u7406\u89e3\uff08TRC\uff09\u548c\u5173\u7cfb\u62bd\u53d6\uff08TRE\uff09\u7b49\u4efb\u52a1\u7684\u5173\u952e\u3002\u4f8b\u5982\uff0c\u5728TRC\u4e2d\uff0c\u9700\u8981\u533a\u5206\u4ee5\u4e0b\u4e24\u4e2a\u8bcd\u6c47\u76f8\u4f3c\u4f46\u8bed\u4e49\u4e0d\u540c\u7684\u95ee\u9898\uff1a\u201c\u4ec0\u4e48\u5728\u51b3\u5b9a\u4e4b\u524d\u5b8c\u6210\uff1f\u201d\u548c\u201c\u4ec0\u4e48\u5728\u51b3\u5b9a\u4e4b\u540e\u5b8c\u6210\uff1f\u201d\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7b54\u6848\u91cd\u53e0\u4f5c\u4e3a\u6807\u7b7e\uff0c\u4f46\u53ef\u80fd\u56e0\u5076\u7136\u76f8\u540c\u7684\u7b54\u6848\u5bfc\u81f4\u4e0d\u53ef\u9760\u7ed3\u679c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u4e8b\u4ef6\u65f6\u95f4\u8de8\u5ea6\u6a21\u5757\u5f15\u5bfc\u5408\u7406\u63a8\u7406\u884c\u4e3a\u3002\u6211\u4eec\u63d0\u51fa\u4e86Timeline Reasoning Network\uff08TRN\uff09\uff0c\u91c7\u7528\u4e24\u6b65\u5f52\u7eb3\u63a8\u7406\uff1a\u9996\u5148\u57fa\u4e8e\u8bed\u4e49\u548c\u53e5\u6cd5\u4fe1\u606f\u56de\u7b54\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u94fe\u5f0f\u95ee\u9898\u9884\u6d4b\u65f6\u95f4\u7ebf\u4ee5\u9a8c\u8bc1\u7b54\u6848\u3002\u5728TORQUE\u548cTB-dense\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTRN\u5728TRC\u548cTRE\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u7ebf\u9884\u6d4b\u6709\u6548\u89e3\u51b3\u4e86\u865a\u5047\u7b54\u6848\u91cd\u53e0\u95ee\u9898\u3002"}}
{"id": "2506.14243", "pdf": "https://arxiv.org/pdf/2506.14243", "abs": "https://arxiv.org/abs/2506.14243", "authors": ["Xiaohui Jiang", "Haijiang Zhu", "Chadei Li", "Fulin Tang", "Ning An"], "title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u5b50\u5730\u56fe\u7684\u8de8\u6a21\u6001\u51e0\u4f55\u5c42\u6b21\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u5ea6\u65e0\u5173\u7684\u51e0\u4f55\u63a8\u7406\u89e3\u51b3\u4e86LiDAR\u70b9\u4e91\u5bc6\u5ea6\u4e0d\u4e00\u81f4\u548c\u5355\u5c42\u51e0\u4f55\u62bd\u8c61\u8868\u5f81\u8106\u5f31\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u76843D\u5730\u70b9\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u7684LiDAR\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u63d0\u53d6\uff0c\u9762\u4e34\u70b9\u4e91\u5bc6\u5ea6\u4e0d\u4e00\u81f4\u548c\u5355\u5c42\u51e0\u4f55\u62bd\u8c61\u8868\u5f81\u8106\u5f31\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63cf\u8ff0\u7b26\u4e0d\u7a33\u5b9a\u4e14\u7f3a\u4e4f\u533a\u5206\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f39\u6027\u70b9\u7684\u9690\u5f0f3D\u8868\u5f81\uff0c\u751f\u6210\u5747\u5300\u5206\u5e03\u7684\u70b9\u4e91\uff1b\u4ece\u4e2d\u63d0\u53d6\u573a\u666f\u7684\u5360\u636e\u7f51\u683c\u548c\u6cd5\u5411\u91cf\u4fe1\u606f\uff1b\u878d\u5408\u9e1f\u77b0\u56fe\u548c3D\u7247\u6bb5\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u751f\u6210\u9ad8\u533a\u5206\u529b\u7684\u63cf\u8ff0\u7b26\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08KITTI\u3001KITTI-360\u3001MulRan\u3001NCLT\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u4f18\u5316\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5bc6\u5ea6\u65e0\u5173\u7684\u51e0\u4f55\u63a8\u7406\u548c\u8de8\u6a21\u6001\u4fe1\u606f\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5730\u70b9\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u8de8\u6a21\u6001\u51e0\u4f55\u5c42\u6b21\u878d\u5408\uff1a\u4e00\u79cd\u9690\u5f0f\u5b50\u5730\u56fe\u9a71\u52a8\u7684\u9c81\u68d23D\u5730\u70b9\u8bc6\u522b\u6846\u67b6", "abstract_zh": "\u57fa\u4e8eLiDAR\u7684\u5730\u70b9\u8bc6\u522b\u662f\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b9e\u73b0\u957f\u671f\u81ea\u4e3b\u6027\u7684\u5173\u952e\u6280\u672f\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u63d0\u53d6\uff0c\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a(1) \u7531\u4e8e\u91cd\u590d\u904d\u5386\u65f6\u7684\u52a8\u6001\u8fd0\u52a8\u548c\u73af\u5883\u5e72\u6270\uff0c\u70b9\u4e91\u5bc6\u5ea6\u4e0d\u4e00\u81f4\u5bfc\u81f4\u63cf\u8ff0\u7b26\u4e0d\u7a33\u5b9a\uff1b(2) \u5355\u5c42\u51e0\u4f55\u62bd\u8c61\u8868\u5f81\u5728\u7ed3\u6784\u590d\u6742\u573a\u666f\u4e2d\u7f3a\u4e4f\u533a\u5206\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u65e0\u5173\u51e0\u4f55\u63a8\u7406\u7684\u65b0\u6846\u67b6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f39\u6027\u70b9\u7684\u9690\u5f0f3D\u8868\u5f81\uff0c\u4e0d\u53d7\u539f\u59cb\u70b9\u4e91\u5bc6\u5ea6\u5e72\u6270\u5e76\u5b9e\u73b0\u5747\u5300\u5206\u5e03\uff1b\u968f\u540e\u4ece\u4e2d\u63d0\u53d6\u573a\u666f\u7684\u5360\u636e\u7f51\u683c\u548c\u6cd5\u5411\u91cf\u4fe1\u606f\uff1b\u6700\u540e\uff0c\u7ed3\u5408\u9e1f\u77b0\u56fe\uff08\u6355\u6349\u5b8f\u89c2\u7a7a\u95f4\u5e03\u5c40\uff09\u548c3D\u7247\u6bb5\uff08\u7f16\u7801\u5fae\u89c2\u8868\u9762\u51e0\u4f55\uff09\u7684\u51e0\u4f55\u4fe1\u606f\u751f\u6210\u63cf\u8ff0\u7b26\u3002\u6211\u4eec\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08KITTI\u3001KITTI-360\u3001MulRan\u3001NCLT\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u5386\u53f2\u5730\u56fe\u5185\u5b58\u4f18\u5316\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u672a\u6765\u6211\u4eec\u5c06\u5f00\u6e90\u4ee3\u7801\u3002"}}
{"id": "2506.14212", "pdf": "https://arxiv.org/pdf/2506.14212", "abs": "https://arxiv.org/abs/2506.14212", "authors": ["Lance Ying", "Daniel Xu", "Alicia Zhang", "Katherine M. Collins", "Max H. Siegel", "Joshua B. Tenenbaum"], "title": "What's in the Box? Reasoning about Unseen Objects from Multimodal Cues", "categories": ["cs.AI"], "comment": "Paper published at CogSci 2025", "summary": "People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u6765\u63a8\u65ad\u672a\u89c1\u7269\u4f53\uff0c\u5e76\u5728\u201c\u76d2\u5b50\u91cc\u6709\u4ec0\u4e48\uff1f\u201d\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u7075\u6d3b\u6574\u5408\u591a\u6e90\u4fe1\u606f\uff08\u5982\u542c\u89c9\u3001\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u5148\u9a8c\u77e5\u8bc6\uff09\u6765\u63a8\u65ad\u672a\u89c1\u7269\u4f53\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u6a21\u578b\u5b9e\u73b0\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\uff1a\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u89e3\u6790\u591a\u6a21\u6001\u8f93\u5165\uff0c\u518d\u901a\u8fc7\u8d1d\u53f6\u65af\u6a21\u578b\u6574\u5408\u4fe1\u606f\u4ee5\u8bc4\u4f30\u4e0d\u540c\u5047\u8bbe\u3002\u5b9e\u9a8c\u91c7\u7528\u201c\u76d2\u5b50\u91cc\u6709\u4ec0\u4e48\uff1f\u201d\u6e38\u620f\uff0c\u8ba9\u6a21\u578b\u548c\u4eba\u7c7b\u901a\u8fc7\u6447\u6643\u76d2\u5b50\u7684\u89c6\u9891\u731c\u6d4b\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u5355\u6a21\u6001\u6a21\u578b\u6216\u591a\u6a21\u6001\u795e\u7ecf\u6a21\u578b\u57fa\u7ebf\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u6a21\u62df\u4eba\u7c7b\u63a8\u65ad\u672a\u89c1\u7269\u4f53\u7684\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u76d2\u5b50\u91cc\u6709\u4ec0\u4e48\uff1f\u57fa\u4e8e\u591a\u6a21\u6001\u7ebf\u7d22\u7684\u672a\u89c1\u7269\u4f53\u63a8\u7406", "abstract_zh": "\u4eba\u4eec\u7ecf\u5e38\u901a\u8fc7\u7075\u6d3b\u6574\u5408\u591a\u6e90\u4fe1\u606f\uff08\u5982\u542c\u89c9\u548c\u89c6\u89c9\u7ebf\u7d22\u3001\u8bed\u8a00\u4ee5\u53ca\u573a\u666f\u5148\u9a8c\u77e5\u8bc6\uff09\u6765\u63a8\u65ad\u672a\u89c1\u7269\u4f53\u3002\u6211\u4eec\u5982\u4f55\u80fd\u591f\u5982\u6b64\u7075\u6d3b\u5730\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\uff0c\u5373\u4f7f\u7f3a\u4e4f\u76f4\u63a5\u77e5\u8bc6\uff1f\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u89e3\u6790\u5f00\u653e\u591a\u6a21\u6001\u8f93\u5165\uff0c\u518d\u901a\u8fc7\u8d1d\u53f6\u65af\u6a21\u578b\u6574\u5408\u4e0d\u540c\u4fe1\u606f\u6e90\u4ee5\u8bc4\u4f30\u5047\u8bbe\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u79cd\u540d\u4e3a\u201c\u76d2\u5b50\u91cc\u6709\u4ec0\u4e48\uff1f\u201d\u7684\u65b0\u9896\u7269\u4f53\u731c\u6d4b\u6e38\u620f\u8bc4\u4f30\u6a21\u578b\uff0c\u4eba\u7c7b\u548c\u6a21\u578b\u89c2\u770b\u5b9e\u9a8c\u8005\u6447\u6643\u76d2\u5b50\u7684\u89c6\u9891\u540e\u731c\u6d4b\u5185\u5bb9\u3002\u901a\u8fc7\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u5355\u6a21\u6001\u6a21\u578b\u6216\u591a\u6a21\u6001\u795e\u7ecf\u6a21\u578b\u57fa\u7ebf\u8868\u73b0\u8f83\u5dee\u3002"}}
{"id": "2506.14234", "pdf": "https://arxiv.org/pdf/2506.14234", "abs": "https://arxiv.org/abs/2506.14234", "authors": ["Md Tanzib Hosain", "Salman Rahman", "Md Kishor Morol", "Md Rizwan Parvez"], "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.", "AI": {"tldr": "Xolver\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6837\u5316\u7684\u7ecf\u9a8c\u6a21\u6001\uff08\u5982\u5916\u90e8\u68c0\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u3001\u534f\u4f5c\u4e92\u52a8\u7b49\uff09\uff0c\u8d4b\u4e88\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u6f14\u5316\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u901a\u5e38\u5b64\u7acb\u8fd0\u884c\uff0c\u7f3a\u4e4f\u7ecf\u9a8c\u79ef\u7d2f\u4e0e\u6574\u5408\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e13\u5bb6\u56e2\u961f\uff08\u5982\u5965\u8d5b\u6216\u7f16\u7a0b\u7ade\u8d5b\u56e2\u961f\uff09\u80fd\u591f\u5229\u7528\u4e30\u5bcc\u7684\u7ecf\u9a8c\uff08\u5982\u6559\u7ec3\u6307\u5bfc\u3001\u5de5\u5177\u4f7f\u7528\u3001\u534f\u4f5c\u5b66\u4e60\u7b49\uff09\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002Xolver\u65e8\u5728\u6a21\u62df\u8fd9\u79cd\u7ecf\u9a8c\u5b66\u4e60\u673a\u5236\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "Xolver\u901a\u8fc7\u6574\u5408\u5916\u90e8\u68c0\u7d22\u3001\u81ea\u6211\u68c0\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u3001\u534f\u4f5c\u4e92\u52a8\u3001\u667a\u80fd\u4f53\u9a71\u52a8\u8bc4\u4f30\u548c\u8fed\u4ee3\u4f18\u5316\u7b49\u591a\u79cd\u7ecf\u9a8c\u6a21\u6001\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6301\u7eed\u6f14\u5316\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5b66\u4e60\u76f8\u5173\u7b56\u7565\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u62bd\u8c61\u63a8\u7406\u6a21\u5f0f\uff0c\u907f\u514d\u4ece\u96f6\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "result": "Xolver\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08\u5982QWQ-32B\uff09\uff0c\u4e5f\u80fd\u8d85\u8d8a\u5305\u62ecQwen3-235B\u3001Gemini 2.5 Pro\u7b49\u5728\u5185\u7684\u5148\u8fdb\u6a21\u578b\u3002\u5728GSM8K\u3001AIME'24\u7b49\u4efb\u52a1\u4e2d\u521b\u4e0b\u65b0\u9ad8\uff0c\u5982GSM8K\u51c6\u786e\u7387\u8fbe98.1%\u3002", "conclusion": "Xolver\u901a\u8fc7\u6574\u5408\u591a\u6837\u5316\u7ecf\u9a8c\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u63a8\u7406\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\u3002", "paper_title_zh": "Xolver\uff1a\u50cf\u5965\u8d5b\u56e2\u961f\u4e00\u6837\u901a\u8fc7\u6574\u4f53\u7ecf\u9a8c\u5b66\u4e60\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u63a8\u7406", "abstract_zh": "\u5c3d\u7ba1\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u5b64\u7acb\u8fd0\u884c\u2014\u2014\u5c06\u6bcf\u4e2a\u95ee\u9898\u89c6\u4e3a\u72ec\u7acb\u5c1d\u8bd5\uff0c\u7f3a\u4e4f\u7ecf\u9a8c\u7684\u79ef\u7d2f\u4e0e\u6574\u5408\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e13\u5bb6\u95ee\u9898\u89e3\u51b3\u8005\uff08\u5982\u5965\u8d5b\u6216\u7f16\u7a0b\u7ade\u8d5b\u56e2\u961f\uff09\u80fd\u591f\u5229\u7528\u4e30\u5bcc\u7684\u7ecf\u9a8c\uff1a\u5438\u6536\u6559\u7ec3\u7684\u6307\u5bfc\u3001\u4ece\u8fc7\u53bb\u95ee\u9898\u4e2d\u57f9\u517b\u76f4\u89c9\u3001\u5229\u7528\u5de5\u5177\u548c\u5e93\u529f\u80fd\u7684\u77e5\u8bc6\u3001\u6839\u636e\u540c\u4f34\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u7ecf\u9a8c\u8c03\u6574\u7b56\u7565\u3001\u901a\u8fc7\u8bd5\u9519\u4e0d\u65ad\u4f18\u5316\u63a8\u7406\uff0c\u751a\u81f3\u5728\u6bd4\u8d5b\u4e2d\u5b66\u4e60\u5176\u4ed6\u76f8\u5173\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86Xolver\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u4e3a\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u914d\u5907\u4e86\u6301\u7eed\u6f14\u5316\u7684\u6574\u4f53\u7ecf\u9a8c\u8bb0\u5fc6\u3002Xolver\u6574\u5408\u4e86\u591a\u6837\u5316\u7684\u7ecf\u9a8c\u6a21\u6001\uff0c\u5305\u62ec\u5916\u90e8\u548c\u81ea\u6211\u68c0\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u3001\u534f\u4f5c\u4e92\u52a8\u3001\u667a\u80fd\u4f53\u9a71\u52a8\u8bc4\u4f30\u548c\u8fed\u4ee3\u4f18\u5316\u3002\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u5b66\u4e60\u76f8\u5173\u7b56\u7565\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u62bd\u8c61\u63a8\u7406\u6a21\u5f0f\uff0cXolver\u907f\u514d\u4e86\u4ece\u96f6\u751f\u6210\u89e3\u51b3\u65b9\u6848\u2014\u2014\u6807\u5fd7\u7740\u4ece\u5b64\u7acb\u63a8\u7406\u5411\u7ecf\u9a8c\u611f\u77e5\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u8f6c\u53d8\u3002\u57fa\u4e8e\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u6784\u5efa\u7684Xolver\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4e13\u7528\u63a8\u7406\u667a\u80fd\u4f53\u3002\u5373\u4f7f\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u6a21\u578b\uff08\u5982QWQ-32B\uff09\uff0c\u5b83\u4e5f\u5e38\u5e38\u8d85\u8d8a\u5305\u62ecQwen3-235B\u3001Gemini 2.5 Pro\u3001o3\u548co4-mini-high\u5728\u5185\u7684\u5148\u8fdb\u6a21\u578b\u3002\u4f7f\u7528o3-mini-high\u65f6\uff0c\u5b83\u5728GSM8K\uff0898.1%\uff09\u3001AIME'24\uff0894.4%\uff09\u3001AIME'25\uff0893.7%\uff09\u3001Math-500\uff0899.8%\uff09\u548cLiveCodeBench-V5\uff0891.6%\uff09\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6210\u7ee9\u2014\u2014\u51f8\u663e\u4e86\u6574\u4f53\u7ecf\u9a8c\u5b66\u4e60\u4f5c\u4e3a\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u63a8\u7406\u901a\u7528\u667a\u80fd\u4f53\u7684\u5173\u952e\u6b65\u9aa4\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://kagnlp.github.io/xolver.github.io/\u83b7\u53d6\u3002"}}
{"id": "2506.14255", "pdf": "https://arxiv.org/pdf/2506.14255", "abs": "https://arxiv.org/abs/2506.14255", "authors": ["Johannes Flotzinger", "Fabian Deuser", "Achref Jaziri", "Heiko Neumann", "Norbert Oswald", "Visvanathan Ramesh", "Thomas Braml"], "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?", "categories": ["cs.CV"], "comment": null, "summary": "Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces \"synth-dacl\", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asynth-dacl\u7684\u5408\u6210\u7f3a\u9677\u6570\u636e\u96c6\u6269\u5c55\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u6865\u6881\u68c0\u6d4b\u6570\u636e\u96c6dacl10k\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c24\u5176\u662f\u63d0\u5347\u88c2\u7f1d\u548c\u5b54\u6d1e\u5206\u5272\u7684\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u540e\uff0c\u6a21\u578b\u5728\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u6865\u6881\u8001\u5316\u3001\u68c0\u6d4b\u8d44\u6e90\u4e0d\u8db3\uff0c\u81ea\u52a8\u5316\u89c6\u89c9\u6865\u6881\u68c0\u6d4b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u7136\u800c\uff0c\u73b0\u6709\u6570\u636e\u96c6dacl10k\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7f3a\u9677\uff08\u5982\u88c2\u7f1d\u548c\u5b54\u6d1e\uff09\u5206\u5272\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u6269\u5c55\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86synth-dacl\uff0c\u57fa\u4e8e\u5408\u6210\u6df7\u51dd\u571f\u7eb9\u7406\u7684\u4e09\u79cd\u6570\u636e\u96c6\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e73\u8861dacl10k\u4e2d\u7684\u7c7b\u522b\u5206\u5e03\u3002\u901a\u8fc7\u5c06\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u8bad\u7ec3\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u572815\u79cd\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u6240\u6709\u5408\u6210\u6269\u5c55\u6570\u636e\u7684\u6a21\u578b\u5728\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5e73\u5747IoU\u3001F1\u5206\u6570\u3001\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u5747\u63d0\u5347\u4e862%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528dacl10k\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "synth-dacl\u901a\u8fc7\u5408\u6210\u6570\u636e\u6269\u5c55\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6865\u6881\u7f3a\u9677\u5206\u5272\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "paper_title_zh": "synth-dacl\uff1a\u5408\u6210\u7f3a\u9677\u6570\u636e\u80fd\u5426\u63d0\u5347\u771f\u5b9e\u6865\u6881\u68c0\u6d4b\u4e2d\u7684\u5206\u5272\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff1f", "abstract_zh": "\u7531\u4e8e\u6865\u6881\u8001\u5316\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u52a0\u4e4b\u68c0\u6d4b\u4eba\u5458\u548c\u8d44\u91d1\u8d44\u6e90\u4e0d\u8db3\uff0c\u8bb8\u591a\u56fd\u5bb6\u7684\u6865\u6881\u68c0\u6d4b\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u81ea\u52a8\u5316\u89c6\u89c9\u6865\u6881\u68c0\u6d4b\u7684\u5173\u952e\u4efb\u52a1\u2014\u2014\u50cf\u7d20\u7ea7\u7f3a\u9677\u548c\u5efa\u7b51\u90e8\u4ef6\u5206\u7c7b\u2014\u2014\u80fd\u591f\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u589e\u5f3a\u68c0\u6d4b\u8fc7\u7a0b\u7684\u5b89\u5168\u6027\u3002\u627f\u62c5\u6b64\u4efb\u52a1\u7684\u6a21\u578b\u5fc5\u987b\u9002\u5e94\u5404\u79cd\u73b0\u5b9e\u6761\u4ef6\uff0c\u5305\u62ec\u56fe\u50cf\u8d28\u91cf\u53d8\u5316\u548c\u80cc\u666f\u7eb9\u7406\u591a\u6837\u6027\u3002dacl10k\u662f\u76ee\u524d\u6700\u5927\u4e14\u6700\u591a\u6837\u5316\u7684\u771f\u5b9e\u6865\u6881\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u4f46\u5176\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7f3a\u9677\uff08\u5982\u88c2\u7f1d\u548c\u5b54\u6d1e\uff09\u5206\u5272\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u4e86\u201csynth-dacl\u201d\uff0c\u57fa\u4e8e\u5408\u6210\u6df7\u51dd\u571f\u7eb9\u7406\u7684\u4e09\u79cd\u6570\u636e\u96c6\u6269\u5c55\u65b9\u6cd5\uff0c\u65e8\u5728\u5e73\u8861dacl10k\u7684\u7c7b\u522b\u5206\u5e03\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u5408\u6210\u6269\u5c55\u6570\u636e\u540e\uff0c\u6a21\u578b\u572815\u79cd\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4f7f\u7528dacl10k\u548c\u6240\u6709\u5408\u6210\u6269\u5c55\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5e73\u5747IoU\u3001F1\u5206\u6570\u3001\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u4e0a\u5747\u6bd4\u4ec5\u4f7f\u7528dacl10k\u8bad\u7ec3\u7684\u6a21\u578b\u63d0\u9ad8\u4e862%\u3002"}}
{"id": "2506.14224", "pdf": "https://arxiv.org/pdf/2506.14224", "abs": "https://arxiv.org/abs/2506.14224", "authors": ["Xinyang Li", "Siqi Liu", "Bochao Zou", "Jiansheng Chen", "Huimin Ma"], "title": "From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models", "categories": ["cs.AI"], "comment": "24 pages, 22 figures, accepted at ICML 2025, project page: see https://annaisavailable.github.io/GridToM/", "summary": "As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001ToM\u6d4b\u8bd5\u6570\u636e\u96c6GridToM\uff0c\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u63d0\u5347\u5176\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4eba\u4eec\u671f\u5f85\u5176\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u80fd\u529b\u4ee5\u8f85\u52a9\u65e5\u5e38\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6a21\u578b\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u89e3\u91ca\u6027\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u591a\u6a21\u6001ToM\u6d4b\u8bd5\u6570\u636e\u96c6GridToM\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u4fe1\u5ff5\u6d4b\u8bd5\u4efb\u52a1\u548c\u591a\u89c6\u89d2\u611f\u77e5\u4fe1\u606f\u3002\u968f\u540e\u5206\u6790\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u5176\u80fd\u533a\u5206\u4e0d\u540c\u89c6\u89d2\u7684\u8ba4\u77e5\u4fe1\u606f\u3002\u6700\u540e\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u5934\u65b9\u5411\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684ToM\u8868\u73b0\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5934\u80fd\u591f\u533a\u5206\u4e0d\u540c\u89c6\u89d2\u7684\u8ba4\u77e5\u4fe1\u606f\uff0c\u9a8c\u8bc1\u4e86\u5176ToM\u80fd\u529b\u3002\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684ToM\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5185\u90e8\u673a\u5236\u9a71\u52a8\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684ToM\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63d0\u5347\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u4ece\u9ed1\u76d2\u5230\u900f\u660e\u5fc3\u667a\uff1a\u8bc4\u4f30\u548c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u667a\u7406\u8bba", "abstract_zh": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4eba\u4eec\u671f\u5f85\u5176\u80fd\u591f\u6a21\u62df\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u4ee5\u8f85\u52a9\u65e5\u5e38\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u673a\u5668ToM\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6a21\u578b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6a21\u578b\u89c6\u4e3a\u9ed1\u76d2\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5185\u90e8\u673a\u5236\u7684\u89e3\u91ca\u6027\u63a2\u7d22\u3002\u4e3a\u6b64\uff0c\u672c\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u5185\u90e8\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684ToM\u8fdb\u884c\u89e3\u91ca\u6027\u8bc4\u4f30\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001ToM\u6d4b\u8bd5\u6570\u636e\u96c6GridToM\uff0c\u5176\u4e2d\u5305\u542b\u591a\u6837\u5316\u7684\u4fe1\u5ff5\u6d4b\u8bd5\u4efb\u52a1\u548c\u591a\u89c6\u89d2\u7684\u611f\u77e5\u4fe1\u606f\u3002\u968f\u540e\uff0c\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u5934\u80fd\u591f\u533a\u5206\u4e0d\u540c\u89c6\u89d2\u7684\u8ba4\u77e5\u4fe1\u606f\uff0c\u4e3aToM\u80fd\u529b\u63d0\u4f9b\u4e86\u8bc1\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u5934\u7684\u65b9\u5411\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u51fa\u7684ToM\u80fd\u529b\u3002"}}
{"id": "2506.14235", "pdf": "https://arxiv.org/pdf/2506.14235", "abs": "https://arxiv.org/abs/2506.14235", "authors": ["Yimin Deng", "Yuxia Wu", "Yejing Wang", "Guoshuai Zhao", "Li Zhu", "Qidong Liu", "Derong Xu", "Zichuan Fu", "Xian Wu", "Yefeng Zheng", "Xiangyu Zhao", "Xueming Qian"], "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs", "categories": ["cs.CL"], "comment": "ACL25 findings", "summary": "Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e13\u5bb6\u7ed3\u6784-\u8bed\u4e49\u6df7\u5408\u6846\u67b6\uff08MESH\uff09\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u5386\u53f2\u4e0e\u975e\u5386\u53f2\u4e8b\u4ef6\u7684\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u4ec5\u5173\u6ce8\u56fe\u7ed3\u6784\u5b66\u4e60\u6216\u8bed\u4e49\u63a8\u7406\uff0c\u672a\u80fd\u6574\u5408\u53cc\u91cd\u63a8\u7406\u89c6\u89d2\uff0c\u4e14\u65e0\u6cd5\u533a\u5206\u5386\u53f2\u4e0e\u975e\u5386\u53f2\u4e8b\u4ef6\u7684\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u65f6\u5e8f\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faMESH\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u4e13\u5bb6\u6a21\u5757\u6574\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u4e0d\u540c\u4e8b\u4ef6\u63d0\u4f9b\u63a8\u7406\u6307\u5bfc\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMESH\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "MESH\u6846\u67b6\u901a\u8fc7\u591a\u4e13\u5bb6\u6a21\u5757\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u7ed3\u6784-\u8bed\u4e49\u6574\u5408\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "\u4e00\u79cd\u591a\u4e13\u5bb6\u7ed3\u6784-\u8bed\u4e49\u6df7\u5408\u6846\u67b6\uff1a\u63ed\u793a\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5386\u53f2\u6a21\u5f0f", "abstract_zh": "\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65e8\u5728\u5229\u7528\u73b0\u6709\u4e8b\u5b9e\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u7ed3\u6784\u5b66\u4e60\u6216\u8bed\u4e49\u63a8\u7406\uff0c\u672a\u80fd\u6574\u5408\u53cc\u91cd\u63a8\u7406\u89c6\u89d2\u4ee5\u5e94\u5bf9\u4e0d\u540c\u9884\u6d4b\u573a\u666f\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5386\u53f2\u4e0e\u975e\u5386\u53f2\u4e8b\u4ef6\u4e4b\u95f4\u7684\u56fa\u6709\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u65f6\u5e8f\u4e0a\u4e0b\u6587\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e13\u5bb6\u7ed3\u6784-\u8bed\u4e49\u6df7\u5408\uff08MESH\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u4e13\u5bb6\u6a21\u5757\u6574\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u4e0d\u540c\u4e8b\u4ef6\u63d0\u4f9b\u63a8\u7406\u6307\u5bfc\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.14256", "pdf": "https://arxiv.org/pdf/2506.14256", "abs": "https://arxiv.org/abs/2506.14256", "authors": ["Deepak Ghimire", "Joonwhoan Lee"], "title": "Comparison of Two Methods for Stationary Incident Detection Based on Background Image", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u57fa\u4e8e\u80cc\u666f\u51cf\u9664\u7684\u9759\u6001\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5206\u522b\u91c7\u7528\u5355\u80cc\u666f\u548c\u53cc\u80cc\u666f\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5f52\u4e00\u5316\u4e92\u76f8\u5173\u56fe\u50cf\u5bf9\u6bd4\u5b9e\u73b0\u5b9e\u65f6\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u80cc\u666f\u51cf\u9664\u65b9\u6cd5\u4e3b\u8981\u7528\u4e8e\u52a8\u6001\u7269\u4f53\u68c0\u6d4b\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9759\u6001\u7269\u4f53\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u906e\u6321\u3001\u77ed\u65f6\u5b8c\u5168\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u7b49\u590d\u6742\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6848\uff1a\u4e00\u662f\u57fa\u4e8e\u5355\u80cc\u666f\u7684\u9759\u6001\u7269\u4f53\u68c0\u6d4b\uff0c\u4e8c\u662f\u57fa\u4e8e\u53cc\u80cc\u666f\uff08\u4e0d\u540c\u5b66\u4e60\u7387\u751f\u6210\uff09\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u5f52\u4e00\u5316\u4e92\u76f8\u5173\uff08NCC\uff09\u8fdb\u884c\u56fe\u50cf\u5bf9\u6bd4\u4ee5\u5b9e\u73b0\u8ddf\u8e2a\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u90e8\u5206\u906e\u6321\u3001\u77ed\u65f6\u5b8c\u5168\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u4e0b\u8868\u73b0\u9c81\u68d2\uff0c\u4e14\u80fd\u5b9e\u65f6\u8fd0\u884c\u3002\u53cc\u80cc\u666f\u65b9\u6cd5\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u80cc\u666f\u65b9\u6cd5\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u7565\u9ad8\u3002", "conclusion": "\u53cc\u80cc\u666f\u65b9\u6cd5\u5728\u9759\u6001\u7269\u4f53\u68c0\u6d4b\u4e2d\u66f4\u5177\u4f18\u52bf\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u4f7f\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "paper_title_zh": "\u57fa\u4e8e\u80cc\u666f\u56fe\u50cf\u7684\u9759\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e24\u79cd\u65b9\u6cd5\u6bd4\u8f83", "abstract_zh": "\u901a\u5e38\uff0c\u57fa\u4e8e\u80cc\u666f\u51cf\u9664\u7684\u65b9\u6cd5\u7528\u4e8e\u89c6\u89c9\u8ddf\u8e2a\u5e94\u7528\u4e2d\u7684\u52a8\u6001\u7269\u4f53\u68c0\u6d4b\u3002\u672c\u6587\u91c7\u7528\u57fa\u4e8e\u80cc\u666f\u51cf\u9664\u7684\u65b9\u6848\u68c0\u6d4b\u6682\u65f6\u9759\u6001\u7684\u7269\u4f53\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u9759\u6001\u7269\u4f53\u68c0\u6d4b\u65b9\u6848\uff0c\u5e76\u4ece\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e24\u65b9\u9762\u8fdb\u884c\u6bd4\u8f83\u3002\u7b2c\u4e00\u79cd\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u80cc\u666f\uff0c\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u4f7f\u7528\u53cc\u80cc\u666f\uff08\u901a\u8fc7\u4e0d\u540c\u5b66\u4e60\u7387\u751f\u6210\uff09\u4ee5\u68c0\u6d4b\u6682\u65f6\u505c\u6b62\u7684\u7269\u4f53\u3002\u6700\u540e\uff0c\u91c7\u7528\u57fa\u4e8e\u5f52\u4e00\u5316\u4e92\u76f8\u5173\uff08NCC\uff09\u7684\u56fe\u50cf\u5bf9\u6bd4\u6765\u76d1\u63a7\u548c\u8ddf\u8e2a\u89c6\u9891\u573a\u666f\u4e2d\u68c0\u6d4b\u5230\u7684\u9759\u6001\u7269\u4f53\u3002\u6240\u63d0\u65b9\u6cd5\u5bf9\u90e8\u5206\u906e\u6321\u3001\u77ed\u65f6\u5b8c\u5168\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5b9e\u65f6\u8fd0\u884c\u3002"}}
{"id": "2506.14231", "pdf": "https://arxiv.org/pdf/2506.14231", "abs": "https://arxiv.org/abs/2506.14231", "authors": ["Omri Haller", "Yair Meidan", "Dudu Mimran", "Yuval Elovici", "Asaf Shabtai"], "title": "ImpReSS: Implicit Recommender System for Support Conversations", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aImpReSS\u7684\u9690\u5f0f\u63a8\u8350\u7cfb\u7edf\uff0c\u4e13\u4e3a\u5ba2\u670d\u5bf9\u8bdd\u8bbe\u8ba1\uff0c\u80fd\u591f\u5728\u7528\u6237\u62a5\u544a\u95ee\u9898\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u7684\u8fc7\u7a0b\u4e2d\uff0c\u9690\u5f0f\u63a8\u8350\u76f8\u5173\u4ea7\u54c1\u7c7b\u522b\u4ee5\u89e3\u51b3\u95ee\u9898\u6216\u9884\u9632\u95ee\u9898\u590d\u53d1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0c\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5ba2\u670d\u9886\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u5982\u4f55\u5728\u5ba2\u670d\u5bf9\u8bdd\u4e2d\u9690\u5f0f\u96c6\u6210\u63a8\u8350\u529f\u80fd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5047\u8bbe\u7528\u6237\u8d2d\u4e70\u610f\u56fe\u7684\u9690\u5f0f\u63a8\u8350\u7cfb\u7edf\u3002", "method": "ImpReSS\u662f\u4e00\u79cd\u4e0e\u73b0\u6709\u5ba2\u670d\u804a\u5929\u673a\u5668\u4eba\u534f\u540c\u5de5\u4f5c\u7684\u9690\u5f0f\u63a8\u8350\u7cfb\u7edf\u3002\u5b83\u901a\u8fc7\u5206\u6790\u5ba2\u670d\u5bf9\u8bdd\u5185\u5bb9\uff0c\u8bc6\u522b\u5e76\u63a8\u8350\u76f8\u5173\u7684\u89e3\u51b3\u65b9\u6848\u4ea7\u54c1\u7c7b\u522b\uff08SPCs\uff09\uff0c\u4ee5\u5e2e\u52a9\u89e3\u51b3\u95ee\u9898\u6216\u9884\u9632\u95ee\u9898\u590d\u53d1\u3002\u4e0e\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4e0d\u540c\uff0cImpReSS\u5b8c\u5168\u9690\u5f0f\u8fd0\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cImpReSS\u5728\u63a8\u8350\u76f8\u5173SPCs\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u901a\u7528\u95ee\u9898\u89e3\u51b3\u3001\u4fe1\u606f\u5b89\u5168\u652f\u6301\u548c\u7f51\u7edc\u5b89\u5168\u6545\u969c\u6392\u9664\u4efb\u52a1\u4e2d\uff0cMRR@1\uff08\u548crecall@3\uff09\u5206\u522b\u8fbe\u52300.72\uff080.89\uff09\u30010.82\uff080.83\uff09\u548c0.85\uff080.67\uff09\u3002", "conclusion": "ImpReSS\u5c55\u793a\u4e86\u5728\u5ba2\u670d\u5bf9\u8bdd\u4e2d\u9690\u5f0f\u63a8\u8350\u76f8\u5173\u4ea7\u54c1\u7c7b\u522b\u7684\u6f5c\u529b\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u89e3\u51b3\u95ee\u9898\uff0c\u8fd8\u80fd\u4fc3\u8fdb\u4e1a\u52a1\u589e\u957f\u3002\u672a\u6765\u7814\u7a76\u53ef\u901a\u8fc7\u516c\u5f00\u6570\u636e\u548c\u4ee3\u7801\u8fdb\u4e00\u6b65\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "paper_title_zh": "ImpReSS\uff1a\u9762\u5411\u5ba2\u670d\u5bf9\u8bdd\u7684\u9690\u5f0f\u63a8\u8350\u7cfb\u7edf", "abstract_zh": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\u5df2\u901a\u8fc7\u81ea\u52a8\u5316\u4ea4\u4e92\u548c\u63d0\u4f9b\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u7684\u670d\u52a1\u6539\u53d8\u4e86\u5ba2\u6237\u652f\u6301\u9886\u57df\u3002\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff08CRSs\uff09\u56e0\u5176\u63d0\u5347\u63a8\u8350\u8d28\u91cf\u7684\u80fd\u529b\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5173\u4e8e\u5982\u4f55\u5728\u5ba2\u6237\u652f\u6301\u4ea4\u4e92\u4e2d\u9690\u5f0f\u96c6\u6210\u63a8\u8350\u7684\u7814\u7a76\u4ecd\u6709\u9650\u3002\u672c\u6587\u63d0\u51faImpReSS\uff0c\u4e00\u79cd\u4e13\u4e3a\u5ba2\u670d\u5bf9\u8bdd\u8bbe\u8ba1\u7684\u9690\u5f0f\u63a8\u8350\u7cfb\u7edf\u3002ImpReSS\u4e0e\u73b0\u6709\u652f\u6301\u804a\u5929\u673a\u5668\u4eba\u534f\u540c\u5de5\u4f5c\uff0c\u7528\u6237\u5728\u5bf9\u8bdd\u4e2d\u62a5\u544a\u95ee\u9898\uff0c\u804a\u5929\u673a\u5668\u4eba\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8e\u5ba2\u670d\u5bf9\u8bdd\u5185\u5bb9\uff0cImpReSS\u8bc6\u522b\u5e76\u63a8\u8350\u76f8\u5173\u7684\u89e3\u51b3\u65b9\u6848\u4ea7\u54c1\u7c7b\u522b\uff08SPCs\uff09\uff0c\u4ee5\u5e2e\u52a9\u89e3\u51b3\u95ee\u9898\u6216\u9884\u9632\u5176\u590d\u53d1\u2014\u2014\u540c\u65f6\u4e5f\u652f\u6301\u4e1a\u52a1\u589e\u957f\u3002\u4e0e\u4f20\u7edfCRSs\u4e0d\u540c\uff0cImpReSS\u5b8c\u5168\u9690\u5f0f\u8fd0\u4f5c\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4efb\u4f55\u7528\u6237\u8d2d\u4e70\u610f\u56fe\u7684\u5047\u8bbe\u3002\u6211\u4eec\u5bf9ImpReSS\u5728\u63a8\u8350\u76f8\u5173SPCs\u4ee5\u89e3\u51b3\u5ba2\u670d\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u7684\u95ee\u9898\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u901a\u7528\u95ee\u9898\u89e3\u51b3\u7684MRR@1\uff08\u548crecall@3\uff09\u4e3a0.72\uff080.89\uff09\u3001\u4fe1\u606f\u5b89\u5168\u652f\u6301\u4e3a0.82\uff080.83\uff09\u3001\u7f51\u7edc\u5b89\u5168\u6545\u969c\u6392\u9664\u4e3a0.85\uff080.67\uff09\u3002\u4e3a\u652f\u6301\u672a\u6765\u7814\u7a76\uff0c\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u5c06\u6839\u636e\u8bf7\u6c42\u5171\u4eab\u3002"}}
{"id": "2506.14248", "pdf": "https://arxiv.org/pdf/2506.14248", "abs": "https://arxiv.org/abs/2506.14248", "authors": ["Chenghao Li", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yibing Zhan"], "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5de5\u5177\u4ee4\u724c\u4e0e\u73b0\u6709\u8bcd\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u6570\u503c\u63a8\u7406\u3001\u89c4\u5212\u751f\u6210\uff09\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u6570\u503c\u63a8\u7406\u3001\u89c4\u5212\u751f\u6210\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u901a\u8fc7\u5916\u90e8\u5de5\u5177\uff08\u5982\u8ba1\u7b97\u5668\u3001\u6570\u636e\u5e93\uff09\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u5de5\u5177\u5206\u914d\u552f\u4e00\u4ee4\u724c\uff0c\u5ffd\u7565\u4e86\u5de5\u5177\u4ee4\u724c\u4e0e\u8bcd\u4ee4\u724c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5de5\u5177\u540d\u79f0\u6216\u63cf\u8ff0\u6784\u5efa\u5148\u9a8c\u4ee4\u724c\u5d4c\u5165\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u548c\u6b63\u5219\u5316\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u4ee4\u724c\u5d4c\u5165\uff0c\u786e\u4fdd\u5176\u4e0e\u8bcd\u4ee4\u724c\u7a7a\u95f4\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728GSM8K-XL\u3001FuncQA\u3001KAMEL\u548cVirtualHome\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u503c\u63a8\u7406\u3001\u77e5\u8bc6\u95ee\u7b54\u548c\u89c4\u5212\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eCoT\u3001REACT\u3001ICL\u548cToolkenGPT\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5de5\u5177\u4ee4\u724c\u4e0e\u8bcd\u4ee4\u724c\u7a7a\u95f4\u5bf9\u9f50\uff0c\u672c\u6587\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5de5\u5177\u589e\u5f3a\u7684LLMs\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5de5\u5177\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u65b0\u521d\u59cb\u5316\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u6570\u503c\u63a8\u7406\u3001\u89c4\u5212\u751f\u6210\uff09\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u5c06\u5916\u90e8\u5de5\u5177\uff08\u5982\u8ba1\u7b97\u5668\u548c\u6570\u636e\u5e93\uff09\u96c6\u6210\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u5bf9\u63d0\u5347\u5176\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u5de5\u5177\u5206\u914d\u552f\u4e00\u4ee4\u724c\uff0c\u4f7fLLMs\u80fd\u591f\u901a\u8fc7\u4ee4\u724c\u9884\u6d4b\u8c03\u7528\u5de5\u5177\uff0c\u7c7b\u4f3c\u4e8e\u8bcd\u751f\u6210\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u672a\u8003\u8651\u5de5\u5177\u4ee4\u724c\u4e0e\u8bcd\u4ee4\u724c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u9884\u8bad\u7ec3LLMs\u7684\u9002\u5e94\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u521d\u59cb\u5316\u7684\u89d2\u5ea6\u5c06\u5de5\u5177\u4ee4\u724c\u4e0e\u73b0\u6709\u8bcd\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u6211\u4eec\u9996\u5148\u57fa\u4e8e\u5de5\u5177\u540d\u79f0\u6216\u63cf\u8ff0\u4e3a\u6bcf\u4e2a\u5de5\u5177\u6784\u5efa\u5148\u9a8c\u4ee4\u724c\u5d4c\u5165\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u548c\u6b63\u5219\u5316\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u4ee4\u724c\u5d4c\u5165\uff0c\u786e\u4fdd\u5176\u4e0e\u8bcd\u4ee4\u724c\u7a7a\u95f4\u5bf9\u9f50\uff0c\u63d0\u9ad8\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u3002\u6211\u4eec\u5728GSM8K-XL\u3001FuncQA\u3001KAMEL\u548cVirtualHome\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u5728\u6570\u503c\u63a8\u7406\u3001\u77e5\u8bc6\u95ee\u7b54\u548c\u89c4\u5212\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8eCoT\u3001REACT\u3001ICL\u548cToolkenGPT\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u76f8\u5173\u4ee4\u724c\u6709\u6548\u589e\u5f3a\u4e86LLMs\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u3002"}}
{"id": "2506.14265", "pdf": "https://arxiv.org/pdf/2506.14265", "abs": "https://arxiv.org/abs/2506.14265", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling", "categories": ["cs.CV"], "comment": "CVPR 2025 Computer Vision for Drug Discovery", "summary": "Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u7ec6\u80de\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u7ec6\u80de\u56fe\u50cf\u7684SSLProfiler\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u591a\u56fe\u50cf\u8f93\u5165\u95ee\u9898\uff0c\u5e76\u5728CVPR 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u80dc\u3002", "motivation": "\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u56e0\u7ec6\u80de\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u5206\u5e03\u5dee\u5f02\u5927\u4e14\u9700\u5904\u7406\u591a\u56fe\u50cf\u8f93\u5165\u800c\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9002\u7528\u4e8e\u7ec6\u80de\u56fe\u50cf\u7684\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u63d0\u51faSSLProfiler\u6846\u67b6\uff0c\u5305\u542b\u9488\u5bf9\u7ec6\u80de\u56fe\u50cf\u7684\u4e13\u7528\u6570\u636e\u589e\u5f3a\u548c\u8868\u5f81\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u56fe\u751f\u6210\u548c\u4fe1\u606f\u6574\u5408\u4e0a\u7684\u4e0d\u8db3\u3002", "result": "SSLProfiler\u5728CVPR 2025\u7684Cell Line Transferability\u6311\u6218\u8d5b\u4e2d\u83b7\u80dc\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7ec6\u80de\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "SSLProfiler\u4e3a\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u63d0\u53d6\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "\u63a2\u7d22\u57fa\u4e8e\u56fe\u50cf\u7684\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u5728\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u57fa\u4e8e\u56fe\u50cf\u7684\u7ec6\u80de\u5206\u6790\u65e8\u5728\u4e3a\u7ec6\u80de\u56fe\u50cf\u521b\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u8868\u5f81\u3002\u8fd9\u4e00\u6280\u672f\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u968f\u7740\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8fdb\u6b65\u53d6\u5f97\u4e86\u663e\u8457\u53d1\u5c55\u3002\u53d7\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u672c\u6587\u521d\u6b65\u63a2\u7d22\u4e86\u4f7f\u7528\u6b64\u7c7b\u65b9\u6cd5\u8bad\u7ec3\u7ec6\u80de\u56fe\u50cf\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\u7684\u53ef\u80fd\u6027\u3002\u7136\u800c\uff0c\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a1\uff09\u7ec6\u80de\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u7684\u5206\u5e03\u5dee\u5f02\u8f83\u5927\uff0c\u5bfc\u81f4\u73b0\u6709SSL\u65b9\u6cd5\u4e2d\u7684\u89c6\u56fe\u751f\u6210\u8fc7\u7a0b\u5931\u6548\uff1b2\uff09\u4e0e\u5178\u578b\u573a\u666f\u4e2d\u6bcf\u4e2a\u8868\u5f81\u57fa\u4e8e\u5355\u5f20\u56fe\u50cf\u4e0d\u540c\uff0c\u7ec6\u80de\u5206\u6790\u901a\u5e38\u6d89\u53ca\u591a\u5f20\u8f93\u5165\u56fe\u50cf\uff0c\u96be\u4ee5\u6709\u6548\u6574\u5408\u6240\u6709\u53ef\u7528\u4fe1\u606f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SSLProfiler\uff0c\u4e00\u4e2a\u4e13\u4e3a\u7ec6\u80de\u5206\u6790\u8bbe\u8ba1\u7684\u975e\u5bf9\u6bd4SSL\u6846\u67b6\u3002\u6211\u4eec\u5f15\u5165\u4e86\u9488\u5bf9\u7ec6\u80de\u56fe\u50cf\u7684\u4e13\u7528\u6570\u636e\u589e\u5f3a\u548c\u8868\u5f81\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u8ff0\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u7279\u5f81\u63d0\u53d6\u5668\u3002\u901a\u8fc7\u8fd9\u4e9b\u6539\u8fdb\uff0cSSLProfiler\u5728CVPR 2025\u7684Cell Line Transferability\u6311\u6218\u8d5b\u4e2d\u83b7\u80dc\u3002"}}
{"id": "2506.14239", "pdf": "https://arxiv.org/pdf/2506.14239", "abs": "https://arxiv.org/abs/2506.14239", "authors": ["Louis Vervoort", "Vitaly Nikolaev"], "title": "Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by Journal for General Philosophy of Science", "summary": "We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54f2\u5b66\u56e0\u679c\u5173\u7cfb\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u3001DeepSeek\u548cGemini\uff09\u7684\u62bd\u8c61\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u6587\u732e\u4e2d\u4e89\u8bae\u7684\u56e0\u679c\u5173\u7cfb\u6848\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5e7f\u6cdb\u6709\u6548\u7684\u795e\u7ecf\u5143\u56fe\u56e0\u679c\u5b9a\u4e49\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\uff08\u5c24\u5176\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5728\u62bd\u8c61\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5229\u7528\u54f2\u5b66\u4e2d\u7684\u795e\u7ecf\u5143\u56fe\u7406\u8bba\u8bbe\u8ba1\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u9a8c\u8bc1\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u56e0\u679c\u5173\u7cfb\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u54f2\u5b66\u4e2d\u7684\u795e\u7ecf\u5143\u56fe\u7406\u8bba\uff08\u7531D. Lewis\u63a8\u5e7f\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u56e0\u679c\u5173\u7cfb\u65f6\u7684\u8868\u73b0\u3002\u6d4b\u8bd5\u5bf9\u8c61\u5305\u62ecChatGPT\u3001DeepSeek\u548cGemini\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u6587\u732e\u4e2d\u4e89\u8bae\u7684\u56e0\u679c\u5173\u7cfb\u6848\u4f8b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u5143\u56fe\u56e0\u679c\u5b9a\u4e49\uff0c\u5176\u6709\u6548\u6027\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5c55\u793a\u4e86\u672a\u6765\u54f2\u5b66\u7814\u7a76\u53ef\u80fd\u7684\u53d1\u5c55\u65b9\u5411\uff1a\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5728\u4e13\u4e1a\u77e5\u8bc6\u4e0a\u7684\u4e92\u52a8\u4e0e\u5408\u4f5c\u3002", "paper_title_zh": "\u795e\u7ecf\u5143\u56fe\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u6d4b\u8bd5\uff1a\u54f2\u5b66\u672a\u6765\u7684\u66d9\u5149\uff1f", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54f2\u5b66\u56e0\u679c\u5173\u7cfb\u7814\u7a76\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7684\u62bd\u8c61\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u501f\u9274\u4e86D. Lewis\u63a8\u5e7f\u7684\u795e\u7ecf\u5143\u56fe\u7406\u8bba\u3002\u6211\u4eec\u4ee5\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u3001DeepSeek\u548cGemini\uff09\u4e3a\u4f8b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u8fd9\u4e9b\u804a\u5929\u673a\u5668\u4eba\u5df2\u7ecf\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u6587\u732e\u4e2d\u4e89\u8bae\u7684\u56e0\u679c\u5173\u7cfb\u6848\u4f8b\u3002\u4e3a\u4e86\u8bc4\u4f30\u8fd9\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ca\u672a\u6765\u4e13\u7528\u4eba\u5de5\u667a\u80fd\u7684\u8868\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5143\u56fe\u4e2d\u56e0\u679c\u5173\u7cfb\u7684\u5b9a\u4e49\uff0c\u5176\u6709\u6548\u6027\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u7814\u7a76\uff0c\u6311\u6218\u4e86\u5173\u4e8e\u6b64\u7c7b\u5b9a\u4e49\u96be\u4ee5\u6349\u6478\u7684\u666e\u904d\u89c2\u70b9\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86\u672a\u6765\u54f2\u5b66\u7814\u7a76\u53ef\u80fd\u7684\u53d1\u5c55\u65b9\u5411\uff1a\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5728\u4e13\u4e1a\u77e5\u8bc6\u4e0a\u7684\u4e92\u52a8\u4e0e\u5408\u4f5c\u3002"}}
{"id": "2506.14285", "pdf": "https://arxiv.org/pdf/2506.14285", "abs": "https://arxiv.org/abs/2506.14285", "authors": ["Seongbo Jang", "Minjin Jeon", "Jaehoon Lee", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u2014\u2014\u53ca\u65f6\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e86TimelyChat\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u9002\u5f53\u65f6\u95f4\u95f4\u9694\u548c\u751f\u6210\u65f6\u95f4\u6761\u4ef6\u54cd\u5e94\u65b9\u9762\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5e38\u8bc6\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3Timer\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u8fde\u8d2f\u54cd\u5e94\u751f\u6210\uff0c\u800c\u5ffd\u7565\u4e86\u57fa\u4e8e\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u54cd\u5e94\u65f6\u673a\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5982\u4f55\u751f\u6210\u7b26\u5408\u65f6\u95f4\u6761\u4ef6\u7684\u53ca\u65f6\u54cd\u5e94\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u53ca\u65f6\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86TimelyChat\u57fa\u51c6\u3002\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5e38\u8bc6\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u672a\u6807\u6ce8\u4e8b\u4ef6\u77e5\u8bc6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4e8655K\u4e8b\u4ef6\u9a71\u52a8\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u3002\u968f\u540e\u8bad\u7ec3\u4e86Timer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u4e3b\u52a8\u9884\u6d4b\u65f6\u95f4\u95f4\u9694\u5e76\u751f\u6210\u7b26\u5408\u65f6\u95f4\u6761\u4ef6\u7684\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTimer\u5728\u8f6e\u6b21\u7ea7\u522b\u548c\u5bf9\u8bdd\u7ea7\u522b\u7684\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684LLM\u548c\u5176\u4ed6\u5fae\u8c03\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u6761\u4ef6\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u548cTimer\u6a21\u578b\uff0c\u4e3a\u5bf9\u8bdd\u4ee3\u7406\u7684\u53ca\u65f6\u54cd\u5e94\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u3002", "paper_title_zh": "\u4ece\u751f\u6210\u4ec0\u4e48\u5230\u4f55\u65f6\u751f\u6210\uff1a\u5f00\u653e\u57df\u5bf9\u8bdd\u4ee3\u7406\u7684\u53ca\u65f6\u54cd\u5e94\u751f\u6210", "abstract_zh": "\u5c3d\u7ba1\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u57fa\u4e8e\u6587\u672c\u4e0a\u4e0b\u6587\u751f\u6210\u8fde\u8d2f\u54cd\u5e94\u4e0a\uff0c\u4f46\u57fa\u4e8e\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u4f55\u65f6\u54cd\u5e94\u7684\u5173\u952e\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53ca\u65f6\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86TimelyChat\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u9002\u5f53\u65f6\u95f4\u95f4\u9694\u548c\u751f\u6210\u65f6\u95f4\u6761\u4ef6\u54cd\u5e94\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5e38\u8bc6\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u672a\u6807\u6ce8\u4e8b\u4ef6\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5408\u6210\u4e8655K\u4e8b\u4ef6\u9a71\u52a8\u7684\u5bf9\u8bdd\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u968f\u540e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86Timer\uff0c\u8fd9\u662f\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u9884\u6d4b\u65f6\u95f4\u95f4\u9694\u5e76\u751f\u6210\u7b26\u5408\u8fd9\u4e9b\u95f4\u9694\u7684\u53ca\u65f6\u54cd\u5e94\u7684\u5bf9\u8bdd\u4ee3\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTimer\u5728\u8f6e\u6b21\u7ea7\u522b\u548c\u5bf9\u8bdd\u7ea7\u522b\u7684\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684LLM\u548c\u5176\u4ed6\u5fae\u8c03\u57fa\u7ebf\u6a21\u578b\u3002\u6211\u4eec\u516c\u5f00\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u3002"}}
{"id": "2506.14271", "pdf": "https://arxiv.org/pdf/2506.14271", "abs": "https://arxiv.org/abs/2506.14271", "authors": ["Weiming Zhang", "Dingwen Xiao", "Aobotao Dai", "Yexin Liu", "Tianbo Pan", "Shiqi Wen", "Lei Chen", "Lin Wang"], "title": "Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment", "categories": ["cs.CV"], "comment": "23 pages, 16 figures", "summary": "360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Leader360V\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u6807\u6ce8\u771f\u5b9e\u4e16\u754c360\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b9e\u4f8b\u5206\u5272\u548c\u8ddf\u8e2a\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u7ed3\u54082D\u5206\u5272\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86360\u89c6\u9891\u6807\u6ce8\u7684\u590d\u6742\u6027\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "360\u89c6\u9891\u56e0\u5176\u8d85\u5e7f\u89c6\u89d2\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u3002\u7403\u5f62\u7279\u6027\uff08\u5982\u6781\u533a\u4e25\u91cd\u5931\u771f\u548c\u5185\u5bb9\u4e0d\u8fde\u7eed\uff09\u4f7f\u5f97\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u5206\u4e09\u9636\u6bb5\uff1a\u521d\u59cb\u6807\u6ce8\u9636\u6bb5\u7ed3\u54082D\u5206\u5272\u5668\u548cLLM\u751f\u6210\u8bed\u4e49\u6807\u7b7e\uff1b\u81ea\u52a8\u4f18\u5316\u9636\u6bb5\u4fee\u6b63\u7f3a\u5931\u6216\u4e0d\u5b8c\u6574\u533a\u57df\uff1b\u4eba\u5de5\u4fee\u8ba2\u9636\u6bb5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u6807\u6ce8\u3002", "result": "\u7528\u6237\u7814\u7a76\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u6807\u6ce8\u6d41\u7a0b\u9ad8\u6548\u4e14Leader360V\u663e\u8457\u63d0\u5347\u4e86360\u89c6\u9891\u5206\u5272\u548c\u8ddf\u8e2a\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Leader360V\u4e3a360\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u89e3\u51b3\u4e86\u6807\u6ce8\u96be\u9898\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53ef\u6269\u5c55\u53d1\u5c55\u3002", "paper_title_zh": "Leader360V\uff1a\u9762\u5411\u591a\u6837\u5316\u73af\u5883\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c360\u89c6\u9891\u6570\u636e\u96c6", "abstract_zh": "360\u89c6\u9891\u4ee5360X180\u7684\u8d85\u5e7f\u89c6\u89d2\u6355\u6349\u5b8c\u6574\u573a\u666f\uff0c\u4f7f\u5f97\u5206\u5272\u548c\u8ddf\u8e2a\u7b49\u4efb\u52a1\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7403\u5f62\u7279\u6027\uff08\u5982\u6781\u533a\u4e25\u91cd\u5931\u771f\u548c\u5185\u5bb9\u4e0d\u8fde\u7eed\uff09\uff0c\u6807\u6ce8\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u672c\u6587\u63d0\u51faLeader360V\uff0c\u9996\u4e2a\u7528\u4e8e\u5b9e\u4f8b\u5206\u5272\u548c\u8ddf\u8e2a\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u771f\u5b9e\u4e16\u754c360\u89c6\u9891\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u6db5\u76d6\u5ba4\u5185\u3001\u57ce\u5e02\u3001\u81ea\u7136\u548c\u52a8\u6001\u6237\u5916\u573a\u666f\uff0c\u591a\u6837\u6027\u9ad8\u3002\u4e3a\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u8bbe\u8ba1\u4e86\u7ed3\u5408\u9884\u8bad\u7ec32D\u5206\u5272\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u5206\u4e09\u9636\u6bb5\uff1a\u521d\u59cb\u6807\u6ce8\u9636\u6bb5\u901a\u8fc7\u8bed\u4e49\u548c\u5931\u771f\u611f\u77e5\u4f18\u5316\u6a21\u5757\u751f\u6210\u6807\u7b7e\uff1b\u81ea\u52a8\u4f18\u5316\u9636\u6bb5\u4fee\u6b63\u7f3a\u5931\u533a\u57df\uff1b\u4eba\u5de5\u4fee\u8ba2\u9636\u6bb5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u8bc1\u660e\u6807\u6ce8\u6d41\u7a0b\u9ad8\u6548\uff0c\u4e14Leader360V\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a360\u573a\u666f\u7406\u89e3\u7684\u53ef\u6269\u5c55\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.14245", "pdf": "https://arxiv.org/pdf/2506.14245", "abs": "https://arxiv.org/abs/2506.14245", "authors": ["Xumeng Wen", "Zihan Liu", "Shun Zheng", "Zhijian Xu", "Shengyu Ye", "Zhirong Wu", "Xiao Liang", "Yang Wang", "Junjie Li", "Ziming Miao", "Jiang Bian", "Mao Yang"], "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4f20\u7edf\u8bc4\u4f30\u6307\u6807$Pass@K$\u7684\u7f3a\u9677\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u6307\u6807$CoT$-$Pass@K$\uff0c\u8bc1\u660eRLVR\u80fd\u6709\u6548\u6fc0\u52b1\u903b\u8f91\u5b8c\u6574\u6027\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RLVR\u88ab\u63d0\u51fa\u7528\u4e8e\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53d1\u73b0RLVR\u8c03\u4f18\u7684\u6a21\u578b\u5728$Pass@K$\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f15\u53d1\u4e86\u5bf9RLVR\u662f\u5426\u4ec5\u91cd\u5206\u914d\u63a8\u7406\u8def\u5f84\u800c\u975e\u63d0\u5347\u63a8\u7406\u591a\u6837\u6027\u7684\u8d28\u7591\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\uff0c\u5e76\u9a8c\u8bc1RLVR\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790$Pass@K$\u6307\u6807\u7684\u7f3a\u9677\uff0c\u63d0\u51fa\u65b0\u6307\u6807$CoT$-$Pass@K$\uff0c\u8981\u6c42\u63a8\u7406\u8def\u5f84\u548c\u6700\u7ec8\u7b54\u6848\u5747\u6b63\u786e\u3002\u4ece\u7406\u8bba\u4e0a\u8bc1\u660eRLVR\u80fd\u6fc0\u52b1\u903b\u8f91\u5b8c\u6574\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528$CoT$-$Pass@K$\u6307\u6807\u65f6\uff0cRLVR\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8fd9\u79cd\u80fd\u529b\u5728\u8bad\u7ec3\u65e9\u671f\u5373\u663e\u73b0\u5e76\u6301\u7eed\u6cdb\u5316\u3002", "conclusion": "\u672c\u6587\u660e\u786e\u4e86RLVR\u7684\u4f5c\u7528\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bc1\u5b9e\u5176\u80fd\u771f\u6b63\u63a8\u52a8\u673a\u5668\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u6b65\u3002", "paper_title_zh": "\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u9690\u5f0f\u6fc0\u52b1\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6b63\u786e\u63a8\u7406", "abstract_zh": "\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5df2\u6210\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5176\u6709\u6548\u6027\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u77db\u76fe\uff1aRLVR\u8c03\u4f18\u7684\u6a21\u578b\u5728\u89e3\u51b3\u65b9\u6848\u53d1\u73b0\u7684$Pass@K$\u6307\u6807\u4e0a\u5e38\u8868\u73b0\u4e0d\u5982\u57fa\u7840\u6a21\u578b\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u79cd\u5047\u8bbe\uff0c\u5373RLVR\u4ec5\u4ee5\u727a\u7272\u63a8\u7406\u591a\u6837\u6027\u4e3a\u4ee3\u4ef7\u91cd\u65b0\u5206\u914d\u73b0\u6709\u63a8\u7406\u8def\u5f84\u3002\u672c\u6587\u901a\u8fc7\u8bc6\u522b\u95ee\u9898\u7684\u6839\u6e90\u89e3\u51b3\u4e86\u8fd9\u4e00\u77db\u76fe\uff1a$Pass@K$\u6307\u6807\u672c\u8eab\u662f\u63a8\u7406\u7684\u7f3a\u9677\u8861\u91cf\u6807\u51c6\uff0c\u56e0\u4e3a\u5b83\u5c06\u6b63\u786e\u6700\u7ec8\u7b54\u6848\u5f52\u529f\u4e8e\u53ef\u80fd\u6e90\u4e8e\u4e0d\u51c6\u786e\u6216\u4e0d\u5b8c\u6574\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u63a8\u7406\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u66f4\u7cbe\u786e\u7684\u8bc4\u4f30\u6307\u6807$CoT$-$Pass@K$\uff0c\u8981\u6c42\u63a8\u7406\u8def\u5f84\u548c\u6700\u7ec8\u7b54\u6848\u5747\u6b63\u786e\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5f62\u5f0f\u5316\u5730\u8bf4\u660e\u4e86RLVR\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e0d\u540c\uff0c\u5176\u72ec\u7279\u7ed3\u6784\u80fd\u591f\u6fc0\u52b1\u903b\u8f91\u5b8c\u6574\u6027\u3002\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u4e86\u8fd9\u4e00\u89c2\u70b9\uff1a\u4f7f\u7528$CoT$-$Pass@K$\u6307\u6807\u65f6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230RLVR\u80fd\u591f\u6fc0\u52b1\u5bf9\u6240\u6709$K$\u503c\u7684\u6b63\u786e\u63a8\u7406\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u52a8\u6001\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u589e\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u5728\u8bad\u7ec3\u65e9\u671f\u5373\u663e\u73b0\u5e76\u5e73\u6ed1\u6cdb\u5316\u3002\u672c\u6587\u4e3aRLVR\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u89c6\u89d2\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bc1\u5b9e\u5176\u771f\u6b63\u63a8\u52a8\u673a\u5668\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14302", "pdf": "https://arxiv.org/pdf/2506.14302", "abs": "https://arxiv.org/abs/2506.14302", "authors": ["Xueyang Feng", "Jingsen Zhang", "Jiakai Tang", "Wei Li", "Guohao Cai", "Xu Chen", "Quanyu Dai", "Yue Zhu", "Zhenhua Dong"], "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aECPO\u7684\u591a\u8f6e\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u671f\u671b\u786e\u8ba4\u7406\u8bba\u5efa\u6a21\u7528\u6237\u6ee1\u610f\u5ea6\u6f14\u53d8\uff0c\u4f18\u5316\u5bf9\u8bdd\u63a8\u8350\u4ee3\u7406\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u63a8\u8350\u4ee3\u7406\uff08CRA\uff09\u5e38\u751f\u6210\u77ed\u89c6\u56de\u5e94\uff0c\u96be\u4ee5\u6301\u7eed\u5f15\u5bfc\u7528\u6237\u6216\u6ee1\u8db3\u5176\u671f\u671b\u3002\u5c3d\u7ba1\u504f\u597d\u4f18\u5316\u5728\u5355\u8f6e\u5bf9\u8bdd\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6210\u672c\u9ad8\u4e14\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u8f6e\u504f\u597d\u4f18\u5316\u8303\u5f0fECPO\uff0c\u5229\u7528\u671f\u671b\u786e\u8ba4\u7406\u8bba\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u6ee1\u610f\u5ea6\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6f14\u53d8\uff0c\u8bc6\u522b\u4e0d\u6ee1\u539f\u56e0\u5e76\u9488\u5bf9\u6027\u4f18\u5316\u3002\u5f15\u5165\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u6a21\u62df\u5668AILO\uff0c\u6a21\u62df\u7528\u6237\u53cd\u9988\u5e76\u6267\u884c\u671f\u671b\u786e\u8ba4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cECPO\u663e\u8457\u63d0\u5347\u4e86CRA\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709MTPO\u65b9\u6cd5\u3002", "conclusion": "ECPO\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u6ee1\u610f\u5ea6\u6f14\u53d8\u548c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u63a8\u8350\u4e2d\u7684\u504f\u597d\u5bf9\u9f50\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u91c7\u6837\u5f00\u9500\u3002", "paper_title_zh": "\u57fa\u4e8e\u671f\u671b\u786e\u8ba4\u7684\u591a\u8f6e\u5bf9\u8bdd\u63a8\u8350\u4ee3\u7406\u504f\u597d\u4f18\u5316\u65b9\u6cd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u663e\u8457\u63a8\u52a8\u4e86\u5bf9\u8bdd\u63a8\u8350\u4ee3\u7406\uff08CRAs\uff09\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4ee3\u7406\u5e38\u751f\u6210\u77ed\u89c6\u7684\u56de\u5e94\uff0c\u96be\u4ee5\u6301\u7eed\u5f15\u5bfc\u7528\u6237\u6216\u6ee1\u8db3\u5176\u671f\u671b\u3002\u5c3d\u7ba1\u504f\u597d\u4f18\u5316\u5728\u5355\u8f6e\u5bf9\u8bdd\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6210\u672c\u9ad8\u4e14\u6548\u679c\u4e0d\u4f73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8f6e\u504f\u597d\u4f18\u5316\uff08MTPO\uff09\u8303\u5f0fECPO\uff0c\u5229\u7528\u671f\u671b\u786e\u8ba4\u7406\u8bba\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u6ee1\u610f\u5ea6\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6f14\u53d8\uff0c\u63ed\u793a\u4e0d\u6ee1\u7684\u6f5c\u5728\u539f\u56e0\u3002\u8fd9\u4e9b\u539f\u56e0\u53ef\u7528\u4e8e\u652f\u6301\u5bf9\u4e0d\u6ee1\u610f\u56de\u5e94\u7684\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u8f6e\u7ea7\u504f\u597d\u4f18\u5316\u3002ECPO\u5de7\u5999\u5730\u6d88\u9664\u4e86\u73b0\u6709MTPO\u65b9\u6cd5\u7684\u663e\u8457\u91c7\u6837\u5f00\u9500\uff0c\u540c\u65f6\u786e\u4fdd\u4f18\u5316\u8fc7\u7a0b\u5e26\u6765\u6709\u610f\u4e49\u7684\u6539\u8fdb\u3002\u4e3a\u652f\u6301ECPO\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7528\u6237\u6a21\u62df\u5668AILO\uff0c\u7528\u4e8e\u6a21\u62df\u7528\u6237\u53cd\u9988\u5e76\u5728\u5bf9\u8bdd\u63a8\u8350\u4e2d\u6267\u884c\u671f\u671b\u786e\u8ba4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECPO\u663e\u8457\u63d0\u5347\u4e86CRA\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709MTPO\u65b9\u6cd5\u3002"}}
{"id": "2506.14322", "pdf": "https://arxiv.org/pdf/2506.14322", "abs": "https://arxiv.org/abs/2506.14322", "authors": ["Avigail Cohen Rimon", "Mirela Ben-Chen", "Or Litany"], "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to SGP 2025 (Symposium on Geometry Processing)", "summary": "We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u529f\u80fd\u6620\u5c04\uff08functional map\uff09\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5904\u7406\u529f\u80fd\u6620\u5c04\u7a7a\u95f4\uff0c\u7ed3\u5408\u70b9\u5bf9\u70b9\u6620\u5c04\u4f5c\u4e3a\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6620\u5c04\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5f62\u72b6\u5bf9\u5e94\u6620\u5c04\u65b9\u6cd5\u5728\u5904\u7406\u529f\u80fd\u6620\u5c04\u65f6\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u6216\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56fe\u50cf\u6269\u6563\u6a21\u578b\u76f4\u63a5\u4f18\u5316\u529f\u80fd\u6620\u5c04\uff0c\u63d0\u5347\u6620\u5c04\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5c06\u529f\u80fd\u6620\u5c04\u89c6\u4e3a2D\u56fe\u50cf\uff0c\u8bad\u7ec3\u4e00\u4e2a\u56fe\u50cf\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5728\u529f\u80fd\u6620\u5c04\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4f18\u5316\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u5229\u7528\u5f53\u524d\u529f\u80fd\u6620\u5c04\u5bf9\u5e94\u7684\u70b9\u5bf9\u70b9\u6620\u5c04\u4f5c\u4e3a\u5f15\u5bfc\uff0c\u540c\u65f6\u652f\u6301\u529f\u80fd\u6620\u5c04\u7684\u5176\u4ed6\u76ee\u6807\uff08\u5982\u6b63\u4ea4\u6027\u548c\u4e0e\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\u7684\u4ea4\u6362\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u529f\u80fd\u6620\u5c04\u4f18\u5316\u4efb\u52a1\u4e2d\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u7ade\u4e89\uff0c\u4e14\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4e3a\u529f\u80fd\u6620\u5c04\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002", "conclusion": "\u901a\u8fc7\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f18\u5316\u529f\u80fd\u6620\u5c04\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4e3a\u529f\u80fd\u6620\u5c04\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "paper_title_zh": "FRIDU\uff1a\u57fa\u4e8e\u5f15\u5bfc\u56fe\u50cf\u6269\u6563\u7684\u529f\u80fd\u6620\u5c04\u4f18\u5316", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u4e24\u4e2a\u5f62\u72b6\u4e4b\u95f4\u7684\u5bf9\u5e94\u6620\u5c04\u3002\u529f\u80fd\u6620\u5c04\uff08\u5373\u57fa\u53d8\u6362\u77e9\u9635\uff09\u53ef\u4ee5\u89c6\u4e3a2D\u56fe\u50cf\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c6\u89d2\uff0c\u6211\u4eec\u76f4\u63a5\u5728\u529f\u80fd\u6620\u5c04\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u57fa\u4e8e\u4e0d\u51c6\u786e\u7684\u521d\u59cb\u6620\u5c04\u751f\u6210\u7cbe\u786e\u7684\u6620\u5c04\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5b8c\u5168\u5728\u529f\u80fd\u7a7a\u95f4\u4e2d\u8fdb\u884c\uff0c\u56e0\u6b64\u6548\u7387\u6781\u9ad8\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528\u5f53\u524d\u529f\u80fd\u6620\u5c04\u5bf9\u5e94\u7684\u70b9\u5bf9\u70b9\u6620\u5c04\u4f5c\u4e3a\u6269\u6563\u8fc7\u7a0b\u7684\u5f15\u5bfc\u3002\u8fd9\u79cd\u5f15\u5bfc\u8fd8\u53ef\u4ee5\u4fc3\u8fdb\u529f\u80fd\u6620\u5c04\u7684\u5176\u4ed6\u76ee\u6807\uff0c\u5982\u6b63\u4ea4\u6027\u548c\u4e0e\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\u7684\u4ea4\u6362\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u6700\u4f18\u7684\u6620\u5c04\u4f18\u5316\u65b9\u6cd5\u7ade\u4e89\uff0c\u4e14\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4e3a\u529f\u80fd\u6620\u5c04\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.14246", "pdf": "https://arxiv.org/pdf/2506.14246", "abs": "https://arxiv.org/abs/2506.14246", "authors": ["Lingfeng Li", "Yunlong Lu", "Yongyi Wang", "Qifan Zheng", "Wenxin Li"], "title": "Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents", "categories": ["cs.AI"], "comment": null, "summary": "People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMxplainer\uff0c\u4e00\u79cd\u53c2\u6570\u5316\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u6a21\u4eff\u9ebb\u5c06AI\u4ee3\u7406\u7684\u884c\u4e3a\u5e76\u63d0\u53d6\u53ef\u7406\u89e3\u7684\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u4eba\u7c7b\u5b66\u4e60AI\u4ee3\u7406\u7684\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u9ebb\u5c06AI\u4ee3\u7406\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u96be\u4ee5\u4ece\u4e2d\u63d0\u53d6\u5bf9\u4eba\u7c7b\u6709\u7528\u7684\u7b56\u7565\u6d1e\u5bdf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u8fd9\u4e9b\u4ee3\u7406\u7684\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u4eba\u7c7b\u5b66\u4e60\u3002", "method": "Mxplainer\u662f\u4e00\u79cd\u53c2\u6570\u5316\u641c\u7d22\u7b97\u6cd5\uff0c\u53ef\u8f6c\u6362\u4e3a\u7b49\u6548\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5b66\u4e60\u9ed1\u7bb1\u4ee3\u7406\u7684\u53c2\u6570\u3002\u901a\u8fc7\u5206\u6790AI\u548c\u4eba\u7c7b\u73a9\u5bb6\u7684\u6570\u636e\uff0c\u63d0\u53d6\u53ef\u7406\u89e3\u7684\u53c2\u6570\u548c\u5c40\u90e8\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMxplainer\u80fd\u591f\u63d0\u53d6\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u53c2\u6570\uff0c\u63ed\u793aAI\u4ee3\u7406\u7684\u7279\u5f81\u548c\u73a9\u6cd5\u98ce\u683c\uff0c\u5e76\u80fd\u5c40\u90e8\u89e3\u91ca\u9ed1\u7bb1\u4ee3\u7406\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "Mxplainer\u4e3a\u9ebb\u5c06AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u4eba\u7c7b\u5b66\u4e60\u548c\u7406\u89e3AI\u7b56\u7565\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u641c\u7d22\u6846\u67b6\u5728\u89e3\u91ca\u9ed1\u7bb1\u4ee3\u7406\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "Mxplainer\uff1a\u901a\u8fc7\u6a21\u4eff\u9ebb\u5c06\u4ee3\u7406\u89e3\u91ca\u548c\u5b66\u4e60\u6d1e\u5bdf", "abstract_zh": "\u4eba\u4eec\u9700\u8981\u5185\u5316AI\u4ee3\u7406\u7684\u6280\u80fd\u4ee5\u63d0\u5347\u81ea\u8eab\u80fd\u529b\u3002\u672c\u6587\u805a\u7126\u4e8e\u9ebb\u5c06\u8fd9\u4e00\u6d89\u53ca\u4e0d\u5b8c\u5168\u4fe1\u606f\u548c\u957f\u671f\u51b3\u7b56\u7684\u591a\u73a9\u5bb6\u6e38\u620f\u3002\u5c3d\u7ba1AI\u7814\u7a76\u8005\u5df2\u5f00\u53d1\u51fa\u6027\u80fd\u5ab2\u7f8e\u804c\u4e1a\u73a9\u5bb6\u7684\u9ebb\u5c06AI\u4ee3\u7406\uff0c\u4f46\u8fd9\u4e9b\u4ee3\u7406\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u96be\u4ee5\u4ece\u4e2d\u63d0\u53d6\u6d1e\u5bdf\u3002\u672c\u6587\u63d0\u51faMxplainer\uff0c\u4e00\u79cd\u53c2\u6570\u5316\u641c\u7d22\u7b97\u6cd5\uff0c\u53ef\u8f6c\u6362\u4e3a\u7b49\u6548\u795e\u7ecf\u7f51\u7edc\u4ee5\u5b66\u4e60\u9ed1\u7bb1\u4ee3\u7406\u7684\u53c2\u6570\u3002\u5728AI\u548c\u4eba\u7c7b\u73a9\u5bb6\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u5230\u7684\u53c2\u6570\u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e9b\u4ee3\u7406\u7279\u5f81\u548c\u73a9\u6cd5\u98ce\u683c\u7684\u4eba\u7c7b\u53ef\u7406\u89e3\u6d1e\u5bdf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u641c\u7d22\u6846\u67b6\u5982\u4f55\u5c40\u90e8\u89e3\u91ca\u9ed1\u7bb1\u4ee3\u7406\u5728\u5927\u591a\u6570\u9ebb\u5c06\u6e38\u620f\u72b6\u6001\u4e0b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2506.14335", "pdf": "https://arxiv.org/pdf/2506.14335", "abs": "https://arxiv.org/abs/2506.14335", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "title": "Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics", "categories": ["cs.CL"], "comment": "17 pages, 13 figures", "summary": "Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6458\u8981\u8bc4\u4f30\u4e2d\u53c2\u8003\u96c6\u7684\u9009\u62e9\u5bf9\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\uff08\u5982ROUGE\uff09\u5f71\u54cd\u663e\u8457\uff0c\u5bfc\u81f4\u6a21\u578b\u6392\u540d\u4e0d\u7a33\u5b9a\uff0c\u5efa\u8bae\u5728\u8bc4\u4f30\u4e2d\u8003\u8651\u53c2\u8003\u96c6\u7684\u591a\u6837\u6027\u4ee5\u63d0\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u4eba\u7c7b\u8bed\u8a00\u8868\u8fbe\u5177\u6709\u4e30\u5bcc\u6027\u548c\u591a\u6837\u6027\uff0c\u4f46\u6458\u8981\u8bc4\u4f30\u4e2d\u5e38\u5ffd\u89c6\u8fd9\u79cd\u53d8\u5316\u3002\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u63a2\u8ba8\u4e0d\u540c\u53c2\u8003\u96c6\u5bf9\u8bc4\u4f30\u6307\u6807\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86\u4e09\u4e2a\u591a\u53c2\u8003\u6458\u8981\u6570\u636e\u96c6\uff08SummEval\u3001GUMSum\u548cDUC2004\uff09\uff0c\u7814\u7a76\u4e86\u53c2\u8003\u96c6\u9009\u62e9\u5bf9\u6d41\u884c\u6307\u6807\u7684\u5f71\u54cd\uff0c\u5e76\u6536\u96c6\u4e86\u4eba\u7c7b\u5bf9LLM\u8f93\u51fa\u7684\u8bc4\u4ef7\u4ee5\u8865\u5145\u73b0\u6709\u53d1\u73b0\u3002", "result": "\u8bb8\u591a\u6d41\u884c\u6307\u6807\uff08\u5c24\u5176\u662f\u57fa\u4e8en-gram\u7684ROUGE\uff09\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u6a21\u578b\u6392\u540d\u56e0\u53c2\u8003\u96c6\u4e0d\u540c\u800c\u53d8\u5316\u3002\u4eba\u7c7b\u8bc4\u4ef7\u4e0e\u6307\u6807\u7684\u76f8\u5173\u6027\u8f83\u5f31\u6216\u65e0\u76f8\u5173\u6027\u3002", "conclusion": "\u5efa\u8bae\u5728\u6458\u8981\u8bc4\u4f30\u4e2d\u7eb3\u5165\u53c2\u8003\u96c6\u7684\u591a\u6837\u6027\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u548c\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bc4\u4f30LLM\u65f6\u3002", "paper_title_zh": "\u8bc4\u4f30\u4e0d\u5e94\u5ffd\u89c6\u591a\u6837\u6027\uff1a\u53c2\u8003\u96c6\u9009\u62e9\u5bf9\u6458\u8981\u6307\u6807\u7684\u5f71\u54cd", "abstract_zh": "\u4eba\u7c7b\u8bed\u8a00\u8868\u8fbe\u5177\u6709\u663e\u8457\u7684\u4e30\u5bcc\u6027\u548c\u591a\u6837\u6027\uff0c\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u4ea4\u6d41\u98ce\u683c\u548c\u610f\u56fe\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u591a\u6837\u6027\u5728\u6458\u8981\u8bc4\u4f30\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002\u867d\u7136\u5df2\u77e5\u591a\u53c2\u8003\u6458\u8981\u80fd\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0d\u540c\u53c2\u8003\u96c6\u5bf9\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u7814\u7a76\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u5bf9\u53c2\u8003\u96c6\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u5206\u6790\u4e86\u4e09\u4e2a\u591a\u6837\u5316\u7684\u591a\u53c2\u8003\u6458\u8981\u6570\u636e\u96c6\uff1aSummEval\u3001GUMSum\u548cDUC2004\u3002\u6211\u4eec\u53d1\u73b0\u8bb8\u591a\u6d41\u884c\u6307\u6807\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u5bf9\u57fa\u4e8en-gram\u7684\u6307\u6807\uff08\u5982ROUGE\uff09\u5c24\u4e3a\u4e25\u91cd\uff0c\u6a21\u578b\u6392\u540d\u56e0\u53c2\u8003\u96c6\u4e0d\u540c\u800c\u53d8\u5316\uff0c\u524a\u5f31\u4e86\u6a21\u578b\u6bd4\u8f83\u7684\u53ef\u9760\u6027\u3002\u6211\u4eec\u8fd8\u6536\u96c6\u4e86\u4eba\u7c7b\u5bf9LLM\u8f93\u51fa\u7684\u8bc4\u4ef7\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u4e0e\u6307\u6807\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u8865\u5145\u65b0\u95fb\u6458\u8981\u4e4b\u5916\u7684\u73b0\u6709\u53d1\u73b0\uff0c\u53d1\u73b0\u76f8\u5173\u6027\u8f83\u5f31\u6216\u65e0\u76f8\u5173\u6027\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u6211\u4eec\u5efa\u8bae\u5728\u6458\u8981\u8bc4\u4f30\u4e2d\u7eb3\u5165\u53c2\u8003\u96c6\u7684\u591a\u6837\u6027\uff0c\u4ee5\u63d0\u9ad8\u4e00\u81f4\u6027\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bc4\u4f30LLM\u65f6\u3002"}}
{"id": "2506.14350", "pdf": "https://arxiv.org/pdf/2506.14350", "abs": "https://arxiv.org/abs/2506.14350", "authors": ["Zoubida Ameur", "Fr\u00e9d\u00e9ric Lefebvre", "Philippe De Lagrange", "Milo\u0161 Radosavljevi\u0107"], "title": "FGA-NN: Film Grain Analysis Neural Network", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFGA-NN\uff0c\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u7535\u5f71\u9897\u7c92\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e0e\u4f20\u7edf\u5408\u6210\u517c\u5bb9\u7684\u9897\u7c92\u53c2\u6570\uff0c\u5728\u5206\u6790\u7cbe\u5ea6\u4e0e\u5408\u6210\u590d\u6742\u5ea6\u95f4\u53d6\u5f97\u4f18\u8d8a\u5e73\u8861\u3002", "motivation": "\u7535\u5f71\u9897\u7c92\u662f\u6a21\u62df\u80f6\u7247\u7684\u526f\u4ea7\u54c1\uff0c\u73b0\u5e38\u7528\u4e8e\u5f71\u89c6\u5185\u5bb9\u4ee5\u589e\u5f3a\u7f8e\u5b66\u6548\u679c\u3002\u4f46\u5728\u4e2d\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u65f6\uff0c\u9897\u7c92\u56e0\u5176\u968f\u673a\u6027\u6613\u4e22\u5931\u3002\u4e3a\u5728\u9ad8\u6548\u538b\u7f29\u7684\u540c\u65f6\u4fdd\u7559\u827a\u672f\u610f\u56fe\uff0c\u9700\u5728\u7f16\u7801\u524d\u5206\u6790\u5e76\u5efa\u6a21\u9897\u7c92\uff0c\u89e3\u7801\u540e\u91cd\u65b0\u5408\u6210\u3002", "method": "FGA-NN\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u7535\u5f71\u9897\u7c92\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u4e0e\u4f20\u7edf\u5408\u6210\u517c\u5bb9\u7684\u9897\u7c92\u53c2\u6570\u3002", "result": "\u5b9a\u91cf\u4e0e\u5b9a\u6027\u7ed3\u679c\u8868\u660e\uff0cFGA-NN\u5728\u5206\u6790\u7cbe\u5ea6\u4e0e\u5408\u6210\u590d\u6742\u5ea6\u95f4\u8868\u73b0\u51fa\u4f18\u8d8a\u5e73\u8861\uff0c\u540c\u65f6\u5177\u5907\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "FGA-NN\u662f\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u7535\u5f71\u9897\u7c92\u5206\u6790\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u538b\u7f29\u4e2d\u9897\u7c92\u4e22\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u5f71\u89c6\u5185\u5bb9\u7684\u9ad8\u6548\u538b\u7f29\u4e0e\u827a\u672f\u4fdd\u7559\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002", "paper_title_zh": "FGA-NN\uff1a\u7535\u5f71\u9897\u7c92\u5206\u6790\u795e\u7ecf\u7f51\u7edc", "abstract_zh": "\u7535\u5f71\u9897\u7c92\u66fe\u662f\u6a21\u62df\u80f6\u7247\u7684\u526f\u4ea7\u54c1\uff0c\u5982\u4eca\u56e0\u7f8e\u5b66\u539f\u56e0\u5e7f\u6cdb\u5b58\u5728\u4e8e\u5f71\u89c6\u5185\u5bb9\u4e2d\u3002\u7136\u800c\uff0c\u5728\u4e2d\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u65f6\uff0c\u9897\u7c92\u56e0\u5176\u968f\u673a\u6027\u6613\u4e22\u5931\u3002\u4e3a\u5728\u9ad8\u6548\u538b\u7f29\u7684\u540c\u65f6\u4fdd\u7559\u827a\u672f\u610f\u56fe\uff0c\u9700\u5728\u7f16\u7801\u524d\u5206\u6790\u5e76\u5efa\u6a21\u9897\u7c92\uff0c\u89e3\u7801\u540e\u91cd\u65b0\u5408\u6210\u3002\u672c\u6587\u63d0\u51faFGA-NN\uff0c\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u7535\u5f71\u9897\u7c92\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e0e\u4f20\u7edf\u5408\u6210\u517c\u5bb9\u7684\u9897\u7c92\u53c2\u6570\u3002\u5b9a\u91cf\u4e0e\u5b9a\u6027\u7ed3\u679c\u8868\u660e\uff0cFGA-NN\u5728\u5206\u6790\u7cbe\u5ea6\u4e0e\u5408\u6210\u590d\u6742\u5ea6\u95f4\u8868\u73b0\u51fa\u4f18\u8d8a\u5e73\u8861\uff0c\u540c\u65f6\u5177\u5907\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2506.14276", "pdf": "https://arxiv.org/pdf/2506.14276", "abs": "https://arxiv.org/abs/2506.14276", "authors": ["Jack Cole", "Mohamed Osman"], "title": "Don't throw the baby out with the bathwater: How and why deep learning for ARC", "categories": ["cs.AI", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u89e3\u51b3\u62bd\u8c61\u4e0e\u63a8\u7406\u8bed\u6599\u5e93\uff08ARC\uff09\u6311\u6218\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff08TTFT\uff09\u548c\u589e\u5f3a\u63a8\u7406\u53cd\u5411\u589e\u5f3a\u4e0e\u6295\u7968\uff08AIRV\uff09\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86ARC\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1ARC\u5bf9AI\u7cfb\u7edf\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u4ecd\u662f\u76ee\u524d\u6700\u6709\u6548\u7684\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u8fdb\u4e00\u6b65\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u52a8\u6001\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347\u5176\u5728ARC\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4ece\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51fa\u53d1\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff08TTFT\uff09\u548c\u589e\u5f3a\u63a8\u7406\u53cd\u5411\u589e\u5f3a\u4e0e\u6295\u7968\uff08AIRV\uff09\u6280\u672f\uff0c\u52a8\u6001\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u548c\u4f18\u5316\u5668\uff0c\u4ee5\u589e\u5f3a\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAIRV\u548cTTFT\u5206\u522b\u5e26\u6765\u4e86260%\u548c300%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6700\u7ec8\u7248\u672c\u5728ARC\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8658%\u7684\u6700\u9ad8\u5206\uff0c\u5e76\u57282023\u5e74ARCathon\u7ade\u8d5b\u4e2d\u593a\u51a0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728ARC\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u52a8\u6001\u4f18\u5316\u548c\u6d4b\u8bd5\u65f6\u6280\u672f\u662f\u63d0\u5347\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u3002", "paper_title_zh": "\u4e0d\u8981\u56e0\u564e\u5e9f\u98df\uff1a\u6df1\u5ea6\u5b66\u4e60\u5728ARC\u4e2d\u7684\u65b9\u6cd5\u4e0e\u539f\u56e0", "abstract_zh": "\u62bd\u8c61\u4e0e\u63a8\u7406\u8bed\u6599\u5e93\uff08ARC-AGI\uff09\u5bf9AI\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u5c3d\u7ba1ARC\u4e0a\u7684\u8868\u73b0\u901a\u5e38\u8f83\u4f4e\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u4ecd\u662f\u751f\u6210\u8de8\u89c6\u89c9\u3001\u8bed\u8a00\u7b49\u591a\u9886\u57df\u6700\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u7684\u6700\u6709\u6548\u7b56\u7565\u3002\u6df1\u5ea6\u5b66\u4e60\u5df2\u8bc1\u660e\u80fd\u591f\u8bad\u7ec3\u8fd9\u4e9b\u7f51\u7edc\u5e76\u5b66\u4e60\u6240\u9700\u62bd\u8c61\u3002\u672c\u6587\u8fdb\u4e00\u6b65\u5229\u7528\u8fd9\u4e00\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u8bad\u7ec3NN\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728ARC\u4e0a\u7684\u6f5c\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5c06\u795e\u7ecf\u7f51\u7edc\u548c\u4f18\u5316\u5668\uff08\u800c\u975e\u4ec5\u9884\u8bad\u7ec3\u7f51\u7edc\uff09\u4f5c\u4e3a\u63a8\u7406\u8fc7\u7a0b\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u4fc3\u8fdb\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9884\u8bad\u7ec3LLM\u51fa\u53d1\u7684ARC\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff08TTFT\uff09\u548c\u589e\u5f3a\u63a8\u7406\u53cd\u5411\u589e\u5f3a\u4e0e\u6295\u7968\uff08AIRV\uff09\u6280\u672f\u3002\u5b9e\u9a8c\u663e\u793a\uff0cAIRV\u548cTTFT\u5206\u522b\u5e26\u6765\u4e86260%\u548c300%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002\u6b64\u65b9\u6cd5\u7684\u65e9\u671f\u7248\u672c\u57282023\u5e74ARCathon\u7ade\u8d5b\u4e2d\u593a\u51a0\uff0c\u6700\u7ec8\u7248\u672c\u5728ARC\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8658%\u7684\u6700\u9ad8\u5206\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5728\u964c\u751f\u9886\u57df\u4e2d\u6784\u5efa\u9c81\u68d2\u63a8\u7406\u7cfb\u7edf\u7684\u5173\u952e\u8981\u7d20\uff0c\u5f3a\u8c03\u4e86\u63d0\u5347\u5e7f\u6cdb\u611f\u77e5\u63a8\u7406\u7684\u6838\u5fc3\u673a\u5236\u3002"}}
{"id": "2506.14345", "pdf": "https://arxiv.org/pdf/2506.14345", "abs": "https://arxiv.org/abs/2506.14345", "authors": ["Bruno Martins", "Piotr Szyma\u0144ski", "Piotr Gramacki"], "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4e00\u4ee3\u5730\u7406\u65f6\u7a7a\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u613f\u666f\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7cfb\u7edf\u5728\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u5f00\u653e\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u4e25\u683c\u8bc4\u4f30\u534f\u8bae\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7f3a\u4e4f\u5904\u7406\u5730\u7406\u65f6\u7a7a\u7ea6\u675f\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u4e9b\u80fd\u529b\u5728\u516c\u5171\u536b\u751f\u3001\u73af\u5883\u79d1\u5b66\u548c\u793e\u4f1a\u7ecf\u6d4e\u5206\u6790\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u6280\u672f\u7a7a\u767d\uff0c\u63a8\u52a8\u66f4\u5168\u9762\u7684\u5730\u7406\u65f6\u7a7a\u4fe1\u606f\u5408\u6210\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u5c06\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u6574\u5408\u5230\u6df1\u5ea6\u7814\u7a76\u6d41\u7a0b\u4e2d\uff0c\u5e76\u5f3a\u8c03\u5f00\u653e\u57fa\u7840\u8bbe\u65bd\u548c\u4e25\u683c\u8bc4\u4f30\u534f\u8bae\u7684\u91cd\u8981\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6280\u672f\u8def\u7ebf\u56fe\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u5730\u7406\u65f6\u7a7a\u7ea6\u675f\u4e0b\u589e\u5f3a\u68c0\u7d22\u548c\u5408\u6210\u80fd\u529b\uff0c\u4e3a\u672a\u6765AI\u9a71\u52a8\u7684\u4fe1\u606f\u8bbf\u95ee\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0b\u4e00\u4ee3\u5730\u7406\u65f6\u7a7a\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u613f\u666f\uff0c\u5f3a\u8c03\u4e86\u5f00\u653e\u6027\u548c\u53ef\u590d\u73b0\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "paper_title_zh": "\u5730\u7406\u65f6\u7a7a\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u613f\u666f\uff1a\u8fc8\u5411\u5168\u9762\u3001\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684\u5730\u7406\u65f6\u7a7a\u4fe1\u606f\u5408\u6210", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51fa\u73b0\u6539\u53d8\u4e86\u4fe1\u606f\u8bbf\u95ee\u65b9\u5f0f\uff0c\u5f53\u524d\u7684LLMs\u8fd8\u652f\u6301\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u8ba1\u5212\u6027\u8fed\u4ee3\u641c\u7d22\u3001\u68c0\u7d22\u548c\u63a8\u7406\u751f\u6210\u5168\u9762\u7684\u62a5\u544a\u5f0f\u7b54\u6848\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7f3a\u4e4f\u5730\u7406\u65f6\u7a7a\u80fd\u529b\uff0c\u800c\u8fd9\u4e9b\u80fd\u529b\u5bf9\u4e8e\u56de\u7b54\u6d89\u53ca\u5730\u7406\u548c/\u6216\u65f6\u95f4\u7ea6\u675f\u7684\u4e0a\u4e0b\u6587\u4e30\u5bcc\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e9b\u95ee\u9898\u5e38\u89c1\u4e8e\u516c\u5171\u536b\u751f\u3001\u73af\u5883\u79d1\u5b66\u6216\u793e\u4f1a\u7ecf\u6d4e\u5206\u6790\u7b49\u9886\u57df\u3002\u672c\u6587\u62a5\u544a\u4e86\u6211\u4eec\u5173\u4e8e\u4e0b\u4e00\u4ee3\u7cfb\u7edf\u7684\u613f\u666f\uff0c\u6307\u51fa\u4e86\u5c06\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u6574\u5408\u5230\u6df1\u5ea6\u7814\u7a76\u6d41\u7a0b\u4e2d\u7684\u91cd\u8981\u6280\u672f\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u8bc4\u4f30\u6311\u6218\u3002\u6211\u4eec\u4e3b\u5f20\u901a\u8fc7\u5f00\u653e\u548c\u53ef\u590d\u73b0\u7684\u57fa\u7840\u8bbe\u65bd\u4ee5\u53ca\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u589e\u5f3a\u68c0\u7d22\u548c\u5408\u6210\u8fc7\u7a0b\u5904\u7406\u5730\u7406\u65f6\u7a7a\u7ea6\u675f\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u613f\u666f\u4e3a\u66f4\u5148\u8fdb\u3001\u5177\u5907\u5730\u7406\u65f6\u7a7a\u610f\u8bc6\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u6307\u660e\u4e86\u65b9\u5411\uff0c\u53ef\u80fd\u5bf9\u672a\u6765AI\u9a71\u52a8\u7684\u4fe1\u606f\u8bbf\u95ee\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2506.14356", "pdf": "https://arxiv.org/pdf/2506.14356", "abs": "https://arxiv.org/abs/2506.14356", "authors": ["Xiaoqi Wang", "Yi Wang", "Lap-Pui Chau"], "title": "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .", "AI": {"tldr": "EVA02-AT\u662f\u4e00\u79cd\u9488\u5bf9\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u9ad8\u6548\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u9884\u8bad\u7ec3\u3001\u7a7a\u95f4-\u65f6\u95f4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u5bf9\u79f0\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u7a7a\u95f4-\u65f6\u95f4\u7f16\u7801\u6548\u7387\u4f4e\u4ee5\u53ca\u591a\u5b9e\u4f8b\u68c0\u7d22\u76ee\u6807\u4e0d\u7cbe\u786e\u3002EVA02-AT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u901a\u8fc7\u5355\u9636\u6bb5\u9884\u8bad\u7ec3\u5c06\u57fa\u4e8e\u56fe\u50cf\u7684CLIP\u6a21\u578b\u8f6c\u5316\u4e3a\u89c6\u9891\u7f16\u7801\u5668\uff1b2. \u5f15\u5165\u7a7a\u95f4-\u65f6\u95f4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u8054\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4-\u65f6\u95f4\u7279\u5f81\u7f16\u7801\uff1b3. \u63d0\u51fa\u5bf9\u79f0\u591a\u76f8\u4f3c\u6027\uff08SMS\uff09\u635f\u5931\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u4f18\u5316\u591a\u5b9e\u4f8b\u68c0\u7d22\u4efb\u52a1\u3002", "result": "\u5728Ego4D\u3001EPIC-Kitchens-100\u548cCharades-Ego\u7b49\u6570\u636e\u96c6\u4e0a\uff0cEVA02-AT\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u91cf\u66f4\u5c11\u3002SMS\u635f\u5931\u5728\u591a\u5b9e\u4f8b\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EVA02-AT\u901a\u8fc7\u9ad8\u6548\u7684\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\u548c\u5bf9\u79f0\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "EVA02-AT\uff1a\u57fa\u4e8e\u7a7a\u95f4-\u65f6\u95f4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u5bf9\u79f0\u4f18\u5316\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bed\u8a00\u7406\u89e3", "abstract_zh": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u9700\u8981\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u6d41\u7a0b\u5bfc\u81f4\u6210\u672c\u8fc7\u9ad8\uff1b2\uff09\u624b\u52a8\u5206\u5272\u76843D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u963b\u788d\u7279\u5f81\u4ea4\u4e92\uff0c\u5bfc\u81f4\u7a7a\u95f4-\u65f6\u95f4\u7f16\u7801\u6548\u7387\u4f4e\u4e0b\uff1b3\uff09\u8f6f\u6807\u7b7e\u591a\u5b9e\u4f8b\u68c0\u7d22\u4e2d\u7684\u5b66\u4e60\u76ee\u6807\u4e0d\u7cbe\u786e\uff0c\u5ffd\u7565\u4e86\u8d1f\u6837\u672c\u5bf9\u7684\u5173\u8054\u6027\u3002\u672c\u6587\u63d0\u51faEVA02-AT\uff0c\u4e00\u5957\u57fa\u4e8eEVA02\u7684\u89c6\u9891\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u8bbe\u8ba1\u3002EVA02-AT\u9996\u5148\u901a\u8fc7\u5355\u9636\u6bb5\u9884\u8bad\u7ec3\u5c06\u57fa\u4e8e\u56fe\u50cf\u7684CLIP\u6a21\u578b\u9ad8\u6548\u8f6c\u5316\u4e3a\u7edf\u4e00\u7684\u89c6\u9891\u7f16\u7801\u5668\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u7a7a\u95f4-\u65f6\u95f4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u8054\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5728\u6574\u4e2a\u9690\u85cf\u7ef4\u5ea6\u4e0a\u6709\u6548\u7f16\u7801\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\u3002\u8fd9\u79cd\u8054\u5408\u7f16\u7801\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u8de8\u8f74\u5173\u7cfb\uff0c\u8fd9\u5bf9\u51c6\u786e\u5efa\u6a21\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u548c\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u7b2c\u4e09\uff0c\u9488\u5bf9\u591a\u5b9e\u4f8b\u89c6\u9891\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\uff0c\u6211\u4eec\u63d0\u51fa\u5bf9\u79f0\u591a\u76f8\u4f3c\u6027\uff08SMS\uff09\u635f\u5931\u548c\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f18\u5316\u6240\u6709\u6b63\u8d1f\u6837\u672c\u5bf9\u7684\u8f6f\u6807\u7b7e\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5b66\u4e60\u76ee\u6807\u3002\u5728Ego4D\u3001EPIC-Kitchens-100\u548cCharades-Ego\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEVA02-AT\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u91cf\u66f4\u5c11\u3002\u91c7\u7528SMS\u635f\u5931\u7684\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u5728https://github.com/xqwang14/EVA02-AT\u3002"}}
{"id": "2506.14299", "pdf": "https://arxiv.org/pdf/2506.14299", "abs": "https://arxiv.org/abs/2506.14299", "authors": ["Fanzhi Zeng", "Siqi Wang", "Chuzhao Zhu", "Li Li"], "title": "ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems", "categories": ["cs.AI"], "comment": null, "summary": "How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u6846\u67b6ADRD\uff0c\u901a\u8fc7\u89c4\u5219\u5316\u51b3\u7b56\u7cfb\u7edf\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u51b3\u7b56\uff0c\u5e76\u5728\u6027\u80fd\u3001\u54cd\u5e94\u901f\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u4e00\u76f4\u662f\u5b66\u672f\u7814\u7a76\u7684\u91cd\u70b9\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u900f\u660e\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u800c\u57fa\u4e8eLLMs\u7684\u89c4\u5219\u5316\u7cfb\u7edf\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ADRD\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4fe1\u606f\u6a21\u5757\uff08\u805a\u5408\u9a7e\u9a76\u573a\u666f\u4fe1\u606f\uff09\u3001\u4ee3\u7406\u6a21\u5757\uff08\u751f\u6210\u89c4\u5219\u5316\u9a7e\u9a76\u7b56\u7565\uff09\u548c\u6d4b\u8bd5\u6a21\u5757\uff08\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\uff09\u3002\u901a\u8fc7LLMs\u7684\u63a8\u7406\u548c\u7f16\u7a0b\u80fd\u529b\u5b9e\u73b0\u9ad8\u6548\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cADRD\u5728\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u53ef\u89e3\u91ca\u6027\u3001\u54cd\u5e94\u901f\u5ea6\u548c\u9a7e\u9a76\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u548c\u73b0\u6709LLM\u65b9\u6cd5\u3002", "conclusion": "ADRD\u9996\u6b21\u5c06LLMs\u4e0e\u89c4\u5219\u5316\u7cfb\u7edf\u7ed3\u5408\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u4e3a\u900f\u660e\u4e14\u6613\u4fee\u6539\u7684\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "ADRD\uff1a\u57fa\u4e8e\u89c4\u5219\u5316\u51b3\u7b56\u7cfb\u7edf\u7684LLM\u9a71\u52a8\u81ea\u52a8\u9a7e\u9a76", "abstract_zh": "\u5982\u4f55\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u5df2\u6210\u4e3a\u5b66\u672f\u7814\u7a76\u7684\u7126\u70b9\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u53ef\u6267\u884c\u7684\u89c4\u5219\u5316\u51b3\u7b56\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u5177\u4f53\u800c\u8a00\uff0c\u501f\u52a9LLMs\u5f3a\u5927\u7684\u63a8\u7406\u548c\u7f16\u7a0b\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ADRD\uff08\u57fa\u4e8e\u89c4\u5219\u5316\u51b3\u7b56\u7cfb\u7edf\u7684LLM\u9a71\u52a8\u81ea\u52a8\u9a7e\u9a76\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4fe1\u606f\u6a21\u5757\u3001\u4ee3\u7406\u6a21\u5757\u548c\u6d4b\u8bd5\u6a21\u5757\u3002\u6846\u67b6\u9996\u5148\u901a\u8fc7\u4fe1\u606f\u6a21\u5757\u805a\u5408\u9a7e\u9a76\u573a\u666f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u968f\u540e\u5229\u7528\u4ee3\u7406\u6a21\u5757\u751f\u6210\u89c4\u5219\u5316\u9a7e\u9a76\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4e0e\u6d4b\u8bd5\u6a21\u5757\u7684\u6301\u7eed\u4ea4\u4e92\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cADRD\u5728\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ca\u6700\u5148\u8fdb\u7684LLM\u65b9\u6cd5\u76f8\u6bd4\uff0cADRD\u5728\u53ef\u89e3\u91ca\u6027\u3001\u54cd\u5e94\u901f\u5ea6\u548c\u9a7e\u9a76\u6027\u80fd\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u8be5\u6846\u67b6\u5bf9\u590d\u6742\u9a7e\u9a76\u573a\u666f\u7684\u5168\u9762\u51c6\u786e\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u900f\u660e\u3001\u6613\u4fee\u6539\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u89c4\u5219\u5316\u51b3\u7b56\u7cfb\u7edf\u7684\u5e7f\u9614\u524d\u666f\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u89c4\u5219\u5316\u7cfb\u7edf\u7ed3\u5408\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u7814\u7a76\uff0c\u6211\u4eec\u7684\u53d1\u73b0\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14370", "pdf": "https://arxiv.org/pdf/2506.14370", "abs": "https://arxiv.org/abs/2506.14370", "authors": ["Amrit Poudel", "Yifan Ding", "Jurgen Pfeffer", "Tim Weninger"], "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u8c37\u6b4c\u4f5c\u4e3a\u6570\u5b57\u5b88\u95e8\u4eba\uff0c\u901a\u8fc7\u7b97\u6cd5\u9009\u62e9\u6027\u63a8\u5e7f\u6216\u538b\u5236\u7279\u5b9a\u6807\u7b7e\u548c\u5b50\u8bba\u575b\uff0c\u5f71\u54cd\u7528\u6237\u63a5\u89e6\u7684\u4fe1\u606f\u5185\u5bb9\u3002", "motivation": "\u63a2\u8ba8\u641c\u7d22\u5f15\u64ce\u5982\u4f55\u901a\u8fc7\u7b97\u6cd5\u7ba1\u7406\u7f51\u7edc\u548c\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u7684\u53ef\u89c1\u6027\uff0c\u63ed\u793a\u5176\u5bf9\u516c\u4f17\u8bdd\u8bed\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u8c37\u6b4c\u641c\u7d22\u7ed3\u679c\u4e0eReddit\u548cTwitter/X\u7684\u975e\u62bd\u6837\u6570\u636e\uff0c\u5206\u6790\u5185\u5bb9\u53ef\u89c1\u6027\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "result": "\u8c37\u6b4c\u7b97\u6cd5\u503e\u5411\u4e8e\u538b\u5236\u4e0e\u8272\u60c5\u5185\u5bb9\u3001\u9634\u8c0b\u8bba\u3001\u5e7f\u544a\u548c\u52a0\u5bc6\u8d27\u5e01\u76f8\u5173\u7684\u5b50\u8bba\u575b\u548c\u6807\u7b7e\uff0c\u540c\u65f6\u63a8\u5e7f\u9ad8\u4e92\u52a8\u5185\u5bb9\u3002", "conclusion": "\u8c37\u6b4c\u7684\u5b88\u95e8\u884c\u4e3a\u901a\u8fc7\u7b5b\u9009\u793e\u4ea4\u5a92\u4f53\u53d9\u4e8b\u5f71\u54cd\u516c\u4f17\u8bdd\u8bed\uff0c\u51f8\u663e\u5176\u4f5c\u4e3a\u6570\u5b57\u5b88\u95e8\u4eba\u7684\u91cd\u8981\u4f5c\u7528\u3002", "paper_title_zh": "\u6570\u5b57\u5b88\u95e8\u4eba\uff1a\u8c37\u6b4c\u5728\u7b5b\u9009\u6807\u7b7e\u548c\u5b50\u8bba\u575b\u4e2d\u7684\u89d2\u8272", "abstract_zh": "\u641c\u7d22\u5f15\u64ce\u4f5c\u4e3a\u6570\u5b57\u5b88\u95e8\u4eba\uff0c\u901a\u8fc7\u7b97\u6cd5\u7b5b\u9009\u7f51\u7edc\u548c\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\uff0c\u5bf9\u5185\u5bb9\u7684\u53ef\u89c1\u6027\u8d77\u5230\u5173\u952e\u4f5c\u7528\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8c37\u6b4c\u7b49\u641c\u7d22\u5f15\u64ce\u5982\u4f55\u9009\u62e9\u6027\u63a8\u5e7f\u6216\u538b\u5236\u7279\u5b9a\u6807\u7b7e\u548c\u5b50\u8bba\u575b\uff0c\u4ece\u800c\u5f71\u54cd\u7528\u6237\u63a5\u89e6\u7684\u4fe1\u606f\u3002\u901a\u8fc7\u6bd4\u8f83\u641c\u7d22\u5f15\u64ce\u7ed3\u679c\u4e0eReddit\u548cTwitter/X\u7684\u975e\u62bd\u6837\u6570\u636e\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u5185\u5bb9\u53ef\u89c1\u6027\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u8c37\u6b4c\u7b97\u6cd5\u503e\u5411\u4e8e\u538b\u5236\u4e0e\u8272\u60c5\u5185\u5bb9\u3001\u9634\u8c0b\u8bba\u3001\u5e7f\u544a\u548c\u52a0\u5bc6\u8d27\u5e01\u76f8\u5173\u7684\u5b50\u8bba\u575b\u548c\u6807\u7b7e\uff0c\u540c\u65f6\u63a8\u5e7f\u9ad8\u4e92\u52a8\u5185\u5bb9\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u8c37\u6b4c\u7684\u5b88\u95e8\u884c\u4e3a\u901a\u8fc7\u7b5b\u9009\u793e\u4ea4\u5a92\u4f53\u53d9\u4e8b\u5f71\u54cd\u516c\u4f17\u8bdd\u8bed\u3002"}}
{"id": "2506.14362", "pdf": "https://arxiv.org/pdf/2506.14362", "abs": "https://arxiv.org/abs/2506.14362", "authors": ["Daniele Rege Cambrin", "Eleonora Poeta", "Eliana Pastor", "Isaac Corley", "Tania Cerquitelli", "Elena Baralis", "Paolo Garza"], "title": "HydroChronos: Forecasting Decades of Surface Water Change", "categories": ["cs.CV"], "comment": null, "summary": "Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHydroChronos\u6570\u636e\u96c6\u548cAquaClimaTempo UNet\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5730\u8868\u6c34\u52a8\u6001\u53d8\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u6c14\u5019\u53d8\u91cf\u3002", "motivation": "\u5730\u8868\u6c34\u52a8\u6001\u9884\u6d4b\u5bf9\u6c34\u8d44\u6e90\u7ba1\u7406\u548c\u6c14\u5019\u53d8\u5316\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u7efc\u5408\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u57fa\u51c6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faHydroChronos\u6570\u636e\u96c6\uff0c\u5305\u542b30\u591a\u5e74\u7684Landsat 5\u548cSentinel-2\u5f71\u50cf\u3001\u6c14\u5019\u6570\u636e\u53ca\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff1b\u8bbe\u8ba1AquaClimaTempo UNet\u6a21\u578b\uff0c\u7ed3\u5408\u6c14\u5019\u6570\u636e\u5206\u652f\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u53d8\u5316\u68c0\u6d4b\u3001\u53d8\u5316\u65b9\u5411\u5206\u7c7b\u548c\u53d8\u5316\u5e45\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534714%\u300111%\u548c0.1 MAE\uff1b\u901a\u8fc7\u53ef\u89e3\u91caAI\u5206\u6790\u8bc6\u522b\u4e86\u5173\u952e\u6c14\u5019\u53d8\u91cf\u3002", "conclusion": "HydroChronos\u6570\u636e\u96c6\u548cAquaClimaTempo UNet\u6a21\u578b\u4e3a\u5730\u8868\u6c34\u52a8\u6001\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u672a\u6765\u5efa\u6a21\u53ef\u53c2\u8003\u5176\u5206\u6790\u7ed3\u679c\u3002", "paper_title_zh": "HydroChronos\uff1a\u9884\u6d4b\u6570\u5341\u5e74\u7684\u5730\u8868\u6c34\u53d8\u5316", "abstract_zh": "\u5730\u8868\u6c34\u52a8\u6001\u9884\u6d4b\u5bf9\u6c34\u8d44\u6e90\u7ba1\u7406\u548c\u6c14\u5019\u53d8\u5316\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u7efc\u5408\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u57fa\u51c6\u3002\u672c\u6587\u63d0\u51faHydroChronos\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u7684\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5730\u8868\u6c34\u52a8\u6001\u9884\u6d4b\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u6b27\u6d32\u3001\u5317\u7f8e\u548c\u5357\u7f8e\u591a\u6837\u6e56\u6cca\u4e0e\u6cb3\u6d4130\u591a\u5e74\u7684Landsat 5\u548cSentinel-2\u5f71\u50cf\u3001\u6c14\u5019\u6570\u636e\u53ca\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff0c\u5e76\u914d\u5957\u4e09\u9879\u9884\u6d4b\u4efb\u52a1\u3002\u6211\u4eec\u8fd8\u63d0\u51faAquaClimaTempo UNet\uff0c\u4e00\u79cd\u7ed3\u5408\u6c14\u5019\u6570\u636e\u5206\u652f\u7684\u65b0\u578b\u65f6\u7a7a\u67b6\u6784\uff0c\u4f5c\u4e3a\u5f3a\u57fa\u51c6\u57fa\u7ebf\u3002\u8be5\u6a21\u578b\u5728\u53d8\u5316\u68c0\u6d4b\u548c\u53d8\u5316\u65b9\u5411\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684F1\u5206\u6570\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534714%\u548c11%\uff0c\u5728\u53d8\u5316\u5e45\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\u7684MAE\u63d0\u53470.1\u3002\u6700\u540e\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u5730\u8868\u6c34\u53d8\u5316\u7684\u5173\u952e\u6c14\u5019\u53d8\u91cf\u548c\u8f93\u5165\u901a\u9053\uff0c\u4e3a\u672a\u6765\u5efa\u6a21\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.14336", "pdf": "https://arxiv.org/pdf/2506.14336", "abs": "https://arxiv.org/abs/2506.14336", "authors": ["Jia'ang Wan", "Feng Shen", "Fujuan Li", "Yanjin Sun", "Yan Li", "Shiwen Zhang"], "title": "AviationLLM: An LLM-based Knowledge System for Aviation Training", "categories": ["cs.AI"], "comment": null, "summary": "Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u822a\u7a7a\u57f9\u8bad\u77e5\u8bc6\u7cfb\u7edfAviationLLM\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bad\u7ec3\u7cfb\u7edf\u4e2d\u4e13\u4e1a\u7b54\u6848\u4e0d\u51c6\u786e\u548c\u77e5\u8bc6\u66f4\u65b0\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u822a\u7a7a\u7406\u8bba\u57f9\u8bad\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u822a\u7a7a\u57f9\u8bad\u662f\u786e\u4fdd\u98de\u884c\u5b89\u5168\u3001\u63d0\u5347\u884c\u4e1a\u6548\u7387\u548c\u4fc3\u8fdb\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u6838\u5fc3\u73af\u8282\uff0c\u4f46\u73b0\u6709\u57f9\u8bad\u7cfb\u7edf\u4f9d\u8d56\u6709\u9650\u7684\u6559\u5458\u8d44\u6e90\uff0c\u4e14\u4e92\u8054\u7f51\u63d0\u4f9b\u7684\u4e13\u4e1a\u7b54\u6848\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u57f9\u8bad\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u5f15\u5165LLM\u6280\u672f\uff0c\u4f46\u57fa\u7840\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u6cd5\u63d0\u4f9b\u4e13\u4e1a\u9886\u57df\u7684\u51c6\u786e\u7b54\u6848\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u672c\u6587\u63d0\u51faRALA-DPO\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5f00\u6e90\u9884\u8bad\u7ec3LLM Qwen\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8fdb\u884c\u9886\u57df\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\uff0c\u751f\u6210\u7cbe\u786e\u4e14\u9ad8\u8d28\u91cf\u7684\u7b54\u6848\uff0c\u907f\u514d\u56e0\u6570\u636e\u504f\u5dee\u3001\u77e5\u8bc6\u8fc7\u65f6\u6216\u9886\u57df\u77e5\u8bc6\u7f3a\u5931\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRALA-DPO\u80fd\u591f\u663e\u8457\u63d0\u5347\u822a\u7a7a\u4e13\u4e1a\u77e5\u8bc6\u7684\u56de\u7b54\u51c6\u786e\u6027\u3002\u7ed3\u5408RAG\u673a\u5236\u540e\uff0c\u7cfb\u7edf\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u96f6\u6210\u672c\u7684\u77e5\u8bc6\u66f4\u65b0\u3002", "conclusion": "RALA-DPO\u901a\u8fc7DPO\u548cRAG\u6280\u672f\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u822a\u7a7a\u57f9\u8bad\u4e2d\u4e13\u4e1a\u7b54\u6848\u4e0d\u51c6\u786e\u548c\u77e5\u8bc6\u66f4\u65b0\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e3a\u822a\u7a7a\u7406\u8bba\u57f9\u8bad\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "AviationLLM\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u822a\u7a7a\u57f9\u8bad\u77e5\u8bc6\u7cfb\u7edf", "abstract_zh": "\u822a\u7a7a\u57f9\u8bad\u662f\u786e\u4fdd\u98de\u884c\u5b89\u5168\u3001\u63d0\u5347\u884c\u4e1a\u6548\u7387\u548c\u4fc3\u8fdb\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u6838\u5fc3\u73af\u8282\uff0c\u4e0d\u4ec5\u6d89\u53ca\u98de\u884c\u6a21\u62df\uff0c\u8fd8\u9700\u5b66\u4e60\u5927\u91cf\u4e13\u4e1a\u822a\u7a7a\u7406\u8bba\u77e5\u8bc6\u3002\u73b0\u6709\u57f9\u8bad\u7cfb\u7edf\u4e2d\uff0c\u77e5\u8bc6\u4e3b\u8981\u7531\u6559\u5458\u4f20\u6388\uff0c\u4f46\u6559\u5458\u6570\u91cf\u6709\u9650\uff0c\u4e14\u4ece\u4e92\u8054\u7f51\u83b7\u53d6\u7684\u4e13\u4e1a\u7b54\u6848\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u57f9\u8bad\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f46\u57fa\u7840\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u6cd5\u63d0\u4f9b\u4e13\u4e1a\u9886\u57df\u7684\u51c6\u786e\u7b54\u6848\uff0c\u56e0\u6b64\u9700\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u3002\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u56e0\u6570\u636e\u8986\u76d6\u4e0d\u8db3\u53ef\u80fd\u5bfc\u81f4\u8868\u9762\u5408\u7406\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u56de\u7b54\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u68c0\u7d22\u589e\u5f3aLLM\u5bf9\u9f50\u65b9\u6cd5\uff08RALA-DPO\uff09\u3002\u6211\u4eec\u9009\u62e9\u5f00\u6e90\u9884\u8bad\u7ec3LLM Qwen\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eDPO\u7684\u9886\u57df\u5bf9\u9f50\u6280\u672f\uff0c\u5c06\u5176\u9002\u914d\u4e8e\u822a\u7a7a\u7406\u8bba\u57f9\u8bad\u3002\u540c\u65f6\uff0c\u4e3a\u51cf\u5c11\u56e0\u8bad\u7ec3\u6570\u636e\u504f\u5dee\u3001\u77e5\u8bc6\u8fc7\u65f6\u6216\u9886\u57df\u77e5\u8bc6\u7f3a\u5931\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u4e0e\u68c0\u7d22\u6a21\u578b\u3002RALA-DPO\u80fd\u591f\u6709\u6548\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u7cbe\u786e\u4e14\u9ad8\u8d28\u91cf\u7684\u7b54\u6848\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRALA-DPO\u80fd\u591f\u63d0\u5347\u822a\u7a7a\u4e13\u4e1a\u77e5\u8bc6\u7684\u56de\u7b54\u51c6\u786e\u6027\u3002\u7ed3\u5408RAG\u673a\u5236\u540e\uff0c\u8be5\u7cfb\u7edf\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u5e76\u540c\u65f6\u5b9e\u73b0\u96f6\u6210\u672c\u7684\u77e5\u8bc6\u66f4\u65b0\u3002"}}
{"id": "2506.14371", "pdf": "https://arxiv.org/pdf/2506.14371", "abs": "https://arxiv.org/abs/2506.14371", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio P\u00e9rez-Ortiz", "Tanja K\u00e4ser", "Nuria Oliver"], "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "categories": ["cs.CL", "cs.HC"], "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e24\u6b65\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u548c\u9009\u62e9\u6279\u5224\u6027\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u6df1\u5ea6\u601d\u8003\u3002\u8be5\u7cfb\u7edf\u5728ACL 2025\u7684\u5171\u4eab\u4efb\u52a1\u4e2d\u83b7\u80dc\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLMs\u7684\u804a\u5929\u754c\u9762\u53ef\u80fd\u52a9\u957f\u6d45\u5c42\u5b66\u4e60\uff0c\u524a\u5f31\u6279\u5224\u6027\u601d\u7ef4\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLMs\u751f\u6210\u6311\u6218\u8fa9\u8bba\u4e2d\u65e0\u652f\u6301\u6216\u6a21\u7cca\u4e3b\u5f20\u7684\u6279\u5224\u6027\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u6df1\u5ea6\u63a8\u7406\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5c0f\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5019\u9009\u95ee\u9898\uff08Questioner\uff09\uff1b2\uff09\u901a\u8fc7\u53e6\u4e00\u4e2a\u6a21\u578b\uff08Judge\uff09\u9009\u62e9\u6700\u76f8\u5173\u7684\u95ee\u9898\u3002", "result": "\u8be5\u7cfb\u7edf\u5728ACL 2025\u7684\u5171\u4eab\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eLLMs\u7684\u65b9\u6cd5\u5728\u4fc3\u8fdb\u5bf9\u8bba\u8bc1\u6587\u672c\u7684\u6279\u5224\u6027\u601d\u8003\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLMs\u53ef\u7528\u4e8e\u751f\u6210\u548c\u9009\u62e9\u6279\u5224\u6027\u95ee\u9898\uff0c\u6709\u6548\u4fc3\u8fdb\u5bf9\u8bba\u8bc1\u6587\u672c\u7684\u6df1\u5ea6\u601d\u8003\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "ELLIS Alicante\u5728CQs-Gen 2025\uff1a\u8d62\u5f97\u6279\u5224\u6027\u95ee\u9898\u751f\u6210\u5171\u4eab\u4efb\u52a1\uff1a\u57fa\u4e8eLLM\u7684\u95ee\u9898\u751f\u6210\u4e0e\u9009\u62e9", "abstract_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u804a\u5929\u754c\u9762\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u6d45\u5c42\u5b66\u4e60\u548c\u6279\u5224\u6027\u601d\u7ef4\u80fd\u529b\u524a\u5f31\u7684\u62c5\u5fe7\u3002\u672c\u6587\u5e76\u672a\u4ec5\u4f9d\u8d56LLMs\u68c0\u7d22\u4e8b\u5b9e\u4fe1\u606f\uff0c\u800c\u662f\u63a2\u7d22\u5176\u901a\u8fc7\u751f\u6210\u6311\u6218\u8fa9\u8bba\u4e2d\u65e0\u652f\u6301\u6216\u6a21\u7cca\u4e3b\u5f20\u7684\u6279\u5224\u6027\u95ee\u9898\u6765\u4fc3\u8fdb\u6df1\u5ea6\u63a8\u7406\u7684\u6f5c\u529b\u3002\u672c\u7814\u7a76\u662f\u7b2c12\u5c4a\u8bba\u8bc1\u6316\u6398\u7814\u8ba8\u4f1a\uff08\u4e0eACL 2025\u8054\u5408\u4e3e\u529e\uff09\u5171\u4eab\u4efb\u52a1\u7684\u4e00\u90e8\u5206\uff0c\u4e13\u6ce8\u4e8e\u81ea\u52a8\u751f\u6210\u6279\u5224\u6027\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u6b65\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5c0f\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff1a\u4e00\u4e2a\u751f\u6210\u591a\u4e2a\u5019\u9009\u95ee\u9898\u7684\u201c\u63d0\u95ee\u8005\u201d\uff08Questioner\uff09\u548c\u4e00\u4e2a\u9009\u62e9\u6700\u76f8\u5173\u95ee\u9898\u7684\u201c\u88c1\u5224\u201d\uff08Judge\uff09\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u5171\u4eab\u4efb\u52a1\u7ade\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u9f13\u52b1\u5bf9\u8bba\u8bc1\u6587\u672c\u7684\u6279\u5224\u6027\u601d\u8003\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14367", "pdf": "https://arxiv.org/pdf/2506.14367", "abs": "https://arxiv.org/abs/2506.14367", "authors": ["Sumshun Nahar Eity", "Mahin Montasir Afif", "Tanisha Fairooz", "Md. Mortuza Ahmmed", "Md Saef Ullah Miah"], "title": "DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\\%, with precision, recall, and F1-score all exceeding 91\\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.", "AI": {"tldr": "DGG-XNet\u662f\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408VGG16\u548cDenseNet121\uff0c\u7528\u4e8e\u591a\u7c7b\u8111\u75be\u75c5\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7Grad-CAM\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe91.33%\u3002", "motivation": "\u4f20\u7edfMRI\u5206\u6790\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8111\u75be\u75c5\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u63d0\u51faDGG-XNet\uff0c\u878d\u5408VGG16\u7684\u5c42\u6b21\u7a7a\u95f4\u8868\u793a\u548cDenseNet121\u7684\u7279\u5f81\u91cd\u7528\u80fd\u529b\uff0c\u7ed3\u5408Grad-CAM\u5b9e\u73b0\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "result": "\u5728BraTS 2021\u548cKaggle\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u7387\u4e3a91.33%\uff0c\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc791%\u3002", "conclusion": "DGG-XNet\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u9000\u884c\u6027\u548c\u80bf\u7624\u6027\u8111\u75be\u75c5\u3002", "paper_title_zh": "DGG-XNet\uff1a\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u591a\u7c7b\u8111\u75be\u75c5\u5206\u7c7b\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "abstract_zh": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u8111\u80bf\u7624\u7b49\u8111\u90e8\u75be\u75c5\u7684\u51c6\u786e\u8bca\u65ad\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u57fa\u4e8e\u624b\u52a8MRI\u5206\u6790\u7684\u65b9\u6cd5\u901a\u5e38\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faDGG-XNet\uff0c\u4e00\u79cd\u878d\u5408VGG16\u548cDenseNet121\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u80fd\u529b\u3002DenseNet121\u901a\u8fc7\u5bc6\u96c6\u8fde\u63a5\u4fc3\u8fdb\u7279\u5f81\u91cd\u7528\u548c\u9ad8\u6548\u68af\u5ea6\u6d41\u52a8\uff0c\u800cVGG16\u63d0\u4f9b\u5f3a\u5927\u7684\u5c42\u6b21\u7a7a\u95f4\u8868\u793a\u3002\u4e24\u8005\u7684\u878d\u5408\u5b9e\u73b0\u4e86\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u591a\u7c7b\u7a33\u5065\u5206\u7c7b\u3002\u5e94\u7528Grad-CAM\u53ef\u89c6\u5316\u5173\u952e\u533a\u57df\uff0c\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002\u5728BraTS 2021\u548cKaggle\u7684\u8054\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cDGG-XNet\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523091.33%\uff0c\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc791%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cDGG-XNet\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u9000\u884c\u6027\u548c\u80bf\u7624\u6027\u8111\u75be\u75c5\u3002"}}
{"id": "2506.14387", "pdf": "https://arxiv.org/pdf/2506.14387", "abs": "https://arxiv.org/abs/2506.14387", "authors": ["William F. Shen", "Xinchi Qiu", "Nicola Cancedda", "Nicholas D. Lane"], "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning", "categories": ["cs.AI"], "comment": null, "summary": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.", "AI": {"tldr": "\u73b0\u6709\u7814\u7a76\u5728\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u65f6\uff0c\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u6570\u636e\u6216\u4efb\u52a1\u7684\u4fdd\u7559\uff0c\u5374\u5ffd\u89c6\u4e86\u5b89\u5168\u5bf9\u9f50\u4e2d\u6a21\u578b\u8868\u8fbe\u65e0\u77e5\u80fd\u529b\u7684\u5173\u952e\u9000\u5316\u3002\u672c\u6587\u63d0\u51faSEAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u8bad\u7ec3\u548c\u5b9e\u4f53\u6270\u52a8\u7ed3\u5408KL\u6563\u5ea6\u6b63\u5219\u5316\uff0c\u663e\u8457\u4fdd\u7559\u6a21\u578b\u7684\u65e0\u77e5\u610f\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4f1a\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u8868\u8fbe\u65e0\u77e5\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7b49\u4e0d\u826f\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e2\u80fd\u4fdd\u7559\u5fae\u8c03\u6027\u80fd\u53c8\u80fd\u7ef4\u6301\u6a21\u578b\u65e0\u77e5\u610f\u8bc6\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faSEAT\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1)\u7a00\u758f\u8bad\u7ec3\uff0c\u9650\u5236\u6fc0\u6d3b\u6f02\u79fb\uff1b(2)\u7ed3\u5408KL\u6563\u5ea6\u6b63\u5219\u5316\u7684\u5b9e\u4f53\u6270\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u6297\u77e5\u8bc6\u7ea0\u7f20\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEAT\u5728\u4fdd\u7559\u65e0\u77e5\u610f\u8bc6\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u4e0d\u5f71\u54cd\u5fae\u8c03\u6027\u80fd\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SEAT\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u8bad\u7ec3\u548c\u5b9e\u4f53\u6270\u52a8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5fae\u8c03\u4e2d\u65e0\u77e5\u610f\u8bc6\u9000\u5316\u7684\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u4e0d\u8981\u7f16\u9020\uff1a\u5728LLM\u5fae\u8c03\u4e2d\u4fdd\u6301\u65e0\u77e5\u610f\u8bc6", "abstract_zh": "\u73b0\u6709\u5173\u4e8e\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4fdd\u7559\u7279\u5b9a\u6570\u636e\u6216\u4efb\u52a1\uff0c\u5374\u4e25\u91cd\u5ffd\u89c6\u4e86\u5b89\u5168\u5bf9\u9f50\u4e2d\u6a21\u578b\u5fe0\u5b9e\u8868\u8fbe\u65e0\u77e5\u80fd\u529b\u7684\u9000\u5316\u3002\u672c\u6587\u8868\u660e\uff0c\u4f20\u7edf\u5fae\u8c03\u4f1a\u663e\u8457\u524a\u5f31\u8fd9\u4e00\u80fd\u529b\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7b49\u4e0d\u826f\u884c\u4e3a\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u65b0\u9896\u4f46\u9ad8\u5ea6\u5b9e\u9645\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faSEAT\uff0c\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e2\u80fd\u4fdd\u7559\u5fae\u8c03\u6027\u80fd\uff0c\u53c8\u80fd\u7ef4\u6301\u6a21\u578b\u627f\u8ba4\u65e0\u77e5\u7684\u5185\u5728\u80fd\u529b\u3002SEAT\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1)\u9650\u5236\u6fc0\u6d3b\u6f02\u79fb\u7684\u7a00\u758f\u8bad\u7ec3\uff1b(2)\u7ed3\u5408KL\u6563\u5ea6\u6b63\u5219\u5316\u7684\u65b0\u578b\u5b9e\u4f53\u6270\u52a8\u65b9\u6cd5\uff0c\u65e8\u5728\u5bf9\u6297\u77e5\u8bc6\u7ea0\u7f20\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEAT\u5728\u4fdd\u7559\u65e0\u77e5\u610f\u8bc6\u7684\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u4e0d\u5f71\u54cd\u5fae\u8c03\u6027\u80fd\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14397", "pdf": "https://arxiv.org/pdf/2506.14397", "abs": "https://arxiv.org/abs/2506.14397", "authors": ["Yeonkyoung So", "Gyuseong Lee", "Sungmok Jung", "Joonhak Lee", "JiA Kang", "Sangho Kim", "Jaejin Lee"], "title": "Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Thunder-NUBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53e5\u5b50\u7ea7\u522b\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u5bf9\u6bd4\u6807\u51c6\u5426\u5b9a\u4e0e\u591a\u6837\u5316\u7ed3\u6784\uff08\u5982\u5c40\u90e8\u5426\u5b9a\u3001\u77db\u76fe\u3001\u6539\u5199\u7b49\uff09\uff0c\u6df1\u5165\u8bc4\u4f30\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5426\u5b9a\u662f\u8bed\u8a00\u4e2d\u7684\u57fa\u672c\u73b0\u8c61\uff0c\u4f46\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bed\u4e49\u7406\u89e3\u63d0\u51fa\u4e86\u6301\u7eed\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5c06\u5426\u5b9a\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7b49\u4efb\u52a1\u7684\u9644\u5e26\u6848\u4f8b\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5426\u5b9a\u7406\u89e3\u7684\u57fa\u51c6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51faThunder-NUBench\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Thunder-NUBench\u901a\u8fc7\u624b\u52a8\u6784\u5efa\u53e5\u5b50-\u5426\u5b9a\u5bf9\u548c\u591a\u9009\u9898\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u6807\u51c6\u5426\u5b9a\u4e0e\u5c40\u90e8\u5426\u5b9a\u3001\u77db\u76fe\u3001\u6539\u5199\u7b49\u591a\u6837\u5316\u7ed3\u6784\uff0c\u6df1\u5165\u8bc4\u4f30\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u3002", "result": "Thunder-NUBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30LLMs\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u6d4b\u8bd5\u6a21\u578b\u5728\u591a\u6837\u5316\u5426\u5b9a\u7ed3\u6784\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "Thunder-NUBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LLMs\u7684\u53e5\u5b50\u7ea7\u522b\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "paper_title_zh": "Thunder-NUBench\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u53e5\u5b50\u7ea7\u522b\u5426\u5b9a\u7406\u89e3\u7684\u57fa\u51c6", "abstract_zh": "\u5426\u5b9a\u662f\u4e00\u79cd\u57fa\u672c\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u63d0\u51fa\u4e86\u6301\u7eed\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5c06\u5426\u5b9a\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7b49\u4efb\u52a1\u7684\u9644\u5e26\u6848\u4f8b\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5426\u5b9a\u7406\u89e3\u7684\u57fa\u51c6\u3002\u672c\u6587\u63d0\u51fa\u4e86\\textbf{Thunder-NUBench}\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30LLMs\u53e5\u5b50\u7ea7\u522b\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u3002Thunder-NUBench\u4e0d\u4ec5\u9650\u4e8e\u8868\u9762\u7ebf\u7d22\u68c0\u6d4b\uff0c\u8fd8\u901a\u8fc7\u5bf9\u6bd4\u6807\u51c6\u5426\u5b9a\u4e0e\u5c40\u90e8\u5426\u5b9a\u3001\u77db\u76fe\u3001\u6539\u5199\u7b49\u591a\u6837\u5316\u7ed3\u6784\uff0c\u6df1\u5165\u8bc4\u4f30\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b\u624b\u52a8\u6784\u5efa\u7684\u53e5\u5b50-\u5426\u5b9a\u5bf9\u548c\u591a\u9009\u9898\u6570\u636e\u96c6\uff0c\u4e3a\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.14373", "pdf": "https://arxiv.org/pdf/2506.14373", "abs": "https://arxiv.org/abs/2506.14373", "authors": ["Junyeob Baek", "Hosung Lee", "Christopher Hoang", "Mengye Ren", "Sungjin Ahn"], "title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiscrete-JEPA\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u6807\u8bb0\u5316\u548c\u65b0\u9896\u7684\u4e92\u8865\u76ee\u6807\u6269\u5c55\u6f5c\u5728\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u6807\u8bb0\u7a7a\u95f4\u4e2d\u81ea\u53d1\u6d8c\u73b0\u7684\u7cfb\u7edf\u6027\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u9700\u8981\u7b26\u53f7\u62bd\u8c61\u548c\u903b\u8f91\u63a8\u7406\u7684\u7cfb\u7edf\u63a8\u65ad\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u963b\u788d\u4e86\u8ba4\u77e5\u667a\u80fd\u7684\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u652f\u6301\u7b26\u53f7\u63a8\u7406\u7684\u9c81\u68d2\u6807\u8bb0\u5316\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u6f5c\u5728\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u5f15\u5165\u8bed\u4e49\u6807\u8bb0\u5316\u548c\u65b0\u9896\u7684\u4e92\u8865\u76ee\u6807\uff0c\u4ee5\u751f\u6210\u9002\u7528\u4e8e\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u7684\u9c81\u68d2\u6807\u8bb0\u5316\u8868\u793a\u3002", "result": "Discrete-JEPA\u5728\u89c6\u89c9\u7b26\u53f7\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u6807\u8bb0\u7a7a\u95f4\u81ea\u53d1\u6d8c\u73b0\u51fa\u7cfb\u7edf\u6027\u6a21\u5f0f\u3002", "conclusion": "\u5c3d\u7ba1\u662f\u521d\u6b65\u6a21\u578b\uff0cDiscrete-JEPA\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u548c\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u63a8\u52a8\u529b\u3002", "paper_title_zh": "\u79bb\u6563JEPA\uff1a\u65e0\u9700\u91cd\u6784\u5b66\u4e60\u79bb\u6563\u6807\u8bb0\u8868\u793a", "abstract_zh": "\u8ba4\u77e5\u667a\u80fd\u7684\u6838\u5fc3\u5728\u4e8e\u4ece\u89c2\u5bdf\u4e2d\u63d0\u53d6\u9690\u85cf\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u539f\u5219\u7cfb\u7edf\u9884\u6d4b\u672a\u6765\u7ed3\u679c\u3002\u7136\u800c\uff0c\u5f53\u524d\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u9700\u8981\u7b26\u53f7\u62bd\u8c61\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u63a8\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u5c40\u9650\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faDiscrete-JEPA\uff0c\u901a\u8fc7\u8bed\u4e49\u6807\u8bb0\u5316\u548c\u65b0\u9896\u4e92\u8865\u76ee\u6807\u6269\u5c55\u6f5c\u5728\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u4e3a\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u521b\u5efa\u9c81\u68d2\u6807\u8bb0\u5316\u3002Discrete-JEPA\u5728\u89c6\u89c9\u7b26\u53f7\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u89c6\u89c9\u8bc1\u636e\u663e\u793a\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u6807\u8bb0\u7a7a\u95f4\u4e2d\u81ea\u53d1\u6d8c\u73b0\u51fa\u7cfb\u7edf\u6027\u6a21\u5f0f\u3002\u5c3d\u7ba1\u662f\u521d\u6b65\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u671b\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u548c\u89c4\u5212\u80fd\u529b\u5e26\u6765\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.14470", "pdf": "https://arxiv.org/pdf/2506.14470", "abs": "https://arxiv.org/abs/2506.14470", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8eAST\u7684\u6df7\u5408\u56fe\u8868\u793a\u5728GNN\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0AST+CFG+DFG\u80fd\u63d0\u5347\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u800cFA-AST\u53ef\u80fd\u56e0\u7ed3\u6784\u590d\u6742\u6027\u964d\u4f4e\u6027\u80fd\u3002GMN\u5728\u6807\u51c6AST\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u51cf\u5c11\u4e86\u5bf9\u589e\u5f3a\u7ed3\u6784\u7684\u9700\u6c42\u3002", "motivation": "\u4ee3\u7801\u514b\u9686\u662f\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u7684\u4e25\u91cd\u95ee\u9898\uff0c\u589e\u52a0\u6210\u672c\u548c\u98ce\u9669\u3002AST\u867d\u80fd\u7cbe\u786e\u8868\u793a\u8bed\u6cd5\u7ed3\u6784\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\u3002\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u56fe\uff08\u5982CFG\u3001DFG\uff09\u589e\u5f3aAST\uff0c\u4f46\u5176\u6548\u679c\u53ca\u4e0e\u4e0d\u540c\u56fe\u5b66\u4e60\u6280\u672f\u7684\u517c\u5bb9\u6027\u4ecd\u9700\u63a2\u7d22\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u591a\u79cd\u6df7\u5408\u56fe\u8868\u793a\uff08AST+CFG+DFG\u3001FA-AST\uff09\u5728\u591a\u79cdGNN\u67b6\u6784\uff08GCN\u3001GAT\u3001GMN\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAST+CFG+DFG\u80fd\u663e\u8457\u63d0\u5347\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u800cFA-AST\u56e0\u7ed3\u6784\u590d\u6742\u6027\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002GMN\u5728\u6807\u51c6AST\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u65e0\u9700\u4f9d\u8d56\u589e\u5f3a\u7ed3\u6784\u3002", "conclusion": "\u6df7\u5408\u56fe\u8868\u793a\u5bf9GNN\u7684\u5f71\u54cd\u56e0\u6a21\u578b\u800c\u5f02\uff0cAST+CFG+DFG\u6548\u679c\u663e\u8457\uff0c\u4f46FA-AST\u9700\u8c28\u614e\u4f7f\u7528\u3002GMN\u5728\u6807\u51c6AST\u4e0b\u5df2\u80fd\u9ad8\u6548\u68c0\u6d4b\u4ee3\u7801\u76f8\u4f3c\u6027\u3002", "paper_title_zh": "AST\u589e\u5f3a\u8fd8\u662fAST\u8fc7\u8f7d\uff1f\u6df7\u5408\u56fe\u8868\u793a\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u7684\u610f\u5916\u5f71\u54cd", "abstract_zh": "\u4f5c\u4e3a\u6700\u5177\u5371\u5bb3\u6027\u7684\u4ee3\u7801\u5f02\u5473\u4e4b\u4e00\uff0c\u4ee3\u7801\u514b\u9686\u663e\u8457\u589e\u52a0\u4e86\u8f6f\u4ef6\u7ef4\u62a4\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u98ce\u9669\uff0c\u4f7f\u5176\u68c0\u6d4b\u6210\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u56e0\u5176\u7cbe\u786e\u7684\u8bed\u6cd5\u7ed3\u6784\u8868\u793a\u5728\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u56fa\u6709\u5730\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u8fc7\u5c06AST\u4e0e\u8bed\u4e49\u56fe\uff08\u5982\u63a7\u5236\u6d41\u56feCFG\u548c\u6570\u636e\u6d41\u56feDFG\uff09\u7ed3\u5408\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002\u7136\u800c\uff0c\u5404\u79cd\u589e\u5f3a\u7684AST\u8868\u793a\u7684\u6709\u6548\u6027\u53ca\u5176\u4e0e\u4e0d\u540c\u57fa\u4e8e\u56fe\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u517c\u5bb9\u6027\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u91ca\u653e\u5176\u5728\u89e3\u51b3\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u590d\u6742\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002\u672c\u6587\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4e25\u683c\u8bc4\u4f30\u4e86\u57fa\u4e8eAST\u7684\u6df7\u5408\u56fe\u8868\u793a\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\u3002\u6211\u4eec\u7cfb\u7edf\u6bd4\u8f83\u4e86\u591a\u79cd\u6df7\u5408\u8868\u793a\uff08AST+CFG+DFG\u3001\u6d41\u589e\u5f3aAST\uff08FA-AST\uff09\uff09\u5728\u591a\u79cdGNN\u67b6\u6784\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6df7\u5408\u8868\u793a\u5bf9GNN\u7684\u5f71\u54cd\u4e0d\u540c\uff1aAST+CFG+DFG\u80fd\u6301\u7eed\u63d0\u5347\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u578b\uff08\u5982\u56fe\u5377\u79ef\u7f51\u7edcGCN\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edcGAT\uff09\u7684\u51c6\u786e\u6027\uff0c\u800cFA-AST\u5e38\u56e0\u7ed3\u6784\u590d\u6742\u6027\u635f\u5bb3\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cGMN\u5373\u4f7f\u5728\u6807\u51c6AST\u8868\u793a\u4e0b\u4e5f\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u8de8\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u4e2d\u7684\u4f18\u52bf\uff0c\u51cf\u5c11\u4e86\u5bf9\u589e\u5f3a\u7ed3\u6784\u7684\u9700\u6c42\u3002"}}
{"id": "2506.14407", "pdf": "https://arxiv.org/pdf/2506.14407", "abs": "https://arxiv.org/abs/2506.14407", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Sch\u00fctze"], "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving \"two days ago\"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.", "AI": {"tldr": "ImpliRet\u662f\u4e00\u4e2a\u65b0\u7684\u68c0\u7d22\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u6863\u4fa7\u9690\u542b\u4e8b\u5b9e\u7684\u63a8\u7406\u6311\u6218\uff0c\u800c\u975e\u4f20\u7edf\u7684\u67e5\u8be2\u4fa7\u5904\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73nDCG@10\u4ec5\u4e3a15.07%\uff0c\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u5982GPT-4.1\u4e5f\u4ec5\u8fbe\u523035.06%\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\uff08\u5982\u5173\u952e\u8bcd\u91cd\u53e0\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u6587\u6863\u4e2d\u9690\u542b\u7684\u4e8b\u5b9e\u63a8\u7406\uff08\u5982\u65f6\u95f4\u3001\u7b97\u672f\u548c\u5e38\u8bc6\u5173\u7cfb\uff09\u3002ImpliRet\u65e8\u5728\u901a\u8fc7\u7b80\u5355\u67e5\u8be2\u4f46\u590d\u6742\u6587\u6863\u4fa7\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u63a8\u52a8\u68c0\u7d22\u7cfb\u7edf\u5728\u9690\u542b\u4e8b\u5b9e\u5904\u7406\u4e0a\u7684\u8fdb\u6b65\u3002", "method": "ImpliRet\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u67e5\u8be2\u7b80\u5355\u4f46\u6587\u6863\u76f8\u5173\u6027\u4f9d\u8d56\u4e8e\u9690\u542b\u4e8b\u5b9e\uff08\u5982\u65f6\u95f4\u89e3\u6790\u3001\u7b97\u672f\u548c\u5e38\u8bc6\u5173\u7cfb\uff09\u3002\u8bc4\u4f30\u4e86\u7a00\u758f\u548c\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u7684\u6027\u80fd\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684\u68c0\u7d22\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73nDCG@10\u4e3a15.07%\u3002GPT-4.1\u5728\u5305\u542b\u6b63\u6587\u6863\u7684\u77ed\u4e0a\u4e0b\u6587\u4e2d\u4ec5\u8fbe\u523035.06%\uff0c\u8868\u660e\u6587\u6863\u4fa7\u63a8\u7406\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "ImpliRet\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u7d22\u7cfb\u7edf\u5728\u9690\u542b\u4e8b\u5b9e\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "paper_title_zh": "ImpliRet\uff1a\u9690\u542b\u4e8b\u5b9e\u68c0\u7d22\u6311\u6218\u7684\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u68c0\u7d22\u7cfb\u7edf\u662f\u8bb8\u591aNLP\u6d41\u7a0b\u7684\u6838\u5fc3\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\uff08\u5982\u5173\u952e\u8bcd\u91cd\u53e0\u548c\u8bcd\u6c47\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u3002\u4e3a\u4e86\u8bc4\u4f30\u8d85\u8d8a\u8fd9\u4e9b\u6d45\u5c42\u4fe1\u53f7\u7684\u68c0\u7d22\u80fd\u529b\uff0c\u8fd1\u671f\u57fa\u51c6\u5f15\u5165\u4e86\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u67e5\u8be2\uff1b\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u5c06\u8d1f\u62c5\u8f6c\u79fb\u5230\u67e5\u8be2\u4fa7\u5904\u7406\u6280\u672f\uff08\u5982\u63d0\u793a\u6216\u591a\u8df3\u68c0\u7d22\uff09\u4e0a\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ImpliRet\uff0c\u4e00\u4e2a\u5c06\u63a8\u7406\u6311\u6218\u8f6c\u79fb\u5230\u6587\u6863\u4fa7\u5904\u7406\u7684\u57fa\u51c6\uff1a\u67e5\u8be2\u7b80\u5355\uff0c\u4f46\u76f8\u5173\u6027\u4f9d\u8d56\u4e8e\u6587\u6863\u4e2d\u901a\u8fc7\u65f6\u95f4\uff08\u5982\u89e3\u6790\u201c\u4e24\u5929\u524d\u201d\uff09\u3001\u7b97\u672f\u548c\u5e38\u8bc6\u5173\u7cfb\u9690\u542b\u7684\u4e8b\u5b9e\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u7a00\u758f\u548c\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\uff0c\u5747\u8868\u73b0\u4e0d\u4f73\uff1a\u6700\u4f73nDCG@10\u4ec5\u4e3a15.07%\u3002\u6211\u4eec\u8fd8\u6d4b\u8bd5\u4e86\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u662f\u5426\u80fd\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002\u4f46\u5373\u4f7f\u5728\u4ec5\u5305\u542b\u5341\u4e2a\u6587\u6863\uff08\u5305\u62ec\u6b63\u6587\u6863\uff09\u7684\u77ed\u4e0a\u4e0b\u6587\u4e2d\uff0cGPT-4.1\u7684\u5f97\u5206\u4e5f\u4ec5\u4e3a35.06%\uff0c\u8868\u660e\u6587\u6863\u4fa7\u63a8\u7406\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4ee3\u7801\u53ef\u5728github.com/ZeinabTaghavi/IMPLIRET.Contribution\u83b7\u53d6\u3002"}}
{"id": "2506.14382", "pdf": "https://arxiv.org/pdf/2506.14382", "abs": "https://arxiv.org/abs/2506.14382", "authors": ["Ning Zhou", "Shanxiong Chen", "Mingting Zhou", "Haigang Sui", "Lieyun Hu", "Han Li", "Li Hua", "Qiming Zhou"], "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDepthSeg\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u63d0\u793a\u6280\u672f\u6539\u8fdb\u9065\u611f\u8bed\u4e49\u5206\u5272\uff0c\u89e3\u51b3\u5149\u8c31\u6df7\u6dc6\u548c\u9634\u5f71\u906e\u6321\u95ee\u9898\uff0c\u63d0\u5347\u5730\u7269\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5149\u8c31\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u76ee\u6807\u9ad8\u7a0b\u5dee\u5f02\uff0c\u5bfc\u81f4\u590d\u6742\u573a\u666f\u4e0b\u7684\u5730\u7269\u5206\u7c7b\u9519\u8bef\u3002DepthSeg\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u4fe1\u606f\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "DepthSeg\u6846\u67b6\u5305\u542b\u4e09\u90e8\u5206\uff1a1) \u8f7b\u91cf\u9002\u914d\u5668\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u7f16\u7801\u5668\uff1b2) \u6df1\u5ea6\u63d0\u793a\u5668\u663e\u5f0f\u5efa\u6a21\u6df1\u5ea6/\u9ad8\u5ea6\u7279\u5f81\uff1b3) \u8bed\u4e49\u5206\u7c7b\u89e3\u7801\u5668\u7ed3\u5408\u6df1\u5ea6\u63d0\u793a\u4e0e\u9ad8\u7ef4\u5730\u7269\u7279\u5f81\u3002", "result": "\u5728LiuZhou\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DepthSeg\u5728\u5730\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6df1\u5ea6\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002", "conclusion": "DepthSeg\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u5730\u7269\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DepthSeg\uff1a\u9065\u611f\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6df1\u5ea6\u63d0\u793a\u6280\u672f", "abstract_zh": "\u9065\u611f\u8bed\u4e49\u5206\u5272\u5bf9\u4e8e\u63d0\u53d6\u5730\u8868\u8be6\u7ec6\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u652f\u6301\u73af\u5883\u76d1\u6d4b\u3001\u571f\u5730\u5229\u7528\u89c4\u5212\u548c\u8d44\u6e90\u8bc4\u4f30\u7b49\u5e94\u7528\u3002\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u63a8\u52a8\u4e86\u81ea\u52a8\u9065\u611f\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e0d\u540c\u76ee\u6807\u7684\u5149\u8c31\u7279\u5f81\u533a\u5206\uff0c\u800c\u5ffd\u7565\u4e86\u76ee\u6807\u9ad8\u7a0b\u5dee\u5f02\uff0c\u5bfc\u81f4\u5728\u9634\u5f71\u906e\u6321\u548c\u5149\u8c31\u6df7\u6dc6\u7684\u590d\u6742\u573a\u666f\u4e2d\u51fa\u73b0\u5730\u7269\u5206\u7c7b\u9519\u8bef\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u63d0\u793a\u4e8c\u7ef4\u9065\u611f\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff08DepthSeg\uff09\uff0c\u80fd\u591f\u4ece\u4e8c\u7ef4\u9065\u611f\u56fe\u50cf\u4e2d\u81ea\u52a8\u5efa\u6a21\u6df1\u5ea6/\u9ad8\u5ea6\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8bed\u4e49\u5206\u5272\u6846\u67b6\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u5149\u8c31\u6df7\u6dc6\u548c\u9634\u5f71\u906e\u6321\u7684\u5f71\u54cd\u3002\u5728DepthSeg\u7684\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\uff0c\u6211\u4eec\u5f15\u5165\u8f7b\u91cf\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u5bf9\u9884\u8bad\u7ec3\u7684\u5927\u53c2\u6570\u89c6\u89c9Transformer\u7f16\u7801\u5668\u7684\u4f4e\u6210\u672c\u5fae\u8c03\u3002\u5728\u6df1\u5ea6\u63d0\u793a\u9636\u6bb5\uff0c\u63d0\u51fa\u6df1\u5ea6\u63d0\u793a\u5668\u663e\u5f0f\u5efa\u6a21\u6df1\u5ea6/\u9ad8\u5ea6\u7279\u5f81\u3002\u5728\u8bed\u4e49\u9884\u6d4b\u9636\u6bb5\uff0c\u5f15\u5165\u8bed\u4e49\u5206\u7c7b\u89e3\u7801\u5668\uff0c\u5c06\u6df1\u5ea6\u63d0\u793a\u4e0e\u9ad8\u7ef4\u5730\u7269\u7279\u5f81\u7ed3\u5408\uff0c\u5b9e\u73b0\u5730\u7269\u7c7b\u578b\u7684\u7cbe\u786e\u63d0\u53d6\u3002\u5728LiuZhou\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DepthSeg\u6846\u67b6\u5728\u5730\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002\u8be6\u7ec6\u7684\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u51f8\u663e\u4e86\u6df1\u5ea6\u63d0\u793a\u5728\u9065\u611f\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.14477", "pdf": "https://arxiv.org/pdf/2506.14477", "abs": "https://arxiv.org/abs/2506.14477", "authors": ["Jingqi Yang", "Zhilong Song", "Jiawei Chen", "Mingli Song", "Sheng Zhou", "linjun sun", "Xiaogang Ouyang", "Chun Chen", "Can Wang"], "title": "GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies", "categories": ["cs.AI"], "comment": "10 pages, 4 figures, submitted to NIPS 2025", "summary": "The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GUI-Robust\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u8bc4\u4f30GUI\u4ee3\u7406\u5728\u771f\u5b9e\u5f02\u5e38\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u800c\u8bbe\u8ba1\uff0c\u5305\u542b\u4e03\u79cd\u5e38\u89c1\u5f02\u5e38\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002\u5b9e\u9a8c\u663e\u793a\u73b0\u6709GUI\u4ee3\u7406\u5728\u5f02\u5e38\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u6570\u636e\u96c6\u591a\u57fa\u4e8e\u7406\u60f3\u5316\u6761\u4ef6\u6784\u5efa\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6837\u5f02\u5e38\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51faGUI-Robust\u6570\u636e\u96c6\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u8303\u5f0f\uff1a\u901a\u8fc7RPA\u5de5\u5177\u6536\u96c6\u81ea\u7136\u4ea4\u4e92\u7684\u7528\u6237\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u501f\u52a9MLLM\u751f\u6210\u5bf9\u5e94\u7684\u6b65\u9aa4\u548c\u4efb\u52a1\u63cf\u8ff0\uff0c\u5c06\u6807\u6ce8\u65f6\u95f4\u6210\u672c\u964d\u4f4e19\u500d\u4ee5\u4e0a\u3002", "result": "\u4f7f\u7528GUI-Robust\u8bc4\u4f30\u73b0\u6709GUI\u4ee3\u7406\uff0c\u53d1\u73b0\u5176\u5728\u5f02\u5e38\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u51f8\u663e\u4e86\u9c81\u68d2\u6027\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002", "conclusion": "GUI-Robust\u6570\u636e\u96c6\u4e3aGUI\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u547c\u5401\u672a\u6765\u66f4\u591a\u5173\u6ce8\u4ee3\u7406\u5728\u5f02\u5e38\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "paper_title_zh": "GUI-Robust\uff1a\u7528\u4e8e\u6d4b\u8bd5GUI\u4ee3\u7406\u5728\u771f\u5b9e\u5f02\u5e38\u4e2d\u9c81\u68d2\u6027\u7684\u7efc\u5408\u6570\u636e\u96c6", "abstract_zh": "\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u5f00\u53d1\u5bf9\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\uff0c\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5728\u7406\u60f3\u5316\u6761\u4ef6\u4e0b\u6784\u5efa\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u90e8\u7f72\u4e2d\u5e38\u89c1\u7684\u591a\u6837\u5f02\u5e38\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GUI-Robust\uff0c\u4e00\u4e2a\u4e13\u4e3a\u5168\u9762\u8bc4\u4f30GUI\u4ee3\u7406\u8bbe\u8ba1\u7684\u65b0\u6570\u636e\u96c6\uff0c\u660e\u786e\u5305\u542b\u4e86\u65e5\u5e38GUI\u4ea4\u4e92\u4e2d\u89c2\u5bdf\u5230\u7684\u4e03\u79cd\u5e38\u89c1\u5f02\u5e38\u7c7b\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u5316\u7684\u6570\u636e\u96c6\u6784\u5efa\u8303\u5f0f\uff1a\u901a\u8fc7RPA\u5de5\u5177\u6536\u96c6\u81ea\u7136\u4ea4\u4e92\u7684\u7528\u6237\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u501f\u52a9MLLM\u4e3a\u8fd9\u4e9b\u52a8\u4f5c\u751f\u6210\u5bf9\u5e94\u7684\u6b65\u9aa4\u548c\u4efb\u52a1\u63cf\u8ff0\u3002\u8fd9\u4e00\u8303\u5f0f\u5c06\u6807\u6ce8\u65f6\u95f4\u6210\u672c\u964d\u4f4e\u4e8619\u500d\u4ee5\u4e0a\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528GUI-Robust\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684GUI\u4ee3\u7406\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5f02\u5e38\u573a\u666f\u4e2d\u7684\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002\u6211\u4eec\u671f\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u5f3a\u8c03GUI\u4ee3\u7406\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6fc0\u53d1\u672a\u6765\u66f4\u591a\u76f8\u5173\u7814\u7a76\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u53ef\u5728https://github.com/chessbean1/GUI-Robust\u83b7\u53d6\u3002"}}
{"id": "2506.14429", "pdf": "https://arxiv.org/pdf/2506.14429", "abs": "https://arxiv.org/abs/2506.14429", "authors": ["Xiaoran Liu", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs", "categories": ["cs.CL"], "comment": "16 pages, 12 figures, work in progress", "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textbf{\\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08diffusion LLMs\uff09\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u76f4\u63a5\u4e0a\u4e0b\u6587\u5916\u63a8\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u56f0\u60d1\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5LongLLaDA\uff0c\u7ed3\u5408NTK-based RoPE\u5916\u63a8\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u6269\u6563LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728NLP\u7814\u7a76\u4e2d\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u5c1a\u672a\u88ab\u7cfb\u7edf\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6bd4\u8f83\u6269\u6563LLMs\u4e0e\u4f20\u7edf\u81ea\u56de\u5f52LLMs\u7684\u957f\u4e0a\u4e0b\u6587\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u5176\u72ec\u7279\u7684\u7279\u6027\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u6269\u6563LLMs\u5728\u76f4\u63a5\u4e0a\u4e0b\u6587\u5916\u63a8\u65f6\u7684\u7a33\u5b9a\u56f0\u60d1\u5ea6\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u201c\u5927\u6d77\u635e\u9488\u201d\u4efb\u52a1\u53d1\u73b0\u5176\u5c40\u90e8\u611f\u77e5\u73b0\u8c61\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86LongLLaDA\u65b9\u6cd5\uff0c\u7ed3\u5408LLaDA\u4e0eNTK-based RoPE\u5916\u63a8\u6280\u672f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u6563LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u5916\u63a8\u65f6\u8868\u73b0\u7a33\u5b9a\uff0c\u4e14\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u4f18\u4e8e\u81ea\u56de\u5f52LLMs\u3002LongLLaDA\u65b9\u6cd5\u9a8c\u8bc1\u4e86RoPE\u5916\u63a8\u5b9a\u5f8b\u5bf9\u6269\u6563LLMs\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5176\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u4e3a\u6269\u6563LLMs\u63d0\u4f9b\u4e86\u4e0a\u4e0b\u6587\u5916\u63a8\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u7684\u72ec\u7279\u73b0\u8c61\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u9a8c\u57fa\u51c6\u3002", "paper_title_zh": "LongLLaDA\uff1a\u89e3\u9501\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b", "abstract_zh": "\u5927\u8bed\u8a00\u6269\u6563\u6a21\u578b\uff08\u6269\u6563LLMs\uff09\u5df2\u6210\u4e3aNLP\u7814\u7a76\u7684\u91cd\u70b9\uff0c\u4f46\u5176\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u63a2\u7d22\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u6269\u6563LLMs\u4e0e\u4f20\u7edf\u81ea\u56de\u5f52LLMs\u7684\u957f\u4e0a\u4e0b\u6587\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6269\u6563LLMs\u5728\u76f4\u63a5\u4e0a\u4e0b\u6587\u5916\u63a8\u65f6\u8868\u73b0\u51fa\u72ec\u7279\u7684\u7a33\u5b9a\u56f0\u60d1\u5ea6\u7279\u6027\uff0c\u5e76\u5728\u201c\u5927\u6d77\u635e\u9488\u201d\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5c40\u90e8\u611f\u77e5\u73b0\u8c61\uff0c\u80fd\u591f\u4ece\u6700\u8fd1\u7684\u4e0a\u4e0b\u6587\u7247\u6bb5\u4e2d\u6210\u529f\u68c0\u7d22\u4fe1\u606f\u3002\u901a\u8fc7\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u7f29\u653e\u7406\u8bba\uff0c\u672c\u6587\u89e3\u91ca\u4e86\u8fd9\u4e24\u79cd\u73b0\u8c61\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86LongLLaDA\u65b9\u6cd5\uff0c\u5c06LLaDA\u4e0e\u57fa\u4e8eNTK\u7684RoPE\u5916\u63a8\u6280\u672f\u7ed3\u5408\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u73b0\u6709\u5916\u63a8\u7f29\u653e\u5b9a\u5f8b\u5bf9\u6269\u6563LLMs\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6269\u6563LLMs\u5728\u90e8\u5206\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u4f18\u4e8e\u81ea\u56de\u5f52LLMs\uff0c\u800c\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u4e0d\u4ec5\u4e3a\u6269\u6563LLMs\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e0a\u4e0b\u6587\u5916\u63a8\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u672a\u6765\u957f\u4e0a\u4e0b\u6587\u6269\u6563LLMs\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u89c1\u89e3\u548c\u5b9e\u9a8c\u57fa\u51c6\u3002"}}
{"id": "2506.14384", "pdf": "https://arxiv.org/pdf/2506.14384", "abs": "https://arxiv.org/abs/2506.14384", "authors": ["Huan Kang", "Hui Li", "Xiao-Jun Wu", "Tianyang Xu", "Rui Wang", "Chunyang Cheng", "Josef Kittler"], "title": "GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion", "categories": ["cs.CV"], "comment": "16 pages, 11 figures", "summary": "In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.\n  However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot\n  encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic\n  similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.\n  While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To\n  address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible\n  image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the\n  Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into\n  high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic\n  fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on\n  a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high\n  correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both\n  qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236GrFormer\uff0c\u7528\u4e8e\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\uff0c\u901a\u8fc7\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6620\u5c04\u548c\u591a\u5c3a\u5ea6\u8bed\u4e49\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u878d\u5408\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5efa\u6a21\uff0c\u65e0\u6cd5\u6355\u6349\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u5185\u5728\u62d3\u6251\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0d\u8db3\u548c\u878d\u5408\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e73\u8861\u4f4e\u9891\u8bed\u4e49\u4e0e\u9ad8\u9891\u7ec6\u8282\u3002", "method": "\u63d0\u51faGrFormer\uff0c\u901a\u8fc7Grassmann\u6d41\u5f62\u4e0a\u7684\u6295\u5f71\u7ea6\u675f\u6784\u5efa\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6620\u5c04\uff0c\u5c06\u6ce8\u610f\u529b\u7279\u5f81\u538b\u7f29\u5230\u4e0d\u540c\u79e9\u7ea7\u522b\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u8bed\u4e49\u878d\u5408\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u534f\u65b9\u5dee\u63a9\u7801\u7684\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff08CMS\uff09\u4ee5\u6700\u5927\u5316\u4e92\u8865\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGrFormer\u5728\u591a\u4e2a\u56fe\u50cf\u878d\u5408\u57fa\u51c6\u4e0a\u5b9a\u6027\u548c\u5b9a\u91cf\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GrFormer\u901a\u8fc7Grassmann\u6d41\u5f62\u548c\u591a\u5c3a\u5ea6\u8bed\u4e49\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "GrFormer\uff1a\u4e00\u79cd\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u65b0\u578bTransformer\u7528\u4e8e\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408", "abstract_zh": "\u5728\u56fe\u50cf\u878d\u5408\u9886\u57df\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u6a21\u6001\u6570\u636e\u5efa\u6a21\u4e3a\u7ebf\u6027\u5b50\u7a7a\u95f4\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5b9e\u9645\u4e2d\u6e90\u56fe\u50cf\u5e38\u4f4d\u4e8e\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u6b27\u51e0\u91cc\u5f97\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5176\u5185\u5728\u62d3\u6251\u7ed3\u6784\u3002\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u5185\u79ef\u8ba1\u7b97\u7684\u662f\u4ee3\u6570\u76f8\u4f3c\u6027\u800c\u975e\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u8f93\u51fa\u4e0d\u7406\u60f3\u548c\u878d\u5408\u6027\u80fd\u4e0b\u964d\u3002\u800c\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u9700\u5e73\u8861\u4f4e\u9891\u7ec6\u8282\u4e0e\u9ad8\u9891\u8bed\u4e49\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236GrFormer\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728Grassmann\u6d41\u5f62\u4e0a\u7684\u6295\u5f71\u7ea6\u675f\u6784\u5efa\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6620\u5c04\uff0c\u5c06\u6ce8\u610f\u529b\u7279\u5f81\u538b\u7f29\u5230\u4e0d\u540c\u79e9\u7ea7\u522b\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u8feb\u4f7f\u7279\u5f81\u89e3\u8026\u4e3a\u9ad8\u9891\u7ec6\u8282\uff08\u5c40\u90e8\u4f4e\u79e9\uff09\u548c\u4f4e\u9891\u8bed\u4e49\uff08\u5168\u5c40\u4f4e\u79e9\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u8bed\u4e49\u878d\u5408\u3002\u6b64\u5916\uff0c\u4e3a\u6709\u6548\u6574\u5408\u91cd\u8981\u4fe1\u606f\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u534f\u65b9\u5dee\u63a9\u7801\u7684\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff08CMS\uff09\uff0c\u4ee5\u6700\u5927\u5316\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u4e92\u8865\u6027\u5e76\u6291\u5236\u9ad8\u76f8\u5173\u6027\u7279\u5f81\uff08\u89c6\u4e3a\u5197\u4f59\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u7f51\u7edc\u5728\u591a\u4e2a\u56fe\u50cf\u878d\u5408\u57fa\u51c6\u4e0a\u5b9a\u6027\u548c\u5b9a\u91cf\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u89c1https://github.com/Shaoyun2023\u3002"}}
{"id": "2506.14496", "pdf": "https://arxiv.org/pdf/2506.14496", "abs": "https://arxiv.org/abs/2506.14496", "authors": ["Muhammad Atta Ur Rahman", "Melanie Schranz"], "title": "LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?", "categories": ["cs.AI"], "comment": "This is the author's version of a paper submitted to IEEE Intelligent Systems. 6 Tables, 3 Figures", "summary": "Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u4f20\u7edf\u7fa4\u667a\u80fd\u7b97\u6cd5\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7fa4\u667a\u80fd\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u73b0\u4ee3AI\u4e2d\u5206\u6563\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6d8c\u73b0\u884c\u4e3a\u7684\u91cd\u65b0\u5b9a\u4e49\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u8303\u5f0f\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c'\u7fa4'\u7684\u6982\u5ff5\u88ab\u6269\u5c55\u5230\u63cf\u8ff0\u7531\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684AI\u7cfb\u7edf\uff0c\u5982OpenAI\u7684Swarm\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4f20\u7edf\u7fa4\u667a\u80fd\u7b97\u6cd5\u4e0eLLM\u9a71\u52a8\u7684\u7fa4\u667a\u80fd\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u73b0\u4ee3AI\u4e2d\u7fa4\u667a\u80fd\u7684\u91cd\u65b0\u5b9a\u4e49\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5b9e\u73b0\u548c\u6bd4\u8f83Boids\u7b97\u6cd5\u548c\u8681\u7fa4\u4f18\u5316\uff08ACO\uff09\u4e24\u79cd\u8303\u5f0f\uff0c\u8bc4\u4f30\u4e86\u5ef6\u8fdf\u3001\u8d44\u6e90\u4f7f\u7528\u548c\u884c\u4e3a\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u4e91\u7aef\u548c\u672c\u5730LLM\u5728\u7fa4\u667a\u80fd\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLM\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u548c\u62bd\u8c61\u80fd\u529b\uff0c\u4f46\u5728\u8ba1\u7b97\u548c\u534f\u8c03\u65b9\u9762\u5f15\u5165\u4e86\u65b0\u7684\u9650\u5236\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7fa4\u667a\u80fd\u8bbe\u8ba1\u7684\u7406\u5ff5\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5c06LLM\u96c6\u6210\u5230\u7fa4\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u673a\u9047\u548c\u9650\u5236\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u4ee3AI\u7814\u7a76\u4e2d'\u7fa4'\u6982\u5ff5\u7684\u6f14\u53d8\u3002", "paper_title_zh": "LLM\u9a71\u52a8\u7684\u7fa4\u667a\u80fd\uff1a\u65b0\u524d\u6cbf\u8fd8\u662f\u6982\u5ff5\u5ef6\u4f38\uff1f", "abstract_zh": "\u4f20\u7edf\u7fa4\u667a\u80fd\u6307\u7684\u662f\u7531\u7b80\u5355\u3001\u5206\u6563\u7684\u4ee3\u7406\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u5176\u5c40\u90e8\u4ea4\u4e92\u5bfc\u81f4\u6d8c\u73b0\u7684\u96c6\u4f53\u884c\u4e3a\u3002\u6700\u8fd1\uff0c'\u7fa4'\u8fd9\u4e00\u672f\u8bed\u88ab\u6269\u5c55\u5230\u63cf\u8ff0\u50cfOpenAI\u7684Swarm\u8fd9\u6837\u7684AI\u7cfb\u7edf\uff0c\u5176\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u534f\u4f5c\u4ee3\u7406\u3002\u672c\u6587\u5bf9\u6bd4\u4e86\u4f20\u7edf\u7fa4\u7b97\u6cd5\u4e0eLLM\u9a71\u52a8\u7684\u7fa4\u667a\u80fd\uff0c\u63a2\u8ba8\u4e86\u73b0\u4ee3\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e2d\u5206\u6563\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6d8c\u73b0\u884c\u4e3a\u7684\u91cd\u65b0\u5b9a\u4e49\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u73b0\u548c\u6bd4\u8f83Boids\u548c\u8681\u7fa4\u4f18\u5316\uff08ACO\uff09\u4e24\u79cd\u8303\u5f0f\uff0c\u8bc4\u4f30\u4e86\u5ef6\u8fdf\u3001\u8d44\u6e90\u4f7f\u7528\u548c\u884c\u4e3a\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u4e91\u7aef\u548c\u672c\u5730LLM\u5728\u7fa4\u667a\u80fd\u4e2d\u7684\u9002\u7528\u6027\u3002\u5c3d\u7ba1LLM\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u548c\u62bd\u8c61\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5f15\u5165\u4e86\u8ba1\u7b97\u548c\u534f\u8c03\u65b9\u9762\u7684\u65b0\u9650\u5236\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7fa4\u667a\u80fd\u8bbe\u8ba1\u7684\u7406\u5ff5\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06LLM\u96c6\u6210\u5230\u7fa4\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u673a\u9047\u548c\u9650\u5236\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u4ee3AI\u7814\u7a76\u4e2d'\u7fa4'\u6982\u5ff5\u7684\u6f14\u53d8\u3002"}}
{"id": "2506.14448", "pdf": "https://arxiv.org/pdf/2506.14448", "abs": "https://arxiv.org/abs/2506.14448", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "categories": ["cs.CL"], "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6d4b\u8bd5\u65f6\u5b66\u4e60\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u5bf9\u6bd4\u3002\u7ed3\u679c\u663e\u793aLLM\u5177\u5907\u6d4b\u8bd5\u65f6\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u8fdb\u6b65\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0d\u53ca\u4eba\u7c7b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u9759\u6001\u77e5\u8bc6\uff0c\u800c\u667a\u80fd\u8fd8\u5305\u62ec\u4ece\u7ecf\u9a8c\u4e2d\u5feb\u901f\u5b66\u4e60\u7684\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30LLM\u5728\u6d4b\u8bd5\u65f6\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8861\u91cf\u5176\u667a\u80fd\u6c34\u5e73\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5ba2\u89c2\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u6709\u9650\u7ecf\u9a8c\u548c\u7d2f\u79ef\u7ecf\u9a8c\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5305\u542b\u56db\u79cd\u7ecf\u9a8c\u8868\u793a\u5f62\u5f0f\u3002\u540c\u65f6\u62db\u52df\u516b\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u5b8c\u6210\u76f8\u540c\u4efb\u52a1\u4f5c\u4e3a\u57fa\u7ebf\u3002", "result": "LLM\u8868\u73b0\u51fa\u53ef\u6d4b\u91cf\u7684\u6d4b\u8bd5\u65f6\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5728\u7d2f\u79ef\u7ecf\u9a8c\u4e0b\u8fdb\u6b65\u4e0d\u7a33\u5b9a\u4e14\u901f\u5ea6\u6162\u4e8e\u4eba\u7c7b\u3002\u8fd9\u8868\u660eLLM\u4e0e\u4eba\u7c7b\u5728\u52a8\u6001\u5b66\u4e60\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "LLM\u5177\u5907\u901a\u7528\u5b66\u4e60\u673a\u5668\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u52a8\u6001\u5b66\u4e60\u80fd\u529b\u4e0a\u4e0e\u4eba\u7c7b\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "paper_title_zh": "LLM\u80fd\u4ece\u7ecf\u9a8c\u4e2d\u8fdb\u6b65\u591a\u8fdc\uff1f\u901a\u8fc7\u4eba\u7c7b\u5bf9\u6bd4\u6d4b\u91cfLLM\u7684\u6d4b\u8bd5\u65f6\u5b66\u4e60\u80fd\u529b", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u8bbe\u8ba1\u53ef\u80fd\u5f71\u54cd\u6211\u4eec\u8fc8\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u8f68\u8ff9\uff0c\u5168\u9762\u4e14\u524d\u77bb\u6027\u7684\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u9759\u6001\u77e5\u8bc6\uff0c\u800c\u667a\u80fd\u8fd8\u5305\u62ec\u4ece\u7ecf\u9a8c\u4e2d\u5feb\u901f\u5b66\u4e60\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u5021\u8bc4\u4f30\u6d4b\u8bd5\u65f6\u5b66\u4e60\u80fd\u529b\uff0c\u5373\u5728\u6d4b\u8bd5\u671f\u95f4\u57fa\u4e8e\u7ecf\u9a8c\u6539\u8fdb\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u8868\u73b0\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u8bed\u4e49\u6e38\u620f\u4f5c\u4e3a\u8bc4\u4f30\u6d4b\u8bd5\u65f6\u5b66\u4e60\u7684\u6709\u6548\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u5176\u6297\u9971\u548c\u6027\u548c\u5bf9\u7b56\u7565\u63a8\u7406\u7684\u56fa\u6709\u9700\u6c42\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5ba2\u89c2\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u6709\u9650\u548c\u7d2f\u79ef\u7ecf\u9a8c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5305\u542b\u56db\u79cd\u7ecf\u9a8c\u8868\u793a\u5f62\u5f0f\u3002\u4e3a\u63d0\u4f9b\u5bf9\u6bd4\u57fa\u7ebf\uff0c\u6211\u4eec\u62db\u52df\u4e86\u516b\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u5b8c\u6210\u76f8\u540c\u4efb\u52a1\u3002\u7ed3\u679c\u663e\u793a\uff0cLLM\u8868\u73b0\u51fa\u53ef\u6d4b\u91cf\u7684\u6d4b\u8bd5\u65f6\u5b66\u4e60\u80fd\u529b\uff1b\u7136\u800c\uff0c\u5176\u5728\u7d2f\u79ef\u7ecf\u9a8c\u4e0b\u7684\u8fdb\u6b65\u4e0d\u5982\u4eba\u7c7b\u7a33\u5b9a\u4e14\u901f\u5ea6\u8f83\u6162\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86LLM\u4f5c\u4e3a\u901a\u7528\u5b66\u4e60\u673a\u5668\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u663e\u8457\u667a\u80fd\u5dee\u8ddd\uff0c\u65e0\u8bbaLLM\u5728\u9759\u6001\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u5982\u4f55\u3002"}}
{"id": "2506.14399", "pdf": "https://arxiv.org/pdf/2506.14399", "abs": "https://arxiv.org/abs/2506.14399", "authors": ["Tian Xia", "Fabio De Sousa Ribeiro", "Rajat R Rasal", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08DCFG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u53cd\u4e8b\u5b9e\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u7ec4\u6761\u4ef6\u63a7\u5236\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5c5e\u6027\u653e\u5927\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5e72\u9884\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u751f\u6210\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u65e8\u5728\u6a21\u62df\u7279\u5b9a\u56e0\u679c\u5e72\u9884\u4e0b\u7684\u771f\u5b9e\u89c6\u89c9\u7ed3\u679c\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u5168\u5c40\u6743\u91cd\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u8eab\u4efd\u4fdd\u6301\u4e0d\u4f73\u548c\u865a\u5047\u5c5e\u6027\u53d8\u5316\uff08\u5c5e\u6027\u653e\u5927\u73b0\u8c61\uff09\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08DCFG\uff09\u6846\u67b6\uff0c\u91c7\u7528\u5c5e\u6027\u5206\u5272\u5d4c\u5165\u7b56\u7565\uff0c\u5c06\u8bed\u4e49\u8f93\u5165\u89e3\u8026\uff0c\u5b9e\u73b0\u5bf9\u7528\u6237\u5b9a\u4e49\u5c5e\u6027\u7ec4\u7684\u9009\u62e9\u6027\u5f15\u5bfc\u3002\u5728\u53cd\u4e8b\u5b9e\u751f\u6210\u4e2d\uff0c\u57fa\u4e8e\u56e0\u679c\u56fe\u5c06\u5c5e\u6027\u5206\u4e3a\u5e72\u9884\u7ec4\u548c\u4e0d\u53d8\u7ec4\uff0c\u5e76\u5206\u522b\u5e94\u7528\u4e0d\u540c\u7684\u5f15\u5bfc\u3002", "result": "\u5728CelebA-HQ\u3001MIMIC-CXR\u548cEMBED\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCFG\u63d0\u9ad8\u4e86\u5e72\u9884\u4fdd\u771f\u5ea6\uff0c\u51cf\u5c11\u4e86\u610f\u5916\u53d8\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u9006\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "DCFG\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7ec4\u6761\u4ef6\u63a7\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5c5e\u6027\u653e\u5927\u95ee\u9898\uff0c\u4e3a\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u53cd\u4e8b\u5b9e\u6269\u6563\u6a21\u578b\u7684\u89e3\u8026\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u65b9\u6cd5", "abstract_zh": "\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u65e8\u5728\u6a21\u62df\u7279\u5b9a\u56e0\u679c\u5e72\u9884\u4e0b\u7684\u771f\u5b9e\u89c6\u89c9\u7ed3\u679c\u3002\u6269\u6563\u6a21\u578b\u6700\u8fd1\u6210\u4e3a\u8fd9\u4e00\u4efb\u52a1\u7684\u6709\u529b\u5de5\u5177\uff0c\u7ed3\u5408\u4e86DDIM\u53cd\u6f14\u548c\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u7684\u6761\u4ef6\u751f\u6210\u3002\u7136\u800c\uff0c\u6807\u51c6CFG\u5bf9\u6240\u6709\u6761\u4ef6\u53d8\u91cf\u5e94\u7528\u5355\u4e00\u5168\u5c40\u6743\u91cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u8eab\u4efd\u4fdd\u6301\u4e0d\u4f73\u548c\u865a\u5047\u5c5e\u6027\u53d8\u5316\uff08\u5c5e\u6027\u653e\u5927\u73b0\u8c61\uff09\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u89e3\u8026\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08DCFG\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u5206\u7ec4\u6761\u4ef6\u63a7\u5236\u3002DCFG\u57fa\u4e8e\u5c5e\u6027\u5206\u5272\u5d4c\u5165\u7b56\u7565\uff0c\u89e3\u8026\u8bed\u4e49\u8f93\u5165\uff0c\u5b9e\u73b0\u5bf9\u7528\u6237\u5b9a\u4e49\u5c5e\u6027\u7ec4\u7684\u9009\u62e9\u6027\u5f15\u5bfc\u3002\u5728\u53cd\u4e8b\u5b9e\u751f\u6210\u4e2d\uff0c\u6211\u4eec\u6839\u636e\u56e0\u679c\u56fe\u5c06\u5c5e\u6027\u5206\u4e3a\u5e72\u9884\u7ec4\u548c\u4e0d\u53d8\u7ec4\uff0c\u5e76\u5bf9\u6bcf\u7ec4\u5e94\u7528\u4e0d\u540c\u7684\u5f15\u5bfc\u3002\u5728CelebA-HQ\u3001MIMIC-CXR\u548cEMBED\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCFG\u63d0\u9ad8\u4e86\u5e72\u9884\u4fdd\u771f\u5ea6\uff0c\u51cf\u5c11\u4e86\u610f\u5916\u53d8\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u9006\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2506.14502", "pdf": "https://arxiv.org/pdf/2506.14502", "abs": "https://arxiv.org/abs/2506.14502", "authors": ["Xiao Wang", "Junru Yu", "Jun Huang", "Qiong Wu", "Ljubo Vacic", "Changyin Sun"], "title": "Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow", "categories": ["cs.AI"], "comment": null, "summary": "Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u4f18\u5148\u7684\u7c7b\u4eba\u51b3\u7b56\u6846\u67b6\uff08SF-HLDM\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4ea4\u901a\u6d41\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u8212\u9002\u4e14\u793e\u4f1a\u517c\u5bb9\u7684\u9a7e\u9a76\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u3001\u793e\u4f1a\u5408\u89c4\u6027\u4f30\u8ba1\u6a21\u5757\u548c\u6df1\u5ea6\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u6709\u6548\u907f\u514d\u5c40\u90e8\u6700\u4f18\u9677\u9631\u5e76\u63d0\u9ad8\u51b3\u7b56\u7075\u6d3b\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u63d0\u5347\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4ea4\u901a\u6d41\uff08\u5c24\u5176\u662f\u5bc6\u96c6\u548c\u4ea4\u4e92\u573a\u666f\uff09\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u8fc1\u79fb\u6027\u5dee\u3001\u641c\u7d22\u6210\u672c\u9ad8\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u884c\u4e3a\u7b56\u7565\u7684\u6548\u7387\u548c\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u517c\u987e\u5b89\u5168\u6027\u548c\u7c7b\u4eba\u51b3\u7b56\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faSF-HLDM\u6846\u67b6\uff0c\u5305\u542b\u4ee5\u4e0b\u6a21\u5757\uff1a1\uff09\u65f6\u7a7a\u6ce8\u610f\u529b\uff08S-TA\uff09\u673a\u5236\uff0c\u7528\u4e8e\u63a8\u65ad\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u610f\u56fe\uff1b2\uff09\u793e\u4f1a\u5408\u89c4\u6027\u4f30\u8ba1\u6a21\u5757\uff0c\u7528\u4e8e\u884c\u4e3a\u8c03\u8282\uff1b3\uff09\u6df1\u5ea6\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\uff08DERL\uff09\u6a21\u578b\uff0c\u9ad8\u6548\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\uff0c\u907f\u514d\u5c40\u90e8\u6700\u4f18\u548c\u8fc7\u62df\u5408\u98ce\u9669\u3002", "result": "SF-HLDM\u6846\u67b6\u4f7f\u81ea\u52a8\u9a7e\u9a76AI\u4ee3\u7406\u80fd\u591f\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u53c2\u6570\uff0c\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb\u5e76\u9002\u5e94\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u9a7e\u9a76\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u8212\u9002\u4e14\u793e\u4f1a\u517c\u5bb9\u7684\u9a7e\u9a76\u51b3\u7b56\u3002", "conclusion": "SF-HLDM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4ea4\u901a\u6d41\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u517c\u5177\u5b89\u5168\u6027\u548c\u7c7b\u4eba\u7075\u6d3b\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u9762\u5411\u65f6\u53d8\u4ea4\u901a\u6d41\u7684\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u4f18\u5148\u7c7b\u4eba\u51b3\u7b56\u7814\u7a76", "abstract_zh": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u8fdb\u6b65\u5728\u63d0\u5347\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u5728\u65f6\u53d8\u4ea4\u901a\u6d41\uff08\u5c24\u5176\u662f\u5bc6\u96c6\u548c\u4ea4\u4e92\u573a\u666f\uff09\u4e2d\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u540c\u65f6\uff0c\u4eba\u7c7b\u5177\u6709\u81ea\u7531\u610f\u5fd7\uff0c\u5373\u4f7f\u5728\u5b8c\u5168\u76f8\u540c\u7684\u573a\u666f\u4e0b\u4e5f\u901a\u5e38\u4e0d\u4f1a\u505a\u51fa\u76f8\u540c\u7684\u51b3\u7b56\uff0c\u5bfc\u81f4\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5b58\u5728\u8fc1\u79fb\u6027\u5dee\u548c\u641c\u7d22\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u884c\u4e3a\u7b56\u7565\u7684\u6548\u7387\u548c\u6548\u679c\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u4f18\u5148\u7684\u7c7b\u4eba\u51b3\u7b56\u6846\u67b6\uff08SF-HLDM\uff09\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u591f\u5b89\u5168\u3001\u8212\u9002\u4e14\u793e\u4f1a\u517c\u5bb9\u5730\u9a7e\u9a76\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u5206\u5c42\u9012\u8fdb\u7ed3\u6784\uff0c\u5305\u62ec\u7528\u4e8e\u63a8\u65ad\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u610f\u56fe\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\uff08S-TA\uff09\u673a\u5236\u3001\u7528\u4e8e\u884c\u4e3a\u8c03\u8282\u7684\u793e\u4f1a\u5408\u89c4\u6027\u4f30\u8ba1\u6a21\u5757\uff0c\u4ee5\u53ca\u7528\u4e8e\u9ad8\u6548\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\u7684\u6df1\u5ea6\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\uff08DERL\uff09\u6a21\u578b\uff0c\u4ece\u800c\u907f\u514d\u9677\u5165\u5c40\u90e8\u6700\u4f18\u9677\u9631\u5e76\u964d\u4f4e\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u5b9e\u73b0\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u7684\u7c7b\u4eba\u51b3\u7b56\u3002SF-HLDM\u6846\u67b6\u4f7f\u81ea\u52a8\u9a7e\u9a76AI\u4ee3\u7406\u80fd\u591f\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb\u7684\u540c\u65f6\u9075\u5faa\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u9a7e\u9a76\u884c\u4e3a\u3002"}}
{"id": "2506.14474", "pdf": "https://arxiv.org/pdf/2506.14474", "abs": "https://arxiv.org/abs/2506.14474", "authors": ["Eyal German", "Sagiv Antebi", "Edan Habler", "Asaf Shabtai", "Yuval Elovici"], "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLexiMark\u7684\u65b0\u578b\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u9ad8\u71b5\u8bcd\u6765\u589e\u5f3aLLM\u5bf9\u6c34\u5370\u6587\u672c\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u4ece\u800c\u5728LLM\u8bad\u7ec3\u6570\u636e\u4e2d\u5b9e\u73b0\u96be\u4ee5\u68c0\u6d4b\u548c\u79fb\u9664\u7684\u6210\u5458\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u80fd\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u7f3a\u4e4f\u9690\u853d\u6027\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\u548c\u79fb\u9664\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9690\u853d\u4e14\u96be\u4ee5\u79fb\u9664\u7684\u6c34\u5370\u6280\u672f\uff0c\u4ee5\u9a8c\u8bc1LLM\u662f\u5426\u4f7f\u7528\u4e86\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u3002", "method": "LexiMark\u901a\u8fc7\u9009\u62e9\u9ad8\u71b5\u8bcd\u5e76\u8fdb\u884c\u540c\u4e49\u8bcd\u66ff\u6362\uff0c\u5c06\u6c34\u5370\u5d4c\u5165\u6587\u672c\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e2\u4e0d\u5f71\u54cd\u6587\u672c\u8bed\u4e49\uff0c\u53c8\u589e\u5f3a\u4e86LLM\u5bf9\u6c34\u5370\u6587\u672c\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f7f\u6c34\u5370\u96be\u4ee5\u88ab\u68c0\u6d4b\u548c\u79fb\u9664\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\uff08\u5982LLaMA-1 7B\u3001Mistral 7B\u7b49\uff09\u548c\u4e0d\u540c\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u8fdb\u884c\uff0c\u7ed3\u679c\u663e\u793aLexiMark\u7684AUROC\u5206\u6570\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u9a8c\u8bc1\u4e86\u6c34\u5370\u6570\u636e\u662f\u5426\u88ab\u7528\u4e8eLLM\u8bad\u7ec3\u3002", "conclusion": "LexiMark\u4f5c\u4e3a\u4e00\u79cd\u9690\u853d\u4e14\u9c81\u68d2\u7684\u6c34\u5370\u6280\u672f\uff0c\u80fd\u591f\u53ef\u9760\u5730\u9a8c\u8bc1LLM\u662f\u5426\u4f7f\u7528\u4e86\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u6570\u636e\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "LexiMark\uff1a\u901a\u8fc7\u8bcd\u6c47\u66ff\u6362\u589e\u5f3aLLM\u6587\u672c\u8bad\u7ec3\u6570\u636e\u6210\u5458\u9a8c\u8bc1\u7684\u9c81\u68d2\u6c34\u5370\u6280\u672f", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u80fd\u5728\u672a\u7ecf\u6570\u636e\u6240\u6709\u8005\u540c\u610f\u7684\u60c5\u51b5\u4e0b\u88ab\u8bad\u7ec3\u6216\u5fae\u8c03\u3002\u9a8c\u8bc1\u7279\u5b9aLLM\u662f\u5426\u4f7f\u7528\u4e86\u67d0\u4e9b\u6570\u636e\u5b9e\u4f8b\u6216\u6574\u4e2a\u6570\u636e\u96c6\u6781\u5177\u6311\u6218\u6027\u3002\u6570\u636e\u96c6\u6c34\u5370\u901a\u8fc7\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u5d4c\u5165\u53ef\u8bc6\u522b\u7684\u4fee\u6539\u6765\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u9690\u853d\u6027\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\u548c\u79fb\u9664\u3002\u9488\u5bf9\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LexiMark\uff0c\u4e00\u79cd\u4e13\u4e3a\u6587\u672c\u548c\u6587\u6863\u8bbe\u8ba1\u7684\u65b0\u578b\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u5bf9\u7cbe\u5fc3\u9009\u62e9\u7684\u9ad8\u71b5\u8bcd\u8fdb\u884c\u540c\u4e49\u8bcd\u66ff\u6362\u6765\u5d4c\u5165\u6c34\u5370\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u589e\u5f3aLLM\u5bf9\u6c34\u5370\u6587\u672c\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u6539\u53d8\u6587\u672c\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u3002\u56e0\u6b64\uff0c\u6c34\u5370\u96be\u4ee5\u88ab\u68c0\u6d4b\uff0c\u5b8c\u7f8e\u878d\u5165\u6587\u672c\u4e14\u65e0\u53ef\u89c1\u6807\u8bb0\uff0c\u5e76\u56e0\u5176\u5fae\u5999\u4e14\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u66ff\u6362\u800c\u62b5\u6297\u81ea\u52a8\u548c\u624b\u52a8\u68c0\u6d4b\u3002\u6211\u4eec\u4f7f\u7528\u8fd1\u671f\u7814\u7a76\u7684\u57fa\u7ebf\u6570\u636e\u96c6\u548c\u4e03\u4e2a\u5f00\u6e90\u6a21\u578b\uff08\u5305\u62ecLLaMA-1 7B\u3001LLaMA-3 8B\u3001Mistral 7B\u3001Pythia 6.9B\u53ca\u5176\u4e09\u4e2a\u8f83\u5c0f\u53d8\u4f53160M\u3001410M\u548c1B\uff09\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u3002\u8bc4\u4f30\u6db5\u76d6\u591a\u79cd\u8bad\u7ec3\u8bbe\u7f6e\uff0c\u5305\u62ec\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u573a\u666f\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cAUROC\u5206\u6570\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53ef\u9760\u9a8c\u8bc1\u672a\u7ecf\u6388\u6743\u6c34\u5370\u6570\u636e\u662f\u5426\u7528\u4e8eLLM\u8bad\u7ec3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.14404", "pdf": "https://arxiv.org/pdf/2506.14404", "abs": "https://arxiv.org/abs/2506.14404", "authors": ["Nikos Spyrou", "Athanasios Vlontzos", "Paraskevas Pegios", "Thomas Melistas", "Nefeli Gkouti", "Yannis Panagakis", "Giorgos Papanastasiou", "Sotirios A. Tsaftaris"], "title": "Causally Steered Diffusion for Automated Video Counterfactual Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic \"what-if\" video scenarios in diverse areas such as healthcare and digital media.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u89c6\u9891\u53cd\u4e8b\u5b9e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6587\u672c\u63d0\u793a\u5f15\u5bfc\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u56e0\u679c\u4e00\u81f4\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u7f16\u8f91\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u867d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bb9\u6613\u5ffd\u7565\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u4e0d\u771f\u5b9e\u6216\u8bef\u5bfc\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u786e\u4fdd\u53cd\u4e8b\u5b9e\u89c6\u9891\u751f\u6210\u7684\u56e0\u679c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u5e95\u5c42\u89c6\u9891\u7f16\u8f91\u7cfb\u7edf\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5f15\u5bfc\u751f\u6210\u3002\u57fa\u4e8e\u5047\u8bbe\u7684\u56e0\u679c\u56fe\u4f18\u5316\u6587\u672c\u63d0\u793a\uff0c\u63a7\u5236\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u786e\u4fdd\u56e0\u679c\u5173\u7cfb\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u56e0\u679c\u4e00\u81f4\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u89c6\u9891\u8d28\u91cf\u6307\u6807\u548c\u53cd\u4e8b\u5b9e\u7279\u5b9a\u6807\u51c6\uff08\u5982\u56e0\u679c\u6709\u6548\u6027\u548c\u6700\u5c0f\u6027\uff09\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u56e0\u679c\u5f15\u5bfc\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5206\u5e03\u5185\u751f\u6210\u771f\u5b9e\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\uff0c\u9002\u7528\u4e8e\u533b\u7597\u548c\u6570\u5b57\u5a92\u4f53\u7b49\u9886\u57df\u3002", "paper_title_zh": "\u57fa\u4e8e\u56e0\u679c\u5f15\u5bfc\u7684\u81ea\u52a8\u89c6\u9891\u53cd\u4e8b\u5b9e\u751f\u6210", "abstract_zh": "\u5c06\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u89c6\u9891\u7f16\u8f91\u5df2\u663e\u793a\u51fa\u8f83\u5f3a\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u4f46\u5728\u4fdd\u6301\u89c6\u9891\u5185\u5bb9\u56e0\u679c\u5173\u7cfb\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u82e5\u5ffd\u7565\u56e0\u679c\u5173\u7cfb\uff0c\u7f16\u8f91\u53ef\u80fd\u5f71\u54cd\u56e0\u679c\u4f9d\u8d56\u5c5e\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u4e0d\u771f\u5b9e\u6216\u8bef\u5bfc\u6027\u7ed3\u679c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5f15\u5bfc\u7684\u56e0\u679c\u5fe0\u5b9e\u6846\u67b6\uff0c\u7528\u4e8e\u53cd\u4e8b\u5b9e\u89c6\u9891\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u5e95\u5c42\u89c6\u9891\u7f16\u8f91\u7cfb\u7edf\uff0c\u65e0\u9700\u8bbf\u95ee\u5176\u5185\u90e8\u673a\u5236\u6216\u5fae\u8c03\uff0c\u800c\u662f\u901a\u8fc7\u57fa\u4e8e\u5047\u8bbe\u56e0\u679c\u56fe\u4f18\u5316\u6587\u672c\u63d0\u793a\uff0c\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u6f5c\u5728\u7a7a\u95f4\u63a7\u5236\u7684\u96be\u9898\u3002\u6211\u4eec\u4f7f\u7528\u6807\u51c6\u89c6\u9891\u8d28\u91cf\u6307\u6807\u548c\u53cd\u4e8b\u5b9e\u7279\u5b9a\u6807\u51c6\uff08\u5982\u56e0\u679c\u6709\u6548\u6027\u548c\u6700\u5c0f\u6027\uff09\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u56e0\u679c\u5f15\u5bfc\uff0c\u53ef\u4ee5\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5206\u5e03\u5185\u6709\u6548\u751f\u6210\u56e0\u679c\u4e00\u81f4\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\u3002\u7531\u4e8e\u4e0e\u4efb\u4f55\u9ed1\u76d2\u89c6\u9891\u7f16\u8f91\u7cfb\u7edf\u517c\u5bb9\uff0c\u8be5\u65b9\u6cd5\u5728\u533b\u7597\u548c\u6570\u5b57\u5a92\u4f53\u7b49\u9886\u57df\u5177\u6709\u751f\u6210\u771f\u5b9e\u201c\u5047\u8bbe\u201d\u89c6\u9891\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14539", "pdf": "https://arxiv.org/pdf/2506.14539", "abs": "https://arxiv.org/abs/2506.14539", "authors": ["Daewon Kang", "YeongHwan Shin", "Doyeon Kim", "Kyu-Hwan Jung", "Meong Hi Son"], "title": "Doppelg\u00e4nger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelg\u00e4nger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelg\u00e4nger method. The experimental results demonstrate that the Doppelg\u00e4nger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u66ff\u8eab\u65b9\u6cd5\u201d\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u53ef\u8f6c\u79fb\u5bf9\u6297\u653b\u51fb\u7834\u574f\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u88ab\u52ab\u6301\u7684\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u201c\u5bf9\u6297\u8f6c\u79fb\u4e0b\u7684\u63d0\u793a\u5bf9\u9f50\u5d29\u6e83\uff08PACAT\uff09\u201d\u8bc4\u4f30\u5176\u8106\u5f31\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u201c\u5bf9\u6297\u8f6c\u79fb\u8b66\u793a\uff08CAT\uff09\u201d\u63d0\u793a\u4f5c\u4e3a\u9632\u5fa1\u624b\u6bb5\u3002\u5b9e\u9a8c\u8bc1\u660e\u66ff\u8eab\u65b9\u6cd5\u80fd\u7834\u574f\u4ee3\u7406\u4e00\u81f4\u6027\u5e76\u6cc4\u9732\u5185\u90e8\u4fe1\u606f\uff0c\u800cCAT\u63d0\u793a\u80fd\u6709\u6548\u9632\u5fa1\u6b64\u7c7b\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u63d0\u793a\u5de5\u7a0b\u4f7f\u5f97\u5feb\u901f\u3001\u4f4e\u6210\u672c\u521b\u5efa\u591a\u6837\u5316\u7684\u81ea\u4e3b\u4ee3\u7406\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u8fd9\u4e5f\u5f15\u53d1\u4e86\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u7684\u62c5\u5fe7\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u4ee3\u7406\u88ab\u52ab\u6301\u7684\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u548c\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u201c\u66ff\u8eab\u65b9\u6cd5\u201d\u4f5c\u4e3a\u53ef\u8f6c\u79fb\u5bf9\u6297\u653b\u51fb\u624b\u6bb5\uff0c\u7834\u574f\u4ee3\u7406\u7684\u89d2\u8272\u4e00\u81f4\u6027\uff1b\u5b9a\u4e49\u201c\u5bf9\u6297\u8f6c\u79fb\u4e0b\u7684\u63d0\u793a\u5bf9\u9f50\u5d29\u6e83\uff08PACAT\uff09\u201d\u8bc4\u4f30\u8106\u5f31\u6027\uff1b\u8bbe\u8ba1\u201c\u5bf9\u6297\u8f6c\u79fb\u8b66\u793a\uff08CAT\uff09\u201d\u63d0\u793a\u4f5c\u4e3a\u9632\u5fa1\u63aa\u65bd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u66ff\u8eab\u65b9\u6cd5\u80fd\u6210\u529f\u7834\u574f\u4ee3\u7406\u7684\u4e00\u81f4\u6027\u5e76\u66b4\u9732\u5176\u5185\u90e8\u4fe1\u606f\uff0c\u800cCAT\u63d0\u793a\u80fd\u6709\u6548\u9632\u5fa1\u6b64\u7c7b\u653b\u51fb\u3002", "conclusion": "\u66ff\u8eab\u65b9\u6cd5\u63ed\u793a\u4e86\u4ee3\u7406\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u800cCAT\u63d0\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u624b\u6bb5\uff0c\u4e3a\u63d0\u5347\u4ee3\u7406\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u66ff\u8eab\u65b9\u6cd5\uff1a\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u53ef\u8f6c\u79fb\u5bf9\u6297\u653b\u51fb\u7834\u574f\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u89d2\u8272\u4e00\u81f4\u6027", "abstract_zh": "\u81ea\u5927\u8bed\u8a00\u6a21\u578b\u95ee\u4e16\u4ee5\u6765\uff0c\u63d0\u793a\u5de5\u7a0b\u4f7f\u5f97\u5feb\u901f\u3001\u4f4e\u6210\u672c\u521b\u5efa\u591a\u6837\u5316\u7684\u81ea\u4e3b\u4ee3\u7406\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u5df2\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u4fbf\u5229\u6027\u5f15\u53d1\u4e86\u5173\u4e8e\u5e95\u5c42\u63d0\u793a\u7684\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u7684\u7d27\u8feb\u62c5\u5fe7\uff0c\u4ee5\u53ca\u9632\u6b62\u8fd9\u4e9b\u63d0\u793a\u88ab\u7528\u6237\u5c1d\u8bd5\u66b4\u9732\u7684\u8feb\u5207\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u201c\u66ff\u8eab\u65b9\u6cd5\u201d\u4ee5\u5c55\u793a\u4ee3\u7406\u88ab\u52ab\u6301\u7684\u98ce\u9669\uff0c\u4ece\u800c\u66b4\u9732\u7cfb\u7edf\u6307\u4ee4\u548c\u5185\u90e8\u4fe1\u606f\u3002\u63a5\u7740\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u201c\u5bf9\u6297\u8f6c\u79fb\u4e0b\u7684\u63d0\u793a\u5bf9\u9f50\u5d29\u6e83\uff08PACAT\uff09\u201d\u7ea7\u522b\u4ee5\u8bc4\u4f30\u5bf9\u6b64\u7c7b\u5bf9\u6297\u8f6c\u79fb\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5bf9\u6297\u8f6c\u79fb\u8b66\u793a\uff08CAT\uff09\u201d\u63d0\u793a\u4ee5\u5e94\u5bf9\u66ff\u8eab\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u66ff\u8eab\u65b9\u6cd5\u80fd\u591f\u7834\u574f\u4ee3\u7406\u7684\u4e00\u81f4\u6027\u5e76\u66b4\u9732\u5176\u5185\u90e8\u4fe1\u606f\uff0c\u800cCAT\u63d0\u793a\u5219\u80fd\u6709\u6548\u9632\u5fa1\u6b64\u7c7b\u5bf9\u6297\u653b\u51fb\u3002"}}
{"id": "2506.14493", "pdf": "https://arxiv.org/pdf/2506.14493", "abs": "https://arxiv.org/abs/2506.14493", "authors": ["Jiyuan Fu", "Kaixun Jiang", "Lingyi Hong", "Jinglun Li", "Haijing Guo", "Dingkang Yang", "Zhaoyu Chen", "Wenqiang Zhang"], "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLingoLoop\u653b\u51fb\uff0c\u901a\u8fc7\u8bed\u8a00\u4e0a\u4e0b\u6587\u548c\u72b6\u6001\u9677\u9631\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9677\u5165\u65e0\u9650\u5faa\u73af\uff0c\u5bfc\u81f4\u8d44\u6e90\u8017\u5c3d\u3002\u653b\u51fb\u5229\u7528\u8bcd\u6027\u6807\u7b7e\u548c\u751f\u6210\u8def\u5f84\u4fee\u526a\u673a\u5236\uff0c\u663e\u8457\u589e\u52a0\u751f\u6210\u4ee4\u724c\u548c\u80fd\u8017\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u8bf1\u5bfc\u5176\u751f\u6210\u8fc7\u591a\u8f93\u51fa\u6765\u8017\u5c3d\u8d44\u6e90\u3002\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bcd\u6027\u6807\u7b7e\u548c\u53e5\u5b50\u7ed3\u6784\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u653b\u51fb\u6548\u679c\u3002", "method": "1. \u63d0\u51fa\u8bcd\u6027\u611f\u77e5\u5ef6\u8fdf\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u63a8\u8fdfEOS\u4ee4\u724c\u751f\u6210\uff1b2. \u5f15\u5165\u751f\u6210\u8def\u5f84\u4fee\u526a\u673a\u5236\uff0c\u9650\u5236\u9690\u85cf\u72b6\u6001\u5e45\u5ea6\u4ee5\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u91cd\u590d\u5faa\u73af\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLingoLoop\u653b\u51fb\u53ef\u5c06\u751f\u6210\u4ee4\u724c\u589e\u52a0\u81f330\u500d\uff0c\u80fd\u8017\u663e\u8457\u63d0\u5347\uff0c\u5982Qwen2.5-VL-3B\u6a21\u578b\u3002", "conclusion": "LingoLoop\u653b\u51fb\u63ed\u793a\u4e86MLLMs\u7684\u91cd\u5927\u6f0f\u6d1e\uff0c\u5bf9\u5176\u53ef\u9760\u90e8\u7f72\u63d0\u51fa\u6311\u6218\u3002\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002", "paper_title_zh": "LingoLoop\u653b\u51fb\uff1a\u901a\u8fc7\u8bed\u8a00\u4e0a\u4e0b\u6587\u548c\u72b6\u6001\u9677\u9631\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9677\u5165\u65e0\u9650\u5faa\u73af", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u8bf1\u5bfc\u5176\u751f\u6210\u8fc7\u591a\u8f93\u51fa\uff0c\u5bfc\u81f4\u8d44\u6e90\u8017\u5c3d\u548c\u670d\u52a1\u964d\u7ea7\u3002\u73b0\u6709\u7684\u80fd\u91cf-\u5ef6\u8fdf\u653b\u51fb\u901a\u8fc7\u5e7f\u6cdb\u504f\u79bbEOS\u4ee4\u724c\u7684\u8f93\u51fa\u4ee4\u724c\u5206\u5e03\u6765\u589e\u52a0\u751f\u6210\u65f6\u95f4\uff0c\u4f46\u5ffd\u7565\u4e86\u8bcd\u6027\uff08POS\uff09\u7279\u5f81\u5bf9EOS\u4ee4\u724c\u751f\u6210\u7684\u5f71\u54cd\u4ee5\u53ca\u53e5\u5b50\u7ed3\u6784\u5bf9\u8f93\u51fa\u6570\u91cf\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u653b\u51fb\u6548\u679c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faLingoLoop\u653b\u51fb\uff0c\u65e8\u5728\u8bf1\u5bfcMLLMs\u751f\u6210\u5197\u957f\u4e14\u91cd\u590d\u7684\u5e8f\u5217\u3002\u9996\u5148\uff0c\u6211\u4eec\u53d1\u73b0\u4ee4\u724c\u7684\u8bcd\u6027\u6807\u7b7e\u663e\u8457\u5f71\u54cdEOS\u4ee4\u724c\u7684\u751f\u6210\u6982\u7387\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u8bcd\u6027\u611f\u77e5\u5ef6\u8fdf\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u63a8\u8fdfEOS\u4ee4\u724c\u751f\u6210\u3002\u5176\u6b21\uff0c\u6211\u4eec\u9650\u5236\u8f93\u51fa\u591a\u6837\u6027\u4ee5\u8bf1\u5bfc\u91cd\u590d\u5faa\u73af\uff0c\u5e76\u63d0\u51fa\u751f\u6210\u8def\u5f84\u4fee\u526a\u673a\u5236\uff0c\u901a\u8fc7\u9650\u5236\u9690\u85cf\u72b6\u6001\u5e45\u5ea6\u4fc3\u4f7f\u6a21\u578b\u751f\u6210\u6301\u7eed\u5faa\u73af\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLingoLoop\u53ef\u5c06\u751f\u6210\u4ee4\u724c\u589e\u52a0\u81f330\u500d\uff0c\u80fd\u8017\u663e\u8457\u63d0\u5347\uff08\u5982Qwen2.5-VL-3B\u6a21\u578b\uff09\uff0c\u6301\u7eed\u5c06MLLMs\u63a8\u5411\u751f\u6210\u6781\u9650\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86MLLMs\u7684\u91cd\u5927\u6f0f\u6d1e\uff0c\u5bf9\u5176\u53ef\u9760\u90e8\u7f72\u63d0\u51fa\u6311\u6218\u3002\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2506.14418", "pdf": "https://arxiv.org/pdf/2506.14418", "abs": "https://arxiv.org/abs/2506.14418", "authors": ["Jiayi Chen", "Yanbiao Ma", "Andi Zhang", "Weidong Tang", "Wei Dai", "Bowei Liu"], "title": "Compositional Attribute Imbalance in Vision Datasets", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u89c6\u89c9\u5c5e\u6027\u5b57\u5178\u6784\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u56fe\u50cf\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u6837\u672c\u91c7\u6837\u6982\u7387\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u89c6\u89c9\u5c5e\u6027\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u89c6\u89c9\u5c5e\u6027\u4e0d\u5e73\u8861\u662f\u56fe\u50cf\u5206\u7c7b\u4e2d\u5e38\u89c1\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u5c5e\u6027\u7a00\u6709\u6027\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u7a00\u6709\u5c5e\u6027\u7684\u8868\u5f81\u80fd\u529b\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u56fe\u50cf\u7684\u4e00\u7ea7\u548c\u4e8c\u7ea7\u5c5e\u6027\uff0c\u5229\u7528CLIP\u6846\u67b6\u6784\u5efa\u89c6\u89c9\u5c5e\u6027\u5b57\u5178\uff0c\u81ea\u52a8\u8bc4\u4f30\u5c5e\u6027\u3002\u901a\u8fc7\u5206\u6790\u5355\u5c5e\u6027\u548c\u7ec4\u5408\u5c5e\u6027\u4e0d\u5e73\u8861\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c5e\u6027\u7a00\u6709\u6027\u8c03\u6574\u6837\u672c\u91c7\u6837\u6982\u7387\uff0c\u5e76\u7ed3\u5408CutMix\u3001Fmix\u7b49\u6570\u636e\u589e\u5f3a\u6280\u672f\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5c5e\u6027\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5efa\u6a21\u89c6\u89c9\u5c5e\u6027\u5206\u5e03\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u957f\u5c3e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u7684\u7ec4\u5408\u5c5e\u6027\u4e0d\u5e73\u8861", "abstract_zh": "\u89c6\u89c9\u5c5e\u6027\u4e0d\u5e73\u8861\u662f\u56fe\u50cf\u5206\u7c7b\u4e2d\u5e38\u89c1\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\uff0c\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u9996\u5148\u5b9a\u4e49\u56fe\u50cf\u7684\u4e00\u7ea7\u548c\u4e8c\u7ea7\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eCLIP\u7684\u6846\u67b6\u6784\u5efa\u89c6\u89c9\u5c5e\u6027\u5b57\u5178\uff0c\u5b9e\u73b0\u5c5e\u6027\u7684\u81ea\u52a8\u8bc4\u4f30\u3002\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5355\u5c5e\u6027\u548c\u7ec4\u5408\u5c5e\u6027\u4e0d\u5e73\u8861\uff0c\u63ed\u793a\u4e86\u5c5e\u6027\u7a00\u6709\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u7ec4\u5408\u5c5e\u6027\u7a00\u6709\u6027\u8c03\u6574\u6837\u672c\u91c7\u6837\u6982\u7387\uff0c\u5e76\u5c06\u8be5\u7b56\u7565\u4e0e\u591a\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982CutMix\u3001Fmix\u548cSaliencyMix\uff09\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u7a00\u6709\u5c5e\u6027\u7684\u8868\u5f81\u80fd\u529b\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5c5e\u6027\u4e0d\u5e73\u8861\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5efa\u6a21\u89c6\u89c9\u5c5e\u6027\u5206\u5e03\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u957f\u5c3e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14568", "pdf": "https://arxiv.org/pdf/2506.14568", "abs": "https://arxiv.org/abs/2506.14568", "authors": ["Eliott Thomas", "Mickael Coustaty", "Aurelie Joseph", "Gaspar Deloin", "Elodie Carel", "Vincent Poulain D'Andecy", "Jean-Marc Ogier"], "title": "QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents", "categories": ["cs.AI"], "comment": "Accepted at ICDAR 2025", "summary": "Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.", "AI": {"tldr": "QUEST\u63d0\u51fa\u4e86\u4e00\u79cd\u8d28\u91cf\u611f\u77e5\u7684\u534a\u76d1\u7763\u8868\u683c\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u8868\u683c\u7684\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u6765\u6539\u8fdb\u4f2a\u6807\u7b7e\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5546\u4e1a\u6587\u6863\u4e2d\u8868\u683c\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u5546\u4e1a\u6587\u6863\u4e2d\u7684\u8868\u683c\u63d0\u53d6\u81ea\u52a8\u5316\u5bf9\u5de5\u4e1a\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u63d0\u53d6\u8d28\u91cf\uff0c\u4e14\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002QUEST\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u8868\u683c\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "QUEST\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4bF1\u5206\u6570\u800c\u975e\u7f6e\u4fe1\u5ea6\u6765\u8bc4\u4f30\u63d0\u53d6\u8868\u683c\u7684\u8d28\u91cf\u3002\u7ed3\u5408\u591a\u6837\u6027\u5ea6\u91cf\uff08\u5982DPP\u3001Vendi\u5206\u6570\u548cIntDiv\uff09\u6765\u7f13\u89e3\u786e\u8ba4\u504f\u5dee\uff0c\u5e76\u5728\u8fed\u4ee3\u7684\u534a\u76d1\u7763\u8bad\u7ec3\u4e2d\u4f18\u5316\u4f2a\u6807\u7b7e\u9009\u62e9\u3002", "result": "\u5728\u4e13\u6709\u5546\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cQUEST\u5c06F1\u5206\u6570\u4ece64%\u63d0\u5347\u81f374%\uff0c\u7a7a\u9884\u6d4b\u7387\u964d\u4f4e\u4e8645%\uff08\u4ece12%\u964d\u81f36.5%\uff09\u3002\u5728DocILE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cF1\u5206\u6570\u4ece42%\u63d0\u5347\u81f350%\uff0c\u7a7a\u9884\u6d4b\u7387\u964d\u4f4e\u4e8619%\uff08\u4ece27%\u964d\u81f322%\uff09\u3002", "conclusion": "QUEST\u901a\u8fc7\u8d28\u91cf\u611f\u77e5\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5546\u4e1a\u6587\u6863\u4e2d\u8868\u683c\u63d0\u53d6\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u6570\u636e\u5b8c\u6574\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6807\u6ce8\u7a00\u7f3a\u7684\u573a\u666f\u3002", "paper_title_zh": "QUEST\uff1a\u9762\u5411\u5546\u4e1a\u6587\u6863\u7684\u8d28\u91cf\u611f\u77e5\u534a\u76d1\u7763\u8868\u683c\u63d0\u53d6", "abstract_zh": "\u81ea\u52a8\u5316\u5546\u4e1a\u6587\u6863\u4e2d\u7684\u8868\u683c\u63d0\u53d6\uff08TE\uff09\u5bf9\u5de5\u4e1a\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6807\u6ce8\u7a00\u758f\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\u6613\u9519\uff0c\u4ecd\u5177\u6311\u6218\u6027\u3002\u5c3d\u7ba1\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u53ef\u4ee5\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u63d0\u53d6\u8d28\u91cf\u3002\u6211\u4eec\u63d0\u51fa\u4e86QUEST\uff0c\u4e00\u79cd\u9762\u5411\u5546\u4e1a\u6587\u6863\u7684\u8d28\u91cf\u611f\u77e5\u534a\u76d1\u7763\u8868\u683c\u63d0\u53d6\u6846\u67b6\u3002QUEST\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u8bc4\u4f30\u63d0\u53d6\u8868\u683c\u7684\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u6765\u9884\u6d4bF1\u5206\u6570\uff0c\u800c\u975e\u4f9d\u8d56\u7f6e\u4fe1\u5ea6\u6307\u6807\u3002\u8fd9\u79cd\u8d28\u91cf\u611f\u77e5\u65b9\u6cd5\u5728\u8fed\u4ee3\u7684SSL\u8bad\u7ec3\u4e2d\u6307\u5bfc\u4f2a\u6807\u7b7e\u9009\u62e9\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u6837\u6027\u5ea6\u91cf\uff08DPP\u3001Vendi\u5206\u6570\u3001IntDiv\uff09\u7f13\u89e3\u786e\u8ba4\u504f\u5dee\u3002\u5728\u4e13\u6709\u5546\u4e1a\u6570\u636e\u96c6\uff081000\u6807\u6ce8+10000\u672a\u6807\u6ce8\u6587\u6863\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQUEST\u5c06F1\u5206\u6570\u4ece64%\u63d0\u5347\u81f374%\uff0c\u7a7a\u9884\u6d4b\u7387\u964d\u4f4e\u4e8645%\uff08\u4ece12%\u964d\u81f36.5%\uff09\u3002\u5728DocILE\u57fa\u51c6\u6d4b\u8bd5\uff08600\u6807\u6ce8+20000\u672a\u6807\u6ce8\u6587\u6863\uff09\u4e2d\uff0cQUEST\u5b9e\u73b0\u4e8650%\u7684F1\u5206\u6570\uff08\u4ece42%\u63d0\u5347\uff09\uff0c\u7a7a\u9884\u6d4b\u7387\u964d\u4f4e\u4e8619%\uff08\u4ece27%\u964d\u81f322%\uff09\u3002\u8be5\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u8d28\u91cf\u8bc4\u4f30\u548c\u5bf9\u6807\u6ce8\u7a00\u7f3a\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u7279\u522b\u9002\u7528\u4e8e\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u6570\u636e\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u7684\u5546\u4e1a\u6587\u6863\u3002"}}
{"id": "2506.14532", "pdf": "https://arxiv.org/pdf/2506.14532", "abs": "https://arxiv.org/abs/2506.14532", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Zitong Yu", "Merouane Debbah"], "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models", "categories": ["cs.CL"], "comment": "13 pages, 20 figures", "summary": "This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM2BeamLLM\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6ce2\u675f\u9884\u6d4b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u96f7\u8fbe\u3001LiDAR\u548cGPS\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-2\uff09\u7684\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6ce2\u675f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6ce2\u675f\u9884\u6d4b\u5bf9\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\uff08V2I\uff09\u901a\u4fe1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u667a\u80fd\u7684\u6ce2\u675f\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "M2BeamLLM\u6846\u67b6\u6574\u5408\u4e86\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\u3001\u96f7\u8fbe\u3001LiDAR\u548cGPS\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-2\uff09\u8fdb\u884c\u6ce2\u675f\u9884\u6d4b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u4f20\u611f\u5668\u6570\u636e\u7f16\u7801\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u4e0e\u878d\u5408\uff0c\u4ee5\u53ca\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cM2BeamLLM\u5728\u6ce2\u675f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u5728\u6807\u51c6\u573a\u666f\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002\u6b64\u5916\uff0c\u968f\u7740\u4f20\u611f\u5668\u6a21\u6001\u591a\u6837\u6027\u7684\u589e\u52a0\uff0c\u5176\u9884\u6d4b\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "M2BeamLLM\u4e3a\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u667a\u80fd\u7684\u6ce2\u675f\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "paper_title_zh": "M2BeamLLM\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u611f\u77e5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6beb\u7c73\u6ce2\u6ce2\u675f\u9884\u6d4b", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM2BeamLLM\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08mMIMO\uff09\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6ce2\u675f\u9884\u6d4b\u3002M2BeamLLM\u6574\u5408\u4e86\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff08\u5305\u62ec\u56fe\u50cf\u3001\u96f7\u8fbe\u3001LiDAR\u548cGPS\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-2\uff09\u7684\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u6ce2\u675f\u9884\u6d4b\u3002\u901a\u8fc7\u7ed3\u5408\u4f20\u611f\u5668\u6570\u636e\u7f16\u7801\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u4e0e\u878d\u5408\u4ee5\u53ca\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0cM2BeamLLM\u663e\u8457\u63d0\u9ad8\u4e86\u6ce2\u675f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u6807\u51c6\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5176\u9884\u6d4b\u6027\u80fd\u968f\u7740\u4f20\u611f\u5668\u6a21\u6001\u591a\u6837\u6027\u7684\u589e\u52a0\u800c\u6301\u7eed\u63d0\u5347\u3002\u672c\u7814\u7a76\u4e3a\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\uff08V2I\uff09\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u667a\u80fd\u7684\u6ce2\u675f\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14428", "pdf": "https://arxiv.org/pdf/2506.14428", "abs": "https://arxiv.org/abs/2506.14428", "authors": ["Ruihao Xi", "Xuekuan Wang", "Yongcheng Li", "Shuhua Li", "Zichen Wang", "Yiwei Wang", "Feng Wei", "Cairong Zhao"], "title": "Toward Rich Video Human-Motion2D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e30\u5bcc\u89c6\u9891\u4eba\u4f53\u8fd0\u52a82D\u751f\u6210\u65b9\u6cd5\uff08RVHM2D\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b15\u4e07\u89c6\u9891\u5e8f\u5217\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08Motion2D-Video-150K\uff09\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u4e14\u53ef\u63a7\u7684\u5355\u4eba\u548c\u53cc\u4eba\u4ea4\u4e92\u52a8\u4f5c\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u9645\u52a8\u6001\u5efa\u6a21\u7684\u590d\u6742\u6027\uff0c\u751f\u6210\u903c\u771f\u4e14\u53ef\u63a7\u7684\u591a\u4eba\u4ea4\u4e92\u52a8\u4f5c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u65b0\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684\u751f\u6210\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86\u5305\u542b15\u4e07\u89c6\u9891\u5e8f\u5217\u7684Motion2D-Video-150K\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faRVHM2D\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u53cc\u6587\u672c\u7f16\u7801\u5668\uff08CLIP-L/B\u6216T5-XXL\uff09\u589e\u5f3a\u6587\u672c\u6761\u4ef6\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u6269\u6563\u76ee\u6807\u8bad\u7ec3\u548c\u57fa\u4e8eFID\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRVHM2D\u5728Motion2D-Video-150K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u4eba\u548c\u53cc\u4eba\u4ea4\u4e92\u52a8\u4f5c\u3002", "conclusion": "RVHM2D\u901a\u8fc7\u7ed3\u5408\u65b0\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e30\u5bcc\u89c6\u9891\u4eba\u4f53\u8fd0\u52a82D\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "paper_title_zh": "\u9762\u5411\u4e30\u5bcc\u89c6\u9891\u4eba\u4f53\u8fd0\u52a82D\u751f\u6210\u7684\u7814\u7a76", "abstract_zh": "\u751f\u6210\u903c\u771f\u4e14\u53ef\u63a7\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u591a\u4eba\u4ea4\u4e92\u7684\u590d\u6742\u52a8\u4f5c\uff0c\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u9645\u52a8\u6001\u5efa\u6a21\u7684\u590d\u6742\u6027\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u4e30\u5bcc\u89c6\u9891\u4eba\u4f53\u8fd0\u52a82D\u6570\u636e\u96c6\uff08Motion2D-Video-150K\uff09\uff0c\u5305\u542b15\u4e07\u89c6\u9891\u5e8f\u5217\u3002Motion2D-Video-150K\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u5355\u4eba\u52a8\u4f5c\u548c\u5173\u952e\u7684\u53cc\u4eba\u4ea4\u4e92\u52a8\u4f5c\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u5747\u914d\u6709\u8be6\u7ec6\u7684\u6587\u672c\u63cf\u8ff0\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u7684\u4e30\u5bcc\u89c6\u9891\u4eba\u4f53\u8fd0\u52a82D\u751f\u6210\u6a21\u578b\uff08RVHM2D\uff09\u3002RVHM2D\u91c7\u7528\u53cc\u6587\u672c\u7f16\u7801\u5668\uff08CLIP-L/B\u6216T5-XXL\uff09\u589e\u5f3a\u6587\u672c\u6761\u4ef6\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528\u6807\u51c6\u6269\u6563\u76ee\u6807\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8eFID\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u52a8\u4f5c\u903c\u771f\u5ea6\u548c\u6587\u672c\u5bf9\u9f50\u6548\u679c\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRVHM2D\u5728Motion2D-Video-150K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u4eba\u548c\u53cc\u4eba\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2506.14569", "pdf": "https://arxiv.org/pdf/2506.14569", "abs": "https://arxiv.org/abs/2506.14569", "authors": ["Stephen Roth", "Lennart Baur", "Derian Boer", "Stefan Kramer"], "title": "Enhancing Symbolic Machine Learning by Subsymbolic Representations", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u795e\u7ecf\u5d4c\u5165\u589e\u5f3a\u7b26\u53f7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u7b80\u5355\u573a\u666f\u4e2d\u63d0\u5347\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7AI\u65e8\u5728\u7ed3\u5408\u7b26\u53f7\u4e0e\u4e9a\u7b26\u53f7\u65b9\u6cd5\u4ee5\u514b\u670d\u5404\u81ea\u5c40\u9650\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5982LTN\u548cDeepProbLog\u5728\u7b80\u5355\u573a\u666f\uff08\u5982\u5224\u522b\u5f0f\u673a\u5668\u5b66\u4e60\uff09\u4e2d\u6548\u7387\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u901a\u8fc7\u795e\u7ecf\u5d4c\u5165\u589e\u5f3a\u7b26\u53f7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u4e3a\u7b26\u53f7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5TILDE\u63d0\u4f9b\u795e\u7ecf\u5d4c\u5165\uff0c\u7279\u522b\u662f\u7528\u4e8e\u76f8\u4f3c\u6027\u8c13\u8bcd\u7684\u5e38\u91cf\u5d4c\u5165\uff0c\u5e76\u53ef\u6839\u636e\u7b26\u53f7\u7406\u8bba\u8fdb\u4e00\u6b65\u5fae\u8c03\u5d4c\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u5f53\u524d\u573a\u666f\uff0c\u8fd8\u53ef\u6269\u5c55\u81f3\u5b9e\u4f8b\u95f4\u76f8\u4f3c\u6027\uff08\u7c7b\u4f3c\u903b\u8f91\u8bed\u8a00\u4e2d\u7684\u6838\u51fd\u6570\uff09\u3001\u7c7b\u6bd4\u63a8\u7406\u6216\u547d\u9898\u5316\u3002", "paper_title_zh": "\u901a\u8fc7\u4e9a\u7b26\u53f7\u8868\u793a\u589e\u5f3a\u7b26\u53f7\u673a\u5668\u5b66\u4e60", "abstract_zh": "\u795e\u7ecf\u7b26\u53f7AI\u7684\u76ee\u6807\u662f\u6574\u5408\u7b26\u53f7\u4e0e\u4e9a\u7b26\u53f7AI\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u5404\u81ea\u7684\u5c40\u9650\u6027\u3002\u73b0\u6709\u7cfb\u7edf\u5982\u903b\u8f91\u5f20\u91cf\u7f51\u7edc\uff08LTN\uff09\u6216DeepProbLog\u63d0\u4f9b\u4e86\u795e\u7ecf\u8c13\u8bcd\u548c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u4f46\u5176\u591a\u529f\u80fd\u6027\u5728\u7b80\u5355\u573a\u666f\uff08\u5982\u5224\u522b\u5f0f\u673a\u5668\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u5e38\u91cf\u8f83\u591a\u7684\u9886\u57df\uff09\u4e2d\u6548\u7387\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u4e0d\u540c\u65b9\u6cd5\uff1a\u901a\u8fc7\u4e3a\u7b26\u53f7\u673a\u5668\u5b66\u4e60\u65b9\u6848\u63d0\u4f9b\u795e\u7ecf\u5d4c\u5165\u6765\u589e\u5f3a\u5176\u6027\u80fd\u3002\u672c\u6587\u4ee5TILDE\u53ca\u5176\u5728\u76f8\u4f3c\u6027\u8c13\u8bcd\u4e2d\u4f7f\u7528\u7684\u5e38\u91cf\u5d4c\u5165\u4e3a\u4f8b\u5c55\u793a\u4e86\u8fd9\u4e00\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u53ef\u901a\u8fc7\u6839\u636e\u7b26\u53f7\u7406\u8bba\u8fdb\u4e00\u6b65\u7ec6\u5316\u5d4c\u5165\u8fdb\u884c\u5fae\u8c03\u3002\u5728\u4e09\u4e2a\u771f\u5b9e\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8fd9\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7684\u5e94\u7528\u4e0d\u4ec5\u9650\u4e8e\u6b64\uff1a\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\u589e\u5f3a\u7b26\u53f7\u5b66\u4e60\u5668\u53ef\u6269\u5c55\u81f3\u5b9e\u4f8b\u95f4\u76f8\u4f3c\u6027\uff08\u7c7b\u4f3c\u903b\u8f91\u8bed\u8a00\u4e2d\u7684\u6838\u51fd\u6570\uff09\u3001\u7c7b\u6bd4\u63a8\u7406\u6216\u547d\u9898\u5316\u3002"}}
{"id": "2506.14562", "pdf": "https://arxiv.org/pdf/2506.14562", "abs": "https://arxiv.org/abs/2506.14562", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify \"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.", "AI": {"tldr": "AlphaDecay\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6a21\u5757\u5316\u6743\u91cd\u8870\u51cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u6a21\u5757\u7684\u9891\u8c31\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u8870\u51cf\u5f3a\u5ea6\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u6743\u91cd\u8870\u51cf\u65b9\u6cd5\u5bf9\u6240\u6709\u6a21\u5757\u4f7f\u7528\u7edf\u4e00\u7684\u8870\u51cf\u7387\uff0c\u5ffd\u7565\u4e86LLM\u4e2d\u4e0d\u540c\u6a21\u5757\u7684\u7ed3\u6784\u591a\u6837\u6027\u548c\u9891\u8c31\u7279\u6027\u5dee\u5f02\u3002AlphaDecay\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u8870\u51cf\u5f3a\u5ea6\uff0c\u5e73\u8861\u6a21\u5757\u95f4\u7684\u9891\u8c31\u7279\u6027\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "AlphaDecay\u57fa\u4e8e\u91cd\u5c3e\u81ea\u6b63\u5219\u5316\uff08HT-SR\uff09\u7406\u8bba\uff0c\u901a\u8fc7\u5206\u6790\u6743\u91cd\u76f8\u5173\u77e9\u9635\u7684\u7ecf\u9a8c\u9891\u8c31\u5bc6\u5ea6\uff08ESD\uff09\u91cf\u5316\u6a21\u5757\u7684\u201c\u91cd\u5c3e\u6027\u201d\u3002\u91cd\u5c3e\u6027\u66f4\u5f3a\u7684\u6a21\u5757\uff08\u53cd\u6620\u66f4\u5f3a\u7684\u7279\u5f81\u5b66\u4e60\u80fd\u529b\uff09\u5206\u914d\u8f83\u5f31\u7684\u8870\u51cf\uff0c\u800c\u9891\u8c31\u8f83\u8f7b\u7684\u6a21\u5757\u5206\u914d\u8f83\u5f3a\u7684\u8870\u51cf\u3002", "result": "\u572860M\u52301B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u5b9e\u9a8c\uff0cAlphaDecay\u5728\u56f0\u60d1\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u7edf\u4e00\u8870\u51cf\u548c\u5176\u4ed6\u81ea\u9002\u5e94\u8870\u51cf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AlphaDecay\u901a\u8fc7\u6a21\u5757\u5316\u7684\u6743\u91cd\u8870\u51cf\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86LLM\u4e2d\u4e0d\u540c\u6a21\u5757\u7684\u9891\u8c31\u7279\u6027\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "paper_title_zh": "AlphaDecay\uff1a\u9762\u5411LLM\u4e2d\u91cd\u5c3e\u5e73\u8861\u7684\u6a21\u5757\u5316\u6743\u91cd\u8870\u51cf", "abstract_zh": "\u6743\u91cd\u8870\u51cf\u662f\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6807\u51c6\u6b63\u5219\u5316\u6280\u672f\u3002\u5c3d\u7ba1\u901a\u5e38\u4e3a\u6bcf\u4e00\u5c42\u5206\u914d\u7edf\u4e00\u7684\u8870\u51cf\u7387\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86LLM\u7684\u7ed3\u6784\u591a\u6837\u6027\u4ee5\u53ca\u6a21\u5757\u95f4\u9891\u8c31\u7279\u6027\u7684\u5dee\u5f02\u3002\u672c\u6587\u63d0\u51faAlphaDecay\uff0c\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u4e3aLLM\u7684\u6bcf\u4e2a\u6a21\u5757\u5206\u914d\u4e0d\u540c\u7684\u6743\u91cd\u8870\u51cf\u5f3a\u5ea6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u91cd\u5c3e\u81ea\u6b63\u5219\u5316\uff08HT-SR\uff09\u7406\u8bba\uff0c\u901a\u8fc7\u5206\u6790\u6743\u91cd\u76f8\u5173\u77e9\u9635\u7684\u7ecf\u9a8c\u9891\u8c31\u5bc6\u5ea6\uff08ESD\uff09\u6765\u91cf\u5316\u201c\u91cd\u5c3e\u6027\u201d\u3002\u8868\u73b0\u51fa\u66f4\u663e\u8457\u91cd\u5c3eESD\u7684\u6a21\u5757\uff08\u53cd\u6620\u66f4\u5f3a\u7684\u7279\u5f81\u5b66\u4e60\u80fd\u529b\uff09\u88ab\u5206\u914d\u8f83\u5f31\u7684\u8870\u51cf\uff0c\u800c\u9891\u8c31\u8f83\u8f7b\u7684\u6a21\u5757\u5219\u5206\u914d\u8f83\u5f3a\u7684\u8870\u51cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u6743\u91cd\u8870\u51cf\u5206\u914d\uff0c\u5e73\u8861\u4e86\u6a21\u5757\u95f4\u9891\u8c31\u7279\u6027\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6027\u80fd\u3002\u572860M\u52301B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u7ed3\u679c\u8868\u660eAlphaDecay\u5728\u56f0\u60d1\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u7edf\u4e00\u8870\u51cf\u548c\u5176\u4ed6\u81ea\u9002\u5e94\u8870\u51cf\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2506.14435", "pdf": "https://arxiv.org/pdf/2506.14435", "abs": "https://arxiv.org/abs/2506.14435", "authors": ["Hongyu Wang", "Jiayu Xu", "Ruiping Wang", "Yan Feng", "Yitao Zhai", "Peng Pei", "Xunliang Cai", "Xilin Chen"], "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoTE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4e09\u5143\u4e13\u5bb6\uff08\u53c2\u6570\u4e3a{-1, 0, 1}\uff09\u66ff\u4ee3\u5168\u7cbe\u5ea6\u4e13\u5bb6\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u867d\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5168\u7cbe\u5ea6\u4e13\u5bb6\u5bfc\u81f4\u9ad8\u5185\u5b58\u5360\u7528\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MoTE\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684FFN\u4f5c\u4e3a\u5171\u4eab\u4e13\u5bb6\uff0c\u8bad\u7ec3\u4e09\u5143\u8def\u7531\u4e13\u5bb6\uff08\u53c2\u6570\u4e3a{-1, 0, 1}\uff09\uff0c\u66ff\u4ee3\u5168\u7cbe\u5ea6\u4e13\u5bb6\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u8bad\u7ec3\u540e\u91cf\u5316\u6280\u672f\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoTE\u5728\u76f8\u540c\u5185\u5b58\u5360\u7528\uff083.4GB\uff09\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5168\u7cbe\u5ea6\u57fa\u7ebfMoE-LLaVA\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53474.3%\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6a21\u578b\u6269\u5c55\u6027\u3002", "conclusion": "MoTE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5185\u5b58\u53cb\u597d\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u8bbe\u5907\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "paper_title_zh": "MoTE\uff1a\u9762\u5411\u5185\u5b58\u9ad8\u6548\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u4e09\u5143\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5", "abstract_zh": "\u5927\u578b\u591a\u6a21\u6001\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u56fa\u5b9a\u7684\u6d3b\u8dc3\u53c2\u6570\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u91c7\u7528\u5168\u7cbe\u5ea6\u4e13\u5bb6\u8fdb\u884c\u7a00\u758f\u5347\u7ea7\uff0c\u5c3d\u7ba1\u5176\u5728\u7ec8\u7aef\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5927\u91cf\u4e13\u5bb6\u5bfc\u81f4\u9ad8\u5185\u5b58\u5360\u7528\uff0c\u5bf9\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u6784\u6210\u6311\u6218\u3002\u672c\u6587\u63d0\u51faMoTE\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4ece\u5bc6\u96c6\u68c0\u67e5\u70b9\u8bad\u7ec3\u4e09\u5143\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u3002\u6211\u4eec\u5efa\u8bae\u5728\u5347\u7ea7\u65f6\u8bad\u7ec3\u66f4\u591a\u4f4e\u7cbe\u5ea6\u4e13\u5bb6\u800c\u975e\u5c11\u91cf\u9ad8\u7cbe\u5ea6\u4e13\u5bb6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684FFN\u4f5c\u4e3a\u5171\u4eab\u4e13\u5bb6\uff0c\u5e76\u8bad\u7ec3\u53c2\u6570\u4e3a{-1, 0, 1}\u7684\u4e09\u5143\u8def\u7531\u4e13\u5bb6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u8868\u73b0\u51fa\u826f\u597d\u8d8b\u52bf\u3002MoTE\u5728\u6027\u80fd\u4e0a\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebfMoE-LLaVA\u76f8\u5f53\uff0c\u540c\u65f6\u5185\u5b58\u5360\u7528\u66f4\u4f4e\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u517c\u5bb9\u8bad\u7ec3\u540e\u91cf\u5316\u6280\u672f\uff0c\u4e14\u5728\u5185\u5b58\u7ea6\u675f\u66f4\u4f4e\u65f6\u4f18\u52bf\u66f4\u663e\u8457\u3002\u5728\u76f8\u540c\u4e13\u5bb6\u5185\u5b58\u5360\u7528\uff083.4GB\uff09\u4e0b\uff0c\u7ed3\u5408\u8bad\u7ec3\u540e\u91cf\u5316\uff0cMoTE\u5728\u7ec8\u7aef\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4MoE-LLaVA\u9ad8\u51fa4.3%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2506.14570", "pdf": "https://arxiv.org/pdf/2506.14570", "abs": "https://arxiv.org/abs/2506.14570", "authors": ["Mohammad Hashemi", "Andreas Zufle"], "title": "From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places", "categories": ["cs.AI"], "comment": null, "summary": "Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u9a71\u52a8\u7684\u52a8\u6001\u533a\u57df\uff08\u5373\u201c\u573a\u6240\u201d\uff09\u6765\u6355\u6349\u4eba\u7c7b\u79fb\u52a8\u6027\uff0c\u4ee5\u652f\u6301\u8de8\u5730\u7406\u548c\u573a\u666f\u7684\u53ef\u6269\u5c55\u5206\u6790\u3002", "motivation": "\u4eba\u7c7b\u79fb\u52a8\u6027\u6570\u636e\u5177\u6709\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u8bed\u4e49\u590d\u6742\u6027\uff0c\u73b0\u6709\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u5904\u7406\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u591a\u7c92\u5ea6\u63a8\u7406\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4ece\u79bb\u6563\u7684\u5174\u8da3\u70b9\u8f6c\u5411\u7406\u89e3\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u201c\u573a\u6240\u201d\uff0c\u5e76\u6574\u5408\u5730\u7406\u4f4d\u7f6e\u8bed\u4e49\u4e0e\u4eba\u7c7b\u79fb\u52a8\u6027\u3002\u7814\u7a76\u91cd\u70b9\u5305\u62ec\u5efa\u6a21\u573a\u6240\u548c\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u652f\u6301\u8de8\u573a\u666f\u7684\u53ef\u6269\u5c55\u5206\u6790\uff0c\u4e3a\u4e2a\u6027\u5316\u573a\u6240\u53d1\u73b0\u3001\u7269\u6d41\u4f18\u5316\u548c\u57ce\u5e02\u89c4\u5212\u7b49\u5e94\u7528\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e0b\u4e00\u4ee3\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u667a\u80fd\u7684\u7a7a\u95f4\u51b3\u7b56\u3002", "paper_title_zh": "\u4ece\u70b9\u5230\u573a\u6240\uff1a\u901a\u8fc7\u7406\u89e3\u573a\u6240\u6784\u5efa\u4eba\u7c7b\u79fb\u52a8\u6027\u9a71\u52a8\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b", "abstract_zh": "\u6355\u6349\u4eba\u7c7b\u79fb\u52a8\u6027\u5bf9\u4e8e\u5efa\u6a21\u4eba\u4eec\u5982\u4f55\u4e0e\u7269\u7406\u7a7a\u95f4\u4e92\u52a8\u548c\u79fb\u52a8\u81f3\u5173\u91cd\u8981\uff0c\u53cd\u6620\u4e86\u793e\u4f1a\u884c\u4e3a\u3001\u8d44\u6e90\u83b7\u53d6\u548c\u52a8\u6001\u7a7a\u95f4\u6a21\u5f0f\u3002\u4e3a\u4e86\u652f\u6301\u8de8\u591a\u6837\u5730\u7406\u548c\u573a\u666f\u7684\u53ef\u6269\u5c55\u548c\u53ef\u8f6c\u79fb\u5206\u6790\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u65f6\u7a7a\u6570\u636e\u57fa\u7840\u6a21\u578b\u3002\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5df2\u7ecf\u6539\u53d8\u4e86\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\uff0c\u4f46\u5728\u5904\u7406\u79fb\u52a8\u6027\u6570\u636e\u72ec\u7279\u7684\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u8bed\u4e49\u590d\u6742\u6027\u65b9\u9762\u4ecd\u6709\u9650\u5236\u3002\u672c\u6587\u5021\u5bfc\u4e00\u7c7b\u65b0\u578b\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u5730\u7406\u4f4d\u7f6e\u8bed\u4e49\u4e0e\u591a\u5c3a\u5ea6\u4eba\u7c7b\u79fb\u52a8\u6027\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u613f\u666f\u7684\u6838\u5fc3\u662f\u4ece\u5efa\u6a21\u79bb\u6563\u7684\u5174\u8da3\u70b9\u8f6c\u5411\u7406\u89e3\u201c\u573a\u6240\u201d\uff1a\u7531\u4eba\u7c7b\u884c\u4e3a\u4e0e\u79fb\u52a8\u6027\u5851\u9020\u7684\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u533a\u57df\uff0c\u53ef\u80fd\u5305\u542b\u591a\u4e2a\u5174\u8da3\u70b9\u3002\u6211\u4eec\u6307\u51fa\u4e86\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u591a\u7c92\u5ea6\u63a8\u7406\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e13\u6ce8\u4e8e\u5efa\u6a21\u573a\u6240\u548c\u9ad8\u6548\u5b66\u4e60\u7684\u7814\u7a76\u65b9\u5411\u3002\u76ee\u6807\u662f\u6307\u5bfc\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e0b\u4e00\u4ee3\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u5c06\u89e3\u9501\u4ece\u4e2a\u6027\u5316\u573a\u6240\u53d1\u73b0\u3001\u7269\u6d41\u4f18\u5316\u5230\u57ce\u5e02\u89c4\u5212\u7684\u5f3a\u5927\u5e94\u7528\uff0c\u6700\u7ec8\u5b9e\u73b0\u66f4\u667a\u80fd\u548c\u54cd\u5e94\u66f4\u5feb\u7684\u7a7a\u95f4\u51b3\u7b56\u3002"}}
{"id": "2506.14580", "pdf": "https://arxiv.org/pdf/2506.14580", "abs": "https://arxiv.org/abs/2506.14580", "authors": ["David Wan", "Eran Hirsch", "Elias Stengel-Eskin", "Ido Dagan", "Mohit Bansal"], "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs", "categories": ["cs.CL", "cs.AI"], "comment": "27 Pages. Code: https://github.com/meetdavidwan/generationprograms", "summary": "Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable \"code agent\" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.", "AI": {"tldr": "GenerationPrograms\u662f\u4e00\u79cd\u6a21\u5757\u5316\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u521b\u5efa\u53ef\u6267\u884c\u7a0b\u5e8f\u8ba1\u5212\u548c\u6267\u884c\u6a21\u5757\u5316\u6587\u672c\u64cd\u4f5c\u4e24\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u5f52\u56e0\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u6761\u4ef6\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8f93\u51fa\u5f52\u56e0\uff0c\u5f71\u54cd\u4e86\u53ef\u9a8c\u8bc1\u6027\u548c\u4fe1\u4efb\u3002\u6b64\u5916\uff0c\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u65e0\u6cd5\u89e3\u91ca\u6a21\u578b\u5982\u4f55\u5229\u7528\u6e90\u6587\u6863\u751f\u6210\u6700\u7ec8\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "method": "GenerationPrograms\u91c7\u7528\u6a21\u5757\u5316\u751f\u6210\u6846\u67b6\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u521b\u5efa\u9488\u5bf9\u67e5\u8be2\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\u8ba1\u5212\uff0c\u5305\u542b\u6a21\u5757\u5316\u6587\u672c\u64cd\u4f5c\uff08\u5982\u6539\u5199\u3001\u538b\u7f29\u548c\u878d\u5408\uff09\uff1b2) \u6267\u884c\u8fd9\u4e9b\u64cd\u4f5c\u4ee5\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGenerationPrograms\u5728\u4e24\u4e2a\u957f\u95ee\u7b54\u4efb\u52a1\u548c\u4e00\u4e2a\u591a\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u7ea7\u548c\u53e5\u5b50\u7ea7\u7684\u5f52\u56e0\u8d28\u91cf\uff0c\u5e76\u80fd\u4f5c\u4e3a\u540e\u9a8c\u5f52\u56e0\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002", "conclusion": "GenerationPrograms\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u53ef\u6267\u884c\u7a0b\u5e8f\u8ba1\u5212\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5f52\u56e0\u8d28\u91cf\uff0c\u8fd8\u652f\u6301\u901a\u8fc7\u6a21\u5757\u7ea7\u6539\u8fdb\u5b9e\u73b0\u5c40\u90e8\u4f18\u5316\uff0c\u589e\u5f3a\u4e86\u6574\u4f53\u5f52\u56e0\u80fd\u529b\u3002", "paper_title_zh": "GenerationPrograms\uff1a\u57fa\u4e8e\u53ef\u6267\u884c\u7a0b\u5e8f\u7684\u7ec6\u7c92\u5ea6\u5f52\u56e0", "abstract_zh": "\u8fd1\u671f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6e90\u6761\u4ef6\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u6b63\u786e\u63d0\u4f9b\u8f93\u51fa\u7684\u7ec6\u7c92\u5ea6\u5f52\u56e0\uff0c\u524a\u5f31\u4e86\u53ef\u9a8c\u8bc1\u6027\u548c\u4fe1\u4efb\u3002\u6b64\u5916\uff0c\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u672a\u80fd\u89e3\u91ca\u6a21\u578b\u5982\u4f55\u5229\u7528\u6e90\u6587\u6863\u751f\u6210\u6700\u7ec8\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u751f\u6210\u6846\u67b6GenerationPrograms\uff0c\u5176\u7075\u611f\u6765\u81ea\u53ef\u6267\u884c\u201c\u4ee3\u7801\u4ee3\u7406\u201d\u67b6\u6784\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u4e0e\u4f20\u7edf\u751f\u6210\u65b9\u6cd5\u4e0d\u540c\uff0cGenerationPrograms\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u521b\u5efa\u9488\u5bf9\u67e5\u8be2\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\u8ba1\u5212\uff0c\u5305\u542b\u6a21\u5757\u5316\u6587\u672c\u64cd\u4f5c\uff08\u5982\u6539\u5199\u3001\u538b\u7f29\u548c\u878d\u5408\uff09\uff1b\u5176\u6b21\uff0c\u6309\u7167\u7a0b\u5e8f\u6307\u4ee4\u6267\u884c\u8fd9\u4e9b\u64cd\u4f5c\u4ee5\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cGenerationPrograms\u5728\u4e24\u4e2a\u957f\u95ee\u7b54\u4efb\u52a1\u548c\u4e00\u4e2a\u591a\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u7ea7\u548c\u53e5\u5b50\u7ea7\u7684\u5f52\u56e0\u8d28\u91cf\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\uff0cGenerationPrograms\u53ef\u4f5c\u4e3a\u540e\u9a8c\u5f52\u56e0\u65b9\u6cd5\uff0c\u5728\u6062\u590d\u51c6\u786e\u5f52\u56e0\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002\u6b64\u5916\uff0cGenerationPrograms\u751f\u6210\u7684\u53ef\u89e3\u91ca\u7a0b\u5e8f\u652f\u6301\u901a\u8fc7\u6a21\u5757\u7ea7\u6539\u8fdb\u5b9e\u73b0\u5c40\u90e8\u4f18\u5316\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6574\u4f53\u5f52\u56e0\u8d28\u91cf\u3002"}}
{"id": "2506.14440", "pdf": "https://arxiv.org/pdf/2506.14440", "abs": "https://arxiv.org/abs/2506.14440", "authors": ["David E. Hernandez", "Jose Chang", "Torbj\u00f6rn E. M. Nordling"], "title": "Model compression using knowledge distillation with integrated gradients", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "49 pages, 12 figures", "summary": "Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u68af\u5ea6\uff08IG\uff09\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u578b\u538b\u7f29\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06IG\u56fe\u53e0\u52a0\u5230\u8f93\u5165\u56fe\u50cf\u4e0a\uff0c\u5b66\u751f\u6a21\u578b\u80fd\u66f4\u6df1\u5165\u7406\u89e3\u6559\u5e08\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728CIFAR-10\u4e0a\u5b9e\u73b0\u4e8692.6%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u538b\u7f29\u6bd4\u4e3a4.1\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u84b8\u998f\u6a21\u578b\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u538b\u7f29\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u6559\u5e08\u6a21\u578b\u51b3\u7b56\u7ec6\u8282\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96c6\u6210\u68af\u5ea6\uff08IG\uff09\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002\u5728\u8bad\u7ec3\u524d\u9884\u8ba1\u7b97IG\u56fe\uff0c\u5e76\u5c06\u5176\u53e0\u52a0\u5230\u8f93\u5165\u56fe\u50cf\u4e0a\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5e2e\u52a9\u5b66\u751f\u6a21\u578b\u66f4\u597d\u5730\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728CIFAR-10\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8692.6%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u538b\u7f29\u6bd4\u4e3a4.1\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u84b8\u998f\u6a21\u578b\uff0891.5%\uff09\u3002\u63a8\u7406\u65f6\u95f4\u4ece140\u6beb\u79d2\u964d\u81f313\u6beb\u79d2\u3002\u5b9e\u9a8c\u8fd8\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u538b\u7f29\u6bd4\u548c\u67b6\u6784\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u57fa\u4e8eIG\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u6a21\u578b\u538b\u7f29\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u80fd\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u53c8\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "paper_title_zh": "\u57fa\u4e8e\u96c6\u6210\u68af\u5ea6\u7684\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\u538b\u7f29\u65b9\u6cd5", "abstract_zh": "\u6a21\u578b\u538b\u7f29\u5bf9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u96c6\u6210\u68af\u5ea6\uff08IG\uff09\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6765\u589e\u5f3a\u77e5\u8bc6\u84b8\u998f\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06IG\u56fe\u53e0\u52a0\u5230\u8f93\u5165\u56fe\u50cf\u4e0a\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u80fd\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6559\u5e08\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u5728CIFAR-10\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684IG\u589e\u5f3a\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u4e8692.6%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u538b\u7f29\u6bd4\u4e3a4.1\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u84b8\u998f\u6a21\u578b\uff0891.5%\uff09\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u4ece140\u6beb\u79d2\u964d\u81f313\u6beb\u79d2\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u524d\u9884\u8ba1\u7b97IG\u56fe\uff0c\u5c06\u8fd0\u884c\u65f6\u6210\u672c\u8f6c\u5316\u4e3a\u4e00\u6b21\u6027\u9884\u5904\u7406\u6b65\u9aa4\u3002\u5b9e\u9a8c\u5305\u62ec\uff1a\uff081\uff09\u4e0e\u6ce8\u610f\u529b\u8f6c\u79fb\u7684\u6bd4\u8f83\uff0c\u663e\u793a\u4e0e\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u65f6\u7684\u4e92\u8865\u4f18\u52bf\uff1b\uff082\uff09\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u7edf\u8ba1\u9c81\u68d2\u6027\uff1b\uff083\uff09\u7cfb\u7edf\u8bc4\u4f30\u538b\u7f29\u6bd4\u4e0e\u51c6\u786e\u7387\u7684\u6743\u8861\uff082.2\u500d\u81f31122\u500d\uff09\uff1b\uff084\uff09\u5728ImageNet\u5b50\u96c6\u4e0a\u7684\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u5728\u521d\u59cb\u6570\u636e\u96c6\u4e4b\u5916\u7684\u901a\u7528\u6027\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eIG\u7684\u77e5\u8bc6\u84b8\u998f\u5728\u4e0d\u540c\u67b6\u6784\u548c\u538b\u7f29\u6bd4\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u538b\u7f29\u6280\u672f\u3002"}}
{"id": "2506.14728", "pdf": "https://arxiv.org/pdf/2506.14728", "abs": "https://arxiv.org/abs/2506.14728", "authors": ["Jiahao Qiu", "Xinzhe Juan", "Yimin Wang", "Ling Yang", "Xuan Qi", "Tongcheng Zhang", "Jiacheng Guo", "Yifu Lu", "Zixin Yao", "Hongru Wang", "Shilong Liu", "Xun Jiang", "Liu Leqi", "Mengdi Wang"], "title": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u84b8\u998f\u6846\u67b6AgentDistill\uff0c\u901a\u8fc7\u76f4\u63a5\u590d\u7528\u6559\u5e08\u667a\u80fd\u4f53\u751f\u6210\u7684\u6a21\u5757\u5316\u4efb\u52a1\u89e3\u51b3\u5355\u5143\uff08MCPs\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7684\u5b66\u751f\u667a\u80fd\u4f53\u5728\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u667a\u80fd\u4f53\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5b8c\u6574\u91cd\u653e\u6559\u5e08\u8f68\u8ff9\u6216\u9010\u6b65\u6a21\u4eff\u6559\u5e08\u5de5\u5177\u4f7f\u7528\uff0c\u96be\u4ee5\u8bad\u7ec3\u5b66\u751f\u667a\u80fd\u4f53\u5728\u65b0\u73af\u5883\u4e2d\u52a8\u6001\u89c4\u5212\u548c\u884c\u52a8\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u53ef\u6cdb\u5316\u7684\u667a\u80fd\u4f53\u84b8\u998f\u65b9\u6cd5\u3002", "method": "AgentDistill\u6846\u67b6\u901a\u8fc7\u590d\u7528\u6559\u5e08\u667a\u80fd\u4f53\u81ea\u4e3b\u751f\u6210\u7684\u7ed3\u6784\u5316\u3001\u53ef\u590d\u7528\u7684\u4efb\u52a1\u89e3\u51b3\u6a21\u5757\uff08MCPs\uff09\uff0c\u5b9e\u73b0\u77e5\u8bc6\u7684\u9ad8\u6548\u8fc1\u79fb\u3002\u5b66\u751f\u667a\u80fd\u4f53\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u76f4\u63a5\u5229\u7528\u8fd9\u4e9b\u6a21\u5757\u89e3\u51b3\u65b0\u95ee\u9898\u3002", "result": "\u5728\u751f\u7269\u533b\u5b66\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u751f\u667a\u80fd\u4f53\u8868\u73b0\u4e0e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u7684\u9ad8\u7ea7\u7cfb\u7edf\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "AgentDistill\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u590d\u7528MCP\u6a21\u5757\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u5f00\u9500\u3002", "paper_title_zh": "AgentDistill\uff1a\u57fa\u4e8e\u901a\u7528MCP\u6a21\u5757\u7684\u65e0\u8bad\u7ec3\u667a\u80fd\u4f53\u84b8\u998f", "abstract_zh": "\u5c3d\u7ba1\u77e5\u8bc6\u84b8\u998f\u5df2\u6210\u4e3a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u538b\u7f29\u4e3a\u5c0f\u578b\u6a21\u578b\u7684\u6210\u719f\u9886\u57df\uff0c\u4f46\u6d89\u53ca\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4f7f\u7528\u7684\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u84b8\u998f\u4ecd\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91cd\u653e\u5b8c\u6574\u6559\u5e08\u8f68\u8ff9\u6216\u9010\u6b65\u6a21\u4eff\u6559\u5e08\u5de5\u5177\u4f7f\u7528\uff0c\u4f46\u96be\u4ee5\u8bad\u7ec3\u5b66\u751f\u667a\u80fd\u4f53\u5728\u65b0\u73af\u5883\u4e2d\u52a8\u6001\u89c4\u5212\u548c\u884c\u52a8\u3002\u6211\u4eec\u63d0\u51faAgentDistill\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u578b\u667a\u80fd\u4f53\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u590d\u7528\u6559\u5e08\u667a\u80fd\u4f53\u81ea\u4e3b\u751f\u6210\u7684\u7ed3\u6784\u5316\u3001\u53ef\u590d\u7528\u4efb\u52a1\u89e3\u51b3\u6a21\u5757\uff08MCPs\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002\u8fd9\u4e9b\u84b8\u998f\u7684MCPs\u4f7f\u5b66\u751f\u667a\u80fd\u4f53\u80fd\u591f\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u6700\u5c0f\u76d1\u7763\u6216\u4eba\u5de5\u5e72\u9884\u4e0b\u89e3\u51b3\u65b0\u95ee\u9898\u3002\u5728\u751f\u7269\u533b\u5b66\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u751f\u667a\u80fd\u4f53\u8868\u73b0\u4e0e\u4f7f\u7528\u5927\u578bLLMs\uff08\u5982OctoTools\uff08GPT-4o\uff09\uff09\u7684\u9ad8\u7ea7\u7cfb\u7edf\u76f8\u5f53\uff0c\u51f8\u663e\u4e86\u8be5\u6846\u67b6\u5728\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.14606", "pdf": "https://arxiv.org/pdf/2506.14606", "abs": "https://arxiv.org/abs/2506.14606", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGG\uff08Guaranteed Guess\uff09\u7684CISC\u5230RISC\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\u8f6c\u6362\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ffb\u8bd1\u80fd\u529b\u548c\u8f6f\u4ef6\u6d4b\u8bd5\u6846\u67b6\uff0c\u786e\u4fdd\u7ffb\u8bd1\u7684\u6b63\u786e\u6027\u548c\u9ad8\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGG\u5728\u529f\u80fd\u6b63\u786e\u6027\u3001\u8fd0\u884c\u6027\u80fd\u3001\u80fd\u6548\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u968f\u7740\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\u5feb\u901f\u3001\u7075\u6d3b\u5730\u5b9e\u73b0\u4e0d\u540c\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\u4e4b\u95f4\u7684\u4ee3\u7801\u8f6c\u6362\uff0c\u6210\u4e3a\u63d0\u5347\u4ee3\u7801\u53ef\u79fb\u690d\u6027\u548c\u957f\u671f\u53ef\u7528\u6027\u7684\u5173\u952e\u6311\u6218\u3002\u7279\u522b\u662fCISC\u548cRISC\u67b6\u6784\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u7531\u4e8e\u6307\u4ee4\u590d\u6742\u5ea6\u3001\u5185\u5b58\u6a21\u578b\u548c\u6267\u884c\u8303\u5f0f\u7684\u6839\u672c\u5dee\u5f02\uff0c\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "GG\u65b9\u6cd5\u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ffb\u8bd1\u80fd\u529b\u548c\u8f6f\u4ef6\u6d4b\u8bd5\u6846\u67b6\uff0c\u9996\u5148\u751f\u6210\u5019\u9009\u7ffb\u8bd1\u4ee3\u7801\uff0c\u7136\u540e\u901a\u8fc7\u6d4b\u8bd5\u6846\u67b6\u91cf\u5316\u7ffb\u8bd1\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b9e\u9a8c\u8986\u76d6\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u786e\u4fdd\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\uff08>98%\uff09\uff0c\u5e76\u9a8c\u8bc1\u529f\u80fd/\u8bed\u4e49\u6b63\u786e\u6027\u3002", "result": "GG\u5728HumanEval\u7a0b\u5e8f\u4e0a\u5b9e\u73b0\u4e8699%\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5728BringupBench\u7a0b\u5e8f\u4e0a\u8fbe\u523049%\u3002\u4e0eApple Silicon\u7684Rosetta 2\u6846\u67b6\u76f8\u6bd4\uff0cGG\u7684\u8fd0\u884c\u6027\u80fd\u63d0\u53471.73\u500d\uff0c\u80fd\u6548\u63d0\u9ad81.47\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c112.41\u500d\u3002", "conclusion": "GG\u65b9\u6cd5\u5728CISC\u5230RISC\u7684\u4ee3\u7801\u8f6c\u6362\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u6b63\u786e\u6027\u3002\u5f00\u6e90\u4ee3\u7801\u3001\u6570\u636e\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u4e3aISA\u7ea7\u4ee3\u7801\u8f6c\u6362\u7814\u7a76\u63d0\u4f9b\u5171\u540c\u57fa\u7840\u3002", "paper_title_zh": "\u4fdd\u8bc1\u731c\u6d4b\uff1a\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u5efa\u6a21\u7684CISC\u5230RISC\u8f6c\u6362\u65b9\u6cd5\u53ca\u5176\u6d4b\u8bd5\u4fdd\u8bc1", "abstract_zh": "\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4e3a\u4e86\u63d0\u5347\u73b0\u6709\u4ee3\u7801\u7684\u53ef\u79fb\u690d\u6027\u548c\u957f\u671f\u53ef\u7528\u6027\uff0c\u5982\u4f55\u5728\u5feb\u901f\u3001\u7075\u6d3b\u4e14\u6b63\u786e\u7684\u65b9\u5f0f\u4e0b\u5b9e\u73b0\u4e0d\u540c\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\u4e4b\u95f4\u7684\u4f4e\u7ea7\u7a0b\u5e8f\u8f6c\u6362\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002CISC\u548cRISC\u67b6\u6784\u4e4b\u95f4\u7684\u8f6c\u6362\u5c24\u4e3a\u56f0\u96be\uff0c\u56e0\u5176\u5728\u6307\u4ee4\u590d\u6742\u5ea6\u3001\u5185\u5b58\u6a21\u578b\u548c\u6267\u884c\u8303\u5f0f\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002\u672c\u6587\u63d0\u51faGG\uff08Guaranteed Guess\uff09\uff0c\u4e00\u79cd\u4ee5ISA\u4e3a\u4e2d\u5fc3\u7684\u8f6c\u6362\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ffb\u8bd1\u80fd\u529b\u548c\u6210\u719f\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u6846\u67b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7LLM\u751f\u6210\u5019\u9009\u7ffb\u8bd1\u4ee3\u7801\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u8f6f\u4ef6\u6d4b\u8bd5\u6846\u67b6\u4e2d\u4ee5\u91cf\u5316\u7ffb\u8bd1\u7684\u7f6e\u4fe1\u5ea6\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30GG\u65b9\u6cd5\uff0c\u786e\u4fdd\u5355\u5143\u6d4b\u8bd5\u7684\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\uff08>98%\uff09\uff0c\u5e76\u5728HumanEval\u7a0b\u5e8f\u4e0a\u5b9e\u73b099%\u7684\u529f\u80fd/\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u5728BringupBench\u7a0b\u5e8f\u4e0a\u8fbe\u523049%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e0eApple Silicon\u7684Rosetta 2\u6846\u67b6\u5bf9\u6bd4\uff0cGG\u7684\u8f6c\u6362\u4ee3\u7801\u5728\u8fd0\u884c\u6027\u80fd\u4e0a\u5feb1.73\u500d\uff0c\u80fd\u6548\u9ad81.47\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c112.41\u500d\uff0c\u8bc1\u660e\u4e86GG\u5728\u5b9e\u9645CISC\u5230RISC\u8f6c\u6362\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5c06\u5f00\u6e90\u4ee3\u7801\u3001\u6570\u636e\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3aISA\u7ea7\u4ee3\u7801\u8f6c\u6362\u7814\u7a76\u5efa\u7acb\u5171\u540c\u57fa\u7840\u3002"}}
{"id": "2506.14451", "pdf": "https://arxiv.org/pdf/2506.14451", "abs": "https://arxiv.org/abs/2506.14451", "authors": ["Aditya Shourya", "Michel Dumontier", "Chang Sun"], "title": "Adapting Lightweight Vision Language Models for Radiological Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff083B\u53c2\u6570\uff09\u7528\u4e8e\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\uff0c\u8bc1\u660e\u5c0f\u6a21\u578b\u5728\u7cbe\u5fc3\u8c03\u4f18\u540e\u80fd\u5728\u5f00\u653e\u548c\u5c01\u95ed\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u63d0\u51fa\u4f4e\u6210\u672c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u5408\u6210\u95ee\u7b54\u5bf9\u751f\u6210\u548c\u591a\u9636\u6bb5\u5fae\u8c03\uff0c\u7ed3\u679c\u867d\u89c4\u6a21\u5c0f\u4f46\u6027\u80fd\u63a5\u8fd1\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u653e\u5c04\u5b66VQA\u6a21\u578b\u9762\u4e34\u6570\u636e\u6807\u6ce8\u6709\u9650\u3001\u56fe\u50cf\u6a21\u5f0f\u590d\u6742\u53ca\u7f3a\u4e4f\u8bc4\u4f30\u5de5\u5177\u7684\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u8bc1\u660e\u8f7b\u91cf\u7ea7\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\u548c\u6570\u636e\u96c6\uff0c\u80fd\u5728\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea73B\u53c2\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u95ee\u7b54\u5bf9\u751f\u6210\u548c\u591a\u9636\u6bb5\u5fae\u8c03\uff08\u5982ROCO v2.0\u548cMedPix v2.0\u6570\u636e\u96c6\uff09\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u663e\u8457\u6027\u7684\u8bca\u65ad\u5de5\u5177\u5206\u6790\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5c3d\u7ba1\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u8fdc\u5c0f\u4e8e\u5148\u8fdb\u6a21\u578b\uff08\u5982LLaVA-Med\uff09\uff0c\u4f46\u5728\u5f00\u653e\u548c\u5c01\u95ed\u95ee\u9898\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u4e14\u901a\u8fc7\u663e\u8457\u6027\u5206\u6790\u5de5\u5177\u80fd\u6709\u6548\u8bc6\u522b\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\u548c\u6570\u636e\u96c6\u53ef\u663e\u8457\u63d0\u5347\u653e\u5c04\u5b66VQA\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u9002\u914d", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u7684\u8fdb\u6b65\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u6a21\u578b\u5f00\u53d1\u7684\u6bcf\u4e2a\u9636\u6bb5\u4ecd\u5b58\u5728\u6311\u6218\uff1a\u4e13\u5bb6\u6807\u6ce8\u56fe\u50cf\u6709\u9650\u963b\u788d\u4e86\u5927\u89c4\u6a21\u6570\u636e\u83b7\u53d6\uff1b\u653e\u5c04\u5b66\u56fe\u50cf\u7684\u590d\u6742\u548c\u7ec6\u5fae\u6a21\u5f0f\u4f7f\u5efa\u6a21\u53d8\u5f97\u56f0\u96be\uff1b\u7f3a\u4e4f\u8bc4\u4f30\u5de5\u4f5c\u4f7f\u5f97\u96be\u4ee5\u8bc6\u522b\u6a21\u578b\u53ef\u80fd\u4e0d\u9002\u7528\u7684\u6848\u4f8b\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u4e00\u4e2a\u8f7b\u91cf\u7ea73B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u653e\u5c04\u5b66VQA\uff0c\u8bc1\u660e\u5c0f\u6a21\u578b\u5728\u7cbe\u5fc3\u8c03\u4f18\u540e\u80fd\u5728\u5f00\u653e\u548c\u5c01\u95ed\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ece\u5408\u6210\u95ee\u7b54\u5bf9\u751f\u6210\u5230\u9488\u5bf9\u653e\u5c04\u5b66\u9886\u57df\u6570\u636e\u96c6\uff08\u5982ROCO v2.0\u3001MedPix v2.0\uff09\u7684\u591a\u9636\u6bb5\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u8fdc\u5c0f\u4e8e\u5148\u8fdb\u6a21\u578b\uff08\u5982LLaVA-Med\uff09\uff0c\u4f46\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4ecd\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u7684\u8f7b\u91cf\u7ea7\u8bca\u65ad\u5de5\u5177\uff0c\u5e2e\u52a9\u9886\u57df\u4e13\u5bb6\u901a\u8fc7\u663e\u8457\u6027\u5206\u6790\u68c0\u67e5VQA\u6a21\u578b\u6027\u80fd\u5e76\u8bc6\u522b\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2506.14755", "pdf": "https://arxiv.org/pdf/2506.14755", "abs": "https://arxiv.org/abs/2506.14755", "authors": ["Zhengxiang Cheng", "Dongping Chen", "Mingyang Fu", "Tianyi Zhou"], "title": "Optimizing Length Compression in Large Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 7 figures, 4 tables", "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5e38\u56e0\u751f\u6210\u5197\u957f\u4e14\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u94fe\u800c\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u539f\u5219\u2014\u2014\u7b80\u6d01\u6027\u548c\u5145\u5206\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86LC-R1\u65b9\u6cd5\uff0c\u901a\u8fc7\u957f\u5ea6\u5956\u52b1\u548c\u538b\u7f29\u5956\u52b1\u663e\u8457\u51cf\u5c11\u63a8\u7406\u94fe\u957f\u5ea6\uff08\u7ea650%\uff09\uff0c\u540c\u65f6\u4ec5\u8f7b\u5fae\u5f71\u54cd\u51c6\u786e\u6027\uff08\u7ea62%\uff09\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e38\u4ea7\u751f\u5197\u4f59\u7684\u2018\u65e0\u6548\u601d\u8003\u2019\uff0c\u5373\u53cd\u590d\u9a8c\u8bc1\u5df2\u6b63\u786e\u7684\u7b54\u6848\u3002\u4e3a\u63d0\u9ad8\u6548\u7387\uff0c\u672c\u6587\u63d0\u51fa\u8d85\u8d8a\u4f20\u7edf\u6548\u80fd\u548c\u6548\u7387\u539f\u5219\u7684\u2018\u7b80\u6d01\u6027\u2019\u548c\u2018\u5145\u5206\u6027\u2019\uff0c\u65e8\u5728\u6d88\u9664\u5197\u4f59\u5e76\u4fdd\u7559\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u672c\u6587\u63d0\u51faLC-R1\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u7ed3\u5408\u957f\u5ea6\u5956\u52b1\uff08\u9f13\u52b1\u6574\u4f53\u7b80\u6d01\uff09\u548c\u538b\u7f29\u5956\u52b1\uff08\u4e13\u95e8\u6d88\u9664\u65e0\u6548\u601d\u8003\u90e8\u5206\uff09\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLC-R1\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u63a8\u7406\u94fe\u957f\u5ea6\uff08\u7ea650%\uff09\uff0c\u4ec5\u8f7b\u5fae\u964d\u4f4e\u51c6\u786e\u6027\uff08\u7ea62%\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "conclusion": "LC-R1\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u94fe\u957f\u5ea6\uff0c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u5728\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "paper_title_zh": "\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u957f\u5ea6\u538b\u7f29", "abstract_zh": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u63a8\u7406\u94fe\u5e38\u5305\u542b\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u5185\u5bb9\u3002\u6211\u4eec\u5c06\u6b64\u95ee\u9898\u7684\u6838\u5fc3\u5f52\u56e0\u4e8e\u2018\u65e0\u6548\u601d\u8003\u2019\u2014\u2014\u6a21\u578b\u5728\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u540e\u4ecd\u53cd\u590d\u9a8c\u8bc1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u4f4e\u6548\u95ee\u9898\uff0c\u6211\u4eec\u8d85\u8d8a\u4f20\u7edf\u7684\u6548\u80fd\u548c\u6548\u7387\u539f\u5219\uff0c\u63d0\u51fa\u4e24\u4e2a\u66f4\u7ec6\u5316\u7684\u65b0\u539f\u5219\uff1a\u7b80\u6d01\u6027\uff08\u6d88\u9664\u5197\u4f59\uff09\u548c\u5145\u5206\u6027\uff08\u4fdd\u7559\u5173\u952e\u63a8\u7406\u6b65\u9aa4\uff09\u3002\u57fa\u4e8e\u8fd9\u4e9b\u539f\u5219\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LC-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002LC-R1\u7ed3\u5408\u4e86\u957f\u5ea6\u5956\u52b1\uff08\u9f13\u52b1\u6574\u4f53\u7b80\u6d01\uff09\u548c\u538b\u7f29\u5956\u52b1\uff08\u4e13\u95e8\u6d88\u9664\u65e0\u6548\u601d\u8003\u90e8\u5206\uff09\u3002\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLC-R1\u663e\u8457\u51cf\u5c11\u4e86\u5e8f\u5217\u957f\u5ea6\uff08\u7ea650%\uff09\uff0c\u4ec5\u8f7b\u5fae\u964d\u4f4e\u51c6\u786e\u6027\uff08\u7ea62%\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u9a8c\u8bc1\u4e86LC-R1\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684LRMs\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/zxiangx/LC-R1\u3002"}}
{"id": "2506.14613", "pdf": "https://arxiv.org/pdf/2506.14613", "abs": "https://arxiv.org/abs/2506.14613", "authors": ["Junghyun Min", "Xiulin Yang", "Shira Wein"], "title": "When Does Meaning Backfire? Investigating the Role of AMRs in NLI", "categories": ["cs.CL"], "comment": "9 pages, 2 figures", "summary": "Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4e2d\uff0c\u6dfb\u52a0\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u5728\u5fae\u8c03\u65f6\u4f1a\u963b\u788d\u6a21\u578b\u6cdb\u5316\uff0c\u800c\u5728\u63d0\u793a\u8bbe\u7f6e\u4e2d\u7565\u6709\u63d0\u5347\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u6e90\u4e8e\u8868\u9762\u5dee\u5f02\u7684\u653e\u5927\u800c\u975e\u8bed\u4e49\u63a8\u7406\u3002", "motivation": "\u63a2\u8ba8\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5f15\u5165\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u662f\u5426\u80fd\u5e2e\u52a9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u5730\u6cdb\u5316\u3002", "method": "\u901a\u8fc7\u5728\u5fae\u8c03\u548c\u63d0\u793a\u4e24\u79cd\u8bbe\u7f6e\u4e2d\u96c6\u6210AMR\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u4ee5\u5206\u6790AMR\u7684\u4f5c\u7528\u3002", "result": "\u5fae\u8c03\u65f6AMR\u963b\u788d\u6a21\u578b\u6cdb\u5316\uff0c\u63d0\u793a\u8bbe\u7f6e\u4e2dAMR\u5e26\u6765\u8f7b\u5fae\u63d0\u5347\uff0c\u4f46\u6d88\u878d\u7814\u7a76\u8868\u660e\u63d0\u5347\u6e90\u4e8e\u8868\u9762\u5dee\u5f02\u653e\u5927\u800c\u975e\u8bed\u4e49\u63a8\u7406\u3002", "conclusion": "AMR\u5728NLI\u4e2d\u7684\u4f5c\u7528\u6709\u9650\uff0c\u5176\u63d0\u5347\u6548\u679c\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u5ffd\u7565\u6838\u5fc3\u8bed\u4e49\u3002", "paper_title_zh": "\u610f\u4e49\u4f55\u65f6\u9002\u5f97\u5176\u53cd\uff1f\u63a2\u7a76AMR\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u4f5c\u7528", "abstract_zh": "\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5bf9\u524d\u63d0\u548c\u5047\u8bbe\u8bed\u4e49\u5185\u5bb9\u7684\u5145\u5206\u89e3\u6790\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee5\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u5f62\u5f0f\u6dfb\u52a0\u8bed\u4e49\u4fe1\u606f\u662f\u5426\u80fd\u5e2e\u52a9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728NLI\u4e2d\u66f4\u597d\u5730\u6cdb\u5316\u3002\u5b9e\u9a8c\u901a\u8fc7\u5728\u5fae\u8c03\u548c\u63d0\u793a\u4e24\u79cd\u8bbe\u7f6e\u4e2d\u96c6\u6210AMR\uff0c\u7ed3\u679c\u663e\u793a\u5fae\u8c03\u65f6AMR\u963b\u788d\u6a21\u578b\u6cdb\u5316\uff0c\u800c\u63d0\u793a\u8bbe\u7f6e\u4e2dAMR\u4e3a\\texttt{GPT-4o}\u5e26\u6765\u8f7b\u5fae\u63d0\u5347\u3002\u7136\u800c\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u63d0\u5347\u6e90\u4e8e\u653e\u5927\u8868\u9762\u5dee\u5f02\u800c\u975e\u8f85\u52a9\u8bed\u4e49\u63a8\u7406\u3002\u8fd9\u79cd\u653e\u5927\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u6838\u5fc3\u610f\u4e49\u4fdd\u7559\u65f6\u4ecd\u9884\u6d4b\u975e\u8574\u542b\u5173\u7cfb\u3002"}}
{"id": "2506.14471", "pdf": "https://arxiv.org/pdf/2506.14471", "abs": "https://arxiv.org/abs/2506.14471", "authors": ["Yikang Zhou", "Tao Zhang", "Dizhe Zhang", "Shunping Ji", "Xiangtai Li", "Lu Qi"], "title": "Dense360: Dense Understanding from Omnidirectional Panoramas", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDense360\uff0c\u9996\u6b21\u901a\u8fc7\u5168\u666f\u56fe\u50cf\u5b9e\u73b0\u5bc6\u96c6\u89c6\u89c9\u7406\u89e3\uff0c\u5e76\u5f15\u5165ERP-RoPE\u4f4d\u7f6e\u7f16\u7801\u89e3\u51b3\u5168\u666f\u6295\u5f71\u7684\u6311\u6218\uff0c\u540c\u65f6\u53d1\u5e03\u9996\u4e2a\u5168\u666f\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u57fa\u51c6Dense360-Bench\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u901a\u8fc7\u6709\u9650\u89c6\u573a\uff08FOV\uff09\u89c6\u89c9\u8f93\u5165\u5b9e\u73b0\u4e16\u754c\u7406\u89e3\uff0c\u4f46\u5168\u666f\u56fe\u50cf\u80fd\u63d0\u4f9b\u66f4\u5b8c\u6574\u3001\u7d27\u51d1\u4e14\u8fde\u7eed\u7684\u573a\u666f\u8868\u793a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5168\u666f\u56fe\u50cf\u63a8\u52a8\u5bc6\u96c6\u89c6\u89c9\u7406\u89e3\u7684\u53d1\u5c55\u3002", "method": "1. \u6784\u5efa\u5305\u542b16\u4e07\u5f20\u5168\u666f\u56fe\u50cf\u3001500\u4e07\u5bc6\u96c6\u5b9e\u4f53\u7ea7\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff1b2. \u63d0\u51faERP-RoPE\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\uff0c\u89e3\u51b3\u5168\u666f\u6295\u5f71\u4e2d\u7684\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\u53d8\u5316\u95ee\u9898\uff1b3. \u53d1\u5e03Dense360-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5168\u666f\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5168\u666f\u56fe\u50cf\u80fd\u663e\u8457\u63d0\u5347\u573a\u666f\u7406\u89e3\u7684\u5b8c\u6574\u6027\uff0cERP-RoPE\u6709\u6548\u89e3\u51b3\u4e86\u5168\u666f\u6295\u5f71\u7684\u6311\u6218\uff0cDense360-Bench\u4e3a\u5168\u666f\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5168\u666f\u56fe\u50cf\u548cERP-RoPE\u7f16\u7801\u63a8\u52a8\u4e86\u5bc6\u96c6\u89c6\u89c9\u7406\u89e3\u7684\u53d1\u5c55\uff0cDense360-Bench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "paper_title_zh": "Dense360\uff1a\u57fa\u4e8e\u5168\u666f\u56fe\u50cf\u7684\u5bc6\u96c6\u89c6\u89c9\u7406\u89e3", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9700\u8981\u5168\u9762\u7684\u89c6\u89c9\u8f93\u5165\u4ee5\u5b9e\u73b0\u5bf9\u7269\u7406\u4e16\u754c\u7684\u5bc6\u96c6\u7406\u89e3\u3002\u5c3d\u7ba1\u73b0\u6709MLLMs\u901a\u8fc7\u6709\u9650\u89c6\u573a\uff08FOV\uff09\u89c6\u89c9\u8f93\u5165\uff08\u598270\u5ea6\uff09\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u4e16\u754c\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u672c\u6587\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u5168\u666f\u56fe\u50cf\u5b9e\u73b0\u5bc6\u96c6\u7406\u89e3\u3002\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u666f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u5168\u9762\u7684\u53ef\u9760\u6027\u8bc4\u5206\u6807\u6ce8\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6570\u636e\u96c6\u5305\u542b16\u4e07\u5f20\u5168\u666f\u56fe\u50cf\u3001500\u4e07\u5bc6\u96c6\u5b9e\u4f53\u7ea7\u6807\u6ce8\u3001100\u4e07\u72ec\u7279\u6307\u4ee3\u8868\u8fbe\u548c10\u4e07\u5b9e\u4f53\u57fa\u7840\u7684\u5168\u666f\u573a\u666f\u63cf\u8ff0\u3002\u4e0e\u591a\u89c6\u56fe\u65b9\u6848\u76f8\u6bd4\uff0c\u5168\u666f\u56fe\u50cf\u901a\u8fc7\u7b49\u8ddd\u67f1\u72b6\u6295\u5f71\uff08ERP\uff09\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u3001\u7d27\u51d1\u4e14\u8fde\u7eed\u7684\u573a\u666f\u8868\u793a\u3002\u7136\u800c\uff0cERP\u7684\u4f7f\u7528\u4e3aMLLMs\u5e26\u6765\u4e86\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1ai\uff09\u6cbf\u7eac\u5ea6\u5708\u7684\u7a7a\u95f4\u8fde\u7eed\u6027\uff0cii\uff09\u4fe1\u606f\u5bc6\u5ea6\u968f\u7eac\u5ea6\u7684\u53d8\u5316\u3002\u6211\u4eec\u901a\u8fc7\u4e13\u4e3a\u5168\u666fERP\u8bbe\u8ba1\u7684ERP-RoPE\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u5e03\u4e86Dense360-Bench\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5168\u666f\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u7684\u57fa\u51c6\uff0c\u4e3a\u5168\u666f\u573a\u666f\u4e0b\u7684\u5bc6\u96c6\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u5efa\u7acb\u4e86\u5168\u9762\u7684\u6846\u67b6\u3002"}}
{"id": "2506.14625", "pdf": "https://arxiv.org/pdf/2506.14625", "abs": "https://arxiv.org/abs/2506.14625", "authors": ["Chenchen Yuan", "Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u805a\u5408\u548c\u5d4c\u5165\u4f18\u5316\uff0c\u5c06\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u5224\u65ad\u6574\u5408\u4e3a\u96c6\u4f53\u5171\u8bc6\uff0c\u5e76\u4f18\u5316\u504f\u79bb\u6a21\u578b\u7684\u5d4c\u5165\uff0c\u4ee5\u63d0\u9ad8\u9053\u5fb7\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u9053\u5fb7\u56f0\u5883\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u96c6\u4f53\u5171\u8bc6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9053\u5fb7\u5224\u65ad\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u6982\u7387\u805a\u5408\u673a\u5236\uff0c\u5c06\u591a\u4e2a\u6a21\u578b\u7684\u9053\u5fb7\u8bc4\u5206\u878d\u5408\u4e3a\u96c6\u4f53\u6982\u7387\uff0c\u5e76\u6839\u636e\u6a21\u578b\u53ef\u9760\u6027\u52a0\u6743\uff1b\u5bf9\u504f\u79bb\u5171\u8bc6\u7684\u6a21\u578b\uff0c\u91c7\u7528\u5d4c\u5165\u4f18\u5316\u6280\u672f\u8c03\u6574\u5176\u9053\u5fb7\u54f2\u5b66\u7406\u8bba\u7684\u8bcd\u5d4c\u5165\uff0c\u4ee5\u6700\u5c0f\u5316\u4e0e\u5171\u8bc6\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u578b\u793e\u4f1a\u9053\u5fb7\u56f0\u5883\u6570\u636e\u96c6\u4e0a\u6210\u529f\u6784\u5efa\u4e86\u7a33\u5065\u7684\u96c6\u4f53\u5171\u8bc6\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u4f53\u6a21\u578b\u7684\u9053\u5fb7\u5224\u65ad\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8de8\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u9053\u5fb7\u5bf9\u9f50\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u4e00\u81f4\u7684AI\u7cfb\u7edf\u3002", "paper_title_zh": "\u6982\u7387\u805a\u5408\u4e0e\u76ee\u6807\u5d4c\u5165\u4f18\u5316\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u96c6\u4f53\u9053\u5fb7\u63a8\u7406", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u7684\u591a\u56e0\u7d20\u9053\u5fb7\u56f0\u5883\u65f6\uff0c\u5176\u5224\u65ad\u5f80\u5f80\u5b58\u5728\u5206\u6b67\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5c06\u591a\u4e2aLLMs\u7684\u9053\u5fb7\u5224\u65ad\u6574\u5408\u4e3a\u96c6\u4f53\u5171\u8bc6\uff0c\u5e76\u5bf9\u663e\u8457\u504f\u79bb\u5171\u8bc6\u7684\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u6821\u51c6\u3002\u6211\u4eec\u7684\u805a\u5408\u673a\u5236\u5c06\u8fde\u7eed\u7684\u9053\u5fb7\u53ef\u63a5\u53d7\u6027\u8bc4\u5206\uff08\u800c\u975e\u4e8c\u5143\u6807\u7b7e\uff09\u878d\u5408\u4e3a\u96c6\u4f53\u6982\u7387\uff0c\u5e76\u6839\u636e\u6a21\u578b\u53ef\u9760\u6027\u52a0\u6743\u8d21\u732e\u3002\u5bf9\u4e8e\u504f\u79bb\u6a21\u578b\uff0c\u91c7\u7528\u76ee\u6807\u5d4c\u5165\u4f18\u5316\u6280\u672f\uff0c\u9488\u5bf9\u9053\u5fb7\u54f2\u5b66\u7406\u8bba\u7684\u8bcd\u5d4c\u5165\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6700\u5c0f\u5316\u4e0e\u5171\u8bc6\u7684JS\u6563\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002\u5728\u5927\u89c4\u6a21\u793e\u4f1a\u9053\u5fb7\u56f0\u5883\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u7a33\u5065\u7684\u5171\u8bc6\uff0c\u5e76\u63d0\u5347\u4e2a\u4f53\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u8de8\u6a21\u578b\u6570\u636e\u9a71\u52a8\u9053\u5fb7\u5bf9\u9f50\u7684\u4ef7\u503c\uff0c\u53ca\u5176\u5728\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u4e00\u81f4\u7684AI\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14473", "pdf": "https://arxiv.org/pdf/2506.14473", "abs": "https://arxiv.org/abs/2506.14473", "authors": ["Zhijing Wan", "Zhixiang Wang", "Zheng Wang", "Xin Xu", "Shin'ichi Satoh"], "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages, 10 figures, accepted by ICML 2025", "summary": "One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u4e00\u79cd\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u578b\u65b9\u6cd5RAM-APL\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5b50\u96c6\u9009\u62e9\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4fe1\u606f\u63d0\u53d6\u5668\uff08IE\uff09\u4f9d\u8d56\u4e8e\u76ee\u6807\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u53ef\u80fd\u63d0\u4f9b\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1FMs\u5728\u5b50\u96c6\u9009\u62e9\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAM-APL\uff08RAnking Mean-Accuracy of Pseudo-class Labels\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4f18\u5316\u7ec6\u7c92\u5ea6\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5b50\u96c6\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFMs\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfIE\u65b9\u6cd5\uff0c\u4f46\u5728\u7c97\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u52bf\u4e0d\u660e\u663e\u3002RAM-APL\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\uff08\u5982Oxford-IIIT Pet\u3001Food-101\u548cCaltech-UCSD Birds-200-2011\uff09\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u5b50\u96c6\u9009\u62e9\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u800cRAM-APL\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u578b\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u7840\u6a21\u578b\u6d1e\u5bdf\u4e0e\u591a\u6a21\u578b\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u5355\u6b21\u5b50\u96c6\u9009\u62e9\u4e2d\u7684\u5353\u8d8a\u8868\u73b0", "abstract_zh": "\u5355\u6b21\u5b50\u96c6\u9009\u62e9\u662f\u4e00\u79cd\u901a\u8fc7\u4fe1\u606f\u63d0\u53d6\u5668\uff08IE\uff09\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u6570\u636e\u5b50\u96c6\u4ee5\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6210\u672c\u7684\u6709\u6548\u5de5\u5177\u3002\u4f20\u7edfIE\u901a\u5e38\u57fa\u4e8e\u76ee\u6807\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u5177\u6709\u6570\u636e\u96c6\u4f9d\u8d56\u6027\u3002\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u672c\u6587\u7814\u7a76\u4e86\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\uff081\uff09\u57fa\u4e8eFM\u7684\u5b50\u96c6\u9009\u62e9\u662f\u5426\u80fd\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u4f20\u7edfIE\u65b9\u6cd5\uff1f\uff082\uff09\u6240\u6709FMs\u5728\u5b50\u96c6\u9009\u62e9\u4e2d\u662f\u5426\u8868\u73b0\u4e00\u81f4\uff1f\u5927\u91cf\u5b9e\u9a8c\u63ed\u793a\u4e86\u4ee4\u4eba\u60ca\u8bb6\u7684\u53d1\u73b0\uff1aFMs\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edfIE\uff0c\u800c\u5728\u6807\u7b7e\u566a\u58f0\u8f83\u591a\u7684\u7c97\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u52bf\u51cf\u5f31\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RAM-APL\uff08RAnking Mean-Accuracy of Pseudo-class Labels\uff09\uff0c\u4e00\u79cd\u4e13\u4e3a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u6570\u636e\u96c6\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002RAM-APL\u5229\u7528\u591a\u4e2aFMs\u7684\u4e92\u8865\u4f18\u52bf\u63d0\u5347\u5b50\u96c6\u9009\u62e9\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\uff08\u5305\u62ecOxford-IIIT Pet\u3001Food-101\u548cCaltech-UCSD Birds-200-2011\uff09\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.13771", "pdf": "https://arxiv.org/pdf/2506.13771", "abs": "https://arxiv.org/abs/2506.13771", "authors": ["Banseok Lee", "Dongkyu Kim", "Youngcheon You", "Youngmin Kim"], "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLittleBit\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u77e9\u9635\u5206\u89e3\u548c\u53cc\u7b26\u53f7-\u503c\u72ec\u7acb\u5206\u89e3\uff08Dual-SVID\uff09\u5b9e\u73b0\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\uff08\u59820.1 BPW\uff09\uff0c\u663e\u8457\u51cf\u5c11LLM\u5185\u5b58\u5360\u7528\uff08\u5982Llama2-13B\u964d\u81f30.9 GB\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u8865\u507f\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u90e8\u7f72\u9762\u4e34\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u4f4e\u4e8e1\u6bd4\u7279\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\u3002LittleBit\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5b9e\u73b0\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u9ad8\u6548\u6a21\u578b\u538b\u7f29\u3002", "method": "LittleBit\u91c7\u7528\u6f5c\u5728\u77e9\u9635\u5206\u89e3\u8868\u793a\u6743\u91cd\uff0c\u5e76\u8fdb\u884c\u4e8c\u503c\u5316\u5904\u7406\uff1b\u5f15\u5165\u591a\u5c3a\u5ea6\u8865\u507f\u673a\u5236\uff08\u884c\u3001\u5217\u548c\u6f5c\u5728\u7ef4\u5ea6\uff09\u4ee5\u5f25\u8865\u4fe1\u606f\u635f\u5931\uff1b\u63d0\u51faDual-SVID\u548c\u6b8b\u5dee\u8865\u507f\u6280\u672f\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLittleBit\u57280.1 BPW\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982Llama2-7B\u6027\u80fd\u8d85\u8fc70.7 BPW\u7684\u9886\u5148\u65b9\u6cd5\uff09\uff0c\u5185\u5b58\u51cf\u5c1131\u500d\uff0c\u5185\u6838\u7ea7\u6d4b\u8bd5\u663e\u793a\u6bd4FP16\u5feb5\u500d\u3002", "conclusion": "LittleBit\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72\u9ad8\u6548LLM\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5b9e\u73b0\u4e86\u5c3a\u5bf8\u4e0e\u6027\u80fd\u7684\u4f18\u5f02\u5e73\u8861\u3002", "paper_title_zh": "LittleBit\uff1a\u901a\u8fc7\u6f5c\u5728\u5206\u89e3\u5b9e\u73b0\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316", "abstract_zh": "\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e38\u9762\u4e34\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\u3002\u91cf\u5316\u662f\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u4f4e\u4e8e1\u6bd4\u7279\u7684\u91cf\u5316\u8303\u56f4\u5185\u6027\u80fd\u4e0b\u964d\u5c24\u4e3a\u4e25\u91cd\u3002\u672c\u6587\u63d0\u51faLittleBit\uff0c\u4e00\u79cd\u5b9e\u73b0\u6781\u7aefLLM\u538b\u7f29\u7684\u65b0\u65b9\u6cd5\uff0c\u76ee\u6807\u4e3a0.1\u6bd4\u7279\u6bcf\u6743\u91cd\uff08BPW\uff09\uff0c\u5185\u5b58\u51cf\u5c11\u8fd131\u500d\uff08\u4f8b\u5982Llama2-13B\u538b\u7f29\u81f30.9 GB\u4ee5\u4e0b\uff09\u3002LittleBit\u901a\u8fc7\u6f5c\u5728\u77e9\u9635\u5206\u89e3\u8868\u793a\u6743\u91cd\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u56e0\u5b50\u8fdb\u884c\u4e8c\u503c\u5316\u3002\u4e3a\u5f25\u8865\u6781\u7aef\u7cbe\u5ea6\u4e0b\u7684\u4fe1\u606f\u635f\u5931\uff0c\u5b83\u96c6\u6210\u4e86\u591a\u5c3a\u5ea6\u8865\u507f\u673a\u5236\uff0c\u5305\u62ec\u884c\u3001\u5217\u548c\u4e00\u4e2a\u989d\u5916\u7684\u6f5c\u5728\u7ef4\u5ea6\uff08\u5b66\u4e60\u6bcf\u79e9\u91cd\u8981\u6027\uff09\u3002\u4e24\u9879\u5173\u952e\u8d21\u732e\u786e\u4fdd\u4e86\u6709\u6548\u8bad\u7ec3\uff1a\u53cc\u7b26\u53f7-\u503c\u72ec\u7acb\u5206\u89e3\uff08Dual-SVID\uff09\u7528\u4e8e\u7a33\u5b9a\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u521d\u59cb\u5316\uff0c\u4ee5\u53ca\u96c6\u6210\u6b8b\u5dee\u8865\u507f\u4ee5\u51cf\u5c11\u8bef\u5dee\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86LittleBit\u5728\u4f4e\u4e8e1\u6bd4\u7279\u91cf\u5316\u4e2d\u7684\u4f18\u52bf\uff1a\u4f8b\u5982\uff0c\u5176\u5728Llama2-7B\u4e0a\u76840.1 BPW\u6027\u80fd\u8d85\u8d8a\u4e86\u9886\u5148\u65b9\u6cd5\u76840.7 BPW\u3002\u8fd9\u786e\u7acb\u4e86\u66f4\u4f18\u7684\u5c3a\u5bf8-\u6027\u80fd\u6743\u8861\uff0c\u5185\u6838\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5176\u6bd4FP16\u5feb5\u500d\u3002LittleBit\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5f3a\u5927LLM\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.14634", "pdf": "https://arxiv.org/pdf/2506.14634", "abs": "https://arxiv.org/abs/2506.14634", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Wei\u00df", "Jessika Daikeler"], "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fb7\u8bed\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u7f16\u7801\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540cLLMs\u548c\u63d0\u793a\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4ec5\u7ecf\u8fc7\u5fae\u8c03\u7684LLM\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u7528\u9014\u548c\u9650\u5236\u3002", "motivation": "\u968f\u7740LLMs\u7684\u53d1\u5c55\u548c\u666e\u53ca\uff0c\u5176\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u5206\u7c7b\u5f00\u653e\u5f0f\u56de\u590d\u7684\u6f5c\u529b\u5f15\u53d1\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u82f1\u8bed\u56de\u590d\u6216\u5355\u4e00LLM\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u8bed\u8a00\u548c\u590d\u6742\u4e3b\u9898\u7684\u9a8c\u8bc1\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4ee5\u5fb7\u8bed\u8c03\u67e5\u56de\u590d\u4e3a\u4f8b\uff0c\u8bc4\u4f30LLMs\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5fb7\u8bed\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u6570\u636e\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u5148\u8fdbLLMs\u548c\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u7f16\u7801\u8bc4\u4f30\u5176\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u4e0d\u540cLLMs\u7684\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4ec5\u5fae\u8c03\u540e\u7684LLM\u8fbe\u5230\u6ee1\u610f\u7684\u9884\u6d4b\u6c34\u5e73\u3002\u63d0\u793a\u65b9\u6cd5\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u6240\u7528LLM\uff0c\u4e14LLMs\u5728\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u4e0d\u5747\u5bfc\u81f4\u5206\u5e03\u5dee\u5f02\u3002", "conclusion": "LLMs\u5728\u5f00\u653e\u5f0f\u56de\u590d\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6743\u8861\u5176\u6027\u80fd\u5dee\u5f02\u548c\u5fae\u8c03\u9700\u6c42\u3002\u7814\u7a76\u4e3aLLMs\u5728\u8c03\u67e5\u7814\u7a76\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u5e76\u5f3a\u8c03\u4e86\u65b9\u6cd5\u9009\u62e9\u548c\u5b9e\u9645\u9650\u5236\u7684\u91cd\u8981\u6027\u3002", "paper_title_zh": "\u53ea\u662f\u4e00\u9879\u8c03\u67e5\u5417\uff1f\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5fb7\u8bed\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u4ee5\u7814\u7a76\u8c03\u67e5\u52a8\u673a", "abstract_zh": "\u6700\u8fd1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u548c\u666e\u53ca\u5f15\u53d1\u4e86\u5173\u4e8e\u5176\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u5e94\u7528\u7684\u8ba8\u8bba\uff0c\u5305\u62ec\u5206\u7c7b\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u3002\u7531\u4e8e\u5176\u8bed\u8a00\u80fd\u529b\uff0cLLMs\u53ef\u80fd\u6210\u4e3a\u8017\u65f6\u7684\u624b\u52a8\u7f16\u7801\u548c\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u82f1\u8bed\u56de\u590d\u6216\u975e\u590d\u6742\u4e3b\u9898\uff0c\u6216\u5355\u4e00LLM\uff0c\u5176\u7ed3\u8bba\u662f\u5426\u5177\u6709\u666e\u9002\u6027\u4ee5\u53ca\u5206\u7c7b\u8d28\u91cf\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u6bd4\u8f83\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u4ee5\u5fb7\u8bed\u8c03\u67e5\u53c2\u4e0e\u52a8\u673a\u6570\u636e\u4e3a\u4f8b\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540cLLMs\u5728\u5176\u4ed6\u8bed\u5883\u4e0b\u7f16\u7801\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u7684\u80fd\u529b\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u591a\u79cd\u5148\u8fdbLLMs\u548c\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u7f16\u7801\u8bc4\u4f30\u5176\u6027\u80fd\u3002\u4e0d\u540cLLMs\u7684\u603b\u4f53\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4ec5\u5fae\u8c03\u540e\u7684LLM\u8fbe\u5230\u6ee1\u610f\u7684\u9884\u6d4b\u6c34\u5e73\u3002\u63d0\u793a\u65b9\u6cd5\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u6240\u7528LLM\u3002\u6b64\u5916\uff0cLLMs\u5728\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u4e0d\u5747\u5bfc\u81f4\u672a\u5fae\u8c03\u65f6\u7684\u7c7b\u522b\u5206\u5e03\u5dee\u5f02\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5f00\u653e\u5f0f\u56de\u590d\u7f16\u7801\u65b9\u6cd5\u5b66\u7814\u7a76\u548c\u5b9e\u8d28\u6027\u5206\u6790\u7684\u610f\u4e49\uff0c\u4ee5\u53ca\u5bf9\u5904\u7406\u6216\u5206\u6790\u6b64\u7c7b\u6570\u636e\u7684\u5b9e\u8df5\u8005\u7684\u542f\u793a\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u5728LLMs\u65f6\u4ee3\u9009\u62e9\u81ea\u52a8\u5316\u65b9\u6cd5\u5206\u7c7b\u5f00\u653e\u5f0f\u56de\u590d\u65f6\u9700\u6743\u8861\u7684\u8bf8\u591a\u56e0\u7d20\u3002\u672c\u7814\u7a76\u4e3aLLMs\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u548c\u53ef\u9760\u5e94\u7528\u7684\u6761\u4ef6\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2506.14495", "pdf": "https://arxiv.org/pdf/2506.14495", "abs": "https://arxiv.org/abs/2506.14495", "authors": ["Yu Qi", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpeechRefer\u7684\u65b0\u578b3D\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u8bed\u97f3\u8f93\u5165\u56e0\u566a\u58f0\u548c\u6b67\u4e49\u5bfc\u81f4\u7684\u8f6c\u5f55\u9519\u8bef\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u97f3\u4e92\u8865\u6a21\u5757\u548c\u5bf9\u6bd4\u4e92\u8865\u6a21\u5757\u63d0\u5347\u73b0\u67093DVG\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u7684\u6587\u672c\u63d0\u793a\uff0c\u800c\u73b0\u5b9e\u4e2d\u7684\u8bed\u97f3\u8f93\u5165\u5e38\u56e0\u53e3\u97f3\u3001\u80cc\u666f\u566a\u58f0\u548c\u8bed\u901f\u95ee\u9898\u4ea7\u751f\u8f6c\u5f55\u9519\u8bef\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f3DVG\u7cfb\u7edf\u66f4\u9002\u5e94\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u566a\u58f0\u8bed\u97f3\u8f93\u5165\u3002", "method": "SpeechRefer\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) \u8bed\u97f3\u4e92\u8865\u6a21\u5757\uff0c\u901a\u8fc7\u6355\u6349\u8bed\u97f3\u4fe1\u53f7\u7684\u58f0\u5b66\u76f8\u4f3c\u6027\u751f\u6210\u8865\u5145\u63d0\u6848\u5206\u6570\uff1b2) \u5bf9\u6bd4\u4e92\u8865\u6a21\u5757\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u9519\u8bef\u6587\u672c\u7279\u5f81\u4e0e\u8bed\u97f3\u7279\u5f81\uff0c\u786e\u4fdd\u8f6c\u5f55\u9519\u8bef\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728SpeechRefer\u548cSpeechNr3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpeechRefer\u663e\u8457\u63d0\u5347\u4e86\u73b0\u67093DVG\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u566a\u58f0\u8bed\u97f3\u8f93\u5165\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "SpeechRefer\u901a\u8fc7\u7ed3\u5408\u8bed\u97f3\u4fe1\u53f7\u548c\u6587\u672c\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u8bed\u97f3\u8f93\u5165\u7684\u6311\u6218\uff0c\u4e3a\u66f4\u76f4\u89c2\u3001\u5b9e\u7528\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "paper_title_zh": "\u6211\u8bf4\u4f60\u627e\uff1a\u57fa\u4e8e\u566a\u58f0\u548c\u6b67\u4e49\u8bed\u97f3\u8f93\u5165\u7684\u9c81\u68d23D\u89c6\u89c9\u5b9a\u4f4d", "abstract_zh": "\u73b0\u6709\u76843D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u6587\u672c\u63d0\u793a\u6765\u5b9a\u4f4d3D\u573a\u666f\u4e2d\u7684\u7269\u4f53\u3002\u8bed\u97f3\u4f5c\u4e3a\u4e00\u79cd\u81ea\u7136\u76f4\u89c2\u7684\u6a21\u6001\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e2d\u7684\u8bed\u97f3\u8f93\u5165\u5e38\u56e0\u53e3\u97f3\u3001\u80cc\u666f\u566a\u58f0\u548c\u8bed\u901f\u95ee\u9898\u5bfc\u81f4\u8f6c\u5f55\u9519\u8bef\uff0c\u9650\u5236\u4e86\u73b0\u67093DVG\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\\textbf{SpeechRefer}\uff0c\u4e00\u79cd\u65b0\u578b3DVG\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u566a\u58f0\u548c\u6b67\u4e49\u8bed\u97f3\u8f6c\u5f55\u4e0b\u7684\u6027\u80fd\u3002SpeechRefer\u4e0e\u73b0\u67093DVG\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u5f15\u5165\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a1) \u8bed\u97f3\u4e92\u8865\u6a21\u5757\u901a\u8fc7\u6355\u6349\u8bed\u97f3\u4fe1\u53f7\u7684\u58f0\u5b66\u76f8\u4f3c\u6027\u751f\u6210\u8865\u5145\u63d0\u6848\u5206\u6570\uff0c\u51cf\u5c11\u5bf9\u6f5c\u5728\u9519\u8bef\u8f6c\u5f55\u7684\u4f9d\u8d56\uff1b2) \u5bf9\u6bd4\u4e92\u8865\u6a21\u5757\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u9519\u8bef\u6587\u672c\u7279\u5f81\u4e0e\u8bed\u97f3\u7279\u5f81\uff0c\u786e\u4fdd\u5728\u8f6c\u5f55\u9519\u8bef\u4e3b\u5bfc\u65f6\u7684\u9c81\u68d2\u6027\u3002\u5728SpeechRefer\u548cSpeechNr3D\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSpeechRefer\u663e\u8457\u63d0\u5347\u4e86\u73b0\u67093DVG\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u5176\u5728\u566a\u58f0\u8bed\u97f3\u8f93\u5165\u4e0e\u53ef\u97603DVG\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u76f4\u89c2\u3001\u5b9e\u7528\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2506.13772", "pdf": "https://arxiv.org/pdf/2506.13772", "abs": "https://arxiv.org/abs/2506.13772", "authors": ["Zhenyan Lu", "Daliang Xu", "Dongqi Cai", "Zexi Li", "Wei Liu", "Fangming Liu", "Shangguang Wang", "Mengwei Xu"], "title": "MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\\times$ less memory, 14.7 $\\times$ less energy and 3.6$\\times$ less latency compared to previous knowledge editing methods.", "AI": {"tldr": "MobiEdit\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u79fb\u52a8\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u5728\u5546\u7528\u79fb\u52a8\u8bbe\u5907\u4e0a\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4e2a\u6027\u5316\u77e5\u8bc6\u7f16\u8f91\uff0c\u901a\u8fc7\u91cf\u5316\u524d\u5411\u68af\u5ea6\u4f30\u8ba1\u66ff\u4ee3\u8d44\u6e90\u5bc6\u96c6\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u3001\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u4e2a\u6027\u5316\u6216\u672a\u89c1\u8fc7\u7684\u67e5\u8be2\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5bfc\u81f4\u9519\u8bef\u6216\u8fc7\u65f6\u7684\u56de\u7b54\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u56e0\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u7684\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u800c\u65e0\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002", "method": "MobiEdit\u91c7\u7528\u91cf\u5316\u524d\u5411\u68af\u5ea6\u4f30\u8ba1\u66ff\u4ee3\u5168\u7cbe\u5ea6\u53cd\u5411\u4f20\u64ad\uff0c\u4f7f\u5176\u517c\u5bb9\u79fb\u52a8\u795e\u7ecf\u5904\u7406\u5355\u5143\uff08NPU\uff09\u3002\u4f18\u5316\u5305\u62ec\u81ea\u9002\u5e94\u6210\u529f\u7ec8\u6b62\u7684\u65e9\u671f\u505c\u6b62\u673a\u5236\u548c\u8de8\u6b65\u9aa4\u8ba1\u7b97\u91cd\u7528\u7684\u524d\u7f00\u7f13\u5b58\u3002", "result": "\u5728\u5546\u7528\u79fb\u52a8\u8bbe\u5907\u4e0a\uff0cMobiEdit\u5b9e\u73b0\u4e86\u5bf93B\u53c2\u6570\u6a21\u578b\uff08Qwen2.5-3B-Instruct\uff09\u7684\u5b9e\u65f6\u7f16\u8f91\uff0c\u5185\u5b58\u51cf\u5c117.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e14.7\u500d\uff0c\u5ef6\u8fdf\u51cf\u5c113.6\u500d\u3002", "conclusion": "MobiEdit\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LLM\u4e2a\u6027\u5316\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "paper_title_zh": "MobiEdit\uff1a\u9762\u5411\u4e2a\u6027\u5316\u8bbe\u5907\u7aef\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d44\u6e90\u9ad8\u6548\u77e5\u8bc6\u7f16\u8f91", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u90e8\u7f72\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u4ee5\u652f\u6301\u667a\u80fd\u52a9\u624b\u7b49\u5173\u952e\u5e94\u7528\u3002\u9884\u8bad\u7ec3\u4e8e\u901a\u7528\u8bed\u6599\u7684LLM\u5728\u5904\u7406\u4e2a\u6027\u5316\u6216\u672a\u89c1\u67e5\u8be2\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5bfc\u81f4\u9519\u8bef\u6216\u8fc7\u65f6\u7684\u56de\u7b54\u3002\u77e5\u8bc6\u7f16\u8f91\u901a\u8fc7\u8bc6\u522b\u5e76\u8c03\u6574\u6a21\u578b\u6743\u91cd\u7684\u5173\u952e\u90e8\u5206\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u901a\u7528\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u56e0\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u7684\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u800c\u65e0\u6cd5\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002\u6211\u4eec\u63d0\u51fa\u4e86MobiEdit\uff0c\u9996\u4e2a\u79fb\u52a8\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u53ef\u5728\u5546\u7528\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u5b9e\u73b0LLM\u4e2a\u6027\u5316\u3002MobiEdit\u7528\u91cf\u5316\u524d\u5411\u68af\u5ea6\u4f30\u8ba1\u66ff\u4ee3\u5168\u7cbe\u5ea6\u53cd\u5411\u4f20\u64ad\uff0c\u4ece\u800c\u517c\u5bb9\u80fd\u6548\u4f18\u5316\u7684\u79fb\u52a8\u795e\u7ecf\u5904\u7406\u5355\u5143\uff08NPU\uff09\u3002\u4e3a\u8fdb\u4e00\u6b65\u63d0\u5347\u68af\u5ea6\u4f30\u8ba1\u6548\u7387\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u9879\u4f18\u5316\uff1a\u81ea\u9002\u5e94\u6210\u529f\u7ec8\u6b62\u7684\u65e9\u671f\u505c\u6b62\u673a\u5236\u548c\u8de8\u6b65\u9aa4\u8ba1\u7b97\u91cd\u7528\u7684\u524d\u7f00\u7f13\u5b58\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5546\u7528\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u5bf93B\u53c2\u6570\u6a21\u578b\uff08Qwen2.5-3B-Instruct\uff09\u7684\u5b9e\u65f6\u7f16\u8f91\uff0c\u5185\u5b58\u51cf\u5c117.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e14.7\u500d\uff0c\u5ef6\u8fdf\u51cf\u5c113.6\u500d\u3002"}}
{"id": "2506.14641", "pdf": "https://arxiv.org/pdf/2506.14641", "abs": "https://arxiv.org/abs/2506.14641", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \\texttt{Qwen2.5-Max} and \\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u8fd1\u671f\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982Qwen2.5\u7cfb\u5217\uff09\uff0c\u4f20\u7edf\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u793a\u4f8b\u5e76\u4e0d\u80fd\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u53cd\u800c\u96f6\u6837\u672cCoT\u8868\u73b0\u66f4\u4f18\u3002\u589e\u5f3a\u7684CoT\u793a\u4f8b\u540c\u6837\u65e0\u6548\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5ffd\u7565\u793a\u4f8b\u800c\u4e13\u6ce8\u4e8e\u6307\u4ee4\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u4e0d\u65ad\u63d0\u5347\uff0c\u4f20\u7edf\u601d\u7ef4\u94fe\uff08CoT\uff09\u793a\u4f8b\u662f\u5426\u4ecd\u80fd\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1CoT\u793a\u4f8b\u5bf9\u8fd1\u671f\u5f3a\u5927\u6a21\u578b\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4f20\u7edfCoT\u793a\u4f8b\u548c\u96f6\u6837\u672cCoT\u5728\u8fd1\u671f\u5f3a\u5927\u6a21\u578b\uff08\u5982Qwen2.5\u7cfb\u5217\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u589e\u5f3a\u7248CoT\u793a\u4f8b\uff08\u57fa\u4e8e\u9ad8\u7ea7\u6a21\u578b\u7b54\u6848\u6784\u5efa\uff09\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f20\u7edf\u548c\u589e\u5f3a\u7684CoT\u793a\u4f8b\u5747\u672a\u80fd\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u5ffd\u7565\u793a\u4f8b\u800c\u5173\u6ce8\u6307\u4ee4\u3002", "conclusion": "\u5f53\u524dICL+CoT\u6846\u67b6\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6ICL\u8303\u5f0f\u53ca\u793a\u4f8b\u5b9a\u4e49\u3002", "paper_title_zh": "\u91cd\u65b0\u5ba1\u89c6\u601d\u7ef4\u94fe\u63d0\u793a\uff1a\u96f6\u6837\u672c\u53ef\u80fd\u4f18\u4e8e\u5c11\u6837\u672c", "abstract_zh": "\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u91cd\u8981\u6d8c\u73b0\u80fd\u529b\uff0c\u8fd1\u671f\u7814\u7a76\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u793a\u4f8b\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u3002\u7136\u800c\uff0c\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u6301\u7eed\u63d0\u5347\uff0cCoT\u793a\u4f8b\u662f\u5426\u4ecd\u5bf9\u8fd1\u671f\u5f3a\u5927\u6a21\u578b\u6709\u76ca\u5c1a\u4e0d\u660e\u786e\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u5bf9\u4e8eQwen2.5\u7cfb\u5217\u7b49\u8fd1\u671f\u5f3a\u5927\u6a21\u578b\uff0c\u4f20\u7edfCoT\u793a\u4f8b\u5e76\u672a\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u53cd\u800c\u96f6\u6837\u672cCoT\u8868\u73b0\u66f4\u4f18\uff0c\u5176\u4e3b\u8981\u4f5c\u7528\u662f\u4f7f\u8f93\u51fa\u683c\u5f0f\u7b26\u5408\u4eba\u7c7b\u9884\u671f\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u9ad8\u7ea7\u6a21\u578b\uff08\u5982Qwen2.5-Max\u548cDeepSeek-R1\uff09\u7b54\u6848\u6784\u5efa\u7684\u589e\u5f3aCoT\u793a\u4f8b\u540c\u6837\u65e0\u6548\u3002\u6df1\u5165\u5206\u6790\u663e\u793a\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5ffd\u7565\u793a\u4f8b\u800c\u4e13\u6ce8\u4e8e\u6307\u4ee4\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u65e0\u663e\u8457\u63d0\u5347\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dICL+CoT\u6846\u67b6\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u547c\u5401\u91cd\u65b0\u5ba1\u89c6ICL\u8303\u5f0f\u53ca\u793a\u4f8b\u5b9a\u4e49\u3002"}}
{"id": "2506.14511", "pdf": "https://arxiv.org/pdf/2506.14511", "abs": "https://arxiv.org/abs/2506.14511", "authors": ["Zhiwen Shao", "Yifan Cheng", "Feiran Li", "Yong Zhou", "Xuequan Lu", "Yuan Xie", "Lizhuang Ma"], "title": "MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution", "categories": ["cs.CV"], "comment": "This paper has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u3001\u56fe\u5377\u79ef\u548c\u666e\u901a\u5377\u79ef\u7684\u7aef\u5230\u7aef\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6MOL\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u5fae\u8868\u60c5\u8bc6\u522b\u3001\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\u56e0\u52a8\u4f5c\u77ed\u6682\u4e14\u7ec6\u5fae\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u6216\u5173\u952e\u5e27\uff0c\u4e14\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u548c\u4f4e\u591a\u6837\u6027\u6570\u636e\u96c6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u76f4\u63a5\u4ece\u672a\u6807\u8bb0\u7684\u539f\u59cb\u5e27\u5e8f\u5217\u4e2d\u63d0\u53d6\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\uff0c\u4ee5\u63d0\u5347MER\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684F5C\u6a21\u5757\uff0c\u7ed3\u5408\u5168\u8fde\u63a5\u5377\u79ef\u548c\u901a\u9053\u5bf9\u5e94\u5377\u79ef\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u5e27\u5e8f\u5217\u4e2d\u63d0\u53d6\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u3002\u901a\u8fc7Transformer\u98ce\u683c\u7684\u5168\u8fde\u63a5\u5377\u79ef\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\u5e76\u4fdd\u6301\u5168\u5c40\u611f\u53d7\u91ce\uff0c\u56fe\u98ce\u683c\u7684\u901a\u9053\u5bf9\u5e94\u5377\u79ef\u5efa\u6a21\u7279\u5f81\u6a21\u5f0f\u95f4\u7684\u76f8\u5173\u6027\u3002\u540c\u65f6\uff0c\u8054\u5408\u8bad\u7ec3MER\u3001\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\uff0c\u5171\u4eab\u7279\u5f81\u4ee5\u589e\u5f3a\u5fae\u8868\u60c5\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMOL\u6846\u67b6\u5728CASME II\u3001SAMM\u548cSMIC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709MER\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6355\u6349\u4e0e\u5fae\u8868\u60c5\u76f8\u5173\u7684\u5c40\u90e8\u808c\u8089\u52a8\u4f5c\u3002", "conclusion": "MOL\u6846\u67b6\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u591a\u4efb\u52a1\u548c\u7ed3\u5408\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e3a\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "MOL\uff1a\u57fa\u4e8eTransformer-\u56fe\u98ce\u683c\u5377\u79ef\u7684\u5fae\u8868\u60c5\u3001\u5149\u6d41\u548c\u5173\u952e\u70b9\u8054\u5408\u4f30\u8ba1", "abstract_zh": "\u9762\u90e8\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\u56e0\u5fae\u8868\u60c5\u52a8\u4f5c\u77ed\u6682\u4e14\u7ec6\u5fae\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u3001\u5173\u952e\u5e27\uff08\u5982\u8d77\u59cb\u5e27\u3001\u5cf0\u503c\u5e27\u548c\u7ed3\u675f\u5e27\uff09\u6216\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u548c\u4f4e\u591a\u6837\u6027\u6570\u636e\u96c6\u7684\u6df1\u5ea6\u7f51\u7edc\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5fae\u52a8\u4f5c\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86Transformer\u3001\u56fe\u5377\u79ef\u548c\u666e\u901a\u5377\u79ef\u7684\u4f18\u52bf\u3002\u7279\u522b\u5730\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684F5C\u6a21\u5757\uff0c\u7531\u5168\u8fde\u63a5\u5377\u79ef\u548c\u901a\u9053\u5bf9\u5e94\u5377\u79ef\u7ec4\u6210\uff0c\u53ef\u76f4\u63a5\u4ece\u539f\u59cb\u5e27\u5e8f\u5217\u4e2d\u63d0\u53d6\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\uff0c\u65e0\u9700\u5173\u952e\u5e27\u7684\u5148\u9a8c\u77e5\u8bc6\u3002Transformer\u98ce\u683c\u7684\u5168\u8fde\u63a5\u5377\u79ef\u7528\u4e8e\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\u5e76\u4fdd\u6301\u5168\u5c40\u611f\u53d7\u91ce\uff0c\u56fe\u98ce\u683c\u7684\u901a\u9053\u5bf9\u5e94\u5377\u79ef\u7528\u4e8e\u5efa\u6a21\u7279\u5f81\u6a21\u5f0f\u95f4\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5171\u4eab\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\uff0c\u8054\u5408\u8bad\u7ec3MER\u3001\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u3002\u540e\u4e24\u9879\u4efb\u52a1\u6709\u52a9\u4e8e\u6355\u6349\u9762\u90e8\u7ec6\u5fae\u52a8\u4f5c\u4fe1\u606f\uff0c\u4ece\u800c\u7f13\u89e3\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u5f71\u54cd\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\uff08i\uff09\u5728CASME II\u3001SAMM\u548cSMIC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709MER\u65b9\u6cd5\uff0c\uff08ii\uff09\u5728\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\uff08iii\uff09\u80fd\u591f\u6355\u6349\u4e0e\u5fae\u8868\u60c5\u76f8\u5173\u7684\u5c40\u90e8\u533a\u57df\u7684\u9762\u90e8\u7ec6\u5fae\u808c\u8089\u52a8\u4f5c\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/CYF-cuber/MOL\u3002"}}
{"id": "2506.13777", "pdf": "https://arxiv.org/pdf/2506.13777", "abs": "https://arxiv.org/abs/2506.13777", "authors": ["En Xu", "Huandong Wang", "Yunke Zhang", "Sibo Li", "Yinzhou Tang", "Zhilun Zhou", "Yuming Lin", "Yuan Yuan", "Xiaochen Fan", "Jingtao Ding", "Yong Li"], "title": "A Survey of Physics-Informed AI for Complex Urban Systems", "categories": ["physics.soc-ph", "cs.AI", "cs.CY"], "comment": null, "summary": "Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7269\u7406\u4fe1\u606f\u4eba\u5de5\u667a\u80fd\u5728\u590d\u6742\u57ce\u5e02\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u516b\u4e2a\u57ce\u5e02\u9886\u57df\u4e2d\u7684\u5177\u4f53\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u57ce\u5e02\u7cfb\u7edf\u662f\u590d\u6742\u7cfb\u7edf\u7684\u5178\u578b\u4ee3\u8868\uff0c\u5c06\u7269\u7406\u6a21\u578b\u4e0e\u4eba\u5de5\u667a\u80fd\u7ed3\u5408\u53ef\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u68b3\u7406\u7269\u7406\u4fe1\u606fAI\u65b9\u6cd5\u5728\u57ce\u5e02\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u7269\u7406\u96c6\u6210AI\u3001\u7269\u7406-AI\u6df7\u5408\u96c6\u6210\u548cAI\u96c6\u6210\u7269\u7406\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u4e03\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u80fd\u6e90\u3001\u73af\u5883\u3001\u7ecf\u6d4e\u7b49\u516b\u4e2a\u57ce\u5e02\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u7269\u7406\u4fe1\u606fAI\u65b9\u6cd5\u5982\u4f55\u7ed3\u5408\u7269\u7406\u89c4\u5f8b\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u89e3\u51b3\u57ce\u5e02\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u901a\u8fc7\u5206\u7c7b\u548c\u5e94\u7528\u5206\u6790\uff0c\u660e\u786e\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u7269\u7406\u4fe1\u606fAI\u5728\u57ce\u5e02\u7cfb\u7edf\u4e2d\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6307\u51fa\u4e86\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u57ce\u5e02\u7cfb\u7edf\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u7269\u7406\u4fe1\u606f\u4eba\u5de5\u667a\u80fd\u5728\u590d\u6742\u57ce\u5e02\u7cfb\u7edf\u4e2d\u7684\u7efc\u8ff0", "abstract_zh": "\u57ce\u5e02\u7cfb\u7edf\u662f\u590d\u6742\u7cfb\u7edf\u7684\u5178\u578b\u4ee3\u8868\uff0c\u5c06\u57fa\u4e8e\u7269\u7406\u7684\u5efa\u6a21\u4e0e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7ed3\u5408\uff0c\u4e3a\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8303\u5f0f\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0cAI\u64c5\u957f\u6355\u6349\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u800c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\u5219\u786e\u4fdd\u4e0e\u73b0\u5b9e\u4e16\u754c\u89c4\u5f8b\u7684\u4e00\u81f4\u6027\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002\u672c\u6587\u5bf9\u57ce\u5e02\u5e94\u7528\u4e2d\u7684\u7269\u7406\u4fe1\u606fAI\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\u3002\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\u2014\u2014\u7269\u7406\u96c6\u6210AI\u3001\u7269\u7406-AI\u6df7\u5408\u96c6\u6210\u548cAI\u96c6\u6210\u7269\u7406\uff0c\u5e76\u8fdb\u4e00\u6b65\u8be6\u8ff0\u4e86\u4e03\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\u3002\u8fd9\u4e00\u5206\u7c7b\u660e\u786e\u4e86\u7269\u7406\u4e0eAI\u7ed3\u5408\u7684\u4e0d\u540c\u7a0b\u5ea6\u548c\u65b9\u5411\uff0c\u6307\u5bfc\u6839\u636e\u5e94\u7528\u9700\u6c42\u548c\u6570\u636e\u53ef\u7528\u6027\u9009\u62e9\u548c\u5f00\u53d1\u5408\u9002\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u80fd\u6e90\u3001\u73af\u5883\u3001\u7ecf\u6d4e\u3001\u4ea4\u901a\u3001\u4fe1\u606f\u3001\u516c\u5171\u670d\u52a1\u3001\u5e94\u6025\u7ba1\u7406\u548c\u57ce\u5e02\u6574\u4f53\u7cfb\u7edf\u7b49\u516b\u4e2a\u5173\u952e\u57ce\u5e02\u9886\u57df\u7684\u5e94\u7528\u3002\u5206\u6790\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u5229\u7528\u7269\u7406\u89c4\u5f8b\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u89e3\u51b3\u57ce\u5e02\u6311\u6218\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u901a\u8fc7\u7efc\u5408\u73b0\u6709\u65b9\u6cd5\u53ca\u5176\u57ce\u5e02\u5e94\u7528\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u5173\u952e\u7a7a\u767d\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u57ce\u5e02\u7cfb\u7edf\u5efa\u6a21\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.14645", "pdf": "https://arxiv.org/pdf/2506.14645", "abs": "https://arxiv.org/abs/2506.14645", "authors": [". Pazzaglia", "V. Vendetti", "L. D. Comencini", "F. Deriu", "V. Modugno"], "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u4e14\u5177\u6709\u717d\u52a8\u6027\u7684\u653f\u6cbb\u8bc4\u8bba\uff0c\u51e0\u4e4e\u65e0\u6cd5\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u8bc4\u8bba\u533a\u5206\u3002\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8eAI\u5728\u653f\u6cbb\u8bdd\u8bed\u4e2d\u6f5c\u5728\u6ee5\u7528\u98ce\u9669\u7684\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65e5\u76ca\u590d\u6742\uff0c\u4eba\u4eec\u62c5\u5fc3\u5176\u53ef\u80fd\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5e26\u6709\u504f\u89c1\u7684\u5185\u5bb9\u52a0\u5267\u610f\u8bc6\u5f62\u6001\u6781\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5fae\u8c03\u540e\u7684LLM\u662f\u5426\u80fd\u591f\u590d\u5236\u5e76\u653e\u5927\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u6781\u5316\u8a00\u8bba\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4eceReddit\u63d0\u53d6\u7684\u653f\u6cbb\u5316\u8ba8\u8bba\u6570\u636e\u96c6\uff0c\u5bf9\u5f00\u6e90LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u4e0e\u610f\u8bc6\u5f62\u6001\u4e00\u81f4\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u54cd\u5e94\u3002\u901a\u8fc7\u8bed\u8a00\u5206\u6790\u3001\u60c5\u611f\u8bc4\u5206\u548c\u4eba\u5de5\u6807\u6ce8\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u53ef\u4fe1\u5ea6\u548c\u4e0e\u539f\u8bdd\u8bed\u7684\u4fee\u8f9e\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u515a\u6d3e\u6570\u636e\u8bad\u7ec3\u7684LLM\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u4e14\u5177\u6709\u717d\u52a8\u6027\u7684\u8bc4\u8bba\uff0c\u751a\u81f3\u96be\u4ee5\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u8bc4\u8bba\u533a\u5206\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI\u5728\u653f\u6cbb\u8bdd\u8bed\u4e2d\u7684\u6f5c\u5728\u6ee5\u7528\u98ce\u9669\uff0c\u547c\u5401\u52a0\u5f3aAI\u6cbb\u7406\u3001\u5e73\u53f0\u76d1\u7ba1\uff0c\u5e76\u5f00\u53d1\u68c0\u6d4b\u5de5\u5177\u4ee5\u5e94\u5bf9\u5fae\u8c03\u98ce\u9669\u3002", "paper_title_zh": "\u5728\u653f\u6cbb\u8bdd\u8bed\u4e2d\u901a\u8fc7\u56fe\u7075\u6d4b\u8bd5\uff1a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u6a21\u4eff\u6781\u5316\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65e5\u76ca\u590d\u6742\u5f15\u53d1\u4e86\u5bf9\u5176\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5177\u6709\u8bf4\u670d\u529b\u548c\u504f\u89c1\u5185\u5bb9\u52a0\u5267\u610f\u8bc6\u5f62\u6001\u6781\u5316\u7684\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5fae\u8c03\u540e\u7684LLM\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u591f\u590d\u5236\u5e76\u653e\u5927\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u6781\u5316\u8a00\u8bba\u3002\u901a\u8fc7\u4f7f\u7528\u4eceReddit\u63d0\u53d6\u7684\u653f\u6cbb\u5316\u8ba8\u8bba\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5bf9\u5f00\u6e90LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u4e0e\u610f\u8bc6\u5f62\u6001\u4e00\u81f4\u7684\u54cd\u5e94\u3002\u901a\u8fc7\u8bed\u8a00\u5206\u6790\u3001\u60c5\u611f\u8bc4\u5206\u548c\u4eba\u5de5\u6807\u6ce8\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u53ef\u4fe1\u5ea6\u548c\u4e0e\u539f\u8bdd\u8bed\u7684\u4fee\u8f9e\u4e00\u81f4\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u515a\u6d3e\u6570\u636e\u8bad\u7ec3\u7684LLM\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u4e14\u5177\u6709\u717d\u52a8\u6027\u7684\u8bc4\u8bba\uff0c\u51e0\u4e4e\u65e0\u6cd5\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u8bc4\u8bba\u533a\u5206\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f15\u53d1\u4e86\u5173\u4e8eAI\u5728\u653f\u6cbb\u8bdd\u8bed\u3001\u865a\u5047\u4fe1\u606f\u548c\u64cd\u7eb5\u6d3b\u52a8\u4e2d\u4f7f\u7528\u7684\u91cd\u5927\u4f26\u7406\u95ee\u9898\u3002\u8bba\u6587\u6700\u540e\u8ba8\u8bba\u4e86AI\u6cbb\u7406\u3001\u5e73\u53f0\u76d1\u7ba1\u4ee5\u53ca\u5f00\u53d1\u68c0\u6d4b\u5de5\u5177\u4ee5\u51cf\u8f7b\u5bf9\u6297\u6027\u5fae\u8c03\u98ce\u9669\u7684\u66f4\u5e7f\u6cdb\u5f71\u54cd\u3002"}}
{"id": "2506.14512", "pdf": "https://arxiv.org/pdf/2506.14512", "abs": "https://arxiv.org/abs/2506.14512", "authors": ["Zijian Song", "Xiaoxin Lin", "Qiuming Huang", "Guangrun Wang", "Liang Lin"], "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks", "categories": ["cs.CV"], "comment": "16 pages, 9 figures", "summary": "Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.", "AI": {"tldr": "SIRI-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7a7a\u95f4\u667a\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u6311\u6218\u5176\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709VLMs\u5728\u6b64\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u7a7a\u95f4\u63a8\u7406\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\uff08\u5982\u6570\u5b66\u548c\u7f16\u7a0b\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u7684\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u4e0d\u8db3\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86SIRI-Bench\uff0c\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u8bc4\u4f30VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86SIRI-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8fd11K\u4e2a\u89c6\u9891-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u6bcf\u4e2a\u95ee\u9898\u5d4c\u5165\u771f\u5b9e\u76843D\u573a\u666f\u5e76\u901a\u8fc7\u89c6\u9891\u5448\u73b0\u3002\u4e3a\u786e\u4fdd\u95ee\u9898\u89e3\u51b3\u9700\u8981\u7a7a\u95f4\u7406\u89e3\u548c\u9ad8\u7ea7\u63a8\u7406\uff0c\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u4e13\u95e8\u76843D\u573a\u666f\u548c\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u81ea\u52a8\u573a\u666f\u751f\u6210\u5f15\u64ce\uff0c\u5229\u7528\u591a\u4e2a\u4e13\u7528LLM\u4ee3\u7406\u4ece\u62bd\u8c61\u6570\u5b66\u95ee\u9898\u751f\u6210\u771f\u5b9e\u76843D\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728SIRI-Bench\u4e0a\u8868\u73b0\u663e\u8457\u4e0d\u4f73\uff0c\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u7684\u6311\u6218\u6027\u3002", "conclusion": "SIRI-Bench\u4e3a\u8bc4\u4f30VLMs\u7684\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u8005\u5e0c\u671b\u8be5\u7814\u7a76\u80fd\u63a8\u52a8VLMs\u5728\u89c6\u89c9\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u8fdb\u6b65\u3002", "paper_title_zh": "SIRI-Bench\uff1a\u901a\u8fc7\u590d\u6742\u63a8\u7406\u4efb\u52a1\u6311\u6218\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u5c24\u5176\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u9886\u57df\u8868\u73b0\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5c3d\u7ba1\u7a7a\u95f4\u667a\u80fd\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4e2d\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SIRI-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u8bc4\u4f30VLMs\u7a7a\u95f4\u667a\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002SIRI-Bench\u5305\u542b\u8fd11K\u4e2a\u89c6\u9891-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u6bcf\u4e2a\u95ee\u9898\u5d4c\u5165\u771f\u5b9e\u76843D\u573a\u666f\u5e76\u901a\u8fc7\u89c6\u9891\u5448\u73b0\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u95ee\u9898\u548c\u5bf9\u5e94\u76843D\u573a\u666f\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u786e\u4fdd\u89e3\u51b3\u95ee\u9898\u65e2\u9700\u8981\u7a7a\u95f4\u7406\u89e3\u4ee5\u63d0\u53d6\u4fe1\u606f\uff0c\u53c8\u9700\u8981\u9ad8\u7ea7\u63a8\u7406\u4ee5\u63a8\u5bfc\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u4f7f\u5176\u6210\u4e3a\u8bc4\u4f30VLMs\u7684\u6311\u6218\u6027\u57fa\u51c6\u3002\u4e3a\u652f\u6301\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u81ea\u52a8\u573a\u666f\u751f\u6210\u5f15\u64ce\u3002\u8be5\u5f15\u64ce\u5229\u7528\u591a\u4e2a\u4e13\u7528LLM\u4ee3\u7406\uff0c\u80fd\u591f\u4ece\u62bd\u8c61\u6570\u5b66\u95ee\u9898\u751f\u6210\u771f\u5b9e\u76843D\u573a\u666f\uff0c\u786e\u4fdd\u4e0e\u539f\u59cb\u63cf\u8ff0\u7684\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728SIRI-Bench\u4e0a\u8868\u73b0\u663e\u8457\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u7a7a\u95f4\u63a8\u7406\u7684\u6311\u6218\u6027\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u7814\u7a76\u80fd\u5f15\u8d77\u7814\u7a76\u8005\u5bf9\u7a7a\u95f4\u63a8\u7406\u7684\u5173\u6ce8\uff0c\u5e76\u63a8\u52a8VLMs\u5728\u89c6\u89c9\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.13778", "pdf": "https://arxiv.org/pdf/2506.13778", "abs": "https://arxiv.org/abs/2506.13778", "authors": ["Anvi Alex Eponon", "Moein Shahiki-Tash", "Ildar Batyrshin", "Christian E. Maldonado-Sifuentes", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.\n  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce \"paper-cards\", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.\n  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.\n  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u751f\u6210\u7684\u77e5\u8bc6\u7f16\u7801\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u901a\u8fc7\u751f\u6210\u8986\u76d6\u8bcd\u6c47\u548c\u8bed\u4e49\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u8bed\u6cd5\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u8df3\u548c\u591a\u8df3\u68c0\u7d22\u4efb\u52a1\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u5fae\u8c03\u6216\u5206\u5757\u5904\u7406\uff0c\u6548\u7387\u8f83\u4f4e\u4e14\u5b58\u50a8\u9700\u6c42\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u9898\u751f\u6210\u548c\u91cd\u6392\u5e8f\u6280\u672f\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u964d\u4f4e\u5ef6\u8fdf\u548c\u5b58\u50a8\u6210\u672c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u751f\u6210\u8986\u76d6\u6587\u672c\u8bcd\u6c47\u548c\u8bed\u4e49\u7a7a\u95f4\u7684\u95ee\u9898\u4f5c\u4e3a\u68c0\u7d22\u7ebf\u7d22\uff0c\u5e76\u7ed3\u5408\u81ea\u5b9a\u4e49\u8bed\u6cd5\u91cd\u6392\u5e8f\u6280\u672f\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u201cpaper-cards\u201d\uff08\u7b80\u6d01\u7684\u8bba\u6587\u6458\u8981\uff09\u4ee5\u589e\u5f3aBM25\u68c0\u7d22\u6548\u679c\u3002", "result": "\u5728\u5355\u8df3\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cRecall@3\u8fbe\u52300.84\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u5757\u65b9\u6cd560%\u3002\u5728\u591a\u8df3\u4efb\u52a1\u4e2d\uff0cF1\u5206\u6570\u4e3a0.52\uff0c\u8d85\u8fc7\u5206\u5757\u548c\u5fae\u8c03\u57fa\u7ebf\u3002\u5b58\u50a8\u9700\u6c42\u964d\u4f4e80%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u95ee\u9898\u751f\u6210\u5b9e\u73b0\u77e5\u8bc6\u538b\u7f29\uff1a\u65e0\u9700\u5fae\u8c03\u7684\u591a\u8df3\u6587\u6863\u68c0\u7d22\u589e\u5f3a", "abstract_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u751f\u6210\u7684\u77e5\u8bc6\u7f16\u7801\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u4f20\u7edf\u5206\u5757\u5373\u53ef\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u901a\u8fc7\u751f\u6210\u8986\u76d6\u8bcd\u6c47\u548c\u8bed\u4e49\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u8bed\u6cd5\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u679c\u3002\u5728109\u7bc7\u79d1\u5b66\u8bba\u6587\u7684\u5355\u8df3\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cRecall@3\u8fbe\u52300.84\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u5757\u65b9\u6cd560%\u3002\u6b64\u5916\uff0c\u5f15\u5165\u7684\u201cpaper-cards\u201d\uff08\u7b80\u6d01\u7684\u8bba\u6587\u6458\u8981\uff09\u589e\u5f3a\u4e86BM25\u68c0\u7d22\uff0c\u5c06\u7b80\u5316\u6280\u672f\u67e5\u8be2\u7684MRR@3\u4ece0.56\u63d0\u5347\u81f30.85\u3002\u5728\u591a\u8df3\u4efb\u52a1\u4e2d\uff0c\u91cd\u6392\u5e8f\u65b9\u6cd5\u5728LongBench 2WikiMultihopQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.52\u7684F1\u5206\u6570\uff08\u4f7f\u7528LLaMA2-Chat-7B\uff09\uff0c\u8d85\u8fc7\u5206\u5757\u548c\u5fae\u8c03\u57fa\u7ebf\uff08\u5206\u522b\u4e3a0.328\u548c0.412\uff09\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5fae\u8c03\uff0c\u964d\u4f4e\u4e86\u68c0\u7d22\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u95ee\u9898\u9a71\u52a8\u77e5\u8bc6\u8bbf\u95ee\uff0c\u5e76\u5c06\u5411\u91cf\u5b58\u50a8\u9700\u6c42\u51cf\u5c1180%\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684RAG\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.14646", "pdf": "https://arxiv.org/pdf/2506.14646", "abs": "https://arxiv.org/abs/2506.14646", "authors": ["Hengyuan Zhang", "Xinrong Chen", "Yingmin Qiu", "Xiao Liang", "Ziyue Li", "Guanyu Wang", "Weiping Li", "Tong Mo", "Wenyue Li", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors", "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.", "AI": {"tldr": "GuiLoMo\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u548c\u5f15\u5bfc\u9009\u62e9\u5411\u91cf\u7684\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u6570\u91cf\u4e0e\u79e9\u5206\u914d\u7b56\u7565\uff0c\u89e3\u51b3\u4e86LoRA-MoE\u4e2d\u4e13\u5bb6\u6570\u91cf\u4e0e\u79e9\u5206\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "LoRA-MoE\u7ed3\u5408\u4e86LoRA\u548cMoE\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u4e13\u5bb6\u6570\u91cf\u5206\u914d\u548c\u79e9\u5206\u914d\u4e0a\u5b58\u5728\u4e0d\u8db3\uff1a1) \u4e0b\u6e38\u4efb\u52a1\u5bf9\u4e13\u5bb6\u6570\u91cf\u5206\u914d\u7684\u5f71\u54cd\u672a\u88ab\u5145\u5206\u8003\u8651\uff1b2) \u6240\u6709LoRA\u4e13\u5bb6\u91c7\u7528\u7edf\u4e00\u79e9\u5206\u914d\uff0c\u9650\u5236\u4e86\u8868\u793a\u591a\u6837\u6027\u3002GuiLoMo\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GuiLoMo\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5b66\u4e60\u5f15\u5bfc\u9009\u62e9\u5411\u91cf\uff08GSVs\uff09\uff0c\u6355\u6349\u6a21\u578b\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u52a8\u6001\u5206\u914d\u6bcf\u5c42\u7684\u4e13\u5bb6\u6570\u91cf\u548c\u79e9\u3002\u5b9e\u9a8c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGuiLoMo\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e13\u5bb6\u6570\u91cf\u548c\u79e9\u5728\u4e0d\u540c\u5c42\u548c\u4efb\u52a1\u4e2d\u7684\u53d8\u5316\u89c4\u5f8b\u3002", "conclusion": "GuiLoMo\u901a\u8fc7\u81ea\u9002\u5e94\u4e13\u5bb6\u914d\u7f6e\u663e\u8457\u63d0\u5347\u4e86LoRA-MoE\u7684\u6027\u80fd\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "GuiLoMo\uff1a\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u4e0e\u5f15\u5bfc\u9009\u62e9\u5411\u91cf\u4e3aLoRA-MoE\u5206\u914d\u4e13\u5bb6\u6570\u91cf\u4e0e\u79e9", "abstract_zh": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ee5\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u8f83\u5c11\u3002\u8fd1\u671f\u5de5\u4f5c\u5c06LoRA\u4e0e\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u7ed3\u5408\uff0c\u5373LoRA-MoE\uff0c\u4ee5\u63d0\u5347\u5bb9\u91cf\uff0c\u4f46\u4ecd\u5b58\u5728\u4e24\u4e2a\u9650\u5236\uff1a1) \u5206\u914d\u4e13\u5bb6\u6570\u91cf\u65f6\u672a\u5145\u5206\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\uff1b2) \u6240\u6709LoRA\u4e13\u5bb6\u91c7\u7528\u7edf\u4e00\u79e9\u5206\u914d\uff0c\u9650\u5236\u4e86\u8868\u793a\u591a\u6837\u6027\u3002\u4e3a\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u6211\u4eec\u63d0\u51faGuiLoMo\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f15\u5bfc\u9009\u62e9\u5411\u91cf\uff08GSVs\uff09\u7684\u7ec6\u7c92\u5ea6\u5206\u5c42\u4e13\u5bb6\u6570\u91cf\u4e0e\u79e9\u5206\u914d\u7b56\u7565\u3002GSVs\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5b66\u4e60\uff0c\u6355\u6349\u6a21\u578b\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u5e76\u7528\u4e8e\u5206\u914d\u6700\u4f18\u4e13\u5bb6\u6570\u91cf\u548c\u79e9\u3002\u5728\u591a\u4e2a\u9aa8\u5e72\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGuiLoMo\u59cb\u7ec8\u4f18\u4e8e\u6216\u4e0e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u63ed\u793a\u4e86\u4e13\u5bb6\u6570\u91cf\u548c\u79e9\u5728\u4e0d\u540c\u5c42\u548c\u4efb\u52a1\u4e2d\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u7a81\u51fa\u4e86\u81ea\u9002\u5e94\u4e13\u5bb6\u914d\u7f6e\u7684\u4f18\u52bf\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/Liar406/Gui-LoMo.git\u3002"}}
{"id": "2506.14525", "pdf": "https://arxiv.org/pdf/2506.14525", "abs": "https://arxiv.org/abs/2506.14525", "authors": ["Zhuoyue Tan", "Boyong He", "Yuxiang Ji", "Liaoni Wu"], "title": "VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IROS2025", "summary": "This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications.", "AI": {"tldr": "VisLanding\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee3D\u611f\u77e5\u7684\u65e0\u4eba\u673a\u5b89\u5168\u7740\u9646\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u9884\u6d4b\u4f18\u5316\u5b89\u5168\u7740\u9646\u533a\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u5e76\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u7740\u9646\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u9884\u6d4b\u6784\u5efa\u7aef\u5230\u7aef\u7684\u5b89\u5168\u7740\u9646\u533a\u4f30\u8ba1\u6846\u67b6\u3002", "method": "\u5229\u7528Metric3D V2\u6a21\u578b\u7684\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u9884\u6d4b\u80fd\u529b\uff0c\u5f15\u5165\u5b89\u5168\u533a\u5206\u5272\u5206\u652f\u5c06\u7740\u9646\u533a\u4f30\u8ba1\u4efb\u52a1\u8f6c\u5316\u4e3a\u4e8c\u503c\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u5e76\u4f7f\u7528WildUAV\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u548c\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVisLanding\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u7ebf\u8054\u5408\u4f18\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u5b89\u5168\u533a\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5e76\u5728\u8de8\u57df\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "VisLanding\u4e0d\u4ec5\u4f18\u5316\u4e86\u5b89\u5168\u7740\u9646\u533a\u8bc6\u522b\uff0c\u8fd8\u80fd\u901a\u8fc7\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4fe1\u606f\u4f30\u8ba1\u7740\u9646\u533a\u9762\u79ef\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u5173\u952e\u51b3\u7b56\u652f\u6301\u3002", "paper_title_zh": "VisLanding\uff1a\u57fa\u4e8e\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u7684\u5355\u76ee3D\u611f\u77e5\u65e0\u4eba\u673a\u5b89\u5168\u7740\u9646\u6280\u672f", "abstract_zh": "\u672c\u6587\u63d0\u51faVisLanding\uff0c\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee3D\u611f\u77e5\u7684\u65e0\u4eba\u673a\u5b89\u5168\u7740\u9646\u6846\u67b6\u3002\u9488\u5bf9\u65e0\u4eba\u673a\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u7740\u9646\u7684\u6838\u5fc3\u6311\u6218\uff0c\u672c\u7814\u7a76\u521b\u65b0\u6027\u5730\u5229\u7528Metric3D V2\u6a21\u578b\u7684\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u9884\u6d4b\u80fd\u529b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5b89\u5168\u7740\u9646\u533a\uff08SLZ\uff09\u4f30\u8ba1\u6846\u67b6\u3002\u901a\u8fc7\u5f15\u5165\u5b89\u5168\u533a\u5206\u5272\u5206\u652f\uff0c\u6211\u4eec\u5c06\u7740\u9646\u533a\u4f30\u8ba1\u4efb\u52a1\u8f6c\u5316\u4e3a\u4e8c\u503c\u8bed\u4e49\u5206\u5272\u95ee\u9898\u3002\u6a21\u578b\u4f7f\u7528\u65e0\u4eba\u673a\u89c6\u89d2\u7684WildUAV\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u548c\u6807\u6ce8\uff0c\u540c\u65f6\u6784\u5efa\u8de8\u57df\u8bc4\u4f30\u6570\u636e\u96c6\u4ee5\u9a8c\u8bc1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVisLanding\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u7ebf\u8054\u5408\u4f18\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u533a\u8bc6\u522b\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u4e86Metric3D V2\u7684\u96f6\u6837\u672c\u6cdb\u5316\u4f18\u52bf\u3002\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u8de8\u57df\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u901a\u8fc7\u6574\u5408\u9884\u6d4b\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4fe1\u606f\u4f30\u8ba1\u7740\u9646\u533a\u9762\u79ef\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u5173\u952e\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2506.14681", "pdf": "https://arxiv.org/pdf/2506.14681", "abs": "https://arxiv.org/abs/2506.14681", "authors": ["Yuto Harada", "Yusuke Yamauchi", "Yusuke Oda", "Yohei Oseki", "Yusuke Miyao", "Yu Takagi"], "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality", "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u5b9e\u9a8c\uff0c\u7814\u7a76\u53d1\u73b0\u6570\u636e\u96c6\u7279\u6027\u3001\u5c42\u95f4\u4fee\u6539\u53ca\u8bad\u7ec3\u4efb\u52a1\u534f\u540c\u6548\u5e94\u663e\u8457\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u8d28\u91cf\uff0c\u56f0\u60d1\u5ea6\u662f\u9884\u6d4b\u5fae\u8c03\u6548\u679c\u7684\u5173\u952e\u6307\u6807\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u662f\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u6307\u4ee4\u548c\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u5176\u8bb8\u591a\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u3001\u5c42\u95f4\u4fee\u6539\u53ca\u8bad\u7ec3\u4efb\u52a1\u5982\u4f55\u5f71\u54cdSFT\u6548\u679c\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5728\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u6570\u5b66\u63a8\u7406\u548c\u901a\u7528\u4efb\u52a1\uff09\u4e0a\u8bad\u7ec3\u4e861000\u591a\u4e2aSFT\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u96c6\u7279\u6027\u3001\u5c42\u95f4\u6743\u91cd\u53d8\u5316\u53ca\u8bad\u7ec3\u4efb\u52a1\u534f\u540c\u6548\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u67d0\u4e9b\u8bad\u7ec3\u4efb\u52a1\u7684\u534f\u540c\u6548\u5e94\u5728\u6240\u6709\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u800c\u5176\u4ed6\u6548\u5e94\u5219\u56e0\u6a21\u578b\u800c\u5f02\uff1b\u56f0\u60d1\u5ea6\u80fd\u6709\u6548\u9884\u6d4bSFT\u6548\u679c\uff0c\u4e14\u4e2d\u5c42\u6743\u91cd\u53d8\u5316\u4e0e\u6027\u80fd\u63d0\u5347\u76f8\u5173\u6027\u6700\u5f3a\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6a21\u578b\u7279\u5b9a\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u56f0\u60d1\u5ea6\u548c\u4e2d\u5c42\u6743\u91cd\u53d8\u5316\u662f\u8bc4\u4f30SFT\u6548\u679c\u7684\u5173\u952e\u6307\u6807\u3002\u56e2\u961f\u5c06\u516c\u5f001000\u591a\u4e2aSFT\u6a21\u578b\u53ca\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u4ee5\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "paper_title_zh": "\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u5b9e\u9a8c\u63ed\u793a\u6570\u636e\u3001\u5c42\u95f4\u53ca\u8bad\u7ec3\u56e0\u7d20\u5982\u4f55\u5851\u9020\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u8d28\u91cf", "abstract_zh": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u662f\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4eba\u7c7b\u6307\u4ee4\u548c\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u5176\u8bb8\u591a\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u6211\u4eec\u5728\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u6570\u5b66\u63a8\u7406\u548c\u901a\u7528\u4efb\u52a1\uff09\u4e0a\u8bad\u7ec3\u4e86\u591a\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u751f\u6210\u4e861000\u591a\u4e2a\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u7684SFT\u6a21\u578b\u3002\u968f\u540e\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u6700\u91cd\u8981\u7684\u6570\u636e\u96c6\u7279\u6027\uff0c\u5e76\u7814\u7a76\u4e86SFT\u5f15\u5165\u7684\u5c42\u95f4\u4fee\u6539\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u67d0\u4e9b\u8bad\u7ec3\u4efb\u52a1\u7684\u534f\u540c\u6548\u5e94\u5728\u6240\u6709\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u800c\u5176\u4ed6\u6548\u5e94\u5219\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u8fd9\u51f8\u663e\u4e86\u6a21\u578b\u7279\u5b9a\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u56f0\u60d1\u5ea6\u80fd\u4e00\u81f4\u9884\u6d4bSFT\u6548\u679c\uff08\u901a\u5e38\u4f18\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u4e4b\u95f4\u7684\u8868\u9762\u76f8\u4f3c\u6027\uff09\uff0c\u4e14\u4e2d\u5c42\u6743\u91cd\u53d8\u5316\u4e0e\u6027\u80fd\u63d0\u5347\u76f8\u5173\u6027\u6700\u5f3a\u3002\u6211\u4eec\u5c06\u516c\u5f00\u8fd91000\u591a\u4e2aSFT\u6a21\u578b\u53ca\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u4ee5\u52a0\u901f\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.14541", "pdf": "https://arxiv.org/pdf/2506.14541", "abs": "https://arxiv.org/abs/2506.14541", "authors": ["Rongchang Lu", "Tianduo Luo", "Yunzhi Zhang", "Conghan Yue", "Pei Yang", "Guibao Liu", "Changyang Gu"], "title": "Exploring Diffusion with Test-Time Training on Efficient Image Restoration", "categories": ["cs.CV"], "comment": "Submitted to The 8th Chinese Conference on Pattern Recognition and Computer Vision (2025). Contact to nomodeset@qq.com. Source code will open in 4 months", "summary": "Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiffRWKVIR\u6846\u67b6\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0e\u9ad8\u6548\u6269\u6563\u65b9\u6cd5\uff0c\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u7279\u5f81\u878d\u5408\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u56fe\u50cf\u4fee\u590d\u9886\u57df\u9762\u4e34\u7279\u5f81\u878d\u5408\u4f4e\u6548\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u9002\u5e94\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u786c\u4ef6\u5229\u7528\u548c\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a(1) Omni-Scale 2D\u72b6\u6001\u6f14\u5316\uff0c\u6269\u5c55RWKV\u7684\u4f4d\u7f6e\u4f9d\u8d56\u53c2\u6570\u5316\u4ee5\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\uff1b(2) \u5206\u5757\u4f18\u5316\u95ea\u5b58\u5904\u7406\uff0c\u901a\u8fc7\u8fde\u7eed\u5206\u5757\u5904\u7406\u52a0\u901f\u5e76\u884c\u8ba1\u7b97\uff1b(3) \u5148\u9a8c\u5f15\u5bfc\u9ad8\u6548\u6269\u6563\uff0c\u63d0\u53d6\u7d27\u51d1\u56fe\u50cf\u5148\u9a8c\u8868\u793a\u4ee5\u52a0\u901f\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\uff0cDiffRWKVIR\u5728PSNR\u3001SSIM\u3001LPIPS\u548c\u6548\u7387\u6307\u6807\u4e0a\u4f18\u4e8eSwinIR\u3001HAT\u548cMambaIR/v2\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u534745%\u3002", "conclusion": "DiffRWKVIR\u4e3a\u81ea\u9002\u5e94\u9ad8\u6548\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u4f18\u5316\u4e86\u786c\u4ef6\u5229\u7528\u548c\u6027\u80fd\u8868\u73b0\u3002", "paper_title_zh": "\u63a2\u7d22\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u7684\u9ad8\u6548\u56fe\u50cf\u4fee\u590d\u6269\u6563\u65b9\u6cd5", "abstract_zh": "\u56fe\u50cf\u4fee\u590d\u9762\u4e34\u7279\u5f81\u878d\u5408\u4f4e\u6548\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e0b\u7b49\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faDiffRWKVIR\u6846\u67b6\uff0c\u5c06\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTT\uff09\u4e0e\u9ad8\u6548\u6269\u6563\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u542b\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a(1) Omni-Scale 2D\u72b6\u6001\u6f14\u5316\uff0c\u5c06RWKV\u7684\u4f4d\u7f6e\u4f9d\u8d56\u53c2\u6570\u5316\u6269\u5c55\u4e3a\u5206\u5c42\u591a\u65b9\u54112D\u626b\u63cf\uff0c\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6O(L)\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\uff1b(2) \u5206\u5757\u4f18\u5316\u95ea\u5b58\u5904\u7406\uff0c\u901a\u8fc7\u8fde\u7eed\u5206\u5757\u5904\u7406\u5c06\u5206\u5757\u5185\u5e76\u884c\u52a0\u901f3.2\u500d\uff08\u590d\u6742\u5ea6O(LCd)\uff09\uff0c\u51cf\u5c11\u987a\u5e8f\u4f9d\u8d56\u548c\u8ba1\u7b97\u5f00\u9500\uff1b(3) \u5148\u5bfc\u5f15\u5bfc\u9ad8\u6548\u6269\u6563\uff0c\u4ec5\u97005-20\u6b65\u63d0\u53d6\u7d27\u51d1\u56fe\u50cf\u5148\u9a8c\u8868\u793a\uff08IPR\uff09\uff0c\u6bd4DiffIR\u8bad\u7ec3/\u63a8\u7406\u901f\u5ea6\u5feb45%\uff0c\u540c\u65f6\u89e3\u51b3\u53bb\u566a\u4e2d\u7684\u8ba1\u7b97\u4f4e\u6548\u95ee\u9898\u3002\u5728\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\uff08Set5\u3001Set14\u3001BSD100\u3001Urban100\u3001Places365\uff09\u4e2d\uff0cDiffRWKVIR\u5728PSNR\u3001SSIM\u3001LPIPS\u548c\u6548\u7387\u6307\u6807\u4e0a\u4f18\u4e8eSwinIR\u3001HAT\u548cMambaIR/v2\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u81ea\u9002\u5e94\u9ad8\u6548\u56fe\u50cf\u4fee\u590d\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4f18\u5316\u4e86\u786c\u4ef6\u5229\u7528\u7387\u3002"}}
{"id": "2506.13781", "pdf": "https://arxiv.org/pdf/2506.13781", "abs": "https://arxiv.org/abs/2506.13781", "authors": ["Pablo Ari\u00f1o Fern\u00e1ndez"], "title": "Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment", "categories": ["cs.LG", "cs.AI", "cs.DM"], "comment": "Bachelor's thesis, Universidad Polit\u00e9cnica de Madrid, 2025. 150 pages, 23 figures", "summary": "The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faJobShopLib\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u5e93\uff0c\u7528\u4e8e\u5b9a\u5236\u5316\u89e3\u51b3\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff08JSSP\uff09\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u8bad\u7ec3\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u8c03\u5ea6\u5668\uff0c\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff08JSSP\uff09\u662f\u4e00\u4e2aNP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982GNN\uff09\u6709\u671b\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u6a21\u5757\u5316\u5de5\u5177\u4f7f\u5f97\u5b9e\u9a8c\u548c\u7814\u7a76\u8017\u65f6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86JobShopLib\u6a21\u5757\u5316\u5e93\uff0c\u652f\u6301\u5b9a\u5236\u56fe\u8868\u793a\u3001\u8282\u70b9\u7279\u5f81\u3001\u52a8\u4f5c\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\u3002\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u591a\u4e2a\u8c03\u5ea6\u5668\uff0c\u5e76\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u4e00\u4e2a\u4ec5\u4f7f\u7528\u5355\u4e2a\u64cd\u4f5c\u7279\u5f81\u7684GNN\u6a21\u578b\u4f18\u4e8e\u591a\u79cd\u57fa\u4e8e\u56fe\u7684\u8c03\u5ea6\u5668\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u63a5\u8fd1\u6700\u4f18\u7ed3\u679c\uff0c\u51f8\u663e\u4e86\u7279\u5f81\u5b9a\u5236\u7684\u91cd\u8981\u6027\u3002", "conclusion": "JobShopLib\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u5de5\u5177\uff0c\u8868\u660e\u6b64\u7c7b\u6a21\u578b\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "paper_title_zh": "\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff1a\u4e00\u4e2a\u53ef\u5b9a\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883", "abstract_zh": "\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\u662f\u4e00\u4e2a\u4e0e\u5236\u9020\u548c\u65f6\u95f4\u8868\u76f8\u5173\u7684NP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u57fa\u4e8e\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u4f7f\u7528\u4f18\u5148\u7ea7\u8c03\u5ea6\u89c4\u5219\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5c1d\u8bd5\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662f\u56fe\u795e\u7ecf\u7f51\u7edcGNN\uff09\u66ff\u4ee3\u8fd9\u4e9b\u89c4\u5219\uff0c\u901a\u8fc7\u5b66\u4e60\u6570\u636e\u5206\u914d\u4f18\u5148\u7ea7\u3002\u7136\u800c\uff0c\u8bad\u7ec3\u6b64\u7c7b\u6a21\u578b\u9700\u8981\u5b9a\u5236\u4f17\u591a\u56e0\u7d20\uff1a\u56fe\u8868\u793a\u3001\u8282\u70b9\u7279\u5f81\u3001\u52a8\u4f5c\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\u3002\u7f3a\u4e4f\u6a21\u5757\u5316\u5b9e\u9a8c\u5e93\u4f7f\u5f97\u7814\u7a76\u8017\u65f6\u3002\u672c\u6587\u4ecb\u7ecd\u4e86JobShopLib\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u5e93\uff0c\u5141\u8bb8\u5b9a\u5236\u8fd9\u4e9b\u56e0\u7d20\u5e76\u901a\u8fc7\u5176\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u521b\u5efa\u65b0\u7ec4\u4ef6\u3002\u6211\u4eec\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u4e86\u591a\u4e2a\u8c03\u5ea6\u5668\u4ee5\u5c55\u793a\u8be5\u73af\u5883\u7684\u5b9e\u7528\u6027\u3002\u4e00\u4e2a\u6a21\u578b\u4ec5\u4f7f\u7528\u5355\u4e2a\u64cd\u4f5c\u7279\u5f81\u5c31\u4f18\u4e8e\u591a\u79cd\u57fa\u4e8e\u56fe\u7684\u8c03\u5ea6\u5668\uff0c\u51f8\u663e\u4e86\u7279\u5f81\u5b9a\u5236\u7684\u91cd\u8981\u6027\u3002\u6211\u4eec\u7684GNN\u6a21\u578b\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7ed3\u679c\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u6b64\u7c7b\u6a21\u578b\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002JobShopLib\u4e3a\u672a\u6765\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u5fc5\u8981\u5de5\u5177\u3002"}}
{"id": "2506.14702", "pdf": "https://arxiv.org/pdf/2506.14702", "abs": "https://arxiv.org/abs/2506.14702", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet \u00dcst\u00fcn", "Sara Hooker"], "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: \"Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?\" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bad\u7ec3\u65f6\u6807\u8bb0\u4f18\u5316\u6a21\u578b\u5728\u957f\u5c3e\u6570\u636e\u4e0a\u8868\u73b0\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f55\u89c1\u7528\u4f8b\u7684\u6027\u80fd\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u662f\u5982\u4f55\u5728\u7f55\u89c1\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u957f\u5c3e\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002\u73b0\u6709\u7684\u5927\u578b\u901a\u7528\u6a21\u578b\u867d\u7136\u5728\u591a\u6570\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u4f4e\u9891\u7528\u4f8b\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u8bad\u7ec3\u540e\u96be\u4ee5\u9488\u5bf9\u7279\u5b9a\u7528\u4f8b\u8fdb\u884c\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u534f\u8bae\uff0c\u63d0\u5347\u6a21\u578b\u5728\u957f\u5c3e\u6570\u636e\u4e0a\u7684\u6027\u80fd\u548c\u53ef\u63a7\u6027\u3002", "method": "\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u8bad\u7ec3\u4e0e\u63a8\u7406\u6280\u672f\u7684\u754c\u9650\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7279\u5f81\u548c\u4efb\u52a1\u6765\u6e90\u7684\u8be6\u7ec6\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u663e\u5f0f\u63a7\u5236\u751f\u6210\u5c5e\u6027\u548c\u9690\u5f0f\u6761\u4ef6\u751f\u6210\u3002\u901a\u8fc7\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u63a8\u65ad\u8fd9\u4e9b\u6807\u8bb0\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u65f6\u53ef\u9009\u62e9\u6027\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5f00\u653e\u751f\u6210\u8d28\u91cf\u4e0a\u5e73\u5747\u63d0\u5347\u4e865.7%\u7684\u80dc\u7387\uff0c\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u9886\u57df\u4e2d\u589e\u76ca\u8d85\u8fc79.1%\u3002\u5728CodeRepair\u7b49\u7f55\u89c1\u4efb\u52a1\u4e0a\uff0c\u76f8\u5bf9\u63d0\u5347\u9ad8\u8fbe14.1%\uff0c\u5728\u957f\u5ea6\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u4e2d\u7edd\u5bf9\u63d0\u534735.3%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u65f6\u6807\u8bb0\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u957f\u5c3e\u6570\u636e\u4e0a\u7684\u6027\u80fd\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u7f55\u89c1\u7528\u4f8b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5bfb\u5b9d\uff1a\u5229\u7528\u8bad\u7ec3\u65f6\u6807\u8bb0\u5b9e\u65f6\u5b9a\u4f4d\u957f\u5c3e\u6570\u636e", "abstract_zh": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6700\u6df1\u523b\u7684\u6311\u6218\u4e4b\u4e00\u662f\u5728\u7f55\u89c1\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u957f\u5c3e\u7279\u5f81\u4e0a\u8868\u73b0\u826f\u597d\u3002\u5927\u578b\u901a\u7528\u6a21\u578b\u867d\u7136\u9488\u5bf9\u591a\u79cd\u4efb\u52a1\u8bad\u7ec3\uff0c\u4f46\u5728\u9ad8\u9891\u7528\u4f8b\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u8bad\u7ec3\u8bed\u6599\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7279\u5b9a\u7528\u4f8b\u3002\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u6216\u5c11\u6837\u672c\u793a\u4f8b\u6765\u6700\u5927\u5316\u7279\u5b9a\u6d4b\u8bd5\u7528\u4f8b\u7684\u8f93\u51fa\u8d28\u91cf\u53ef\u80fd\u4ee4\u4eba\u6cae\u4e27\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5bf9\u5c0f\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\uff0c\u4ee5\u4e0d\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u53cd\u5e94\uff0c\u6216\u4f9d\u8d56\u56fa\u5b9a\u7cfb\u7edf\u63d0\u793a\u6765\u7ef4\u6301\u6027\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\uff1a\u201c\u80fd\u5426\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u534f\u8bae\uff0c\u5728\u63a8\u7406\u65f6\u540c\u65f6\u63d0\u5347\u53ef\u63a7\u6027\u548c\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7528\u4f8b\u4e0a\u7684\u8868\u73b0\uff1f\u201d\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u8bad\u7ec3\u4e0e\u63a8\u7406\u6280\u672f\u7684\u754c\u9650\uff0c\u4ee5\u63d0\u5347\u957f\u5c3e\u6027\u80fd\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u4e00\u7ec4\u6a21\u578b\u8bad\u7ec3\u65f6\u80fd\u591f\u54cd\u5e94\u7684\u63a7\u5236\u6760\u6746\u3002\u6211\u4eec\u521b\u5efa\u4e86\u6570\u636e\u7279\u5f81\u548c\u4efb\u52a1\u6765\u6e90\u7684\u8be6\u7ec6\u5206\u7c7b\u6cd5\uff0c\u4ee5\u663e\u5f0f\u63a7\u5236\u751f\u6210\u5c5e\u6027\u5e76\u5728\u63a8\u7406\u65f6\u9690\u5f0f\u6761\u4ef6\u751f\u6210\u3002\u6211\u4eec\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u4ee5\u81ea\u52a8\u63a8\u65ad\u8fd9\u4e9b\u6807\u8bb0\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u65f6\u53ef\u9009\u62e9\u6027\u4f7f\u7528\u3002\u8fd9\u79cd\u539f\u5219\u6027\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u5206\u5e03\u7684\u957f\u5c3e\u793a\u4f8b\u4e0a\u3002\u5c3d\u7ba1\u6211\u4eec\u7684\u6807\u8bb0\u5728\u5f00\u653e\u751f\u6210\u8d28\u91cf\u4e0a\u5e73\u5747\u63d0\u5347\u4e865.7%\u7684\u80dc\u7387\uff0c\u4f46\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u9886\u57df\u4e2d\u589e\u76ca\u8d85\u8fc79.1%\u3002\u5728CodeRepair\u7b49\u7f55\u89c1\u4efb\u52a1\u4e0a\uff0c\u76f8\u5bf9\u63d0\u5347\u9ad8\u8fbe14.1%\uff0c\u5728\u957f\u5ea6\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u4e2d\u7edd\u5bf9\u63d0\u534735.3%\u3002"}}
{"id": "2506.14549", "pdf": "https://arxiv.org/pdf/2506.14549", "abs": "https://arxiv.org/abs/2506.14549", "authors": ["Yong Liu", "Wenpeng Xiao", "Qianqian Wang", "Junlin Chen", "Shiyin Wang", "Yitong Wang", "Xinglong Wu", "Yansong Tang"], "title": "DreamLight: Towards Harmonious and Consistent Image Relighting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDreamLight\u7684\u901a\u7528\u56fe\u50cf\u91cd\u5149\u7167\u6a21\u578b\uff0c\u80fd\u591f\u65e0\u7f1d\u5730\u5c06\u4e3b\u4f53\u5408\u6210\u5230\u65b0\u80cc\u666f\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u5149\u7167\u548c\u8272\u8c03\u7684\u7f8e\u5b66\u4e00\u81f4\u6027\u3002\u8be5\u6a21\u578b\u652f\u6301\u57fa\u4e8e\u56fe\u50cf\u6216\u6587\u672c\u7684\u91cd\u5149\u7167\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u4f4d\u7f6e\u5f15\u5bfc\u5149\u9002\u914d\u5668\u548c\u9891\u8c31\u524d\u666f\u4fee\u590d\u5668\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u56fe\u50cf\u7684\u91cd\u5149\u7167\uff0c\u5bf9\u57fa\u4e8e\u6587\u672c\u7684\u573a\u666f\u63a2\u7d22\u8f83\u5c11\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u590d\u6742\u7684\u6570\u636e\u5206\u89e3\u6d41\u7a0b\uff0c\u8981\u4e48\u5c06\u4efb\u52a1\u89c6\u4e3a\u56fe\u50cf\u7ffb\u8bd1\u95ee\u9898\uff0c\u96be\u4ee5\u751f\u6210\u771f\u5b9e\u7684\u5149\u7167\u4ea4\u4e92\u6548\u679c\u3002DreamLight\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u548c\u4e00\u81f4\u7684\u91cd\u5149\u7167\u7ed3\u679c\u3002", "method": "DreamLight\u5c06\u8f93\u5165\u6570\u636e\u7edf\u4e00\u683c\u5f0f\u5316\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u751f\u6210\u81ea\u7136\u7ed3\u679c\u3002\u63d0\u51fa\u4f4d\u7f6e\u5f15\u5bfc\u5149\u9002\u914d\u5668\uff08PGLA\uff09\u5c06\u80cc\u666f\u5149\u4fe1\u606f\u538b\u7f29\u4e3a\u5149\u67e5\u8be2\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u65b9\u5411\u504f\u7f6e\u63a9\u7801\u6ce8\u610f\u529b\u8c03\u5236\u524d\u666f\u3002\u6b64\u5916\uff0c\u9891\u8c31\u524d\u666f\u4fee\u590d\u5668\uff08SFF\uff09\u6a21\u5757\u81ea\u9002\u5e94\u91cd\u7ec4\u524d\u666f\u548c\u80cc\u666f\u7684\u9891\u7387\u5206\u91cf\u4ee5\u589e\u5f3a\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5bf9\u6bd4\u548c\u7528\u6237\u7814\u7a76\uff0cDreamLight\u5728\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u751f\u6210\u771f\u5b9e\u4e14\u81ea\u7136\u7684\u5149\u7167\u4ea4\u4e92\u6548\u679c\u3002", "conclusion": "DreamLight\u901a\u8fc7\u521b\u65b0\u7684PGLA\u548cSFF\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5149\u7167\u7684\u81ea\u7136\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u57fa\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u7684\u91cd\u5149\u7167\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DreamLight\uff1a\u8fc8\u5411\u548c\u8c10\u4e00\u81f4\u7684\u56fe\u50cf\u91cd\u5149\u7167", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aDreamLight\u7684\u901a\u7528\u56fe\u50cf\u91cd\u5149\u7167\u6a21\u578b\uff0c\u80fd\u591f\u65e0\u7f1d\u5730\u5c06\u4e3b\u4f53\u5408\u6210\u5230\u65b0\u80cc\u666f\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u5149\u7167\u548c\u8272\u8c03\u7684\u7f8e\u5b66\u4e00\u81f4\u6027\u3002\u80cc\u666f\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u56fe\u50cf\uff08\u57fa\u4e8e\u56fe\u50cf\u7684\u91cd\u5149\u7167\uff09\u6216\u65e0\u9650\u6587\u672c\u63d0\u793a\uff08\u57fa\u4e8e\u6587\u672c\u7684\u91cd\u5149\u7167\uff09\u6307\u5b9a\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u56fe\u50cf\u7684\u91cd\u5149\u7167\uff0c\u800c\u5bf9\u57fa\u4e8e\u6587\u672c\u7684\u573a\u666f\u63a2\u7d22\u8f83\u5c11\u3002\u4e00\u4e9b\u5de5\u4f5c\u4f9d\u8d56\u590d\u6742\u7684\u73af\u5883\u56fe\u5206\u89e3\u6d41\u7a0b\uff0c\u9700\u8981\u6602\u8d35\u7684\u6570\u636e\u6210\u672c\u8fdb\u884c\u56fa\u6709\u5206\u89e3\u548c\u5149\u6e90\u63d0\u53d6\uff1b\u5176\u4ed6\u65b9\u6cd5\u5c06\u6b64\u4efb\u52a1\u89c6\u4e3a\u56fe\u50cf\u7ffb\u8bd1\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u67b6\u6784\u8fdb\u884c\u50cf\u7d20\u7ea7\u53d8\u6362\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u548c\u8c10\u6548\u679c\uff0c\u4f46\u96be\u4ee5\u751f\u6210\u524d\u666f\u4e0e\u80cc\u666f\u4e4b\u95f4\u771f\u5b9e\u81ea\u7136\u7684\u5149\u7167\u4ea4\u4e92\u6548\u679c\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u8f93\u5165\u6570\u636e\u7edf\u4e00\u683c\u5f0f\u5316\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u4fc3\u8fdb\u81ea\u7136\u7ed3\u679c\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7f6e\u5f15\u5bfc\u5149\u9002\u914d\u5668\uff08PGLA\uff09\uff0c\u5c06\u80cc\u666f\u4e2d\u4e0d\u540c\u65b9\u5411\u7684\u5149\u4fe1\u606f\u538b\u7f29\u4e3a\u8bbe\u8ba1\u7684\u5149\u67e5\u8be2\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u65b9\u5411\u504f\u7f6e\u63a9\u7801\u6ce8\u610f\u529b\u8c03\u5236\u524d\u666f\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u6a21\u5757\u2014\u2014\u9891\u8c31\u524d\u666f\u4fee\u590d\u5668\uff08SFF\uff09\uff0c\u81ea\u9002\u5e94\u5730\u91cd\u7ec4\u4e3b\u4f53\u548c\u91cd\u5149\u7167\u80cc\u666f\u7684\u4e0d\u540c\u9891\u7387\u5206\u91cf\uff0c\u4ee5\u589e\u5f3a\u524d\u666f\u5916\u89c2\u7684\u4e00\u81f4\u6027\u3002\u5e7f\u6cdb\u7684\u5bf9\u6bd4\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cDreamLight\u5728\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.13782", "pdf": "https://arxiv.org/pdf/2506.13782", "abs": "https://arxiv.org/abs/2506.13782", "authors": ["Ke Wang", "Bo Pan", "Yingchaojie Feng", "Yuwei Wu", "Jieyi Chen", "Minfeng Zhu", "Wei Chen"], "title": "XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted to IEEE Pacific Visualization Conference 2025", "summary": "Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faXGraphRAG\uff0c\u4e00\u79cd\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u5e2e\u52a9\u5f00\u53d1\u8005\u5206\u6790\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u7684\u6548\u679c\uff0c\u63d0\u5347\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "GraphRAG\u901a\u8fc7\u5f15\u5165\u56fe\u7ed3\u6784\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u7cbe\u5ea6\u548c\u5168\u9762\u6027\uff0c\u4f46\u5176\u590d\u6742\u7684\u4fe1\u606f\u5904\u7406\u6d41\u7a0b\u548c\u5927\u91cfLLM\u8c03\u7528\u9650\u5236\u4e86\u5f00\u53d1\u8005\u7684\u5206\u6790\u80fd\u529b\uff0c\u4e9f\u9700\u4e00\u79cd\u5de5\u5177\u6765\u63d0\u5347\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u5206\u6790\u6846\u67b6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522bGraphRAG\u4e2d\u7684\u5173\u952e\u53ec\u56de\u5e76\u8ffd\u8e2a\u5176\u5904\u7406\u6d41\u7a0b\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u5f00\u53d1\u4e86XGraphRAG\u539f\u578b\u7cfb\u7edf\uff0c\u96c6\u6210\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u4ee5\u652f\u6301\u7528\u6237\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cXGraphRAG\u80fd\u6709\u6548\u63d0\u5347\u5f00\u53d1\u8005\u5bf9GraphRAG\u7684\u5206\u6790\u80fd\u529b\uff0c\u5e2e\u52a9\u8bc6\u522b\u5931\u8d25\u6848\u4f8b\u548c\u6539\u8fdb\u673a\u4f1a\u3002", "conclusion": "XGraphRAG\u4e3aGraphRAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u3002", "paper_title_zh": "XGraphRAG\uff1a\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790", "abstract_zh": "\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u901a\u8fc7\u5f15\u5165\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5916\u90e8\u77e5\u8bc6\u5e93\u652f\u6301\u4e0b\u7684\u751f\u6210\u7cbe\u5ea6\u548c\u5168\u9762\u6027\u3002\u7136\u800c\uff0c\u5f00\u53d1\u8005\u901a\u5e38\u96be\u4ee5\u5206\u6790GraphRAG\u5728\u5176\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\uff0c\u539f\u56e0\u5728\u4e8e\u5176\u590d\u6742\u7684\u4fe1\u606f\u5904\u7406\u6d41\u7a0b\u548c\u5927\u91cf\u7684LLM\u8c03\u7528\uff0c\u8fd9\u9650\u5236\u4e86GraphRAG\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u5206\u6790\u6846\u67b6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522bGraphRAG\u4e2d\u7684\u5173\u952e\u53ec\u56de\u5e76\u8ffd\u8e2a\u5176\u5904\u7406\u6d41\u7a0b\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86XGraphRAG\u539f\u578b\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u4e00\u5957\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u4ee5\u652f\u6301\u7528\u6237\u7684\u5206\u6790\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u5931\u8d25\u6848\u4f8b\u7684\u6536\u96c6\u548c\u6539\u8fdb\u673a\u4f1a\u7684\u8bc6\u522b\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u7528\u6027\u3002\u672c\u5de5\u4f5c\u5df2\u5f00\u6e90\uff0c\u8be6\u89c1https://github.com/Gk0Wk/XGraphRAG\u3002"}}
{"id": "2506.14704", "pdf": "https://arxiv.org/pdf/2506.14704", "abs": "https://arxiv.org/abs/2506.14704", "authors": ["Anton Changalidis", "Aki H\u00e4rm\u00e4"], "title": "Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data", "categories": ["cs.CL"], "comment": "This work has been accepted for publication at the First Workshop on Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria", "summary": "This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fTransformer\u7684\u8bb0\u5fc6\u80fd\u529b\u5982\u4f55\u53d7\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u914d\u7f6e\u5f71\u54cd\uff0c\u53d1\u73b0\u5d4c\u5165\u5927\u5c0f\u662f\u5b66\u4e60\u901f\u5ea6\u548c\u5bb9\u91cf\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff0c\u800cSoftmax\u6fc0\u6d3b\u51fd\u6570\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "motivation": "\u63a2\u8ba8\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u914d\u7f6e\u5bf9\u751f\u6210\u5f0fTransformer\u8bb0\u5fc6\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u5e76\u63d0\u5347\u5bf9\u7ed3\u6784\u5316\u771f\u5b9e\u6570\u636e\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eSNOMED\u77e5\u8bc6\u56fe\u8c31\u7684\u5408\u6210\u6587\u672c\u6570\u636e\u96c6\uff08\u4e09\u5143\u7ec4\u548c\u5e8f\u5217\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u5206\u6790\u5d4c\u5165\u5927\u5c0f\u3001\u5c42\u6570\u548c\u6fc0\u6d3b\u51fd\u6570\u5bf9\u8bb0\u5fc6\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5d4c\u5165\u5927\u5c0f\u662f\u5b66\u4e60\u901f\u5ea6\u548c\u5bb9\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u989d\u5916\u5c42\u6570\u5bf9\u7b80\u5355\u6570\u636e\u96c6\u6548\u679c\u6709\u9650\u751a\u81f3\u6709\u5bb3\uff0cSoftmax\u6fc0\u6d3b\u51fd\u6570\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u6570\u636e\u590d\u6742\u6027\u63d0\u5347\u6700\u7ec8\u8bb0\u5fc6\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Transformer\u8bb0\u5fc6\u673a\u5236\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u771f\u5b9e\u6570\u636e\u573a\u666f\u3002", "paper_title_zh": "\u5bb9\u91cf\u81f3\u5173\u91cd\u8981\uff1aTransformer\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u8bb0\u5fc6\u80fd\u529b\u6982\u5ff5\u9a8c\u8bc1", "abstract_zh": "\u672c\u6587\u7814\u7a76\u4e86\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5f0fTransformer\u7684\u7ecf\u9a8c\u8bb0\u5fc6\u80fd\u529b\u3002\u6a21\u578b\u4f7f\u7528\u57fa\u4e8e\u7cfb\u7edf\u5316\u533b\u5b66\u547d\u540d\u6cd5\uff08SNOMED\uff09\u77e5\u8bc6\u56fe\u8c31\u7684\u5408\u6210\u6587\u672c\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff1a\u4e09\u5143\u7ec4\u4ee3\u8868\u9759\u6001\u8fde\u63a5\uff0c\u5e8f\u5217\u6a21\u62df\u590d\u6742\u5173\u7cfb\u6a21\u5f0f\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5d4c\u5165\u5927\u5c0f\u662f\u5b66\u4e60\u901f\u5ea6\u548c\u5bb9\u91cf\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff0c\u800c\u989d\u5916\u5c42\u6570\u5e26\u6765\u7684\u76ca\u5904\u6709\u9650\uff0c\u751a\u81f3\u53ef\u80fd\u5bf9\u7b80\u5355\u6570\u636e\u96c6\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u6fc0\u6d3b\u51fd\u6570\u8d77\u5173\u952e\u4f5c\u7528\uff0cSoftmax\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u5bb9\u91cf\u3002\u6b64\u5916\uff0c\u589e\u52a0\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u4f3c\u4e4e\u80fd\u63d0\u5347\u6700\u7ec8\u7684\u8bb0\u5fc6\u80fd\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u6df1\u5316\u4e86\u6211\u4eec\u5bf9Transformer\u8bb0\u5fc6\u673a\u5236\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u771f\u5b9e\u6570\u636e\u3002"}}
{"id": "2506.14560", "pdf": "https://arxiv.org/pdf/2506.14560", "abs": "https://arxiv.org/abs/2506.14560", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u4ece\u9ad8\u6548\u6269\u6563\u6a21\u578b\u751f\u6210\u7684X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u4f30\u8ba1\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\uff08OA\uff09\u8fdb\u5c55\u98ce\u9669\uff0c\u540c\u65f6\u9884\u6d4b\u672a\u6765OA\u4e25\u91cd\u7a0b\u5ea6\u548c\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u819d\u5173\u8282OA\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u73b0\u6709\u751f\u6210\u672a\u6765\u56fe\u50cf\u7684\u65b9\u6cd5\u590d\u6742\u4e14\u4e0d\u5b9e\u7528\u3002\u6b64\u5916\uff0c\u5148\u524d\u65b9\u6cd5\u672a\u80fd\u5b9a\u4f4d\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u7c7b\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\u7684\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u672a\u6765\u56fe\u50cf\uff0c\u5e76\u540c\u65f6\u5206\u7c7b\u672a\u6765OA\u4e25\u91cd\u7a0b\u5ea6\u548c\u9884\u6d4b\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u70b9\u3002", "result": "\u5728Osteoarthritis Initiative\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u9884\u6d4b\u819d\u5173\u8282OA\u8fdb\u5c55\u7684AUC\u63d0\u5347\u81f30.71\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad82%\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u7ea69%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u819d\u5173\u8282OA\u8fdb\u5c55\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684\u672a\u6765\u56fe\u50cf\u548c\u5b9a\u4f4d\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u9002\u7528\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u9ad8\u6548\u6269\u6563\u6a21\u578b\u7684\u591a\u4efb\u52a1\u9884\u6d4b\u5efa\u6a21\u4eceX\u5c04\u7ebf\u56fe\u50cf\u4e2d\u4f30\u8ba1\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u8fdb\u5c55\u98ce\u9669", "abstract_zh": "\u533b\u5b66\u5f71\u50cf\u5728\u8bc4\u4f30\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\uff08OA\uff09\u98ce\u9669\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u53ef\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\u548c\u75be\u75c5\u76d1\u6d4b\u3002\u6700\u8fd1\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u533b\u5b66\u56fe\u50cf\u6539\u8fdb\u4e86\u98ce\u9669\u4f30\u8ba1\uff08\u5373\u9884\u6d4b\u75be\u75c5\u8fdb\u5c55\u7684\u53ef\u80fd\u6027\uff09\u548c\u9884\u6d4b\u5efa\u6a21\uff08\u5373\u57fa\u4e8e\u5f53\u524d\u6570\u636e\u9884\u6d4b\u672a\u6765\u7ed3\u679c\uff09\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e34\u5e8a\u91c7\u7528\u4ecd\u6709\u9650\u3002\u73b0\u6709\u751f\u6210\u672a\u6765\u56fe\u50cf\u4ee5\u4f30\u8ba1\u98ce\u9669\u7684\u65b9\u6cd5\u590d\u6742\u4e14\u4e0d\u5b9e\u7528\u3002\u6b64\u5916\uff0c\u5148\u524d\u65b9\u6cd5\u672a\u80fd\u5b9a\u4f4d\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u586b\u8865\u4e86\u8fd9\u4e9b\u7a7a\u767d\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u4ece\u9ad8\u6548\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u672a\u6765\u56fe\u50cf\u4e2d\u5206\u7c7b\u672a\u6765OA\u4e25\u91cd\u7a0b\u5ea6\u5e76\u9884\u6d4b\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u4ece\u800c\u4f30\u8ba1\u819d\u5173\u8282OA\u8fdb\u5c55\u98ce\u9669\u3002\u8fd9\u79cd\u56fe\u50cf\u751f\u6210\u662f\u901a\u8fc7\u5728\u7c7b\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5229\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u75be\u75c5\u8fdb\u5c55\u5b9e\u73b0\u7684\uff0c\u63d0\u4f9b\u4e86\u7279\u5b9a\u5065\u5eb7\u72b6\u51b5\u53ef\u80fd\u5982\u4f55\u6f14\u53d8\u7684\u89c6\u89c9\u8868\u793a\u3002\u5728Osteoarthritis Initiative\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9884\u6d4b\u819d\u5173\u8282OA\u8fdb\u5c55\u7684AUC\u63d0\u5347\u81f30.71\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad82%\uff0c\u540c\u65f6\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u7ea69%\u3002"}}
{"id": "2506.13786", "pdf": "https://arxiv.org/pdf/2506.13786", "abs": "https://arxiv.org/abs/2506.13786", "authors": ["Vuong M. Ngo", "Tran Quang Vinh", "Patricia Kearney", "Mark Roantree"], "title": "Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "17th International Conference on Computational Collective Intelligence, LNAI, Springer, 11 pages", "summary": "Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u88c5\u888b\u96c6\u6210\u56de\u5f52\u6a21\u578b\uff08EBMBag+\uff09\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4ee5\u9884\u6d4b\u7f8e\u56fd\u57ce\u5e02\u7684\u7cd6\u5c3f\u75c5\u60a3\u75c5\u7387\u3002\u901a\u8fc7\u6570\u636e\u6574\u5408\u548c\u6a21\u578b\u4f18\u5316\uff0cEBMBag+\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u662f\u4e00\u79cd\u6162\u6027\u4ee3\u8c22\u75be\u75c5\uff0c\u51c6\u786e\u9884\u6d4b\u5176\u60a3\u75c5\u7387\u5bf9\u533b\u7597\u89c4\u5212\u548c\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u6570\u636e\u5f80\u5f80\u4e0d\u5b8c\u6574\uff0c\u56e0\u6b64\u9700\u8981\u6574\u5408\u6570\u636e\u5e76\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u7814\u7a76\u9996\u5148\u901a\u8fc7\u6570\u636e\u5de5\u7a0b\u6574\u54082011\u5e74\u81f32021\u5e74\u7684\u7cd6\u5c3f\u75c5\u76f8\u5173\u6570\u636e\u96c6\uff0c\u6784\u5efa\u5168\u9762\u7684\u7279\u5f81\u96c6\u3002\u968f\u540e\u63d0\u51faEBMBag+\u6a21\u578b\uff0c\u5e76\u4e0eSVMReg\u3001BDTree\u3001LSBoost\u3001NN\u3001LSTM\u548cERMBag\u7b49\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEBMBag+\u5728MAE\uff080.41\uff09\u3001RMSE\uff080.53\uff09\u3001MAPE\uff084.01\uff09\u548cR2\uff080.9\uff09\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "EBMBag+\u6a21\u578b\u5728\u7cd6\u5c3f\u75c5\u60a3\u75c5\u7387\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u533b\u7597\u89c4\u5212\u548c\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u3002", "paper_title_zh": "\u901a\u8fc7\u6570\u636e\u96c6\u6210\u589e\u5f3a\u88c5\u888b\u96c6\u6210\u56de\u5f52\u6a21\u578b\u7528\u4e8e\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u7cd6\u5c3f\u75c5\u9884\u6d4b", "abstract_zh": "\u7cd6\u5c3f\u75c5\u662f\u4e00\u79cd\u4ee5\u8840\u7cd6\u6c34\u5e73\u5347\u9ad8\u4e3a\u7279\u5f81\u7684\u6162\u6027\u4ee3\u8c22\u75be\u75c5\uff0c\u53ef\u80fd\u5bfc\u81f4\u5fc3\u810f\u75c5\u3001\u80be\u8870\u7aed\u548c\u795e\u7ecf\u635f\u4f24\u7b49\u5e76\u53d1\u75c7\u3002\u51c6\u786e\u9884\u6d4b\u60a3\u75c5\u7387\u5bf9\u533b\u7597\u89c4\u5212\u548c\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u5f80\u5f80\u4e0d\u5b8c\u6574\u3002\u672c\u7814\u7a76\u9996\u5148\u901a\u8fc7\u6570\u636e\u5de5\u7a0b\u6574\u54082011\u5e74\u81f32021\u5e74\u7684\u7cd6\u5c3f\u75c5\u76f8\u5173\u6570\u636e\u96c6\uff0c\u6784\u5efa\u5168\u9762\u7684\u7279\u5f81\u96c6\u3002\u968f\u540e\u63d0\u51fa\u4e00\u79cd\u589e\u5f3a\u7684\u88c5\u888b\u96c6\u6210\u56de\u5f52\u6a21\u578b\uff08EBMBag+\uff09\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4ee5\u9884\u6d4b\u7f8e\u56fd\u57ce\u5e02\u7684\u7cd6\u5c3f\u75c5\u60a3\u75c5\u7387\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecSVMReg\u3001BDTree\u3001LSBoost\u3001NN\u3001LSTM\u548cERMBag\uff09\u4e0eEBMBag+\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEBMBag+\u8868\u73b0\u6700\u4f73\uff0c\u5176MAE\u4e3a0.41\u3001RMSE\u4e3a0.53\u3001MAPE\u4e3a4.01\u3001R2\u4e3a0.9\u3002"}}
{"id": "2506.14731", "pdf": "https://arxiv.org/pdf/2506.14731", "abs": "https://arxiv.org/abs/2506.14731", "authors": ["Ring Team", "Bin Hu", "Cai Chen", "Deng Zhao", "Ding Liu", "Dingnan Jin", "Feng Zhu", "Hao Dai", "Hongzhi Luan", "Jia Guo", "Jiaming Liu", "Jiewei Wu", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junwu Xiong", "Kaihong Zhang", "Kuan Xu", "Lei Liang", "Liang Jiang", "Liangcheng Fu", "Longfei Zheng", "Qiang Gao", "Qing Cui", "Quan Wan", "Shaomian Zheng", "Shuaicheng Li", "Tongkai Yang", "Wang Ren", "Xiaodong Yan", "Xiaopei Wan", "Xiaoyun Feng", "Xin Zhao", "Xinxing Yang", "Xinyu Kong", "Xuemin Yang", "Yang Li", "Yingting Wu", "Yongkang Liu", "Zhankai Xu", "Zhenduo Zhang", "Zhenglei Zhou", "Zhenyu Huang", "Zhiqiang Zhang", "Zihao Wang", "Zujie Wen"], "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.", "AI": {"tldr": "Ring-lite\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u63a8\u7406\u80fd\u529b\u3002\u5176\u6027\u80fd\u5ab2\u7f8e\u5c0f\u578b\u63a8\u7406\u6a21\u578b\uff0c\u4f46\u4ec5\u9700\u6fc0\u6d3b\u4e09\u5206\u4e4b\u4e00\u7684\u53c2\u6570\u3002\u8bba\u6587\u63d0\u51fa\u4e86C3PO\u65b9\u6cd5\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u6574\u5408\u591a\u9886\u57df\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u9700\u8981\u6fc0\u6d3b\u5927\u91cf\u53c2\u6570\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316MoE\u6a21\u578b\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u89e3\u51b3\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u591a\u9886\u57df\u6570\u636e\u6574\u5408\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7ed3\u5408\u84b8\u998f\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u5e76\u5f15\u5165C3PO\u65b9\u6cd5\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u71b5\u635f\u5931\u9009\u62e9\u84b8\u998f\u68c0\u67e5\u70b9\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u5904\u7406\u591a\u9886\u57df\u6570\u636e\u51b2\u7a81\u3002", "result": "Ring-lite\u5728AIME\u3001LiveCodeBench\u548cGPQA-Diamond\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u6fc0\u6d3b2.75\u4ebf\u53c2\u6570\u5373\u53ef\u5ab2\u7f8e\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u3002C3PO\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u51b2\u7a81\u95ee\u9898\u3002", "conclusion": "Ring-lite\u901a\u8fc7C3PO\u65b9\u6cd5\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3aMoE\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u8bba\u6587\u5c06\u53d1\u5e03\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "paper_title_zh": "Ring-lite\uff1a\u57fa\u4e8eC3PO\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u6269\u5c55\u63a8\u7406", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86Ring-lite\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u516c\u5f00\u7684Ling-lite\u6a21\u578b\uff08168\u4ebf\u53c2\u6570\uff0c\u6fc0\u6d3b27.5\u4ebf\u53c2\u6570\uff09\uff0c\u5728AIME\u3001LiveCodeBench\u548cGPQA-Diamond\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5ab2\u7f8e\u5c0f\u578b\u63a8\u7406\u6a21\u578b\uff0c\u4f46\u4ec5\u9700\u6fc0\u6d3b\u540c\u7c7b\u6a21\u578b\u4e09\u5206\u4e4b\u4e00\u7684\u53c2\u6570\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u84b8\u998f\u4e0eRL\u7684\u8054\u5408\u8bad\u7ec3\u6d41\u7a0b\uff0c\u63ed\u793a\u4e86MoE RL\u8bad\u7ec3\u4e2d\u672a\u8bb0\u5f55\u7684\u6311\u6218\u3002\u9996\u5148\uff0c\u6211\u4eec\u53d1\u73b0RL\u8bad\u7ec3\u4e2d\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684C3PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u541e\u5410\u91cf\u3002\u5176\u6b21\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u71b5\u635f\u5931\u9009\u62e9\u84b8\u998f\u68c0\u67e5\u70b9\u7528\u4e8eRL\u8bad\u7ec3\uff0c\u800c\u975e\u9a8c\u8bc1\u6307\u6807\uff0c\u80fd\u5728\u540e\u7eedRL\u8bad\u7ec3\u4e2d\u83b7\u5f97\u66f4\u4f18\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u534f\u8c03\u591a\u9886\u57df\u6570\u636e\u6574\u5408\uff0c\u89e3\u51b3\u4e86\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u4e2d\u7684\u9886\u57df\u51b2\u7a81\u95ee\u9898\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2506.14583", "pdf": "https://arxiv.org/pdf/2506.14583", "abs": "https://arxiv.org/abs/2506.14583", "authors": ["Krishna Sahukara", "Zineddine Bettouche", "Andreas Fischer"], "title": "Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLaTeX\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b\u591a\u6837\u5316\u8868\u683c\u5e03\u5c40\u7684\u5408\u6210\u6587\u6863\u56fe\u50cf\uff0c\u4ee5\u589e\u5f3a\u771f\u5b9e\u6570\u636e\u96c6Marmot\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684TableNet\u5728\u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728Marmot\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u6216\u626b\u63cf\u4eea\u6355\u83b7\u7684\u6587\u6863\u9875\u9762\u901a\u5e38\u5305\u542b\u8868\u683c\uff0c\u4f46\u624b\u52a8\u63d0\u53d6\u901f\u5ea6\u6162\u4e14\u6613\u51fa\u9519\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u8868\u683c\u5e03\u5c40\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u8868\u683c\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eLaTeX\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u80fd\u591f\u5408\u6210\u5177\u6709\u591a\u6837\u5316\u8868\u683c\u5e03\u5c40\u7684\u53cc\u680f\u6587\u6863\u56fe\u50cf\uff0c\u5e76\u751f\u6210\u5bf9\u9f50\u7684\u771f\u5b9e\u63a9\u7801\u3002\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u7528\u4e8e\u589e\u5f3a\u771f\u5b9e\u6570\u636e\u96c6Marmot\uff0c\u5e76\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30TableNet\u7684\u6027\u80fd\u3002", "result": "\u5728\u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\uff0cTableNet\u5728256x256\u5206\u8fa8\u7387\u4e0b\u7684\u50cf\u7d20\u7ea7XOR\u8bef\u5dee\u4e3a4.04%\uff0c\u57281024x1024\u5206\u8fa8\u7387\u4e0b\u4e3a4.33%\u3002\u5728Marmot\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u4f73\u6027\u80fd\u4e3a9.18%\uff08256x256\u5206\u8fa8\u7387\uff09\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u52a8\u5316\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0cTableNet\u5728\u8868\u683c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u4e3a\u6587\u6863\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5408\u6210\u6570\u636e\u589e\u5f3a\u7528\u4e8e\u8868\u683c\u68c0\u6d4b\uff1a\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u6587\u6863\u56fe\u50cf\u91cd\u65b0\u8bc4\u4f30TableNet\u7684\u6027\u80fd", "abstract_zh": "\u667a\u80fd\u624b\u673a\u6216\u626b\u63cf\u4eea\u6355\u83b7\u7684\u6587\u6863\u9875\u9762\u901a\u5e38\u5305\u542b\u8868\u683c\uff0c\u4f46\u624b\u52a8\u63d0\u53d6\u901f\u5ea6\u6162\u4e14\u6613\u51fa\u9519\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLaTeX\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u80fd\u591f\u5408\u6210\u5177\u6709\u591a\u6837\u5316\u8868\u683c\u5e03\u5c40\u7684\u53cc\u680f\u6587\u6863\u56fe\u50cf\uff0c\u5e76\u751f\u6210\u5bf9\u9f50\u7684\u771f\u5b9e\u63a9\u7801\u3002\u751f\u6210\u7684\u8bed\u6599\u5e93\u589e\u5f3a\u4e86\u771f\u5b9e\u6570\u636e\u96c6Marmot\uff0c\u5e76\u652f\u6301\u5bf9TableNet\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u8fa8\u7387\u7814\u7a76\u3002\u5728\u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\uff0cTableNet\u5728256x256\u5206\u8fa8\u7387\u4e0b\u7684\u50cf\u7d20\u7ea7XOR\u8bef\u5dee\u4e3a4.04%\uff0c\u57281024x1024\u5206\u8fa8\u7387\u4e0b\u4e3a4.33%\u3002\u5728Marmot\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u4f73\u6027\u80fd\u4e3a9.18%\uff08256x256\u5206\u8fa8\u7387\uff09\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u52a8\u5316\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2506.13787", "pdf": "https://arxiv.org/pdf/2506.13787", "abs": "https://arxiv.org/abs/2506.13787", "authors": ["Yanjun Dai", "Haoyang Feng", "Yuan Gao"], "title": "Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u65f6\u5e8f\u5c42\u6b21\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DTH-GNN\uff09\uff0c\u901a\u8fc7\u591a\u901a\u9053\u7279\u5f81\u63d0\u53d6\u548c\u5c42\u6b21\u5f02\u6784\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533f\u540d\u7528\u6237\u5e7f\u544a\u53cd\u9988\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u6a21\u578b\u96be\u4ee5\u6355\u6349\u533f\u540d\u7528\u6237\u4ea4\u4e92\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u65f6\u5e8f\u3001\u8bed\u4e49\u548c\u9ad8\u9636\u4f9d\u8d56\u7279\u5f81\uff0c\u65e0\u6cd5\u63cf\u8ff0\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u3002", "method": "DTH-GNN\u91c7\u7528\u65f6\u5e8f\u8fb9\u5206\u89e3\u5c06\u4ea4\u4e92\u5206\u4e3a\u77ed\u65f6\u7a81\u53d1\u3001\u663c\u591c\u5468\u671f\u548c\u957f\u65f6\u8bb0\u5fc6\u4e09\u901a\u9053\uff0c\u5e76\u5229\u7528\u5e76\u884c\u6b8b\u5dee\u5377\u79ef\u6838\u63d0\u53d6\u7279\u5f81\uff1b\u901a\u8fc7\u5143\u8def\u5f84\u6761\u4ef6Transformer\u7f16\u7801\u5668\u5b9e\u73b0\u5c42\u6b21\u5f02\u6784\u805a\u5408\uff0c\u7ed3\u5408\u8de8\u901a\u9053\u81ea\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u5173\u7cfb\u9009\u62e9\u5668\u6291\u5236\u566a\u58f0\uff1b\u63d0\u51fa\u53cd\u9988\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u4f18\u5316\u8282\u70b9\u8868\u793a\u3002", "result": "DTH-GNN\u7684AUC\u63d0\u53478.2%\uff0c\u5bf9\u6570\u635f\u5931\u964d\u4f4e5.7%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "DTH-GNN\u901a\u8fc7\u591a\u901a\u9053\u65f6\u5e8f\u5efa\u6a21\u548c\u5c42\u6b21\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533f\u540d\u7528\u6237\u5e7f\u544a\u53cd\u9988\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u5e7f\u544a\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u533f\u540d\u7528\u6237\u4ea4\u4e92\u5173\u7cfb\u5206\u6790\u4e0e\u5e7f\u544a\u53cd\u9988\u9884\u6d4b", "abstract_zh": "\u5728\u7ebf\u5e7f\u544a\u9ad8\u5ea6\u4f9d\u8d56\u533f\u540d\u7528\u6237\u7684\u9690\u5f0f\u4ea4\u4e92\u7f51\u7edc\u8fdb\u884c\u53c2\u4e0e\u5ea6\u63a8\u65ad\u548c\u6295\u653e\u7b56\u7565\u4f18\u5316\uff0c\u4f46\u73b0\u6709\u56fe\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8fd9\u4e9b\u4ea4\u4e92\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u65f6\u5e8f\u3001\u8bed\u4e49\u548c\u9ad8\u9636\u4f9d\u8d56\u7279\u5f81\uff0c\u65e0\u6cd5\u63cf\u8ff0\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\u3002\u672c\u6587\u63d0\u51fa\u89e3\u8026\u65f6\u5e8f\u5c42\u6b21\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DTH-GNN\uff09\uff0c\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a\u9996\u5148\uff0c\u5f15\u5165\u65f6\u5e8f\u8fb9\u5206\u89e3\uff0c\u5c06\u4ea4\u4e92\u5206\u4e3a\u77ed\u65f6\u7a81\u53d1\u3001\u663c\u591c\u5468\u671f\u548c\u957f\u65f6\u8bb0\u5fc6\u4e09\u901a\u9053\uff0c\u5229\u7528\u5e76\u884c\u6b8b\u5dee\u5377\u79ef\u6838\u63d0\u53d6\u7279\u5f81\uff1b\u5176\u6b21\uff0c\u6784\u5efa\u5c42\u6b21\u5f02\u6784\u805a\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u5143\u8def\u5f84\u6761\u4ef6Transformer\u7f16\u7801\u5668\u7ed3\u5408\u7528\u6237-\u7528\u6237\u3001\u7528\u6237-\u5e7f\u544a\u3001\u5e7f\u544a-\u5e7f\u544a\u5b50\u56fe\uff0c\u5e76\u5229\u7528\u8de8\u901a\u9053\u81ea\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u5173\u7cfb\u9009\u62e9\u5668\u52a8\u6001\u6291\u5236\u566a\u58f0\u7ed3\u6784\uff1b\u7b2c\u4e09\uff0c\u63d0\u51fa\u53cd\u9988\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u6700\u5927\u5316\u5404\u65f6\u95f4\u7247\u4e00\u81f4\u6027\uff0c\u4f18\u5316\u53cc\u89c6\u56fe\u76ee\u6807\u63a7\u5236\u66dd\u5149\u4fe1\u606f\u71b5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7b56\u7565\u68af\u5ea6\u5c42\u548c\u5ef6\u8fdf\u53d8\u6362\u4fe1\u53f7\u5fae\u8c03\u8282\u70b9\u8868\u793a\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDTH-GNN\u7684AUC\u63d0\u53478.2%\uff0c\u5bf9\u6570\u635f\u5931\u964d\u4f4e5.7%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2506.14758", "pdf": "https://arxiv.org/pdf/2506.14758", "abs": "https://arxiv.org/abs/2506.14758", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "title": "Reasoning with Exploration: An Entropy Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7814\u7a76\u71b5\u4e0e\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u6027\u63a8\u7406\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u4f18\u52bf\u51fd\u6570\u6539\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u504f\u5411\u5229\u7528\u800c\u975e\u63a2\u7d22\uff0c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u71b5\u4fe1\u53f7\u91cd\u65b0\u5ba1\u89c6\u63a2\u7d22\u6027\u63a8\u7406\uff0c\u4ee5\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\u9ad8\u71b5\u533a\u57df\u4e0e\u63a2\u7d22\u6027\u63a8\u7406\u884c\u4e3a\uff08\u5982\u5173\u952e\u6807\u8bb0\u3001\u81ea\u6211\u9a8c\u8bc1\u548c\u7f55\u89c1\u884c\u4e3a\uff09\u5f3a\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6539\u8fdb\uff1a\u5728\u4f18\u52bf\u51fd\u6570\u4e2d\u589e\u52a0\u57fa\u4e8e\u71b5\u7684\u9879\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Pass@K\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5373\u4f7fK\u503c\u6781\u5927\u65f6\u4ecd\u6709\u6548\uff0c\u62d3\u5c55\u4e86\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u8fb9\u754c\u3002", "conclusion": "\u901a\u8fc7\u71b5\u9a71\u52a8\u7684\u63a2\u7d22\u6027\u63a8\u7406\u6539\u8fdb\uff0c\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u957f\u3001\u66f4\u6df1\u7684\u63a8\u7406\u94fe\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u71b5\u7684\u63a2\u7d22\u6027\u63a8\u7406", "abstract_zh": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u662f\u4e00\u4e2a\u6838\u5fc3\u76ee\u6807\u3002\u5c3d\u7ba1\u8fd1\u671f\u5728\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u591a\u6570\u65b9\u6cd5\u504f\u5411\u5229\u7528\uff0c\u9010\u6e10\u906d\u9047\u6027\u80fd\u74f6\u9888\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u71b5\u2014\u2014\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u4fe1\u53f7\uff0c\u5e76\u7814\u7a76\u5176\u4e0e\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u6027\u63a8\u7406\u7684\u5173\u7cfb\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u9ad8\u71b5\u533a\u57df\u4e0e\u4e09\u7c7b\u63a2\u7d22\u6027\u63a8\u7406\u884c\u4e3a\u5448\u5f3a\u6b63\u76f8\u5173\uff1a\uff081\uff09\u51b3\u5b9a\u6216\u8fde\u63a5\u903b\u8f91\u6b65\u9aa4\u7684\u5173\u952e\u6807\u8bb0\uff0c\uff082\uff09\u81ea\u6211\u9a8c\u8bc1\u548c\u4fee\u6b63\u7b49\u53cd\u601d\u884c\u4e3a\uff0c\uff083\uff09\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u672a\u5145\u5206\u63a2\u7d22\u7684\u7f55\u89c1\u884c\u4e3a\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u5bf9\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7684\u6781\u5c0f\u6539\u8fdb\uff1a\u5728\u4f18\u52bf\u51fd\u6570\u4e2d\u589e\u52a0\u57fa\u4e8e\u71b5\u7684\u9879\u3002\u4e0e\u4f20\u7edf\u6700\u5927\u71b5\u65b9\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u4e0d\u786e\u5b9a\u6027\u9f13\u52b1\u63a2\u7d22\u4e0d\u540c\uff0c\u6211\u4eec\u901a\u8fc7\u4fc3\u8fdb\u66f4\u957f\u3001\u66f4\u6df1\u7684\u63a8\u7406\u94fe\u6765\u9f13\u52b1\u63a2\u7d22\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u5728\u6781\u5927K\u503c\u4e0b\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728Pass@K\u6307\u6807\uff08\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e0a\u9650\u4f30\u8ba1\u5668\uff09\u4e0a\u4ecd\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u62d3\u5c55\u4e86\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u8fb9\u754c\u3002"}}
{"id": "2506.14596", "pdf": "https://arxiv.org/pdf/2506.14596", "abs": "https://arxiv.org/abs/2506.14596", "authors": ["Ming Xu", "Xu Zhang"], "title": "PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.", "AI": {"tldr": "PoseGRAF\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u5f3a\u5316\u7684\u81ea\u9002\u5e94\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u76ee3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u53cc\u56fe\u5377\u79ef\u7ed3\u6784\u548c\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u6355\u6349\u5173\u8282\u4e0e\u9aa8\u9abc\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5355\u76ee3D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5173\u8282\u4f4d\u7f6e\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u9aa8\u9abc\u5185\u5728\u7684\u65b9\u5411\u548c\u89d2\u5ea6\u5173\u8054\uff0c\u5bfc\u81f4\u5728\u5173\u8282\u906e\u6321\u6216\u5feb\u901f\u8fd0\u52a8\u65f6\u751f\u6210\u4e0d\u5408\u7406\u7684\u59ff\u6001\u3002PoseGRAF\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u5f3a\u5316\u548c\u81ea\u9002\u5e94\u878d\u5408\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PoseGRAF\u6784\u5efa\u4e86\u53cc\u56fe\u5377\u79ef\u7ed3\u6784\uff0c\u5206\u522b\u5904\u7406\u5173\u8282\u56fe\u548c\u9aa8\u9abc\u56fe\uff0c\u6355\u6349\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff1b\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u5efa\u6a21\u9aa8\u9abc\u65b9\u5411\u4e0e\u5173\u8282\u7279\u5f81\u7684\u76f8\u4e92\u4f9d\u8d56\uff1b\u8bbe\u8ba1\u52a8\u6001\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u6574\u5408\u4e24\u79cd\u7279\u5f81\uff1b\u6700\u540e\u901a\u8fc7\u6539\u8fdb\u7684Transformer\u7f16\u7801\u5668\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPoseGRAF\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u573a\u666f\u89c6\u9891\u4e2d\u7684\u989d\u5916\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PoseGRAF\u901a\u8fc7\u51e0\u4f55\u5f3a\u5316\u548c\u81ea\u9002\u5e94\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "PoseGRAF\uff1a\u57fa\u4e8e\u51e0\u4f55\u5f3a\u5316\u7684\u81ea\u9002\u5e94\u878d\u5408\u5355\u76ee3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1", "abstract_zh": "\u73b0\u6709\u7684\u5355\u76ee3D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5173\u8282\u4f4d\u7f6e\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u4e86\u9aa8\u9abc\u5185\u5728\u7684\u65b9\u5411\u548c\u89d2\u5ea6\u5173\u8054\uff0c\u5bfc\u81f4\u5728\u5173\u8282\u906e\u6321\u6216\u5feb\u901f\u8fd0\u52a8\u65f6\u751f\u6210\u4e0d\u5408\u7406\u7684\u59ff\u6001\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PoseGRAF\u6846\u67b6\u3002\u9996\u5148\u6784\u5efa\u53cc\u56fe\u5377\u79ef\u7ed3\u6784\uff0c\u5206\u522b\u5904\u7406\u5173\u8282\u56fe\u548c\u9aa8\u9abc\u56fe\uff0c\u6709\u6548\u6355\u6349\u5176\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff1b\u968f\u540e\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u5efa\u6a21\u9aa8\u9abc\u65b9\u5411\u4e0e\u5173\u8282\u7279\u5f81\u7684\u76f8\u4e92\u4f9d\u8d56\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u52a8\u6001\u878d\u5408\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u6574\u5408\u4e24\u79cd\u7279\u5f81\uff1b\u6700\u540e\u901a\u8fc7\u6539\u8fdb\u7684Transformer\u7f16\u7801\u5668\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u573a\u666f\u89c6\u9891\u4e2d\u7684\u989d\u5916\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u4e8ehttps://github.com/iCityLab/PoseGRAF\u3002"}}
{"id": "2506.14761", "pdf": "https://arxiv.org/pdf/2506.14761", "abs": "https://arxiv.org/abs/2506.14761", "authors": ["Mathurin Videau", "Badr Youbi Idrissi", "Alessandro Leite", "Marc Schoenauer", "Olivier Teytaud", "David Lopez-Paz"], "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52U-Net\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u5d4c\u5165\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u8bcd\u65b9\u6cd5\uff08\u5982BPE\uff09\u7684\u56fa\u5b9a\u7c92\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u5c3a\u5ea6\u5e8f\u5217\u5904\u7406\uff0c\u5e76\u5728\u4e0d\u540c\u5c42\u6b21\u4e0a\u9884\u6d4b\u672a\u6765\u5185\u5bb9\u3002", "motivation": "\u4f20\u7edf\u5206\u8bcd\u65b9\u6cd5\uff08\u5982BPE\uff09\u5bf9\u8f93\u5165\u6587\u672c\u65bd\u52a0\u56fa\u5b9a\u7684\u7c92\u5ea6\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u64cd\u4f5c\u65b9\u5f0f\u548c\u9884\u6d4b\u8303\u56f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u5d4c\u5165\u6807\u8bb0\uff0c\u6253\u7834\u8fd9\u79cd\u521a\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u7075\u6d3b\u5904\u7406\u4e0d\u540c\u7c92\u5ea6\u7684\u6587\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52U-Net\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ece\u539f\u59cb\u5b57\u8282\u5f00\u59cb\uff0c\u9010\u6b65\u5c06\u5b57\u8282\u6c60\u5316\u4e3a\u5355\u8bcd\u3001\u5355\u8bcd\u5bf9\u4ee5\u53ca\u6700\u591a4\u4e2a\u5355\u8bcd\u7684\u7ec4\u5408\uff0c\u4ece\u800c\u83b7\u5f97\u5e8f\u5217\u7684\u591a\u5c3a\u5ea6\u89c6\u56fe\u3002\u6df1\u5c42\u9636\u6bb5\u4e13\u6ce8\u4e8e\u66f4\u5e7f\u6cdb\u7684\u8bed\u4e49\u6a21\u5f0f\uff0c\u800c\u6d45\u5c42\u9636\u6bb5\u5904\u7406\u7ec6\u8282\u3002", "result": "\u5728\u7cbe\u5fc3\u8c03\u6574\u9884\u8bad\u7ec3\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u6d45\u5c42\u6b21\u7ed3\u6784\u4e0e\u5f3aBPE\u57fa\u7ebf\u76f8\u5f53\uff0c\u800c\u6df1\u5c42\u6b21\u7ed3\u6784\u663e\u793a\u51fa\u826f\u597d\u7684\u8d8b\u52bf\u3002\u7531\u4e8e\u5206\u8bcd\u5d4c\u5165\u6a21\u578b\u5185\u90e8\uff0c\u540c\u4e00\u7cfb\u7edf\u53ef\u4ee5\u5904\u7406\u5b57\u7b26\u7ea7\u4efb\u52a1\u5e76\u8de8\u4f4e\u8d44\u6e90\u8bed\u8a00\u4f20\u9012\u77e5\u8bc6\u3002", "conclusion": "\u52a8\u6001\u5d4c\u5165\u6807\u8bb0\u7684\u81ea\u56de\u5f52U-Net\u6a21\u578b\u80fd\u591f\u7075\u6d3b\u5904\u7406\u591a\u5c3a\u5ea6\u6587\u672c\uff0c\u5e76\u5728\u4e0d\u540c\u5c42\u6b21\u4e0a\u5b9e\u73b0\u8bed\u4e49\u9884\u6d4b\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u5206\u8bcd\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u4ece\u5b57\u8282\u5230\u601d\u60f3\uff1a\u57fa\u4e8e\u81ea\u56de\u5f52U-Net\u7684\u8bed\u8a00\u5efa\u6a21", "abstract_zh": "\u5206\u8bcd\u5bf9\u8f93\u5165\u6587\u672c\u65bd\u52a0\u4e86\u56fa\u5b9a\u7684\u7c92\u5ea6\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u6570\u636e\u7684\u64cd\u4f5c\u65b9\u5f0f\u548c\u672a\u6765\u9884\u6d4b\u7684\u8303\u56f4\u3002\u5b57\u8282\u5bf9\u7f16\u7801\uff08BPE\uff09\u7b49\u65b9\u6848\u5c06\u6587\u672c\u5206\u5272\u4e00\u6b21\uff0c\u6784\u5efa\u9759\u6001\u8bcd\u6c47\u8868\uff0c\u4f7f\u6a21\u578b\u65e0\u6cd5\u6446\u8131\u8fd9\u4e00\u9009\u62e9\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u81ea\u56de\u5f52U-Net\u6a21\u578b\u6765\u7f13\u89e3\u8fd9\u79cd\u521a\u6027\uff0c\u8be5\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u5d4c\u5165\u81ea\u5df1\u7684\u6807\u8bb0\u3002\u7f51\u7edc\u8bfb\u53d6\u539f\u59cb\u5b57\u8282\uff0c\u5c06\u5176\u6c60\u5316\u4e3a\u5355\u8bcd\u3001\u5355\u8bcd\u5bf9\uff0c\u6700\u591a4\u4e2a\u5355\u8bcd\u7684\u7ec4\u5408\uff0c\u4ece\u800c\u83b7\u5f97\u5e8f\u5217\u7684\u591a\u5c3a\u5ea6\u89c6\u56fe\u3002\u5728\u66f4\u6df1\u5c42\u6b21\u9636\u6bb5\uff0c\u6a21\u578b\u9700\u8981\u9884\u6d4b\u66f4\u8fdc\u7684\u672a\u6765\u2014\u2014\u9884\u6d4b\u63a5\u4e0b\u6765\u7684\u51e0\u4e2a\u5355\u8bcd\u800c\u975e\u4e0b\u4e00\u4e2a\u5b57\u8282\u2014\u2014\u56e0\u6b64\u6df1\u5c42\u9636\u6bb5\u4e13\u6ce8\u4e8e\u66f4\u5e7f\u6cdb\u7684\u8bed\u4e49\u6a21\u5f0f\uff0c\u800c\u6d45\u5c42\u9636\u6bb5\u5904\u7406\u7ec6\u8282\u3002\u5728\u7cbe\u5fc3\u8c03\u6574\u548c\u63a7\u5236\u9884\u8bad\u7ec3\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u6d45\u5c42\u6b21\u7ed3\u6784\u4e0e\u5f3aBPE\u57fa\u7ebf\u76f8\u5f53\uff0c\u800c\u6df1\u5c42\u6b21\u7ed3\u6784\u663e\u793a\u51fa\u826f\u597d\u7684\u8d8b\u52bf\u3002\u7531\u4e8e\u5206\u8bcd\u73b0\u5728\u5d4c\u5165\u6a21\u578b\u5185\u90e8\uff0c\u540c\u4e00\u7cfb\u7edf\u53ef\u4ee5\u5904\u7406\u5b57\u7b26\u7ea7\u4efb\u52a1\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u4f20\u9012\u77e5\u8bc6\u3002"}}
{"id": "2506.14603", "pdf": "https://arxiv.org/pdf/2506.14603", "abs": "https://arxiv.org/abs/2506.14603", "authors": ["Amirmojtaba Sabour", "Sanja Fidler", "Karsten Kreis"], "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/", "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAlign Your Flow\u7684\u8fde\u7eed\u65f6\u95f4\u6d41\u56fe\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u76ee\u6807\u548c\u5f15\u5165\u65b0\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e00\u81f4\u6027\u6a21\u578b\u5728\u6b65\u6570\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6548\u679c\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u548c\u6d41\u6a21\u578b\u662f\u76ee\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u4f46\u9700\u8981\u5927\u91cf\u91c7\u6837\u6b65\u9aa4\u3002\u4e00\u81f4\u6027\u6a21\u578b\u53ef\u4ee5\u5c06\u5176\u84b8\u998f\u4e3a\u9ad8\u6548\u7684\u4e00\u6b65\u751f\u6210\u5668\uff0c\u4f46\u5728\u6b65\u6570\u589e\u52a0\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6d41\u56fe\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5c11\u6b65\u751f\u6210\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u8fde\u7eed\u65f6\u95f4\u8bad\u7ec3\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u65b0\u6280\u672f\uff08\u5982\u81ea\u52a8\u5f15\u5bfc\u548c\u5bf9\u6297\u5fae\u8c03\uff09\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u4e00\u81f4\u6027\u548c\u6d41\u5339\u914d\u76ee\u6807\u3002\u6d41\u56fe\u65b9\u6cd5\u80fd\u591f\u5728\u4efb\u4f55\u4e24\u4e2a\u566a\u58f0\u7ea7\u522b\u4e4b\u95f4\u5355\u6b65\u8fde\u63a5\uff0c\u4e14\u5728\u6240\u6709\u6b65\u6570\u4e0b\u5747\u6709\u6548\u3002", "result": "\u5728ImageNet 64x64\u548c512x512\u7b49\u6311\u6218\u6027\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAlign Your Flow\u6a21\u578b\u53d6\u5f97\u4e86\u5c11\u6b65\u751f\u6210\u7684\u9886\u5148\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6837\u672c\u591a\u6837\u6027\u3002\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u975e\u5bf9\u6297\u8bad\u7ec3\u7684\u5c11\u6b65\u91c7\u6837\u5668\u3002", "conclusion": "Align Your Flow\u901a\u8fc7\u6d41\u56fe\u84b8\u998f\u548c\u65b0\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6b65\u751f\u6210\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u5bf9\u9f50\u4f60\u7684\u6d41\uff1a\u6269\u5c55\u8fde\u7eed\u65f6\u95f4\u6d41\u56fe\u84b8\u998f", "abstract_zh": "\u6269\u6563\u6a21\u578b\u548c\u6d41\u6a21\u578b\u5df2\u6210\u4e3a\u6700\u5148\u8fdb\u7684\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u91c7\u6837\u6b65\u9aa4\u3002\u4e00\u81f4\u6027\u6a21\u578b\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u6a21\u578b\u84b8\u998f\u4e3a\u9ad8\u6548\u7684\u4e00\u6b65\u751f\u6210\u5668\uff1b\u7136\u800c\uff0c\u4e0e\u6d41\u548c\u6269\u6563\u65b9\u6cd5\u4e0d\u540c\uff0c\u5176\u6027\u80fd\u5728\u6b65\u6570\u589e\u52a0\u65f6\u4e0d\u53ef\u907f\u514d\u5730\u4e0b\u964d\uff0c\u6211\u4eec\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002\u6d41\u56fe\u65b9\u6cd5\u901a\u8fc7\u5355\u6b65\u8fde\u63a5\u4efb\u610f\u4e24\u4e2a\u566a\u58f0\u7ea7\u522b\uff0c\u5e76\u5728\u6240\u6709\u6b65\u6570\u4e0b\u4fdd\u6301\u6709\u6548\u6027\uff0c\u4ece\u800c\u63a8\u5e7f\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u8fde\u7eed\u65f6\u95f4\u8bad\u7ec3\u76ee\u6807\uff0c\u4ee5\u53ca\u989d\u5916\u7684\u521b\u65b0\u8bad\u7ec3\u6280\u672f\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u4e00\u81f4\u6027\u548c\u6d41\u5339\u914d\u76ee\u6807\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\uff0c\u81ea\u52a8\u5f15\u5bfc\u53ef\u4ee5\u901a\u8fc7\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4f4e\u8d28\u91cf\u6a21\u578b\u8fdb\u884c\u6307\u5bfc\u6765\u63d0\u5347\u6027\u80fd\uff0c\u800c\u5bf9\u6297\u5fae\u8c03\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u6548\u679c\uff0c\u540c\u65f6\u6837\u672c\u591a\u6837\u6027\u635f\u5931\u6781\u5c0f\u3002\u6211\u4eec\u5e7f\u6cdb\u9a8c\u8bc1\u4e86\u540d\u4e3aAlign Your Flow\u7684\u6d41\u56fe\u6a21\u578b\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u5c11\u6b65\u751f\u6210\u6027\u80fd\uff0c\u5305\u62ecImageNet 64x64\u548c512x512\uff0c\u4e14\u4f7f\u7528\u5c0f\u578b\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u6587\u672c\u6761\u4ef6\u5408\u6210\u4e2d\uff0c\u6587\u672c\u5230\u56fe\u50cf\u6d41\u56fe\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684\u975e\u5bf9\u6297\u8bad\u7ec3\u7684\u5c11\u6b65\u91c7\u6837\u5668\u3002"}}
{"id": "2506.13798", "pdf": "https://arxiv.org/pdf/2506.13798", "abs": "https://arxiv.org/abs/2506.13798", "authors": ["Roger Brent", "T. Greg McKelvey"], "title": "Contemporary AI foundation models increase biological weapons risk", "categories": ["cs.CY", "cs.AI"], "comment": "58 pages, 10 figures, 4 tables", "summary": "The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify \"elements of success\" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.", "AI": {"tldr": "\u5f53\u4ee3AI\u57fa\u7840\u6a21\u578b\u589e\u52a0\u4e86\u751f\u7269\u6b66\u5668\u98ce\u9669\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4f4e\u4f30\u4e86\u8fd9\u79cd\u98ce\u9669\uff0c\u56e0\u4e3a\u5047\u8bbe\u548c\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u7ea7AI\u6a21\u578b\u53ef\u4ee5\u6307\u5bfc\u7528\u6237\u5b8c\u6210\u590d\u6742\u751f\u7269\u6280\u672f\u4efb\u52a1\uff0c\u5982\u5408\u6210\u810a\u9ad3\u7070\u8d28\u708e\u75c5\u6bd2\uff0c\u547c\u5401\u6539\u8fdb\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u80fd\u4fc3\u8fdb\u751f\u7269\u6b66\u5668\u5f00\u53d1\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4f4e\u4f30\u4e86\u98ce\u9669\uff0c\u4e3b\u8981\u7531\u4e8e\u9519\u8bef\u7684\u5047\u8bbe\u548c\u4e0d\u5145\u5206\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65e0\u6b63\u5f0f\u4e13\u4e1a\u77e5\u8bc6\u4f46\u6210\u529f\u5b8c\u6210\u590d\u6742\u6280\u672f\u4efb\u52a1\u7684\u6848\u4f8b\uff0c\u4ee5\u53ca\u5ba1\u67e5\u75c5\u539f\u4f53\u6784\u5efa\u8fc7\u7a0b\u7684\u6587\u6863\u5316\uff0c\u9a8c\u8bc1AI\u6a21\u578b\u53ef\u4ee5\u63cf\u8ff0\u751f\u7269\u6b66\u5668\u5f00\u53d1\u7684\u5173\u952e\u6b65\u9aa4\u3002\u6d4b\u8bd5\u4e86Llama 3.1 405B\u3001ChatGPT-4o\u548cClaude 3.5 Sonnet\u7b49\u6a21\u578b\u5728\u6307\u5bfc\u5408\u6210\u810a\u9ad3\u7070\u8d28\u708e\u75c5\u6bd2\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u7ea7AI\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6307\u5bfc\u7528\u6237\u4ece\u5546\u4e1a\u5408\u6210DNA\u4e2d\u6062\u590d\u6d3b\u810a\u9ad3\u7070\u8d28\u708e\u75c5\u6bd2\uff0c\u53cd\u9a73\u4e86\u5f53\u524d\u6a21\u578b\u5bf9\u751f\u7269\u5b89\u5168\u98ce\u9669\u8f83\u5c0f\u7684\u89c2\u70b9\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u6539\u8fdb\u8bc4\u4f30\u6807\u51c6\uff0c\u4f46\u53ef\u80fd\u5df2\u9519\u8fc7\u6709\u6548\u5b9e\u65bd\u7684\u65f6\u95f4\u7a97\u53e3\u3002", "paper_title_zh": "\u5f53\u4ee3AI\u57fa\u7840\u6a21\u578b\u589e\u52a0\u751f\u7269\u6b66\u5668\u98ce\u9669", "abstract_zh": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u80fd\u4fc3\u8fdb\u751f\u7269\u6b66\u5668\u5f00\u53d1\u7684\u62c5\u5fe7\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u73b0\u6709\u5bf9\u5f53\u4ee3AI\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u8bc4\u4f30\u4f4e\u4f30\u4e86\u8fd9\u4e00\u98ce\u9669\uff0c\u4e3b\u8981\u7531\u4e8e\u9519\u8bef\u7684\u5047\u8bbe\u548c\u4e0d\u5145\u5206\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u8bc4\u4f30\u9519\u8bef\u5730\u5047\u8bbe\u751f\u7269\u6b66\u5668\u5f00\u53d1\u9700\u8981\u9690\u6027\u77e5\u8bc6\uff0c\u5373\u901a\u8fc7\u5b9e\u8df5\u7ecf\u9a8c\u83b7\u5f97\u4e14\u96be\u4ee5\u8a00\u4f20\u7684\u6280\u80fd\u3002\u5176\u6b21\uff0c\u5b83\u4eec\u4f9d\u8d56\u4e0d\u5b8c\u5584\u7684\u57fa\u51c6\uff0c\u5ffd\u89c6\u4e86AI\u5982\u4f55\u63d0\u5347\u975e\u4e13\u4e1a\u4eba\u58eb\u548c\u5df2\u6709\u6280\u80fd\u8005\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u6311\u6218\u9690\u6027\u77e5\u8bc6\u5047\u8bbe\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u65e0\u6b63\u5f0f\u4e13\u4e1a\u77e5\u8bc6\u7684\u4e2a\u4f53\uff08\u59822011\u5e74\u632a\u5a01\u6781\u7aef\u6c11\u65cf\u4e3b\u4e49\u8005\u6210\u529f\u5408\u6210\u7206\u70b8\u7269\uff09\u5b8c\u6210\u590d\u6742\u6280\u672f\u4efb\u52a1\u7684\u6848\u4f8b\u3002\u6211\u4eec\u8fd8\u5ba1\u67e5\u4e86\u75c5\u539f\u4f53\u6784\u5efa\u8fc7\u7a0b\u7684\u6587\u6863\u5316\u52aa\u529b\uff0c\u5f3a\u8c03\u8fd9\u4e9b\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u4f20\u8fbe\u3002\u6211\u4eec\u786e\u5b9a\u4e86\u751f\u7269\u6b66\u5668\u5f00\u53d1\u7684\u201c\u6210\u529f\u8981\u7d20\u201d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u7528\u6587\u5b57\u63cf\u8ff0\u8fd9\u4e9b\u6b65\u9aa4\uff0c\u5305\u62ec\u83b7\u53d6\u6750\u6599\u548c\u6267\u884c\u6280\u672f\u7a0b\u5e8f\u3002\u5e94\u7528\u8fd9\u4e00\u6846\u67b6\uff0c\u6211\u4eec\u53d1\u73b0\u9ad8\u7ea7AI\u6a21\u578bLlama 3.1 405B\u3001ChatGPT-4o\u548cClaude 3.5 Sonnet\u53ef\u4ee5\u51c6\u786e\u6307\u5bfc\u7528\u6237\u4ece\u5546\u4e1a\u5408\u6210DNA\u4e2d\u6062\u590d\u6d3b\u810a\u9ad3\u7070\u8d28\u708e\u75c5\u6bd2\uff0c\u6311\u6218\u4e86\u5f53\u524d\u6a21\u578b\u5bf9\u751f\u7269\u5b89\u5168\u98ce\u9669\u8f83\u5c0f\u7684\u89c2\u70b9\u3002\u6211\u4eec\u547c\u5401\u6539\u8fdb\u57fa\u51c6\uff0c\u540c\u65f6\u627f\u8ba4\u6709\u6548\u5b9e\u65bd\u7684\u7a97\u53e3\u53ef\u80fd\u5df2\u7ecf\u5173\u95ed\u3002"}}
{"id": "2506.14767", "pdf": "https://arxiv.org/pdf/2506.14767", "abs": "https://arxiv.org/abs/2506.14767", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u7f16\u7801\u8fde\u7eed\u8bed\u97f3\u5c5e\u6027\u6765\u589e\u5f3a\u8bed\u4e49\u6807\u8bb0\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\uff0c\u907f\u514d\u4e86\u624b\u52a8\u63d0\u53d6\u7279\u5f81\u7684\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u81ea\u76d1\u7763\u6a21\u578b\u7684\u8bed\u97f3\u6807\u8bb0\uff08\u8bed\u4e49\u6807\u8bb0\uff09\u4e3b\u8981\u5173\u6ce8\u8bed\u97f3\u7684\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u4e86\u97f5\u5f8b\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8bed\u97f3\u81ea\u7136\u5ea6\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6dfb\u52a0\u97f3\u9ad8\u7279\u5f81\u6765\u6539\u8fdb\uff0c\u4f46\u97f3\u9ad8\u65e0\u6cd5\u5b8c\u5168\u4ee3\u8868\u526f\u8bed\u8a00\u5c5e\u6027\uff0c\u4e14\u9700\u8981\u624b\u52a8\u9009\u62e9\u7279\u5f81\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u81ea\u52a8\u5b66\u4e60\u7f16\u7801\u8fde\u7eed\u8bed\u97f3\u5c5e\u6027\u4ee5\u589e\u5f3a\u8bed\u4e49\u6807\u8bb0\uff0c\u65e0\u9700\u624b\u52a8\u63d0\u53d6\u6216\u9009\u62e9\u526f\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8bed\u97f3\u5728\u4eba\u7c7b\u8bc4\u5206\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u907f\u514d\u4e86\u624b\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53d8\u5206\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\uff0c\u4e3a\u8bed\u97f3\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u4e00\u79cd\u6539\u8fdb\u751f\u6210\u5f0f\u53e3\u8bed\u6a21\u578b\u81ea\u7136\u5ea6\u7684\u53d8\u5206\u6846\u67b6", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5176\u5728\u8bed\u97f3\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8bed\u97f3\u662f\u8fde\u7eed\u4e14\u590d\u6742\u7684\uff0c\u901a\u5e38\u9700\u8981\u79bb\u6563\u5316\u4ee5\u8fdb\u884c\u81ea\u56de\u5f52\u5efa\u6a21\u3002\u57fa\u4e8e\u81ea\u76d1\u7763\u6a21\u578b\u7684\u8bed\u97f3\u6807\u8bb0\uff08\u79f0\u4e3a\u8bed\u4e49\u6807\u8bb0\uff09\u901a\u5e38\u5173\u6ce8\u8bed\u97f3\u7684\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u4e86\u97f5\u5f8b\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8bed\u97f3\u81ea\u7136\u5ea6\u964d\u4f4e\u3002\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u901a\u8fc7\u4e3a\u8bed\u4e49\u6807\u8bb0\u6dfb\u52a0\u97f3\u9ad8\u7279\u5f81\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u97f3\u9ad8\u65e0\u6cd5\u5b8c\u5168\u4ee3\u8868\u526f\u8bed\u8a00\u5c5e\u6027\u7684\u8303\u56f4\uff0c\u4e14\u9009\u62e9\u5408\u9002\u7684\u7279\u5f81\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u81ea\u52a8\u5b66\u4e60\u7f16\u7801\u8fd9\u4e9b\u8fde\u7eed\u8bed\u97f3\u5c5e\u6027\u4ee5\u589e\u5f3a\u8bed\u4e49\u6807\u8bb0\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u624b\u52a8\u63d0\u53d6\u548c\u9009\u62e9\u526f\u8bed\u8a00\u7279\u5f81\uff0c\u4e14\u751f\u6210\u7684\u8bed\u97f3\u5728\u4eba\u7c7b\u8bc4\u5206\u4e2d\u66f4\u53d7\u9752\u7750\u3002\u4ee3\u7801\u3001\u6837\u672c\u548c\u6a21\u578b\u53ef\u5728https://github.com/b04901014/vae-gslm\u83b7\u53d6\u3002"}}
{"id": "2506.14605", "pdf": "https://arxiv.org/pdf/2506.14605", "abs": "https://arxiv.org/abs/2506.14605", "authors": ["Giacomo Meanti", "Thomas Ryckeboer", "Michael Arbel", "Julien Mairal"], "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Code available at https://github.com/inria-thoth/ddm4ip", "summary": "This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u914d\u5bf9\u6570\u636e\u96c6\u7684\u56fe\u50cf\u590d\u539f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u548c\u5206\u5e03\u5339\u914d\u635f\u5931\u5b66\u4e60\u9000\u5316\u89c2\u6d4b\u5206\u5e03\u548c\u524d\u5411\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4e2d\u524d\u5411\u6a21\u578b\u672a\u77e5\u6216\u6570\u636e\u96be\u4ee5\u914d\u5bf9\u7684\u60c5\u51b5\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u590d\u539f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5b8c\u6574\u7684\u524d\u5411\u6a21\u578b\u77e5\u8bc6\u6216\u914d\u5bf9\u7684\u9000\u5316\u4e0e\u771f\u5b9e\u56fe\u50cf\u6570\u636e\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u524d\u5411\u6a21\u578b\u5f80\u5f80\u672a\u77e5\u6216\u9519\u8bef\uff0c\u4e14\u914d\u5bf9\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u4ec5\u9700\u5c11\u91cf\u65e0\u914d\u5bf9\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u6761\u4ef6\u6d41\u5339\u914d\u5efa\u6a21\u9000\u5316\u89c2\u6d4b\u7684\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5339\u914d\u635f\u5931\u81ea\u7136\u5b66\u4e60\u524d\u5411\u6a21\u578b\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u6700\u5c0f\u5316\u5047\u8bbe\uff0c\u4ec5\u4f9d\u8d56\u5c11\u91cf\u65e0\u914d\u5bf9\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53bb\u6a21\u7cca\u548c\u975e\u5747\u5300\u70b9\u6269\u6563\u51fd\u6570\u6821\u51c6\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5355\u56fe\u50cf\u76f2\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u5728\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u5728\u955c\u5934\u6821\u51c6\u8fd9\u4e00\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u65e0\u9700\u914d\u5bf9\u6570\u636e\u6216\u5b8c\u6574\u524d\u5411\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u590d\u539f\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u591a\u79cd\u4efb\u52a1\u3002", "paper_title_zh": "\u57fa\u4e8e\u6269\u6563\u5206\u5e03\u5339\u914d\u7684\u65e0\u76d1\u7763\u6210\u50cf\u9006\u95ee\u9898", "abstract_zh": "\u672c\u6587\u901a\u8fc7\u9006\u95ee\u9898\u7684\u89c6\u89d2\uff0c\u5229\u7528\u65e0\u914d\u5bf9\u6570\u636e\u96c6\u89e3\u51b3\u56fe\u50cf\u590d\u539f\u4efb\u52a1\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5b8c\u5168\u4e86\u89e3\u524d\u5411\u6a21\u578b\u6216\u80fd\u591f\u83b7\u53d6\u914d\u5bf9\u7684\u9000\u5316\u4e0e\u771f\u5b9e\u56fe\u50cf\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u65e0\u914d\u5bf9\u6570\u636e\uff0c\u9002\u7528\u4e8e\u524d\u5411\u6a21\u578b\u672a\u77e5\u6216\u9519\u8bef\u4e14\u914d\u5bf9\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u771f\u5b9e\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u5efa\u6a21\u9000\u5316\u89c2\u6d4b\u7684\u5206\u5e03\uff0c\u540c\u65f6\u901a\u8fc7\u6846\u67b6\u4e2d\u81ea\u7136\u4ea7\u751f\u7684\u5206\u5e03\u5339\u914d\u635f\u5931\u5b66\u4e60\u524d\u5411\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53bb\u6a21\u7cca\u548c\u975e\u5747\u5300\u70b9\u6269\u6563\u51fd\u6570\u6821\u51c6\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5355\u56fe\u50cf\u76f2\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u5728\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u5728\u955c\u5934\u6821\u51c6\u8fd9\u4e00\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff1a\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8017\u65f6\u5b9e\u9a8c\u548c\u4e13\u7528\u8bbe\u5907\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u4ec5\u9700\u6781\u5c11\u6570\u636e\u91c7\u96c6\u5373\u53ef\u5b9e\u73b0\u3002"}}
{"id": "2506.13800", "pdf": "https://arxiv.org/pdf/2506.13800", "abs": "https://arxiv.org/abs/2506.13800", "authors": ["Abul Ehtesham", "Aditi Singh", "Saket Kumar"], "title": "Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u63d0\u53d6\u548c\u5206\u6790\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\uff0c\u4ee5\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\uff08CDS\uff09\u3001\u51cf\u8f7b\u6587\u6863\u8d1f\u62c5\u5e76\u63d0\u5347\u60a3\u8005\u5065\u5eb7\u7d20\u517b\u3002", "motivation": "\u6570\u5b57\u5065\u5eb7\u9886\u57df\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u6587\u6863\u8d1f\u62c5\u548c\u60a3\u8005\u5065\u5eb7\u7d20\u517b\u65b9\u9762\u9762\u4e34\u6301\u7eed\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u4e92\u64cd\u4f5c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06LLMs\u4e0eHL7 FHIR\u6570\u636e\u96c6\u6210\uff0c\u5229\u7528MCP\u534f\u8bae\u5b9e\u73b0\u52a8\u6001\u6570\u636e\u63d0\u53d6\u548c\u63a8\u7406\uff0c\u652f\u6301\u5b9e\u65f6\u603b\u7ed3\u3001\u89e3\u91ca\u548c\u4e2a\u6027\u5316\u6c9f\u901a\u3002\u6846\u67b6\u57fa\u4e8eJSON\u914d\u7f6e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7528\u6237\u89d2\u8272\u3002", "result": "\u4f7f\u7528\u7b26\u5408FHIR R4\u6807\u51c6\u7684\u5408\u6210EHR\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u5de5\u4f5c\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e2a\u6027\u5316\u6570\u5b57\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u652f\u6301\u591a\u79cdFHIR\u683c\u5f0f\uff0c\u63a8\u52a8\u4e86AI\u5728EHR\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002", "paper_title_zh": "\u901a\u8fc7LLMs\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e0eEHR\u6d1e\u5bdf\uff1a\u5f00\u6e90MCP-FHIR\u6846\u67b6", "abstract_zh": "\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\uff08CDS\uff09\u3001\u51cf\u8f7b\u6587\u6863\u8d1f\u62c5\u5e76\u63d0\u5347\u60a3\u8005\u5065\u5eb7\u7d20\u517b\u662f\u6570\u5b57\u5065\u5eb7\u9886\u57df\u7684\u6301\u7eed\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0eHL7 FHIR\u6570\u636e\u96c6\u6210\uff0c\u5b9e\u73b0\u5bf9\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u7684\u52a8\u6001\u63d0\u53d6\u548c\u63a8\u7406\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u73b0\u6709\u7684MCP-FHIR\u5b9e\u73b0\uff0c\u901a\u8fc7\u57fa\u4e8eJSON\u7684\u914d\u7f6e\u5b9e\u73b0\u5bf9\u591a\u79cdFHIR\u8d44\u6e90\u7684\u58f0\u660e\u5f0f\u8bbf\u95ee\uff0c\u652f\u6301\u5b9e\u65f6\u603b\u7ed3\u3001\u89e3\u91ca\u548c\u4e2a\u6027\u5316\u6c9f\u901a\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u533b\u751f\u3001\u62a4\u7406\u4eba\u5458\u548c\u60a3\u8005\u7b49\u591a\u79cd\u7528\u6237\u89d2\u8272\u3002\u4e3a\u786e\u4fdd\u9690\u79c1\u548c\u53ef\u91cd\u590d\u6027\uff0c\u6846\u67b6\u4f7f\u7528\u7b26\u5408FHIR R4\u6807\u51c6\u7684SMART Health IT\u6c99\u7bb1\u4e2d\u7684\u5408\u6210EHR\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002\u4e0e\u4f20\u7edf\u4f9d\u8d56\u786c\u7f16\u7801\u68c0\u7d22\u548c\u9759\u6001\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u4e92\u64cd\u4f5c\u7684AI\u9a71\u52a8EHR\u5e94\u7528\u3002\u4ee3\u7406\u67b6\u6784\u8fdb\u4e00\u6b65\u652f\u6301\u591a\u79cdFHIR\u683c\u5f0f\uff0c\u4e3a\u63a8\u8fdb\u4e2a\u6027\u5316\u6570\u5b57\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.14629", "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86VisText-Mosquito\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u6570\u636e\u652f\u6301\u868a\u866b\u6ecb\u751f\u5730\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u3001\u5206\u5272\u4e0e\u63a8\u7406\u5206\u6790\uff0c\u5c55\u793a\u4e86AI\u5728\u9884\u9632\u868a\u5a92\u75be\u75c5\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u868a\u5a92\u75be\u75c5\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u5a01\u80c1\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u4e3b\u52a8\u63a7\u5236\u868a\u866b\u6ecb\u751f\u5730\u662f\u9884\u9632\u75ab\u60c5\u7684\u5173\u952e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u548cAI\u6a21\u578b\uff0c\u63d0\u5347\u868a\u866b\u6ecb\u751f\u5730\u7684\u81ea\u52a8\u5316\u5206\u6790\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaVisText-Mosquito\u6570\u636e\u96c6\uff0c\u5305\u542b1,828\u5f20\u6807\u6ce8\u56fe\u50cf\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u3001142\u5f20\u56fe\u50cf\u7528\u4e8e\u6c34\u9762\u5206\u5272\uff0c\u4ee5\u53ca\u6bcf\u5f20\u56fe\u50cf\u5173\u8054\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6587\u672c\u3002\u91c7\u7528YOLOv9s\u548cYOLOv11n-Seg\u6a21\u578b\u5206\u522b\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u5e76\u5fae\u8c03BLIP\u6a21\u578b\u751f\u6210\u63a8\u7406\u6587\u672c\u3002", "result": "YOLOv9s\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u9ad8\u7cbe\u5ea60.92926\u548cmAP@50\u4e3a0.92891\uff1bYOLOv11n-Seg\u7684\u5206\u5272\u7cbe\u5ea6\u4e3a0.91587\uff0cmAP@50\u4e3a0.79795\u3002BLIP\u6a21\u578b\u7684\u63a8\u7406\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6700\u7ec8\u635f\u5931\u4e3a0.0028\uff0cBLEU\u5f97\u5206\u4e3a54.7\uff0cBERTScore\u4e3a0.91\uff0cROUGE-L\u4e3a0.87\u3002", "conclusion": "VisText-Mosquito\u6570\u636e\u96c6\u548c\u6a21\u578b\u6846\u67b6\u5c55\u793a\u4e86AI\u5728\u868a\u5a92\u75be\u75c5\u9884\u9632\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5f3a\u8c03\u201c\u9884\u9632\u80dc\u4e8e\u6cbb\u7597\u201d\u7684\u7406\u5ff5\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "paper_title_zh": "VisText-Mosquito\uff1a\u57fa\u4e8eAI\u7684\u868a\u866b\u6ecb\u751f\u5730\u68c0\u6d4b\u4e0e\u63a8\u7406\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0e\u57fa\u51c6", "abstract_zh": "\u868a\u5a92\u75be\u75c5\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u5a01\u80c1\uff0c\u9700\u901a\u8fc7\u65e9\u671f\u68c0\u6d4b\u548c\u4e3b\u52a8\u63a7\u5236\u868a\u866b\u6ecb\u751f\u5730\u4ee5\u9884\u9632\u75ab\u60c5\u3002\u672c\u6587\u63d0\u51faVisText-Mosquito\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6574\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u6570\u636e\uff0c\u652f\u6301\u868a\u866b\u6ecb\u751f\u5730\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u3001\u5206\u5272\u4e0e\u63a8\u7406\u5206\u6790\u3002\u6570\u636e\u96c6\u5305\u542b1,828\u5f20\u6807\u6ce8\u56fe\u50cf\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u3001142\u5f20\u56fe\u50cf\u7528\u4e8e\u6c34\u9762\u5206\u5272\uff0c\u4ee5\u53ca\u6bcf\u5f20\u56fe\u50cf\u5173\u8054\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6587\u672c\u3002YOLOv9s\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u9ad8\u7cbe\u5ea60.92926\u548cmAP@50\u4e3a0.92891\uff1bYOLOv11n-Seg\u7684\u5206\u5272\u7cbe\u5ea6\u4e3a0.91587\uff0cmAP@50\u4e3a0.79795\u3002\u63a8\u7406\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u7684BLIP\u6a21\u578b\u6700\u7ec8\u635f\u5931\u4e3a0.0028\uff0cBLEU\u5f97\u5206\u4e3a54.7\uff0cBERTScore\u4e3a0.91\uff0cROUGE-L\u4e3a0.87\u3002\u8be5\u6570\u636e\u96c6\u548c\u6a21\u578b\u6846\u67b6\u4f53\u73b0\u4e86\u201c\u9884\u9632\u80dc\u4e8e\u6cbb\u7597\u201d\u7684\u4e3b\u9898\uff0c\u5c55\u793a\u4e86AI\u5728\u868a\u5a92\u75be\u75c5\u98ce\u9669\u4e3b\u52a8\u5e94\u5bf9\u4e2d\u7684\u6f5c\u529b\u3002\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u4ee3\u7801\u5df2\u516c\u5f00\u4e8eGitHub\uff1ahttps://github.com/adnanul-islam-jisun/VisText-Mosquito"}}
{"id": "2506.13804", "pdf": "https://arxiv.org/pdf/2506.13804", "abs": "https://arxiv.org/abs/2506.13804", "authors": ["Edward McDaid", "Sarah McDaid"], "title": "Instruction and Solution Probabilities as Heuristics for Inductive Programming", "categories": ["cs.SE", "cs.AI"], "comment": "10 pages, 10 figures", "summary": "Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u6982\u7387\u548c\u89e3\u51b3\u65b9\u6848\u6982\u7387\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u5e45\u7f29\u5c0f\u5f52\u7eb3\u7f16\u7a0b\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u901a\u8fc7\u7ed3\u5408\u6307\u4ee4\u5b50\u96c6\uff08IS\uff09\u548c\u6982\u7387\u9608\u503c\uff0c\u5b9e\u73b0\u4e86\u641c\u7d22\u7a7a\u95f4\u51cf\u5c11\u9ad8\u8fbe100\u4e2a\u6570\u91cf\u7ea7\u7684\u6548\u679c\u3002", "motivation": "\u6307\u4ee4\u5b50\u96c6\uff08IS\uff09\u867d\u80fd\u663e\u8457\u7f29\u5c0f\u5f52\u7eb3\u7f16\u7a0b\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4f46\u4ecd\u6709\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u6307\u4ee4\u6982\u7387\u548c\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4f5c\u4e3a\u989d\u5916\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u641c\u7d22\u6548\u7387\u3002", "method": "1. \u5f15\u5165\u6307\u4ee4\u6982\u7387\uff0c\u57fa\u4e8e\u5927\u578b\u4ee3\u7801\u6837\u672c\u4e2d\u6307\u4ee4\u7684\u51fa\u73b0\u9891\u7387\u8ba1\u7b97\uff1b2. \u5b9a\u4e49\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4e3a\u6240\u6709\u6307\u4ee4\u6982\u7387\u7684\u4e58\u79ef\uff1b3. \u4f7f\u7528\u4e0d\u540c\u89c4\u6a21\u4ee3\u7801\u5355\u5143\u4e2d\u7684\u6700\u5c0f\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4f5c\u4e3a\u9608\u503c\uff0c\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff1b4. \u6d4b\u8bd5\u4e24\u79cd\u6307\u4ee4\u6982\u7387\u8ba1\u7b97\u65b9\u5f0f\uff1a\u5168\u5c40\u6837\u672c\u548c\u5355\u72ecIS\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u6307\u4ee4\u6982\u7387\u8ba1\u7b97\u65b9\u5f0f\u5747\u80fd\u663e\u8457\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff0c\u6700\u9ad8\u53ef\u8fbe\u6570\u5341\u4e2a\u6570\u91cf\u7ea7\u3002\u7ed3\u5408IS\u540e\uff0c\u603b\u51cf\u5c11\u91cf\u8d85\u8fc7100\u4e2a\u6570\u91cf\u7ea7\u3002\u4ea4\u53c9\u9a8c\u8bc1\u8bc1\u660e\u8be5\u65b9\u6cd5\u5bf9\u672a\u89c1\u4ee3\u7801\u540c\u6837\u6709\u6548\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6307\u4ee4\u548c\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4f5c\u4e3a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u672c\u6587\u663e\u8457\u63d0\u5347\u4e86\u5f52\u7eb3\u7f16\u7a0b\u7684\u6548\u7387\u3002\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6982\u7387\u6a21\u578b\u5e76\u6269\u5c55\u5e94\u7528\u573a\u666f\u3002", "paper_title_zh": "\u6307\u4ee4\u6982\u7387\u4e0e\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4f5c\u4e3a\u5f52\u7eb3\u7f16\u7a0b\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5", "abstract_zh": "\u6307\u4ee4\u5b50\u96c6\uff08IS\uff09\u662f\u4e00\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u53ef\u5c06\u5f52\u7eb3\u7f16\u7a0b\uff08IP\uff09\u7684\u641c\u7d22\u7a7a\u95f4\u7f29\u5c0f\u6570\u5341\u4e2a\u6570\u91cf\u7ea7\u3002\u672c\u6587\u6269\u5c55\u4e86IS\u65b9\u6cd5\uff0c\u5f15\u5165\u6307\u4ee4\u6982\u7387\u548c\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4f5c\u4e3a\u989d\u5916\u542f\u53d1\u5f0f\u3002\u6307\u4ee4\u6982\u7387\u57fa\u4e8e\u5927\u578b\u4ee3\u7801\u6837\u672c\u4e2d\u6307\u4ee4\u7684\u51fa\u73b0\u9891\u7387\uff0c\u53cd\u6620\u5176\u5728\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u9884\u671f\u51fa\u73b0\u6982\u7387\u3002\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4e3a\u6240\u6709\u6307\u4ee4\u6982\u7387\uff08\u5305\u62ec\u91cd\u590d\u6307\u4ee4\uff09\u7684\u4e58\u79ef\u3002\u6211\u4eec\u4ee5\u4e0d\u540c\u89c4\u6a21\u4ee3\u7801\u5355\u5143\u4e2d\u7684\u6700\u5c0f\u89e3\u51b3\u65b9\u6848\u6982\u7387\u4f5c\u4e3a\u9608\u503c\uff0c\u7528\u4e8e\u5728\u6784\u5efa\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u65f6\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u6d88\u9664\u5305\u542b\u4f4e\u6982\u7387\u6307\u4ee4\u7ec4\u5408\u7684\u5206\u652f\u3002\u65b0\u65b9\u6cd5\u5df2\u901a\u8fc7\u5927\u91cf\u4eba\u7c7b\u4ee3\u7801\u6837\u672c\u8fdb\u884c\u8bc4\u4f30\u3002\u6d4b\u8bd5\u4e86\u4e24\u79cd\u6307\u4ee4\u6982\u7387\u8ba1\u7b97\u65b9\u5f0f\uff1a\u4e00\u79cd\u57fa\u4e8e\u6574\u4e2a\u4ee3\u7801\u6837\u672c\u7684\u6307\u4ee4\u51fa\u73b0\u9891\u7387\uff0c\u53e6\u4e00\u79cd\u4e3a\u6bcf\u4e2aIS\u5355\u72ec\u8ba1\u7b97\u5206\u5e03\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u5f0f\u5747\u80fd\u8fdb\u4e00\u6b65\u663e\u8457\u51cf\u5c11IP\u641c\u7d22\u7a7a\u95f4\uff0c\u6700\u9ad8\u53ef\u8fbe\u6570\u5341\u4e2a\u6570\u91cf\u7ea7\uff08\u53d6\u51b3\u4e8e\u89e3\u51b3\u65b9\u6848\u89c4\u6a21\uff09\u3002\u7ed3\u5408IS\u540e\uff0c\u603b\u51cf\u5c11\u91cf\u53ef\u8d85\u8fc7100\u4e2a\u6570\u91cf\u7ea7\u3002\u4ea4\u53c9\u9a8c\u8bc1\u8868\u660e\uff0c\u8fd9\u4e9b\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bf9\u672a\u89c1\u4ee3\u7801\u540c\u6837\u6709\u6548\u3002\u672c\u6587\u8be6\u7ec6\u63cf\u8ff0\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u7ed3\u679c\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.14642", "pdf": "https://arxiv.org/pdf/2506.14642", "abs": "https://arxiv.org/abs/2506.14642", "authors": ["Yuke Xing", "Jiarui Wang", "Peizhi Niu", "Wenjie Huang", "Guangtao Zhai", "Yiling Xu"], "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf93D\u9ad8\u65af\u70b9\u4e91\uff083DGS\uff09\u538b\u7f29\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c63DGS-IEval-15K\uff0c\u5305\u542b15,200\u5f20\u56fe\u50cf\uff0c\u901a\u8fc7\u4e3b\u89c2\u5b9e\u9a8c\u6536\u96c6\u4e8660\u540d\u89c2\u4f17\u7684\u8bc4\u4ef7\u6570\u636e\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u5206\u5e03\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d13DGS\u4e13\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "3D\u9ad8\u65af\u70b9\u4e91\uff083DGS\uff09\u5728\u5b9e\u65f6\u6e32\u67d3\u548c\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u5b58\u50a8\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5f15\u5165\u4e86\u538b\u7f29\u6a21\u5757\uff0c\u4f46\u7f3a\u4e4f\u8bc4\u4f30\u5176\u611f\u77e5\u5f71\u54cd\u7684\u5168\u9762\u6846\u67b6\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b15,200\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u56fe\u50cf\u6765\u81ea10\u4e2a\u771f\u5b9e\u573a\u666f\uff0c\u901a\u8fc76\u79cd\u4ee3\u8868\u60273DGS\u7b97\u6cd5\u572820\u4e2a\u89c6\u89d2\u4e0b\u6e32\u67d3\uff0c\u5e76\u5e94\u7528\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u4ee5\u4ea7\u751f\u591a\u79cd\u5931\u771f\u6548\u679c\u3002\u901a\u8fc7\u4e3b\u89c2\u5b9e\u9a8c\u6536\u96c6\u4e8660\u540d\u89c2\u4f17\u7684\u8bc4\u4ef7\u6570\u636e\uff0c\u5e76\u5206\u6790\u4e86\u573a\u666f\u591a\u6837\u6027\u548c\u5e73\u5747\u610f\u89c1\u5206\u6570\uff08MOS\uff09\u5206\u5e03\u3002", "result": "\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u591a\u6837\u6027\u548c\u8d28\u91cf\u5206\u5e03\u7684\u6709\u6548\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b30\u79cd\u4ee3\u8868\u6027\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u7684\u57fa\u51c6\u3002\u8fd9\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u76843DGS\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "conclusion": "3DGS-IEval-15K\u4e3a\u5f00\u53d13DGS\u4e13\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u7814\u7a763DGS\u7279\u6709\u7684\u89c6\u89d2\u4f9d\u8d56\u6027\u8d28\u91cf\u5206\u5e03\u6a21\u5f0f\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "paper_title_zh": "3DGS-IEval-15K\uff1a\u9762\u54113D\u9ad8\u65af\u70b9\u4e91\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u5e93", "abstract_zh": "3D\u9ad8\u65af\u70b9\u4e91\uff083DGS\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u5b9e\u65f6\u6e32\u67d3\uff0c\u4f46\u5176\u9ad8\u5b58\u50a8\u9700\u6c42\u4e3a\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u4e86\u6311\u6218\u3002\u5c3d\u7ba1\u6700\u65b0\u76843DGS\u65b9\u6cd5\u9010\u6e10\u5f15\u5165\u4e86\u4e13\u7528\u538b\u7f29\u6a21\u5757\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u8bc4\u4f30\u5176\u611f\u77e5\u5f71\u54cd\u7684\u5168\u9762\u6846\u67b6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e863DGS-IEval-15K\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u538b\u7f293DGS\u8868\u793a\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5305\u542b15,200\u5f20\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u901a\u8fc76\u79cd\u4ee3\u8868\u60273DGS\u7b97\u6cd5\u572810\u4e2a\u771f\u5b9e\u573a\u666f\u768420\u4e2a\u9009\u5b9a\u89c6\u89d2\u4e0b\u6e32\u67d3\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u4ea7\u751f\u591a\u79cd\u5931\u771f\u6548\u679c\u3002\u901a\u8fc7\u53d7\u63a7\u4e3b\u89c2\u5b9e\u9a8c\uff0c\u6211\u4eec\u6536\u96c6\u4e8660\u540d\u89c2\u4f17\u7684\u4eba\u7c7b\u611f\u77e5\u6570\u636e\u3002\u901a\u8fc7\u573a\u666f\u591a\u6837\u6027\u548cMOS\u5206\u5e03\u5206\u6790\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b30\u79cd\u4ee3\u8868\u6027IQA\u6307\u6807\u7684\u5168\u9762\u57fa\u51c6\u3002\u4f5c\u4e3a\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u89c4\u6a21\u76843DGS\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u5f00\u53d13DGS\u4e13\u7528IQA\u6307\u6807\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u7814\u7a763DGS\u7279\u6709\u7684\u89c6\u89d2\u4f9d\u8d56\u6027\u8d28\u91cf\u5206\u5e03\u6a21\u5f0f\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u3002\u8be5\u6570\u636e\u5e93\u5df2\u516c\u5f00\u53d1\u5e03\u4e8ehttps://github.com/YukeXing/3DGS-IEval-15K\u3002"}}
{"id": "2506.13805", "pdf": "https://arxiv.org/pdf/2506.13805", "abs": "https://arxiv.org/abs/2506.13805", "authors": ["Bonam Mingole", "Aditya Majumdar", "Firdaus Ahmed Choudhury", "Jennifer L. Kraschnewski", "Shyam S. Sundar", "Amulya Yadav"], "title": "Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4f17\u5305\u65b9\u5f0f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56de\u7b54\u65e5\u5e38\u5065\u5eb7\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b076%\u7684LLM\u56de\u7b54\u88ab\u533b\u751f\u8ba4\u4e3a\u51c6\u786e\uff0c\u5e76\u63a2\u8ba8\u4e86\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u5e93\u7684RAG\u7248\u672c\u662f\u5426\u80fd\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5ffd\u89c6\u4e86LLMs\u5728\u56de\u7b54\u666e\u901a\u7528\u6237\u65e5\u5e38\u5065\u5eb7\u95ee\u9898\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u800c\u8fd9\u662f\u66f4\u5e38\u89c1\u7684\u5e94\u7528\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4e16\u754c\u5065\u5eb7\u6c9f\u901a\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5927\u5b66\u7ade\u8d5b\u5f62\u5f0f\uff0c34\u540d\u53c2\u4e0e\u8005\u54114\u4e2a\u516c\u5f00LLMs\u63d0\u4ea4212\u4e2a\u771f\u5b9e\u6216\u865a\u6784\u7684\u5065\u5eb7\u95ee\u9898\uff0c\u75319\u540d\u8ba4\u8bc1\u533b\u751f\u8bc4\u4f30\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u63a2\u8ba8RAG\u7248\u672cLLMs\u7684\u6539\u8fdb\u6f5c\u529b\u3002", "result": "\u533b\u751f\u8ba4\u4e3a76%\u7684LLM\u56de\u7b54\u51c6\u786e\uff0c\u4e14\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u5e93\u7684RAG\u7248\u672c\u53ef\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u3002\u5b9a\u6027\u8bbf\u8c08\u63ed\u793a\u4e86\u5b9a\u91cf\u7ed3\u679c\u7684\u80cc\u540e\u539f\u56e0\u3002", "conclusion": "LLMs\u5728\u56de\u7b54\u65e5\u5e38\u5065\u5eb7\u95ee\u9898\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u7ed3\u5408\u4e13\u4e1a\u533b\u5b66\u77e5\u8bc6\u5e93\u4ee5\u4f18\u5316\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u53ef\u9760\u652f\u6301\u3002", "paper_title_zh": "GPT\u533b\u751f\u73b0\u5728\u53ef\u4ee5\u63a5\u8bca\u4e86\uff0c\u4f46\u5b83\u5e94\u8be5\u5417\uff1f\u2014\u2014\u57fa\u4e8e\u4f17\u5305\u4e34\u5e8a\u6848\u4f8b\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u5229\u5f0a", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9ad8\u98ce\u9669\u5e94\u7528\uff08\u5982\u533b\u5b66\uff08\u81ea\u6211\uff09\u8bca\u65ad\u548c\u521d\u6b65\u5206\u8bca\uff09\u4e2d\u7684\u666e\u53ca\u5f15\u53d1\u4e86\u5bf9\u5176\u6709\u6548\u6027\u3001\u9002\u7528\u6027\u53ca\u6f5c\u5728\u5371\u5bb3\u7684\u4f26\u7406\u548c\u5b9e\u8df5\u62c5\u5fe7\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8LLMs\u56de\u7b54\u4e13\u5bb6\u7f16\u5199\u7684\u5065\u5eb7\u95ee\u9898\u6216\u533b\u5b66\u8003\u8bd5\u9898\u76ee\uff0c\u5374\u5ffd\u89c6\u4e86\u5176\u5728\u666e\u901a\u7528\u6237\u65e5\u5e38\u5065\u5eb7\u95ee\u9898\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u901a\u8fc7\u5927\u5b66\u7ade\u8d5b\u5f62\u5f0f\uff0c\u91c7\u7528\u4f17\u5305\u65b9\u6cd5\u8bc4\u4f30LLMs\u56de\u7b54\u65e5\u5e38\u5065\u5eb7\u95ee\u9898\u7684\u6548\u679c\u3002\u4e00\u5468\u5185\uff0c34\u540d\u53c2\u4e0e\u8005\u54114\u4e2a\u516c\u5f00LLMs\u63d0\u4ea4212\u4e2a\u771f\u5b9e\u6216\u865a\u6784\u7684\u5065\u5eb7\u95ee\u9898\uff0c\u5e76\u75319\u540d\u8ba4\u8bc1\u533b\u751f\u8bc4\u4f30\u56de\u7b54\u8d28\u91cf\u3002\u7ed3\u679c\u8868\u660e\uff0c76%\u7684LLM\u56de\u7b54\u88ab\u533b\u751f\u8ba4\u4e3a\u51c6\u786e\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u5e93\u7684RAG\u7248\u672c\u53ef\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u3002\u901a\u8fc7\u8bbf\u8c087\u540d\u533b\u5b66\u4e13\u5bb6\uff0c\u672c\u6587\u8fd8\u63ed\u793a\u4e86\u5b9a\u91cf\u7ed3\u679c\u80cc\u540e\u7684\u5b9a\u6027\u6d1e\u5bdf\uff0c\u65e8\u5728\u4e3aLLMs\u5728\u771f\u5b9e\u4e16\u754c\u5065\u5eb7\u6c9f\u901a\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u7406\u89e3\u3002"}}
{"id": "2506.13784", "pdf": "https://arxiv.org/pdf/2506.13784", "abs": "https://arxiv.org/abs/2506.13784", "authors": ["Junting Zhou", "Wang Li", "Yiyan Liao", "Nengyuan Zhang", "Tingjia Miaoand Zhihui Qi", "Yuhan Wu", "Tong Yang"], "title": "AcademicBrowse: Benchmarking Academic Browse Ability of LLMs", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u7684\u6570\u636e\u96c6AcademicBrowse\uff0c\u5177\u5907\u5b66\u672f\u5b9e\u7528\u6027\u3001\u9ad8\u96be\u5ea6\u3001\u7b80\u6d01\u8bc4\u4f30\u548c\u5e7f\u6cdb\u8986\u76d6\u7b49\u7279\u70b9\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u641c\u7d22\u80fd\u529b\u8bc4\u6d4b\u57fa\u51c6\uff08\u5982OpenAI\u7684BrowseComp\uff09\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u641c\u7d22\u573a\u666f\uff0c\u672a\u80fd\u5145\u5206\u6ee1\u8db3\u5b66\u672f\u641c\u7d22\u7684\u7279\u5b9a\u9700\u6c42\uff0c\u5982\u6df1\u5ea6\u6587\u732e\u8ffd\u8e2a\u3001\u5b66\u672f\u6570\u636e\u5e93\u652f\u6301\u3001\u957f\u5c3e\u5b66\u672f\u77e5\u8bc6\u5bfc\u822a\u53ca\u5b66\u672f\u4e25\u8c28\u6027\u4fdd\u969c\u3002", "method": "\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86AcademicBrowse\u6570\u636e\u96c6\uff0c\u5176\u7279\u70b9\u5305\u62ec\uff1a\u5b66\u672f\u5b9e\u7528\u6027\uff08\u95ee\u9898\u8d34\u8fd1\u771f\u5b9e\u5b66\u672f\u73af\u5883\uff09\u3001\u9ad8\u96be\u5ea6\uff08\u9700\u591a\u6b21\u6df1\u5ea6\u641c\u7d22\u624d\u80fd\u89e3\u7b54\uff09\u3001\u7b80\u6d01\u8bc4\u4f30\uff08\u7b54\u6848\u552f\u4e00\u4e14\u6709\u660e\u786e\u6765\u6e90\uff09\u3001\u5e7f\u6cdb\u8986\u76d6\uff08\u6db5\u76d615\u4e2a\u5b66\u79d1\uff09\u3002", "result": "AcademicBrowse\u6570\u636e\u96c6\u586b\u8865\u4e86\u56fd\u5185\u5916\u7f3a\u4e4f\u5206\u6790\u6027\u641c\u7d22\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u8861\u91cf\u548c\u63a8\u52a8LLMs\u5728\u590d\u6742\u5b66\u672f\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AcademicBrowse\u4e3aLLMs\u5728\u5b66\u672f\u641c\u7d22\u9886\u57df\u7684\u6027\u80fd\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u95e8\u5de5\u5177\uff0c\u6709\u671b\u4fc3\u8fdb\u76f8\u5173\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "paper_title_zh": "AcademicBrowse\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u672f\u6d4f\u89c8\u80fd\u529b\u7684\u57fa\u51c6\u8bc4\u6d4b", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u641c\u7d22\u80fd\u529b\u5907\u53d7\u5173\u6ce8\u3002\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\uff08\u5982OpenAI\u7684BrowseComp\uff09\u4e3b\u8981\u9488\u5bf9\u901a\u7528\u641c\u7d22\u573a\u666f\uff0c\u672a\u80fd\u5145\u5206\u6ee1\u8db3\u5b66\u672f\u641c\u7d22\u7684\u7279\u5b9a\u9700\u6c42\uff0c\u5305\u62ec\u6df1\u5ea6\u6587\u732e\u8ffd\u8e2a\u4e0e\u7ec4\u7ec7\u3001\u5b66\u672f\u6570\u636e\u5e93\u7684\u4e13\u4e1a\u652f\u6301\u3001\u957f\u5c3e\u5b66\u672f\u77e5\u8bc6\u7684\u5bfc\u822a\u80fd\u529b\u53ca\u5b66\u672f\u4e25\u8c28\u6027\u4fdd\u969c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AcademicBrowse\uff0c\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u7684\u6570\u636e\u96c6\u3002AcademicBrowse\u5177\u5907\u4ee5\u4e0b\u5173\u952e\u7279\u70b9\uff1a\u5b66\u672f\u5b9e\u7528\u6027\uff08\u95ee\u9898\u5185\u5bb9\u8d34\u8fd1\u771f\u5b9e\u5b66\u672f\u73af\u5883\uff0c\u907f\u514d\u523b\u610f\u8bef\u5bfc\u6a21\u578b\uff09\u3001\u9ad8\u96be\u5ea6\uff08\u7b54\u6848\u9700\u81f3\u5c11\u4e09\u6b21\u6df1\u5ea6\u641c\u7d22\u624d\u80fd\u5f97\u51fa\uff09\u3001\u7b80\u6d01\u8bc4\u4f30\uff08\u9650\u5236\u6761\u4ef6\u786e\u4fdd\u7b54\u6848\u552f\u4e00\uff0c\u9644\u5e26\u660e\u786e\u6765\u6e90\u548c\u7b80\u77ed\u89e3\u91ca\uff0c\u4fbf\u4e8e\u540e\u7eed\u5ba1\u6838\u9a8c\u8bc1\uff09\u3001\u5e7f\u6cdb\u8986\u76d6\uff08\u6db5\u76d6\u81f3\u5c1115\u4e2a\u5b66\u79d1\uff09\u3002\u901a\u8fc7AcademicBrowse\uff0c\u6211\u4eec\u671f\u671b\u66f4\u7cbe\u51c6\u5730\u8861\u91cf\u5e76\u63a8\u52a8LLMs\u5728\u590d\u6742\u5b66\u672f\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002\u6570\u636e\u96c6\u5730\u5740\uff1ahttps://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse"}}
{"id": "2506.14667", "pdf": "https://arxiv.org/pdf/2506.14667", "abs": "https://arxiv.org/abs/2506.14667", "authors": ["Matt Poyser", "Toby P. Breckon"], "title": "DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification", "categories": ["cs.CV"], "comment": "27 single-column pages, 8 figures, to be published in Pattern Recognition", "summary": "In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDDS-NAS\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u636e\u9009\u62e9\u548c\u8bfe\u7a0b\u5b66\u4e60\u52a0\u901f\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u8bad\u7ec3\uff0c\u63d0\u5347\u6548\u738727\u500d\u4e14\u4e0d\u635f\u5931\u6027\u80fd\u3002", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7684\u53ef\u6269\u5c55\u6027\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u8bad\u7ec3\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u6570\u636e\u9009\u62e9\u548c\u8bfe\u7a0b\u5b66\u4e60\u4f18\u5316NAS\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8fed\u4ee3\u6b21\u6570\u3002", "method": "\u5229\u7528\u81ea\u7f16\u7801\u5668\u6784\u5efa\u56fe\u50cf\u76f8\u4f3c\u6027\u5d4c\u5165\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7kd\u6811\u7ed3\u6784\u6309\u6700\u8fdc\u90bb\u4e0d\u76f8\u4f3c\u6027\u6392\u5e8f\u56fe\u50cf\u3002\u52a8\u6001\u9009\u62e9\u5f53\u524dNAS\u67b6\u6784\u8868\u73b0\u8f83\u5dee\u7684\u56fe\u50cf\u5b50\u96c6\uff0c\u91cd\u65b0\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u3002", "result": "DDS-NAS\u6846\u67b6\u5c06\u57fa\u4e8e\u68af\u5ea6\u7684NAS\u7b56\u7565\u52a0\u901f\u9ad8\u8fbe27\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\u3002\u901a\u8fc7\u6700\u5927\u5316\u6bcf\u5f20\u56fe\u50cf\u7684\u8d21\u732e\uff0c\u663e\u8457\u7f29\u77ed\u8bad\u7ec3\u5468\u671f\u548c\u6536\u655b\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "DDS-NAS\u901a\u8fc7\u52a8\u6001\u6570\u636e\u9009\u62e9\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86NAS\u7684\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DDS-NAS\uff1a\u57fa\u4e8e\u5728\u7ebf\u56f0\u96be\u6837\u672c\u6316\u6398\u7684\u52a8\u6001\u6570\u636e\u9009\u62e9\u5728\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u7684\u5e94\u7528\u2014\u2014\u4ee5\u56fe\u50cf\u5206\u7c7b\u4e3a\u4f8b", "abstract_zh": "\u4e3a\u4e86\u89e3\u51b3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u6211\u4eec\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u52a8\u6001\u56f0\u96be\u6837\u672c\u6316\u6398\u52a0\u901fNAS\u8bad\u7ec3\u3002\u5229\u7528\u81ea\u7f16\u7801\u5668\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f3a\u5236\u56fe\u50cf\u76f8\u4f3c\u6027\u5d4c\u5165\uff0c\u6784\u5efa\u9ad8\u6548\u7684kd\u6811\u7ed3\u6784\uff0c\u6309\u6700\u8fdc\u90bb\u4e0d\u76f8\u4f3c\u6027\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u5e8f\u3002\u4ece\u5b50\u6837\u672c\u6570\u636e\u96c6\u4e2d\u7ed9\u5b9a\u67e5\u8be2\u56fe\u50cf\uff0c\u53ef\u4ee5\u5728\u5bf9\u6570\u65f6\u95f4\u5185\u8bc6\u522b\u5168\u5c40\u6570\u636e\u96c6\u4e2d\u6700\u4e0d\u76f8\u4f3c\u7684\u56fe\u50cf\u3002\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\uff0c\u52a8\u6001\u91cd\u65b0\u6784\u5efa\u4e00\u4e2a\u65e0\u504f\u5b50\u6837\u672c\u6570\u636e\u96c6\uff0c\u7528\u4e8eNAS\u4f18\u5316\uff0c\u5f53\u524dNAS\u89e3\u51b3\u65b9\u6848\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDDS-NAS\u6846\u67b6\u5c06\u57fa\u4e8e\u68af\u5ea6\u7684NAS\u7b56\u7565\u52a0\u901f\u9ad8\u8fbe27\u500d\uff0c\u4e14\u6027\u80fd\u65e0\u635f\u5931\u3002\u901a\u8fc7\u6700\u5927\u5316\u6bcf\u5f20\u56fe\u50cf\u6837\u672c\u5728\u8bad\u7ec3\u4e2d\u7684\u8d21\u732e\uff0c\u51cf\u5c11\u4e86NAS\u8bad\u7ec3\u5468\u671f\u7684\u6301\u7eed\u65f6\u95f4\u548c\u6536\u655b\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u3002"}}
{"id": "2506.13807", "pdf": "https://arxiv.org/pdf/2506.13807", "abs": "https://arxiv.org/abs/2506.13807", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Ujjwal Baid", "Hendrik M\u00f6ller", "Josef A. Buchner", "Felix Steinbauer", "Eva Oswald", "Ezequiel de la Rosa", "Ivan Ezhov", "Constantin von See", "Jan Kirschke", "Anton Schmick", "Sarthak Pati", "Akis Linardos", "Carla Pitarch", "Sanyukta Adap", "Jeffrey Rudie", "Maria Correia de Verdier", "Rachit Saluja", "Evan Calabrese", "Dominic LaBella", "Mariam Aboian", "Ahmed W. Moawad", "Nazanin Maleki", "Udunna Anazodo", "Maruf Adewole", "Marius George Linguraru", "Anahita Fathi Kazerooni", "Zhifan Jiang", "Gian Marco Conte", "Hongwei Li", "Juan Eugenio Iglesias", "Spyridon Bakas", "Benedikt Wiestler", "Marie Piraud", "Bjoern Menze"], "title": "BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "27p, 2figs, 3tabs", "summary": "The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.", "AI": {"tldr": "BraTS orchestrator\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u65e8\u5728\u7b80\u5316\u8111\u80bf\u7624\u56fe\u50cf\u5206\u6790\u4e2d\u5148\u8fdb\u7b97\u6cd5\u7684\u4f7f\u7528\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u79d1\u7814\u548c\u4e34\u5e8a\u793e\u533a\u91c7\u7eb3\u3002", "motivation": "\u5c3d\u7ba1BraTS\u6311\u6218\u8d5b\u5728\u8111\u80bf\u7624\u56fe\u50cf\u5206\u6790\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5f00\u53d1\u7684\u7b97\u6cd5\u548c\u6a21\u578b\u5728\u79d1\u5b66\u548c\u4e34\u5e8a\u793e\u533a\u4e2d\u7684\u91c7\u7eb3\u7387\u6709\u9650\u3002\u4e3a\u4e86\u52a0\u901f\u8fd9\u4e9b\u6280\u672f\u7684\u4f20\u64ad\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86BraTS orchestrator\u3002", "method": "BraTS orchestrator\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u5bf9BraTS\u6311\u6218\u8d5b\u4e2d\u5148\u8fdb\u5206\u5272\u548c\u5408\u6210\u7b97\u6cd5\u7684\u65e0\u7f1d\u8bbf\u95ee\u3002\u5de5\u5177\u5305\u5305\u542b\u76f4\u89c2\u7684\u6559\u7a0b\uff0c\u9002\u5408\u7f16\u7a0b\u7ecf\u9a8c\u6709\u9650\u7684\u7528\u6237\uff0c\u4fbf\u4e8e\u90e8\u7f72\u7b97\u6cd5\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u7b80\u5316\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u590d\u6742\u6027\uff0cBraTS orchestrator\u4f7f\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u653e\u5c04\u5b66\u548c\u795e\u7ecf\u80bf\u7624\u5b66\u53d7\u4f17\u80fd\u591f\u8f7b\u677e\u4f7f\u7528BraTS\u793e\u533a\u5f00\u53d1\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "BraTS orchestrator\u6210\u529f\u5730\u5c06\u8111\u80bf\u7624\u56fe\u50cf\u5206\u6790\u7684\u5148\u8fdb\u6280\u672f\u6c11\u4e3b\u5316\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u88ab\u79d1\u7814\u548c\u4e34\u5e8a\u793e\u533a\u91c7\u7eb3\u3002", "paper_title_zh": "BraTS\u534f\u8c03\u5668\uff1a\u6c11\u4e3b\u5316\u548c\u4f20\u64ad\u6700\u5148\u8fdb\u7684\u8111\u80bf\u7624\u56fe\u50cf\u5206\u6790\u6280\u672f", "abstract_zh": "\u8111\u80bf\u7624\u5206\u5272\uff08BraTS\uff09\u7cfb\u5217\u6311\u6218\u8d5b\u901a\u8fc7\u63d0\u4f9b\u5927\u91cf\u7ecf\u8fc7\u6574\u7406\u7684\u6570\u636e\u96c6\u5e76\u89e3\u51b3\u4e34\u5e8a\u76f8\u5173\u4efb\u52a1\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u8111\u80bf\u7624\u56fe\u50cf\u5206\u6790\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5176\u53d6\u5f97\u4e86\u6210\u529f\u5e76\u5e7f\u53d7\u6b22\u8fce\uff0c\u901a\u8fc7BraTS\u5f00\u53d1\u7684\u7b97\u6cd5\u548c\u6a21\u578b\u5728\u79d1\u5b66\u548c\u4e34\u5e8a\u793e\u533a\u4e2d\u7684\u91c7\u7eb3\u7387\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u52a0\u901f\u5176\u4f20\u64ad\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BraTS\u534f\u8c03\u5668\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u5bf9BraTS\u6311\u6218\u8d5b\u751f\u6001\u7cfb\u7edf\u4e2d\u591a\u6837\u8111\u80bf\u7624\u7684\u6700\u5148\u8fdb\u5206\u5272\u548c\u5408\u6210\u7b97\u6cd5\u7684\u65e0\u7f1d\u8bbf\u95ee\u3002\u8be5\u5de5\u5177\u5305\u53ef\u5728GitHub\uff08https://github.com/BrainLesion/BraTS\uff09\u4e0a\u83b7\u53d6\uff0c\u5e76\u5305\u542b\u4e3a\u7f16\u7a0b\u7ecf\u9a8c\u6709\u9650\u7684\u7528\u6237\u8bbe\u8ba1\u7684\u76f4\u89c2\u6559\u7a0b\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u8f7b\u677e\u90e8\u7f72\u83b7\u80dc\u7684BraTS\u7b97\u6cd5\u8fdb\u884c\u63a8\u7406\u3002\u901a\u8fc7\u62bd\u8c61\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u590d\u6742\u6027\uff0cBraTS\u534f\u8c03\u5668\u6c11\u4e3b\u5316\u4e86BraTS\u793e\u533a\u5f00\u53d1\u7684\u4e13\u4e1a\u77e5\u8bc6\u7684\u8bbf\u95ee\uff0c\u4f7f\u8fd9\u4e9b\u8fdb\u6b65\u66f4\u6613\u4e8e\u88ab\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u653e\u5c04\u5b66\u548c\u795e\u7ecf\u80bf\u7624\u5b66\u53d7\u4f17\u4f7f\u7528\u3002"}}
{"id": "2506.14674", "pdf": "https://arxiv.org/pdf/2506.14674", "abs": "https://arxiv.org/abs/2506.14674", "authors": ["Ling Li", "Yao Zhou", "Yuxuan Liang", "Fugee Tsung", "Jiaheng Wei"], "title": "Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b0\u65b9\u6cd5GLOBE\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6MP16-Reason\u548c\u4f18\u5316\u89c6\u89c9\u7ebf\u7d22\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u591a\u4e3a\u5206\u7c7b\u6216\u68c0\u7d22\u4efb\u52a1\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u57fa\u4e8e\u63a8\u7406\u7684\u5730\u7406\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u5728\u591a\u6837\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGLOBE\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6MP16-Reason\uff0c\u5e76\u91c7\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08Group-relative policy optimization\uff09\u8054\u5408\u63d0\u5347\u5b9a\u4f4d\u80fd\u529b\u8bc4\u4f30\u3001\u89c6\u89c9\u7ebf\u7d22\u63a8\u7406\u548c\u5730\u7406\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "result": "GLOBE\u5728\u591a\u6837\u5316\u89c6\u89c9\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90LVLM\u6a21\u578b\uff0c\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u751f\u6210\u66f4\u5177\u6d1e\u5bdf\u529b\u548c\u53ef\u89e3\u91ca\u6027\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "GLOBE\u901a\u8fc7\u7ed3\u5408\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u4f18\u5316\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u57fa\u4e8e\u63a8\u7406\u7684\u5730\u7406\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u901a\u8fc7\u63a8\u7406\u5b9e\u73b0\u8bc6\u522b\uff1a\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d", "abstract_zh": "\u4ee5\u5f80\u7684\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u5206\u7c7b\u6216\u68c0\u7d22\u4efb\u52a1\uff0c\u4f9d\u8d56\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u9ed1\u76d2\u51b3\u7b56\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u5174\u8d77\u4f7f\u5f97\u57fa\u4e8e\u89c6\u89c9\u7ebf\u7d22\u7684\u63a8\u7406\u9a71\u52a8\u5730\u7406\u5b9a\u4f4d\u6210\u4e3a\u53ef\u80fd\u3002\u7136\u800c\uff0c\u4ecd\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a\u6570\u636e\u65b9\u9762\uff0c\u73b0\u6709\u63a8\u7406\u6570\u636e\u96c6\u4e3b\u8981\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\uff0c\u573a\u666f\u591a\u6837\u6027\u548c\u89c6\u89d2\u53d7\u9650\uff1b\u6a21\u578b\u65b9\u9762\uff0c\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff0c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6709\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6d41\u7a0b\uff0c\u5229\u7528\u591a\u6837\u5316\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u6784\u5efa\u63a8\u7406\u5bfc\u5411\u7684\u5730\u7406\u5b9a\u4f4d\u6570\u636e\u96c6MP16-Reason\uff0c\u5e76\u5f15\u5165GLOBE\u65b9\u6cd5\uff08Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning\uff09\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5b9a\u4f4d\u80fd\u529b\u8bc4\u4f30\u3001\u89c6\u89c9\u7ebf\u7d22\u63a8\u7406\u548c\u5730\u7406\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u53cc\u76ee\u6807\u589e\u5f3a\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u8868\u660e\uff0cGLOBE\u5728\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90LVLM\u6a21\u578b\uff0c\u5c24\u5176\u5728\u591a\u6837\u5316\u89c6\u89c9\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u751f\u6210\u66f4\u5177\u6d1e\u5bdf\u529b\u548c\u53ef\u89e3\u91ca\u6027\u7684\u63a8\u7406\u8f68\u8ff9\u3002"}}
{"id": "2506.13809", "pdf": "https://arxiv.org/pdf/2506.13809", "abs": "https://arxiv.org/abs/2506.13809", "authors": ["Roman V. Belavkin"], "title": "Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space", "categories": ["q-bio.PE", "cs.AI", "math.OC"], "comment": "42 pages", "summary": "Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6c49\u660e\u7a7a\u95f4\u4e2d\u5b57\u7b26\u4e32\u7684\u6709\u5229\u7a81\u53d8\u548c\u4ea4\u53c9\u91cd\u7ec4\u6982\u7387\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u7ec4\u5408\u65b9\u6cd5\u63a8\u5bfc\u4e86\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u4f18\u5316\u4e86\u7a81\u53d8\u548c\u91cd\u7ec4\u7684\u53c2\u6570\uff0c\u63ed\u793a\u4e86\u7a81\u53d8\u548c\u91cd\u7ec4\u5728\u8fdb\u5316\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "motivation": "\u53d7Fisher\u51e0\u4f55\u65b9\u6cd5\u542f\u53d1\uff0c\u7814\u7a76\u6c49\u660e\u7a7a\u95f4\u4e2d\u5b57\u7b26\u4e32\u7684\u6709\u5229\u7a81\u53d8\u548c\u4ea4\u53c9\u91cd\u7ec4\u6982\u7387\uff0c\u4ee5\u4f18\u5316\u8fdb\u5316\u7b97\u6cd5\u7684\u53c2\u6570\uff0c\u63d0\u5347\u8fdb\u5316\u6548\u7387\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u548c\u7ec4\u5408\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u6c49\u660e\u7a7a\u95f4\u4e2d\u5b57\u7b26\u4e32\u8ddd\u79bb\u6700\u4f18\u89e3\u7684\u8f6c\u79fb\u6982\u7387\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u4f18\u5316\u4e86\u7a81\u53d8\u548c\u91cd\u7ec4\u7684\u534a\u5f84\u53c2\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7a81\u53d8\u548c\u91cd\u7ec4\u5728\u8fdb\u5316\u4e2d\u4f5c\u7528\u4e0d\u540c\uff1a\u7a81\u53d8\u6982\u7387\u968f\u8ddd\u79bb\u51cf\u5c0f\u800c\u964d\u4f4e\uff0c\u9700\u8c03\u6574\u534a\u5f84\u4ee5\u51cf\u7f13\u8fdb\u5316\u901f\u5ea6\uff1b\u91cd\u7ec4\u5728\u5b50\u7a7a\u95f4\u5185\u5e73\u8861\u6709\u5229\u4e0e\u6709\u5bb3\u6982\u7387\uff0c\u53ef\u8865\u5145\u7a81\u53d8\u5e76\u52a0\u901f\u8fdb\u5316\u3002", "conclusion": "\u7a81\u53d8\u548c\u91cd\u7ec4\u5728\u8fdb\u5316\u4e2d\u5404\u6709\u4f18\u52a3\uff0c\u91cd\u7ec4\u53ef\u8865\u5145\u7a81\u53d8\u7684\u4e0d\u8db3\uff0c\u4f18\u5316\u53c2\u6570\u80fd\u63d0\u5347\u8fdb\u5316\u6548\u7387\u3002", "paper_title_zh": "\u6c49\u660e\u7a7a\u95f4\u4e2d\u6709\u5229\u7a81\u53d8\u548c\u4ea4\u53c9\u91cd\u7ec4\u6982\u7387\u7684\u5206\u6790\u4e0e\u4f18\u5316", "abstract_zh": "\u53d7Fisher\u51e0\u4f55\u65b9\u6cd5\u542f\u53d1\uff0c\u6211\u4eec\u5206\u6790\u4e86\u6c49\u660e\u7a7a\u95f4\u4e2d\u5b57\u7b26\u4e32\u7684\u6709\u5229\u7a81\u53d8\u548c\u4ea4\u53c9\u91cd\u7ec4\u6982\u7387\uff0c\u5176\u4e2d\u7a81\u53d8\u548c\u91cd\u7ec4\u82e5\u51cf\u5c11\u4e0e\u6700\u4f18\u89e3\u7684\u8ddd\u79bb\u5219\u89c6\u4e3a\u6709\u5229\u3002\u901a\u8fc7\u51e0\u4f55\u548c\u7ec4\u5408\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u56f4\u7ed5\u6700\u4f18\u89e3\u7684\u7403\u4f53\u95f4\u8f6c\u79fb\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5b8c\u6574\u63cf\u8ff0\u4e86\u591a\u4ee3\u8fdb\u5316\u4e2d\u8ddd\u79bb\u6700\u4f18\u89e3\u7684\u9a6c\u5c14\u53ef\u592b\u6f14\u5316\u8fc7\u7a0b\u3002\u8fd9\u4e3a\u4f18\u5316\u7a81\u53d8\u548c\u91cd\u7ec4\u7b97\u5b50\u7684\u53c2\u6570\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u672c\u6587\u63a8\u5bfc\u4e86\u7a81\u53d8\u548c\u91cd\u7ec4\u534a\u5f84\u6700\u5927\u5316\u8fdb\u5165\u6700\u4f18\u89e3\u6982\u7387\u7684\u6700\u4f18\u6761\u4ef6\u3002\u5206\u6790\u63ed\u793a\u4e86\u8fd9\u4e9b\u8fdb\u5316\u7b97\u5b50\u7684\u91cd\u8981\u5dee\u5f02\uff1a\u7a81\u53d8\u53ef\u8986\u76d6\u6574\u4e2a\u641c\u7d22\u7a7a\u95f4\uff0c\u4f46\u6709\u5229\u7a81\u53d8\u6982\u7387\u968f\u8ddd\u79bb\u51cf\u5c0f\u800c\u964d\u4f4e\uff0c\u6700\u4f18\u7a81\u53d8\u534a\u5f84\u6216\u901f\u7387\u4e5f\u9700\u51cf\u5c0f\uff0c\u5bfc\u81f4\u8fdb\u5316\u901f\u5ea6\u51cf\u7f13\uff1b\u800c\u4ea4\u53c9\u91cd\u7ec4\u4f5c\u7528\u4e8e\u5f53\u524d\u5b57\u7b26\u4e32\u79cd\u7fa4\u5b9a\u4e49\u7684\u5b50\u7a7a\u95f4\uff0c\u5176\u6709\u5229\u4e0e\u6709\u5bb3\u6982\u7387\u5e73\u8861\u4e14\u7279\u5f81\uff08\u5982\u65b9\u5dee\uff09\u5728\u6c49\u660e\u7a7a\u95f4\u4e2d\u5177\u6709\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u8868\u660e\u91cd\u7ec4\u53ef\u8865\u5145\u7a81\u53d8\u5e76\u52a0\u901f\u8fdb\u5316\u3002"}}
{"id": "2506.13811", "pdf": "https://arxiv.org/pdf/2506.13811", "abs": "https://arxiv.org/abs/2506.13811", "authors": ["Sompote Youwai", "David Phim", "Vianne Gayl Murcia", "Rianne Clair Onas"], "title": "Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8def\u7531\u5668\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u57fa\u7840\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u7c7b\u548c\u4e13\u5bb6\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8def\u7531\u5668\u914d\u7f6e\u5728\u6d45\u57fa\u7840\u548c\u6869\u57fa\u8bbe\u8ba1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u57fa\u7840\u8bbe\u8ba1\u8ba1\u7b97\u662f\u571f\u6728\u5de5\u7a0b\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u667a\u80fd\u4efb\u52a1\u5206\u7c7b\u548c\u4e13\u5bb6\u9009\u62e9\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u5347\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u540c\u65f6\u786e\u4fdd\u4e13\u4e1a\u6587\u6863\u6807\u51c6\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u5355\u667a\u80fd\u4f53\u5904\u7406\u3001\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1-\u68c0\u67e5\u67b6\u6784\u548c\u57fa\u4e8e\u8def\u7531\u5668\u7684\u4e13\u5bb6\u9009\u62e9\u3002\u4f7f\u7528DeepSeek R1\u3001ChatGPT 4 Turbo\u3001Grok 3\u548cGemini 2.5 Pro\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u6d45\u57fa\u7840\u548c\u6869\u57fa\u8bbe\u8ba1\u573a\u666f\u4e2d\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u57fa\u4e8e\u8def\u7531\u5668\u7684\u914d\u7f6e\u5728\u6d45\u57fa\u7840\u548c\u6869\u57fa\u8bbe\u8ba1\u4e2d\u5206\u522b\u8fbe\u523095.00%\u548c90.63%\u7684\u6027\u80fd\u5f97\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002Grok 3\u5728\u72ec\u7acb\u8fd0\u884c\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8868\u660eLLM\u5728\u5de5\u7a0b\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8fdb\u6b65\u3002", "conclusion": "\u8def\u7531\u5668\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u57fa\u7840\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u6700\u4f18\u9009\u62e9\uff0c\u4f46\u4ecd\u9700\u4eba\u5de5\u76d1\u7763\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u3002\u8fd9\u4e9b\u7cfb\u7edf\u5e94\u4f5c\u4e3a\u9ad8\u7ea7\u8ba1\u7b97\u8f85\u52a9\u5de5\u5177\uff0c\u800c\u975e\u5b8c\u5168\u81ea\u4e3b\u7684\u8bbe\u8ba1\u66ff\u4ee3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8def\u7531\u5668\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u57fa\u7840\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u6f5c\u529b\u7814\u7a76\uff1a\u4efb\u52a1\u5206\u7c7b\u4e0e\u4e13\u5bb6\u9009\u62e9", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8def\u7531\u5668\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u667a\u80fd\u4efb\u52a1\u5206\u7c7b\u548c\u4e13\u5bb6\u9009\u62e9\u5b9e\u73b0\u57fa\u7840\u8bbe\u8ba1\u8ba1\u7b97\u7684\u81ea\u52a8\u5316\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u5355\u667a\u80fd\u4f53\u5904\u7406\u3001\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1-\u68c0\u67e5\u67b6\u6784\u548c\u57fa\u4e8e\u8def\u7531\u5668\u7684\u4e13\u5bb6\u9009\u62e9\u3002\u6027\u80fd\u6d4b\u8bd5\u4f7f\u7528\u4e86DeepSeek R1\u3001ChatGPT 4 Turbo\u3001Grok 3\u548cGemini 2.5 Pro\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u8986\u76d6\u6d45\u57fa\u7840\u548c\u6869\u57fa\u8bbe\u8ba1\u573a\u666f\u3002\u57fa\u4e8e\u8def\u7531\u5668\u7684\u914d\u7f6e\u5728\u6d45\u57fa\u7840\u548c\u6869\u57fa\u8bbe\u8ba1\u4e2d\u5206\u522b\u8fbe\u523095.00%\u548c90.63%\u7684\u6027\u80fd\u5f97\u5206\uff0c\u6bd4\u72ec\u7acb\u8fd0\u884c\u7684Grok 3\u5206\u522b\u63d0\u5347\u4e868.75\u548c3.13\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u4f18\u4e8e\u4f20\u7edf\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b10.0\u81f343.75\u4e2a\u767e\u5206\u70b9\u3002Grok 3\u5728\u65e0\u5916\u90e8\u8ba1\u7b97\u5de5\u5177\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u8868\u660eLLM\u5728\u5de5\u7a0b\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8fdb\u6b65\u3002\u53cc\u5c42\u6b21\u5206\u7c7b\u6846\u67b6\u6210\u529f\u533a\u5206\u4e86\u57fa\u7840\u7c7b\u578b\uff0c\u652f\u6301\u4e86\u9002\u5f53\u7684\u5206\u6790\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u8def\u7531\u5668\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u57fa\u7840\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u6700\u4f18\u9009\u62e9\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e13\u4e1a\u6587\u6863\u6807\u51c6\u3002\u9274\u4e8e\u571f\u6728\u5de5\u7a0b\u7684\u5b89\u5168\u5173\u952e\u6027\u8981\u6c42\uff0c\u4ecd\u9700\u6301\u7eed\u4eba\u5de5\u76d1\u7763\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5e94\u4f5c\u4e3a\u9ad8\u7ea7\u8ba1\u7b97\u8f85\u52a9\u5de5\u5177\uff0c\u800c\u975e\u5b8c\u5168\u81ea\u4e3b\u7684\u8bbe\u8ba1\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.14686", "pdf": "https://arxiv.org/pdf/2506.14686", "abs": "https://arxiv.org/abs/2506.14686", "authors": ["Xi Chen", "Hengshuang Zhao"], "title": "FocalClick-XL: Towards Unified and High-quality Interactive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFocalClick-XL\uff0c\u4e00\u79cd\u65b0\u578b\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ea7\u5b50\u7f51\u7edc\u8bbe\u8ba1\u652f\u6301\u591a\u79cd\u4ea4\u4e92\u5f62\u5f0f\uff08\u5982\u70b9\u51fb\u3001\u6d82\u9e26\u548c\u6846\u9009\uff09\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\u652f\u6301\u7684\u4ea4\u4e92\u5f62\u5f0f\u6709\u9650\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u8282\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u5c55FocalClick\u7684\u591a\u9636\u6bb5\u8bbe\u8ba1\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u5347\u5206\u5272\u8d28\u91cf\u3002", "method": "\u63d0\u51faFocalClick-XL\uff0c\u5c06\u4ea4\u4e92\u5f0f\u5206\u5272\u5206\u89e3\u4e3a\u6355\u6349\u4e0d\u540c\u7ea7\u522b\u4fe1\u606f\uff08\u4e0a\u4e0b\u6587\u3001\u5bf9\u8c61\u548c\u7ec6\u8282\uff09\u7684\u5143\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7531\u4e13\u7528\u5b50\u7f51\u7edc\u5904\u7406\u3002\u901a\u8fc7\u72ec\u7acb\u6570\u636e\u548c\u76d1\u7763\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u5bf9\u8c61\u7ea7\u522b\u5f15\u5165\u63d0\u793a\u5c42\u4ee5\u7f16\u7801\u7279\u5b9a\u4ea4\u4e92\u7c7b\u578b\u3002", "result": "FocalClick-XL\u5728\u70b9\u51fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u9002\u5e94\u591a\u79cd\u4ea4\u4e92\u5f62\u5f0f\uff08\u5982\u6846\u9009\u3001\u6d82\u9e26\u548c\u7c97\u63a9\u6a21\uff09\uff0c\u8fd8\u80fd\u9884\u6d4b\u7cbe\u7ec6\u7684alpha\u906e\u7f69\u3002", "conclusion": "FocalClick-XL\u662f\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u5f3a\u5927\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u4ea4\u4e92\u5f62\u5f0f\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u5272\u7ed3\u679c\u3002", "paper_title_zh": "FocalClick-XL\uff1a\u8fc8\u5411\u7edf\u4e00\u4e14\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u5206\u5272", "abstract_zh": "\u4ea4\u4e92\u5f0f\u5206\u5272\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u70b9\u51fb\u3001\u6d82\u9e26\u548c\u6846\u9009\u7b49\u7b80\u5355\u4ea4\u4e92\u63d0\u53d6\u76ee\u6807\u5bf9\u8c61\u7684\u4e8c\u503c\u63a9\u6a21\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u652f\u6301\u6709\u9650\u7684\u4ea4\u4e92\u5f62\u5f0f\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u8282\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6FocalClick\u7684\u7ecf\u5178\u7531\u7c97\u5230\u7ec6\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165\u91cd\u8981\u6269\u5c55\u3002\u53d7\u5176\u591a\u9636\u6bb5\u7b56\u7565\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u6d41\u7a0bFocalClick-XL\uff0c\u4ee5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u9075\u5faa\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u8d8b\u52bf\uff0c\u6211\u4eec\u5c06\u4ea4\u4e92\u5f0f\u5206\u5272\u5206\u89e3\u4e3a\u6355\u6349\u4e0d\u540c\u7ea7\u522b\u4fe1\u606f\uff08\u4e0a\u4e0b\u6587\u3001\u5bf9\u8c61\u548c\u7ec6\u8282\uff09\u7684\u5143\u4efb\u52a1\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7ea7\u522b\u5206\u914d\u4e13\u7528\u5b50\u7f51\u7edc\u3002\u8fd9\u79cd\u5206\u89e3\u5141\u8bb8\u6bcf\u4e2a\u5b50\u7f51\u7edc\u901a\u8fc7\u72ec\u7acb\u6570\u636e\u548c\u76d1\u7763\u8fdb\u884c\u89c4\u6a21\u5316\u9884\u8bad\u7ec3\uff0c\u6700\u5927\u5316\u5176\u6548\u679c\u3002\u4e3a\u589e\u5f3a\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5728\u4e0a\u4e0b\u6587\u548c\u7ec6\u8282\u7ea7\u522b\u5171\u4eab\u4e0d\u540c\u4ea4\u4e92\u5f62\u5f0f\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u540c\u65f6\u5728\u5bf9\u8c61\u7ea7\u522b\u5f15\u5165\u63d0\u793a\u5c42\u4ee5\u7f16\u7801\u7279\u5b9a\u4ea4\u4e92\u7c7b\u578b\u3002\u6700\u7ec8\uff0cFocalClick-XL\u5728\u70b9\u51fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u591a\u79cd\u4ea4\u4e92\u5f62\u5f0f\uff08\u5982\u6846\u9009\u3001\u6d82\u9e26\u548c\u7c97\u63a9\u6a21\uff09\u7684\u663e\u8457\u9002\u5e94\u6027\u3002\u9664\u4e8c\u503c\u63a9\u6a21\u751f\u6210\u5916\uff0c\u5b83\u8fd8\u80fd\u9884\u6d4b\u5305\u542b\u7cbe\u7ec6\u7ec6\u8282\u7684alpha\u906e\u7f69\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u5f3a\u5927\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u5de5\u5177\u3002"}}
{"id": "2506.13923", "pdf": "https://arxiv.org/pdf/2506.13923", "abs": "https://arxiv.org/abs/2506.13923", "authors": ["Vaskar Nath", "Elaine Lau", "Anisha Gunjal", "Manasi Sharma", "Nikhil Baharte", "Sean Hendryx"], "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new class of online training algorithms. $\\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the \"off-policy\" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\uff08RLVR\uff09\u89e3\u51b3\u65b0\u95ee\u9898\u7684\u8fc7\u7a0b\uff0c\u53d1\u73b0RLVR\u901a\u8fc7\u538b\u7f29pass@k\u5230pass@1\u548c\u201c\u80fd\u529b\u589e\u76ca\u201d\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u7b97\u6cd5Guide\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u6570\u5b66\u7b49\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\u89e3\u51b3\u65b0\u95ee\u9898\uff0c\u63a2\u7d22RLVR\u7684\u6027\u80fd\u63d0\u5347\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u4f18\u5316\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790RLVR\u7684\u6027\u80fd\u63d0\u5347\u673a\u5236\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u5f15\u5bfc\u7b97\u6cd5Guide\uff0c\u52a8\u6001\u5f15\u5165\u63d0\u793a\u5e76\u8c03\u6574\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u4f8b\uff0c\u4f18\u5316\u6a21\u578b\u5728\u65e0\u63d0\u793a\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u57287B\u548c32B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cGuide-GRPO\u76f8\u6bd4\u539f\u59cb\u65b9\u6cd5\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe4%\u7684\u5b8f\u89c2\u5e73\u5747\u63d0\u5347\u3002", "conclusion": "Guide\u7b97\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u81ea\u9002\u5e94\u5f15\u5bfc\u52a0\u901f\u63a8\u7406\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60", "abstract_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u7684\u63a8\u7406\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u89e3\u51b3\u65b0\u95ee\u9898\u3002\u53d1\u73b0RLVR\u901a\u8fc7\u4e24\u79cd\u4e3b\u8981\u65b9\u5f0f\u63d0\u5347\u6027\u80fd\uff1a\uff081\uff09\u5c06pass@k\u538b\u7f29\u4e3apass@1\uff1b\uff082\uff09\u901a\u8fc7\u201c\u80fd\u529b\u589e\u76ca\u201d\u4f7f\u6a21\u578b\u89e3\u51b3\u6b64\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u80fd\u529b\u589e\u76ca\u5b58\u5728\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\uff0c\u4f46\u89e3\u51b3\u65b0\u95ee\u9898\u7684\u80fd\u529b\u4e3b\u8981\u901a\u8fc7\u81ea\u84b8\u998f\u9a71\u52a8\u3002\u6211\u4eec\u57280.5B\u81f372B\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6d89\u53ca\u6570\u5b66\u3001\u79d1\u5b66\u548c\u4ee3\u7801\u9886\u57df\u768450\u591a\u4e07\u4e2a\u63a8\u7406\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u601d\u8003\uff0c\u540c\u65f6\u8981\u6c42\u5176\u4ece\u5934\u63a8\u5bfc\u89e3\u51b3\u65b9\u6848\u94fe\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8pass@k\u7387\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Guide\u2014\u2014\u4e00\u7c7b\u65b0\u7684\u5728\u7ebf\u8bad\u7ec3\u7b97\u6cd5\u3002Guide\u81ea\u9002\u5e94\u5730\u5c06\u63d0\u793a\u5f15\u5165\u521d\u59cb\u9519\u8bef\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u8c03\u6574\u201c\u79bb\u7b56\u7565\u201d\u8f68\u8ff9\u7684\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u4f8b\uff0c\u4ee5\u4f18\u5316\u65e0\u63d0\u793a\u73af\u5883\u4e0b\u7684\u7b56\u7565\u3002\u6211\u4eec\u63cf\u8ff0\u4e86Guide\u5728GRPO\u548cPPO\u4e2d\u7684\u53d8\u4f53\uff0c\u5e76\u57287B\u548c32B\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u8bc1\u663e\u793a\uff0cGuide-GRPO\u76f8\u6bd4\u539f\u59cb\u65b9\u6cd5\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe4%\u7684\u5b8f\u89c2\u5e73\u5747\u63d0\u5347\u3002\u901a\u8fc7\u8be6\u7ec6\u6d88\u878d\u5b9e\u9a8c\u5206\u6790Guide\u7684\u7ec4\u4ef6\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5176\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2506.14696", "pdf": "https://arxiv.org/pdf/2506.14696", "abs": "https://arxiv.org/abs/2506.14696", "authors": ["Dahang Wan", "Rongsheng Lu", "Yang Fang", "Xianli Lang", "Shuangbao Shu", "Jingjing Chen", "Siyuan Shen", "Ting Xu", "Zecong Ye"], "title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework", "categories": ["cs.CV"], "comment": "28 pages, 8 figures", "summary": "Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faYOLOv11-RGBT\uff0c\u4e00\u79cd\u57fa\u4e8eYOLOv11\u7684\u5355\u9636\u6bb5\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u516d\u79cd\u591a\u5149\u8c31\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u63d0\u51faP3\u4e2d\u878d\u5408\u7b56\u7565\u548c\u591a\u5149\u8c31\u53ef\u63a7\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u4ea4\u4e92\u3001\u4f4e\u5149\u6761\u4ef6\u548c\u6a21\u578b\u8f7b\u91cf\u5316\u65b9\u9762\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u7edf\u4e00\u7684\u5355\u9636\u6bb5\u6846\u67b6\uff0c\u4e14\u6027\u80fd\u548c\u878d\u5408\u7b56\u7565\u96be\u4ee5\u5e73\u8861\uff0c\u6a21\u6001\u6743\u91cd\u5206\u914d\u4e0d\u5408\u7406\u3002", "method": "\u57fa\u4e8eYOLOv11\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u516d\u79cd\u591a\u5149\u8c31\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u5e94\u7528\u4e8eYOLOv3\u81f3YOLOv12\u548cRT-DETR\u6a21\u578b\uff1b\u63d0\u51faP3\u4e2d\u878d\u5408\u7b56\u7565\u548c\u591a\u5149\u8c31\u53ef\u63a7\u5fae\u8c03\uff08MCF\uff09\u7b56\u7565\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\uff0c\u51cf\u5c11\u5197\u4f59\u548c\u5931\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728LLVIP\u548cFLIR\u7b49\u4e09\u5927\u5f00\u6e90\u591a\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cMCF\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0cFLIR\u6570\u636e\u96c6\u4e0aYOLOv11\u6a21\u578b\u7684mAP\u63d0\u53473.41%-5.65%\uff0c\u6700\u9ad8\u8fbe47.61%\u3002", "conclusion": "YOLOv11-RGBT\u6846\u67b6\u53ca\u63d0\u51fa\u7684\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "YOLOv11-RGBT\uff1a\u8fc8\u5411\u5168\u9762\u7684\u5355\u9636\u6bb5\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u6846\u67b6", "abstract_zh": "\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u901a\u8fc7\u6574\u5408\u591a\u6ce2\u6bb5\u4fe1\u606f\uff0c\u53ef\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u73af\u5883\u9002\u5e94\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u4ea4\u4e92\u3001\u4f4e\u5149\u6761\u4ef6\u548c\u6a21\u578b\u8f7b\u91cf\u5316\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u7f3a\u4e4f\u7edf\u4e00\u5355\u9636\u6bb5\u6846\u67b6\u3001\u6027\u80fd\u4e0e\u878d\u5408\u7b56\u7565\u96be\u4ee5\u5e73\u8861\u53ca\u6a21\u6001\u6743\u91cd\u5206\u914d\u4e0d\u5408\u7406\u7b49\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u57fa\u4e8eYOLOv11\u6846\u67b6\uff0c\u6211\u4eec\u63d0\u51faYOLOv11-RGBT\uff0c\u4e00\u79cd\u65b0\u578b\u7efc\u5408\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u516d\u79cd\u591a\u5149\u8c31\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eYOLOv3\u81f3YOLOv12\u548cRT-DETR\u6a21\u578b\u3002\u5728\u91cd\u65b0\u8bc4\u4f30\u4e24\u79cd\u6a21\u6001\u91cd\u8981\u6027\u540e\uff0c\u63d0\u51fa\u4e86P3\u4e2d\u878d\u5408\u7b56\u7565\u548c\u591a\u5149\u8c31\u53ef\u63a7\u5fae\u8c03\uff08MCF\uff09\u7b56\u7565\u3002\u8fd9\u4e9b\u6539\u8fdb\u4f18\u5316\u4e86\u7279\u5f81\u878d\u5408\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u548c\u5931\u914d\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728LLVIP\u548cFLIR\u7b49\u4e09\u5927\u5f00\u6e90\u591a\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u7279\u522b\u662f\u591a\u5149\u8c31\u53ef\u63a7\u5fae\u8c03\u7b56\u7565\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728FLIR\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7b56\u7565\u4f7fYOLOv11\u6a21\u578b\u7684mAP\u6301\u7eed\u63d0\u53473.41%-5.65%\uff0c\u6700\u9ad8\u8fbe47.61%\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u548c\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/wandahangFY/YOLOv11-RGBT\u3002"}}
{"id": "2506.13817", "pdf": "https://arxiv.org/pdf/2506.13817", "abs": "https://arxiv.org/abs/2506.13817", "authors": ["Saleem A. Al Dajani", "Abel Sanchez", "John R. Williams"], "title": "DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models", "categories": ["q-bio.GN", "cs.AI", "cs.LG", "cs.SE", "q-bio.QM"], "comment": "4 pages, 5 figures, Accepted by ICML 2025 FM4LS https://openreview.net/forum?id=zNjXOZxEYB . Workshop on Multi-modal Foundation Models and Large Language Models for Life Sciences (FM4LS)}, July 2025", "summary": "Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\u548c\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\u7684\u65b9\u6cd5DeepSeq\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6807\u6ce8\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe82.5%\uff0c\u89e3\u51b3\u4e86\u76d1\u7763\u5b66\u4e60\u4e2d\u4eba\u5de5\u6807\u6ce8\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u89c4\u6a21\u8fc5\u901f\u6269\u5927\u81f3\u6570\u5341\u4ebf\u7ec6\u80de\uff0c\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u63a8\u52a8\u865a\u62df\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\u6280\u672f\uff0c\u81ea\u52a8\u5316\u6807\u6ce8\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u3002\u901a\u8fc7\u4ee3\u7406\u5f0f\u6a21\u578b\u63d0\u5347\u6807\u6ce8\u6548\u7387\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6807\u6ce8\u51c6\u786e\u7387\u8fbe\u523082.5%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u6ce8\u541e\u5410\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21\u6270\u52a8\u7b5b\u67e5\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002", "conclusion": "DeepSeq\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u521b\u65b0\u5e94\u7528\uff0c\u672a\u6765\u53ef\u80fd\u8d85\u8d8a\u4eba\u5de5\u6807\u6ce8\u6027\u80fd\uff0c\u63a8\u52a8\u5065\u5eb7\u76d1\u6d4b\u548c\u8bca\u65ad\u6280\u672f\u7684\u53d1\u5c55\u3002", "paper_title_zh": "DeepSeq\uff1a\u57fa\u4e8e\u7f51\u7edc\u641c\u7d22\u589e\u5f3a\u7684\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u9ad8\u901a\u91cf\u6807\u6ce8", "abstract_zh": "\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\u4e3a\u5904\u7406\u7ed3\u6784\u5316\u751f\u7269\u6570\u636e\uff08\u5c24\u5176\u662f\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\uff09\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u8fd9\u4e9b\u6570\u636e\u6b63\u8fc5\u901f\u6269\u5c55\u81f3\u6570\u5341\u4ebf\u7ec6\u80de\u89c4\u6a21\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\u7684\u4ee3\u7406\u5f0f\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6807\u6ce8\u5b9e\u9a8c\u6570\u636e\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe82.5%\u3002\u8fd9\u89e3\u51b3\u4e86\u76d1\u7763\u5b66\u4e60\u4e2d\u7ed3\u6784\u5316\u7ec4\u5b66\u6570\u636e\u6807\u6ce8\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6807\u6ce8\u541e\u5410\u91cf\uff0c\u907f\u514d\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u8bef\u5dee\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u652f\u6301\u5f00\u53d1\u865a\u62df\u7ec6\u80de\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u7ec6\u80de\u5206\u578b\u548c\u6270\u52a8\u9884\u6d4b\uff09\u3002\u968f\u7740\u6570\u636e\u91cf\u7684\u589e\u957f\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5728\u6807\u6ce8\u6027\u80fd\u4e0a\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4e3a\u5927\u89c4\u6a21\u6270\u52a8\u7b5b\u67e5\u7684\u53ef\u9760\u63a8\u65ad\u94fa\u5e73\u9053\u8def\u3002\u8fd9\u4e00\u5e94\u7528\u5c55\u793a\u4e86\u5065\u5eb7\u76d1\u6d4b\u548c\u8bca\u65ad\u9886\u57df\u7684\u521b\u65b0\uff0c\u4e0e\u4eba\u7c7b\u7ec6\u80de\u56fe\u8c31\u548c\u4eba\u7c7b\u80bf\u7624\u56fe\u8c31\u7f51\u7edc\u7b49\u8ba1\u5212\u76f8\u547c\u5e94\u3002"}}
{"id": "2506.13971", "pdf": "https://arxiv.org/pdf/2506.13971", "abs": "https://arxiv.org/abs/2506.13971", "authors": ["Andrew Chang", "Chenkai Hu", "Ji Qi", "Zhuojian Wei", "Kexin Zhang", "Viswadruth Akkaraju", "David Poeppel", "Dustin Freeman"], "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.MM"], "comment": "Interspeech 2025", "summary": "Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u9891\u4f1a\u8bae\u5bf9\u8bdd\u4f53\u9a8c\u5efa\u6a21\u6240\u9700\u7684\u6807\u6ce8\u6570\u636e\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u7fa4\u7ec4\u5bf9\u8bdd\u662f\u4e00\u79cd\u590d\u6742\u7684\u793e\u4f1a\u884c\u4e3a\uff0c\u4f46\u5bf9\u8bdd\u4e2d\u4e0d\u6d41\u7545\u6216\u4e0d\u6109\u5feb\u7684\u8d1f\u9762\u4f53\u9a8c\u65f6\u523b\u7814\u7a76\u4e0d\u8db3\u3002\u8fd9\u4e9b\u65f6\u523b\u5728\u81ea\u7136\u6570\u636e\u4e2d\u8f83\u4e3a\u7f55\u89c1\uff0c\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7ed3\u5408\u591a\u6a21\u6001\uff08\u97f3\u9891\u3001\u9762\u90e8\u3001\u6587\u672c\uff09\u6df1\u5ea6\u7279\u5f81\uff0c\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u9884\u6d4b\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u8d1f\u9762\u4f53\u9a8c\u65f6\u523b\u3002\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u7684\u534f\u540c\u8bad\u7ec3SSL\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u591a\u6a21\u6001\u878d\u5408\u7684\u534f\u540c\u8bad\u7ec3SSL\u6a21\u578b\u5728ROC-AUC\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u8fbe\u52300.9\u548c0.6\uff0c\u4f18\u4e8e\u76f8\u540c\u6807\u6ce8\u6570\u636e\u91cf\u7684\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08\u63d0\u53474%\uff09\u3002\u4ec5\u4f7f\u75288%\u6807\u6ce8\u6570\u636e\u7684SSL\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u5168\u6570\u636e\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u768496%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5229\u7528\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u89c6\u9891\u4f1a\u8bae\u4f53\u9a8c\u5efa\u6a21\u7684\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "paper_title_zh": "\u57fa\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u591a\u6a21\u6001\u878d\u5408\u6700\u5c0f\u5316\u89c6\u9891\u4f1a\u8bae\u5bf9\u8bdd\u4f53\u9a8c\u5efa\u6a21\u7684\u6807\u6ce8\u91cf", "abstract_zh": "\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u7fa4\u7ec4\u5bf9\u8bdd\u662f\u4e00\u79cd\u590d\u6742\u7684\u793e\u4f1a\u884c\u4e3a\u3002\u7136\u800c\uff0c\u5bf9\u8bdd\u4e2d\u5931\u53bb\u6d41\u7545\u6027\u6216\u6109\u60a6\u611f\u7684\u8d1f\u9762\u4f53\u9a8c\u65f6\u523b\u7814\u7a76\u8f83\u5c11\u3002\u8fd9\u4e9b\u65f6\u523b\u5728\u81ea\u7136\u6570\u636e\u4e2d\u8f83\u4e3a\u7f55\u89c1\uff0c\u56e0\u6b64\u8bad\u7ec3\u76d1\u7763\u5b66\u4e60\uff08SL\uff09\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002\u6211\u4eec\u5e94\u7528\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7ed3\u5408\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u7247\u6bb5\uff0c\u8bad\u7ec3\u591a\u6a21\u6001\uff08\u97f3\u9891\u3001\u9762\u90e8\u3001\u6587\u672c\uff09\u6df1\u5ea6\u7279\u5f81\uff0c\u4ee5\u9884\u6d4b\u4fdd\u7559\u89c6\u9891\u4f1a\u8bae\u4f1a\u8bdd\u4e2d\u7684\u4e0d\u6d41\u7545\u6216\u4e0d\u6109\u5feb\u65f6\u523b\u3002\u591a\u6a21\u6001\u878d\u5408\u7684\u534f\u540c\u8bad\u7ec3SSL\u5b9e\u73b0\u4e86ROC-AUC 0.9\u548cF1\u5206\u65700.6\uff0c\u4f18\u4e8e\u76f8\u540c\u6807\u6ce8\u6570\u636e\u91cf\u7684SL\u6a21\u578b\uff08\u63d0\u53474%\uff09\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4ec5\u4f7f\u75288%\u6807\u6ce8\u6570\u636e\u7684\u6700\u4f73SSL\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u5168\u6570\u636eSL\u6a21\u578b\u768496%\u3002\u8fd9\u8868\u660e\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u89c6\u9891\u4f1a\u8bae\u4f53\u9a8c\u3002"}}
{"id": "2506.14706", "pdf": "https://arxiv.org/pdf/2506.14706", "abs": "https://arxiv.org/abs/2506.14706", "authors": ["Ni Ou", "Zhuo Chen", "Xinru Zhang", "Junzheng Wang"], "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion", "categories": ["cs.CV"], "comment": "7 pages, 4 figures, accepted by IROS 2025", "summary": "Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u6269\u6563\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u76f8\u673a\u4e0eLiDAR\u7684\u5916\u53c2\u6807\u5b9a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6807\u5b9a\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u76f8\u673a\u548cLiDAR\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5173\u952e\u4f20\u611f\u5668\uff0c\u4f46\u5176\u6570\u636e\u878d\u5408\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u5916\u53c2\u6807\u5b9a\u3002\u73b0\u6709\u7aef\u5230\u7aef\u6807\u5b9a\u65b9\u6cd5\u591a\u4e3a\u5355\u6b65\u9884\u6d4b\uff0c\u7f3a\u4e4f\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u66ff\u4ee3\u6269\u6563\u6846\u67b6\uff0c\u5c06\u521d\u59cb\u5916\u53c2\u6807\u5b9a\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316\u3002\u539f\u59cb\u6807\u5b9a\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u53bb\u566a\u5668\uff0c\u9010\u6b65\u4f30\u8ba1\u6700\u7ec8\u5916\u53c2\uff0c\u65e0\u9700\u4fee\u6539\u5176\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u540e\uff0c\u56db\u79cd\u5148\u8fdb\u6807\u5b9a\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u8fed\u4ee3\u65b9\u6cd5\u53ca\u5176\u5355\u6b65\u7248\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fed\u4ee3\u6846\u67b6\u4e3a\u5916\u53c2\u6807\u5b9a\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u9700\u6c42\u573a\u666f\u3002", "paper_title_zh": "\u57fa\u4e8e\u66ff\u4ee3\u6269\u6563\u7684\u76f8\u673a-LiDAR\u5916\u53c2\u8fed\u4ee3\u4f18\u5316", "abstract_zh": "\u76f8\u673a\u548cLiDAR\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5173\u952e\u4f20\u611f\u5668\u3002\u76f8\u673a\u4e0eLiDAR\u6570\u636e\u7684\u878d\u5408\u5f25\u8865\u4e86\u5355\u4e00\u4f20\u611f\u5668\u7684\u5c40\u9650\u6027\uff0c\u4f46\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u5916\u53c2\u6807\u5b9a\u3002\u8fd1\u5e74\u6765\uff0c\u8bb8\u591a\u7aef\u5230\u7aef\u6807\u5b9a\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u591a\u6570\u4e3a\u5355\u6b65\u9884\u6d4b\u5916\u53c2\uff0c\u7f3a\u4e4f\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\u3002\u4e3a\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u9ad8\u7cbe\u5ea6\u9700\u6c42\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u6269\u6563\u7684\u901a\u7528\u8fed\u4ee3\u6846\u67b6\u3002\u8be5\u6846\u67b6\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u63d0\u5347\u4efb\u4f55\u6807\u5b9a\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u521d\u59cb\u5916\u53c2\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u8fed\u4ee3\u4f18\u5316\uff0c\u539f\u59cb\u6807\u5b9a\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u53bb\u566a\u5668\u9010\u6b65\u4f30\u8ba1\u6700\u7ec8\u5916\u53c2\u3002\u4e3a\u5bf9\u6bd4\u5206\u6790\uff0c\u6211\u4eec\u9009\u53d6\u4e86\u56db\u79cd\u5148\u8fdb\u6807\u5b9a\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u53bb\u566a\u5668\uff0c\u5e76\u5c06\u6269\u6563\u7ed3\u679c\u4e0e\u5176\u4ed6\u4e24\u79cd\u8fed\u4ee3\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u540e\uff0c\u6240\u6709\u6807\u5b9a\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u8fed\u4ee3\u6280\u672f\u53ca\u5176\u5355\u6b65\u7248\u672c\u3002"}}
{"id": "2506.13820", "pdf": "https://arxiv.org/pdf/2506.13820", "abs": "https://arxiv.org/abs/2506.13820", "authors": ["Shraddha Surana", "Ashwin Srinivasan", "Michael Bain"], "title": "Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7ed3\u6784\u5316\u5f52\u7eb3\u7f16\u7a0b\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86IPARC\u6311\u6218\u4e2d\u7684600\u9879\u4efb\u52a1\uff0c\u63ed\u793a\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u6f5c\u529b\u3002", "motivation": "IPARC\u6311\u6218\u63d0\u4f9b\u4e86600\u9879\u5408\u6210\u56fe\u50cf\u4e0a\u7684\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u81ea\u52a8\u7a0b\u5e8f\u6784\u5efa\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5e8f\u5217\u3001\u9009\u62e9\u548c\u8fed\u4ee3\u3002\u8fd9\u4e9b\u4efb\u52a1\u4e00\u76f4\u96be\u4ee5\u88ab\u81ea\u52a8\u5316\u89e3\u51b3\uff0c\u56e0\u6b64\u672c\u6587\u63a2\u7d22\u4e86LLM\u5728\u7ed3\u6784\u5316\u7a0b\u5e8f\u5408\u6210\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7ed3\u6784\u5316\u5f52\u7eb3\u7f16\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u9a8c\u7ed3\u6784\u5316\u3001\u4ee3\u7801\u51bb\u7ed3\u548c\u4ee3\u7801\u91cd\u7528\u7b49\u6280\u672f\uff0c\u7ed3\u5408\u4eba\u7c7b\u7ec6\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86IPARC\u7684\u6240\u6709\u7c7b\u522b\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728IPARC\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u6240\u6709\u4efb\u52a1\u7c7b\u522b\uff0c\u5e76\u63ed\u793a\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5982\u7ed3\u6784\u5316\u8f85\u52a9\u3001\u4ee3\u7801\u51bb\u7ed3\u7684\u5fc5\u8981\u6027\u4ee5\u53ca\u4ee3\u7801\u91cd\u7528\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u7a0b\u5e8f\u5408\u6210\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u6fc0\u53d1\u4eba\u7c7b\u521b\u9020\u529b\uff0c\u5e76\u4e3a\u4eba\u673a\u534f\u4f5c\u89e3\u51b3\u590d\u6742\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u673a\u5236\u3002", "paper_title_zh": "\u4f7f\u7528LLM\u7684\u7ed3\u6784\u5316\u7a0b\u5e8f\u5408\u6210\uff1aIPARC\u6311\u6218\u7684\u7ed3\u679c\u4e0e\u6d1e\u89c1", "abstract_zh": "IPARC\u6311\u6218\u53d7ARC\u542f\u53d1\uff0c\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u5408\u6210\u56fe\u50cf\u7684\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u81ea\u52a8\u7a0b\u5e8f\u6784\u5efa\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u5e8f\u5217\u3001\u9009\u62e9\u548c\u8fed\u4ee3\u3002\u8fd9600\u9879\u4efb\u52a1\u4e00\u76f4\u96be\u4ee5\u88ab\u81ea\u52a8\u5316\u89e3\u51b3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7ed3\u6784\u5316\u5f52\u7eb3\u7f16\u7a0b\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86IPARC\u7684\u6240\u6709\u7c7b\u522b\u4efb\u52a1\u3002IPARC\u7684\u53d7\u63a7\u6027\u8d28\u63ed\u793a\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5173\u952e\u6d1e\u89c1\uff0c\u5305\u62ec\u5148\u9a8c\u7ed3\u6784\u5316\u7684\u91cd\u8981\u6027\u3001LLM\u8f85\u52a9\u7ed3\u6784\u5316\u7684\u80fd\u529b\uff08\u9700\u4eba\u7c7b\u7ec6\u5316\uff09\u3001\u51bb\u7ed3\u6b63\u786e\u4ee3\u7801\u7684\u5fc5\u8981\u6027\u3001\u4ee3\u7801\u91cd\u7528\u7684\u9ad8\u6548\u6027\uff0c\u4ee5\u53caLLM\u751f\u6210\u4ee3\u7801\u5982\u4f55\u6fc0\u53d1\u4eba\u7c7b\u521b\u9020\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u4eba\u7c7b\u4e0eLLM\u534f\u4f5c\u89e3\u51b3\u590d\u6742\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u673a\u5236\u3002"}}
{"id": "2506.13977", "pdf": "https://arxiv.org/pdf/2506.13977", "abs": "https://arxiv.org/abs/2506.13977", "authors": ["Shiting Huang", "Zhen Fang", "Zehui Chen", "Siyu Yuan", "Junjie Ye", "Yu Zeng", "Lin Chen", "Qi Mao", "Feng Zhao"], "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CRITICTOOL\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u9519\u8bef\u573a\u666f\u4e2d\u81ea\u6211\u6279\u5224\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002\u901a\u8fc7\u5206\u6790\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u57fa\u4e8e\u8fdb\u5316\u7b56\u7565\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\uff0cCRITICTOOL\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u5de5\u5177\u53cd\u601d\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5de5\u5177\u4f7f\u7528\u7684\u589e\u52a0\uff0c\u5de5\u5177\u8c03\u7528\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5f15\u53d1\u5404\u79cd\u610f\u5916\u9519\u8bef\u3002\u5982\u4f55\u6709\u6548\u8bc6\u522b\u3001\u8bca\u65ad\u548c\u6062\u590d\u8fd9\u4e9b\u9519\u8bef\u6210\u4e3a\u63a8\u52a8\u5de5\u5177\u5b66\u4e60\u53d1\u5c55\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u591a\u4e2a\u5de5\u5177\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u51fd\u6570\u8c03\u7528\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u57fa\u4e8e\u8fdb\u5316\u7b56\u7565\u6784\u5efa\u4e86CRITICTOOL\uff0c\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5316\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u7684\u7efc\u5408\u6279\u5224\u8bc4\u4f30\u57fa\u51c6\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCRITICTOOL\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u4e0d\u540cLLMs\u5728\u5de5\u5177\u8c03\u7528\u9519\u8bef\u573a\u666f\u4e2d\u7684\u81ea\u6211\u6279\u5224\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u5de5\u5177\u53cd\u601d\u80fd\u529b\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "CRITICTOOL\u4e3a\u5de5\u5177\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u89c6\u89d2\uff0c\u5176\u591a\u6837\u5316\u7684\u9519\u8bef\u573a\u666f\u548c\u8fdb\u5316\u7b56\u7565\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "CRITICTOOL\uff1a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u9519\u8bef\u573a\u666f\u4e2d\u7684\u81ea\u6211\u6279\u5224\u80fd\u529b", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5229\u7528\u5916\u90e8\u5de5\u5177\u7684\u80fd\u529b\u4f7f\u5176\u80fd\u591f\u5904\u7406\u65e5\u76ca\u591a\u6837\u5316\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u968f\u7740\u4efb\u52a1\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u548c\u957f\u671f\u5316\uff0c\u590d\u6742\u7684\u5de5\u5177\u4f7f\u7528\u8fc7\u7a0b\u53ef\u80fd\u5f15\u53d1\u5404\u79cd\u610f\u5916\u9519\u8bef\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u9519\u8bef\uff0c\u5305\u62ec\u8bc6\u522b\u3001\u8bca\u65ad\u548c\u6062\u590d\uff0c\u5df2\u6210\u4e3a\u63a8\u52a8\u5de5\u5177\u5b66\u4e60\u53d1\u5c55\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002\u672c\u7814\u7a76\u9996\u5148\u5e7f\u6cdb\u5206\u6790\u4e86\u591a\u4e2a\u7ade\u4e89\u6027\u5de5\u5177\u8bc4\u4f30\u57fa\u51c6\u4e2d\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u7c7b\u578b\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CRITICTOOL\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5de5\u5177\u5b66\u4e60\u7684\u7efc\u5408\u6279\u5224\u8bc4\u4f30\u57fa\u51c6\u3002\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u8fdb\u5316\u7b56\u7565\u6784\u5efa\u6570\u636e\u96c6\uff0cCRITICTOOL\u5305\u542b\u4e86\u591a\u6837\u5316\u7684\u5de5\u5177\u4f7f\u7528\u9519\u8bef\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u771f\u5b9e\u573a\u666f\u3002\u6211\u4eec\u5728CRITICTOOL\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u6784\u5efa\u57fa\u51c6\u7b56\u7565\u7684\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8fd8\u6df1\u5165\u5206\u6790\u4e86\u4e0d\u540cLLMs\u7684\u5de5\u5177\u53cd\u601d\u80fd\u529b\uff0c\u4e3a\u5de5\u5177\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u4ee3\u7801\u53ef\u5728\\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}\u83b7\u53d6\u3002"}}
{"id": "2506.14709", "pdf": "https://arxiv.org/pdf/2506.14709", "abs": "https://arxiv.org/abs/2506.14709", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted in IROS 2025", "summary": "Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.", "AI": {"tldr": "DiFuse-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u50cf\u7d20\uff08DP\uff09\u548cRGB\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u901a\u8fc7\u7a97\u53e3\u53cc\u5411\u89c6\u5dee\u6ce8\u610f\u529b\u673a\u5236\uff08WBiPAM\uff09\u548c\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\uff08CmTL\uff09\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u8d21\u732e\u4e86\u9ad8\u8d28\u91cfRGB-DP-D\u6570\u636e\u96c6DCDP\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u7acb\u4f53\u89c6\u89c9\u548c\u4e3b\u52a8\u4f20\u611f\u5668\uff09\u5728\u6210\u672c\u3001\u529f\u8017\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u73b0\u4ee3\u76f8\u673a\u666e\u904d\u91c7\u7528\u7684\u53cc\u50cf\u7d20\uff08DP\uff09\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u672c\u6587\u65e8\u5728\u5229\u7528DP\u6280\u672f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002", "method": "DiFuse-Net\u91c7\u7528\u6a21\u6001\u89e3\u8026\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7a97\u53e3\u53cc\u5411\u89c6\u5dee\u6ce8\u610f\u529b\u673a\u5236\uff08WBiPAM\uff09\u6355\u6349\u667a\u80fd\u624b\u673a\u76f8\u673a\u7684\u5c0f\u5149\u5708\u72ec\u7279\u89c6\u5dee\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\uff08CmTL\uff09\u5229\u7528\u5927\u89c4\u6a21RGB-D\u6570\u636e\u96c6\u5f25\u8865RGB-DP-D\u6570\u636e\u4e0d\u8db3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiFuse-Net\u5728DP\u548c\u7acb\u4f53\u89c6\u89c9\u57fa\u51c6\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u8d21\u732e\u4e86\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u4e16\u754cRGB-DP-D\u6570\u636e\u96c6DCDP\u3002", "conclusion": "DiFuse-Net\u901a\u8fc7\u65b0\u9896\u7684\u7f51\u7edc\u8bbe\u8ba1\u548c\u8de8\u6a21\u6001\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u50cf\u7d20\u548cRGB\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u548c\u6570\u636e\u652f\u6301\u3002", "paper_title_zh": "DiFuse-Net\uff1a\u57fa\u4e8e\u7a97\u53e3\u53cc\u5411\u89c6\u5dee\u6ce8\u610f\u529b\u548c\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u7684RGB\u4e0e\u53cc\u50cf\u7d20\u6df1\u5ea6\u4f30\u8ba1", "abstract_zh": "\u6df1\u5ea6\u4f30\u8ba1\u5bf9\u667a\u80fd\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u9886\u57df\u3002\u4f20\u7edf\u7acb\u4f53\u89c6\u89c9\u548c\u4e3b\u52a8\u6df1\u5ea6\u4f20\u611f\u5668\u5728\u6210\u672c\u3001\u529f\u8017\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u73b0\u4ee3\u76f8\u673a\u666e\u904d\u91c7\u7528\u7684\u53cc\u50cf\u7d20\uff08DP\uff09\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u9009\u62e9\u3002\u672c\u6587\u63d0\u51faDiFuse-Net\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u6001\u89e3\u8026\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7528\u4e8e\u89e3\u8026RGB\u548cDP\u6df1\u5ea6\u4f30\u8ba1\u3002DiFuse-Net\u91c7\u7528\u7a97\u53e3\u53cc\u5411\u89c6\u5dee\u6ce8\u610f\u529b\u673a\u5236\uff08WBiPAM\uff09\uff0c\u4e13\u95e8\u6355\u6349\u667a\u80fd\u624b\u673a\u76f8\u673a\u5c0f\u5149\u5708\u7684\u72ec\u7279DP\u89c6\u5dee\u7ebf\u7d22\u3002\u72ec\u7acb\u7684\u7f16\u7801\u5668\u4eceRGB\u56fe\u50cf\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u878d\u5408\u63d0\u5347\u6df1\u5ea6\u9884\u6d4b\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\uff08CmTL\uff09\u673a\u5236\uff0c\u5229\u7528\u6587\u732e\u4e2d\u7684\u5927\u89c4\u6a21RGB-D\u6570\u636e\u96c6\u5e94\u5bf9RGB-DP-D\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728DP\u548c\u7acb\u4f53\u89c6\u89c9\u57fa\u51c6\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8d21\u732e\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u4e16\u754cRGB-DP-D\u8bad\u7ec3\u6570\u636e\u96c6DCDP\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5bf9\u79f0\u7acb\u4f53\u76f8\u673a\u786c\u4ef6\u8bbe\u7f6e\u3001\u7acb\u4f53\u6821\u51c6\u4e0e\u6821\u6b63\u534f\u8bae\u53caAI\u7acb\u4f53\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u6784\u5efa\u3002"}}
{"id": "2506.13824", "pdf": "https://arxiv.org/pdf/2506.13824", "abs": "https://arxiv.org/abs/2506.13824", "authors": ["Jinyang Huang", "Xiachong Feng", "Qiguang Chen", "Hanjie Zhao", "Zihui Cheng", "Jiesong Bai", "Jingxuan Zhou", "Min Li", "Libo Qin"], "title": "MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios", "categories": ["cs.SE", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51faMLDebugging\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u5e93Python\u4ee3\u7801\u8c03\u8bd5\u80fd\u529b\uff0c\u6db5\u76d6126\u4e2a\u5e93\u548c\u4e03\u7c7b\u95ee\u9898\uff0c\u53d1\u73b0\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5e93\u8c03\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u8c03\u8bd5\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65e0\u5e93\u6216\u5355\u5e93\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u5e94\u7528\u4e2d\u590d\u6742\u7684\u591a\u5e93\u73af\u5883\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u591a\u5e93\u8c03\u8bd5\u7814\u7a76\u3002", "method": "\u6784\u5efaMLDebugging\u57fa\u51c6\uff0c\u5305\u542b126\u4e2aPython\u5e93\u548c\u4e03\u7c7b\u591a\u5e93\u4ee3\u7801\u95ee\u9898\uff0c\u5e76\u5bf9\u4e3b\u6d41\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5e93\u8c03\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u4e86\u591a\u5e93\u573a\u666f\u7684\u6311\u6218\u6027\u3002", "conclusion": "MLDebugging\u63ed\u793a\u4e86\u591a\u5e93\u8c03\u8bd5\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u540c\u65f6\u547c\u5401\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u8868\u73b0\u3002", "paper_title_zh": "MLDebugging\uff1a\u9762\u5411\u591a\u5e93\u573a\u666f\u7684\u4ee3\u7801\u8c03\u8bd5\u57fa\u51c6\u7814\u7a76", "abstract_zh": "\u4ee3\u7801\u8c03\u8bd5\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u4ecd\u96c6\u4e2d\u4e8e\u7b80\u5355\u7684\u65e0\u5e93\u6216\u5355\u5e93\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u590d\u6742\u591a\u5e93\u73af\u5883\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u6211\u4eec\u9996\u6b21\u63d0\u51faMLDebugging\uff08\u591a\u5e93\u8c03\u8bd5\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u591a\u5e93Python\u4ee3\u7801\u8c03\u8bd5\u6311\u6218\u7684\u57fa\u51c6\u3002\u5177\u4f53\u800c\u8a00\uff0cMLDebugging\u6db5\u76d6126\u4e2a\u4e0d\u540c\u7684Python\u5e93\uff0c\u6d89\u53ca\u4e03\u7c7b\u591a\u5e93\u4ee3\u7801\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u4e3b\u6d41\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u5bf9MLDebugging\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5f53\u524dLLMs\u5728\u591a\u5e93\u8c03\u8bd5\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u63ed\u793aLLMs\u5728\u591a\u5e93\u8c03\u8bd5\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2506.13992", "pdf": "https://arxiv.org/pdf/2506.13992", "abs": "https://arxiv.org/abs/2506.13992", "authors": ["An Luo", "Xun Xian", "Jin Du", "Fangqiao Tian", "Ganghua Wang", "Ming Zhong", "Shengchun Zhao", "Xuan Bi", "Zirui Liu", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "comment": null, "summary": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7AssistedDS\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u5229\u7528\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u5bf9\u6297\u6027\u4fe1\u606f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u96be\u4ee5\u6709\u6548\u5229\u7528\u6709\u76ca\u77e5\u8bc6\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u4e00\u6837\u6279\u5224\u6027\u5229\u7528\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86AssistedDS\u57fa\u51c6\uff0c\u5305\u542b\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9eKaggle\u7ade\u8d5b\u6570\u636e\uff0c\u5e76\u914d\u4ee5\u6709\u76ca\u548c\u5bf9\u6297\u6027\u6587\u6863\uff0c\u8bc4\u4f30LLMs\u5728\u6570\u636e\u6e05\u7406\u3001\u7279\u5f81\u5de5\u7a0b\u548c\u6a21\u578b\u9009\u62e9\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) LLMs\u6613\u76f2\u76ee\u91c7\u7eb3\u4fe1\u606f\uff0c\u5bf9\u6297\u6027\u5185\u5bb9\u663e\u8457\u964d\u4f4e\u5176\u9884\u6d4b\u6027\u80fd\uff1b(2) \u6709\u76ca\u6307\u5bfc\u96be\u4ee5\u62b5\u6d88\u5bf9\u6297\u6027\u4fe1\u606f\u7684\u8d1f\u9762\u5f71\u54cd\uff1b(3) LLMs\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u5206\u7c7b\u53d8\u91cf\u65f6\u6613\u51fa\u9519\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u6279\u5224\u6027\u8bc4\u4f30\u548c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "paper_title_zh": "AssistedDS\uff1a\u8bc4\u4f30\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u5982\u4f55\u8f85\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u52a8\u4e86\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5316\uff0c\u4f46\u5176\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u4e00\u6837\u6279\u5224\u6027\u5229\u7528\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u5c1a\u4e0d\u660e\u786e\u3002\u4e3a\u89e3\u7b54\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AssistedDS\uff08\u8f85\u52a9\u6570\u636e\u79d1\u5b66\uff09\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u5904\u7406\u9886\u57df\u77e5\u8bc6\u7684\u80fd\u529b\u3002AssistedDS\u5305\u542b\u5177\u6709\u660e\u786e\u751f\u6210\u673a\u5236\u7684\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9eKaggle\u7ade\u8d5b\u6570\u636e\uff0c\u6bcf\u9879\u4efb\u52a1\u5747\u914d\u6709\u7cbe\u5fc3\u7b5b\u9009\u7684\u6709\u76ca\u548c\u5bf9\u6297\u6027\u6587\u6863\uff0c\u6db5\u76d6\u6570\u636e\u6e05\u7406\u3001\u7279\u5f81\u5de5\u7a0b\u548c\u6a21\u578b\u9009\u62e9\u7b49\u9886\u57df\u77e5\u8bc6\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u524d\u6cbfLLMs\u5728\u8fa8\u522b\u548c\u5e94\u7528\u6709\u76ca\u4e0e\u6709\u5bb3\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u62ec\u63d0\u4ea4\u6709\u6548\u6027\u3001\u4fe1\u606f\u53ec\u56de\u548c\u9884\u6d4b\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e09\u70b9\u5173\u952e\u53d1\u73b0\uff1a(1) LLMs\u5e38\u76f2\u76ee\u91c7\u7eb3\u4fe1\u606f\uff0c\u5bf9\u6297\u6027\u5185\u5bb9\u663e\u8457\u964d\u4f4e\u5176\u9884\u6d4b\u6027\u80fd\uff1b(2) \u6709\u76ca\u6307\u5bfc\u5f80\u5f80\u96be\u4ee5\u62b5\u6d88\u5bf9\u6297\u6027\u4fe1\u606f\u7684\u8d1f\u9762\u5f71\u54cd\uff1b(3) \u5728Kaggle\u6570\u636e\u96c6\u4e2d\uff0cLLMs\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3001\u8de8\u4e0d\u540c\u6298\u53e0\u5e94\u7528\u4e00\u81f4\u7279\u5f81\u5de5\u7a0b\u53ca\u6b63\u786e\u89e3\u91ca\u5206\u7c7b\u53d8\u91cf\u65f6\u6613\u51fa\u9519\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6279\u5224\u6027\u8bc4\u4f30\u548c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u65b9\u9762\u7684\u91cd\u5927\u4e0d\u8db3\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u77e5\u8bc6\u611f\u77e5\u7684\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u6307\u660e\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.14730", "pdf": "https://arxiv.org/pdf/2506.14730", "abs": "https://arxiv.org/abs/2506.14730", "authors": ["Corey Scher", "Jamon Van Den Hoek"], "title": "Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War", "categories": ["cs.CV"], "comment": null, "summary": "Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the most intense bombing campaigns of the twenty-first century, driving widespread urban damage. Characterizing damage over a geographically dynamic and protracted armed conflict requires active monitoring. Synthetic aperture radar (SAR) has precedence for mapping disaster-induced damage with bi-temporal methods but applications to active monitoring during sustained crises are limited. Using interferometric SAR data from Sentinel-1, we apply a long temporal-arc coherent change detection (LT-CCD) approach to track weekly damage trends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of damage labels in reference data from the United Nations with a negligible (1.2%) false positive rate. The temporal fidelity of our approach reveals rapidly increasing damage during the first three months of the war focused in northern Gaza, a notable pause in damage during a temporary ceasefire, and surges of new damage as conflict hot-spots shift from north to south. Three-fifths (191,263) of all buildings are damaged or destroyed by the end of the study. With massive need for timely data on damage in armed conflict zones, our low-cost and low-latency approach enables rapid uptake of damage information at humanitarian and journalistic organizations.", "AI": {"tldr": "\u672c\u6587\u5229\u7528Sentinel-1\u7684\u5e72\u6d89\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08InSAR\uff09\u6570\u636e\uff0c\u91c7\u7528\u957f\u65f6\u95f4\u76f8\u5e72\u53d8\u5316\u68c0\u6d4b\uff08LT-CCD\uff09\u65b9\u6cd5\uff0c\u5b9e\u65f6\u76d1\u6d4b\u4e862023\u5e74\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u671f\u95f4\u52a0\u6c99\u5730\u5e26\u7684\u5efa\u7b51\u635f\u6bc1\u60c5\u51b5\u3002\u7ed3\u679c\u663e\u793a\uff0c92.5%\u7684\u635f\u6bc1\u6807\u7b7e\u4e0e\u8054\u5408\u56fd\u53c2\u8003\u6570\u636e\u4e00\u81f4\uff0c\u5047\u9633\u6027\u7387\u4ec5\u4e3a1.2%\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6218\u4e89\u524d\u4e09\u4e2a\u6708\u635f\u6bc1\u8fc5\u901f\u589e\u52a0\uff0c\u505c\u706b\u671f\u95f4\u635f\u6bc1\u6682\u505c\uff0c\u968f\u540e\u635f\u6bc1\u70ed\u70b9\u4ece\u5317\u90e8\u8f6c\u79fb\u81f3\u5357\u90e8\u3002\u7814\u7a76\u7ed3\u675f\u65f6\uff0c\u52a0\u6c99\u5730\u5e26\u4e94\u5206\u4e4b\u4e09\u7684\u5efa\u7b51\uff08191,263\u680b\uff09\u53d7\u635f\u6216\u88ab\u6bc1\u3002", "motivation": "2023\u5e7410\u67087\u65e5\u5f00\u59cb\u7684\u52a0\u6c99\u5730\u5e26\u7a7a\u88ad\u662f21\u4e16\u7eaa\u6700\u731b\u70c8\u7684\u8f70\u70b8\u884c\u52a8\u4e4b\u4e00\uff0c\u5bfc\u81f4\u5e7f\u6cdb\u7684\u57ce\u5e02\u635f\u6bc1\u3002\u5728\u52a8\u6001\u4e14\u6301\u4e45\u7684\u6b66\u88c5\u51b2\u7a81\u4e2d\uff0c\u5b9e\u65f6\u76d1\u6d4b\u635f\u6bc1\u60c5\u51b5\u81f3\u5173\u91cd\u8981\u3002\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u5728\u707e\u5bb3\u635f\u6bc1\u6d4b\u7ed8\u4e2d\u6709\u5e94\u7528\u5148\u4f8b\uff0c\u4f46\u5728\u6301\u7eed\u5371\u673a\u4e2d\u7684\u5b9e\u65f6\u76d1\u6d4b\u5e94\u7528\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4f7f\u7528Sentinel-1\u7684\u5e72\u6d89SAR\u6570\u636e\uff0c\u91c7\u7528\u957f\u65f6\u95f4\u76f8\u5e72\u53d8\u5316\u68c0\u6d4b\uff08LT-CCD\uff09\u65b9\u6cd5\uff0c\u6bcf\u5468\u8ddf\u8e2a2023\u5e74\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u7b2c\u4e00\u5e74\u7684\u635f\u6bc1\u8d8b\u52bf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c92.5%\u7684\u635f\u6bc1\u6807\u7b7e\u4e0e\u8054\u5408\u56fd\u53c2\u8003\u6570\u636e\u4e00\u81f4\uff0c\u5047\u9633\u6027\u7387\u4ec5\u4e3a1.2%\u3002\u6218\u4e89\u524d\u4e09\u4e2a\u6708\u635f\u6bc1\u8fc5\u901f\u589e\u52a0\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u52a0\u6c99\u5317\u90e8\uff1b\u505c\u706b\u671f\u95f4\u635f\u6bc1\u6682\u505c\uff1b\u968f\u540e\u635f\u6bc1\u70ed\u70b9\u8f6c\u79fb\u81f3\u5357\u90e8\u3002\u7814\u7a76\u7ed3\u675f\u65f6\uff0c\u4e94\u5206\u4e4b\u4e09\u7684\u5efa\u7b51\uff08191,263\u680b\uff09\u53d7\u635f\u6216\u88ab\u6bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4f4e\u6210\u672c\u3001\u4f4e\u5ef6\u8fdf\u65b9\u6cd5\u4e3a\u6b66\u88c5\u51b2\u7a81\u533a\u57df\u7684\u635f\u6bc1\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ca\u65f6\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u4eba\u9053\u4e3b\u4e49\u548c\u65b0\u95fb\u673a\u6784\u5feb\u901f\u83b7\u53d6\u4fe1\u606f\u3002", "paper_title_zh": "\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u671f\u95f4\u52a0\u6c99\u5730\u5e26\u5efa\u7b51\u635f\u6bc1\u7684\u4e3b\u52a8InSAR\u76d1\u6d4b", "abstract_zh": "2023\u5e7410\u67087\u65e5\u5f00\u59cb\u7684\u52a0\u6c99\u5730\u5e26\u7a7a\u88ad\u662f21\u4e16\u7eaa\u6700\u731b\u70c8\u7684\u8f70\u70b8\u884c\u52a8\u4e4b\u4e00\uff0c\u5bfc\u81f4\u5e7f\u6cdb\u7684\u57ce\u5e02\u635f\u6bc1\u3002\u5728\u52a8\u6001\u4e14\u6301\u4e45\u7684\u6b66\u88c5\u51b2\u7a81\u4e2d\uff0c\u5b9e\u65f6\u76d1\u6d4b\u635f\u6bc1\u60c5\u51b5\u81f3\u5173\u91cd\u8981\u3002\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u5728\u707e\u5bb3\u635f\u6bc1\u6d4b\u7ed8\u4e2d\u6709\u5e94\u7528\u5148\u4f8b\uff0c\u4f46\u5728\u6301\u7eed\u5371\u673a\u4e2d\u7684\u5b9e\u65f6\u76d1\u6d4b\u5e94\u7528\u6709\u9650\u3002\u672c\u7814\u7a76\u4f7f\u7528Sentinel-1\u7684\u5e72\u6d89SAR\u6570\u636e\uff0c\u91c7\u7528\u957f\u65f6\u95f4\u76f8\u5e72\u53d8\u5316\u68c0\u6d4b\uff08LT-CCD\uff09\u65b9\u6cd5\uff0c\u6bcf\u5468\u8ddf\u8e2a2023\u5e74\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u7b2c\u4e00\u5e74\u7684\u635f\u6bc1\u8d8b\u52bf\u3002\u7ed3\u679c\u663e\u793a\uff0c92.5%\u7684\u635f\u6bc1\u6807\u7b7e\u4e0e\u8054\u5408\u56fd\u53c2\u8003\u6570\u636e\u4e00\u81f4\uff0c\u5047\u9633\u6027\u7387\u4ec5\u4e3a1.2%\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6218\u4e89\u524d\u4e09\u4e2a\u6708\u635f\u6bc1\u8fc5\u901f\u589e\u52a0\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u52a0\u6c99\u5317\u90e8\uff1b\u505c\u706b\u671f\u95f4\u635f\u6bc1\u6682\u505c\uff1b\u968f\u540e\u635f\u6bc1\u70ed\u70b9\u8f6c\u79fb\u81f3\u5357\u90e8\u3002\u7814\u7a76\u7ed3\u675f\u65f6\uff0c\u4e94\u5206\u4e4b\u4e09\u7684\u5efa\u7b51\uff08191,263\u680b\uff09\u53d7\u635f\u6216\u88ab\u6bc1\u3002\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4f4e\u6210\u672c\u3001\u4f4e\u5ef6\u8fdf\u65b9\u6cd5\u4e3a\u6b66\u88c5\u51b2\u7a81\u533a\u57df\u7684\u635f\u6bc1\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ca\u65f6\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u4eba\u9053\u4e3b\u4e49\u548c\u65b0\u95fb\u673a\u6784\u5feb\u901f\u83b7\u53d6\u4fe1\u606f\u3002"}}
{"id": "2506.13827", "pdf": "https://arxiv.org/pdf/2506.13827", "abs": "https://arxiv.org/abs/2506.13827", "authors": ["Zhuoying Li", "Zhu Xu", "Yuxin Peng", "Yang Liu"], "title": "Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBPM\u7684\u65b0\u6307\u6807\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u8d28\u91cf\uff0c\u901a\u8fc7\u5206\u79bb\u7f16\u8f91\u76f8\u5173\u4e0e\u65e0\u5173\u533a\u57df\uff0c\u5b9e\u73b0\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u73b0\u6709\u6307\u6807\u8981\u4e48\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u4eba\u5de5\u8bc4\u4f30\uff0c\u8981\u4e48\u65e0\u6cd5\u517c\u987e\u7f16\u8f91\u76f8\u5173\u4e0e\u65e0\u5173\u533a\u57df\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u8bc4\u4f30\u504f\u5dee\u3002", "method": "BPM\u6307\u6807\u901a\u8fc7\u5b9a\u4f4d\u7f16\u8f91\u76f8\u5173\u533a\u57df\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\uff1a\u533a\u57df\u611f\u77e5\u5224\u65ad\u7f16\u8f91\u533a\u57df\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\u662f\u5426\u7b26\u5408\u6307\u4ee4\uff0c\u8bed\u4e49\u611f\u77e5\u5224\u65ad\u7f16\u8f91\u5185\u5bb9\u5408\u89c4\u6027\u53ca\u65e0\u5173\u533a\u57df\u7684\u5185\u5bb9\u4fdd\u7559\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBPM\u6307\u6807\u5728\u7efc\u5408\u6307\u4ee4\u7f16\u8f91\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u5951\u5408\u5ea6\u6700\u9ad8\uff0c\u4e14\u5176\u5b9a\u4f4d\u529f\u80fd\u53ef\u63d0\u5347\u7f16\u8f91\u65b9\u6cd5\u7684\u8d28\u91cf\u3002", "conclusion": "BPM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u5e76\u80fd\u63d0\u5347\u7f16\u8f91\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "paper_title_zh": "\u5e73\u8861\u4fdd\u7559\u4e0e\u4fee\u6539\uff1a\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u548c\u8bed\u4e49\u611f\u77e5\u7684\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u6307\u6807", "abstract_zh": "\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65e8\u5728\u6839\u636e\u6307\u4ee4\u4fee\u6539\u56fe\u50cf\u7684\u540c\u65f6\u4fdd\u7559\u65e0\u5173\u5185\u5bb9\uff0c\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002\u73b0\u6709\u6307\u6807\u8981\u4e48\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u4eba\u5de5\u8bc4\u4f30\uff0c\u963b\u788d\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u8981\u4e48\u4ece\u5176\u4ed6\u4efb\u52a1\u8fc1\u79fb\u800c\u6765\uff0c\u65e0\u6cd5\u517c\u987e\u7f16\u8f91\u76f8\u5173\u4e0e\u65e0\u5173\u533a\u57df\u7684\u8bc4\u4f30\uff0c\u5bfc\u81f4\u504f\u5dee\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBPM\u7684\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u660e\u786e\u5206\u79bb\u7f16\u8f91\u76f8\u5173\u4e0e\u65e0\u5173\u533a\u57df\uff0c\u5b9e\u73b0\u9488\u5bf9\u6027\u8bc4\u4f30\u3002\u9996\u5148\u5b9a\u4f4d\u7f16\u8f91\u76f8\u5173\u533a\u57df\uff0c\u968f\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\uff1a\u533a\u57df\u611f\u77e5\u5224\u65ad\u7f16\u8f91\u533a\u57df\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\u662f\u5426\u7b26\u5408\u6307\u4ee4\uff0c\u8bed\u4e49\u611f\u77e5\u8fdb\u4e00\u6b65\u8bc4\u4f30\u7f16\u8f91\u76f8\u5173\u533a\u57df\u7684\u6307\u4ee4\u5408\u89c4\u6027\u53ca\u65e0\u5173\u533a\u57df\u7684\u5185\u5bb9\u4fdd\u7559\uff0c\u63d0\u4f9b\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u8d28\u91cf\u8bc4\u4f30\u3002\u6b64\u5916\uff0cBPM\u7684\u7f16\u8f91\u76f8\u5173\u533a\u57df\u5b9a\u4f4d\u529f\u80fd\u53ef\u6574\u5408\u5230\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e2d\uff0c\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\uff0c\u5c55\u73b0\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002\u6211\u4eec\u5728\u7efc\u5408\u6307\u4ee4\u7f16\u8f91\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86BPM\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u5951\u5408\u5ea6\u6700\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u3002\u4ee3\u7801\u53d1\u5e03\u4e8e\uff1ahttps://joyli-x.github.io/BPM/"}}
{"id": "2506.14086", "pdf": "https://arxiv.org/pdf/2506.14086", "abs": "https://arxiv.org/abs/2506.14086", "authors": ["Rahul Seetharaman", "Kaustubh D. Dhole", "Aman Bansal"], "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.", "AI": {"tldr": "InsertRank\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408BM25\u5206\u6570\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u9886\u57df\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u6237\u5bf9\u590d\u6742\u67e5\u8be2\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u5173\u952e\u8bcd\u5339\u914d\u6216\u8bed\u4e49\u76f8\u4f3c\u6027\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u63a8\u7406\u80fd\u529b\u548c\u4f20\u7edf\u68c0\u7d22\u4fe1\u53f7\uff08\u5982BM25\u5206\u6570\uff09\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u3002", "method": "InsertRank\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408BM25\u5206\u6570\u7b49\u8bcd\u6c47\u4fe1\u53f7\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ece\u800c\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aLLM\u5bb6\u65cf\uff08\u5982GPT\u3001Gemini\u548cDeepseek\uff09\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5728BRIGHT\uff08\u6db5\u76d612\u4e2a\u9886\u57df\u7684\u63a8\u7406\u57fa\u51c6\uff09\u548cR2MED\uff08\u6db5\u76d68\u4e2a\u4efb\u52a1\u7684\u533b\u5b66\u63a8\u7406\u57fa\u51c6\uff09\u4e0a\uff0cInsertRank\u8868\u73b0\u4f18\u5f02\u3002\u4f8b\u5982\uff0c\u4f7f\u7528Deepseek-R1\u65f6\uff0cInsertRank\u5728BRIGHT\u548cR2MED\u4e0a\u7684\u5f97\u5206\u5206\u522b\u4e3a37.5\u548c51.1\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "InsertRank\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f20\u7edf\u68c0\u7d22\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u67e5\u8be2\u573a\u666f\u3002", "paper_title_zh": "InsertRank\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u901a\u8fc7BM25\u5206\u6570\u63a8\u7406\u63d0\u5347\u5217\u8868\u5f0f\u91cd\u65b0\u6392\u5e8f\u6548\u679c", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51ed\u501f\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4ece\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u83b7\u5f97\u7684\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\uff0c\u5728\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u91cd\u65b0\u6392\u5e8f\u4efb\u52a1\u4e2d\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u57fa\u4e8eLLM\u7684\u804a\u5929\u754c\u9762\u7684\u5174\u8d77\u63d0\u9ad8\u4e86\u7528\u6237\u671f\u671b\uff0c\u9f13\u52b1\u7528\u6237\u63d0\u51fa\u66f4\u590d\u6742\u7684\u67e5\u8be2\uff0c\u8fd9\u4e9b\u67e5\u8be2\u9700\u8981\u901a\u8fc7\u201c\u63a8\u7406\u201d\u6587\u6863\u800c\u975e\u7b80\u5355\u7684\u5173\u952e\u8bcd\u5339\u914d\u6216\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u68c0\u7d22\u3002\u5c3d\u7ba1\u6700\u8fd1\u4e00\u4e9b\u7814\u7a76\u5c1d\u8bd5\u5229\u7528LLMs\u7684\u63a8\u7406\u80fd\u529b\u91cd\u65b0\u6392\u5e8f\u6b64\u7c7b\u67e5\u8be2\uff0c\u4f46\u4ecd\u5b58\u5728\u8f83\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86InsertRank\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u5728\u91cd\u65b0\u6392\u5e8f\u8fc7\u7a0b\u4e2d\u5229\u7528BM25\u5206\u6570\u7b49\u8bcd\u6c47\u4fe1\u53f7\u8fdb\u4e00\u6b65\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002InsertRank\u5728BRIGHT\uff08\u6db5\u76d612\u4e2a\u591a\u6837\u5316\u9886\u57df\u7684\u63a8\u7406\u57fa\u51c6\uff09\u548cR2MED\uff08\u6db5\u76d68\u4e2a\u4e0d\u540c\u4efb\u52a1\u7684\u533b\u5b66\u63a8\u7406\u57fa\u51c6\uff09\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u68c0\u7d22\u6548\u679c\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u8bc4\u4f30\u548c\u591a\u9879\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660eInsertRank\u5728\u5305\u62ecGPT\u3001Gemini\u548cDeepseek\u6a21\u578b\u5728\u5185\u7684\u591a\u4e2aLLM\u5bb6\u65cf\u4e2d\u5747\u80fd\u6301\u7eed\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002\u4f7f\u7528Deepseek-R1\u65f6\uff0cInsertRank\u5728BRIGHT\u548cR2MED\u57fa\u51c6\u4e0a\u7684\u5f97\u5206\u5206\u522b\u4e3a37.5\u548c51.1\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.14742", "pdf": "https://arxiv.org/pdf/2506.14742", "abs": "https://arxiv.org/abs/2506.14742", "authors": ["Ziqiao Peng", "Wentao Hu", "Junyuan Ma", "Xiangyu Zhu", "Xiaomei Zhang", "Hao Zhao", "Hui Tian", "Jun He", "Hongyan Liu", "Zhaoxin Fan"], "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.", "AI": {"tldr": "SyncTalk++\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u548c\u591a\u6a21\u5757\u534f\u540c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u9ad8\u6548\u7684\u8bed\u97f3\u9a71\u52a8\u8bf4\u8bdd\u5934\u89c6\u9891\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u540c\u6b65\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "motivation": "\u5408\u6210\u903c\u771f\u7684\u8bed\u97f3\u9a71\u52a8\u8bf4\u8bdd\u5934\u89c6\u9891\u65f6\uff0c\u540c\u6b65\u6027\u95ee\u9898\uff08\u5982\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u5507\u52a8\u3001\u8868\u60c5\u548c\u5934\u90e8\u59ff\u6001\uff09\u662f\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u56e0\u540c\u6b65\u6027\u4e0d\u8db3\u5bfc\u81f4\u7ed3\u679c\u4e0d\u771f\u5b9e\u3002SyncTalk++\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u5408\u6210\u89c6\u9891\u7684\u540c\u6b65\u6027\u548c\u771f\u5b9e\u611f\u3002", "method": "SyncTalk++\u91c7\u7528\u52a8\u6001\u8096\u50cf\u6e32\u67d3\u5668\uff08\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff09\u786e\u4fdd\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9762\u90e8\u540c\u6b65\u63a7\u5236\u5668\uff083D\u9762\u90e8\u6df7\u5408\u5f62\u72b6\u6a21\u578b\uff09\u7cbe\u51c6\u5bf9\u9f50\u5507\u52a8\u4e0e\u8bed\u97f3\uff0c\u5934\u90e8\u540c\u6b65\u7a33\u5b9a\u5668\u4f18\u5316\u5934\u90e8\u59ff\u6001\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8868\u60c5\u751f\u6210\u5668\u548c\u8eaf\u5e72\u4fee\u590d\u5668\u589e\u5f3a\u5bf9\u5206\u5e03\u5916\u97f3\u9891\u7684\u9c81\u68d2\u6027\u3002", "result": "SyncTalk++\u5728\u540c\u6b65\u6027\u548c\u771f\u5b9e\u611f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6e32\u67d3\u901f\u5ea6\u8fbe\u6bcf\u79d2101\u5e27\uff0c\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u5176\u663e\u8457\u63d0\u5347\u7684\u89c6\u89c9\u6548\u679c\u548c\u6d41\u7545\u6027\u3002", "conclusion": "SyncTalk++\u901a\u8fc7\u591a\u6a21\u5757\u534f\u540c\u548c\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bf4\u8bdd\u5934\u89c6\u9891\u5408\u6210\u7684\u540c\u6b65\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u9ad8\u6548\u6e32\u67d3\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SyncTalk++\uff1a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u9ad8\u4fdd\u771f\u9ad8\u6548\u540c\u6b65\u8bf4\u8bdd\u5934\u5408\u6210", "abstract_zh": "\u5728\u5408\u6210\u903c\u771f\u7684\u8bed\u97f3\u9a71\u52a8\u8bf4\u8bdd\u5934\u89c6\u9891\u65f6\uff0c\u5b9e\u73b0\u9ad8\u5ea6\u540c\u6b65\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u903c\u771f\u7684\u8bf4\u8bdd\u5934\u9700\u8981\u540c\u6b65\u534f\u8c03\u4e3b\u4f53\u8eab\u4efd\u3001\u5507\u52a8\u3001\u8868\u60c5\u548c\u5934\u90e8\u59ff\u6001\u3002\u7f3a\u4e4f\u8fd9\u4e9b\u540c\u6b65\u4f1a\u5bfc\u81f4\u7ed3\u679c\u4e0d\u771f\u5b9e\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff08\u88ab\u79f0\u4e3a\u521b\u5efa\u903c\u771f\u8bf4\u8bdd\u5934\u7684\u201c\u9b54\u9b3c\u201d\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SyncTalk++\u3002\u5b83\u91c7\u7528\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u52a8\u6001\u8096\u50cf\u6e32\u67d3\u5668\u786e\u4fdd\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9762\u90e8\u540c\u6b65\u63a7\u5236\u5668\u901a\u8fc73D\u9762\u90e8\u6df7\u5408\u5f62\u72b6\u6a21\u578b\u7cbe\u51c6\u5bf9\u9f50\u5507\u52a8\u4e0e\u8bed\u97f3\uff0c\u5934\u90e8\u540c\u6b65\u7a33\u5b9a\u5668\u4f18\u5316\u5934\u90e8\u59ff\u6001\u4ee5\u589e\u5f3a\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0cSyncTalk++\u901a\u8fc7\u8868\u60c5\u751f\u6210\u5668\u548c\u8eaf\u5e72\u4fee\u590d\u5668\u63d0\u5347\u5bf9\u5206\u5e03\u5916\u97f3\u9891\u7684\u9c81\u68d2\u6027\uff0c\u751f\u6210\u4e0e\u8bed\u97f3\u5339\u914d\u7684\u8868\u60c5\u548c\u65e0\u7f1d\u8eaf\u5e72\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u5728\u5e27\u95f4\u4fdd\u6301\u89c6\u89c9\u7ec6\u8282\u7684\u4e00\u81f4\u6027\u548c\u8fde\u7eed\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u548c\u8d28\u91cf\uff0c\u6700\u9ad8\u53ef\u8fbe\u6bcf\u79d2101\u5e27\u3002\u5927\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cSyncTalk++\u5728\u540c\u6b65\u6027\u548c\u771f\u5b9e\u611f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5efa\u8bae\u89c2\u770b\u8865\u5145\u89c6\u9891\uff1ahttps://ziqiaopeng.github.io/synctalk++\u3002"}}
{"id": "2506.13831", "pdf": "https://arxiv.org/pdf/2506.13831", "abs": "https://arxiv.org/abs/2506.13831", "authors": ["Jitian Zhao", "Chenghui Li", "Frederic Sala", "Karl Rohe"], "title": "Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316CLIP\u5d4c\u5165\u4e2d\u7684\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u9a8c\u8bc1\u6982\u5ff5\u7684\u7a33\u5065\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u9a8c\u6982\u5ff5\u5206\u89e3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6982\u5ff5\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u5728\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5982CLIP\uff09\u5d4c\u5165\u65f6\u7f3a\u4e4f\u7edf\u8ba1\u4e25\u8c28\u6027\uff0c\u96be\u4ee5\u9a8c\u8bc1\u6982\u5ff5\u7684\u6709\u6548\u6027\u6216\u6bd4\u8f83\u4e0d\u540c\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u91cf\u5316\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u7684\u7edf\u8ba1\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316CLIP\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u65cb\u8f6c\u654f\u611f\u7684\u7ed3\u6784\u3002\u968f\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u9a8c\u6982\u5ff5\u5206\u89e3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u80fd\u591f\u53d1\u73b0\u7a33\u5065\u4e14\u53ef\u590d\u73b0\u7684\u6982\u5ff5\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8bef\u5dee\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u6d88\u9664\u6570\u636e\u4e2d\u7684\u865a\u5047\u7ebf\u7d22\u540e\uff0c\u5c06\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u63d0\u5347\u4e8622.6%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u91cd\u5efa\u7cbe\u5ea6\uff0c\u8fd8\u80fd\u6709\u6548\u51cf\u5c11\u6570\u636e\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002", "paper_title_zh": "\u91cf\u5316CLIP\u5d4c\u5165\u4e2d\u7684\u7ed3\u6784\uff1a\u4e00\u79cd\u6982\u5ff5\u89e3\u91ca\u7684\u7edf\u8ba1\u6846\u67b6", "abstract_zh": "\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u65e8\u5728\u4ece\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u5185\u90e8\u8868\u793a\u4e2d\u8bc6\u522b\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u6982\u5ff5\uff0c\u662f\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u8ba1\u4e25\u8c28\u6027\uff0c\u96be\u4ee5\u9a8c\u8bc1\u8bc6\u522b\u7684\u6982\u5ff5\u6216\u6bd4\u8f83\u4e0d\u540c\u6280\u672f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316CLIP\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u65cb\u8f6c\u654f\u611f\u7684\u7ed3\u6784\u3002\u4e00\u65e6\u8bc6\u522b\u51fa\u8fd9\u4e9b\u7ed3\u6784\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u9a8c\u6982\u5ff5\u5206\u89e3\u65b9\u6cd5\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u80fd\u591f\u53d1\u73b0\u7a33\u5065\u4e14\u53ef\u590d\u73b0\u7684\u6982\u5ff5\u6a21\u5f0f\uff08\u800c\u975e\u65b9\u6cd5\u7279\u5b9a\u7684\u4f2a\u5f71\uff09\uff0c\u5e76\u5728\u91cd\u5efa\u8bef\u5dee\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6982\u5ff5\u5206\u89e3\u7b97\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0e\u6982\u5ff5\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5e76\u6709\u52a9\u4e8e\u51cf\u5c11\u6570\u636e\u4e2d\u7684\u865a\u5047\u7ebf\u7d22\u3002\u5728\u5e94\u7528\u4e8e\u4e00\u4e2a\u6d41\u884c\u7684\u865a\u5047\u76f8\u5173\u6570\u636e\u96c6\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53bb\u9664\u865a\u5047\u80cc\u666f\u6982\u5ff5\u540e\uff0c\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u63d0\u5347\u4e8622.6%\u3002"}}
{"id": "2506.14104", "pdf": "https://arxiv.org/pdf/2506.14104", "abs": "https://arxiv.org/abs/2506.14104", "authors": ["RuiKun Yang", "ZhongLiang Wei", "Longdi Xian"], "title": "Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints", "categories": ["cs.GR", "cs.CL", "cs.CY"], "comment": null, "summary": "Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fr\u00e9chet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability (\u03c3 = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408DeepSeek\u548cMidJourney\u6280\u672f\uff0c\u521b\u65b0\u6027\u5730\u751f\u6210\u4ee5\u6768\u67f3\u9752\u6728\u7248\u5e74\u753b\u4e3a\u4e3b\u9898\u7684\u6297\u51fbCOVID-19\u548c\u6b22\u4e50\u83b7\u80dc\u8005\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5728FID\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u53c2\u4e0e\u8005\u53cd\u9988\u663e\u793a\u5176\u6700\u5177\u4ee3\u8868\u6027\u548c\u6587\u5316\u63a8\u5e7f\u4ef7\u503c\u3002", "motivation": "\u6768\u67f3\u9752\u6728\u7248\u5e74\u753b\u4f5c\u4e3a\u4e2d\u56fd\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\uff0c\u5176\u4f20\u7edf\u827a\u672f\u5f62\u5f0f\u5728\u4fdd\u62a4\u4e0e\u521b\u65b0\u4e4b\u95f4\u5b58\u5728\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u73b0\u4ee3AI\u6280\u672f\u4e3a\u5176\u6ce8\u5165\u65b0\u6d3b\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u5316\u4f20\u627f\u3002", "method": "\u7814\u7a76\u91c7\u7528DeepSeek\u751f\u6210\u4e3b\u9898\u63d0\u793a\uff0cMidJourney\u751f\u6210\u4e3b\u9898\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u6768\u67f3\u9752\u5e74\u753b\u548cDeepSeek\u751f\u6210\u7684\u5173\u952e\u63d0\u793a\u3002\u901a\u8fc7FID\u8bc4\u5206\u548c62\u540d\u53c2\u4e0e\u8005\u7684\u95ee\u5377\u53cd\u9988\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u5728FID\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f18\uff08\u5747\u503c\u4e3a150.2\uff0c\u6807\u51c6\u5dee\u4e3a4.9\uff09\uff0c\u4e14\u53c2\u4e0e\u8005\u8ba4\u4e3a\u5176\u6700\u5177\u4ee3\u8868\u6027\uff0c\u5e76\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u6587\u5316\u63a8\u5e7f\u610f\u613f\u548c\u6d88\u8d39\u5174\u8da3\u3002", "conclusion": "\u7ed3\u5408\u4f20\u7edf\u827a\u672f\u5143\u7d20\u4e0e\u73b0\u4ee3AI\u6280\u672f\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u65e2\u80fd\u6709\u6548\u4fdd\u62a4\u6587\u5316\u9057\u4ea7\uff0c\u53c8\u80fd\u8d4b\u4e88\u5176\u5f53\u4ee3\u610f\u4e49\uff0c\u4e3a\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\u7684\u4f20\u627f\u4e0e\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u5229\u7528DeepSeek + MidJourney\u521b\u65b0\u4e2d\u56fd\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\uff1a\u4ee5\u6768\u67f3\u9752\u4e3b\u9898\u6728\u7248\u5e74\u753b\u4e3a\u4f8b", "abstract_zh": "\u6768\u67f3\u9752\u6728\u7248\u5e74\u753b\u4f5c\u4e3a\u4e2d\u56fd\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\u7684\u4ee3\u8868\uff0c\u4ee5\u5176\u7cbe\u7f8e\u7684\u8bbe\u8ba1\u548c\u9c9c\u8273\u7684\u8272\u5f69\u95fb\u540d\u3002\u7136\u800c\uff0c\u5982\u4f55\u5728\u4fdd\u62a4\u4f20\u7edf\u827a\u672f\u7684\u540c\u65f6\u63a8\u52a8\u521b\u65b0\u6210\u4e3a\u4e00\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86DeepSeek + MidJourney\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4ee5\u6297\u51fbCOVID-19\u548c\u6b22\u4e50\u83b7\u80dc\u8005\u4e3a\u4e3b\u9898\u7684\u521b\u610f\u6768\u67f3\u9752\u6728\u7248\u5e74\u753b\u3002\u901a\u8fc7Fr\u00e9chet Inception Distance\uff08FID\uff09\u8bc4\u5206\u8bc4\u4f30\uff0c\u7ed3\u5408DeepSeek\u751f\u6210\u7684\u4e3b\u9898\u63d0\u793a\u3001MidJourney\u751f\u6210\u7684\u4e3b\u9898\u56fe\u50cf\u3001\u539f\u59cb\u6768\u67f3\u9752\u5e74\u753b\u4ee5\u53caDeepSeek\u5728MidJourney\u8f93\u51fa\u4e2d\u751f\u6210\u7684\u5173\u952e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u4f4e\u7684FID\u5747\u503c\uff08150.2\uff09\u548c\u6700\u5c0f\u7684\u53d8\u5f02\u6027\uff08\u03c3 = 4.9\uff09\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u95ee\u5377\u6536\u96c6\u768462\u540d\u53c2\u4e0e\u8005\u7684\u53cd\u9988\u8bc1\u5b9e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u6700\u5177\u4ee3\u8868\u6027\u3002\u95ee\u5377\u6570\u636e\u8fd8\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u5bf9\u8fd9\u79cdAI\u751f\u6210\u56fe\u50cf\u7684\u6587\u5316\u63a8\u5e7f\u610f\u613f\u548c\u6d88\u8d39\u5174\u8da3\u6700\u9ad8\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5c06\u4f20\u7edf\u827a\u672f\u5143\u7d20\u4e0e\u73b0\u4ee3AI\u9a71\u52a8\u7684\u521b\u9020\u529b\u65e0\u7f1d\u7ed3\u5408\uff0c\u65e2\u80fd\u786e\u4fdd\u6587\u5316\u4fdd\u62a4\uff0c\u53c8\u80fd\u8d4b\u4e88\u5176\u5f53\u4ee3\u610f\u4e49\u3002"}}
{"id": "2506.14753", "pdf": "https://arxiv.org/pdf/2506.14753", "abs": "https://arxiv.org/abs/2506.14753", "authors": ["Qinchan", "Li", "Kenneth Chen", "Changyue", "Su", "Wittawat Jitkrittum", "Qi Sun", "Patsorn Sangkloy"], "title": "Cost-Aware Routing for Efficient Text-To-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u611f\u77e5\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u63d0\u793a\u8bcd\u7684\u590d\u6742\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ee5\u4f18\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u5176\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u6839\u636e\u63d0\u793a\u8bcd\u7684\u590d\u6742\u5ea6\u5206\u914d\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\u6216\u6b65\u9aa4\uff0c\u4ee5\u5b9e\u73b0\u8d28\u91cf\u4e0e\u6210\u672c\u7684\u6700\u4f18\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u81ea\u52a8\u5c06\u63d0\u793a\u8bcd\u8def\u7531\u5230\u6700\u9002\u5408\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u53ef\u80fd\u5bf9\u5e94\u4e0d\u540c\u53bb\u566a\u6b65\u9aa4\u7684\u6269\u6563\u6a21\u578b\u6216\u72ec\u7acb\u7684\u751f\u6210\u6a21\u578b\u3002\u901a\u8fc7\u4fdd\u7559\u9ad8\u6210\u672c\u9009\u62e9\uff08\u5982100+\u53bb\u566a\u6b65\u9aa4\uff09\u4ec5\u7528\u4e8e\u590d\u6742\u63d0\u793a\u8bcd\uff0c\u800c\u5bf9\u7b80\u5355\u63d0\u793a\u8bcd\u91c7\u7528\u4f4e\u6210\u672c\u9009\u62e9\uff08\u5982\u5c0f\u578b\u84b8\u998f\u6a21\u578b\uff09\u3002", "result": "\u5728COCO\u548cDiffusionDB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5b66\u4e60\u8def\u7531\u5230\u4e5d\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u6a21\u578b\u66f4\u9ad8\u7684\u5e73\u5747\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6210\u672c\u611f\u77e5\u8def\u7531\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u590d\u6742\u63d0\u793a\u8bcd\u4fdd\u7559\u9ad8\u6210\u672c\u9009\u62e9\uff0c\u540c\u65f6\u4e3a\u7b80\u5355\u63d0\u793a\u8bcd\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "paper_title_zh": "\u6210\u672c\u611f\u77e5\u8def\u7531\u7528\u4e8e\u9ad8\u6548\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210", "abstract_zh": "\u6269\u6563\u6a21\u578b\u4ee5\u5176\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4e3a\u8f93\u5165\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u80fd\u529b\u800c\u95fb\u540d\u3002\u7136\u800c\uff0c\u7531\u4e8e\u56fa\u6709\u7684\u987a\u5e8f\u751f\u6210\u8fc7\u7a0b\uff0c\u9ad8\u8d28\u91cf\u4e5f\u4f34\u968f\u7740\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u5e73\u8861\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u6839\u636e\u63d0\u793a\u8bcd\u7684\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u91cf\u3002\u6bcf\u4e2a\u63d0\u793a\u8bcd\u4f1a\u81ea\u52a8\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u53ef\u80fd\u5bf9\u5e94\u6269\u6563\u6a21\u578b\u7684\u4e0d\u540c\u53bb\u566a\u6b65\u9aa4\uff0c\u6216\u72ec\u7acb\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002\u4e0e\u7edf\u4e00\u6210\u672c\u964d\u4f4e\u6280\u672f\uff08\u5982\u84b8\u998f\u3001\u6a21\u578b\u91cf\u5316\uff09\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u4ec5\u4e3a\u5c11\u6570\u590d\u6742\u63d0\u793a\u8bcd\u4fdd\u7559\u6602\u8d35\u9009\u62e9\uff08\u5982100+\u53bb\u566a\u6b65\u9aa4\uff09\uff0c\u800c\u5bf9\u4e0d\u592a\u590d\u6742\u7684\u63d0\u793a\u8bcd\u91c7\u7528\u66f4\u7ecf\u6d4e\u7684\u9009\u62e9\uff08\u5982\u5c0f\u578b\u84b8\u998f\u6a21\u578b\uff09\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u6743\u8861\u3002\u5728COCO\u548cDiffusionDB\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5b66\u4e60\u8def\u7531\u5230\u4e5d\u4e2a\u5df2\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u6a21\u578b\u66f4\u9ad8\u7684\u5e73\u5747\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.13832", "pdf": "https://arxiv.org/pdf/2506.13832", "abs": "https://arxiv.org/abs/2506.13832", "authors": ["Hongda Zhu", "Yiwen Zhang", "Bing Zhao", "Jingzhe Ding", "Siyao Liu", "Tong Liu", "Dandan Wang", "Yanan Liu", "Zhaojian Li"], "title": "FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FrontendBench\uff0c\u4e00\u4e2a\u7531\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5171\u540c\u5f00\u53d1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLM\u5728\u524d\u7aef\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u57fa\u51c6\u5305\u542b148\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u8986\u76d6\u4ece\u57fa\u7840UI\u5143\u7d20\u5230\u590d\u6742\u4ea4\u4e92\u529f\u80fd\u7684\u4e94\u4e2a\u5c42\u7ea7\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u524d\u7aef\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u4e2d\u5b58\u5728\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5355\u3001\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u4e25\u8c28\u53ca\u7f3a\u4e4f\u7aef\u5230\u7aef\u9a8c\u8bc1\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u6a21\u578b\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86FrontendBench\u3002", "method": "FrontendBench\u5c06\u4efb\u52a1\u6309\u4ee3\u7801\u529f\u80fd\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u4ea4\u4e92\u5f0f\u6d4b\u8bd5\u573a\u666f\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u6267\u884c\u751f\u6210\u7684\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u9884\u5b9a\u4e49\u6d4b\u8bd5\u811a\u672c\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u4e0e\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u8fbe\u523090.54%\u3002\u5bf9\u591a\u4e2a\u5148\u8fdbLLM\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u5b83\u4eec\u5728\u5904\u7406\u771f\u5b9e\u524d\u7aef\u4efb\u52a1\u65f6\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "FrontendBench\u662f\u4e00\u4e2a\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u591a\u6a21\u6001\u8bc4\u4f30\uff0c\u4e3a\u524d\u7aef\u4ee3\u7801\u751f\u6210\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "paper_title_zh": "FrontendBench\uff1a\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u8bc4\u4f30LLM\u5728\u524d\u7aef\u5f00\u53d1\u4e2d\u7684\u8868\u73b0\u7684\u57fa\u51c6", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u524d\u7aef\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u591a\u4e2a\u5173\u952e\u9650\u5236\uff1a\u8bb8\u591a\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5355\uff0c\u6d4b\u8bd5\u7528\u4f8b\u5f80\u5f80\u4e0d\u591f\u4e25\u8c28\uff0c\u4e14\u7f3a\u4e4f\u7aef\u5230\u7aef\u9a8c\u8bc1\u3002\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u51c6\u786e\u8bc4\u4f30\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FrontendBench\uff0c\u4e00\u4e2a\u7531\u4eba\u7c7b\u4e0eLLM\u5171\u540c\u5f00\u53d1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002FrontendBench\u6839\u636e\u4ee3\u7801\u529f\u80fd\u5bf9\u4efb\u52a1\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u4ea4\u4e92\u5f0f\u6d4b\u8bd5\u573a\u666f\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u524d\u7aef\u4ee3\u7801\u751f\u6210\u80fd\u529b\u66f4\u5168\u9762\u548c\u5b9e\u9645\u7684\u8bc4\u4f30\u3002\u8be5\u57fa\u51c6\u5305\u542b148\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a-\u6d4b\u8bd5\u7528\u4f8b\u5bf9\uff0c\u8986\u76d6\u4ece\u57fa\u7840UI\u5143\u7d20\u5230\u590d\u6742\u4ea4\u4e92\u529f\u80fd\u7684\u4e94\u4e2a\u5c42\u7ea7\u3002\u6bcf\u4e2a\u4efb\u52a1\u5747\u53cd\u6620\u771f\u5b9e\u7684\u524d\u7aef\u5f00\u53d1\u6311\u6218\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u6267\u884c\u751f\u6210\u7684\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u9884\u5b9a\u4e49\u6d4b\u8bd5\u811a\u672c\u8bc4\u4f30\u7ed3\u679c\u3002\u8be5\u6846\u67b6\u4e0e\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u8fbe\u523090.54%\uff0c\u663e\u793a\u51fa\u9ad8\u53ef\u9760\u6027\u3002\u6211\u4eec\u5728FrontendBench\u4e0a\u6d4b\u8bd5\u4e86\u591a\u4e2a\u5148\u8fdb\u7684LLM\uff0c\u89c2\u5bdf\u5230\u5b83\u4eec\u5728\u5904\u7406\u771f\u5b9e\u524d\u7aef\u4efb\u52a1\u65f6\u7684\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86FrontendBench\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u4ef7\u503c\uff0c\u652f\u6301\u4e00\u81f4\u7684\u591a\u6a21\u6001\u8bc4\u4f30\uff0c\u5e76\u4e3a\u524d\u7aef\u4ee3\u7801\u751f\u6210\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u5c06\u5f88\u5feb\u53d1\u5e03\u3002"}}
{"id": "2506.14765", "pdf": "https://arxiv.org/pdf/2506.14765", "abs": "https://arxiv.org/abs/2506.14765", "authors": ["Nikolaos Dionelis", "Jente Bosmans", "Riccardo Musto", "Giancarlo Paoletti", "Simone Sarti", "Giacomo Cascarano", "Casper Fibaek", "Luke Camilleri", "Bertrand Le Saux", "Nicolas Long\u00e9p\u00e9"], "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset", "categories": ["cs.CV"], "comment": "6 pages, 9 figures, 1 table, 29 references", "summary": "Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578bPhilEO\u6269\u5c55\u81f323TB\u7684MajorTOM\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u53c2\u6570\u548c\u67b6\u6784\u7684\u6a21\u578b\u53d8\u4f53\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u5316\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u536b\u661f\uff08\u5982Copernicus Sentinel-2\uff09\u6bcf\u5929\u4ea7\u751f\u5927\u91cf\u6570\u636e\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u5c55PhilEO\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u89c4\u6a21\uff0c\u63d0\u5347\u5176\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5c06PhilEO Geo-Aware U-Net\u6a21\u578b\u6269\u5c55\u81f323TB\u7684MajorTOM\u6570\u636e\u96c6\u548c2TB\u7684FastTOM\u5b50\u96c6\uff0c\u5f00\u53d1\u4e86\u4e0d\u540c\u53c2\u6570\u548c\u67b6\u6784\u7684\u6a21\u578b\u53d8\u4f53\uff0c\u5e76\u5728PhilEO Bench\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cPhilEO 44M MajorTOM 23TB\u6a21\u578b\u5728\u9053\u8def\u5bc6\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8ePhilEO Globe 0.5TB 44M\uff1bPhilEO 200M FastTOM\u5728\u9053\u8def\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5efa\u7b51\u5bc6\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u901a\u8fc7\u9a8c\u8bc1\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u5316\u7684\u6709\u6548\u6027\uff0c\u672c\u6587\u8bc1\u660e\u4e86\u6269\u5c55\u9884\u8bad\u7ec3\u89c4\u6a21\u5bf9\u63d0\u5347\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "paper_title_zh": "\u5c06\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578bPhilEO\u6269\u5c55\u81f3MajorTOM\u6570\u636e\u96c6", "abstract_zh": "\u5982\u4eca\uff0c\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u536b\u661f\u6bcf\u5929\u4ea7\u751f\u5927\u91cf\u6570\u636e\uff0c\u4ec5Copernicus Sentinel-2\u661f\u5ea7\u6bcf\u5929\u5c31\u4ea7\u751f\u7ea61.6TB\u6570\u636e\u3002\u4e3a\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\uff0c\u5fc5\u987b\u5728\u5927\u578b\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3EO\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\uff0c\u4ee5\u4fbf\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u9ad8\u6548\u5fae\u8c03\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002\u672c\u6587\u5c55\u793a\u4e86\u6211\u4eec\u6700\u8fd1\u63d0\u51fa\u7684EO\u57fa\u7840\u6a21\u578bPhilEO Geo-Aware U-Net\u572823TB\u672a\u6807\u8bb0\u6570\u636e\u96c6MajorTOM\uff08\u8986\u76d6\u5730\u7403\u5927\u90e8\u5206\u8868\u9762\uff09\u548c2TB\u5b50\u96c6FastTOM\uff08\u4e0d\u5305\u62ec\u6d77\u6d0b\u548c\u51b0\uff09\u4e0a\u7684\u6269\u5c55\u3002\u6211\u4eec\u5f00\u53d1\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u53c2\u6570\u548c\u67b6\u6784\u7684PhilEO\u6a21\u578b\u53d8\u4f53\uff0c\u5e76\u5728PhilEO Bench\u4e0a\u5bf9\u9053\u8def\u5bc6\u5ea6\u4f30\u8ba1\u3001\u5efa\u7b51\u5bc6\u5ea6\u50cf\u7d20\u56de\u5f52\u548c\u571f\u5730\u8986\u76d6\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u6027\u80fd\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9053\u8def\u5bc6\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\uff0cPhilEO 44M MajorTOM 23TB\u6a21\u578b\u5728\u6240\u6709n-shot\u60c5\u51b5\u4e0b\u5747\u4f18\u4e8ePhilEO Globe 0.5TB 44M\uff1b\u5728\u9053\u8def\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5efa\u7b51\u5bc6\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\uff0cPhilEO 200M FastTOM\u5728\u5927\u591a\u6570n-shot\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\u3002\u901a\u8fc7PhilEO Bench\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u5316\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u4eceU-Net\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5230\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u7684\u67b6\u6784\u6269\u5c55\u5f71\u54cd\u3002"}}
{"id": "2506.13833", "pdf": "https://arxiv.org/pdf/2506.13833", "abs": "https://arxiv.org/abs/2506.13833", "authors": ["Xiaoliang Chen", "Le Chang", "Xin Yu", "Yunhe Huang", "Xianling Tu"], "title": "A Survey on World Models Grounded in Acoustic Physical Information", "categories": ["cs.SD", "cs.AI", "cs.RO", "eess.AS", "physics.app-ph"], "comment": "28 pages,11 equations", "summary": "This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal \"intuitive physics\" engine through sound.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u58f0\u5b66\u7269\u7406\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\u7814\u7a76\uff0c\u6db5\u76d6\u7406\u8bba\u3001\u65b9\u6cd5\u3001\u6280\u672f\u8fdb\u5c55\u53ca\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u58f0\u5b66\u4fe1\u53f7\u4f5c\u4e3a\u7269\u7406\u4e8b\u4ef6\u7684\u76f4\u63a5\u8f7d\u4f53\uff0c\u8574\u542b\u4e30\u5bcc\u7684\u73af\u5883\u4fe1\u606f\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u58f0\u5b66\u4fe1\u53f7\u6784\u5efa\u9ad8\u4fdd\u771f\u73af\u5883\u611f\u77e5\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u58f0\u5b66\u4fe1\u53f7\u4e2d\u7684\u7269\u7406\u4fe1\u606f\u7f16\u7801\uff0c\u7efc\u8ff0\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u3001\u751f\u6210\u6a21\u578b\u548c\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u7b49\u6838\u5fc3\u65b9\u6cd5\u3002", "result": "\u58f0\u5b66\u4e16\u754c\u6a21\u578b\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u548c\u91d1\u878d\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6\u9762\u4e34\u6280\u672f\u548c\u4f26\u7406\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u9c81\u68d2\u6027\u3001\u56e0\u679c\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u4f26\u7406\u8d23\u4efb\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u58f0\u5b66\u7684\u4e3b\u52a8\u667a\u80fd\u7cfb\u7edf\u3002", "paper_title_zh": "\u57fa\u4e8e\u58f0\u5b66\u7269\u7406\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7efc\u8ff0", "abstract_zh": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u58f0\u5b66\u7269\u7406\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u5168\u9762\u6982\u51b5\u3002\u63a2\u8ba8\u4e86\u5229\u7528\u58f0\u5b66\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u4fdd\u771f\u73af\u5883\u611f\u77e5\u3001\u56e0\u679c\u7269\u7406\u63a8\u7406\u548c\u52a8\u6001\u4e8b\u4ef6\u9884\u6d4b\u7684\u7406\u8bba\u57fa\u7840\u3001\u65b9\u6cd5\u8bba\u6846\u67b6\u548c\u6700\u65b0\u6280\u672f\u8fdb\u5c55\u3002\u58f0\u5b66\u4fe1\u53f7\u4f5c\u4e3a\u7269\u7406\u4e8b\u4ef6\u673a\u68b0\u6ce2\u80fd\u91cf\u7684\u76f4\u63a5\u8f7d\u4f53\uff0c\u7f16\u7801\u4e86\u6750\u6599\u7279\u6027\u3001\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u548c\u590d\u6742\u4ea4\u4e92\u52a8\u529b\u5b66\u7684\u4e30\u5bcc\u6f5c\u5728\u4fe1\u606f\u3002\u672c\u6587\u9996\u5148\u901a\u8fc7\u89e3\u91ca\u57fa\u672c\u7269\u7406\u5b9a\u5f8b\u5982\u4f55\u652f\u914d\u58f0\u5b66\u4fe1\u53f7\u4e2d\u7684\u7269\u7406\u4fe1\u606f\u7f16\u7801\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\uff1b\u968f\u540e\u56de\u987e\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u3001\u751f\u6210\u6a21\u578b\u548c\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u7b49\u6838\u5fc3\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u58f0\u5b66\u4e16\u754c\u6a21\u578b\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u548c\u91d1\u878d\u7b49\u9886\u57df\u7684\u91cd\u8981\u5e94\u7528\u3002\u6700\u540e\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u5173\u952e\u6280\u672f\u6311\u6218\u548c\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u5177\u4f53\u8def\u7ebf\u56fe\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u3001\u56e0\u679c\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u8d1f\u8d23\u4efb\u7684\u58f0\u5b66\u667a\u80fd\u3002\u8fd9\u4e9b\u8981\u7d20\u5171\u540c\u6307\u5411\u4e00\u79cd\u57fa\u4e8e\u58f0\u97f3\u7684\u5177\u8eab\u4e3b\u52a8\u667a\u80fd\u7814\u7a76\u8def\u5f84\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u58f0\u97f3\u6784\u5efa\u5185\u90e8\u201c\u76f4\u89c9\u7269\u7406\u201d\u5f15\u64ce\u3002"}}
{"id": "2506.14148", "pdf": "https://arxiv.org/pdf/2506.14148", "abs": "https://arxiv.org/abs/2506.14148", "authors": ["Long-Vu Hoang", "Tuan Nguyen", "Tran Huy Dat"], "title": "Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u6563\u5c04\u7684\u975e\u4fb5\u5165\u5f0f\u7269\u4f53\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5934\u53d1\u8bc4\u4f30\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002\u5229\u7528AI\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u58f0\u97f3\u5206\u7c7b\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5934\u53d1\u7c7b\u578b\u548c\u6e7f\u5ea6\u7684\u51c6\u786e\u5206\u7c7b\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe90%\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u5206\u7c7b\u65b9\u6cd5\u53ef\u80fd\u4fb5\u72af\u9690\u79c1\u4e14\u9700\u8981\u63a5\u89e6\u7269\u4f53\uff0c\u800c\u58f0\u5b66\u6563\u5c04\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u63a5\u89e6\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u58f0\u5b66\u6563\u5c04\u5728\u7269\u4f53\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5934\u53d1\u8bc4\u4f30\u9886\u57df\u3002", "method": "\u901a\u8fc7\u53d1\u5c04\u58f0\u5b66\u523a\u6fc0\u5e76\u6355\u83b7\u5934\u53d1\u6837\u672c\u7684\u6563\u5c04\u4fe1\u53f7\uff0c\u7ed3\u5408\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\uff08\u5168\u76d1\u7763\u5b66\u4e60\u3001\u5d4c\u5165\u5206\u7c7b\u3001\u76d1\u7763\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u3001\u81ea\u76d1\u7763\u6a21\u578b\u5fae\u8c03\uff09\u8fdb\u884c\u5206\u7c7b\u3002\u6700\u4f73\u7b56\u7565\u4e3a\u81ea\u76d1\u7763\u6a21\u578b\u7684\u5168\u53c2\u6570\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u76d1\u7763\u6a21\u578b\u5fae\u8c03\u7b56\u7565\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd190%\uff0c\u9a8c\u8bc1\u4e86\u58f0\u5b66\u6563\u5c04\u5728\u975e\u4fb5\u5165\u5f0f\u5206\u7c7b\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u58f0\u5b66\u6563\u5c04\u4f5c\u4e3a\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u3001\u975e\u63a5\u89e6\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u5934\u53d1\u8bc4\u4f30\u53ca\u5176\u4ed6\u884c\u4e1a\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u58f0\u5b66\u6563\u5c04AI\u7684\u975e\u4fb5\u5165\u5f0f\u7269\u4f53\u5206\u7c7b\uff1a\u4ee5\u5934\u53d1\u8bc4\u4f30\u4e3a\u4f8b", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u6563\u5c04\u7684\u975e\u4fb5\u5165\u5f0f\u7269\u4f53\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u4ee5\u5934\u53d1\u8bc4\u4f30\u4e3a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5f53\u5165\u5c04\u6ce2\u4e0e\u7269\u4f53\u76f8\u4e92\u4f5c\u7528\u65f6\uff0c\u4f1a\u4ea7\u751f\u5305\u542b\u7ed3\u6784\u548c\u6750\u6599\u7279\u6027\u7684\u6563\u5c04\u58f0\u573a\u3002\u901a\u8fc7\u53d1\u5c04\u58f0\u5b66\u523a\u6fc0\u5e76\u6355\u83b7\u5934\u53d1\u6837\u672c\u7684\u6563\u5c04\u4fe1\u53f7\uff0c\u5229\u7528AI\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u58f0\u97f3\u5206\u7c7b\u6280\u672f\u5bf9\u5934\u53d1\u7c7b\u578b\u548c\u6e7f\u5ea6\u8fdb\u884c\u5206\u7c7b\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u56db\u79cd\u65b9\u6cd5\uff1a\uff08i\uff09\u5168\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\uff0c\uff08ii\uff09\u57fa\u4e8e\u5d4c\u5165\u7684\u5206\u7c7b\uff0c\uff08iii\uff09\u76d1\u7763\u57fa\u7840\u6a21\u578b\u5fae\u8c03\uff0c\uff08iv\uff09\u81ea\u76d1\u7763\u6a21\u578b\u5fae\u8c03\u3002\u6700\u4f73\u7b56\u7565\u4e3a\u81ea\u76d1\u7763\u6a21\u578b\u7684\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd190%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u58f0\u5b66\u6563\u5c04\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u3001\u975e\u63a5\u89e6\u7684\u89c6\u89c9\u5206\u7c7b\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u591a\u4e2a\u884c\u4e1a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.14766", "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "categories": ["cs.CV", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u53ef\u8c03\u63a7\u7684\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff08ASCD\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5e38\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u90e8\u5206\u7ebf\u7d22\u800c\u4ea7\u751f\u5e7b\u89c9\uff0c\u751f\u6210\u9519\u8bef\u56de\u7b54\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982VCD\u548cICD\uff09\u901a\u8fc7\u5bf9\u6bd4\u6270\u52a8\u6216\u8d1f\u524d\u7f00\u8f93\u5165\u7684\u9884\u6d4b\u7ed3\u679c\u6765\u7f13\u89e3\u5e7b\u89c9\uff0c\u4f46\u5176\u6709\u6548\u6027\u53ef\u80fd\u6e90\u4e8e\u6ce8\u610f\u529b\u5206\u5e03\u7684\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u76f4\u63a5\u5e72\u9884\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u66f4\u7cfb\u7edf\u5730\u51cf\u5c11\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u53ef\u8c03\u63a7\u7684\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff08ASCD\uff09\uff0c\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u800c\u975e\u4ec5\u901a\u8fc7\u8868\u9762\u7ea7\u7684logits\u8c03\u6574\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASCD\u5728\u591a\u4e2aMLLM\u67b6\u6784\u548c\u4e0d\u540c\u89e3\u7801\u65b9\u6cd5\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5728POPE\u3001CHAIR\u548cMMHal-Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6807\u51c6VQA\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "ASCD\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u51cf\u5c11MLLM\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "paper_title_zh": "ASCD\uff1a\u6ce8\u610f\u529b\u53ef\u8c03\u63a7\u7684\u5bf9\u6bd4\u89e3\u7801\u4ee5\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5e38\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u90e8\u5206\u7ebf\u7d22\u800c\u4ea7\u751f\u5e7b\u89c9\uff0c\u751f\u6210\u9519\u8bef\u56de\u7b54\u3002\u8fd1\u671f\u63d0\u51fa\u7684\u65b9\u6cd5\uff08\u5982\u89c6\u89c9\u5bf9\u6bd4\u89e3\u7801VCD\u548c\u6307\u4ee4\u5bf9\u6bd4\u89e3\u7801ICD\uff09\u901a\u8fc7\u5bf9\u6bd4\u6270\u52a8\u6216\u8d1f\u524d\u7f00\u8f93\u5165\u7684\u9884\u6d4b\u7ed3\u679c\u6765\u7f13\u89e3\u5e7b\u89c9\u3002\u672c\u6587\u53d1\u73b0\uff0cVCD\u548cICD\u7b49\u65b9\u6cd5\u4ece\u6839\u672c\u4e0a\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u5185\u90e8\u6ce8\u610f\u529b\u52a8\u6001\uff0c\u8868\u660e\u5176\u6709\u6548\u6027\u53ef\u80fd\u6e90\u4e8e\u6ce8\u610f\u529b\u5206\u5e03\u7684\u6df1\u5c42\u53d8\u5316\uff0c\u800c\u975e\u4ec5\u8868\u9762\u7ea7\u7684logits\u8c03\u6574\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u53ef\u8c03\u63a7\u7684\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff0c\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u66f4\u7cfb\u7edf\u5730\u51cf\u5c11\u5e7b\u89c9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cdMLLM\u67b6\u6784\u548c\u89e3\u7801\u65b9\u6cd5\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5728POPE\u3001CHAIR\u548cMMHal-Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6807\u51c6VQA\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.13834", "pdf": "https://arxiv.org/pdf/2506.13834", "abs": "https://arxiv.org/abs/2506.13834", "authors": ["Zhao Wei", "Chin Chun Ooi", "Abhishek Gupta", "Jian Cheng Wong", "Pao-Hsiung Chiu", "Sheares Xue Wen Toh", "Yew-Soon Ong"], "title": "Evolvable Conditional Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8fdb\u5316\u7684\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u5229\u7528\u9ed1\u7bb1\u3001\u4e0d\u53ef\u5fae\u7684\u591a\u7269\u7406\u6a21\u578b\uff08\u5982\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u7535\u78c1\u5b66\u4e2d\u7684\u5e38\u89c1\u6a21\u578b\uff09\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u63cf\u8ff0\u6027\u7edf\u8ba1\u91cf\u5b9e\u73b0\u76ee\u6807\u51fd\u6570\u7684\u4f18\u5316\uff0c\u65e0\u9700\u8ba1\u7b97\u5bfc\u6570\u3002", "motivation": "\u79d1\u5b66\u9886\u57df\u4e2d\u5e7f\u6cdb\u5b58\u5728\u9ed1\u7bb1\u3001\u4e0d\u53ef\u5fae\u7684\u591a\u7269\u7406\u6a21\u578b\uff08\u5982\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u7535\u78c1\u5b66\u6a21\u578b\uff09\uff0c\u8fd9\u4e9b\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8ba1\u7b97\u5bfc\u6570\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u7684\u751f\u6210\u4f18\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u8fdb\u5316\u7684\u53ef\u8fdb\u5316\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\u3002\u901a\u8fc7\u4f18\u5316\u53bb\u566a\u5206\u5e03\u7684\u63cf\u8ff0\u6027\u7edf\u8ba1\u91cf\uff0c\u5b9e\u73b0\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4f18\u5316\u3002\u6700\u7ec8\u63a8\u5bfc\u51fa\u7684\u66f4\u65b0\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u6269\u6563\u6a21\u578b\uff0c\u4f46\u65e0\u9700\u8ba1\u7b97\u4efb\u4f55\u5bfc\u6570\u3002", "result": "\u5728\u6d41\u4f53\u62d3\u6251\u548c\u8d85\u8868\u9762\u8bbe\u8ba1\u7684\u4e24\u4e2aAI\u79d1\u5b66\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u7279\u5b9a\u4f18\u5316\u76ee\u6807\u7684\u8bbe\u8ba1\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u53ef\u5fae\u4ee3\u7406\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53ef\u8fdb\u5316\u6269\u6563\u65b9\u6cd5\u4e3a\u5229\u7528\u9ed1\u7bb1\u3001\u4e0d\u53ef\u5fae\u591a\u7269\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5bfc\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u751f\u6210\u4f18\u5316\u3002", "paper_title_zh": "\u53ef\u8fdb\u5316\u6761\u4ef6\u6269\u6563", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8fdb\u5316\u7684\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\uff0c\u4f7f\u5f97\u9ed1\u7bb1\u3001\u4e0d\u53ef\u5fae\u7684\u591a\u7269\u7406\u6a21\u578b\uff08\u5982\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u7535\u78c1\u5b66\u4e2d\u7684\u5e38\u89c1\u6a21\u578b\uff09\u80fd\u591f\u6709\u6548\u7528\u4e8e\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u3002\u6211\u4eec\u5c06\u6307\u5bfc\u95ee\u9898\u8868\u8ff0\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u66f4\u65b0\u53bb\u566a\u5206\u5e03\u7684\u63cf\u8ff0\u6027\u7edf\u8ba1\u91cf\u6765\u4f18\u5316\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u4ece\u6982\u7387\u8fdb\u5316\u7684\u89d2\u5ea6\u63a8\u5bfc\u51fa\u4e00\u79cd\u8fdb\u5316\u5f15\u5bfc\u65b9\u6cd5\u3002\u6709\u8da3\u7684\u662f\uff0c\u6700\u7ec8\u63a8\u5bfc\u51fa\u7684\u66f4\u65b0\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u5e38\u89c1\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u6269\u6563\u6a21\u578b\uff0c\u4f46\u65e0\u9700\u8ba1\u7b97\u4efb\u4f55\u5bfc\u6570\u3002\u6211\u4eec\u5728\u4e24\u4e2aAI\u79d1\u5b66\u573a\u666f\uff08\u6d41\u4f53\u62d3\u6251\u548c\u8d85\u8868\u9762\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\uff09\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u53ef\u8fdb\u5316\u6269\u6563\u7b97\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u66f4\u7b26\u5408\u7279\u5b9a\u4f18\u5316\u76ee\u6807\u7684\u8bbe\u8ba1\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u53ef\u5fae\u4ee3\u7406\u6a21\u578b\uff0c\u4e3a\u57fa\u4e8e\u6307\u5bfc\u7684\u6269\u6563\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u624b\u6bb5\uff0c\u80fd\u591f\u5145\u5206\u5229\u7528\u79d1\u5b66\u9886\u57df\u4e2d\u5e38\u89c1\u7684\u9ed1\u7bb1\u3001\u4e0d\u53ef\u5fae\u591a\u7269\u7406\u6570\u503c\u6a21\u578b\u3002"}}
{"id": "2506.14153", "pdf": "https://arxiv.org/pdf/2506.14153", "abs": "https://arxiv.org/abs/2506.14153", "authors": ["Tuan Dat Phuong", "Long-Vu Hoang", "Huy Dat Tran"], "title": "Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5c06Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u5f15\u5165\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684XLSR-Conformer\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u5347\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728ASVspoof2021\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4f2a\u9020\u8bed\u97f3\u653b\u51fb\u65e5\u76ca\u590d\u6742\uff0c\u5bf9\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u5c3d\u7ba1\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684\u6a21\u578b\uff08\u5982XLSR-Conformer\uff09\u5728\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u67b6\u6784\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06XLSR-Conformer\u6a21\u578b\u4e2d\u7684\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u66ff\u6362\u4e3a\u57fa\u4e8eKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u3002\u8fd9\u79cd\u67b6\u6784\u6539\u8fdb\u65e8\u5728\u66f4\u9ad8\u6548\u5730\u5904\u7406\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5728ASVspoof2021\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06KAN\u96c6\u6210\u5230SSL\u6a21\u578b\u4e2d\uff0c\u76f8\u5bf9\u63d0\u5347\u4e86LA\u548cDF\u96c6\u7684\u6027\u80fd60.55%\uff0c\u5e76\u572821LA\u96c6\u4e0a\u5b9e\u73b0\u4e860.70%\u7684\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u5f15\u5165\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6a21\u578b\u662f\u63d0\u5347\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u5229\u7528Kolmogorov-Arnold\u7f51\u7edc\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u6027\u80fd", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u8fdb\u6b65\u5bfc\u81f4\u4e86\u65e5\u76ca\u590d\u6742\u7684\u4f2a\u9020\u8bed\u97f3\u653b\u51fb\uff0c\u5bf9\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5c3d\u7ba1\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684\u6a21\u578b\uff08\u5c24\u5176\u662fXLSR-Conformer\u6a21\u578b\uff09\u5728\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u67b6\u6784\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06XLSR-Conformer\u6a21\u578b\u4e2d\u7684\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\u66ff\u6362\u4e3a\u57fa\u4e8eKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u3002\u5728ASVspoof2021\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c06KAN\u96c6\u6210\u5230SSL\u6a21\u578b\u4e2d\uff0c\u76f8\u5bf9\u63d0\u5347\u4e86LA\u548cDF\u96c6\u7684\u6027\u80fd60.55%\uff0c\u5e76\u572821LA\u96c6\u4e0a\u5b9e\u73b0\u4e860.70%\u7684\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5c06KAN\u5f15\u5165\u57fa\u4e8eSSL\u7684\u6a21\u578b\u662f\u63d0\u5347\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u6027\u80fd\u7684\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2506.14769", "pdf": "https://arxiv.org/pdf/2506.14769", "abs": "https://arxiv.org/abs/2506.14769", "authors": ["Jiahua Ma", "Yiran Qin", "Yixiong Li", "Xuanqi Liao", "Yulan Guo", "Ruimao Zhang"], "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u6269\u6563\u7684\u7a33\u5065\u81ea\u56de\u5f52\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff08CDP\uff09\uff0c\u901a\u8fc7\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u589e\u5f3a\u52a8\u4f5c\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u786c\u4ef6\u9650\u5236\u548c\u6570\u636e\u8d28\u91cf\u4e0b\u964d\u4f1a\u4e25\u91cd\u5f71\u54cd\u57fa\u4e8e\u4e13\u5bb6\u6f14\u793a\u7684\u5b66\u4e60\u6548\u679c\uff0c\u5bfc\u81f4\u5bf9\u8c61\u5b9a\u4f4d\u3001\u6293\u53d6\u89c4\u5212\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u6267\u884c\u5931\u8d25\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86CDP\u65b9\u6cd5\u3002", "method": "CDP\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u5b9e\u73b0\u66f4\u8fde\u8d2f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u4f5c\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u7f13\u5b58\u673a\u5236\u4ee5\u51cf\u5c11\u81ea\u56de\u5f52\u63a8\u7406\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u7684\u591a\u79cd2D\u548c3D\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cCDP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u8f93\u5165\u89c2\u6d4b\u8d28\u91cf\u4e0b\u964d\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5b9e\u9645\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CDP\u901a\u8fc7\u5229\u7528\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u548c\u56e0\u679c\u6269\u6563\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u5b8c\u7f8e\u7684\u63a7\u5236\u6761\u4ef6\u3002", "paper_title_zh": "CDP\uff1a\u57fa\u4e8e\u56e0\u679c\u6269\u6563\u7684\u7a33\u5065\u81ea\u56de\u5f52\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60", "abstract_zh": "\u6269\u6563\u7b56\u7565\uff08DP\uff09\u901a\u8fc7\u52a8\u4f5c\u6269\u6563\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u6f14\u793a\u5b66\u4e60\u590d\u6742\u884c\u4e3a\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u786c\u4ef6\u9650\u5236\u4f1a\u964d\u4f4e\u6570\u636e\u8d28\u91cf\uff0c\u800c\u5b9e\u65f6\u6027\u7ea6\u675f\u8981\u6c42\u6a21\u578b\u4ec5\u57fa\u4e8e\u77ac\u65f6\u72b6\u6001\u548c\u573a\u666f\u89c2\u6d4b\u8fdb\u884c\u63a8\u7406\u3002\u8fd9\u4e9b\u9650\u5236\u4e25\u91cd\u5f71\u54cd\u4e86\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5bfc\u81f4\u5bf9\u8c61\u5b9a\u4f4d\u3001\u6293\u53d6\u89c4\u5212\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u6267\u884c\u5931\u8d25\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u56e0\u679c\u6269\u6563\u7b56\u7565\uff08CDP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u8fde\u8d2f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u964d\u4f4e\u81ea\u56de\u5f52\u63a8\u7406\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u8fd8\u5f15\u5165\u4e86\u7f13\u5b58\u673a\u5236\u4ee5\u5b58\u50a8\u5148\u524d\u65f6\u95f4\u6b65\u7684\u6ce8\u610f\u529b\u952e\u503c\u5bf9\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u6267\u884c\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u3002\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u7684\u591a\u79cd2D\u548c3D\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5b9e\u9a8c\u8868\u660eCDP\u901a\u8fc7\u5229\u7528\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u8f93\u5165\u89c2\u6d4b\u8d28\u91cf\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\uff0cCDP\u4ecd\u80fd\u901a\u8fc7\u65f6\u95f4\u8fde\u7eed\u6027\u63a8\u7406\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u73b0\u5b9e\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u673a\u5668\u4eba\u63a7\u5236\u7684\u5b9e\u7528\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.13836", "pdf": "https://arxiv.org/pdf/2506.13836", "abs": "https://arxiv.org/abs/2506.13836", "authors": ["Dang Viet Anh Nguyen", "Carlos Lima Azevedo", "Tomer Toledo", "Filipe Rodrigues"], "title": "Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study", "categories": ["cs.LG", "cs.AI"], "comment": "35 pages, 5 figures, 3 tables", "summary": "Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08RL-TSC\uff09\u5728\u4ea4\u901a\u4e8b\u4ef6\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86T-REX\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u53d1\u73b0\u5206\u5c42\u534f\u8c03\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "motivation": "\u5c3d\u7ba1RL-TSC\u5728\u63d0\u5347\u57ce\u5e02\u4ea4\u901a\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u4ea4\u901a\u4e8b\u4ef6\u4e2d\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86T-REX\u4eff\u771f\u6846\u67b6\uff0c\u6a21\u62df\u4ea4\u901a\u4e8b\u4ef6\u4e0b\u7684\u52a8\u6001\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u8bc4\u4f30\u9c81\u68d2\u6027\u7684\u6307\u6807\uff0c\u7528\u4e8e\u6d4b\u8bd5\u591a\u79cdRL-TSC\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u72ec\u7acb\u503c\u57fa\u548c\u5206\u6563\u538b\u529b\u57fa\u65b9\u6cd5\u5728\u7a33\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e8b\u4ef6\u9a71\u52a8\u7684\u5206\u5e03\u53d8\u5316\u4e2d\u6027\u80fd\u4e0b\u964d\uff1b\u5206\u5c42\u534f\u8c03\u65b9\u6cd5\u5728\u590d\u6742\u7f51\u7edc\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "RL-TSC\u7814\u7a76\u9700\u5173\u6ce8\u9c81\u68d2\u6027\u8bbe\u8ba1\uff0cT-REX\u4e3a\u52a8\u6001\u573a\u666f\u4e0b\u7684\u65b9\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5e73\u53f0\u3002", "paper_title_zh": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u5728\u4e8b\u4ef6\u4e2d\u7684\u9c81\u68d2\u6027\uff1a\u4e00\u9879\u5bf9\u6bd4\u7814\u7a76", "abstract_zh": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08RL-TSC\uff09\u5df2\u6210\u4e3a\u6539\u5584\u57ce\u5e02\u4ea4\u901a\u6d41\u52a8\u6027\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u4ea4\u901a\u4e8b\u4ef6\u4e2d\u7684\u9c81\u68d2\u6027\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86T-REX\uff0c\u4e00\u4e2a\u57fa\u4e8eSUMO\u7684\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u4e8b\u4ef6\u573a\u666f\u4e0b\u8bad\u7ec3\u548c\u8bc4\u4f30RL-TSC\u65b9\u6cd5\u3002T-REX\u901a\u8fc7\u6a21\u62df\u9a7e\u9a76\u5458\u7684\u6982\u7387\u6027\u6539\u9053\u3001\u901f\u5ea6\u9002\u5e94\u548c\u4e0a\u4e0b\u6587\u6362\u9053\uff0c\u5b9e\u73b0\u4e86\u4e8b\u4ef6\u4e0b\u62e5\u5835\u4f20\u64ad\u7684\u4eff\u771f\u3002\u4e3a\u8bc4\u4f30\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u5957\u8d85\u8d8a\u4f20\u7edf\u4ea4\u901a\u6548\u7387\u6307\u6807\u7684\u5ea6\u91cf\u6807\u51c6\u3002\u901a\u8fc7\u5728\u5408\u6210\u548c\u771f\u5b9e\u7f51\u7edc\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86T-REX\u5728\u591a\u79cd\u5b9e\u9645\u90e8\u7f72\u8303\u5f0f\u4e0b\u5bf9\u591a\u79cd\u5148\u8fdbRL-TSC\u65b9\u6cd5\u7684\u8bc4\u4f30\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u72ec\u7acb\u503c\u57fa\u548c\u5206\u6563\u538b\u529b\u57fa\u65b9\u6cd5\u5728\u7a33\u5b9a\u4ea4\u901a\u6761\u4ef6\u548c\u540c\u8d28\u7f51\u7edc\u4e2d\u5177\u6709\u5feb\u901f\u6536\u655b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u4e8b\u4ef6\u9a71\u52a8\u7684\u5206\u5e03\u53d8\u5316\u4e2d\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5206\u5c42\u534f\u8c03\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u4e0d\u89c4\u5219\u7f51\u7edc\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a\u548c\u9002\u5e94\u6027\u5f3a\uff0c\u4f46\u5176\u6536\u655b\u901f\u5ea6\u8f83\u6162\u4e14\u8bad\u7ec3\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86RL-TSC\u7814\u7a76\u4e2d\u9c81\u68d2\u6027\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002T-REX\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5f00\u653e\u3001\u6807\u51c6\u5316\u4e14\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u4e3a\u52a8\u6001\u548c\u5e72\u6270\u6027\u4ea4\u901a\u573a\u666f\u4e0b\u7684RL\u65b9\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2506.14204", "pdf": "https://arxiv.org/pdf/2506.14204", "abs": "https://arxiv.org/abs/2506.14204", "authors": ["Aswin Shanmugam Subramanian", "Amit Das", "Naoyuki Kanda", "Jinyu Li", "Xiaofei Wang", "Yifan Gong"], "title": "Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u5e8f\u5217\u5316\u8f93\u51fa\u8bad\u7ec3\uff08SOT\uff09\u6846\u67b6\uff0c\u4ee5\u6ee1\u8db3\u6d41\u5f0f\u548c\u79bb\u7ebf\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u91cd\u70b9\u5e73\u8861\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9879\u5173\u952e\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6d41\u5f0f\u548c\u79bb\u7ebfASR\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u5982\u5b9e\u65f6\u5b57\u5e55\u548c\u6458\u8981\u9700\u6c42\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdbSOT\u6846\u67b6\uff0c\u63d0\u5347\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "1. \u7ed3\u5408\u8fde\u7eed\u8bed\u97f3\u5206\u79bb\uff08CSS\uff09\u5355\u901a\u9053\u524d\u7aef\u4e0e\u7aef\u5230\u7aef\uff08E2E\uff09\u7cfb\u7edf\uff0c\u5904\u7406\u9ad8\u5ea6\u91cd\u53e0\u7684\u8bed\u97f3\u573a\u666f\uff1b2. \u91c7\u7528\u53cc\u6a21\u578b\u7b56\u7565\uff08\u6d41\u5f0fConformer Transducer\u548c\u79bb\u7ebf\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff09\u6216\u57fa\u4e8e\u7ea7\u8054\u7f16\u7801\u5668\u7684\u4e24\u9636\u6bb5\u6a21\u578b\uff1b3. \u63a2\u7d22\u66f4\u9002\u5408\u79bb\u7ebf\u573a\u666f\u7684\u57fa\u4e8e\u7247\u6bb5\u7684SOT\uff08segSOT\uff09\uff0c\u63d0\u5347\u591a\u8bf4\u8bdd\u4eba\u8f6c\u5f55\u7684\u53ef\u8bfb\u6027\u3002", "result": "CSS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86ASR\u7cfb\u7edf\u5728\u591a\u8bf4\u8bdd\u4eba\u91cd\u53e0\u8bed\u97f3\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\uff1b\u53cc\u6a21\u578b\u548csegSOT\u5206\u522b\u4f18\u5316\u4e86\u6d41\u5f0f\u548c\u79bb\u7ebf\u573a\u666f\u7684\u6027\u80fd\u4e0e\u53ef\u8bfb\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6539\u8fdb\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86ASR\u7cfb\u7edf\u7684\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u6d41\u5f0f\u548c\u79bb\u7ebf\u573a\u666f\uff0c\u4e3a\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u6539\u8fdb\u7aef\u5230\u7aef\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\u5728\u5728\u7ebf\u4e0e\u79bb\u7ebf\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528", "abstract_zh": "\u6211\u4eec\u6269\u5c55\u4e86\u5e8f\u5217\u5316\u8f93\u51fa\u8bad\u7ec3\uff08SOT\uff09\u6846\u67b6\uff0c\u4ee5\u6ee1\u8db3\u6d41\u5f0f\u548c\u79bb\u7ebf\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5e94\u7528\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u91cd\u70b9\u5173\u6ce8\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\uff0c\u4ee5\u9002\u5e94\u5b9e\u65f6\u5b57\u5e55\u548c\u6458\u8981\u7684\u9700\u6c42\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u9879\u5173\u952e\u6539\u8fdb\uff1a\uff081\uff09\u7ed3\u5408\u8fde\u7eed\u8bed\u97f3\u5206\u79bb\uff08CSS\uff09\u5355\u901a\u9053\u524d\u7aef\u4e0e\u7aef\u5230\u7aef\uff08E2E\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u5ea6\u91cd\u53e0\u7684\u8bed\u97f3\u573a\u666f\uff0c\u6311\u6218\u4e86E2E\u4e0e\u7ea7\u8054\u8bbe\u7f6e\u7684\u4f20\u7edf\u89c2\u5ff5\u3002CSS\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u591a\u8bf4\u8bdd\u4eba\u7684\u91cd\u53e0\u8bed\u97f3\uff0c\u63d0\u5347\u4e86ASR\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002\uff082\uff09\u91c7\u7528\u53cc\u6a21\u578b\u7b56\u7565\u2014\u2014\u6d41\u5f0fConformer Transducer\u548c\u79bb\u7ebf\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff0c\u6216\u57fa\u4e8e\u7ea7\u8054\u7f16\u7801\u5668\u7684\u4e24\u9636\u6bb5\u6a21\u578b\u3002\uff083\uff09\u63a2\u7d22\u66f4\u9002\u5408\u79bb\u7ebf\u573a\u666f\u7684\u57fa\u4e8e\u7247\u6bb5\u7684SOT\uff08segSOT\uff09\uff0c\u540c\u65f6\u63d0\u5347\u591a\u8bf4\u8bdd\u4eba\u8f6c\u5f55\u7684\u53ef\u8bfb\u6027\u3002"}}
{"id": "2506.13838", "pdf": "https://arxiv.org/pdf/2506.13838", "abs": "https://arxiv.org/abs/2506.13838", "authors": ["Lorena Poenaru-Olaru", "June Sallou", "Luis Cruz", "Jan Rellermeyer", "Arie van Deursen"], "title": "Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "12 pages. Accepted at ICT4Sustainability 2025 conference", "summary": "The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u91cd\u8bad\u7ec3\u4e2d\u7684\u80fd\u6e90\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4ec5\u4f7f\u7528\u6700\u65b0\u6570\u636e\u6216\u6309\u9700\u91cd\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u8017\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u5b9a\u671f\u91cd\u8bad\u7ec3\u4ee5\u5e94\u5bf9\u6570\u636e\u53d8\u5316\uff0c\u4f46\u4f20\u7edf\u91cd\u8bad\u7ec3\u65b9\u6cd5\u80fd\u8017\u9ad8\uff0c\u5bf9\u73af\u5883\u9020\u6210\u8d1f\u62c5\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u8282\u80fd\u7684\u91cd\u8bad\u7ec3\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u91cd\u8bad\u7ec3\u6280\u672f\u7684\u80fd\u6e90\u6d88\u8017\u548c\u51c6\u786e\u6027\uff0c\u5305\u62ec\u4ec5\u4f7f\u7528\u6700\u65b0\u6570\u636e\u548c\u6309\u9700\u91cd\u8bad\u7ec3\u4e24\u79cd\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8282\u80fd\u6548\u679c\u3002", "result": "\u4ec5\u4f7f\u7528\u6700\u65b0\u6570\u636e\u91cd\u8bad\u7ec3\u53ef\u964d\u4f4e25%\u80fd\u8017\uff1b\u6309\u9700\u91cd\u8bad\u7ec3\u5219\u53ef\u51cf\u5c1140%\u80fd\u8017\uff0c\u524d\u63d0\u662f\u914d\u5907\u53ef\u9760\u7684\u6570\u636e\u53d8\u5316\u68c0\u6d4b\u5668\u3002", "conclusion": "\u672c\u6587\u4e3a\u673a\u5668\u5b66\u4e60\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u8282\u80fd\u91cd\u8bad\u7ec3\u6280\u672f\u7684\u5efa\u8bae\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u53ef\u6301\u7eed\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002", "paper_title_zh": "\u53ef\u6301\u7eed\u7684\u673a\u5668\u5b66\u4e60\u91cd\u8bad\u7ec3\uff1a\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u4f18\u5316\u80fd\u6e90\u6548\u7387", "abstract_zh": "\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u53d7\u6570\u636e\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5f71\u54cd\u5f88\u5927\u3002\u56e0\u6b64\uff0cML\u7cfb\u7edf\u9700\u8981\u5b9a\u671f\u7ef4\u62a4\uff0c\u901a\u5e38\u57fa\u4e8e\u6a21\u578b\u91cd\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u91cd\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u80fd\u8017\u9ad8\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u73af\u5883\u5f71\u54cd\u7684\u62c5\u5fe7\u3002\u4e3a\u4e86\u89e3\u5728\u8bbe\u8ba1\u53ef\u6301\u7eedML\u5e94\u7528\u65f6\u5e94\u8003\u8651\u54ea\u4e9b\u91cd\u8bad\u7ec3\u6280\u672f\uff0c\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5e38\u89c1\u91cd\u8bad\u7ec3\u6280\u672f\u7684\u80fd\u6e90\u6d88\u8017\u3002\u7531\u4e8eML\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u540c\u6837\u91cd\u8981\uff0c\u6211\u4eec\u4ece\u80fd\u6e90\u6548\u7387\u548c\u51c6\u786e\u6027\u4e24\u65b9\u9762\u6bd4\u8f83\u4e86\u91cd\u8bad\u7ec3\u6280\u672f\u3002\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f7f\u7528\u6240\u6709\u53ef\u7528\u6570\u636e\u76f8\u6bd4\uff0c\u4ec5\u4f7f\u7528\u6700\u65b0\u6570\u636e\u91cd\u8bad\u7ec3\u53ef\u964d\u4f4e25%\u7684\u80fd\u8017\uff0c\u662f\u4e00\u79cd\u53ef\u6301\u7eed\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u53d1\u73b0\u8868\u660e\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u66f4\u65b0\u5fc5\u8981\u65f6\u624d\u91cd\u8bad\u7ec3\u6a21\u578b\uff08\u800c\u975e\u6309\u56fa\u5b9a\u65f6\u95f4\u8868\uff09\uff0c\u53ef\u51cf\u5c1140%\u7684\u80fd\u8017\uff0c\u524d\u63d0\u662f\u914d\u5907\u53ef\u9760\u7684\u6570\u636e\u53d8\u5316\u68c0\u6d4b\u5668\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3aML\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5efa\u8bae\uff0c\u6307\u5bfc\u4ed6\u4eec\u5728\u8bbe\u8ba1\u53ef\u6301\u7eedML\u8f6f\u4ef6\u7cfb\u7edf\u65f6\u91c7\u7528\u66f4\u8282\u80fd\u7684\u91cd\u8bad\u7ec3\u6280\u672f\u3002"}}
{"id": "2506.14223", "pdf": "https://arxiv.org/pdf/2506.14223", "abs": "https://arxiv.org/abs/2506.14223", "authors": ["Anna Hamberger", "Sebastian Murgul", "Jochen Schmidt", "Michael Heizmann"], "title": "Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 50th International Computer Music Conference (ICMC), 2025", "summary": "Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFretting-Transformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u7528\u4e8e\u5c06MIDI\u5e8f\u5217\u81ea\u52a8\u8f6c\u5f55\u4e3a\u5409\u4ed6\u6307\u6cd5\u8c31\uff0c\u89e3\u51b3\u4e86\u5f26-\u54c1\u6a21\u7cca\u6027\u548c\u6f14\u594f\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u5546\u4e1a\u5e94\u7528\u3002", "motivation": "\u97f3\u4e50\u8f6c\u5f55\u5728\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\uff08MIR\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5409\u4ed6\u7b49\u5f26\u4e50\u5668\uff0cMIDI\u7b49\u7b26\u53f7\u97f3\u4e50\u8868\u793a\u7f3a\u4e4f\u5173\u952e\u7684\u6f14\u594f\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u8f6c\u5f55\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8eT5\u53d8\u6362\u5668\u67b6\u6784\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5c06\u4efb\u52a1\u89c6\u4e3a\u7b26\u53f7\u7ffb\u8bd1\u95ee\u9898\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u5229\u7528DadaGP\u3001GuitarToday\u548cLeduc\u7b49\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFretting-Transformer\u5728\u6307\u6cd5\u8c31\u51c6\u786e\u6027\u548c\u6f14\u594f\u6027\u65b9\u9762\u4f18\u4e8eA*\u7b49\u57fa\u7ebf\u65b9\u6cd5\u548cGuitar Pro\u7b49\u5546\u4e1a\u5e94\u7528\uff0c\u4e0a\u4e0b\u6587\u654f\u611f\u5904\u7406\u548c\u8c03\u5f26/\u53d8\u8c03\u5939\u6761\u4ef6\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "Fretting-Transformer\u4e3a\u5409\u4ed6\u81ea\u52a8\u8f6c\u5f55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "paper_title_zh": "Fretting-Transformer\uff1a\u7528\u4e8eMIDI\u5230\u6307\u6cd5\u8c31\u8f6c\u5f55\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b", "abstract_zh": "\u97f3\u4e50\u8f6c\u5f55\u5728\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\uff08MIR\uff09\u4e2d\u626e\u6f14\u7740\u5173\u952e\u89d2\u8272\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5409\u4ed6\u7b49\u5f26\u4e50\u5668\uff0cMIDI\u7b49\u7b26\u53f7\u97f3\u4e50\u8868\u793a\u7f3a\u4e4f\u5173\u952e\u7684\u6f14\u594f\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u4e86Fretting-Transformer\uff0c\u4e00\u79cd\u57fa\u4e8eT5\u53d8\u6362\u5668\u67b6\u6784\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u7528\u4e8e\u5c06MIDI\u5e8f\u5217\u81ea\u52a8\u8f6c\u5f55\u4e3a\u5409\u4ed6\u6307\u6cd5\u8c31\u3002\u901a\u8fc7\u5c06\u4efb\u52a1\u89c6\u4e3a\u7b26\u53f7\u7ffb\u8bd1\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u89e3\u51b3\u4e86\u5f26-\u54c1\u6a21\u7cca\u6027\u548c\u7269\u7406\u6f14\u594f\u6027\u7b49\u5173\u952e\u6311\u6218\u3002\u7cfb\u7edf\u5229\u7528\u4e86\u5305\u62ecDadaGP\u3001GuitarToday\u548cLeduc\u5728\u5185\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u4e86\u65b0\u9896\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u6807\u8bb0\u5316\u7b56\u7565\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u6307\u6cd5\u8c31\u51c6\u786e\u6027\u548c\u6f14\u594f\u6027\u7684\u8bc4\u4f30\u6307\u6807\u4ee5\u91cf\u5316\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFretting-Transformer\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86A*\u7b49\u57fa\u7ebf\u65b9\u6cd5\u548cGuitar Pro\u7b49\u5546\u4e1a\u5e94\u7528\u3002\u4e0a\u4e0b\u6587\u654f\u611f\u5904\u7406\u4ee5\u53ca\u8c03\u5f26/\u53d8\u8c03\u5939\u6761\u4ef6\u7684\u96c6\u6210\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5409\u4ed6\u81ea\u52a8\u8f6c\u5f55\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.13819", "pdf": "https://arxiv.org/pdf/2506.13819", "abs": "https://arxiv.org/abs/2506.13819", "authors": ["El Arbi Belfarsi", "Henry Flores", "Maria Valero"], "title": "Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Submitted to the IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI 2025)", "summary": "In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u6ce2\u7ea2\u5916\u5149\u8c31\u7684\u53cc\u6a21\u6001AI\u6846\u67b6\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u5149\u7535\u4e8c\u6781\u7ba1\u7535\u538b\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u65e0\u521b\u8840\u7cd6\u76d1\u6d4b\u3002", "motivation": "\u4f20\u7edf\u8840\u7cd6\u76d1\u6d4b\u65b9\u6cd5\u9700\u8981\u4fb5\u5165\u6027\u64cd\u4f5c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u9760\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u7a7f\u6234\u7684\u65e0\u521b\u8840\u7cd6\u76d1\u6d4b\u6280\u672f\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53cc\u6a21\u6001\u65b9\u6cd5\uff1a\u7b2c\u4e00\u6a21\u6001\u4f7f\u7528\u591a\u6ce2\u957f\u77ed\u6ce2\u7ea2\u5916\u6210\u50cf\u7cfb\u7edf\u548cCNN\u6355\u6349\u4e0e\u8461\u8404\u7cd6\u5438\u6536\u76f8\u5173\u7684\u7a7a\u95f4\u7279\u5f81\uff1b\u7b2c\u4e8c\u6a21\u6001\u4f7f\u7528\u7d27\u51d1\u578b\u5149\u7535\u4e8c\u6781\u7ba1\u7535\u538b\u4f20\u611f\u5668\u548c\u673a\u5668\u5b66\u4e60\u56de\u5f52\u5668\u5904\u7406\u5f52\u4e00\u5316\u5149\u5b66\u4fe1\u53f7\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u5728\u5408\u6210\u8840\u6db2\u6a21\u578b\u548c\u6a21\u62df\u76ae\u80a4\u6750\u6599\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "CNN\u5728650 nm\u6ce2\u957f\u4e0b\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u4e3a4.82%\uff0cClarke\u8bef\u5dee\u7f51\u683c\u4e2dZone A\u8986\u76d6\u7387\u8fbe100%\uff1b\u5149\u7535\u4e8c\u6781\u7ba1\u7cfb\u7edf\u7684Zone A\u51c6\u786e\u7387\u4e3a86.4%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e34\u5e8a\u7cbe\u5ea6\u3001\u6210\u672c\u6548\u76ca\u548c\u53ef\u7a7f\u6234\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u53ef\u9760\u7684\u65e0\u521b\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "paper_title_zh": "\u57fa\u4e8eCNN\u5149\u8c31\u6280\u672f\u7684\u53ef\u9760\u65e0\u521b\u8840\u7cd6\u4f20\u611f", "abstract_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u6ce2\u7ea2\u5916\uff08SWIR\uff09\u5149\u8c31\u7684\u53cc\u6a21\u6001AI\u6846\u67b6\u3002\u7b2c\u4e00\u6a21\u6001\u91c7\u7528\u591a\u6ce2\u957fSWIR\u6210\u50cf\u7cfb\u7edf\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6355\u6349\u4e0e\u8461\u8404\u7cd6\u5438\u6536\u76f8\u5173\u7684\u7a7a\u95f4\u7279\u5f81\uff1b\u7b2c\u4e8c\u6a21\u6001\u4f7f\u7528\u7d27\u51d1\u578b\u5149\u7535\u4e8c\u6781\u7ba1\u7535\u538b\u4f20\u611f\u5668\u548c\u673a\u5668\u5b66\u4e60\u56de\u5f52\u5668\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u5904\u7406\u5f52\u4e00\u5316\u5149\u5b66\u4fe1\u53f7\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u5728\u751f\u7406\u8461\u8404\u7cd6\u6c34\u5e73\uff0870\u81f3200 mg/dL\uff09\u7684\u5408\u6210\u8840\u6db2\u6a21\u578b\u548c\u6a21\u62df\u76ae\u80a4\u6750\u6599\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002CNN\u5728650 nm\u6ce2\u957f\u4e0b\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a4.82%\uff0cClarke\u8bef\u5dee\u7f51\u683c\u4e2dZone A\u8986\u76d6\u7387\u8fbe100%\uff1b\u5149\u7535\u4e8c\u6781\u7ba1\u7cfb\u7edf\u7684Zone A\u51c6\u786e\u7387\u4e3a86.4%\u3002\u8be5\u6846\u67b6\u5728\u4e34\u5e8a\u7cbe\u5ea6\u3001\u6210\u672c\u6548\u76ca\u548c\u53ef\u7a7f\u6234\u96c6\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u53ef\u9760\u7684\u65e0\u521b\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.13845", "pdf": "https://arxiv.org/pdf/2506.13845", "abs": "https://arxiv.org/abs/2506.13845", "authors": ["Griffin Pitts", "Neha Rani", "Weedguet Mildort", "Eva-Marie Cook"], "title": "Students' Reliance on AI in Higher Education: Identifying Contributing Factors", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u5b66\u751f\u5bf9AI\u7684\u4f9d\u8d56\u7a0b\u5ea6\u4e0e\u7f16\u7a0b\u81ea\u6211\u6548\u80fd\u3001\u7f16\u7a0b\u7d20\u517b\u548c\u8ba4\u77e5\u9700\u6c42\u76f8\u5173\uff0c\u9002\u5f53\u4f9d\u8d56\u4e0e\u4efb\u52a1\u540e\u4fe1\u4efb\u548c\u6ee1\u610f\u5ea6\u8d1f\u76f8\u5173\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u4e0e\u4efb\u52a1\u540e\u4fe1\u4efb\u548c\u6ee1\u610f\u5ea6\u6b63\u76f8\u5173\uff0c\u800c\u4f9d\u8d56\u4e0d\u8db3\u4e0e\u7f16\u7a0b\u7d20\u517b\u3001\u81ea\u6211\u6548\u80fd\u548c\u8ba4\u77e5\u9700\u6c42\u8d1f\u76f8\u5173\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5b66\u751f\u5bf9AI\u7684\u8fc7\u5ea6\u4f9d\u8d56\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6548\u679c\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5927\u5b66\u751f\u5bf9AI\u4f9d\u8d56\u6a21\u5f0f\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u3001\u9002\u5f53\u4f9d\u8d56\u548c\u4f9d\u8d56\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u91c7\u7528\u524d\u540e\u95ee\u5377\u8c03\u67e5\u548c\u5b9e\u9a8c\u4efb\u52a1\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u53c2\u4e0e\u8005\u5728\u4f7f\u7528\u63d0\u4f9b\u51c6\u786e\u548c\u9519\u8bef\u5efa\u8bae\u7684AI\u52a9\u624b\u89e3\u51b3\u7f16\u7a0b\u95ee\u9898\u65f6\uff0c\u5176\u4f9d\u8d56\u6a21\u5f0f\u88ab\u76f4\u63a5\u89c2\u5bdf\u3002", "result": "\u9002\u5f53\u4f9d\u8d56\u4e0e\u7f16\u7a0b\u81ea\u6211\u6548\u80fd\u3001\u7f16\u7a0b\u7d20\u517b\u548c\u8ba4\u77e5\u9700\u6c42\u663e\u8457\u76f8\u5173\uff0c\u4f46\u4e0e\u4efb\u52a1\u540e\u4fe1\u4efb\u548c\u6ee1\u610f\u5ea6\u8d1f\u76f8\u5173\uff1b\u8fc7\u5ea6\u4f9d\u8d56\u4e0e\u4efb\u52a1\u540e\u4fe1\u4efb\u548c\u6ee1\u610f\u5ea6\u6b63\u76f8\u5173\uff1b\u4f9d\u8d56\u4e0d\u8db3\u4e0e\u7f16\u7a0b\u7d20\u517b\u3001\u81ea\u6211\u6548\u80fd\u548c\u8ba4\u77e5\u9700\u6c42\u8d1f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u4ee5\u4fc3\u8fdb\u5b66\u751f\u5bf9AI\u5de5\u5177\u7684\u9002\u5f53\u4f9d\u8d56\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5bf9AI\u5728\u6559\u80b2\u6280\u672f\u548c\u8bfe\u7a0b\u4e2d\u7684\u6574\u5408\u5177\u6709\u542f\u793a\u610f\u4e49\u3002", "paper_title_zh": "\u9ad8\u7b49\u6559\u80b2\u4e2d\u5b66\u751f\u5bf9AI\u7684\u4f9d\u8d56\uff1a\u8bc6\u522b\u5f71\u54cd\u56e0\u7d20", "abstract_zh": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5de5\u5177\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\u548c\u4f7f\u7528\uff0c\u5b66\u751f\u5bf9AI\u7684\u8fc7\u5ea6\u4f9d\u8d56\u5f15\u53d1\u4e86\u62c5\u5fe7\u3002\u8fc7\u5ea6\u4f9d\u8d56\u8868\u73b0\u4e3a\u4e2a\u4f53\u672a\u7ecf\u6279\u5224\u6027\u8bc4\u4f30\u5373\u63a5\u53d7AI\u751f\u6210\u7684\u4e0d\u6b63\u786e\u5efa\u8bae\uff0c\u5bfc\u81f4\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u7f3a\u9677\u5e76\u5f71\u54cd\u5b66\u4e60\u6548\u679c\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u672c\u79d1\u751f\u5bf9AI\u4f9d\u8d56\u6a21\u5f0f\u7684\u6f5c\u5728\u5f71\u54cd\u56e0\u7d20\uff0c\u4e0d\u4ec5\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u8fd8\u5305\u62ec\u9002\u5f53\u4f9d\u8d56\uff08\u6b63\u786e\u63a5\u53d7\u6709\u76ca\u5e76\u62d2\u7edd\u6709\u5bb3\u5efa\u8bae\uff09\u548c\u4f9d\u8d56\u4e0d\u8db3\uff08\u9519\u8bef\u62d2\u7edd\u6709\u76ca\u5efa\u8bae\uff09\u3002\u7814\u7a76\u65b9\u6cd5\u7ed3\u5408\u4e86\u524d\u540e\u95ee\u5377\u8c03\u67e5\u548c\u4e00\u9879\u53d7\u63a7\u5b9e\u9a8c\u4efb\u52a1\uff0c\u53c2\u4e0e\u8005\u5728AI\u52a9\u624b\u7684\u5e2e\u52a9\u4e0b\u89e3\u51b3\u7f16\u7a0b\u95ee\u9898\uff0c\u8be5\u52a9\u624b\u63d0\u4f9b\u51c6\u786e\u548c\u6545\u610f\u9519\u8bef\u7684\u5efa\u8bae\uff0c\u4ece\u800c\u76f4\u63a5\u89c2\u5bdf\u5b66\u751f\u5728\u9762\u5bf9\u4e0d\u540cAI\u53ef\u9760\u6027\u65f6\u7684\u4f9d\u8d56\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9002\u5f53\u4f9d\u8d56\u4e0e\u5b66\u751f\u7684\u7f16\u7a0b\u81ea\u6211\u6548\u80fd\u3001\u7f16\u7a0b\u7d20\u517b\u548c\u8ba4\u77e5\u9700\u6c42\u663e\u8457\u76f8\u5173\uff0c\u4f46\u4e0e\u4efb\u52a1\u540e\u4fe1\u4efb\u548c\u6ee1\u610f\u5ea6\u5448\u8d1f\u76f8\u5173\uff1b\u8fc7\u5ea6\u4f9d\u8d56\u4e0e\u4efb\u52a1\u540e\u4fe1\u4efb\u548c\u5bf9AI\u52a9\u624b\u7684\u6ee1\u610f\u5ea6\u663e\u8457\u76f8\u5173\uff1b\u4f9d\u8d56\u4e0d\u8db3\u4e0e\u7f16\u7a0b\u7d20\u517b\u3001\u7f16\u7a0b\u81ea\u6211\u6548\u80fd\u548c\u8ba4\u77e5\u9700\u6c42\u5448\u8d1f\u76f8\u5173\u3002\u603b\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u4ee5\u4fc3\u8fdb\u5bf9AI\u5de5\u5177\u7684\u9002\u5f53\u4f9d\u8d56\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5bf9AI\u5728\u8bfe\u7a0b\u548c\u6559\u80b2\u6280\u672f\u4e2d\u7684\u6574\u5408\u5177\u6709\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2506.14280", "pdf": "https://arxiv.org/pdf/2506.14280", "abs": "https://arxiv.org/abs/2506.14280", "authors": ["Bai Cong", "Nico Daheim", "Yuesong Shen", "Rio Yokota", "Mohammad Emtiyaz Khan", "Thomas M\u00f6llenhoff"], "title": "Improving LoRA with Variational Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "16 pages, 4 figures", "summary": "Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u53d8\u5206\u5b66\u4e60\u7b97\u6cd5IVON\u6539\u8fdbLoRA\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u591a\u9879\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\u548c\u6821\u51c6\u8bef\u5dee\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u4e0eAdamW\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u65b9\u6cd5\u867d\u80fd\u6539\u8fdbLoRA\u5fae\u8c03\u7684\u6821\u51c6\u6548\u679c\uff0c\u4f46\u5bf9\u5176\u4ed6\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\uff09\u63d0\u5347\u6709\u9650\u751a\u81f3\u6709\u5bb3\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7IVON\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53d8\u5206\u7b97\u6cd5IVON\u7ed3\u5408\u540e\u9a8c\u526a\u679d\u6280\u672f\uff0c\u5bf9LoRA\u5fae\u8c03\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5728\u5927\u89c4\u6a21LLM\uff08\u5982Llama\u548cQwen\u7cfb\u5217\uff09\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5728Llama-3.2-3B\u6a21\u578b\u4e0a\uff0cIVON\u5c06\u51c6\u786e\u7387\u63d0\u53471.3%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e5.4%\uff0c\u4f18\u4e8eAdamW\u53ca\u5176\u4ed6\u8d1d\u53f6\u65af\u65b9\u6cd5\uff08\u5982Laplace-LoRA\u548cBLoB\uff09\u3002", "conclusion": "\u53d8\u5206\u5b66\u4e60\u7b97\u6cd5IVON\u80fd\u9ad8\u6548\u6539\u8fdbLoRA\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u6307\u6807\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "paper_title_zh": "\u901a\u8fc7\u53d8\u5206\u5b66\u4e60\u6539\u8fdbLoRA", "abstract_zh": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u8fd1\u671f\u88ab\u7528\u4e8e\u6539\u8fdbLoRA\u5fae\u8c03\uff0c\u867d\u80fd\u63d0\u5347\u6821\u51c6\u6548\u679c\uff0c\u4f46\u5bf9\u5176\u4ed6\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\uff09\u5f71\u54cd\u751a\u5fae\u751a\u81f3\u6709\u5bb3\uff0c\u4e14\u589e\u52a0\u4e86\u8ba1\u7b97\u5f00\u9500\u3002\u672c\u6587\u901a\u8fc7\u53d8\u5206\u7b97\u6cd5IVON\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002IVON\u6613\u4e8e\u5b9e\u73b0\uff0c\u8ba1\u7b97\u6210\u672c\u4e0eAdamW\u76f8\u5f53\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u7684\u540e\u9a8c\u526a\u679d\u6280\u672f\u663e\u8457\u63d0\u5347\u591a\u9879\u6307\u6807\u3002\u6211\u4eec\u5728\u5341\u4ebf\u7ea7LLM\uff08\u5982Llama\u548cQwen\u7cfb\u5217\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8fdc\u8d85IVON\u73b0\u6709\u5e94\u7528\u89c4\u6a21\u3002\u4f8b\u5982\uff0c\u5728Llama-3.2-3B\u6a21\u578b\u4e0a\u8fdb\u884c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u5fae\u8c03\uff0c\u51c6\u786e\u7387\u6bd4AdamW\u63d0\u53471.3%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e5.4%\uff0c\u4f18\u4e8eAdamW\u53ca\u5176\u4ed6\u8d1d\u53f6\u65af\u65b9\u6cd5\uff08\u5982Laplace-LoRA\u548cBLoB\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0cIVON\u53d8\u5206\u5b66\u4e60\u80fd\u6709\u6548\u6539\u8fdbLoRA\u5fae\u8c03\u3002"}}
{"id": "2506.14107", "pdf": "https://arxiv.org/pdf/2506.14107", "abs": "https://arxiv.org/abs/2506.14107", "authors": ["Jinwoo Hwang", "Daeun Kim", "Sangyeop Lee", "Yoonsung Kim", "Guseul Heo", "Hojoon Kim", "Yunseok Jeong", "Tadiwos Meaza", "Eunhyeok Park", "Jeongseob Ahn", "Jongse Park"], "title": "D\u00e9j\u00e0 Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse", "categories": ["cs.DC", "cs.CV"], "comment": "Accepted to 2025 VLDB", "summary": "Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces D\u00e9j\u00e0 Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, D\u00e9j\u00e0 Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that D\u00e9j\u00e0 Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faD\u00e9j\u00e0 Vu\uff0c\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u578b\u5e27\u95f4\u8ba1\u7b97\u91cd\u7528\u7684\u89c6\u9891-\u8bed\u8a00\u67e5\u8be2\u5f15\u64ce\uff0c\u901a\u8fc7\u6539\u8fdb\u7684ReuseViT\u6a21\u578b\u548c\u5185\u5b58-\u8ba1\u7b97\u8054\u5408\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u52a0\u901f\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u751f\u6210\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VideoLMs\uff09\u4f9d\u8d56Vision Transformers\uff08ViTs\uff09\u9010\u5e27\u5904\u7406\u89c6\u9891\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faD\u00e9j\u00e0 Vu\u5f15\u64ce\uff0c\u6838\u5fc3\u4e3aReuseViT\u6a21\u578b\uff0c\u5b66\u4e60\u68c0\u6d4b\u5e27\u95f4\u8ba1\u7b97\u91cd\u7528\u673a\u4f1a\uff1b\u7ed3\u5408\u5185\u5b58-\u8ba1\u7b97\u8054\u5408\u538b\u7f29\u6280\u672f\uff0c\u5c06\u8ba1\u7b97\u8282\u7701\u8f6c\u5316\u4e3a\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728\u4e09\u4e2aVideoLM\u4efb\u52a1\u4e2d\uff0cD\u00e9j\u00e0 Vu\u5c06\u5d4c\u5165\u751f\u6210\u901f\u5ea6\u63d0\u5347\u81f3\u591a2.64\u500d\uff0c\u8bef\u5dee\u63a7\u5236\u57282%\u4ee5\u5185\uff0c\u663e\u8457\u589e\u5f3a\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u7684\u53ef\u884c\u6027\u3002", "conclusion": "D\u00e9j\u00e0 Vu\u901a\u8fc7\u8ba1\u7b97\u91cd\u7528\u548c\u4f18\u5316\u6280\u672f\uff0c\u9ad8\u6548\u52a0\u901f\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "D\u00e9j\u00e0 Vu\uff1a\u57fa\u4e8e\u5b66\u4e60\u578b\u5e27\u95f4\u8ba1\u7b97\u91cd\u7528\u7684\u9ad8\u6548\u89c6\u9891-\u8bed\u8a00\u67e5\u8be2\u5f15\u64ce", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VideoLMs\uff09\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4e3a\u7075\u6d3b\u9ad8\u6548\u7684\u89c6\u9891\u67e5\u8be2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u4f9d\u8d56Vision Transformers\uff08ViTs\uff09\u9010\u5e27\u63d0\u53d6\u89c6\u89c9\u5d4c\u5165\uff0c\u4f46\u5927\u89c4\u6a21\u89c6\u9891\u7684\u5d4c\u5165\u751f\u6210\u9700\u8981\u5927\u91cf\u8ba1\u7b97\uff0c\u6210\u4e3a\u5b9e\u9645\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\u3002\u672c\u6587\u63d0\u51faD\u00e9j\u00e0 Vu\uff0c\u4e00\u79cd\u89c6\u9891-\u8bed\u8a00\u67e5\u8be2\u5f15\u64ce\uff0c\u901a\u8fc7\u91cd\u7528\u8fde\u7eed\u5e27\u95f4\u7684\u8ba1\u7b97\u52a0\u901fViT-based VideoLMs\u3002\u5176\u6838\u5fc3\u662fReuseViT\uff0c\u4e00\u79cd\u4e13\u4e3aVideoLM\u4efb\u52a1\u8bbe\u8ba1\u7684\u6539\u8fdbViT\u6a21\u578b\uff0c\u5b66\u4e60\u68c0\u6d4b\u5e27\u95f4\u91cd\u7528\u673a\u4f1a\uff0c\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u91cd\u7528\u6548\u7387\u3002\u5c3d\u7ba1ReuseViT\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u8fd9\u4e9b\u8282\u7701\u672a\u76f4\u63a5\u8f6c\u5316\u4e3aGPU\u6027\u80fd\u63d0\u5347\u3002\u4e3a\u6b64\uff0cD\u00e9j\u00e0 Vu\u96c6\u6210\u5185\u5b58-\u8ba1\u7b97\u8054\u5408\u538b\u7f29\u6280\u672f\uff0c\u5c06\u8ba1\u7b97\u8282\u7701\u8f6c\u5316\u4e3a\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002\u5728\u4e09\u4e2aVideoLM\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cD\u00e9j\u00e0 Vu\u57282%\u8bef\u5dee\u8303\u56f4\u5185\u5c06\u5d4c\u5165\u751f\u6210\u901f\u5ea6\u63d0\u5347\u81f3\u591a2.64\u500d\uff0c\u6781\u5927\u589e\u5f3a\u4e86VideoLMs\u5728\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.13862", "pdf": "https://arxiv.org/pdf/2506.13862", "abs": "https://arxiv.org/abs/2506.13862", "authors": ["Alena Shilova", "Alex Davey", "Brahim Driss", "Riad Akrour"], "title": "StaQ it! Growing neural networks for Policy Mirror Descent", "categories": ["cs.LG", "cs.AI"], "comment": "44 pages, 12 figures", "summary": "In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStaQ\u7684\u65b0\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08PMD\uff09\u56e0\u5b58\u50a8\u6240\u6709\u5386\u53f2Q\u51fd\u6570\u800c\u5bfc\u81f4\u7684\u5b9e\u8df5\u96be\u9898\uff0c\u901a\u8fc7\u4ec5\u4fdd\u7559\u6700\u8fd1\u7684M\u4e2aQ\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6b63\u5219\u5316\u5de5\u5177\uff08\u5982\u71b5\u5956\u52b1\u6216KL\u6563\u5ea6\uff09\u867d\u80fd\u63d0\u5347\u63a2\u7d22\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4f46\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08PMD\uff09\u7684\u7406\u8bba\u6846\u67b6\u56e0\u9700\u5b58\u50a8\u6240\u6709\u5386\u53f2Q\u51fd\u6570\u800c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684PMD\u5b9e\u73b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faStaQ\u7b97\u6cd5\uff0c\u4ec5\u4fdd\u7559\u6700\u8fd1\u7684M\u4e2aQ\u51fd\u6570\uff0c\u907f\u514d\u5b58\u50a8\u6240\u6709\u5386\u53f2\u6570\u636e\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\uff0c\u5f53M\u8db3\u591f\u5927\u65f6\uff0c\u7b97\u6cd5\u80fd\u5b9e\u73b0\u6536\u655b\u4e14\u4e0d\u5f15\u5165\u7b56\u7565\u66f4\u65b0\u8bef\u5dee\u3002", "result": "StaQ\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u5f3a\u4fdd\u8bc1\uff0c\u5b9e\u9645\u8868\u73b0\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u7ade\u4e89\uff0c\u4e14\u6027\u80fd\u6ce2\u52a8\u66f4\u5c0f\uff0c\u4e3a\u5b8c\u5168\u7a33\u5b9a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "StaQ\u7b97\u6cd5\u901a\u8fc7\u7b80\u5316PMD\u7684\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u5b58\u50a8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7406\u8bba\u4f18\u52bf\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "StaQ\u7b97\u6cd5\uff1a\u4e3a\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u6784\u5efa\u795e\u7ecf\u7f51\u7edc", "abstract_zh": "\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\uff0c\u6b63\u5219\u5316\u5df2\u6210\u4e3a\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5e38\u7528\u7684\u5de5\u5177\uff0c\u901a\u5e38\u57fa\u4e8e\u71b5\u5956\u52b1\u6216KL\u6563\u5ea6\u6765\u7ea6\u675f\u8fde\u7eed\u7b56\u7565\u3002\u5b9e\u8df5\u4e2d\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u63d0\u5347\u4e86\u63a2\u7d22\u6027\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u50ac\u751f\u4e86SAC\u548cTRPO\u7b49\u6d41\u884c\u7b97\u6cd5\u3002\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08PMD\uff09\u662f\u89e3\u51b3\u6b64\u7c7b\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\u95ee\u9898\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4f46\u5176\u95ed\u5f0f\u89e3\u9700\u5b58\u50a8\u6240\u6709\u5386\u53f2Q\u51fd\u6570\uff0c\u5b9e\u9645\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u4ec5\u4fdd\u7559\u6700\u8fd1M\u4e2aQ\u51fd\u6570\u7684PMD\u7c7b\u7b97\u6cd5\uff0c\u8bc1\u660e\u5f53M\u8db3\u591f\u5927\u65f6\uff0c\u53ef\u5bfc\u51fa\u6536\u655b\u7b97\u6cd5\u4e14\u4e0d\u5f15\u5165\u7b56\u7565\u66f4\u65b0\u8bef\u5dee\u3002\u6700\u7ec8\u7b97\u6cd5StaQ\u5177\u6709\u5f3a\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e0e\u6df1\u5ea6RL\u57fa\u7ebf\u7ade\u4e89\uff0c\u4e14\u6027\u80fd\u6ce2\u52a8\u66f4\u5c0f\uff0c\u4e3a\u5b8c\u5168\u7a33\u5b9a\u7684\u6df1\u5ea6RL\u7b97\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u4e3aPMD\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2506.14574", "pdf": "https://arxiv.org/pdf/2506.14574", "abs": "https://arxiv.org/abs/2506.14574", "authors": ["Mingkang Zhu", "Xi Chen", "Zhongdao Wang", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTGDPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5e8f\u5217\u7ea7PPO\u4e3a\u4ee4\u724c\u7ea7\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5229\u7528\u4ee4\u724c\u7ea7\u5956\u52b1\u6307\u5bfcDPO\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u660e\u663e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u4ee4\u724c\u7ea7\u5956\u52b1\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347PPO\u6027\u80fd\uff0c\u4f46\u96be\u4ee5\u76f4\u63a5\u7528\u4e8eDPO\uff0c\u56e0\u4e3aDPO\u662f\u5e8f\u5217\u7ea7\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5c06\u5e8f\u5217\u7ea7PPO\u5206\u89e3\u4e3a\u4ee4\u724c\u7ea7\u4f18\u5316\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u4ee4\u724c\u7ea7\u6700\u4f18\u7b56\u7565\u548c\u5956\u52b1\uff0c\u5e76\u57fa\u4e8eBradley-Terry\u6a21\u578b\u8bbe\u8ba1\u53ef\u8ba1\u7b97\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bf1\u5bfc\u5956\u52b1\u7684\u5b9e\u7528\u6307\u5bfc\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTGDPO\u5728MT-Bench\u3001AlpacaEval 2\u548cArena-Hard\u4e0a\u5206\u522b\u53d6\u5f977.5\u30016.2\u548c4.3\u70b9\u7684\u80dc\u7387\u63d0\u5347\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8eDPO\u3002", "conclusion": "TGDPO\u901a\u8fc7\u4ee4\u724c\u7ea7\u5956\u52b1\u6307\u5bfc\u6709\u6548\u63d0\u5347\u4e86DPO\u6027\u80fd\uff0c\u4e3a\u5e8f\u5217\u7ea7\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "TGDPO\uff1a\u5229\u7528\u4ee4\u724c\u7ea7\u5956\u52b1\u6307\u5bfc\u589e\u5f3a\u76f4\u63a5\u504f\u597d\u4f18\u5316", "abstract_zh": "\u8fd1\u671f\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u7ec6\u7c92\u5ea6\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u7531\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u88ab\u8868\u8ff0\u4e3a\u5e8f\u5217\u7ea7\u8d4c\u535a\u95ee\u9898\uff0c\u96be\u4ee5\u5c06\u6b64\u7c7b\u4ee4\u724c\u7ea7\u5956\u52b1\u4f5c\u4e3a\u6307\u5bfc\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u7814\u7a76\u5c06\u5e8f\u5217\u7ea7PPO\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u4ee4\u724c\u7ea7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63a8\u5bfc\u51fa\u4ee4\u724c\u7ea7\u6700\u4f18\u7b56\u7565\u53ca\u5bf9\u5e94\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u3002\u5229\u7528\u6240\u5f97\u5956\u52b1\u548cBradley-Terry\u6a21\u578b\uff0c\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee4\u724c\u7ea7\u5956\u52b1\u6307\u5bfc\u7684\u53ef\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bf1\u5bfcDPO\u5956\u52b1\u7684\u5b9e\u7528\u6307\u5bfc\u65b9\u6cd5\u3002\u8fd9\u4e00\u65b9\u6cd5\u4f7f\u5f97\u4e0d\u540c\u4ee4\u724c\u53ef\u4ee5\u6839\u636e\u5176\u5956\u52b1\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u53c2\u8003\u7b56\u7565\u504f\u79bb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728MT-Bench\u3001AlpacaEval 2\u548cArena-Hard\u4e0a\u5206\u522b\u5b9e\u73b0\u4e867.5\u30016.2\u548c4.3\u70b9\u7684\u80dc\u7387\u63d0\u5347\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8eDPO\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/dvlab-research/TGDPO\u3002"}}
{"id": "2506.14135", "pdf": "https://arxiv.org/pdf/2506.14135", "abs": "https://arxiv.org/abs/2506.14135", "authors": ["Ying Chai", "Litao Deng", "Ruizhi Shao", "Jiajun Zhang", "Liangjun Xing", "Hongwen Zhang", "Yebin Liu"], "title": "GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation", "categories": ["cs.RO", "cs.CV"], "comment": "http://chaiying1.github.io/GAF.github.io/project_page/", "summary": "Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAF\uff08\u9ad8\u65af\u52a8\u4f5c\u573a\uff09\u7684\u52a8\u6001\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u53ef\u5b66\u4e60\u7684\u8fd0\u52a8\u5c5e\u6027\u878d\u51653D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4ece4D\u8868\u793a\u76f4\u63a5\u63a8\u7406\u52a8\u4f5c\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u5230\u52a8\u4f5c\uff08V-A\uff09\u6216\u89c6\u89c9\u52303D\u5230\u52a8\u4f5c\uff08V-3D-A\uff09\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u5e38\u56e0\u52a8\u4f5c\u63a8\u65ad\u4e0d\u51c6\u786e\u800c\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u51654D\u8868\u793a\u548c\u52a8\u6001\u5efa\u6a21\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u52a8\u4f5c\u63a8\u7406\u95ee\u9898\u3002", "method": "GAF\u6269\u5c55\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8fd0\u52a8\u5c5e\u6027\uff0c\u652f\u6301\u4e09\u79cd\u5173\u952e\u67e5\u8be2\uff1a\u5f53\u524d\u573a\u666f\u91cd\u5efa\u3001\u672a\u6765\u5e27\u9884\u6d4b\u548c\u521d\u59cb\u52a8\u4f5c\u4f30\u8ba1\u3002\u6b64\u5916\uff0c\u901a\u8fc7GAF\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u4f18\u5316\u64cd\u4f5c\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAF\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u63d0\u5347\u4e8611.5385 dB PSNR\u548c\u964d\u4f4e\u4e860.5574 LPIPS\uff0c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8610.33%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GAF\u901a\u8fc7\u52a8\u60014D\u8868\u793a\u548c\u52a8\u4f5c\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u52a8\u4f5c\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "GAF\uff1a\u9ad8\u65af\u52a8\u4f5c\u573a\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u52a8\u6001\u4e16\u754c\u6a21\u578b", "abstract_zh": "\u51c6\u786e\u7684\u884c\u52a8\u63a8\u7406\u5bf9\u4e8e\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9075\u5faa\u89c6\u89c9\u5230\u52a8\u4f5c\uff08V-A\uff09\u8303\u5f0f\uff0c\u76f4\u63a5\u4ece\u89c6\u89c9\u8f93\u5165\u9884\u6d4b\u52a8\u4f5c\uff0c\u6216\u89c6\u89c9\u52303D\u5230\u52a8\u4f5c\uff08V-3D-A\uff09\u8303\u5f0f\uff0c\u5229\u7528\u4e2d\u95f43D\u8868\u793a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5e38\u56e0\u590d\u6742\u52a8\u6001\u573a\u666f\u5bfc\u81f4\u52a8\u4f5c\u4e0d\u51c6\u786e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdV-4D-A\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u52a8\u4f5c\u573a\uff08GAF\uff09\u5b9e\u73b0\u4ece\u8fd0\u52a8\u611f\u77e5\u76844D\u8868\u793a\u76f4\u63a5\u63a8\u7406\u52a8\u4f5c\u3002GAF\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8fd0\u52a8\u5c5e\u6027\u6269\u5c55\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u80fd\u591f\u540c\u65f6\u5efa\u6a21\u52a8\u6001\u573a\u666f\u548c\u64cd\u4f5c\u52a8\u4f5c\u3002\u4e3a\u5b66\u4e60\u65f6\u53d8\u573a\u666f\u51e0\u4f55\u548c\u52a8\u4f5c\u611f\u77e5\u7684\u673a\u5668\u4eba\u8fd0\u52a8\uff0cGAF\u652f\u6301\u4e09\u79cd\u5173\u952e\u67e5\u8be2\uff1a\u5f53\u524d\u573a\u666f\u91cd\u5efa\u3001\u672a\u6765\u5e27\u9884\u6d4b\u548c\u521d\u59cb\u52a8\u4f5c\u4f30\u8ba1\u3002\u6b64\u5916\uff0cGAF\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u5f53\u524d\u548c\u672a\u6765\u5e27\u901a\u8fc7GAF\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u4f18\u5316\u64cd\u4f5c\u52a8\u4f5c\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGAF\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u63d0\u5347\u4e8611.5385 dB PSNR\u548c\u964d\u4f4e\u4e860.5574 LPIPS\uff0c\u540c\u65f6\u5c06\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8610.33%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttp://chaiying1.github.io/GAF.github.io/project_page/"}}
{"id": "2506.14602", "pdf": "https://arxiv.org/pdf/2506.14602", "abs": "https://arxiv.org/abs/2506.14602", "authors": ["Haoyang Gui", "Thales Bertaglia", "Catalina Goanta", "Gerasimos Spanakis"], "title": "Computational Studies in Influencer Marketing: A Systematic Literature Review", "categories": ["cs.CY", "cs.CL"], "comment": "journal submission, under review", "summary": "Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u5206\u6790\u4e8669\u7bc7\u5173\u4e8e\u5f71\u54cd\u8005\u8425\u9500\u7684\u8ba1\u7b97\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u56db\u5927\u7814\u7a76\u4e3b\u9898\u548c\u65b9\u6cd5\u5206\u7c7b\uff0c\u5e76\u547c\u5401\u66f4\u591a\u5173\u6ce8\u4f26\u7406\u3001\u5408\u89c4\u53ca\u591a\u5b66\u79d1\u7814\u7a76\u3002", "motivation": "\u5f71\u54cd\u8005\u8425\u9500\u5728\u6570\u5b57\u8425\u9500\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u76f8\u5173\u8ba1\u7b97\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5bfc\u81f4\u79d1\u5b66\u6d4b\u91cf\u4e0d\u8db3\uff0c\u5f71\u54cd\u5e73\u53f0\u5916\u7684\u5229\u76ca\u76f8\u5173\u8005\uff08\u5982\u76d1\u7ba1\u8005\u548c\u5176\u4ed6\u9886\u57df\u7814\u7a76\u8005\uff09\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8ePRISMA\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u679069\u7bc7\u7814\u7a76\uff0c\u8bc6\u522b\u4e3b\u9898\u548c\u65b9\u6cd5\uff08\u673a\u5668\u5b66\u4e60\u4e0e\u975e\u673a\u5668\u5b66\u4e60\u6280\u672f\uff09\u3002", "result": "\u53d1\u73b0\u56db\u5927\u7814\u7a76\u4e3b\u9898\uff1a\u5f71\u54cd\u8005\u8bc6\u522b\u4e0e\u7279\u5f81\u3001\u5e7f\u544a\u7b56\u7565\u4e0e\u4e92\u52a8\u3001\u8d5e\u52a9\u5185\u5bb9\u5206\u6790\u4e0e\u53d1\u73b0\u3001\u516c\u5e73\u6027\uff1b\u65b9\u6cd5\u4ee5\u673a\u5668\u5b66\u4e60\u4e3a\u4e3b\uff0c\u4f46\u4f26\u7406\u548c\u5408\u89c4\u7814\u7a76\u4e0d\u8db3\u3002", "conclusion": "\u9700\u66f4\u591a\u7ed3\u5408\u4e0a\u4e0b\u6587\u56e0\u7d20\uff08\u5982\u8bed\u8a00\u3001\u5e73\u53f0\uff09\u7684\u7cbe\u7ec6\u5316\u7814\u7a76\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u63a8\u52a8\u591a\u5b66\u79d1\u7814\u7a76\u8bae\u7a0b\u3002", "paper_title_zh": "\u5f71\u54cd\u8005\u8425\u9500\u7684\u8ba1\u7b97\u7814\u7a76\uff1a\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0", "abstract_zh": "\u5f71\u54cd\u8005\u8425\u9500\u5df2\u6210\u4e3a\u6570\u5b57\u8425\u9500\u7b56\u7565\u7684\u5173\u952e\u90e8\u5206\u3002\u5c3d\u7ba1\u5176\u589e\u957f\u8fc5\u901f\u4e14\u7b97\u6cd5\u76f8\u5173\uff0c\u4f46\u5f71\u54cd\u8005\u8425\u9500\u7684\u8ba1\u7b97\u7814\u7a76\u9886\u57df\u4ecd\u8f83\u4e3a\u5206\u6563\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u6db5\u76d6\u8ba1\u7b97\u65b9\u6cd5\u3002\u8fd9\u4f7f\u5f97\u5f71\u54cd\u8005\u7ecf\u6d4e\u7684\u79d1\u5b66\u6d4b\u91cf\u975e\u5e38\u7a00\u7f3a\uff0c\u635f\u5bb3\u4e86\u5e73\u53f0\u5916\u5229\u76ca\u76f8\u5173\u8005\uff08\u5982\u76d1\u7ba1\u8005\u548c\u5176\u4ed6\u9886\u57df\u7814\u7a76\u8005\uff09\u7684\u5229\u76ca\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u57fa\u4e8ePRISMA\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\uff0c\u6982\u8ff0\u5f71\u54cd\u8005\u8425\u9500\u8ba1\u7b97\u7814\u7a76\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u6587\u7ae0\u5206\u6790\u4e8669\u9879\u7814\u7a76\uff0c\u4ee5\u8bc6\u522b\u8be5\u9886\u57df\u7684\u5173\u952e\u7814\u7a76\u4e3b\u9898\u3001\u65b9\u6cd5\u548c\u672a\u6765\u65b9\u5411\u3002\u7efc\u8ff0\u53d1\u73b0\u56db\u5927\u7814\u7a76\u4e3b\u9898\uff1a\u5f71\u54cd\u8005\u8bc6\u522b\u4e0e\u7279\u5f81\u3001\u5e7f\u544a\u7b56\u7565\u4e0e\u4e92\u52a8\u3001\u8d5e\u52a9\u5185\u5bb9\u5206\u6790\u4e0e\u53d1\u73b0\u3001\u516c\u5e73\u6027\u3002\u65b9\u6cd5\u4e0a\uff0c\u7814\u7a76\u5206\u4e3a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6280\u672f\uff08\u5982\u5206\u7c7b\u3001\u805a\u7c7b\uff09\u548c\u975e\u673a\u5668\u5b66\u4e60\u6280\u672f\uff08\u5982\u7edf\u8ba1\u5206\u6790\u3001\u7f51\u7edc\u5206\u6790\uff09\u3002\u4e3b\u8981\u53d1\u73b0\u663e\u793a\u7814\u7a76\u9ad8\u5ea6\u5173\u6ce8\u5546\u4e1a\u7ed3\u679c\u4f18\u5316\uff0c\u4f46\u5bf9\u5408\u89c4\u548c\u4f26\u7406\u5173\u6ce8\u6709\u9650\u3002\u7efc\u8ff0\u5f3a\u8c03\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8ba1\u7b97\u7814\u7a76\uff0c\u7ed3\u5408\u8bed\u8a00\u3001\u5e73\u53f0\u548c\u884c\u4e1a\u7c7b\u578b\u7b49\u4e0a\u4e0b\u6587\u56e0\u7d20\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u53ef\u91cd\u590d\u6027\u3002\u6587\u7ae0\u6700\u540e\u63d0\u51fa\u591a\u5b66\u79d1\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03\u9700\u52a0\u5f3a\u4e0e\u5408\u89c4\u6280\u672f\u7684\u8054\u7cfb\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\uff0c\u4ee5\u53ca\u6807\u51c6\u5316\u6570\u636e\u96c6\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.14198", "pdf": "https://arxiv.org/pdf/2506.14198", "abs": "https://arxiv.org/abs/2506.14198", "authors": ["Jeremy A. Collins", "Lor\u00e1nd Cheng", "Kunal Aneja", "Albert Wilcox", "Benjamin Joffe", "Animesh Garg"], "title": "AMPLIFY: Actionless Motion Priors for Robot Learning from Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.", "AI": {"tldr": "AMPLIFY\u662f\u4e00\u79cd\u5229\u7528\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\u5148\u9a8c\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u4e0e\u52a8\u4f5c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9700\u8981\u5927\u91cf\u5e26\u52a8\u4f5c\u6807\u7b7e\u7684\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u800c\u65e0\u52a8\u4f5c\u6807\u7b7e\u7684\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u4f46\u96be\u4ee5\u8f6c\u5316\u4e3a\u6709\u6548\u7b56\u7565\u3002AMPLIFY\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "AMPLIFY\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u63d0\u53d6\u7d27\u51d1\u7684\u8fd0\u52a8\u4ee4\u724c\uff0c\u5c06\u89c6\u89c9\u52a8\u6001\u7f16\u7801\u4e3a\u79bb\u6563\u8868\u793a\u3002\u5176\u6a21\u5757\u5316\u65b9\u6cd5\u5206\u79bb\u4e86\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u548c\u52a8\u4f5c\u63a8\u7406\uff0c\u5206\u522b\u5229\u7528\u65e0\u52a8\u4f5c\u89c6\u9891\u548c\u6709\u9650\u5e26\u52a8\u4f5c\u6570\u636e\u8bad\u7ec3\u524d\u5411\u548c\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "AMPLIFY\u5728\u52a8\u6001\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cMSE\u548c\u50cf\u7d20\u9884\u6d4b\u7cbe\u5ea6\u5206\u522b\u63d0\u53473.7\u500d\u548c2.5\u500d\u3002\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u53471.2-2.2\u500d\uff0c\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u5e73\u5747\u63d0\u53471.4\u500d\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u96f6\u5206\u5e03\u52a8\u4f5c\u6570\u636e\u7684LIBERO\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "AMPLIFY\u5c55\u793a\u4e86\u5229\u7528\u5f02\u6784\u6570\u636e\u6e90\u6784\u5efa\u9ad8\u6548\u3001\u901a\u7528\u4e16\u754c\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u5176\u5b66\u4e60\u5230\u7684\u52a8\u6001\u6a21\u578b\u8fd8\u53ef\u63d0\u5347\u89c6\u9891\u9884\u6d4b\u8d28\u91cf\u3002", "paper_title_zh": "AMPLIFY\uff1a\u57fa\u4e8e\u65e0\u52a8\u4f5c\u89c6\u9891\u7684\u673a\u5668\u4eba\u5b66\u4e60\u8fd0\u52a8\u5148\u9a8c", "abstract_zh": "\u673a\u5668\u4eba\u5b66\u4e60\u6240\u9700\u7684\u5e26\u52a8\u4f5c\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u91cf\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u6613\u4e8e\u83b7\u53d6\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u6548\u7b56\u7565\u4ecd\u5177\u6311\u6218\u3002\u672c\u6587\u63d0\u51faAMPLIFY\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u63d0\u53d6\u7d27\u51d1\u7684\u8fd0\u52a8\u4ee4\u724c\uff0c\u5c06\u89c6\u89c9\u52a8\u6001\u7f16\u7801\u4e3a\u79bb\u6563\u8868\u793a\u3002\u5176\u6a21\u5757\u5316\u65b9\u6cd5\u5206\u79bb\u4e86\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u4e0e\u52a8\u4f5c\u63a8\u7406\uff0c\u5206\u522b\u5229\u7528\u65e0\u52a8\u4f5c\u89c6\u9891\u548c\u6709\u9650\u5e26\u52a8\u4f5c\u6570\u636e\u8bad\u7ec3\u524d\u5411\u548c\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAMPLIFY\u7684\u52a8\u6001\u9884\u6d4b\u7cbe\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08MSE\u63d0\u53473.7\u500d\uff0c\u50cf\u7d20\u9884\u6d4b\u7cbe\u5ea6\u63d0\u53472.5\u500d\uff09\u3002\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u53471.2-2.2\u500d\uff0c\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u5e73\u5747\u63d0\u53471.4\u500d\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u96f6\u5206\u5e03\u52a8\u4f5c\u6570\u636e\u7684LIBERO\u4efb\u52a1\u6cdb\u5316\u3002\u6b64\u5916\uff0cAMPLIFY\u5b66\u4e60\u5230\u7684\u52a8\u6001\u6a21\u578b\u8fd8\u53ef\u4f5c\u4e3a\u901a\u7528\u6f5c\u5728\u4e16\u754c\u6a21\u578b\uff0c\u63d0\u5347\u89c6\u9891\u9884\u6d4b\u8d28\u91cf\u3002\u672c\u7814\u7a76\u4e3a\u5229\u7528\u5f02\u6784\u6570\u636e\u6e90\u6784\u5efa\u9ad8\u6548\u3001\u901a\u7528\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u8bbf\u95eehttps://amplify-robotics.github.io/\u3002"}}
{"id": "2506.13892", "pdf": "https://arxiv.org/pdf/2506.13892", "abs": "https://arxiv.org/abs/2506.13892", "authors": ["Samuel Beaussant", "Mehdi Mounsif"], "title": "Scaling Algorithm Distillation for Continuous Control with Mamba", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528S6\u6a21\u578b\uff08Mamba\uff09\u6539\u8fdb\u7b97\u6cd5\u84b8\u998f\uff08AD\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u4e8eTransformer\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u7b97\u6cd5\u84b8\u998f\uff08AD\uff09\u65b9\u6cd5\u56e0Transformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\uff0c\u4ec5\u9002\u7528\u4e8e\u7b80\u5355\u79bb\u6563\u73af\u5883\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7S6\u6a21\u578b\uff08Mamba\uff09\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6269\u5c55AD\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8eS6\u5c42\u7684Mamba\u6a21\u578b\u66ff\u4ee3Transformer\uff0c\u5229\u7528\u5176\u7ebf\u6027\u590d\u6742\u5ea6\u4f18\u52bf\uff0c\u5728\u56db\u4e2a\u590d\u6742\u7684\u8fde\u7eed\u5143\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMamba\u5728\u7b97\u6cd5\u84b8\u998f\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eTransformer\uff0c\u4e14\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f7f\u5176\u4e0e\u5f53\u524d\u6700\u4f18\u7684\u5728\u7ebf\u5143\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u7ade\u4e89\u3002", "conclusion": "Mamba\u6a21\u578b\u4e3a\u7b97\u6cd5\u84b8\u998f\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u5229\u7528Mamba\u6269\u5c55\u7b97\u6cd5\u84b8\u998f\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u7b97\u6cd5\u84b8\u998f\uff08AD\uff09\u6700\u8fd1\u88ab\u63d0\u51fa\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u56e0\u679cTransformer\u6a21\u578b\u5bf9\u8de8\u7247\u6bb5\u8bad\u7ec3\u5386\u53f2\u8fdb\u884c\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff08ICRL\uff09\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5b9e\u8df5\u9650\u5236\uff0c\u5b9e\u9a8c\u53d7\u9650\u4e8eTransformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u4ec5\u9002\u7528\u4e8e\u65f6\u95f4\u8de8\u5ea6\u77ed\u7684\u7b80\u5355\u79bb\u6563\u73af\u5883\u3002\u672c\u6587\u63d0\u51fa\u5229\u7528\u6700\u8fd1\u63d0\u51fa\u7684\u9009\u62e9\u6027\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217\uff08S6\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5e8f\u5217\u957f\u5ea6\u5448\u7ebf\u6027\u6269\u5c55\u3002\u901a\u8fc7\u5728\u56db\u4e2a\u590d\u6742\u7684\u8fde\u7eed\u5143\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u57fa\u4e8eS6\u5c42\u7684Mamba\u6a21\u578b\u5728AD\u4efb\u52a1\u4e2d\u4f18\u4e8eTransformer\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u5c06AD\u6269\u5c55\u5230\u975e\u5e38\u957f\u7684\u4e0a\u4e0b\u6587\u53ef\u4ee5\u63d0\u5347ICRL\u6027\u80fd\uff0c\u4f7f\u5176\u4e0e\u5f53\u524d\u6700\u4f18\u7684\u5728\u7ebf\u5143\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u7ade\u4e89\u3002"}}
{"id": "2506.14209", "pdf": "https://arxiv.org/pdf/2506.14209", "abs": "https://arxiv.org/abs/2506.14209", "authors": ["Pengwei Wang"], "title": "Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.\n  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.\n  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.\n  The proposed method achieves successful segmentation on both simulated and real patient data.\n  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801VQ-GAN\u7684\u65e0\u76d1\u7763\u533b\u5b66CBCT\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u81ea\u52a8\u5206\u5272\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\uff0c\u5e76\u53ef\u76f4\u63a5\u7528\u4e8e3D\u6253\u5370\u3002", "motivation": "\u7531\u4e8e\u988c\u9aa8\u653e\u5c04\u6027\u9aa8\u574f\u6b7b\uff08ONJ\uff09\u5f71\u50cf\u6570\u636e\u6807\u6ce8\u7a00\u7f3a\uff0c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e0d\u9002\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\uff0c\u81ea\u52a8\u8bc6\u522b\u5f71\u50cf\u4e2d\u7684\u5f02\u5e38\u533a\u57df\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3VQ-GAN\u7cbe\u786e\u91cd\u5efa\u6b63\u5e38\u6837\u672c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u968f\u673a\u7acb\u65b9\u4f53\u63a9\u7801\u548cONJ\u7279\u5b9a\u63a9\u7801\u8bad\u7ec3\u65b0\u7f16\u7801\u5668\u4ee5\u6062\u590d\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u4e0a\u5747\u5b9e\u73b0\u4e86\u6210\u529f\u5206\u5272\uff0c\u4e3a\u5feb\u901f\u521d\u59cb\u5206\u5272\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\uff0c\u8fd8\u80fd\u7ed3\u5408\u540e\u5904\u7406\u76f4\u63a5\u7528\u4e8e3D\u6253\u5370\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u6f5c\u5728\u5f02\u5e38\u68c0\u6d4b\uff1a\u57fa\u4e8e\u63a9\u7801VQ-GAN\u7684\u533b\u5b66CBCT\u65e0\u76d1\u7763\u5206\u5272", "abstract_zh": "\u6cbb\u7597\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97\u53ef\u5b9a\u5236\u76843D\u6253\u5370\u6c34\u51dd\u80f6\u4f24\u53e3\u6577\u6599\u53ef\u7528\u4e8e\u988c\u9aa8\u653e\u5c04\u6027\u9aa8\u574f\u6b7b\uff08ORN\uff09\u60a3\u8005\u3002\u540c\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5982nnUNet\u5df2\u80fd\u7cbe\u786e\u5206\u52723D\u533b\u5b66\u5f71\u50cf\u3002\u7136\u800c\uff0cONJ\u5f71\u50cf\u4e2d\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u4f7f\u5f97\u76d1\u7763\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\uff0c\u81ea\u52a8\u8bc6\u522b\u5f71\u50cf\u626b\u63cf\u4e2d\u7684\u5f02\u5e38\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3VQ-GAN\u4ee5\u7cbe\u786e\u91cd\u5efa\u6b63\u5e38\u6837\u672c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u968f\u673a\u7acb\u65b9\u4f53\u63a9\u7801\u548cONJ\u7279\u5b9a\u63a9\u7801\u8bad\u7ec3\u65b0\u7f16\u7801\u5668\u4ee5\u6062\u590d\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u4e0a\u5747\u5b9e\u73b0\u4e86\u6210\u529f\u5206\u5272\u3002\u8fd9\u4e00\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5feb\u901f\u7684\u521d\u59cb\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u8f7b\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u8d1f\u62c5\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u624b\u52a8\u8c03\u6574\u7684\u540e\u5904\u7406\uff0c\u8be5\u65b9\u6cd5\u8fd8\u6709\u6f5c\u529b\u76f4\u63a5\u7528\u4e8e3D\u6253\u5370\u3002"}}
{"id": "2506.13900", "pdf": "https://arxiv.org/pdf/2506.13900", "abs": "https://arxiv.org/abs/2506.13900", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Arthur Charpentier"], "title": "Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5408\u4f5c\u535a\u5f08\u8bba\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u6027\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u8d85\u8d8aShapley\u503c\u7684\u66f4\u5e7f\u6cdb\u5de5\u5177\uff0c\u5982Weber\u548cHarsanyi\u96c6\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89e3\u8bfb\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1Shapley\u503c\u5728\u673a\u5668\u5b66\u4e60\u89e3\u91ca\u6027\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u516c\u7406\u57fa\u7840\u4e0e\u7279\u5f81\u5f52\u56e0\u7684\u76f8\u5173\u6027\u4ecd\u5b58\u4e89\u8bae\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u5408\u4f5c\u535a\u5f08\u8bba\u7684\u5176\u4ed6\u5de5\u5177\uff0c\u4e3aXAI\u793e\u533a\u63d0\u4f9b\u66f4\u4e30\u5bcc\u4e14\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u5408\u4f5c\u535a\u5f08\u8bba\uff0c\u63d0\u51faWeber\u548cHarsanyi\u96c6\u4f5c\u4e3aShapley\u503c\u7684\u6269\u5c55\uff0c\u5e76\u533a\u5206\u4ef7\u503c\u51fd\u6570\u4e0e\u805a\u5408\u89c4\u5219\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u6b65\u84dd\u56fe\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u9760\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cWeber\u548cHarsanyi\u96c6\u80fd\u591f\u63d0\u4f9b\u6bd4Shapley\u503c\u66f4\u7075\u6d3b\u7684\u89e3\u8bfb\u65b9\u5f0f\uff0c\u540c\u65f6\u4e3a\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u547c\u5401\u8d85\u8d8a\u56fa\u5b9a\u516c\u7406\uff0c\u5229\u7528\u5408\u4f5c\u535a\u5f08\u8bba\u7684\u591a\u6837\u5316\u5de5\u5177\uff0c\u4e3aXAI\u793e\u533a\u8bbe\u8ba1\u66f4\u5177\u610f\u4e49\u4e14\u9002\u5e94\u65b9\u6cd5\u53d8\u5316\u7684\u7279\u5f81\u5f52\u56e0\u6846\u67b6\u3002", "paper_title_zh": "\u8d85\u8d8aShapley\u503c\uff1a\u5408\u4f5c\u535a\u5f08\u8bba\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u5408\u4f5c\u535a\u5f08\u8bba\u5df2\u6210\u4e3a\u673a\u5668\u5b66\u4e60\u4e8b\u540e\u89e3\u91ca\u6027\u7684\u57fa\u77f3\uff0c\u4e3b\u8981\u901a\u8fc7Shapley\u503c\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u5c3d\u7ba1Shapley\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u5176\u516c\u7406\u57fa\u7840\u5bf9\u7279\u5f81\u5f52\u56e0\u7684\u76f8\u5173\u6027\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u672c\u6587\u4ece\u89e3\u91ca\u6027\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u5408\u4f5c\u535a\u5f08\u8bba\uff0c\u4e3b\u5f20\u66f4\u5e7f\u6cdb\u4e14\u57fa\u4e8e\u539f\u5219\u5730\u4f7f\u7528\u5176\u5de5\u5177\u3002\u6211\u4eec\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4e24\u79cd\u9ad8\u6548\u7684\u5206\u914d\u65cf\u2014\u2014Weber\u96c6\u548cHarsanyi\u96c6\uff0c\u5b83\u4eec\u8d85\u8d8a\u4e86Shapley\u503c\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\u7075\u6d3b\u6027\u3002\u672c\u6587\u5bf9\u8fd9\u4e9b\u5206\u914d\u65b9\u6848\u8fdb\u884c\u4e86\u901a\u4fd7\u6613\u61c2\u7684\u6982\u8ff0\uff0c\u6f84\u6e05\u4e86\u4ef7\u503c\u51fd\u6570\u4e0e\u805a\u5408\u89c4\u5219\u7684\u533a\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u6b65\u84dd\u56fe\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u9760\u4e14\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u8d85\u8d8a\u56fa\u5b9a\u516c\u7406\uff0c\u4e3aXAI\u793e\u533a\u63d0\u4f9b\u4e00\u4e2a\u8fde\u8d2f\u7684\u6846\u67b6\uff0c\u4ee5\u8bbe\u8ba1\u65e2\u6709\u610f\u4e49\u53c8\u80fd\u9002\u5e94\u65b9\u6cd5\u53d8\u5316\u7684\u5f52\u56e0\u65b9\u6cd5\u3002"}}
{"id": "2506.14303", "pdf": "https://arxiv.org/pdf/2506.14303", "abs": "https://arxiv.org/abs/2506.14303", "authors": ["Niran Nataraj", "Maina Sogabe", "Kenji Kawashima"], "title": "orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "24 pages, 7figures", "summary": "Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small \"mimicking organ\" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faorGAN\uff0c\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5e26\u6ce8\u91ca\u7684\u624b\u672f\u51fa\u8840\u56fe\u50cf\uff0c\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4f26\u7406\u95ee\u9898\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4f26\u7406\u95ee\u9898\u3001\u91c7\u96c6\u6210\u672c\u9ad8\u548c\u7cbe\u786e\u6807\u6ce8\u9700\u6c42\u7b49\u6311\u6218\uff0c\u5c24\u5176\u662f\u624b\u672f\u4e2d\u51fa\u8840\u68c0\u6d4b\u548c\u5b9a\u4f4d\u56e0\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\u800c\u5c24\u4e3a\u56f0\u96be\u3002", "method": "orGAN\u57fa\u4e8eStyleGAN\u548c\u5173\u7cfb\u4f4d\u7f6e\u5b66\u4e60\uff0c\u5229\u7528\u5c0f\u578b\u201c\u6a21\u62df\u5668\u5b98\u201d\u6570\u636e\u96c6\u751f\u6210\u903c\u771f\u7684\u51fa\u8840\u56fe\u50cf\u5e76\u6807\u6ce8\u51fa\u8840\u5750\u6807\uff0c\u518d\u901a\u8fc7LaMa\u4fee\u590d\u6a21\u5757\u6062\u590d\u5e72\u51c0\u56fe\u50cf\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0corGAN\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u6a21\u62df\u5668\u5b98\u56fe\u50cf\u7ec4\u5408\u7684\u6570\u636e\u96c6\u5728\u624b\u672f\u573a\u666f\u4e2d\u8fbe\u523090%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u548c99%\u7684\u5e27\u7ea7\u51c6\u786e\u7387\u3002", "conclusion": "\u5c3d\u7ba1\u5f00\u53d1\u6570\u636e\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u5305\u542b\u672f\u4e2d\u4f2a\u5f71\uff0corGAN\u663e\u8457\u63a8\u52a8\u4e86\u4f26\u7406\u3001\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u771f\u5b9e\u6807\u6ce8\u51fa\u8840\u6570\u636e\u96c6\u7684\u751f\u6210\uff0c\u652f\u6301AI\u5728\u624b\u672f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "paper_title_zh": "orGAN\uff1a\u4e00\u79cd\u7528\u4e8e\u540c\u65f6\u751f\u6210\u624b\u672f\u56fe\u50cf\u548c\u771f\u5b9e\u6807\u7b7e\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u6d41\u7a0b", "abstract_zh": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u591a\u6837\u6027\u6709\u9650\u3001\u4f26\u7406\u95ee\u9898\u3001\u91c7\u96c6\u6210\u672c\u9ad8\u548c\u7cbe\u786e\u6807\u6ce8\u9700\u6c42\u7b49\u969c\u788d\u3002\u624b\u672f\u4e2d\u7684\u51fa\u8840\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5c24\u5176\u56f0\u96be\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u53cd\u6620\u771f\u5b9e\u624b\u672f\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u6211\u4eec\u63d0\u51faorGAN\uff0c\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5e26\u6ce8\u91ca\u7684\u624b\u672f\u51fa\u8840\u56fe\u50cf\u3002\u901a\u8fc7\u5229\u7528\u5c0f\u578b\u201c\u6a21\u62df\u5668\u5b98\u201d\u6570\u636e\u96c6\u548c\u6a21\u62df\u7ec4\u7ec7\u7279\u6027\u53ca\u51fa\u8840\u7684\u5408\u6210\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u4f26\u7406\u95ee\u9898\u548c\u6570\u636e\u91c7\u96c6\u6210\u672c\u3002orGAN\u57fa\u4e8eStyleGAN\u548c\u5173\u7cfb\u4f4d\u7f6e\u5b66\u4e60\uff0c\u903c\u771f\u6a21\u62df\u51fa\u8840\u4e8b\u4ef6\u5e76\u6807\u6ce8\u51fa\u8840\u5750\u6807\u3002\u968f\u540e\uff0c\u57fa\u4e8eLaMa\u7684\u4fee\u590d\u6a21\u5757\u6062\u590d\u5e72\u51c0\u7684\u51fa\u8840\u524d\u56fe\u50cf\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002\u8bc4\u4f30\u4e2d\uff0corGAN\u548c\u6a21\u62df\u5668\u5b98\u56fe\u50cf\u7684\u5e73\u8861\u6570\u636e\u96c6\u5728\u624b\u672f\u573a\u666f\u4e2d\u8fbe\u523090%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u548c99%\u7684\u5e27\u7ea7\u51c6\u786e\u7387\u3002\u5c3d\u7ba1\u5f00\u53d1\u6570\u636e\u7f3a\u4e4f\u5668\u5b98\u5f62\u6001\u591a\u6837\u6027\u548c\u5305\u542b\u672f\u4e2d\u4f2a\u5f71\uff0corGAN\u663e\u8457\u63a8\u52a8\u4e86\u4f26\u7406\u3001\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u771f\u5b9e\u6807\u6ce8\u51fa\u8840\u6570\u636e\u96c6\u7684\u751f\u6210\uff0c\u652f\u6301AI\u5728\u624b\u672f\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.14315", "pdf": "https://arxiv.org/pdf/2506.14315", "abs": "https://arxiv.org/abs/2506.14315", "authors": ["Jinyan Yuan", "Bangbang Yang", "Keke Wang", "Panwang Pan", "Lin Ma", "Xuehai Zhang", "Xiao Liu", "Zhaopeng Cui", "Yuewen Ma"], "title": "ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured Proxies", "categories": ["cs.GR", "cs.CV"], "comment": "Project webpage: https://immersegen.github.io", "summary": "Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery.This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.", "AI": {"tldr": "ImmerseGen\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u8f7b\u91cf\u7ea73D\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210RGBA\u7eb9\u7406\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\uff0c\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\uff0c\u663e\u8457\u7b80\u5316\u4e86\u4f20\u7edf\u590d\u6742\u5efa\u6a21\u6d41\u7a0b\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u7f51\u683c\u5efa\u6a21\u6216\u5927\u91cf3D\u9ad8\u65af\u6a21\u578b\uff0c\u5bfc\u81f4\u6d41\u7a0b\u590d\u6742\u6216\u89c6\u89c9\u771f\u5b9e\u611f\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4ee3\u7406\u548c\u7eb9\u7406\u5408\u6210\u6280\u672f\uff0c\u7b80\u5316\u5efa\u6a21\u6d41\u7a0b\u5e76\u63d0\u5347\u6c89\u6d78\u4f53\u9a8c\u3002", "method": "ImmerseGen\u91c7\u7528\u5206\u5c42\u8f7b\u91cf\u51e0\u4f55\u4ee3\u7406\uff08\u5982\u7b80\u5316\u5730\u5f62\u548c\u5e7f\u544a\u724c\u7f51\u683c\uff09\u8868\u793a\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u5408\u6210RGBA\u7eb9\u7406\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\u3002\u63d0\u51fa\u5730\u5f62\u6761\u4ef6\u7eb9\u7406\u5316\u548cRGBA\u8d44\u4ea7\u7eb9\u7406\u5316\u6280\u672f\uff0c\u7ed3\u5408\u57fa\u4e8eVLM\u7684\u5efa\u6a21\u4ee3\u7406\u5b9e\u73b0\u81ea\u52a8\u5316\u573a\u666f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cImmerseGen\u5728\u573a\u666f\u751f\u6210\u548c\u5b9e\u65f6VR\u5c55\u793a\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "ImmerseGen\u901a\u8fc7\u8f7b\u91cf\u4ee3\u7406\u548c\u7eb9\u7406\u5408\u6210\u6280\u672f\uff0c\u663e\u8457\u7b80\u5316\u4e863D\u573a\u666f\u751f\u6210\u6d41\u7a0b\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u771f\u5b9e\u611f\u548c\u5b9e\u65f6\u6e32\u67d3\u80fd\u529b\uff0c\u4e3a\u79fb\u52a8VR\u8bbe\u5907\u63d0\u4f9b\u4e86\u7d27\u51d1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "ImmerseGen\uff1a\u57fa\u4e8eAlpha\u7eb9\u7406\u4ee3\u7406\u7684\u667a\u80fd\u4f53\u5f15\u5bfc\u6c89\u6d78\u5f0f\u4e16\u754c\u751f\u6210", "abstract_zh": "\u81ea\u52a8\u751f\u62103D\u573a\u666f\u4ee5\u5b9e\u73b0\u6c89\u6d78\u5f0fVR\u4f53\u9a8c\u662f\u6570\u5341\u5e74\u6765\u7814\u7a76\u7684\u91cd\u70b9\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9ad8\u591a\u8fb9\u5f62\u7f51\u683c\u5efa\u6a21\u540e\u7b80\u5316\u6216\u5927\u91cf3D\u9ad8\u65af\u6a21\u578b\uff0c\u5bfc\u81f4\u6d41\u7a0b\u590d\u6742\u6216\u89c6\u89c9\u771f\u5b9e\u611f\u6709\u9650\u3002\u672c\u6587\u8bc1\u660e\uff0c\u5b9e\u73b0\u5f15\u4eba\u5165\u80dc\u7684\u6c89\u6d78\u4f53\u9a8c\u65e0\u9700\u5982\u6b64\u590d\u6742\u7684\u5efa\u6a21\u3002\u6211\u4eec\u63d0\u51faImmerseGen\uff0c\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u5f15\u5bfc\u7684\u7d27\u51d1\u4e14\u9ad8\u771f\u5b9e\u611f\u7684\u4e16\u754c\u5efa\u6a21\u6846\u67b6\u3002ImmerseGen\u5c06\u573a\u666f\u8868\u793a\u4e3a\u8f7b\u91cf\u51e0\u4f55\u4ee3\u7406\uff08\u5982\u7b80\u5316\u5730\u5f62\u548c\u5e7f\u544a\u724c\u7f51\u683c\uff09\u7684\u5206\u5c42\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u5728\u8fd9\u4e9b\u4ee3\u7406\u4e0a\u5408\u6210RGBA\u7eb9\u7406\u751f\u6210\u9ad8\u771f\u5b9e\u611f\u5916\u89c2\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u5730\u5f62\u6761\u4ef6\u7eb9\u7406\u5316\u6280\u672f\u7528\u4e8e\u7528\u6237\u4e2d\u5fc3\u7684\u57fa\u7840\u4e16\u754c\u5408\u6210\uff0c\u4ee5\u53caRGBA\u8d44\u4ea7\u7eb9\u7406\u5316\u6280\u672f\u7528\u4e8e\u4e2d\u666f\u548c\u524d\u666f\u573a\u666f\u3002\u8fd9\u4e00\u91cd\u6784\u5177\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a(i) \u901a\u8fc7\u667a\u80fd\u4f53\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u751f\u6210\u4e0e\u573a\u666f\u65e0\u7f1d\u878d\u5408\u7684\u7eb9\u7406\uff0c\u7b80\u5316\u5efa\u6a21\uff1b(ii) \u7ed5\u8fc7\u590d\u6742\u51e0\u4f55\u521b\u5efa\u4e0e\u7b80\u5316\uff0c\u76f4\u63a5\u5728\u4ee3\u7406\u4e0a\u5408\u6210\u9ad8\u771f\u5b9e\u611f\u7eb9\u7406\uff0c\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\uff1b(iii) \u63d0\u4f9b\u9002\u5408\u79fb\u52a8VR\u5934\u663e\u5b9e\u65f6\u6e32\u67d3\u7684\u7d27\u51d1\u8868\u793a\u3002\u4e3a\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u81ea\u52a8\u5316\u573a\u666f\u751f\u6210\uff0c\u6211\u4eec\u5f15\u5165\u57fa\u4e8eVLM\u7684\u5efa\u6a21\u4ee3\u7406\uff0c\u7ed3\u5408\u8bed\u4e49\u7f51\u683c\u5206\u6790\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u548c\u8d44\u4ea7\u5b9a\u4f4d\u51c6\u786e\u6027\u3002ImmerseGen\u8fd8\u901a\u8fc7\u52a8\u6001\u6548\u679c\u548c\u73af\u5883\u97f3\u9891\u589e\u5f3a\u573a\u666f\uff0c\u652f\u6301\u591a\u611f\u5b98\u6c89\u6d78\u3002\u573a\u666f\u751f\u6210\u548c\u5b9e\u65f6VR\u5c55\u793a\u5b9e\u9a8c\u8868\u660e\uff0cImmerseGen\u5728\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u9879\u76ee\u7f51\u9875\uff1ahttps://immersegen.github.io\u3002"}}
{"id": "2506.13903", "pdf": "https://arxiv.org/pdf/2506.13903", "abs": "https://arxiv.org/abs/2506.13903", "authors": ["Christel Sirocchi", "Damiano Verda"], "title": "Enhancing interpretability of rule-based classifiers through feature graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u89c4\u5219\u5206\u7c7b\u5668\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u65b0\u7279\u5f81\u91cd\u8981\u6027\u5ea6\u91cf\u548c\u89c4\u5219\u96c6\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9700\u8981\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u7684\u9886\u57df\uff0c\u89c4\u5219\u7cfb\u7edf\u56e0\u5176\u53ef\u89e3\u91ca\u6027\u800c\u88ab\u5e7f\u6cdb\u4f7f\u7528\u3002\u7136\u800c\uff0c\u968f\u7740\u89c4\u5219\u7cfb\u7edf\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u7406\u89e3\u7279\u5f81\u4ea4\u4e92\u548c\u8d21\u732e\u53d8\u5f97\u56f0\u96be\uff0c\u4e9f\u9700\u4e00\u79cd\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u7279\u5f81\u53ef\u89c6\u5316\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e0e\u89c4\u5219\u9884\u6d4b\u5668\u65e0\u5173\u7684\u7279\u5f81\u91cd\u8981\u6027\u5ea6\u91cf\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7279\u5f81\u8d21\u732e\u7684\u89c4\u5219\u96c6\u6bd4\u8f83\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8986\u76d6\u4e24\u79cd\u4e34\u5e8a\u6570\u636e\u548c\u56db\u79cd\u89c4\u5219\u65b9\u6cd5\uff08\u51b3\u7b56\u6811\u3001\u903b\u8f91\u5b66\u4e60\u673a\u3001\u5173\u8054\u89c4\u5219\u548c\u89c4\u5219\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\uff09\u3002", "result": "\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u7279\u5f81\u7ec4\u5408\u7684\u9884\u6d4b\u4ef7\u503c\uff0c\u5e2e\u52a9\u8bc6\u522b\u98ce\u9669\u56e0\u7d20\u548c\u751f\u7269\u6807\u5fd7\u7269\u3002\u572815\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5176\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u4e14\u66f4\u7a33\u5065\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5219\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u4fe1\u7684\u652f\u6301\u5de5\u5177\u3002", "paper_title_zh": "\u901a\u8fc7\u7279\u5f81\u56fe\u589e\u5f3a\u89c4\u5219\u5206\u7c7b\u5668\u7684\u53ef\u89e3\u91ca\u6027", "abstract_zh": "\u5728\u533b\u7597\u7b49\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\u7684\u9886\u57df\uff0c\u89c4\u5219\u7cfb\u7edf\u56e0\u5176\u56fa\u6709\u53ef\u89e3\u91ca\u6027\u800c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u968f\u7740\u89c4\u5219\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u8bc6\u522b\u5173\u952e\u7279\u5f81\u3001\u7406\u89e3\u5176\u4ea4\u4e92\u4ee5\u53ca\u6bd4\u8f83\u4e0d\u540c\u89c4\u5219\u96c6\u7684\u7279\u5f81\u8d21\u732e\u53d8\u5f97\u56f0\u96be\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u89c4\u5219\u7cfb\u7edf\u4e2d\u7684\u7279\u5f81\u8d21\u732e\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u7684\u7279\u5f81\u53ef\u89c6\u5316\u7b56\u7565\u3001\u4e00\u79cd\u4e0e\u89c4\u5219\u9884\u6d4b\u5668\u65e0\u5173\u7684\u65b0\u7279\u5f81\u91cd\u8981\u6027\u5ea6\u91cf\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7279\u5f81\u8d21\u732e\u7684\u89c4\u5219\u96c6\u6bd4\u8f83\u65b9\u6cd5\u3002\u901a\u8fc7\u5728\u4e24\u79cd\u4e34\u5e8a\u6570\u636e\u548c\u56db\u79cd\u89c4\u5219\u65b9\u6cd5\uff08\u51b3\u7b56\u6811\u3001\u903b\u8f91\u5b66\u4e60\u673a\u3001\u5173\u8054\u89c4\u5219\u548c\u89c4\u5219\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\uff09\u4e0a\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u548c\u7c7b\u522b\u5c42\u9762\u63ed\u793a\u4e34\u5e8a\u7279\u5f81\u7ec4\u5408\u9884\u6d4b\u4ef7\u503c\u7684\u80fd\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u8bc6\u522b\u65b0\u98ce\u9669\u56e0\u7d20\u3001\u6807\u5fd7\u57fa\u56e0\u548c\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u786e\u5b9a\u5e94\u4f18\u5148\u8003\u8651\u7684\u4e34\u5e8a\u4fe1\u606f\u5b50\u96c6\u4ee5\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002\u572815\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7279\u5f81\u91cd\u8981\u6027\u5ea6\u91cf\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\u548c\u66f4\u5f3a\u7684\u7a33\u5065\u6027\u3002\u65b9\u6cd5\u5b9e\u73b0\u5df2\u53d1\u5e03\u4e8eGitHub\uff1ahttps://github.com/ChristelSirocchi/rule-graph\u3002"}}
{"id": "2506.14318", "pdf": "https://arxiv.org/pdf/2506.14318", "abs": "https://arxiv.org/abs/2506.14318", "authors": ["Amirreza Fateh", "Yasin Rezvani", "Sara Moayedi", "Sadjad Rezvani", "Fatemeh Fateh", "Mansoor Fateh"], "title": "BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, largely due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a new curated MRI dataset designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians, spanning three major tumor types-glioma, meningioma, and pituitary-as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. Our method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. Importantly, this study serves primarily as an introduction to the dataset, establishing foundational benchmarks for future research. We envision this dataset as a valuable resource for advancing machine learning applications in neuro-oncology, supporting both academic research and clinical decision-support development. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u8111\u80bf\u7624\u5206\u5272\u4e0e\u5206\u7c7b\u6570\u636e\u96c6BRISC\uff0c\u5305\u542b6000\u4e2a\u6807\u6ce8MRI\u626b\u63cf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\uff0c\u53d6\u5f97\u4e8682.3%\u7684\u52a0\u6743\u5e73\u5747IoU\u3002", "motivation": "\u5f53\u524d\u8111\u80bf\u7624\u7684MRI\u5206\u5272\u4e0e\u5206\u7c7b\u9762\u4e34\u9ad8\u8d28\u91cf\u3001\u5e73\u8861\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b6000\u4e2a\u6807\u6ce8MRI\u626b\u63cf\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u79cd\u4e3b\u8981\u8111\u80bf\u7624\u7c7b\u578b\u53ca\u975e\u80bf\u7624\u75c5\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578bSwin-HAFNet\u3002", "result": "\u63d0\u51fa\u7684\u5206\u5272\u6a21\u578b\u5728\u52a0\u6743\u5e73\u5747IoU\u4e0a\u8fbe\u523082.3%\uff0c\u5728\u6240\u6709\u80bf\u7624\u7c7b\u522b\u4e2d\u5747\u8868\u73b0\u51fa\u6539\u8fdb\u3002", "conclusion": "BRISC\u6570\u636e\u96c6\u4e3a\u8111\u80bf\u7624\u5206\u5272\u4e0e\u5206\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u652f\u6301\u5b66\u672f\u7814\u7a76\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "paper_title_zh": "BRISC\uff1a\u57fa\u4e8eSwin-HAFNet\u7684\u8111\u80bf\u7624\u5206\u5272\u4e0e\u5206\u7c7b\u6807\u6ce8\u6570\u636e\u96c6", "abstract_zh": "\u8111\u80bf\u7624\u7684\u51c6\u786e\u5206\u5272\u4e0e\u5206\u7c7b\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5e73\u8861\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\u4e0e\u5206\u7c7b\u4efb\u52a1\u7684\u65b0MRI\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b6000\u4e2a\u7531\u8ba4\u8bc1\u653e\u5c04\u79d1\u533b\u751f\u548c\u533b\u5e08\u6807\u6ce8\u7684\u5bf9\u6bd4\u589e\u5f3aT1\u52a0\u6743MRI\u626b\u63cf\uff0c\u6db5\u76d6\u4e09\u79cd\u4e3b\u8981\u80bf\u7624\u7c7b\u578b\uff08\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u548c\u5782\u4f53\u7624\uff09\u4ee5\u53ca\u975e\u80bf\u7624\u75c5\u4f8b\u3002\u6bcf\u4e2a\u6837\u672c\u5747\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u6807\u7b7e\uff0c\u5e76\u6309\u8f74\u5411\u3001\u77e2\u72b6\u9762\u548c\u51a0\u72b6\u9762\u5206\u7c7b\uff0c\u4ee5\u652f\u6301\u7a33\u5065\u7684\u6a21\u578b\u5f00\u53d1\u548c\u8de8\u89c6\u56fe\u6cdb\u5316\u3002\u4e3a\u5c55\u793a\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\uff0c\u5e76\u4e0e\u73b0\u6709\u57fa\u7ebf\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u80bf\u7624\u7c7b\u522b\u4e2d\u5747\u53d6\u5f97\u6539\u8fdb\uff0c\u52a0\u6743\u5e73\u5747IoU\u8fbe\u523082.3%\u3002\u672c\u7814\u7a76\u4e3b\u8981\u4f5c\u4e3a\u6570\u636e\u96c6\u7684\u4ecb\u7ecd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002\u6211\u4eec\u671f\u671b\u8be5\u6570\u636e\u96c6\u80fd\u6210\u4e3a\u63a8\u52a8\u795e\u7ecf\u80bf\u7624\u5b66\u4e2d\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u652f\u6301\u5b66\u672f\u7814\u7a76\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u5f00\u53d1\u3002\u6570\u636e\u96c6\u94fe\u63a5\uff1ahttps://www.kaggle.com/datasets/briscdataset/brisc2025/"}}
{"id": "2506.13904", "pdf": "https://arxiv.org/pdf/2506.13904", "abs": "https://arxiv.org/abs/2506.13904", "authors": ["Ivania Donoso-Guzm\u00e1n", "Krist\u00fdna Sirka Kacaf\u00edrkov\u00e1", "Maxwell Szymanski", "An Jacobs", "Denis Parra", "Katrien Verbert"], "title": "A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.\n  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.\n  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.\n  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff082\u9879\u7528\u6237\u7814\u7a76\uff0c\u63d0\u51fa\u533b\u7597\u9886\u57df\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u7528\u6237\u4f53\u9a8c\u6846\u67b6\u548c\u8bc4\u4f30\u6307\u5357\uff0c\u586b\u8865\u4e86XAI\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8bc4\u4f30\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "motivation": "\u5c3d\u7ba1\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u5b9e\u9645\u4ef7\u503c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4ecd\u672a\u5145\u5206\u9a8c\u8bc1\u3002\u7f3a\u4e4f\u660e\u786e\u7684\u7528\u6237\u8bc4\u4f30\u6307\u5357\u5bfc\u81f4XAI\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u7528\u6027\u96be\u4ee5\u4fdd\u8bc1\uff0c\u5c24\u5176\u5728\u533b\u7597\u9886\u57df\u3002", "method": "\u7814\u7a76\u5bf9\u6765\u81ea\u4e94\u4e2a\u6570\u636e\u5e93\u768482\u9879\u533b\u7597\u9886\u57dfXAI\u7528\u6237\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u7f16\u7801\u65b9\u6848\u548c\u8fed\u4ee3\u5f00\u53d1\u7684\u5f52\u7eb3\u7f16\u7801\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u8d21\u732e\u5305\u62ec\uff1a\uff081\uff09\u603b\u7ed3\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\uff0c\u663e\u793a\u533b\u7597XAI\u4e2d\u4ee5\u4eba\u4e3a\u672c\u65b9\u6cd5\u7684\u589e\u957f\u8d8b\u52bf\uff1b\uff082\uff09\u63ed\u793a\u89e3\u91ca\u5c5e\u6027\u95f4\u7684\u5173\u8054\uff1b\uff083\uff09\u63d0\u51fa\u66f4\u65b0\u7684\u6846\u67b6\u548c\u5b9e\u7528\u6307\u5357\uff0c\u652f\u6301\u8de8\u5b66\u79d1\u56e2\u961f\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u7684XAI\u8bc4\u4f30\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86XAI\u5728\u533b\u7597\u9886\u57df\u7528\u6237\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u6846\u67b6\u548c\u6307\u5357\uff0c\u6709\u52a9\u4e8e\u63d0\u5347XAI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "paper_title_zh": "\u533b\u7597\u9886\u57df\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7684\u7528\u6237\u4e2d\u5fc3\u8bc4\u4f30\u7cfb\u7edf\u7efc\u8ff0", "abstract_zh": "\u5c3d\u7ba1\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5b9e\u9645\u4ef7\u503c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u548c\u9a8c\u8bc1\u3002\u7a33\u5065\u4e14\u60c5\u5883\u611f\u77e5\u7684\u8bc4\u4f30\u4e0d\u4ec5\u5bf9\u751f\u6210\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u81f3\u5173\u91cd\u8981\uff0c\u8fd8\u80fd\u786e\u4fdd\u5176\u5bf9\u76ee\u6807\u7528\u6237\u7684\u4fe1\u4efb\u5ea6\u548c\u53ef\u7528\u6027\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u660e\u786e\u7684\u7528\u6237\u8bc4\u4f30\u8bbe\u8ba1\u6307\u5357\uff0c\u8fd9\u4e00\u9886\u57df\u5e38\u88ab\u5ffd\u89c6\u3002\u672c\u7814\u7a76\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u76ee\u6807\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff1a\uff081\uff09\u5f00\u53d1\u4e00\u4e2a\u5b9a\u4e49\u660e\u786e\u7684\u539f\u5b50\u5c5e\u6027\u6846\u67b6\uff0c\u4ee5\u63cf\u8ff0\u533b\u7597\u9886\u57dfXAI\u7684\u7528\u6237\u4f53\u9a8c\uff1b\uff082\uff09\u63d0\u4f9b\u6e05\u6670\u3001\u60c5\u5883\u654f\u611f\u7684\u6307\u5357\uff0c\u57fa\u4e8e\u7cfb\u7edf\u7279\u6027\u5b9a\u4e49\u8bc4\u4f30\u7b56\u7565\u3002\u7814\u7a76\u5bf9\u6765\u81ea\u4e94\u4e2a\u6570\u636e\u5e93\u768482\u9879\u533b\u7597\u9886\u57dfXAI\u7528\u6237\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5206\u6790\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u7f16\u7801\u65b9\u6848\u548c\u8fed\u4ee3\u5f00\u53d1\u7684\u5f52\u7eb3\u7f16\u7801\u3002\u7efc\u8ff0\u5f97\u51fa\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a\uff081\uff09\u603b\u7ed3\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\uff0c\u7a81\u663e\u533b\u7597XAI\u4e2d\u4ee5\u4eba\u4e3a\u672c\u65b9\u6cd5\u7684\u589e\u957f\u8d8b\u52bf\uff1b\uff082\uff09\u63ed\u793a\u89e3\u91ca\u5c5e\u6027\u95f4\u7684\u5173\u8054\uff1b\uff083\uff09\u63d0\u51fa\u66f4\u65b0\u7684\u6846\u67b6\u548c\u5b9e\u7528\u6307\u5357\uff0c\u652f\u6301\u8de8\u5b66\u79d1\u56e2\u961f\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u573a\u666f\u7684XAI\u7cfb\u7edf\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2506.14381", "pdf": "https://arxiv.org/pdf/2506.14381", "abs": "https://arxiv.org/abs/2506.14381", "authors": ["Yuxuan Jiang", "Siyue Teng", "Qiang Zhu", "Chen Feng", "Chengxi Zeng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull"], "title": "Compressed Video Super-Resolution based on Hierarchical Encoding", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVSR-HE\u7684\u901a\u7528\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4e13\u4e3a\u63d0\u5347\u538b\u7f29\u5185\u5bb9\u7684\u611f\u77e5\u8d28\u91cf\u800c\u8bbe\u8ba1\u3002\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6d88\u9664H.265/HEVC\u7f16\u7801\u5f15\u5165\u7684\u538b\u7f29\u4f2a\u5f71\uff0c\u5e76\u5728\u591a\u79cd\u91cf\u5316\u53c2\u6570\u4e0b\u6062\u590d\u7cbe\u7ec6\u7ec6\u8282\u3002", "motivation": "\u9488\u5bf9\u9ad8\u538b\u7f29\u573a\u666f\u4e0b\u7684\u89c6\u9891\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\uff08\u5982\u4ece180p\u5230720p\uff09\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u540c\u65f6\u6d88\u9664\u538b\u7f29\u4f2a\u5f71\uff0c\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "method": "VSR-HE\u91c7\u7528\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\uff0c\u901a\u8fc7\u7cbe\u5fc3\u4f18\u5316\u7684\u6a21\u578b\u5904\u7406H.265/HEVC\u7f16\u7801\u5f15\u5165\u7684\u538b\u7f29\u4f2a\u5f71\uff0c\u5e76\u5728\u591a\u79cd\u91cf\u5316\u53c2\u6570\u4e0b\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVSR-HE\u80fd\u591f\u6709\u6548\u6062\u590d\u538b\u7f29\u89c6\u9891\u7684\u7cbe\u7ec6\u7ec6\u8282\uff0c\u5e76\u5728\u591a\u79cd\u538b\u7f29\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002\u8be5\u65b9\u6cd5\u5df2\u63d0\u4ea4\u81f3ICME 2025\u7684VSR\u6311\u6218\u8d5b\u3002", "conclusion": "VSR-HE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u538b\u7f29\u573a\u666f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u5e76\u6d88\u9664\u538b\u7f29\u4f2a\u5f71\u3002", "paper_title_zh": "\u57fa\u4e8e\u5206\u5c42\u7f16\u7801\u7684\u538b\u7f29\u89c6\u9891\u8d85\u5206\u8fa8\u7387", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5VSR-HE\uff0c\u4e13\u95e8\u7528\u4e8e\u63d0\u5347\u538b\u7f29\u5185\u5bb9\u7684\u611f\u77e5\u8d28\u91cf\u3002\u9488\u5bf9\u9ad8\u538b\u7f29\u573a\u666f\uff0c\u8be5\u65b9\u6cd5\u5c06\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\uff08\u5982\u4ece180p\u5230720p\u6216\u4ece270p\u52301080p\uff09\u8fdb\u884c\u56db\u500d\u653e\u5927\u3002VSR-HE\u91c7\u7528\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\uff0c\u5e76\u7ecf\u8fc7\u7cbe\u5fc3\u4f18\u5316\uff0c\u4ee5\u6d88\u9664H.265/HEVC\u7f16\u7801\u5728\u4e0d\u540c\u91cf\u5316\u53c2\u6570\uff08QP\uff09\u6c34\u5e73\u4e0b\u5f15\u5165\u7684\u591a\u79cd\u538b\u7f29\u4f2a\u5f71\u3002\u4e3a\u786e\u4fdd\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u538b\u7f29\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u80fd\u591f\u6709\u6548\u6062\u590d\u7cbe\u7ec6\u7ec6\u8282\u5e76\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002VSR-HE\u5df2\u6b63\u5f0f\u63d0\u4ea4\u81f3ICME 2025\u89c6\u9891\u4f1a\u8bae\u8d85\u5206\u8fa8\u7387\u6311\u6218\u8d5b\uff08Team BVI-VSR\uff09\uff0c\u53c2\u4e0eTrack 1\uff08\u901a\u7528\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5185\u5bb9\uff09\u548cTrack 2\uff08\u5934\u90e8\u89c6\u9891\uff09\u7684\u7ade\u8d5b\u3002"}}
{"id": "2506.13909", "pdf": "https://arxiv.org/pdf/2506.13909", "abs": "https://arxiv.org/abs/2506.13909", "authors": ["Xinyuan Tu", "Haocheng Zhang", "Tao Chengxu", "Zuyi Chen"], "title": "Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \\emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \\textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.\n  Two FSL paradigms are investigated: the metric-based \\emph{Prototypical Network} and the gradient-based \\emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \\emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \\textbf{0.944 weighted F1} in the multi-class regime and \\textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\\% F1 over traditional class-based sampling.\n  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u87ba\u4e1d\u7d27\u56fa\u8fc7\u7a0b\u76d1\u63a7\u7684\u6848\u4f8b\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u5728\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u7b7e\u611f\u77e5\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7CNN\u7ed3\u5408\u5ea6\u91cf\u5b66\u4e60\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u4f18\u4e8e\u5927\u578b\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u5728\u89c6\u89c9\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5c11\u6837\u672c\u5b66\u4e60\u5728\u87ba\u4e1d\u7d27\u56fa\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u5305\u542b2300\u4e2a\u6837\u672c\u7684\u591a\u53d8\u91cf\u626d\u77e9\u6570\u636e\u96c6\uff0c\u8986\u76d616\u79cd\u5355\u56e0\u7d20\u548c\u591a\u56e0\u7d20\u7f3a\u9677\u7c7b\u578b\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u7b7e\u611f\u77e5\u7684\u7247\u6bb5\u91c7\u6837\u5668\uff0c\u5c06\u591a\u6807\u7b7e\u5e8f\u5217\u5206\u89e3\u4e3a\u591a\u4e2a\u5355\u6807\u7b7e\u4efb\u52a1\u3002\u6bd4\u8f83\u4e86\u4e24\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u8303\u5f0f\uff1a\u57fa\u4e8e\u5ea6\u91cf\u7684\u539f\u578b\u7f51\u7edc\u548c\u57fa\u4e8e\u68af\u5ea6\u7684MAML\uff0c\u5206\u522b\u4e0e1D CNN\u3001InceptionTime\u548c341M\u53c2\u6570\u7684Moment Transformer\u7ed3\u5408\u3002", "result": "\u572810-shot\u30013-way\u8bc4\u4f30\u4e2d\uff0cInceptionTime\u4e0e\u539f\u578b\u7f51\u7edc\u7ec4\u5408\u5728\u591a\u7c7b\u548c\u591a\u6807\u7b7e\u573a\u666f\u4e0b\u5206\u522b\u8fbe\u52300.944\u548c0.935\u7684\u52a0\u6743F1\u5206\u6570\uff0c\u4f18\u4e8e\u5fae\u8c03\u7684Moment Transformer\uff0c\u4e14\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002\u5ea6\u91cf\u5b66\u4e60\u5728\u6240\u6709\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5747\u4f18\u4e8eMAML\uff0c\u6807\u7b7e\u611f\u77e5\u91c7\u6837\u6bd4\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u63d0\u53471.7%\u7684F1\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u8f7b\u91cf\u7ea7CNN\u7ed3\u5408\u7b80\u5355\u5ea6\u91cf\u5b66\u4e60\u4e0d\u4ec5\u6536\u655b\u66f4\u5feb\uff0c\u6cdb\u5316\u80fd\u529b\u4e5f\u66f4\u5f3a\uff0c\u6311\u6218\u4e86\u5927\u578b\u57fa\u7840\u6a21\u578b\u603b\u662f\u66f4\u4f18\u7684\u5047\u8bbe\u3002\u7814\u7a76\u5f00\u6e90\u4e86\u4ee3\u7801\u3001\u6570\u636e\u5206\u5272\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u3002", "paper_title_zh": "\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u7684\u5c11\u6837\u672c\u5b66\u4e60\uff1a\u4ee5\u87ba\u4e1d\u7d27\u56fa\u8fc7\u7a0b\u76d1\u63a7\u4e3a\u4f8b\u7684\u6bd4\u8f83\u5206\u6790", "abstract_zh": "\u5c11\u6837\u672c\u5b66\u4e60\uff08FSL\uff09\u5728\u89c6\u89c9\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u6807\u6ce8\u65b0\u7f3a\u9677\u7684\u6210\u672c\u6781\u9ad8\u3002\u672c\u6587\u4ee5\u87ba\u4e1d\u7d27\u56fa\u8fc7\u7a0b\u76d1\u63a7\u4e3a\u4f8b\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86FSL\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b2300\u4e2a\u6837\u672c\u7684\u591a\u53d8\u91cf\u626d\u77e9\u6570\u636e\u96c6\uff0c\u8986\u76d616\u79cd\u5355\u56e0\u7d20\u548c\u591a\u56e0\u7d20\u7f3a\u9677\u7c7b\u578b\u3002\u9664\u57fa\u51c6\u6d4b\u8bd5\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u7b7e\u611f\u77e5\u7684\u7247\u6bb5\u91c7\u6837\u5668\uff0c\u5c06\u591a\u6807\u7b7e\u5e8f\u5217\u5206\u89e3\u4e3a\u591a\u4e2a\u5355\u6807\u7b7e\u4efb\u52a1\uff0c\u4fdd\u6301\u8f93\u51fa\u7ef4\u5ea6\u56fa\u5b9a\u540c\u65f6\u4fdd\u7559\u7ec4\u5408\u6807\u7b7e\u4fe1\u606f\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cdFSL\u8303\u5f0f\uff1a\u57fa\u4e8e\u5ea6\u91cf\u7684\u539f\u578b\u7f51\u7edc\u548c\u57fa\u4e8e\u68af\u5ea6\u7684MAML\uff0c\u5206\u522b\u4e0e1D CNN\u3001InceptionTime\u548c341M\u53c2\u6570\u7684Moment Transformer\u7ed3\u5408\u3002\u572810-shot\u30013-way\u8bc4\u4f30\u4e2d\uff0cInceptionTime\u4e0e\u539f\u578b\u7f51\u7edc\u7ec4\u5408\u5728\u591a\u7c7b\u548c\u591a\u6807\u7b7e\u573a\u666f\u4e0b\u5206\u522b\u8fbe\u52300.944\u548c0.935\u7684\u52a0\u6743F1\u5206\u6570\uff0c\u4f18\u4e8e\u5fae\u8c03\u7684Moment Transformer\uff0c\u4e14\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002\u5ea6\u91cf\u5b66\u4e60\u5728\u6240\u6709\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5747\u4f18\u4e8eMAML\uff0c\u6807\u7b7e\u611f\u77e5\u91c7\u6837\u6bd4\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u63d0\u53471.7%\u7684F1\u5206\u6570\u3002\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u5927\u578b\u57fa\u7840\u6a21\u578b\u603b\u662f\u66f4\u4f18\u7684\u5047\u8bbe\uff1a\u5728\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u8f7b\u91cf\u7ea7CNN\u7ed3\u5408\u7b80\u5355\u5ea6\u91cf\u5b66\u4e60\u4e0d\u4ec5\u6536\u655b\u66f4\u5feb\uff0c\u6cdb\u5316\u80fd\u529b\u4e5f\u66f4\u5f3a\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u4ee3\u7801\u3001\u6570\u636e\u5206\u5272\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2506.14390", "pdf": "https://arxiv.org/pdf/2506.14390", "abs": "https://arxiv.org/abs/2506.14390", "authors": ["Conrad Orglmeister", "Erik Bochinski", "Volker Eiselein", "Elvira Fleig"], "title": "Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in Computer Safety, Reliability and Security - SAFECOMP 2024 Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online at https://doi.org/10.1007/978-3-031-68738-9_29", "summary": "Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u539f\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u53ef\u89e3\u91ca\u6027\u79bb\u7fa4\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u7d27\u51d1\u7684\u5206\u5e03\u5185\u533a\u57df\u5e76\u5f15\u5165\u9650\u5236\u635f\u5931\uff0c\u63d0\u5347\u4e86\u79bb\u7fa4\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u51b3\u7b56\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u539f\u578b\u53d8\u5206\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u7684\u79bb\u7fa4\u68c0\u6d4b\u80fd\u529b\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u79bb\u7fa4\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7528\u4e8e\u57fa\u4e8e\u8ddd\u79bb\u7684\u5206\u7c7b\u3001\u4f3c\u7136\u4f30\u8ba1\u548c\u91cd\u5efa\u3002\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u5b9a\u4e49\u5206\u5e03\u5185\u533a\u57df\uff0c\u5e76\u5f15\u5165\u9650\u5236\u635f\u5931\u4ee5\u4fdd\u6301\u6f5c\u5728\u7a7a\u95f4\u7684\u7d27\u51d1\u6027\u3002\u81ea\u7f16\u7801\u5668\u7684\u91cd\u5efa\u80fd\u529b\u589e\u5f3a\u4e86\u539f\u578b\u548c\u5206\u7c7b\u5668\u5206\u5e03\u5185\u533a\u57df\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u5e38\u89c1\u79bb\u7fa4\u68c0\u6d4b\u57fa\u51c6\u548c\u5b9e\u9645\u94c1\u8def\u5e94\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79bb\u7fa4\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u79bb\u7fa4\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u539f\u578b\u548c\u91cd\u5efa\u80fd\u529b\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5b89\u5168\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002", "paper_title_zh": "\u57fa\u4e8e\u539f\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u53ef\u89e3\u91ca\u79bb\u7fa4\u68c0\u6d4b\u65b9\u6cd5", "abstract_zh": "\u7406\u89e3\u6df1\u5ea6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u5e76\u4fe1\u4efb\u5176\u53ef\u9760\u6027\u5bf9\u4e8e\u5c06\u5176\u5e94\u7528\u4e8e\u5b89\u5168\u76f8\u5173\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u6269\u5c55\u4e86\u81ea\u89e3\u91ca\u7684\u539f\u578b\u53d8\u5206\u6a21\u578b\uff0c\u7ed3\u5408\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u79bb\u7fa4\u68c0\u6d4b\u65b9\u6cd5\uff1a\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7528\u4e8e\u57fa\u4e8e\u8ddd\u79bb\u7684\u5206\u7c7b\u3001\u79bb\u7fa4\u68c0\u6d4b\u7684\u4f3c\u7136\u4f30\u8ba1\u4ee5\u53ca\u91cd\u5efa\u3002\u5206\u5e03\u5185\u533a\u57df\u7531\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u5b9a\u4e49\uff0c\u5176\u539f\u578b\u4ee3\u8868\u6bcf\u4e2a\u6a21\u5f0f\u7684\u4e2d\u5fc3\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u9650\u5236\u635f\u5931\uff0c\u4fc3\u8fdb\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5206\u5e03\u5185\u533a\u57df\u7684\u7d27\u51d1\u6027\u800c\u4e0d\u4f7f\u5176\u574d\u7f29\u4e3a\u5355\u70b9\u3002\u81ea\u7f16\u7801\u5668\u7684\u91cd\u5efa\u80fd\u529b\u786e\u4fdd\u4e86\u539f\u578b\u548c\u5206\u7c7b\u5668\u5206\u5e03\u5185\u533a\u57df\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fdb\u4e00\u6b65\u5e2e\u52a9\u533a\u5206\u79bb\u7fa4\u6837\u672c\u3002\u5728\u5e38\u89c1\u79bb\u7fa4\u68c0\u6d4b\u57fa\u51c6\u548c\u5b9e\u9645\u94c1\u8def\u5e94\u7528\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.14432", "pdf": "https://arxiv.org/pdf/2506.14432", "abs": "https://arxiv.org/abs/2506.14432", "authors": ["Asbj\u00f8rn Munk", "Stefano Cerri", "Jakob Ambsdorf", "Julia Machnio", "Sebastian N\u00f8rgaard Llambias", "Vardan Nersesjan", "Christian Hedeager Krag", "Peirong Liu", "Pablo Rocamora Garc\u00eda", "Mostafa Mehdipour Ghazi", "Mikael Boesen", "Michael Eriksen Benros", "Juan Eugenio Iglesias", "Mads Nielsen"], "title": "A large-scale heterogeneous 3D magnetic resonance brain imaging dataset for self-supervised learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present FOMO60K, a large-scale, heterogeneous dataset of 60,529 brain Magnetic Resonance Imaging (MRI) scans from 13,900 sessions and 11,187 subjects, aggregated from 16 publicly available sources. The dataset includes both clinical- and research-grade images, multiple MRI sequences, and a wide range of anatomical and pathological variability, including scans with large brain anomalies. Minimal preprocessing was applied to preserve the original image characteristics while reducing barriers to entry for new users. Accompanying code for self-supervised pretraining and finetuning is provided. FOMO60K is intended to support the development and benchmarking of self-supervised learning methods in medical imaging at scale.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86FOMO60K\uff0c\u4e00\u4e2a\u5305\u542b60,529\u4e2a\u8111\u90e8MRI\u626b\u63cf\u7684\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u533b\u5b66\u5f71\u50cf\u4e2d\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u3002FOMO60K\u7684\u63d0\u51fa\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u4e34\u5e8a\u548c\u7814\u7a76\u7ea7\u56fe\u50cf\u8d44\u6e90\u3002", "method": "\u6570\u636e\u96c6FOMO60K\u6574\u5408\u4e8616\u4e2a\u516c\u5f00\u6765\u6e90\u768460,529\u4e2a\u8111\u90e8MRI\u626b\u63cf\uff0c\u6db5\u76d6\u591a\u79cdMRI\u5e8f\u5217\u548c\u5e7f\u6cdb\u7684\u89e3\u5256\u53ca\u75c5\u7406\u53d8\u5f02\u3002\u4ec5\u8fdb\u884c\u6700\u5c0f\u9884\u5904\u7406\u4ee5\u4fdd\u7559\u539f\u59cb\u56fe\u50cf\u7279\u5f81\u3002", "result": "FOMO60K\u5305\u542b13,900\u6b21\u626b\u63cf\u4f1a\u8bdd\u548c11,187\u540d\u53d7\u8bd5\u8005\u7684\u6570\u636e\uff0c\u652f\u6301\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u5e76\u9644\u5e26\u76f8\u5173\u4ee3\u7801\u3002", "conclusion": "FOMO60K\u4e3a\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "paper_title_zh": "\u7528\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u5927\u89c4\u6a21\u5f02\u67843D\u78c1\u5171\u632f\u8111\u6210\u50cf\u6570\u636e\u96c6", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86FOMO60K\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b60,529\u4e2a\u8111\u90e8\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u626b\u63cf\u7684\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u96c6\uff0c\u6765\u81ea13,900\u6b21\u626b\u63cf\u4f1a\u8bdd\u548c11,187\u540d\u53d7\u8bd5\u8005\uff0c\u6574\u5408\u4e8616\u4e2a\u516c\u5f00\u6765\u6e90\u7684\u6570\u636e\u3002\u6570\u636e\u96c6\u5305\u542b\u4e34\u5e8a\u548c\u7814\u7a76\u7ea7\u56fe\u50cf\u3001\u591a\u79cdMRI\u5e8f\u5217\uff0c\u4ee5\u53ca\u5e7f\u6cdb\u7684\u89e3\u5256\u548c\u75c5\u7406\u53d8\u5f02\uff0c\u5305\u62ec\u5177\u6709\u663e\u8457\u8111\u90e8\u5f02\u5e38\u7684\u626b\u63cf\u3002\u4ec5\u8fdb\u884c\u4e86\u6700\u5c0f\u9884\u5904\u7406\u4ee5\u4fdd\u7559\u539f\u59cb\u56fe\u50cf\u7279\u5f81\uff0c\u540c\u65f6\u964d\u4f4e\u65b0\u7528\u6237\u7684\u4f7f\u7528\u95e8\u69db\u3002\u63d0\u4f9b\u4e86\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u914d\u5957\u4ee3\u7801\u3002FOMO60K\u65e8\u5728\u652f\u6301\u5927\u89c4\u6a21\u533b\u5b66\u5f71\u50cf\u4e2d\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2506.13911", "pdf": "https://arxiv.org/pdf/2506.13911", "abs": "https://arxiv.org/abs/2506.13911", "authors": ["Arie Soeteman", "Balder ten Cate"], "title": "Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "Submitted to NeurIPS 2025, 28 pages, 5 figures", "summary": "We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHEGNNs\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u8282\u70b9\u4e2a\u6027\u5316\u589e\u5f3a\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u533a\u5206\u540c\u6784\u56fe\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edfGNN\u67b6\u6784\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u533a\u5206\u540c\u6784\u56fe\u65f6\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6765\u63d0\u5347\u5176\u533a\u5206\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51faHEGNNs\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u8282\u70b9\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b50\u56fe\u9650\u5236\u4e0e\u975e\u9650\u5236\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u903b\u8f91\u523b\u753b\u5176\u8282\u70b9\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHEGNNs\u5728\u533a\u5206\u540c\u6784\u56fe\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfGNN\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "conclusion": "HEGNNs\u901a\u8fc7\u5206\u5c42\u8282\u70b9\u4e2a\u6027\u5316\u663e\u8457\u63d0\u5347\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u56fe\u540c\u6784\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5c42\u8282\u70b9\u4e2a\u6027\u5316\u4e0b\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u5206\u5c42\u81ea\u6211\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HEGNNs\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u56fe\u540c\u6784\u6d4b\u8bd5\u4e2d\u4e2a\u6027\u5316-\u7ec6\u5316\u8303\u5f0f\u542f\u53d1\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u8868\u8fbe\u6027\u6269\u5c55\u3002HEGNNs\u63a8\u5e7f\u4e86\u5b50\u56fe-GNNs\uff0c\u5e76\u5f62\u6210\u4e86\u4e00\u7cfb\u5217\u8868\u8fbe\u80fd\u529b\u9010\u6e10\u589e\u5f3a\u7684\u6a21\u578b\uff0c\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u80fd\u591f\u533a\u5206\u540c\u6784\u56fe\u3002\u6211\u4eec\u4f7f\u7528\u5206\u7ea7\u6df7\u5408\u903b\u8f91\u5bf9HEGNN\u8282\u70b9\u5206\u7c7b\u5668\uff08\u5e26\u6216\u4e0d\u5e26\u5b50\u56fe\u9650\u5236\uff09\u8fdb\u884c\u4e86\u903b\u8f91\u523b\u753b\u3002\u8fd9\u4e00\u523b\u753b\u4f7f\u6211\u4eec\u80fd\u591f\u5c06HEGNNs\u7684\u533a\u5206\u80fd\u529b\u4e0e\u9ad8\u9636GNNs\u3001\u5e26\u6709\u5c40\u90e8\u540c\u6001\u8ba1\u6570\u7279\u5f81\u7684GNNs\u4ee5\u53ca\u57fa\u4e8e\u4e2a\u6027\u5316-\u7ec6\u5316\u7b97\u6cd5\u7684\u989c\u8272\u7ec6\u5316\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86HEGNNs\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5e76\u663e\u793a\u4e86\u5176\u4e0e\u4f20\u7edfGNN\u67b6\u6784\u76f8\u6bd4\u7684\u4f18\u52bf\uff0c\u65e0\u8bba\u662f\u5426\u5e26\u6709\u5c40\u90e8\u540c\u6001\u8ba1\u6570\u7279\u5f81\u3002"}}
{"id": "2506.14497", "pdf": "https://arxiv.org/pdf/2506.14497", "abs": "https://arxiv.org/abs/2506.14497", "authors": ["Franco Matzkin", "Agostina Larrazabal", "Diego H Milone", "Jose Dolz", "Enzo Ferrante"], "title": "Towards Reliable WMH Segmentation under Domain Shift: An Application Study using Maximum Entropy Regularization to Improve Uncertainty Estimation", "categories": ["eess.IV", "cs.CV"], "comment": "32 pages, 7 figures", "summary": "Accurate segmentation of white matter hyperintensities (WMH) is crucial for clinical decision-making, particularly in the context of multiple sclerosis. However, domain shifts, such as variations in MRI machine types or acquisition parameters, pose significant challenges to model calibration and uncertainty estimation. This study investigates the impact of domain shift on WMH segmentation by proposing maximum-entropy regularization techniques to enhance model calibration and uncertainty estimation, with the purpose of identifying errors post-deployment using predictive uncertainty as a proxy measure that does not require ground-truth labels. To do this, we conducted experiments using a U-Net architecture to evaluate these regularization schemes on two publicly available datasets, assessing performance with the Dice coefficient, expected calibration error, and entropy-based uncertainty estimates. Our results show that entropy-based uncertainty estimates can anticipate segmentation errors, and that maximum-entropy regularization further strengthens the correlation between uncertainty and segmentation performance while also improving model calibration under domain shift.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u767d\u8d28\u9ad8\u4fe1\u53f7\uff08WMH\uff09\u5206\u5272\u5728\u57df\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\u4ee5\u63d0\u5347\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9884\u6d4b\u5206\u5272\u9519\u8bef\u5e76\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u767d\u8d28\u9ad8\u4fe1\u53f7\uff08WMH\uff09\u5206\u5272\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46MRI\u8bbe\u5907\u6216\u53c2\u6570\u5dee\u5f02\u5bfc\u81f4\u7684\u57df\u504f\u79fb\u4f1a\u5f71\u54cd\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u8bc6\u522b\u5206\u5272\u9519\u8bef\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\uff0c\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u7528Dice\u7cfb\u6570\u3001\u9884\u671f\u6821\u51c6\u8bef\u5dee\u548c\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4f5c\u4e3a\u6307\u6807\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u9884\u6d4b\u5206\u5272\u9519\u8bef\uff0c\u6700\u5927\u71b5\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u4e0d\u786e\u5b9a\u6027\u4e0e\u5206\u5272\u6027\u80fd\u7684\u5173\u8054\uff0c\u5e76\u6539\u5584\u4e86\u57df\u504f\u79fb\u4e0b\u7684\u6a21\u578b\u6821\u51c6\u3002", "conclusion": "\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347WMH\u5206\u5272\u5728\u57df\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002", "paper_title_zh": "\u9762\u5411\u57df\u504f\u79fb\u4e0b\u53ef\u9760\u7684\u767d\u8d28\u9ad8\u4fe1\u53f7\u5206\u5272\uff1a\u57fa\u4e8e\u6700\u5927\u71b5\u6b63\u5219\u5316\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5e94\u7528\u7814\u7a76", "abstract_zh": "\u767d\u8d28\u9ad8\u4fe1\u53f7\uff08WMH\uff09\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u591a\u53d1\u6027\u786c\u5316\u75c7\u80cc\u666f\u4e0b\u3002\u7136\u800c\uff0cMRI\u8bbe\u5907\u7c7b\u578b\u6216\u91c7\u96c6\u53c2\u6570\u7684\u5dee\u5f02\u7b49\u57df\u504f\u79fb\u5bf9\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\u6765\u589e\u5f3a\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63a2\u8ba8\u4e86\u57df\u504f\u79fb\u5bf9WMH\u5206\u5272\u7684\u5f71\u54cd\uff0c\u76ee\u7684\u662f\u5229\u7528\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u7684\u4ee3\u7406\u6307\u6807\u6765\u8bc6\u522b\u90e8\u7f72\u540e\u7684\u9519\u8bef\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4f7f\u7528U-Net\u67b6\u6784\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u8fd9\u4e9b\u6b63\u5219\u5316\u65b9\u6848\uff0c\u5e76\u901a\u8fc7Dice\u7cfb\u6570\u3001\u9884\u671f\u6821\u51c6\u8bef\u5dee\u548c\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u8bc4\u4f30\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u4ee5\u9884\u6d4b\u5206\u5272\u9519\u8bef\uff0c\u800c\u6700\u5927\u71b5\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u4e0d\u786e\u5b9a\u6027\u4e0e\u5206\u5272\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u540c\u65f6\u6539\u5584\u4e86\u57df\u504f\u79fb\u4e0b\u7684\u6a21\u578b\u6821\u51c6\u3002"}}
{"id": "2506.14513", "pdf": "https://arxiv.org/pdf/2506.14513", "abs": "https://arxiv.org/abs/2506.14513", "authors": ["Farha Abdul Wasay", "Mohammed Abdul Rahman", "Hania Ghouse"], "title": "GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.", "AI": {"tldr": "GAMORA\u662f\u4e00\u79cd\u57fa\u4e8eVR\u624b\u52bf\u63a7\u5236\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u7684\u5371\u9669\u6750\u6599\u5904\u7406\uff0c\u901a\u8fc7\u5b9e\u65f6\u6c89\u6d78\u5f0f\u63a7\u5236\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740\u751f\u7269\u5371\u5bb3\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u51cf\u5c11\u4eba\u7c7b\u76f4\u63a5\u63a5\u89e6\u540c\u65f6\u4fdd\u6301\u64cd\u4f5c\u7cbe\u5ea6\u6210\u4e3a\u5173\u952e\u9700\u6c42\u3002\u4f20\u7edf\u811a\u672c\u81ea\u52a8\u5316\u6216\u8fdc\u7a0b\u64cd\u4f5c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u578b\u7684VR\u5f15\u5bfc\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "GAMORA\u7ed3\u5408Oculus Quest 2\u3001NVIDIA Jetson Nano\u548cROS\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6c89\u6d78\u5f0f\u63a7\u5236\u3001\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u548c\u9006\u8fd0\u52a8\u5b66\u9a71\u52a8\u7684\u673a\u68b0\u81c2\u64cd\u4f5c\u3002\u7cfb\u7edf\u901a\u8fc7Unity\u6784\u5efa3D\u73af\u5883\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u548c\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\uff0c\u5e76\u96c6\u6210YOLOv8\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u3002", "result": "GAMORA\u572850\u6b21\u8bd5\u9a8c\u4e2d\u5e73\u5747\u4f4d\u7f6e\u504f\u5dee\u4e3a2.2\u6beb\u7c73\uff08\u6539\u8fdb\u81ea4\u6beb\u7c73\uff09\uff0c\u79fb\u6db2\u7cbe\u5ea6\u57280.2\u6beb\u5347\u5185\uff0c\u91cd\u590d\u6027\u4e3a1.2\u6beb\u7c73\u3002\u7cfb\u7edf\u80fd\u6548\u63d0\u534750%\uff0c\u5e76\u901a\u8fc7\u6570\u5b57-\u7269\u7406\u53cd\u9988\u5faa\u73af\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u3002", "conclusion": "GAMORA\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6c89\u6d78\u5f0f\u7684\u673a\u5668\u4eba\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b89\u5168\u3001\u7cbe\u786e\u4e14\u53ef\u91cd\u590d\u5730\u5b8c\u6210\u9ad8\u98ce\u9669\u5b9e\u9a8c\u5ba4\u4efb\u52a1\u3002", "paper_title_zh": "GAMORA\uff1a\u4e00\u79cd\u7528\u4e8e\u5bc6\u95ed\u73af\u5883\u4e2d\u5371\u9669\u6750\u6599\u5904\u7406\u7684\u624b\u52bf\u9a71\u52a8\u5143\u64cd\u4f5c\u673a\u5668\u4eba\u81c2", "abstract_zh": "\u673a\u5668\u4eba\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u7684\u878d\u5408\u4e3a\u9ad8\u98ce\u9669\u5b9e\u9a8c\u5ba4\uff08\u5c24\u5176\u662f\u75c5\u6bd2\u5b66\u5b9e\u9a8c\u5ba4\uff09\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u968f\u7740\u751f\u7269\u5371\u5bb3\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u51cf\u5c11\u4eba\u7c7b\u76f4\u63a5\u63a5\u89e6\u540c\u65f6\u4fdd\u6301\u64cd\u4f5c\u7cbe\u5ea6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51faGAMORA\uff08\u624b\u52bf\u9a71\u52a8\u5143\u64cd\u4f5c\u673a\u5668\u4eba\u81c2\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578bVR\u5f15\u5bfc\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u53ef\u901a\u8fc7\u81ea\u7136\u624b\u52bf\u8fdc\u7a0b\u6267\u884c\u5371\u9669\u4efb\u52a1\u3002\u4e0e\u73b0\u6709\u7684\u811a\u672c\u81ea\u52a8\u5316\u6216\u4f20\u7edf\u8fdc\u7a0b\u64cd\u4f5c\u4e0d\u540c\uff0cGAMORA\u96c6\u6210\u4e86Oculus Quest 2\u3001NVIDIA Jetson Nano\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff08ROS\uff09\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6c89\u6d78\u5f0f\u63a7\u5236\u3001\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u548c\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u673a\u68b0\u81c2\u64cd\u4f5c\u3002\u7cfb\u7edf\u652f\u6301VR\u57f9\u8bad\u548c\u6a21\u62df\uff0c\u540c\u65f6\u901a\u8fc73D\u6253\u5370\u673a\u68b0\u81c2\u5728\u7269\u7406\u73af\u5883\u4e2d\u6267\u884c\u7cbe\u786e\u4efb\u52a1\u3002\u9006\u8fd0\u52a8\u5b66\u786e\u4fdd\u4e86\u6807\u672c\u5904\u7406\u548c\u79fb\u6db2\u7b49\u7cbe\u7ec6\u64cd\u4f5c\u7684\u51c6\u786e\u6027\u3002\u6d41\u7a0b\u5305\u62ec\u57fa\u4e8eUnity\u76843D\u73af\u5883\u6784\u5efa\u3001\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u548c\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u3002GAMORA\u572850\u6b21\u8bd5\u9a8c\u4e2d\u5e73\u5747\u4f4d\u7f6e\u504f\u5dee\u4e3a2.2\u6beb\u7c73\uff08\u6539\u8fdb\u81ea4\u6beb\u7c73\uff09\uff0c\u79fb\u6db2\u7cbe\u5ea6\u57280.2\u6beb\u5347\u5185\uff0c\u91cd\u590d\u6027\u4e3a1.2\u6beb\u7c73\u3002\u901a\u8fc7YOLOv8\u96c6\u6210\u7684\u7269\u4f53\u68c0\u6d4b\u589e\u5f3a\u4e86\u7a7a\u95f4\u611f\u77e5\uff0c\u800c\u80fd\u6548\u63d0\u534750%\u786e\u4fdd\u4e86\u53ef\u6301\u7eed\u90e8\u7f72\u3002\u7cfb\u7edf\u7684\u6570\u5b57-\u7269\u7406\u53cd\u9988\u5faa\u73af\u5b9e\u73b0\u4e86\u9ad8\u98ce\u9669\u5b9e\u9a8c\u5ba4\u4efb\u52a1\u7684\u5b89\u5168\u3001\u7cbe\u786e\u548c\u53ef\u91cd\u590d\u81ea\u52a8\u5316\u3002GAMORA\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6c89\u6d78\u5f0f\u7684\u673a\u5668\u4eba\u63a7\u5236\u548c\u751f\u7269\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14515", "pdf": "https://arxiv.org/pdf/2506.14515", "abs": "https://arxiv.org/abs/2506.14515", "authors": ["Prabhav Sanga", "Jaskaran Singh", "Arun K. Dubey"], "title": "Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at ICML MUGen'25", "summary": "As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \\textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAMR\u7684\u9ad8\u6548\u540e\u9a8c\u9057\u5fd8\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6df1\u5ea6\u56fe\u50cf\u5206\u7c7b\u5668\u4e2d\u7cbe\u786e\u79fb\u9664\u7279\u5b9a\u8bad\u7ec3\u6837\u672c\u3001\u8bed\u4e49\u7c7b\u522b\u6216\u89c6\u89c9\u98ce\u683c\u7684\u5f71\u54cd\uff0c\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u53d7\u9690\u79c1\u6cd5\u89c4\u7ea6\u675f\u7684\u6570\u636e\uff0c\u4ece\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u9009\u62e9\u6027\u9057\u5fd8\u7279\u5b9a\u4fe1\u606f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u4e2d\u5982\u4f55\u9ad8\u6548\u79fb\u9664\u7279\u5b9a\u6570\u636e\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Forget-Aligned Model Reconstruction (FAMR)\u6846\u67b6\uff0c\u5c06\u9057\u5fd8\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u9057\u5fd8\u96c6\u4e0a\u6700\u5c0f\u5316\u5747\u5300\u9884\u6d4b\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u21132\u60e9\u7f5a\u5c06\u6a21\u578b\u53c2\u6570\u951a\u5b9a\u5230\u539f\u59cb\u503c\u3002\u7406\u8bba\u5206\u6790\u8868\u660eFAMR\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u91cd\u65b0\u8bad\u7ec3\u8fd1\u4f3c\u76f8\u5173\u3002", "result": "\u5728CIFAR-10\u548cImageNet-100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFAMR\u5728\u7c7b\u522b\u9057\u5fd8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e2\u80fd\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u53c8\u5177\u6709\u6781\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u8be5\u6846\u67b6\u8fd8\u53ef\u63a8\u5e7f\u5230\u6982\u5ff5\u548c\u98ce\u683c\u64e6\u9664\u4efb\u52a1\u3002", "conclusion": "FAMR\u4e3a\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u540e\u9a8c\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9057\u5fd8\u4efb\u52a1\u3002", "paper_title_zh": "\u4e00\u6b21\u8bad\u7ec3\uff0c\u7cbe\u786e\u9057\u5fd8\uff1a\u57fa\u4e8e\u951a\u5b9a\u4f18\u5316\u7684\u9ad8\u6548\u540e\u9a8c\u9057\u5fd8\u65b9\u6cd5", "abstract_zh": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u53d7\u9690\u79c1\u6cd5\u89c4\u7ea6\u675f\u7684\u6570\u636e\uff0c\u4ece\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u9009\u62e9\u6027\u9057\u5fd8\u7279\u5b9a\u4fe1\u606f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u8fd9\u6d89\u53ca\u79fb\u9664\u7279\u5b9a\u8bad\u7ec3\u6837\u672c\u3001\u8bed\u4e49\u7c7b\u522b\u6216\u89c6\u89c9\u98ce\u683c\u7684\u5f71\u54cd\uff0c\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002\u6211\u4eec\u63d0\u51fa\u4e86Forget-Aligned Model Reconstruction (FAMR)\uff0c\u8fd9\u662f\u4e00\u4e2a\u7406\u8bba\u4e25\u8c28\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u540e\u9a8c\u9057\u5fd8\u6846\u67b6\u3002FAMR\u5c06\u9057\u5fd8\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u9057\u5fd8\u96c6\u4e0a\u6700\u5c0f\u5316\u5747\u5300\u9884\u6d4b\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u21132\u60e9\u7f5a\u5c06\u6a21\u578b\u53c2\u6570\u951a\u5b9a\u5230\u539f\u59cb\u503c\u3002\u7406\u8bba\u5206\u6790\u5c06FAMR\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u91cd\u65b0\u8bad\u7ec3\u8fd1\u4f3c\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u7ed9\u51fa\u4e86\u53c2\u6570\u548c\u8f93\u51fa\u504f\u5dee\u7684\u754c\u9650\u3002\u5728CIFAR-10\u548cImageNet-100\u4e0a\u7684\u7c7b\u522b\u9057\u5fd8\u4efb\u52a1\u4e2d\uff0cFAMR\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u53c8\u5177\u6709\u6781\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u8be5\u6846\u67b6\u8fd8\u53ef\u63a8\u5e7f\u5230\u6982\u5ff5\u548c\u98ce\u683c\u64e6\u9664\u4efb\u52a1\uff0c\u4e3a\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u540e\u9a8c\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9a8c\u8bc1\u7684\u9014\u5f84\u3002"}}
{"id": "2506.13932", "pdf": "https://arxiv.org/pdf/2506.13932", "abs": "https://arxiv.org/abs/2506.13932", "authors": ["Ira Ceka", "Saurabh Pujar", "Irene Manotas", "Gail Kaiser", "Baishakhi Ray", "Shyam Ramji"], "title": "How Does LLM Reasoning Work for Code? A Survey and a Call to Action", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8c03\u67e5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6280\u672f\uff0c\u603b\u7ed3\u4e86\u4e3b\u6d41\u7b56\u7565\u3001\u6df7\u5408\u4e0e\u4ee3\u7406\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4ee3\u7801\u63a8\u7406\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ee3\u7801\u6838\u5fc3\u7279\u6027\u5982\u4f55\u89e3\u91ca\u4e0d\u540c\u63a8\u7406\u6280\u672f\u3002\u540c\u65f6\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u5176\u5728\u4ee3\u7801\u9886\u57df\u7684\u5e94\u7528\u4e5f\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5982\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u548c\u4fee\u590d\u7b49\u3002\u7136\u800c\uff0cLLM\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8\u4ee3\u7801\u63a8\u7406\u7684\u6280\u672f\u53ca\u5176\u6027\u80fd\u9a71\u52a8\u8303\u5f0f\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8c03\u67e5\u548c\u5206\u7c7b\u6cd5\u6784\u5efa\uff0c\u603b\u7ed3\u4e86\u4ee3\u7801\u63a8\u7406\u7684\u4e3b\u6d41\u7b56\u7565\uff08\u5982\u6df7\u5408\u4e0e\u4ee3\u7406\u65b9\u6cd5\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u4ee3\u7801\u6838\u5fc3\u7279\u6027\u5bf9\u63a8\u7406\u6280\u672f\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4ee3\u7801\u4efb\u52a1\u7684\u63a8\u7406\u6280\u672f\u8c03\u67e5\uff0c\u5efa\u7acb\u4e86\u4ee3\u7801\u63a8\u7406\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u57fa\u51c6\u6d4b\u8bd5\u7684\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u63ed\u793a\u4e86\u4ee3\u7801\u7279\u6027\u4e0e\u63a8\u7406\u6280\u672f\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u4e3a\u4ee3\u7801\u63a8\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7814\u7a76\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u548c\u672a\u6765\u63a2\u7d22\u7684\u65b9\u5411\uff0c\u5c24\u5176\u662f\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "LLM\u5982\u4f55\u4e3a\u4ee3\u7801\u63a8\u7406\u5de5\u4f5c\uff1f\u4e00\u9879\u8c03\u67e5\u4e0e\u884c\u52a8\u547c\u5401", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5d1b\u8d77\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u6269\u5c55\u81f3\u4ee3\u7801\u9886\u57df\uff0c\u652f\u6301\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u3001\u6458\u8981\u548c\u4fee\u590d\u7b49\u590d\u6742\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5176\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4efb\u52a1\uff08\u5982GitHub\u95ee\u9898\u89e3\u51b3\uff09\u4e2d\u7684\u5b9e\u7528\u6027\u8fd1\u671f\u624d\u88ab\u7814\u7a76\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u652f\u6491\u8fd9\u4e9b\u4efb\u52a1\u7684\u4ee3\u7801\u63a8\u7406\u6280\u672f\u53ca\u5176\u6027\u80fd\u9a71\u52a8\u8303\u5f0f\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a\uff081\uff09\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4ee3\u7801\u4efb\u52a1\u7684\u63a8\u7406\u6280\u672f\u8c03\u67e5\uff0c\u603b\u7ed3\u4e86\u4e3b\u6d41\u7b56\u7565\u3001\u6df7\u5408\u4e0e\u4ee3\u7406\u65b9\u6cd5\uff1b\uff082\uff09\u4ee3\u7801\u63a8\u7406\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff1b\uff083\uff09\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u7efc\u8ff0\u53ca\u6f5c\u5728\u65b0\u57fa\u51c6\u7684\u5c55\u793a\uff1b\uff084\uff09\u4ee3\u7801\u6838\u5fc3\u7279\u6027\u5982\u4f55\u89e3\u91ca\u4e0d\u540c\u63a8\u7406\u6280\u672f\u7684\u63a2\u8ba8\uff1b\uff085\uff09\u672a\u6765\u7814\u7a76\u7684\u7a7a\u767d\u4e0e\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2506.14524", "pdf": "https://arxiv.org/pdf/2506.14524", "abs": "https://arxiv.org/abs/2506.14524", "authors": ["Nadezhda Alsahanova", "Pavel Bartenev", "Maksim Sharaev", "Milos Ljubisavljevic", "Taleb Al. Mansoori", "Yauhen Statsenko"], "title": "Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis Lesion Delineation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Accurate lesion segmentation is critical for multiple sclerosis (MS) diagnosis, yet current deep learning approaches face robustness challenges.\n  Aim: This study improves MS lesion segmentation by combining data fusion and deep learning techniques.\n  Materials and Methods: We suggested novel radiomic features (concentration rate and R\u00e9nyi entropy) to characterize different MS lesion types and fused these with raw imaging data. The study integrated radiomic features with imaging data through a ResNeXt-UNet architecture and attention-augmented U-Net architecture. Our approach was evaluated on scans from 46 patients (1102 slices), comparing performance before and after data fusion.\n  Results: The radiomics-enhanced ResNeXt-UNet demonstrated high segmentation accuracy, achieving significant improvements in precision and sensitivity over the MRI-only baseline and a Dice score of 0.774$\\pm$0.05; p<0.001 according to Bonferroni-adjusted Wilcoxon signed-rank tests. The radiomics-enhanced attention-augmented U-Net model showed a greater model stability evidenced by reduced performance variability (SDD = 0.18 $\\pm$ 0.09 vs. 0.21 $\\pm$ 0.06; p=0.03) and smoother validation curves with radiomics integration.\n  Conclusion: These results validate our hypothesis that fusing radiomics with raw imaging data boosts segmentation performance and stability in state-of-the-art models.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u653e\u5c04\u7ec4\u5b66\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u53d1\u6027\u786c\u5316\uff08MS\uff09\u75c5\u7076\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6a21\u578b\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u53d1\u6027\u786c\u5316\uff08MS\uff09\u75c5\u7076\u5206\u5272\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4e0e\u539f\u59cb\u5f71\u50cf\u6570\u636e\uff0c\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u65b0\u578b\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff08\u6d53\u5ea6\u7387\u548cR\u00e9nyi\u71b5\uff09\u4ee5\u533a\u5206\u4e0d\u540cMS\u75c5\u7076\u7c7b\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u539f\u59cb\u5f71\u50cf\u6570\u636e\u878d\u5408\u3002\u91c7\u7528ResNeXt-UNet\u548c\u6ce8\u610f\u529b\u589e\u5f3aU-Net\u67b6\u6784\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u572846\u540d\u60a3\u8005\u76841102\u5f20\u5207\u7247\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u7684ResNeXt-UNet\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u654f\u611f\u6027\uff08Dice\u5206\u65700.774\u00b10.05\uff1bp<0.001\uff09\uff0c\u800c\u6ce8\u610f\u529b\u589e\u5f3aU-Net\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\uff08SDD\u964d\u4f4e\u81f30.18\u00b10.09\uff1bp=0.03\uff09\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff0c\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u4e0e\u539f\u59cb\u5f71\u50cf\u6570\u636e\u80fd\u591f\u663e\u8457\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u5206\u5272\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "paper_title_zh": "\u7ed3\u5408\u653e\u5c04\u7ec4\u5b66\u4e0e\u6df1\u5ea6\u5b66\u4e60\u589e\u5f3a\u591a\u53d1\u6027\u786c\u5316\u75c5\u7076\u5206\u5272", "abstract_zh": "\u80cc\u666f\uff1a\u51c6\u786e\u7684\u75c5\u7076\u5206\u5272\u5bf9\u591a\u53d1\u6027\u786c\u5316\uff08MS\uff09\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u9c81\u68d2\u6027\u6311\u6218\u3002\n\u76ee\u6807\uff1a\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u878d\u5408\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6539\u8fdbMS\u75c5\u7076\u5206\u5272\u3002\n\u6750\u6599\u4e0e\u65b9\u6cd5\uff1a\u63d0\u51fa\u65b0\u578b\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff08\u6d53\u5ea6\u7387\u548cR\u00e9nyi\u71b5\uff09\u4ee5\u533a\u5206\u4e0d\u540cMS\u75c5\u7076\u7c7b\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u539f\u59cb\u5f71\u50cf\u6570\u636e\u878d\u5408\u3002\u7814\u7a76\u901a\u8fc7ResNeXt-UNet\u548c\u6ce8\u610f\u529b\u589e\u5f3aU-Net\u67b6\u6784\u6574\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff0c\u5e76\u572846\u540d\u60a3\u8005\u76841102\u5f20\u5207\u7247\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002\n\u7ed3\u679c\uff1a\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u7684ResNeXt-UNet\u8868\u73b0\u51fa\u9ad8\u5206\u5272\u7cbe\u5ea6\uff08Dice\u5206\u65700.774\u00b10.05\uff1bp<0.001\uff09\uff0c\u800c\u6ce8\u610f\u529b\u589e\u5f3aU-Net\u6a21\u578b\u7a33\u5b9a\u6027\u66f4\u9ad8\uff08SDD=0.18\u00b10.09\uff1bp=0.03\uff09\u3002\n\u7ed3\u8bba\uff1a\u7ed3\u679c\u9a8c\u8bc1\u4e86\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u4e0e\u539f\u59cb\u5f71\u50cf\u6570\u636e\u80fd\u591f\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u5206\u5272\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.14542", "pdf": "https://arxiv.org/pdf/2506.14542", "abs": "https://arxiv.org/abs/2506.14542", "authors": ["Xie Shuyang", "Zhou Jie", "Xu Bo", "Wang Jun", "Xu Renjing"], "title": "MobileHolo: A Lightweight Complex-Valued Deformable CNN for High-Quality Computer-Generated Hologram", "categories": ["physics.optics", "cs.CV"], "comment": "8 pages, 9 figures", "summary": "Holographic displays have significant potential in virtual reality and augmented reality owing to their ability to provide all the depth cues. Deep learning-based methods play an important role in computer-generated holograms (CGH). During the diffraction process, each pixel exerts an influence on the reconstructed image. However, previous works face challenges in capturing sufficient information to accurately model this process, primarily due to the inadequacy of their effective receptive field (ERF). Here, we designed complex-valued deformable convolution for integration into network, enabling dynamic adjustment of the convolution kernel's shape to increase flexibility of ERF for better feature extraction. This approach allows us to utilize a single model while achieving state-of-the-art performance in both simulated and optical experiment reconstructions, surpassing existing open-source models. Specifically, our method has a peak signal-to-noise ratio that is 2.04 dB, 5.31 dB, and 9.71 dB higher than that of CCNN-CGH, HoloNet, and Holo-encoder, respectively, when the resolution is 1920$\\times$1072. The number of parameters of our model is only about one-eighth of that of CCNN-CGH.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u590d\u6570\u53ef\u53d8\u5f62\u5377\u79ef\u7f51\u7edcMobileHolo\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u56fe\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5377\u79ef\u6838\u5f62\u72b6\u63d0\u5347\u6709\u6548\u611f\u53d7\u91ce\uff0c\u5728\u6a21\u62df\u548c\u5149\u5b66\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u5168\u606f\u663e\u793a\u5728\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u6709\u6548\u611f\u53d7\u91ce\u4e0d\u8db3\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u884d\u5c04\u8fc7\u7a0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5377\u79ef\u7f51\u7edc\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u590d\u6570\u53ef\u53d8\u5f62\u5377\u79ef\uff0c\u52a8\u6001\u8c03\u6574\u5377\u79ef\u6838\u5f62\u72b6\u4ee5\u589e\u5f3a\u6709\u6548\u611f\u53d7\u91ce\u7684\u7075\u6d3b\u6027\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u884d\u5c04\u8fc7\u7a0b\u3002", "result": "\u57281920\u00d71072\u5206\u8fa8\u7387\u4e0b\uff0c\u5cf0\u503c\u4fe1\u566a\u6bd4\u5206\u522b\u6bd4CCNN-CGH\u3001HoloNet\u548cHolo-encoder\u9ad82.04 dB\u30015.31 dB\u548c9.71 dB\uff0c\u4e14\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u4e3aCCNN-CGH\u7684\u516b\u5206\u4e4b\u4e00\u3002", "conclusion": "MobileHolo\u901a\u8fc7\u590d\u6570\u53ef\u53d8\u5f62\u5377\u79ef\u663e\u8457\u63d0\u5347\u4e86\u5168\u606f\u56fe\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u4e3a\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "MobileHolo\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u590d\u6570\u53ef\u53d8\u5f62CNN\u7528\u4e8e\u9ad8\u8d28\u91cf\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u56fe", "abstract_zh": "\u5168\u606f\u663e\u793a\u56e0\u5176\u80fd\u591f\u63d0\u4f9b\u6240\u6709\u6df1\u5ea6\u7ebf\u7d22\u800c\u5728\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u56fe\uff08CGH\uff09\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002\u5728\u884d\u5c04\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5bf9\u91cd\u5efa\u56fe\u50cf\u4ea7\u751f\u5f71\u54cd\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u4f5c\u56e0\u6709\u6548\u611f\u53d7\u91ce\uff08ERF\uff09\u4e0d\u8db3\u800c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u8fd9\u4e00\u8fc7\u7a0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u590d\u6570\u53ef\u53d8\u5f62\u5377\u79ef\u5e76\u5c06\u5176\u96c6\u6210\u5230\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5377\u79ef\u6838\u5f62\u72b6\u63d0\u5347ERF\u7684\u7075\u6d3b\u6027\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u7279\u5f81\u63d0\u53d6\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5355\u4e00\u6a21\u578b\u5373\u53ef\u5728\u6a21\u62df\u548c\u5149\u5b66\u5b9e\u9a8c\u91cd\u5efa\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u57281920\u00d71072\u5206\u8fa8\u7387\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5cf0\u503c\u4fe1\u566a\u6bd4\u5206\u522b\u6bd4CCNN-CGH\u3001HoloNet\u548cHolo-encoder\u9ad82.04 dB\u30015.31 dB\u548c9.71 dB\uff0c\u4e14\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u4e3aCCNN-CGH\u7684\u516b\u5206\u4e4b\u4e00\u3002"}}
{"id": "2506.13958", "pdf": "https://arxiv.org/pdf/2506.13958", "abs": "https://arxiv.org/abs/2506.13958", "authors": ["Leonardo Guiducci", "Antonio Rizzo", "Giovanna Maria Dimitri"], "title": "Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5185\u5728\u52a8\u673a\u5982\u4f55\u5f71\u54cd\u5f39\u6027\u51b3\u7b56\u53d8\u6362\u5668\uff08EDTs\uff09\u7684\u5d4c\u5165\u8868\u793a\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u5185\u5728\u52a8\u673a\u53d8\u4f53\u5982\u4f55\u5851\u9020\u4e0d\u540c\u7684\u8868\u793a\u7ed3\u6784\uff0c\u5e76\u89e3\u91ca\u4e86\u5176\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u6027\u80fd\u7684\u673a\u5236\u3002", "motivation": "\u5f39\u6027\u51b3\u7b56\u53d8\u6362\u5668\uff08EDTs\uff09\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5185\u5728\u52a8\u673a\u673a\u5236\u5982\u4f55\u5f71\u54cd\u5176\u5d4c\u5165\u8868\u793a\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u5185\u5728\u52a8\u673a\u5982\u4f55\u901a\u8fc7\u5851\u9020\u5d4c\u5165\u8868\u793a\u7684\u7ed3\u6784\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5f15\u5165\u7cfb\u7edf\u6027\u7684\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u5d4c\u5165\u7279\u6027\uff08\u5982\u534f\u65b9\u5dee\u7ed3\u6784\u3001\u5411\u91cf\u5927\u5c0f\u548c\u6b63\u4ea4\u6027\uff09\uff0c\u7814\u7a76\u5185\u5728\u52a8\u673a\u5982\u4f55\u5851\u9020EDTs\u7684\u5d4c\u5165\u8868\u793a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u5185\u5728\u52a8\u673a\u53d8\u4f53\u4f1a\u5f62\u6210\u622a\u7136\u4e0d\u540c\u7684\u8868\u793a\u7ed3\u6784\uff0c\u4e14\u5d4c\u5165\u6307\u6807\u4e0e\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u73af\u5883\u7279\u5b9a\u7684\u76f8\u5173\u6027\u6a21\u5f0f\uff0c\u89e3\u91ca\u4e86\u5185\u5728\u52a8\u673a\u5982\u4f55\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u5185\u5728\u52a8\u673a\u4e0d\u4ec5\u901a\u8fc7\u63a2\u7d22\u5956\u52b1\u53d1\u6325\u4f5c\u7528\uff0c\u8fd8\u4f5c\u4e3a\u4e00\u79cd\u8868\u793a\u5148\u9a8c\uff0c\u4ee5\u751f\u7269\u5b66\u5408\u7406\u7684\u65b9\u5f0f\u5851\u9020\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u597d\u7684\u51b3\u7b56\u3002", "paper_title_zh": "\u8fc8\u5411\u53ef\u89e3\u91ca\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff1a\u5206\u6790\u5185\u5728\u52a8\u673a\u51b3\u7b56\u53d8\u6362\u5668\u4e2d\u7684\u8868\u793a", "abstract_zh": "\u5f39\u6027\u51b3\u7b56\u53d8\u6362\u5668\uff08EDTs\uff09\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c06\u5e8f\u5217\u5efa\u6a21\u4e0e\u4e0d\u786e\u5b9a\u6027\u4e0b\u51b3\u7b56\u7edf\u4e00\u8d77\u6765\u7684\u7075\u6d3b\u6846\u67b6\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5185\u5728\u52a8\u673a\u673a\u5236\u878d\u5165EDTs\u53ef\u4ee5\u63d0\u5347\u63a2\u7d22\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u6539\u8fdb\u80cc\u540e\u7684\u8868\u793a\u673a\u5236\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5206\u6790\u5185\u5728\u52a8\u673a\u5982\u4f55\u5851\u9020EDTs\u4e2d\u7684\u5b66\u4e60\u5d4c\u5165\u3002\u901a\u8fc7\u5bf9\u5d4c\u5165\u7279\u6027\uff08\u5305\u62ec\u534f\u65b9\u5dee\u7ed3\u6784\u3001\u5411\u91cf\u5927\u5c0f\u548c\u6b63\u4ea4\u6027\uff09\u7684\u7edf\u8ba1\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u4e0d\u540c\u7684\u5185\u5728\u52a8\u673a\u53d8\u4f53\u4f1a\u5f62\u6210\u6839\u672c\u4e0d\u540c\u7684\u8868\u793a\u7ed3\u6784\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5d4c\u5165\u6307\u6807\u4e0e\u6027\u80fd\u4e4b\u95f4\u73af\u5883\u7279\u5b9a\u7684\u76f8\u5173\u6027\u6a21\u5f0f\uff0c\u89e3\u91ca\u4e86\u5185\u5728\u52a8\u673a\u5982\u4f55\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5185\u5728\u52a8\u673a\u4e0d\u4ec5\u901a\u8fc7\u7b80\u5355\u7684\u63a2\u7d22\u5956\u52b1\u53d1\u6325\u4f5c\u7528\uff0c\u8fd8\u4f5c\u4e3a\u4e00\u79cd\u8868\u793a\u5148\u9a8c\uff0c\u4ee5\u751f\u7269\u5b66\u5408\u7406\u7684\u65b9\u5f0f\u5851\u9020\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\uff0c\u5f62\u6210\u73af\u5883\u7279\u5b9a\u7684\u7ec4\u7ec7\u7ed3\u6784\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u597d\u7684\u51b3\u7b56\u3002"}}
{"id": "2506.14582", "pdf": "https://arxiv.org/pdf/2506.14582", "abs": "https://arxiv.org/abs/2506.14582", "authors": ["Kaleel Mahmood", "Caleb Manicke", "Ethan Rathbun", "Aayushi Verma", "Sohaib Ahmad", "Nicholas Stamatakis", "Laurent Michel", "Benjamin Fuller"], "title": "Busting the Paper Ballot: Voting Meets Adversarial Machine Learning", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "18 Pages. Author version of article to appear at CCS 2025", "summary": "We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u7f8e\u56fd\u9009\u4e3e\u8ba1\u7968\u673a\u4e2d\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u591a\u79cd\u6a21\u578b\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u5bf9\u6297\u6027\u653b\u51fb\u5728\u9009\u4e3e\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5728\u9009\u4e3e\u8ba1\u7968\u673a\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u653b\u51fb\u53ef\u80fd\u5bf9\u9009\u4e3e\u7ed3\u679c\u4ea7\u751f\u7684\u91cd\u5927\u5f71\u54cd\u3002", "method": "1. \u5f15\u5165\u56db\u4e2a\u65b0\u7684\u9009\u7968\u6570\u636e\u96c6\uff1b2. \u8bad\u7ec3\u548c\u6d4b\u8bd5\u591a\u79cd\u6a21\u578b\uff08\u5982SVM\u3001CNN\u3001VGG\u3001ResNet\u3001\u89c6\u89c9\u53d8\u6362\u5668\u7b49\uff09\uff1b3. \u5206\u6790\u4f20\u7edf\u767d\u76d2\u653b\u51fb\u7684\u65e0\u6548\u6027\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff1b4. \u5728\u7269\u7406\u4e16\u754c\u4e2d\u9a8c\u8bc1\u5bf9\u6297\u6027\u653b\u51fb\u7684\u53ef\u884c\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u68af\u5ea6\u63a9\u853d\u5bfc\u81f4\u4f20\u7edf\u767d\u76d2\u653b\u51fb\u65e0\u6548\uff0c\u4f46\u901a\u8fc7\u6539\u8fdb\u65b9\u6cd5\u6210\u529f\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u8bc1\u660e\u5373\u4f7f5%\u7684\u653b\u51fb\u6210\u529f\u7387\u4e5f\u53ef\u80fd\u6539\u53d8\u9009\u4e3e\u7ed3\u679c\u3002", "conclusion": "\u9009\u4e3e\u8ba1\u7968\u673a\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5bf9\u6297\u6027\u653b\u51fb\u53ef\u80fd\u5bf9\u9009\u4e3e\u7ed3\u679c\u4ea7\u751f\u5b9e\u9645\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u63aa\u65bd\u3002", "paper_title_zh": "\u6253\u7834\u7eb8\u8d28\u9009\u7968\uff1a\u6295\u7968\u4e0e\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u7684\u78b0\u649e", "abstract_zh": "\u672c\u6587\u63ed\u793a\u4e86\u7f8e\u56fd\u9009\u4e3e\u8ba1\u7968\u673a\u4e2d\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u5b89\u5168\u98ce\u9669\u3002\u9009\u4e3e\u8ba1\u7968\u7684\u6838\u5fc3\u5206\u7c7b\u4efb\u52a1\u662f\u5224\u65ad\u9009\u7968\u4e0a\u67d0\u4e2a\u9009\u9879\u7684\u6807\u8bb0\u662f\u5426\u5b58\u5728\u3002Barretto\u7b49\u4eba\uff08E-Vote-ID 2021\uff09\u66fe\u62a5\u544a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u6b64\u9886\u57df\u8868\u73b0\u4f18\u4e8e\u7b80\u5355\u7279\u5f81\u5206\u7c7b\u5668\u3002\n\n\u6211\u4eec\u7684\u8d21\u732e\u5206\u4e3a\u56db\u90e8\u5206\uff1a\u9996\u5148\uff0c\u5f15\u5165\u56db\u4e2a\u65b0\u7684\u9009\u7968\u6570\u636e\u96c6\u4ee5\u5206\u6790\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6f5c\u5728\u6f0f\u6d1e\uff1b\u5176\u6b21\uff0c\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u591a\u79cd\u6a21\u578b\uff08\u5305\u62ec\u652f\u6301\u5411\u91cf\u673a\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff09\uff1b\u7b2c\u4e09\uff0c\u53d1\u73b0\u4f20\u7edf\u767d\u76d2\u653b\u51fb\u56e0\u68af\u5ea6\u63a9\u853d\u800c\u65e0\u6548\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff1b\u7b2c\u56db\uff0c\u5728\u7269\u7406\u4e16\u754c\u4e2d\u9a8c\u8bc1\u5bf9\u6297\u6027\u653b\u51fb\u7684\u53ef\u884c\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f5%\u7684\u653b\u51fb\u6210\u529f\u7387\u4e5f\u53ef\u80fd\u6539\u53d8\u9009\u4e3e\u7ed3\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u6253\u5370\u548c\u626b\u63cf\u5bf9\u6297\u6837\u672c\u7684\u6311\u6218\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.13961", "pdf": "https://arxiv.org/pdf/2506.13961", "abs": "https://arxiv.org/abs/2506.13961", "authors": ["Mohamed Serry", "Haoyu Li", "Ruikun Zhou", "Huan Zhang", "Jun Liu"], "title": "Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation", "categories": ["eess.SY", "cs.AI"], "comment": null, "summary": "Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $\u03b1,\\!\u03b2$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5b89\u5168\u5438\u5f15\u57df\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u5bfc\u65b0\u7684Zubov\u65b9\u7a0b\u5e76\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u6c42\u89e3\uff0c\u7ed3\u5408\u9a8c\u8bc1\u5de5\u5177\u5b9e\u73b0\u53ef\u8ba4\u8bc1\u7684\u4f30\u8ba1\u3002", "motivation": "\u975e\u7ebf\u6027\u81ea\u6cbb\u7cfb\u7edf\u7684\u5438\u5f15\u57df\u4f30\u8ba1\u4e00\u76f4\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4fdd\u5b88\u6216\u4ec5\u9002\u7528\u4e8e\u4f4e\u7ef4\u7cfb\u7edf\uff0c\u4e14\u5728\u8003\u8651\u72b6\u6001\u7ea6\u675f\u65f6\u66f4\u4e3a\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u4e00\u4e2a\u65b0\u7684Zubov\u65b9\u7a0b\uff0c\u5176\u89e3\u5bf9\u5e94\u4e8e\u7cbe\u786e\u7684\u5b89\u5168\u5438\u5f15\u57df\uff1b\u968f\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u6c42\u89e3\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6807\u51c6\u9a8c\u8bc1\u5de5\u5177\uff08\u5982\u03b1,\u03b2-CROWN\u548cdReal\uff09\u5b9e\u73b0\u53ef\u8ba4\u8bc1\u7684\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u5177\u6709\u72b6\u6001\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5b89\u5168\u5438\u5f15\u57df\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5b89\u5168\u5438\u5f15\u57df\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "paper_title_zh": "\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5b89\u5168\u5438\u5f15\u57df\uff1a\u8868\u5f81\u4e0e\u53ef\u9a8c\u8bc1\u7684\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1", "abstract_zh": "\u975e\u7ebf\u6027\u81ea\u6cbb\u7cfb\u7edf\u7684\u5206\u6790\u901a\u5e38\u6d89\u53ca\u5438\u5f15\u57df\u7684\u4f30\u8ba1\uff0c\u8fd9\u4e00\u8bfe\u9898\u51e0\u5341\u5e74\u6765\u4e00\u76f4\u662f\u7814\u7a76\u70ed\u70b9\u3002\u7136\u800c\uff0c\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5438\u5f15\u57df\u4ecd\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4fdd\u5b88\u6216\u4ec5\u9002\u7528\u4e8e\u4f4e\u7ef4\u7cfb\u7edf\u3002\u5728\u8003\u8651\u72b6\u6001\u7ea6\u675f\u65f6\uff0c\u8fd9\u4e00\u95ee\u9898\u66f4\u4e3a\u590d\u6742\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u4f30\u8ba1\u79bb\u6563\u65f6\u95f4\u81ea\u6cbb\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5b89\u5168\uff08\u72b6\u6001\u7ea6\u675f\uff09\u5438\u5f15\u57df\u3002\u5728\u5efa\u7acb\u8be5\u6846\u67b6\u65f6\uff0c\u6211\u4eec\u9996\u5148\u63a8\u5bfc\u4e86\u4e00\u4e2a\u65b0\u7684Zubov\u65b9\u7a0b\uff0c\u5176\u89e3\u5bf9\u5e94\u4e8e\u7cbe\u786e\u7684\u5b89\u5168\u5438\u5f15\u57df\u3002\u8be5Zubov\u65b9\u7a0b\u7684\u89e3\u88ab\u8bc1\u660e\u5728\u6574\u4e2a\u72b6\u6001\u7a7a\u95f4\u5185\u662f\u552f\u4e00\u4e14\u8fde\u7eed\u7684\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u6c42\u89e3Zubov\u65b9\u7a0b\u3002\u4e3a\u4e86\u4ece\u795e\u7ecf\u7f51\u7edc\u7684\u8fd1\u4f3c\u89e3\u4e2d\u83b7\u5f97\u53ef\u8ba4\u8bc1\u7684\u5438\u5f15\u57df\u4f30\u8ba1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u53ef\u901a\u8fc7\u6807\u51c6\u9a8c\u8bc1\u5de5\u5177\uff08\u5982\u03b1,\u03b2-CROWN\u548cdReal\uff09\u5b9e\u73b0\u3002\u4e3a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u6d89\u53ca\u72b6\u6001\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u3002"}}
{"id": "2506.14698", "pdf": "https://arxiv.org/pdf/2506.14698", "abs": "https://arxiv.org/abs/2506.14698", "authors": ["Sidney Bender", "Jan Herrmann", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Towards Desiderata-Driven Design of Visual Counterfactual Explainers", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\uff08VCE\uff09\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5168\u9762\u6ee1\u8db3\u89e3\u91ca\u9700\u6c42\uff08\u5982\u4fdd\u771f\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5145\u5206\u6027\uff09\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u6837\u672c\u8d28\u91cf\u6216\u6700\u5c0f\u53d8\u5316\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u201c\u5e73\u6ed1\u53cd\u4e8b\u5b9e\u63a2\u7d22\u5668\u201d\uff08SCE\uff09\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\uff08VCEs\uff09\u8fc7\u4e8e\u5173\u6ce8\u6837\u672c\u8d28\u91cf\u6216\u6700\u5c0f\u53d8\u5316\uff0c\u5ffd\u7565\u4e86\u66f4\u5168\u9762\u7684\u89e3\u91ca\u9700\u6c42\uff08\u5982\u4fdd\u771f\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5145\u5206\u6027\uff09\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u65b0\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u673a\u5236\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u201c\u5e73\u6ed1\u53cd\u4e8b\u5b9e\u63a2\u7d22\u5668\u201d\uff08SCE\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u591a\u79cd\u53cd\u4e8b\u5b9e\u751f\u6210\u673a\u5236\uff0c\u65e8\u5728\u6ee1\u8db3\u4fdd\u771f\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5145\u5206\u6027\u7b49\u89e3\u91ca\u9700\u6c42\u3002\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u7684\u7cfb\u7edf\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSCE\u7b97\u6cd5\u5728\u6ee1\u8db3\u4fdd\u771f\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5145\u5206\u6027\u7b49\u89e3\u91ca\u9700\u6c42\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u66f4\u5168\u9762\u7684\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\u8bbe\u8ba1\u4e2d\u5168\u9762\u6ee1\u8db3\u89e3\u91ca\u9700\u6c42\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684SCE\u7b97\u6cd5\u3002\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u5e94\u7528\u573a\u666f\u3002", "paper_title_zh": "\u9762\u5411\u9700\u6c42\u9a71\u52a8\u7684\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\u8bbe\u8ba1", "abstract_zh": "\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\uff08VCEs\uff09\u662f\u4e00\u79cd\u76f4\u63a5\u4e14\u6709\u671b\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u5668\u900f\u660e\u6027\u7684\u65b9\u6cd5\u3002VCEs\u901a\u8fc7\u63ed\u793a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6700\u654f\u611f\u7684\u6570\u636e\u53d8\u6362\uff0c\u8865\u5145\u4e86\u7279\u5f81\u5f52\u56e0\u7b49\u5176\u4ed6\u89e3\u91ca\u7c7b\u578b\u3002\u672c\u6587\u8ba4\u4e3a\uff0c\u73b0\u6709VCEs\u8fc7\u4e8e\u5173\u6ce8\u4f18\u5316\u6837\u672c\u8d28\u91cf\u6216\u6700\u5c0f\u53d8\u5316\uff0c\u800c\u5ffd\u7565\u4e86\u66f4\u5168\u9762\u7684\u89e3\u91ca\u9700\u6c42\uff08\u5982\u4fdd\u771f\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5145\u5206\u6027\uff09\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u65b0\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u673a\u5236\uff0c\u5e76\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u8fd9\u4e9b\u673a\u5236\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u673a\u5236\u7ed3\u5408\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5e73\u6ed1\u53cd\u4e8b\u5b9e\u63a2\u7d22\u5668\u201d\uff08SCE\uff09\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u7684\u7cfb\u7edf\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.13970", "pdf": "https://arxiv.org/pdf/2506.13970", "abs": "https://arxiv.org/abs/2506.13970", "authors": ["Charles C Onu"], "title": "Making deep neural networks work for medical audio: representation, compression and domain adaptation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "PhD Thesis", "summary": "This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.\n  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.\n  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5904\u7406\u533b\u7597\u97f3\u9891\u4fe1\u53f7\uff0c\u5c24\u5176\u662f\u5a74\u513f\u54ed\u58f0\uff0c\u4ee5\u9884\u6d4b\u5065\u5eb7\u72b6\u51b5\u3002\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u6a21\u578b\u538b\u7f29\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5f00\u6e90\u5a74\u513f\u54ed\u58f0\u6570\u636e\u96c6\u3002", "motivation": "\u533b\u7597\u97f3\u9891\u4fe1\u53f7\uff08\u5982\u80ba\u97f3\u3001\u5fc3\u97f3\u548c\u54ed\u58f0\uff09\u8574\u542b\u91cd\u8981\u5065\u5eb7\u4fe1\u606f\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4e13\u5bb6\u542c\u89c9\u5206\u6790\u3002\u81ea\u52a8\u5316\u5206\u6790\u53ef\u6807\u51c6\u5316\u5904\u7406\u3001\u652f\u6301\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u7684\u7b5b\u67e5\uff0c\u5e76\u53d1\u73b0\u4eba\u7c7b\u96be\u4ee5\u5bdf\u89c9\u7684\u7ec6\u5fae\u6a21\u5f0f\uff0c\u4ece\u800c\u4fc3\u8fdb\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\u3002", "method": "1. \u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5229\u7528\u6210\u4eba\u8bed\u97f3\u6570\u636e\u5e93\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u5a74\u513f\u54ed\u58f0\u5206\u6790\u7684\u51c6\u786e\u6027\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684\u7aef\u5230\u7aef\u5faa\u73af\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\uff1b3. \u9488\u5bf9\u97f3\u9891\u6a21\u578b\u8bbe\u8ba1\u9886\u57df\u9002\u5e94\u6280\u672f\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u89e3\u51b3\u6570\u636e\u96c6\u504f\u5dee\uff1b4. \u53d1\u5e03\u5f00\u6e90\u5a74\u513f\u54ed\u58f0\u6570\u636e\u96c6\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff1b\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6570\u767e\u500d\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff1b\u9886\u57df\u9002\u5e94\u6280\u672f\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u5f00\u6e90\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5c06\u5a74\u513f\u54ed\u58f0\u4f5c\u4e3a\u91cd\u8981\u5065\u5eb7\u6307\u6807\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86AI\u9a71\u52a8\u7684\u97f3\u9891\u76d1\u6d4b\u5728\u63a8\u52a8\u666e\u60e0\u533b\u7597\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u8ba9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9002\u7528\u4e8e\u533b\u7597\u97f3\u9891\uff1a\u8868\u5f81\u3001\u538b\u7f29\u4e0e\u9886\u57df\u9002\u5e94", "abstract_zh": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5e94\u7528\u673a\u5668\u5b66\u4e60\u7406\u89e3\u548c\u89e3\u6790\u533b\u7597\u97f3\u9891\u4fe1\u53f7\u7684\u6280\u672f\u6311\u6218\u3002\u80ba\u97f3\u3001\u5fc3\u97f3\u548c\u8bed\u97f3\u7b49\u58f0\u97f3\u4f20\u9012\u4e86\u91cd\u8981\u7684\u5065\u5eb7\u4fe1\u606f\uff0c\u4f46\u5728\u73b0\u4ee3\u533b\u5b66\u4e2d\uff0c\u8fd9\u4e9b\u58f0\u97f3\u4e3b\u8981\u901a\u8fc7\u4e13\u5bb6\u4f7f\u7528\u542c\u8bca\u5668\u7b49\u8bbe\u5907\u8fdb\u884c\u542c\u89c9\u5206\u6790\u3002\u81ea\u52a8\u5316\u5206\u6790\u6709\u671b\u6807\u51c6\u5316\u533b\u7597\u58f0\u97f3\u5904\u7406\uff0c\u652f\u6301\u533b\u5e08\u7a00\u7f3a\u7684\u4f4e\u8d44\u6e90\u5730\u533a\u7b5b\u67e5\uff0c\u5e76\u68c0\u6d4b\u4eba\u7c7b\u96be\u4ee5\u5bdf\u89c9\u7684\u7ec6\u5fae\u6a21\u5f0f\uff0c\u4ece\u800c\u4fc3\u8fdb\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\u3002\n\n\u805a\u7126\u4e8e\u901a\u8fc7\u5a74\u513f\u54ed\u58f0\u9884\u6d4b\u5065\u5eb7\u72b6\u51b5\uff0c\u672c\u8bba\u6587\u5728\u56db\u4e2a\u5173\u952e\u65b9\u9762\u505a\u51fa\u8d21\u732e\uff1a\u9996\u5148\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u8bc1\u660e\u53ef\u901a\u8fc7\u795e\u7ecf\u8fc1\u79fb\u5b66\u4e60\u5229\u7528\u6210\u4eba\u8bed\u97f3\u5927\u578b\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u7684\u5a74\u513f\u54ed\u58f0\u5206\u6790\u6a21\u578b\uff1b\u5176\u6b21\uff0c\u5728\u6210\u672c\u6548\u76ca\u5efa\u6a21\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684\u7aef\u5230\u7aef\u5faa\u73af\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\uff0c\u65e0\u9700\u540e\u5904\u7406\u5373\u53ef\u5b9e\u73b0\u6570\u767e\u500d\u7684\u538b\u7f29\u7387\uff0c\u751f\u6210\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u4fbf\u643a\u6a21\u578b\uff1b\u7b2c\u4e09\uff0c\u6211\u4eec\u9488\u5bf9\u97f3\u9891\u6a21\u578b\u63d0\u51fa\u65b0\u9896\u7684\u9886\u57df\u9002\u5e94\u6280\u672f\uff0c\u5e76\u501f\u9274\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u504f\u5dee\u95ee\u9898\uff0c\u589e\u5f3a\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff1b\u6700\u540e\uff0c\u4e3a\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u5f00\u6e90\u5a74\u513f\u54ed\u58f0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u4e0e\u5168\u7403\u4e34\u5e8a\u533b\u751f\u5408\u4f5c\u5f00\u53d1\u7684\u3002\n\n\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u5a74\u513f\u54ed\u58f0\u89c6\u4e3a\u91cd\u8981\u5065\u5eb7\u6307\u6807\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u51f8\u663e\u4e86AI\u9a71\u52a8\u7684\u97f3\u9891\u76d1\u6d4b\u5728\u5851\u9020\u666e\u60e0\u533b\u7597\u672a\u6765\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2506.14719", "pdf": "https://arxiv.org/pdf/2506.14719", "abs": "https://arxiv.org/abs/2506.14719", "authors": ["Haley Duba-Sullivan", "Aniket Pramanik", "Venkatakrishnan Singanallur", "Amirkoushyar Ziabari"], "title": "Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted to Journal of Nondestructive Evaluation", "summary": "Cone-beam X-ray computed tomography (XCT) is an essential imaging technique for generating 3D reconstructions of internal structures, with applications ranging from medical to industrial imaging. Producing high-quality reconstructions typically requires many X-ray measurements; this process can be slow and expensive, especially for dense materials. Recent work incorporating artifact reduction priors within a plug-and-play (PnP) reconstruction framework has shown promising results in improving image quality from sparse-view XCT scans while enhancing the generalizability of deep learning-based solutions. However, this method uses a 2D convolutional neural network (CNN) for artifact reduction, which captures only slice-independent information from the 3D reconstruction, limiting performance. In this paper, we propose a PnP reconstruction method that uses a 2.5D artifact reduction CNN as the prior. This approach leverages inter-slice information from adjacent slices, capturing richer spatial context while remaining computationally efficient. We show that this 2.5D prior not only improves the quality of reconstructions but also enables the model to directly suppress commonly occurring XCT artifacts (such as beam hardening), eliminating the need for artifact correction pre-processing. Experiments on both experimental and synthetic cone-beam XCT data demonstrate that the proposed method better preserves fine structural details, such as pore size and shape, leading to more accurate defect detection compared to 2D priors. In particular, we demonstrate strong performance on experimental XCT data using a 2.5D artifact reduction prior trained entirely on simulated scans, highlighting the proposed method's ability to generalize across domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2.5D\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\u7684\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u5de5\u4e1aCT\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u76f8\u90bb\u5207\u7247\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56feCT\u626b\u63cf\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u76f4\u63a5\u6291\u5236\u5e38\u89c1\u4f2a\u5f71\uff0c\u65e0\u9700\u989d\u5916\u9884\u5904\u7406\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u65b9\u6cd5\u4f7f\u75282D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4f5c\u4e3a\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\uff0c\u4ec5\u80fd\u6355\u83b7\u5207\u7247\u72ec\u7acb\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u51652.5D CNN\u5148\u9a8c\uff0c\u5229\u7528\u76f8\u90bb\u5207\u7247\u4fe1\u606f\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u76f4\u63a5\u6291\u5236\u5e38\u89c1\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u91cd\u5efa\u6846\u67b6\uff0c\u91c7\u75282.5D CNN\u4f5c\u4e3a\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6355\u83b7\u76f8\u90bb\u5207\u7247\u95f4\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u6291\u5236\u5982\u675f\u786c\u5316\u7b49\u5e38\u89c1\u4f2a\u5f71\uff0c\u65e0\u9700\u989d\u5916\u9884\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e2D\u5148\u9a8c\u76f8\u6bd4\uff0c2.5D\u5148\u9a8c\u5728\u5408\u6210\u548c\u5b9e\u9a8c\u9525\u675fCT\u6570\u636e\u4e0a\u5747\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u7ec6\u5fae\u7ed3\u6784\uff08\u5982\u5b54\u9699\u5927\u5c0f\u548c\u5f62\u72b6\uff09\uff0c\u4ece\u800c\u63d0\u5347\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u626b\u63cf\u8bad\u7ec3\u76842.5D\u5148\u9a8c\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u76842.5D\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u540c\u65f6\u80fd\u591f\u76f4\u63a5\u6291\u5236\u5e38\u89c1\u4f2a\u5f71\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5de5\u4e1aCT\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e2.5D\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\u7684\u5373\u63d2\u5373\u7528\u5feb\u901f\u7cbe\u786e\u5de5\u4e1aCT\u91cd\u5efa\u65b9\u6cd5", "abstract_zh": "\u9525\u675fX\u5c04\u7ebf\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08XCT\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u5185\u90e8\u7ed3\u67843D\u91cd\u5efa\u7684\u91cd\u8981\u6210\u50cf\u6280\u672f\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u533b\u5b66\u548c\u5de5\u4e1a\u6210\u50cf\u9886\u57df\u3002\u9ad8\u8d28\u91cf\u91cd\u5efa\u901a\u5e38\u9700\u8981\u5927\u91cfX\u5c04\u7ebf\u6d4b\u91cf\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u5c24\u5176\u662f\u5bf9\u9ad8\u5bc6\u5ea6\u6750\u6599\u3002\u8fd1\u5e74\u6765\uff0c\u5c06\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\u7eb3\u5165\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u91cd\u5efa\u6846\u67b6\u7684\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56feXCT\u626b\u63cf\u4e2d\u663e\u793a\u51fa\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u6cdb\u5316\u6027\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8be5\u65b9\u6cd5\u4f7f\u75282D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u4f2a\u5f71\u51cf\u5c11\uff0c\u4ec5\u80fd\u6355\u83b73D\u91cd\u5efa\u4e2d\u7684\u5207\u7247\u72ec\u7acb\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u75282.5D\u4f2a\u5f71\u51cf\u5c11CNN\u4f5c\u4e3a\u5148\u9a8c\u7684PnP\u91cd\u5efa\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u76f8\u90bb\u5207\u7247\u7684\u5207\u7247\u95f4\u4fe1\u606f\uff0c\u6355\u83b7\u66f4\u4e30\u5bcc\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c2.5D\u5148\u9a8c\u4e0d\u4ec5\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u8fd8\u80fd\u76f4\u63a5\u6291\u5236\u5e38\u89c1XCT\u4f2a\u5f71\uff08\u5982\u675f\u786c\u5316\uff09\uff0c\u65e0\u9700\u989d\u5916\u9884\u5904\u7406\u3002\u5728\u5408\u6210\u548c\u5b9e\u9a8c\u9525\u675fXCT\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e2D\u5148\u9a8c\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u7ec6\u5fae\u7ed3\u6784\uff08\u5982\u5b54\u9699\u5927\u5c0f\u548c\u5f62\u72b6\uff09\uff0c\u4ece\u800c\u63d0\u5347\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u7279\u522b\u5730\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u57fa\u4e8e\u6a21\u62df\u626b\u63cf\u8bad\u7ec3\u76842.5D\u4f2a\u5f71\u51cf\u5c11\u5148\u9a8c\u5728\u5b9e\u9a8cXCT\u6570\u636e\u4e0a\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u8be5\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2506.13981", "pdf": "https://arxiv.org/pdf/2506.13981", "abs": "https://arxiv.org/abs/2506.13981", "authors": ["Thanh Dan Bui"], "title": "HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6ce8\u610f\u529b\u96c6\u6210\u5b66\u4e60Transformer\u6846\u67b6\uff08HAELT\uff09\uff0c\u7528\u4e8e\u9ad8\u9891\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86ResNet\u964d\u566a\u6a21\u5757\u3001\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548cLSTM-Transformer\u6df7\u5408\u6838\u5fc3\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u96c6\u6210\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u9ad8\u9891\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u9762\u4e34\u975e\u5e73\u7a33\u6027\u3001\u566a\u58f0\u548c\u6ce2\u52a8\u6027\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51faHAELT\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "HAELT\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8eResNet\u7684\u964d\u566a\u6a21\u5757\uff0c\u7528\u4e8e\u51cf\u5c11\u6570\u636e\u566a\u58f0\uff1b2\uff09\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u52a8\u6001\u5173\u6ce8\u76f8\u5173\u5386\u53f2\u4fe1\u606f\uff1b3\uff09LSTM-Transformer\u6df7\u5408\u6838\u5fc3\uff0c\u6355\u6349\u5c40\u90e8\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u6839\u636e\u8fd1\u671f\u8868\u73b0\u81ea\u9002\u5e94\u96c6\u6210\u3002", "result": "\u57282024\u5e741\u6708\u81f32025\u5e745\u6708\u7684\u82f9\u679c\u516c\u53f8\uff08AAPL\uff09\u6bcf\u5c0f\u65f6\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0cHAELT\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u6570\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u80a1\u4ef7\u7684\u4e0a\u6da8\u548c\u4e0b\u8dcc\u8d8b\u52bf\u3002", "conclusion": "HAELT\u6846\u67b6\u5728\u9ad8\u9891\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u91d1\u878d\u9884\u6d4b\u548c\u7b97\u6cd5\u4ea4\u6613\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "HAELT\uff1a\u4e00\u79cd\u6df7\u5408\u6ce8\u610f\u529b\u96c6\u6210\u5b66\u4e60Transformer\u6846\u67b6\u7528\u4e8e\u9ad8\u9891\u80a1\u7968\u4ef7\u683c\u9884\u6d4b", "abstract_zh": "\u9ad8\u9891\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u56e0\u975e\u5e73\u7a33\u6027\u3001\u566a\u58f0\u548c\u6ce2\u52a8\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u96c6\u6210\u5b66\u4e60Transformer\uff08HAELT\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8eResNet\u7684\u964d\u566a\u6a21\u5757\u3001\u7528\u4e8e\u52a8\u6001\u5173\u6ce8\u76f8\u5173\u5386\u53f2\u7684\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u6355\u6349\u5c40\u90e8\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u7684LSTM-Transformer\u6df7\u5408\u6838\u5fc3\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u6839\u636e\u8fd1\u671f\u8868\u73b0\u81ea\u9002\u5e94\u96c6\u6210\u3002\u57282024\u5e741\u6708\u81f32025\u5e745\u6708\u7684\u82f9\u679c\u516c\u53f8\uff08AAPL\uff09\u6bcf\u5c0f\u65f6\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0cHAELT\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u6570\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u80a1\u4ef7\u7684\u4e0a\u6da8\u548c\u4e0b\u8dcc\u8d8b\u52bf\u3002\u8fd9\u8868\u660eHAELT\u5728\u7a33\u5065\u3001\u5b9e\u7528\u7684\u91d1\u878d\u9884\u6d4b\u548c\u7b97\u6cd5\u4ea4\u6613\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.13984", "pdf": "https://arxiv.org/pdf/2506.13984", "abs": "https://arxiv.org/abs/2506.13984", "authors": ["Andrzej Cichocki"], "title": "Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7c7b\u57fa\u4e8eTempesta\u591a\u53c2\u6570\u5bf9\u6570\u53d8\u5f62\u7684\u955c\u50cf\u4e0b\u964d\uff08MD\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570\u9002\u5e94\u6570\u636e\u5206\u5e03\u6216\u51e0\u4f55\u7279\u6027\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5e7f\u6cdb\u7684\u4f18\u5316\u5de5\u5177\u3002", "motivation": "\u955c\u50cf\u4e0b\u964d\u7b97\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165Tempesta\u591a\u53c2\u6570\u5bf9\u6570\u53d8\u5f62\uff0c\u5f00\u53d1\u4e00\u7c7b\u66f4\u5e7f\u6cdb\u7684MD\u7b97\u6cd5\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5206\u5e03\u548c\u51e0\u4f55\u7279\u6027\u3002", "method": "\u672c\u6587\u5229\u7528Tempesta\u591a\u53c2\u6570\u5bf9\u6570\u53d8\u5f62\u4f5c\u4e3a\u94fe\u63a5\u51fd\u6570\uff0c\u6784\u5efaBregman\u6563\u5ea6\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f30\u8ba1\u5e7f\u4e49\u6307\u6570\u51fd\u6570\u63a8\u5bfc\u65b0\u7684MD\u66f4\u65b0\u89c4\u5219\u3002\u8d85\u53c2\u6570\u7684\u5b66\u4e60\u4f7f\u7b97\u6cd5\u80fd\u591f\u9002\u5e94\u6570\u636e\u7279\u6027\u3002", "result": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u7c7b\u57fa\u4e8eTempesta\u591a\u53c2\u6570\u5bf9\u6570\u7684MD\u7b97\u6cd5\uff0c\u5176\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u5b9e\u73b0\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165Tempesta\u591a\u53c2\u6570\u5bf9\u6570\u53d8\u5f62\uff0c\u672c\u6587\u6269\u5c55\u4e86MD\u7b97\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5176\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u4e3a\u673a\u5668\u5b66\u4e60\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8eTempesta\u5e7f\u4e49\u591a\u53c2\u6570\u5bf9\u6570\u7684\u955c\u50cf\u4e0b\u964d\u65b9\u6cd5", "abstract_zh": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u7c7b\u5e7f\u6cdb\u7684\u955c\u50cf\u4e0b\u964d\uff08MD\uff09\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5177\u6709\u5173\u952e\u4f5c\u7528\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528Bregman\u6563\u5ea6\uff0c\u5e76\u4ee5Tempesta\u591a\u53c2\u6570\u53d8\u5f62\u5bf9\u6570\u4f5c\u4e3a\u94fe\u63a5\u51fd\u6570\u3002\u8be5\u94fe\u63a5\u51fd\u6570\uff08\u4e5f\u79f0\u4e3a\u955c\u50cf\u51fd\u6570\uff09\u5b9a\u4e49\u4e86\u539f\u59cb\u7a7a\u95f4\u548c\u5bf9\u5076\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u5e76\u4e0e\u4e00\u7c7b\u975e\u5e38\u5e7f\u6cdb\uff08\u7406\u8bba\u4e0a\u65e0\u9650\uff09\u7684\u5e7f\u4e49\u8ff9\u5f62\u5f0f\u71b5\u76f8\u5173\u8054\u3002\u4e3a\u4e86\u63a8\u5bfc\u65b0\u7684MD\u66f4\u65b0\u89c4\u5219\uff0c\u6211\u4eec\u4f30\u8ba1\u4e86\u5e7f\u4e49\u6307\u6570\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5bc6\u5207\u8fd1\u4f3c\u4e8e\u591a\u53c2\u6570Tempesta\u5e7f\u4e49\u5bf9\u6570\u7684\u9006\u51fd\u6570\u3002Tempesta\u5bf9\u6570\u53ca\u5176\u9006\u53d8\u5f62\u6307\u6570\u51fd\u6570\u7684\u5f62\u72b6\u548c\u6027\u8d28\u53ef\u4ee5\u901a\u8fc7\u591a\u4e2a\u8d85\u53c2\u6570\u8fdb\u884c\u8c03\u6574\u3002\u901a\u8fc7\u5b66\u4e60\u8fd9\u4e9b\u8d85\u53c2\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u9002\u5e94\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u6216\u51e0\u4f55\u7279\u6027\uff0c\u5e76\u8c03\u6574\u5b83\u4eec\u4ee5\u5b9e\u73b0MD\u7b97\u6cd5\u7684\u671f\u671b\u6027\u8d28\u3002\u5e94\u7528\u591a\u53c2\u6570\u5bf9\u6570\u7684\u6982\u5ff5\u4f7f\u6211\u4eec\u80fd\u591f\u751f\u6210\u4e00\u7c7b\u65b0\u7684\u5e7f\u6cdb\u4e14\u7075\u6d3b\u7684MD\u548c\u65e0\u955c\u50cfMD\u66f4\u65b0\u89c4\u5219\u3002"}}
{"id": "2506.13989", "pdf": "https://arxiv.org/pdf/2506.13989", "abs": "https://arxiv.org/abs/2506.13989", "authors": ["Johan \u00d6stman", "Edvin Callisen", "Anton Chen", "Kristiina Ausmees", "Emanuel G\u00e5rdh", "Jovan Zamac", "Jolanta Goldsteine", "Hugo Wefer", "Simon Whelan", "Markus Reimeg\u00e5rd"], "title": "AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering", "categories": ["cs.SI", "cs.AI", "cs.DB", "cs.LG"], "comment": "21 figures, 22 pages", "summary": "Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.", "AI": {"tldr": "AMLgentex\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u53ef\u914d\u7f6e\u4ea4\u6613\u6570\u636e\u5e76\u6d4b\u8bd5\u53cd\u6d17\u94b1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u65e0\u6cd5\u6a21\u62df\u771f\u5b9e\u6d17\u94b1\u590d\u6742\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u6d17\u94b1\u6d3b\u52a8\u4f7f\u975e\u6cd5\u8d44\u91d1\u8fdb\u5165\u5408\u6cd5\u7ecf\u6d4e\uff0c\u6bcf\u5e74\u6d89\u53ca\u6570\u4e07\u4ebf\u7f8e\u5143\uff0c\u4f46\u4ec5\u6709\u6781\u5c11\u6570\u88ab\u63ed\u9732\u3002\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u65e0\u6cd5\u6a21\u62df\u771f\u5b9e\u6d17\u94b1\u7684\u590d\u6742\u6027\u548c\u6311\u6218\uff0c\u5982\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u3001\u7a00\u758f\u6807\u7b7e\u3001\u6218\u7565\u884c\u4e3a\u7b49\u3002", "method": "\u63d0\u51faAMLgentex\uff0c\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u4ea4\u6613\u6570\u636e\u5e76\u6d4b\u8bd5\u53cd\u6d17\u94b1\u65b9\u6cd5\u3002\u8be5\u5de5\u5177\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5982\u7f51\u7edc\u7ea7\u4f9d\u8d56\u6027\u548c\u65f6\u95f4\u52a8\u6001\u3002", "result": "AMLgentex\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u53cd\u6d17\u94b1\u7cfb\u7edf\uff0c\u5e76\u5728\u6a21\u62df\u590d\u6742\u5b9e\u9645\u573a\u666f\u7684\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "AMLgentex\u4e3a\u53cd\u6d17\u94b1\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u73b0\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u68c0\u6d4b\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "paper_title_zh": "AMLgentex\uff1a\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u7814\u7a76\u4ee5\u6253\u51fb\u6d17\u94b1\u6d3b\u52a8", "abstract_zh": "\u6d17\u94b1\u6d3b\u52a8\u901a\u8fc7\u4f7f\u975e\u6cd5\u8d44\u91d1\u8fdb\u5165\u5408\u6cd5\u7ecf\u6d4e\uff0c\u52a9\u957f\u4e86\u6709\u7ec4\u7ec7\u72af\u7f6a\u3002\u5c3d\u7ba1\u6bcf\u5e74\u6709\u6570\u4e07\u4ebf\u7f8e\u5143\u88ab\u6d17\u767d\uff0c\u4f46\u4ec5\u6709\u6781\u5c11\u6570\u88ab\u63ed\u9732\u3002\u8fd9\u6e90\u4e8e\u591a\u79cd\u56e0\u7d20\uff0c\u5305\u62ec\u6d17\u94b1\u8005\u7684\u6545\u610f\u89c4\u907f\u3001\u786e\u8ba4\u6848\u4f8b\u7684\u7a00\u7f3a\u6027\u4ee5\u53ca\u91d1\u878d\u673a\u6784\u5bf9\u5168\u7403\u4ea4\u6613\u7f51\u7edc\u7684\u6709\u9650\u53ef\u89c1\u6027\u3002\u867d\u7136\u5df2\u6709\u4e00\u4e9b\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f46\u5b83\u4eec\u672a\u80fd\u6a21\u62df\u771f\u5b9e\u6d17\u94b1\u7684\u7ed3\u6784\u548c\u884c\u4e3a\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u3001\u7a00\u758f\u548c\u4e0d\u786e\u5b9a\u6807\u7b7e\u3001\u6218\u7565\u884c\u4e3a\u3001\u65f6\u95f4\u52a8\u6001\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7f51\u7edc\u7ea7\u4f9d\u8d56\u6027\u7b49\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AMLgentex\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u903c\u771f\u3001\u53ef\u914d\u7f6e\u4ea4\u6613\u6570\u636e\u5e76\u6d4b\u8bd5\u68c0\u6d4b\u65b9\u6cd5\u7684\u5f00\u6e90\u5de5\u5177\u3002\u5b83\u80fd\u591f\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u53cd\u6d17\u94b1\u7cfb\u7edf\uff0c\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5982\u4f55\u5728\u53cd\u6620\u5b9e\u9645\u53cd\u6d17\u94b1\u573a\u666f\u590d\u6742\u6027\u7684\u6761\u4ef6\u4e0b\u4e25\u683c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2506.14002", "pdf": "https://arxiv.org/pdf/2506.14002", "abs": "https://arxiv.org/abs/2506.14002", "authors": ["Siyu Chen", "Heejune Sheen", "Xuyuan Xiong", "Tianhao Wang", "Zhuoran Yang"], "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.IT", "stat.ML"], "comment": "136 pages, 21 figures", "summary": "We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \\highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \\highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u591a\u4e49\u6027\u7279\u5f81\u7684\u6062\u590d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u8bad\u7ec3\u7b97\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u5b66\u4fdd\u8bc1\uff0c\u4e14\u5b58\u5728\u8d85\u53c2\u6570\u654f\u611f\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u7b49\u5b9e\u8df5\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548c\u65b0\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u5c06\u591a\u4e49\u6027\u7279\u5f81\u5efa\u6a21\u4e3a\u5e95\u5c42\u5355\u4e49\u6027\u6982\u5ff5\u7684\u7a00\u758f\u6df7\u5408\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u504f\u7f6e\u9002\u5e94\u201d\u7684SAE\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u6b63\u786e\u6062\u590d\u5355\u4e49\u6027\u7279\u5f81\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u5b9e\u8bc1\u53d8\u4f53\u2014\u2014\u7ec4\u504f\u7f6e\u9002\u5e94\uff08GBA\uff09\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u8f93\u5165\u6570\u636e\u7b26\u5408\u7edf\u8ba1\u6a21\u578b\u65f6\u80fd\u6b63\u786e\u6062\u590d\u6240\u6709\u5355\u4e49\u6027\u7279\u5f81\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGBA\u5728\u53c2\u6570\u89c4\u6a21\u8fbe15\u4ebf\u7684LLM\u4e2d\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3aSAE\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9996\u4e2a\u5177\u6709\u7406\u8bba\u6062\u590d\u4fdd\u8bc1\u7684\u7b97\u6cd5\uff0c\u63a8\u52a8\u4e86\u901a\u8fc7\u589e\u5f3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5f00\u53d1\u66f4\u900f\u660e\u3001\u53ef\u4fe1AI\u7cfb\u7edf\u7684\u8fdb\u7a0b\u3002", "paper_title_zh": "\u9a6f\u670dLLM\u4e2d\u7684\u591a\u4e49\u6027\uff1a\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u53ef\u8bc1\u660e\u7279\u5f81\u6062\u590d", "abstract_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5b9e\u73b0\u7406\u8bba\u652f\u6301\u7684\u7279\u5f81\u6062\u590d\u4ee5\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6311\u6218\u3002\u73b0\u6709SAE\u8bad\u7ec3\u7b97\u6cd5\u901a\u5e38\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u5b66\u4fdd\u8bc1\uff0c\u5e76\u5b58\u5728\u8d85\u53c2\u6570\u654f\u611f\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u7b49\u5b9e\u8df5\u9650\u5236\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7279\u5f81\u6062\u590d\u95ee\u9898\u7684\u65b0\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u4e49\u6027\u7279\u5f81\u5efa\u6a21\u4e3a\u5e95\u5c42\u5355\u4e49\u6027\u6982\u5ff5\u7684\u7a00\u758f\u6df7\u5408\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u53ef\u8bc6\u522b\u6027\u6982\u5ff5\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u504f\u7f6e\u9002\u5e94\u201d\u7684\u65b0SAE\u8bad\u7ec3\u7b97\u6cd5\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u504f\u7f6e\u53c2\u6570\u4ee5\u786e\u4fdd\u9002\u5f53\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u8f93\u5165\u6570\u636e\u91c7\u6837\u81ea\u6240\u63d0\u7edf\u8ba1\u6a21\u578b\u65f6\u80fd\u6b63\u786e\u6062\u590d\u6240\u6709\u5355\u4e49\u6027\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5b9e\u8bc1\u53d8\u4f53\u2014\u2014\u7ec4\u504f\u7f6e\u9002\u5e94\uff08GBA\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5e94\u7528\u4e8e\u53c2\u6570\u89c4\u6a21\u8fbe15\u4ebf\u7684LLM\u65f6\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u9996\u4e2a\u5177\u6709\u7406\u8bba\u6062\u590d\u4fdd\u8bc1\u7684SAE\u7b97\u6cd5\uff0c\u4e3a\u63ed\u5f00SAE\u8bad\u7ec3\u7684\u795e\u79d8\u9762\u7eb1\u8fc8\u51fa\u4e86\u57fa\u7840\u6027\u4e00\u6b65\uff0c\u4ece\u800c\u901a\u8fc7\u589e\u5f3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u63a8\u52a8\u4e86\u66f4\u900f\u660e\u3001\u53ef\u4fe1AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.14020", "pdf": "https://arxiv.org/pdf/2506.14020", "abs": "https://arxiv.org/abs/2506.14020", "authors": ["Keyue Jiang", "Jiahao Cui", "Xiaowen Dong", "Laura Toni"], "title": "Bures-Wasserstein Flow Matching for Graph Generation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBures-Wasserstein\u8ddd\u79bb\u7684\u6d41\u5339\u914d\u6846\u67b6BWFlow\uff0c\u7528\u4e8e\u56fe\u751f\u6210\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u6a21\u8282\u70b9\u548c\u8fb9\u7684\u8054\u5408\u6f14\u5316\uff0c\u5229\u7528\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u8868\u793a\u56fe\uff0c\u5e76\u8bbe\u8ba1\u6982\u7387\u8def\u5f84\u4ee5\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u666e\u901a\u56fe\u751f\u6210\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u6a21\u578b\u7684\u56fe\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6570\u636e\u4f4d\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u72ec\u7acb\u5efa\u6a21\u8282\u70b9\u548c\u8fb9\u7684\u6f14\u5316\uff0c\u5ffd\u7565\u4e86\u56fe\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\u548c\u4e92\u8054\u6a21\u5f0f\uff0c\u53ef\u80fd\u5bfc\u81f4\u91c7\u6837\u6536\u655b\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u6982\u7387\u8def\u5f84\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u63d0\u51faBWFlow\u6846\u67b6\uff0c\u5c06\u56fe\u8868\u793a\u4e3a\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff08MRF\uff09\u53c2\u6570\u5316\u7684\u8fde\u63a5\u7cfb\u7edf\uff0c\u5229\u7528MRF\u5bf9\u8c61\u4e4b\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u4f4d\u79fb\u8bbe\u8ba1\u6982\u7387\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u8fde\u7eed\u548c\u79bb\u6563\u6d41\u5339\u914d\u7b97\u6cd5\uff0c\u5e76\u5c0a\u91cd\u56fe\u7684\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBWFlow\u5728\u666e\u901a\u56fe\u751f\u6210\u548c2D/3D\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u7ade\u4e89\u529b\u5f3a\u7684\u6027\u80fd\u3001\u7a33\u5b9a\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u53ef\u4fdd\u8bc1\u7684\u91c7\u6837\u6536\u655b\u6027\u3002", "conclusion": "BWFlow\u901a\u8fc7\u5efa\u6a21\u56fe\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\u548c\u8054\u5408\u6f14\u5316\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u5206\u5b50\u8bbe\u8ba1\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u4e8eBures-Wasserstein\u6d41\u5339\u914d\u7684\u56fe\u751f\u6210\u65b9\u6cd5", "abstract_zh": "\u56fe\u751f\u6210\u5df2\u6210\u4e3a\u4ece\u5206\u5b50\u8bbe\u8ba1\u5230\u836f\u7269\u53d1\u73b0\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5173\u952e\u4efb\u52a1\u3002\u5f53\u524d\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5982\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u53c2\u8003\u5206\u5e03\u4e0e\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684\u6982\u7387\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u8f83\u597d\u7684\u751f\u6210\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5efa\u6a21\u8282\u70b9\u548c\u8fb9\u7684\u6f14\u5316\uff0c\u5e76\u5047\u8bbe\u6570\u636e\u4f4d\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u4f7f\u7528\u7ebf\u6027\u63d2\u503c\u6784\u5efa\u8def\u5f84\u3002\u6211\u4eec\u6307\u51fa\uff0c\u8fd9\u79cd\u505a\u6cd5\u5ffd\u7565\u4e86\u56fe\u7684\u56fa\u6709\u975e\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\u548c\u4e92\u8054\u6a21\u5f0f\uff0c\u53ef\u80fd\u5bfc\u81f4\u91c7\u6837\u6536\u655b\u6027\u95ee\u9898\u3002\u4e3a\u6784\u5efa\u66f4\u4f18\u7684\u6982\u7387\u8def\u5f84\uff0c\u6211\u4eec\u5c06\u56fe\u8868\u793a\u4e3a\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff08MRF\uff09\u53c2\u6570\u5316\u7684\u8fde\u63a5\u7cfb\u7edf\uff0c\u5e76\u5229\u7528MRF\u5bf9\u8c61\u4e4b\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u4f4d\u79fb\u8bbe\u8ba1\u8def\u5f84\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86BWFlow\uff0c\u4e00\u79cd\u5c0a\u91cd\u56fe\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u5e76\u63d0\u4f9b\u5e73\u6ed1\u6982\u7387\u8def\u5f84\u901f\u5ea6\u7684\u6d41\u5339\u914d\u6846\u67b6\u3002\u8be5\u6846\u67b6\u53ef\u9002\u914d\u8fde\u7eed\u548c\u79bb\u6563\u6d41\u5339\u914d\u7b97\u6cd5\u3002\u5728\u666e\u901a\u56fe\u751f\u6210\u548c2D/3D\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BWFlow\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u4f18\u8d8a\u3001\u8bad\u7ec3\u7a33\u5b9a\u4e14\u91c7\u6837\u6536\u655b\u6027\u6709\u4fdd\u969c\u3002"}}
{"id": "2506.14042", "pdf": "https://arxiv.org/pdf/2506.14042", "abs": "https://arxiv.org/abs/2506.14042", "authors": ["Bernardo Subercaseaux"], "title": "Asymptotically Smaller Encodings for Graph Problems and Scheduling", "categories": ["cs.LO", "cs.AI", "cs.DS"], "comment": null, "summary": "We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \\lg |V|)$ many clauses, as opposed to the $\u03a9(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erd\u0151s, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of \"Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \\lg |V|)$ clauses (the direct encoding uses $\u03a9(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \\lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u591a\u4e2a\u56fe\u95ee\u9898\uff08\u5982\u9876\u70b9\u8986\u76d6\u3001\u72ec\u7acb\u96c6\u3001$k$-\u7740\u8272\uff09\u7f16\u7801\u4e3aCNF\uff0c\u4ec5\u9700$O(|V|^2 / \\lg |V|)$\u5b50\u53e5\uff0c\u800c\u975e\u4f20\u7edf\u7f16\u7801\u7684$\u03a9(|V|^2)$\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7a20\u5bc6\u533a\u95f4\u56fe\u4e2d\u72ec\u7acb\u96c6\u7684\u65b0\u7f16\u7801\u65b9\u6cd5\uff0c\u4ec5\u9700$O(|V| \\lg |V|)$\u5b50\u53e5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b57\u7b26\u4e32\u538b\u7f29\u7f16\u7801\u548c\u8c03\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7f16\u7801\u89c4\u6a21\u3002", "motivation": "\u4f20\u7edf\u56fe\u95ee\u9898\u7f16\u7801\u65b9\u6cd5\u901a\u5e38\u9700\u8981$\u03a9(|V|^2)$\u7ea6\u675f\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u65b9\u5f0f\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u4e3a\u7406\u89e3\u201c\u6709\u754c\u53d8\u91cf\u6dfb\u52a0\u201d\u9884\u5904\u7406\u5de5\u5177\u7684\u6210\u529f\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u5229\u7528Erd\u0151s\u3001Chung\u548cSpencer\uff081983\uff09\u5173\u4e8e\u56fe\u7684\u4e8c\u5206\u8986\u76d6\u7684\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CNF\u7f16\u7801\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u7a20\u5bc6\u533a\u95f4\u56fe\u4e2d\u7684\u72ec\u7acb\u96c6\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4ec5\u9700$O(|V| \\lg |V|)$\u5b50\u53e5\u7684\u7f16\u7801\u65b9\u6cd5\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u4e2a\u56fe\u95ee\u9898\u7684\u7f16\u7801\u89c4\u6a21\u4ece$\u03a9(|V|^2)$\u964d\u81f3$O(|V|^2 / \\lg |V|)$\u3002\u5bf9\u4e8e\u7a20\u5bc6\u533a\u95f4\u56fe\u4e2d\u7684\u72ec\u7acb\u96c6\u95ee\u9898\uff0c\u7f16\u7801\u89c4\u6a21\u8fdb\u4e00\u6b65\u964d\u81f3$O(|V| \\lg |V|)$\u3002\u6b64\u5916\uff0c\u8c03\u5ea6\u95ee\u9898\u7684\u7f16\u7801\u89c4\u6a21\u4ece$O(NMT^2)$\u51cf\u5c11\u5230$O(NMT + M T^2 \\lg T)$\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7f16\u7801\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u56fe\u95ee\u9898\u548c\u8c03\u5ea6\u95ee\u9898\u7684\u7f16\u7801\u89c4\u6a21\uff0c\u4e3a\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u540c\u65f6\uff0c\u4e3a\u7406\u89e3\u9884\u5904\u7406\u5de5\u5177\u7684\u6210\u529f\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002", "paper_title_zh": "\u56fe\u95ee\u9898\u4e0e\u8c03\u5ea6\u95ee\u9898\u7684\u6e10\u8fd1\u66f4\u5c0f\u7f16\u7801", "abstract_zh": "\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u591a\u4e2a\u56fe\u95ee\u9898\uff08\u5982\u9876\u70b9\u8986\u76d6\u3001\u72ec\u7acb\u96c6\u3001$k$-\u7740\u8272\uff09\u7f16\u7801\u4e3aCNF\uff0c\u4ec5\u9700$O(|V|^2 / \\lg |V|)$\u5b50\u53e5\uff0c\u800c\u975e\u4f20\u7edf\u7f16\u7801\u7684$\u03a9(|V|^2)$\u7ea6\u675f\u3002\u8fd9\u4e00\u7ed3\u679c\u6e90\u4e8eErd\u0151s\u3001Chung\u548cSpencer\uff081983\uff09\u5173\u4e8e\u56fe\u7684\u4e8c\u5206\u8986\u76d6\u7684\u7814\u7a76\uff0c\u5e76\u4e3a\u7406\u89e3\u201c\u6709\u754c\u53d8\u91cf\u6dfb\u52a0\u201d\uff08Manthey\u3001Heule\u548cBiere\uff0c2012\uff09\u9884\u5904\u7406\u5de5\u5177\u7684\u6210\u529f\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7a20\u5bc6\u533a\u95f4\u56fe\u4e2d\u72ec\u7acb\u96c6\u7684\u65b0\u7f16\u7801\u65b9\u6cd5\uff0c\u4ec5\u9700$O(|V| \\lg |V|)$\u5b50\u53e5\uff08\u76f4\u63a5\u7f16\u7801\u9700$\u03a9(|V|^2)$\uff09\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eBannai\u7b49\u4eba\uff082022\uff09\u63d0\u51fa\u7684\u5b57\u7b26\u4e32\u538b\u7f29\u7f16\u7801\u3002\u4f5c\u4e3a\u76f4\u63a5\u6210\u679c\uff0c\u6211\u4eec\u8fd8\u5c06Mayank\u548cModal\uff082020\uff09\u63d0\u51fa\u7684\u8c03\u5ea6\u95ee\u9898\u7684\u7f16\u7801\u89c4\u6a21\u4ece$O(NMT^2)$\u51cf\u5c11\u5230$O(NMT + M T^2 \\lg T)$\uff0c\u5176\u4e2d$N$\u4e3a\u4efb\u52a1\u6570\uff0c$T$\u4e3a\u603b\u65f6\u95f4\u8de8\u5ea6\uff0c$M$\u4e3a\u673a\u5668\u6570\u3002"}}
{"id": "2506.14054", "pdf": "https://arxiv.org/pdf/2506.14054", "abs": "https://arxiv.org/abs/2506.14054", "authors": ["Joshua Fan", "Haodi Xu", "Feng Tao", "Md Nasim", "Marc Grimson", "Yiqi Luo", "Carla P. Gomes"], "title": "Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, submitted to NeurIPS 2025", "summary": "Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aScIReN\u7684\u900f\u660e\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u63a8\u7406\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u63ed\u793a\u79d1\u5b66\u673a\u5236\u7684\u95ee\u9898\u3002ScIReN\u901a\u8fc7\u9884\u6d4b\u79d1\u5b66\u610f\u4e49\u7684\u6f5c\u5728\u53c2\u6570\uff0c\u5e76\u5229\u7528\u53ef\u5fae\u5206\u7684\u8fc7\u7a0b\u89e3\u7801\u5668\u8f93\u51fa\u7ed3\u679c\uff0c\u540c\u65f6\u901a\u8fc7\u786cSigmoid\u7ea6\u675f\u5c42\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cScIReN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u79d1\u5b66\u89e3\u91ca\u6027\u4e0a\u5747\u4f18\u4e8e\u9ed1\u76d2\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u80fd\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6a21\u5f0f\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u65e0\u6cd5\u63ed\u793a\u79d1\u5b66\u89c4\u5f8b\u6216\u65b0\u673a\u5236\u3002\u800c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u6a21\u578b\u867d\u7136\u80fd\u5b9a\u91cf\u89e3\u91ca\u79d1\u5b66\u539f\u7406\uff0c\u4f46\u4f9d\u8d56\u5927\u91cf\u81ea\u7531\u53c2\u6570\u4e14\u9884\u6d4b\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4f18\u52bf\uff0c\u53c8\u80fd\u4fdd\u6301\u79d1\u5b66\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faScIReN\u6846\u67b6\uff0c\u5305\u542b\u53ef\u89e3\u91ca\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u89e3\u7801\u5668\u3002\u7f16\u7801\u5668\u9884\u6d4b\u79d1\u5b66\u610f\u4e49\u7684\u6f5c\u5728\u53c2\u6570\uff0c\u89e3\u7801\u5668\u901a\u8fc7\u53ef\u5fae\u5206\u8fc7\u7a0b\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u3002\u5f15\u5165\u786cSigmoid\u7ea6\u675f\u5c42\uff0c\u9650\u5236\u6f5c\u5728\u53c2\u6570\u5728\u79d1\u5b66\u5148\u9a8c\u77e5\u8bc6\u5b9a\u4e49\u7684\u8303\u56f4\u5185\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "ScIReN\u5728\u6a21\u62df\u571f\u58e4\u6709\u673a\u78b3\u6d41\u52a8\u548c\u751f\u6001\u7cfb\u7edf\u547c\u5438\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u4f18\u4e8e\u9ed1\u76d2\u6a21\u578b\uff0c\u5e76\u80fd\u63a8\u65ad\u6f5c\u5728\u79d1\u5b66\u673a\u5236\u53ca\u5176\u4e0e\u8f93\u5165\u7279\u5f81\u7684\u5173\u7cfb\u3002", "conclusion": "ScIReN\u6210\u529f\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u8fc7\u7a0b\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u79d1\u5b66\u89e3\u91ca\u6027\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "paper_title_zh": "\u79d1\u5b66\u53ef\u89e3\u91ca\u63a8\u7406\u7f51\u7edc\uff08ScIReN\uff09\uff1a\u63ed\u5f00\u81ea\u7136\u7684\u9ed1\u76d2", "abstract_zh": "\u795e\u7ecf\u7f51\u7edc\u662f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6a21\u5f0f\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u7531\u4e8e\u5176\u9ed1\u76d2\u7279\u6027\uff0c\u5b83\u4eec\u65e0\u6cd5\u9075\u5faa\u5df2\u77e5\u79d1\u5b66\u5b9a\u5f8b\uff0c\u4e5f\u65e0\u6cd5\u63ed\u793a\u65b0\u7684\u79d1\u5b66\u89c1\u89e3\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u79d1\u5b66\u63a8\u7406\u901a\u8fc7\u89c2\u5bdf\u548c\u53d7\u63a7\u5b9e\u9a8c\u63d0\u70bc\u751f\u7269\u6216\u7269\u7406\u539f\u7406\uff0c\u5e76\u7528\u57fa\u4e8e\u6570\u5b66\u65b9\u7a0b\u7684\u8fc7\u7a0b\u6a21\u578b\u5b9a\u91cf\u89e3\u91ca\u8fd9\u4e9b\u539f\u7406\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u81ea\u7531\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u901a\u5e38\u9700\u8981\u4e34\u65f6\u8bbe\u7f6e\uff0c\u56e0\u6b64\u5728\u8de8\u5c3a\u5ea6\u9884\u6d4b\u4e2d\u5f80\u5f80\u62df\u5408\u6548\u679c\u4e0d\u4f73\u3002\u5c3d\u7ba1\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u5c06\u57fa\u4e8e\u8fc7\u7a0b\u7684\u6a21\u578b\u5d4c\u5165\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u53d1\u73b0\u57fa\u4e8e\u8fc7\u7a0b\u6a21\u578b\u4e2d\u53c2\u6570\u4e0e\u8f93\u5165\u7279\u5f81\u4e4b\u95f4\u7684\u53ef\u89e3\u91ca\u5173\u7cfb\u4ecd\u662f\u79d1\u5b66\u53d1\u73b0\u7684\u91cd\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u79d1\u5b66\u53ef\u89e3\u91ca\u63a8\u7406\u7f51\u7edc\uff08ScIReN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b8c\u5168\u900f\u660e\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u63a8\u7406\u3002\u53ef\u89e3\u91ca\u7f16\u7801\u5668\u9884\u6d4b\u5177\u6709\u79d1\u5b66\u610f\u4e49\u7684\u6f5c\u5728\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u968f\u540e\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u8fc7\u7a0b\u89e3\u7801\u5668\u9884\u6d4b\u6807\u8bb0\u7684\u8f93\u51fa\u53d8\u91cf\u3002ScIReN\u8fd8\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u786cSigmoid\u7ea6\u675f\u5c42\uff0c\u5c06\u6f5c\u5728\u53c2\u6570\u9650\u5236\u5728\u79d1\u5b66\u5148\u9a8c\u77e5\u8bc6\u5b9a\u4e49\u7684\u8303\u56f4\u5185\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002\u5d4c\u5165\u7684\u57fa\u4e8e\u8fc7\u7a0b\u6a21\u578b\u5f3a\u5236\u6267\u884c\u5df2\u786e\u7acb\u7684\u79d1\u5b66\u77e5\u8bc6\uff0c\u800c\u7f16\u7801\u5668\u5219\u63ed\u793a\u4e86\u4f20\u7edf\u9ed1\u76d2\u6a21\u578b\u4e2d\u9690\u85cf\u7684\u65b0\u79d1\u5b66\u673a\u5236\u548c\u5173\u7cfb\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\u5e94\u7528\u4e86ScIReN\uff1a\u6a21\u62df\u571f\u58e4\u4e2d\u6709\u673a\u78b3\u7684\u6d41\u52a8\uff0c\u4ee5\u53ca\u5efa\u6a21\u690d\u7269\u751f\u6001\u7cfb\u7edf\u547c\u5438\u3002\u5728\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e2d\uff0cScIReN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u9ed1\u76d2\u7f51\u7edc\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u79d1\u5b66\u89e3\u91ca\u6027\u2014\u2014\u5b83\u80fd\u591f\u63a8\u65ad\u6f5c\u5728\u79d1\u5b66\u673a\u5236\u53ca\u5176\u4e0e\u8f93\u5165\u7279\u5f81\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.14098", "pdf": "https://arxiv.org/pdf/2506.14098", "abs": "https://arxiv.org/abs/2506.14098", "authors": ["Ziyuan Tang", "Jie Chen"], "title": "Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u56fe\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5176\u5728\u56fe\u7ed3\u6784\u6570\u636e\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u8bed\u8a00\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\uff08\u5982GPT\uff09\u5df2\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u4f46\u56fe\u6570\u636e\u9886\u57df\u5c1a\u672a\u6709\u7c7b\u4f3c\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6784\u5efa\u4e00\u4e2a\u901a\u7528\u7684\u56fe\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u56fe\u6570\u636e\u5904\u7406\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u5c06\u8282\u70b9\u8868\u793a\u4e3a\u591a\u6761\u968f\u673a\u6e38\u8d70\u5e8f\u5217\uff0c\u5229\u7528Transformer\u4ece\u5e8f\u5217\u4e2d\u63d0\u53d6\u8282\u70b9\u8868\u793a\uff0c\u8fdb\u800c\u751f\u6210\u8fb9\u548c\u56fe\u8868\u793a\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u635f\u5931\u51fd\u6570\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86\u968f\u673a\u6e38\u8d70\u5728\u533a\u5206\u90bb\u57df\u548c\u56fe\u7ed3\u6784\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u6210\u529f\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\u5904\u7406\u57fa\u7840\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u56fe\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u56fe\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "paper_title_zh": "\u8fc8\u5411\u56fe\u57fa\u7840\u6a21\u578b\uff1a\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684Transformer\u9884\u8bad\u7ec3", "abstract_zh": "\u50cfGPT\u8fd9\u6837\u7684\u57fa\u7840\u6a21\u578b\u56e0\u5176\u5e7f\u6cdb\u7684\u6570\u636e\u9884\u8bad\u7ec3\u548c\u5f3a\u5927\u7684Transformer\u67b6\u6784\u800c\u5c55\u73b0\u51fa\u8bb8\u591a\u6d8c\u73b0\u80fd\u529b\u3002\u5c3d\u7ba1\u81ea\u7136\u8bed\u8a00\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u5df2\u5f88\u666e\u904d\uff0c\u4f46\u6211\u4eec\u80fd\u5426\u4e3a\u56fe\u6570\u636e\u6784\u5efa\u7c7b\u4f3c\u7684\u6a21\u578b\uff1f\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6837\u5316\u56fe\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u56fe\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574Transformer\u67b6\u6784\u5b9e\u73b0\u3002\u5176\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u7528\u5e8f\u5217\u6a21\u578b\u7f16\u7801\u4e0d\u540c\u89c4\u6a21\u548c\u9886\u57df\u7684\u56fe\u3002\u6211\u4eec\u63d0\u51fa\u5c06\u8282\u70b9\u8868\u793a\u4e3a\u591a\u6761\u968f\u673a\u6e38\u8d70\u5e8f\u5217\uff0c\u4f7f\u5f97Transformer\u53ef\u4ee5\u4ece\u5e8f\u5217\u4e2d\u63d0\u53d6\u8282\u70b9\u8868\u793a\uff0c\u8fdb\u800c\u751f\u6210\u8fb9\u548c\u56fe\u8868\u793a\u3002\u6211\u4eec\u4e3a\u8fd9\u4e9b\u968f\u673a\u6e38\u8d70\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5176\u5728\u533a\u5206\u90bb\u57df\u548c\u56fe\u7ed3\u6784\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u53ca\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\u5904\u7406\u548c\u63a8\u7406\u57fa\u7840\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14113", "pdf": "https://arxiv.org/pdf/2506.14113", "abs": "https://arxiv.org/abs/2506.14113", "authors": ["Yitian Zhang", "Liheng Ma", "Antonios Valkanas", "Boris N. Oreshkin", "Mark Coates"], "title": "SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.", "AI": {"tldr": "SKOLR\u662f\u4e00\u79cd\u57fa\u4e8eKoopman\u7b97\u5b50\u7406\u8bba\u548c\u7ebf\u6027RNN\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316Koopman\u7b97\u5b50\u548c\u591a\u5c42\u611f\u77e5\u5668\u5b9e\u73b0\u9ad8\u6548\u9884\u6d4b\u3002", "motivation": "Koopman\u7b97\u5b50\u7406\u8bba\u4e3a\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u5206\u6790\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u4f46\u5176\u65e0\u9650\u7ef4\u7279\u6027\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316Koopman\u7b97\u5b50\u4e0e\u7ebf\u6027RNN\u7684\u7b49\u6548\u6027\uff0c\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u63d0\u51faSKOLR\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u8f93\u5165\u4fe1\u53f7\u8c31\u5206\u89e3\u548c\u591a\u5c42\u611f\u77e5\u5668\u4f5c\u4e3a\u6d4b\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u9ad8\u5ea6\u5e76\u884c\u7684\u7ebf\u6027RNN\u5806\u6808\u5b9e\u73b0\u7ed3\u6784\u5316Koopman\u7b97\u5b50\u3002", "result": "\u5728\u591a\u79cd\u9884\u6d4b\u57fa\u51c6\u548c\u52a8\u6001\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSKOLR\u57fa\u4e8eKoopman\u7406\u8bba\u7684\u7b80\u5316\u8bbe\u8ba1\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "SKOLR\u901a\u8fc7\u7ed3\u5408Koopman\u7b97\u5b50\u548c\u7ebf\u6027RNN\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u5f02\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u3002", "paper_title_zh": "SKOLR\uff1a\u57fa\u4e8e\u7ed3\u6784\u5316Koopman\u7b97\u5b50\u7684\u7ebf\u6027RNN\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5", "abstract_zh": "Koopman\u7b97\u5b50\u7406\u8bba\u901a\u8fc7\u5c06\u52a8\u6001\u6620\u5c04\u5230\u5b9e\u503c\u6d4b\u91cf\u51fd\u6570\u7a7a\u95f4\uff0c\u4e3a\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u5206\u6790\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u7ebf\u6027\u7b97\u5b50\u8868\u793a\u6846\u67b6\u3002\u5c3d\u7ba1\u7ebf\u6027\u5316\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u7b97\u5b50\u901a\u5e38\u662f\u65e0\u9650\u7ef4\u7684\u3002\u56e0\u6b64\uff0c\u76ee\u6807\u662f\u5b66\u4e60\u80fd\u591f\u4ea7\u751f\u53ef\u5904\u7406\u7684\u6709\u9650\u7ef4Koopman\u7b97\u5b50\u8fd1\u4f3c\u7684\u6d4b\u91cf\u51fd\u6570\u3002\u672c\u6587\u5efa\u7acb\u4e86Koopman\u7b97\u5b50\u8fd1\u4f3c\u4e0e\u7ebf\u6027\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u540e\u8005\u6700\u8fd1\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6210\u529f\u3002\u901a\u8fc7\u8003\u8651\u7531\u6ede\u540e\u89c2\u6d4b\u7ec4\u6210\u7684\u6269\u5c55\u72b6\u6001\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u7ed3\u6784\u5316Koopman\u7b97\u5b50\u4e0e\u7ebf\u6027RNN\u66f4\u65b0\u7684\u7b49\u6548\u6027\u3002\u57fa\u4e8e\u8fd9\u4e00\u8054\u7cfb\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SKOLR\uff0c\u5b83\u5c06\u8f93\u5165\u4fe1\u53f7\u7684\u53ef\u5b66\u4e60\u8c31\u5206\u89e3\u4e0e\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u4f5c\u4e3a\u6d4b\u91cf\u51fd\u6570\u76f8\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u9ad8\u5ea6\u5e76\u884c\u7684\u7ebf\u6027RNN\u5806\u6808\u5b9e\u73b0\u7ed3\u6784\u5316Koopman\u7b97\u5b50\u3002\u5728\u591a\u79cd\u9884\u6d4b\u57fa\u51c6\u548c\u52a8\u6001\u7cfb\u7edf\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u57fa\u4e8eKoopman\u7406\u8bba\u7684\u7b80\u5316\u8bbe\u8ba1\u5177\u6709\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.14122", "pdf": "https://arxiv.org/pdf/2506.14122", "abs": "https://arxiv.org/abs/2506.14122", "authors": ["Tianming Zhang", "Renbo Zhang", "Zhengyi Yang", "Yunjun Gao", "Bin Cao", "Jing Fan"], "title": "CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\\times$ lower MAE and 16.7~$\\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\\times$ lower MAE and 3.9~$\\times$ higher Spearman correlation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578bCLGNN\uff0c\u7528\u4e8e\u9884\u6d4b\u65f6\u5e8f\u56fe\u4e2d\u7684\u4ecb\u6570\u4e2d\u5fc3\u6027\uff08TBC\uff09\u3002CLGNN\u901a\u8fc7\u6784\u5efa\u5b9e\u4f8b\u56fe\u4fdd\u7559\u8def\u5f84\u6709\u6548\u6027\u548c\u65f6\u5e8f\u987a\u5e8f\uff0c\u5e76\u5229\u7528\u53cc\u805a\u5408\u673a\u5236\u7f16\u7801\u7ed3\u6784\u548c\u65f6\u5e8f\u7279\u5f81\u3002\u901a\u8fc7\u5f15\u5165\u7a33\u5b9a\u6027\u805a\u7c7b\u5f15\u5bfc\u7684\u5bf9\u6bd4\u6a21\u5757\uff08KContrastNet\uff09\u548c\u56de\u5f52\u6a21\u5757\uff08ValueNet\uff09\uff0cCLGNN\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u65f6\u5e8f\u4ecb\u6570\u4e2d\u5fc3\u6027\uff08TBC\uff09\u662f\u8861\u91cf\u8282\u70b9\u5728\u65f6\u5e8f\u7f51\u7edc\u4e2d\u91cd\u8981\u6027\u7684\u6307\u6807\uff0c\u4f46\u5176\u7cbe\u786e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u771f\u5b9e\u6570\u636e\u5206\u5e03\u6781\u4e0d\u5e73\u8861\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u4e0d\u5e73\u8861\u6216\u5ffd\u7565\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CLGNN\u901a\u8fc7\u6784\u5efa\u5b9e\u4f8b\u56fe\u4fdd\u7559\u8def\u5f84\u6709\u6548\u6027\u548c\u65f6\u5e8f\u987a\u5e8f\uff0c\u5e76\u91c7\u7528\u53cc\u805a\u5408\u673a\u5236\uff08\u5747\u503c\u805a\u5408\u548c\u8fb9\u5230\u8282\u70b9\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff09\u7f16\u7801\u7ed3\u6784\u548c\u65f6\u5e8f\u7279\u5f81\u3002\u5f15\u5165KContrastNet\u6a21\u5757\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5206\u79bb\u9ad8\u3001\u4e2d\u3001\u4f4e\u4e2d\u5fc3\u6027\u8282\u70b9\uff0c\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff1bValueNet\u6a21\u5757\u7528\u4e8e\u56de\u5f52\u9884\u6d4bTBC\u503c\u3002\u6b64\u5916\uff0cCLGNN\u652f\u6301\u591a\u79cd\u6700\u4f18\u8def\u5f84\u5b9a\u4e49\u4ee5\u9002\u5e94\u4e0d\u540c\u65f6\u5e8f\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCLGNN\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u4e0e\u73b0\u6709\u7cbe\u786e\u8ba1\u7b97\u65b9\u6cd5\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe663.7\u500d\uff1b\u4e0e\u9759\u6001GNN\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e31.4\u500d\uff0cSpearman\u76f8\u5173\u6027\u63d0\u9ad816.7\u500d\uff1b\u4e0e\u73b0\u6709\u65f6\u5e8fGNN\u76f8\u6bd4\uff0cMAE\u964d\u4f4e5.7\u500d\uff0cSpearman\u76f8\u5173\u6027\u63d0\u9ad83.9\u500d\u3002", "conclusion": "CLGNN\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u53cc\u805a\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u4ecb\u6570\u4e2d\u5fc3\u6027\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u65f6\u5e8f\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u65f6\u5e8f\u8bed\u4e49\u573a\u666f\u3002", "paper_title_zh": "CLGNN\uff1a\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7528\u4e8e\u65f6\u5e8f\u56fe\u4ecb\u6570\u4e2d\u5fc3\u6027\u9884\u6d4b", "abstract_zh": "\u65f6\u5e8f\u4ecb\u6570\u4e2d\u5fc3\u6027\uff08TBC\uff09\u8861\u91cf\u8282\u70b9\u5728\u6700\u4f18\u65f6\u5e8f\u8def\u5f84\u4e0a\u51fa\u73b0\u7684\u9891\u7387\uff0c\u53cd\u6620\u5176\u5728\u65f6\u5e8f\u7f51\u7edc\u4e2d\u7684\u91cd\u8981\u6027\u3002\u7136\u800c\uff0c\u7cbe\u786e\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u4e14\u771f\u5b9eTBC\u5206\u5e03\u6781\u4e0d\u5e73\u8861\u3002\u8fd9\u79cd\u4e25\u91cd\u4e0d\u5e73\u8861\u5bfc\u81f4\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u5bf9\u96f6\u4e2d\u5fc3\u6027\u8282\u70b9\u8fc7\u62df\u5408\uff0c\u9884\u6d4b\u4e0d\u51c6\u786e\u4e14\u65e0\u6cd5\u8bc6\u522b\u771f\u6b63\u91cd\u8981\u7684\u8282\u70b9\u3002\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u4e0d\u5e73\u8861\uff0c\u8981\u4e48\u5b8c\u5168\u5ffd\u7565\u65f6\u5e8f\u4f9d\u8d56\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5f52\u7eb3\u7684\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684GNN\u6a21\u578b\uff08CLGNN\uff09\uff0c\u7528\u4e8e\u51c6\u786e\u9ad8\u6548\u5730\u9884\u6d4bTBC\u3002CLGNN\u6784\u5efa\u5b9e\u4f8b\u56fe\u4ee5\u4fdd\u7559\u8def\u5f84\u6709\u6548\u6027\u548c\u65f6\u5e8f\u987a\u5e8f\uff0c\u5e76\u901a\u8fc7\u53cc\u805a\u5408\u673a\u5236\uff08\u5747\u503c\u805a\u5408\u548c\u8fb9\u5230\u8282\u70b9\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff09\u7ed3\u5408\u65f6\u5e8f\u8def\u5f84\u8ba1\u6570\u548c\u65f6\u95f4\u7f16\u7801\uff0c\u7f16\u7801\u7ed3\u6784\u548c\u65f6\u5e8f\u7279\u5f81\u3002\u5f15\u5165\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u805a\u7c7b\u5f15\u5bfc\u5bf9\u6bd4\u6a21\u5757\uff08KContrastNet\uff09\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5206\u79bb\u9ad8\u3001\u4e2d\u3001\u4f4e\u4e2d\u5fc3\u6027\u8282\u70b9\uff0c\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u56de\u5f52\u6a21\u5757\uff08ValueNet\uff09\u7528\u4e8e\u4f30\u8ba1TBC\u503c\u3002CLGNN\u8fd8\u652f\u6301\u591a\u79cd\u6700\u4f18\u8def\u5f84\u5b9a\u4e49\u4ee5\u9002\u5e94\u4e0d\u540c\u65f6\u5e8f\u8bed\u4e49\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86CLGNN\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002\u4e0e\u73b0\u6709\u7cbe\u786eTBC\u8ba1\u7b97\u65b9\u6cd5\u76f8\u6bd4\uff0cCLGNN\u5b9e\u73b0\u4e86\u9ad8\u8fbe663.7\u500d\u7684\u52a0\u901f\uff1b\u4e0e\u9886\u5148\u7684\u9759\u6001GNN\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e31.4\u500d\uff0cSpearman\u76f8\u5173\u6027\u63d0\u9ad816.7\u500d\uff1b\u4e0e\u73b0\u6709\u65f6\u5e8fGNN\u76f8\u6bd4\uff0cMAE\u964d\u4f4e5.7\u500d\uff0cSpearman\u76f8\u5173\u6027\u63d0\u9ad83.9\u500d\u3002"}}
{"id": "2506.14126", "pdf": "https://arxiv.org/pdf/2506.14126", "abs": "https://arxiv.org/abs/2506.14126", "authors": ["Stefan Horoi", "Guy Wolf", "Eugene Belilovsky", "Gintare Karolina Dziugaite"], "title": "Less is More: Undertraining Experts Improves Model Upcycling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e13\u5bb6\u6a21\u578b\u7684\u8fc7\u5ea6\u5fae\u8c03\u4f1a\u635f\u5bb3\u6a21\u578b\u5347\u7ea7\u6027\u80fd\uff0c\u800c\u91c7\u7528\u4efb\u52a1\u4f9d\u8d56\u7684\u65e9\u671f\u505c\u6b62\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u5347\u7ea7\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5fae\u8c03\u4e13\u5bb6\u6a21\u578b\uff0c\u4f46\u666e\u904d\u5047\u8bbe\u5fae\u8c03\u9636\u6bb5\u7684\u6539\u8fdb\u4f1a\u81ea\u7136\u63d0\u5347\u540e\u7eed\u6a21\u578b\u5347\u7ea7\u6027\u80fd\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u8ba8\u4e13\u5bb6\u5fae\u8c03\u5bf9\u6a21\u578b\u5347\u7ea7\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e13\u5bb6\u6a21\u578b\u957f\u65f6\u95f4\u5fae\u8c03\u5bf9\u6a21\u578b\u5347\u7ea7\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u671f\u5bf9\u56f0\u96be\u6837\u672c\u7684\u8fc7\u62df\u5408\u662f\u6027\u80fd\u4e0b\u964d\u7684\u4e3b\u56e0\uff0c\u5e76\u63d0\u51fa\u4efb\u52a1\u4f9d\u8d56\u7684\u65e9\u671f\u505c\u6b62\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fc7\u5ea6\u5fae\u8c03\u7684\u4e13\u5bb6\u6a21\u578b\u5728\u5347\u7ea7\u540e\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u65e9\u671f\u505c\u6b62\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u5347\u7ea7\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728LoRA\u9002\u914d\u5668\u5347\u7ea7\u4e3aMoE\u5c42\u65f6\u3002", "conclusion": "\u4e13\u5bb6\u6a21\u578b\u7684\u5fae\u8c03\u9700\u8c28\u614e\uff0c\u907f\u514d\u8fc7\u5ea6\u4f18\u5316\u5355\u4efb\u52a1\u6027\u80fd\uff0c\u4efb\u52a1\u4f9d\u8d56\u7684\u65e9\u671f\u505c\u6b62\u7b56\u7565\u662f\u63d0\u5347\u6a21\u578b\u5347\u7ea7\u6548\u679c\u7684\u6709\u6548\u65b9\u6cd5\u3002", "paper_title_zh": "\u5c11\u5373\u662f\u591a\uff1a\u4e13\u5bb6\u6a21\u578b\u6b20\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u5347\u7ea7\u6027\u80fd", "abstract_zh": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5f00\u653e\u6743\u91cd\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u5728\u4e13\u7528\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u5bfc\u81f4\u4e13\u5bb6\u6a21\u578b\u548c\u9002\u914d\u5668\u6fc0\u589e\uff0c\u5e76\u901a\u8fc7HuggingFace\u7b49\u5e73\u53f0\u5171\u4eab\u3002\u4e3a\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\uff0c\u6d8c\u73b0\u4e86\u8bb8\u591a\u6a21\u578b\u5347\u7ea7\u65b9\u6cd5\uff0c\u4f7f\u5fae\u8c03\u6a21\u578b\u80fd\u5728\u591a\u4efb\u52a1\u7cfb\u7edf\u4e2d\u590d\u7528\u3002\u4e00\u4e2a\u81ea\u7136\u7684\u6d41\u7a0b\u7531\u6b64\u5f62\u6210\uff1a\u6a21\u578b\u5728\u901a\u7528\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u5fae\u8c03\uff0c\u518d\u5347\u7ea7\u4e3a\u66f4\u901a\u7528\u7684\u7cfb\u7edf\u3002\u666e\u904d\u5047\u8bbe\u662f\u6d41\u7a0b\u4e2d\u67d0\u4e00\u9636\u6bb5\u7684\u6539\u8fdb\u4f1a\u4f20\u9012\u5230\u4e0b\u6e38\uff0c\u5e26\u6765\u540e\u7eed\u589e\u76ca\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u7814\u7a76\u4e13\u5bb6\u5fae\u8c03\u5bf9\u6a21\u578b\u5347\u7ea7\u7684\u5f71\u54cd\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u957f\u65f6\u95f4\u5fae\u8c03\u4e13\u5bb6\u6a21\u578b\u4ee5\u4f18\u5316\u5176\u4e2a\u4f53\u6027\u80fd\u4f1a\u5bfc\u81f4\u5408\u5e76\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u8bba\u662f\u5b8c\u5168\u5fae\u8c03\u8fd8\u662fLoRA\u9002\u914d\u6a21\u578b\uff0c\u4e14\u5728LoRA\u9002\u914d\u5668\u5347\u7ea7\u4e3aMoE\u5c42\u65f6\u4e0b\u6e38\u7ed3\u679c\u66f4\u5dee\u3002\u8fd9\u79cd\u9000\u5316\u6e90\u4e8e\u5fae\u8c03\u540e\u671f\u5bf9\u5c11\u91cf\u56f0\u96be\u6837\u672c\u7684\u8fc7\u62df\u5408\uff0c\u968f\u540e\u5728\u5408\u5e76\u65f6\u88ab\u9057\u5fd8\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4efb\u52a1\u4f9d\u8d56\u7684\u6fc0\u8fdb\u65e9\u671f\u505c\u6b62\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u5347\u7ea7\u6027\u80fd\u3002"}}
{"id": "2506.14138", "pdf": "https://arxiv.org/pdf/2506.14138", "abs": "https://arxiv.org/abs/2506.14138", "authors": ["Ashish Gautam", "Prasanna Date", "Shruti Kulkarni", "Robert Patton", "Thomas Potok"], "title": "NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning", "categories": ["cs.NE", "cs.AI"], "comment": "Neuromorphic computing, FPGA, STDP, Spiking Graph Neural Networks, Spiking Neural Networks, VHDL", "summary": "Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86NeuroCoreX\uff0c\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u5f00\u6e90\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u6a21\u62df\u5668\uff0c\u652f\u6301\u7247\u4e0a\u5b66\u4e60\u548c\u591a\u6837\u5316\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\uff0c\u65e8\u5728\u63a8\u52a8\u9ad8\u6548\u80fd\u751f\u7269\u542f\u53d1\u8ba1\u7b97\u7684\u7814\u7a76\u4e0e\u5f00\u53d1\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u56e0\u5176\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u7684\u67b6\u6784\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u652f\u6301\u591a\u6837\u5316\u8fde\u63a5\u6a21\u5f0f\u548c\u7247\u4e0a\u5b66\u4e60\u7684FPGA\u6a21\u62df\u5668\uff0c\u89e3\u51b3SNN\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "NeuroCoreX\u57fa\u4e8eFPGA\u5b9e\u73b0\uff0c\u652f\u6301\u5168\u8fde\u63a5\u7f51\u7edc\u62d3\u6251\uff0c\u91c7\u7528\u6cc4\u6f0f\u79ef\u5206\u53d1\u653e\uff08LIF\uff09\u795e\u7ecf\u5143\u6a21\u578b\u548c\u57fa\u4e8e\u7535\u6d41\u7684\u7a81\u89e6\u3002\u5176\u5b66\u4e60\u673a\u5236\u57fa\u4e8e\u8109\u51b2\u65f6\u5e8f\u4f9d\u8d56\u53ef\u5851\u6027\uff08STDP\uff09\uff0c\u5e76\u901a\u8fc7UART\u63a5\u53e3\u5b9e\u73b0\u7f51\u7edc\u53c2\u6570\u914d\u7f6e\u3002\u7528\u6237\u53ef\u901a\u8fc7Python\u63a5\u53e3\u8fdb\u884c\u4ea4\u4e92\u3002", "result": "NeuroCoreX\u6210\u529f\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684SNN\u6a21\u62df\uff0c\u652f\u6301\u591a\u79cd\u7f51\u7edc\u62d3\u6251\u548c\u7247\u4e0a\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u52a0\u901f\u4e86\u9ad8\u6548\u80fd\u751f\u7269\u542f\u53d1\u8ba1\u7b97\u7684\u7814\u7a76\u3002", "conclusion": "NeuroCoreX\u4e3aSNN\u7684\u7814\u7a76\u4e0e\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u751f\u7269\u542f\u53d1\u8ba1\u7b97\u9886\u57df\u7684\u8fdb\u6b65\u3002", "paper_title_zh": "NeuroCoreX\uff1a\u4e00\u79cd\u652f\u6301\u7247\u4e0a\u5b66\u4e60\u7684\u5f00\u6e90FPGA\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5668", "abstract_zh": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u662f\u4e00\u79cd\u53d7\u751f\u7269\u795e\u7ecf\u5143\u7f51\u7edc\u7ed3\u6784\u548c\u52a8\u6001\u542f\u53d1\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u5176\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u4f7f\u5176\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u5e73\u53f0\u4e0a\u90e8\u7f72\u65f6\u3002\u4e0e\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u4e3b\u8981\u4f9d\u8d56\u5206\u5c42\u67b6\u6784\u4e0d\u540c\uff0cSNN\u5929\u7136\u652f\u6301\u4ece\u4f20\u7edf\u5206\u5c42\u7ed3\u6784\u5230\u5c40\u90e8\u5bc6\u96c6\u3001\u5168\u5c40\u7a00\u758f\u7684\u5c0f\u4e16\u754c\u56fe\u7b49\u591a\u79cd\u8fde\u63a5\u6a21\u5f0f\u3002\u672c\u6587\u4ecb\u7ecd\u4e86NeuroCoreX\uff0c\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u6a21\u62df\u5668\uff0c\u4e13\u4e3a\u7075\u6d3b\u534f\u540c\u8bbe\u8ba1\u548c\u6d4b\u8bd5SNN\u800c\u8bbe\u8ba1\u3002NeuroCoreX\u652f\u6301\u5168\u8fde\u63a5\u7f51\u7edc\u62d3\u6251\uff0c\u80fd\u591f\u5b9e\u73b0\u591a\u6837\u5316\u7684\u7f51\u7edc\u7ed3\u6784\u800c\u4e0d\u53d7\u67b6\u6784\u9650\u5236\u3002\u5176\u91c7\u7528\u57fa\u4e8e\u8109\u51b2\u65f6\u5e8f\u4f9d\u8d56\u53ef\u5851\u6027\uff08STDP\uff09\u7684\u751f\u7269\u542f\u53d1\u5c40\u90e8\u5b66\u4e60\u673a\u5236\uff0c\u795e\u7ecf\u5143\u6a21\u578b\u4e3a\u6cc4\u6f0f\u79ef\u5206\u53d1\u653e\uff08LIF\uff09\u6a21\u578b\uff0c\u7a81\u89e6\u57fa\u4e8e\u7535\u6d41\u5b9e\u73b0\u8109\u51b2\u7684\u6574\u5408\u4e0e\u4f20\u8f93\u3002\u901a\u8fc7\u901a\u7528\u5f02\u6b65\u6536\u53d1\u5668\uff08UART\uff09\u63a5\u53e3\uff0c\u7528\u6237\u53ef\u4ee5\u7f16\u7a0b\u548c\u914d\u7f6e\u7f51\u7edc\u53c2\u6570\uff0c\u5305\u62ec\u795e\u7ecf\u5143\u3001\u7a81\u89e6\u548c\u5b66\u4e60\u89c4\u5219\u8bbe\u7f6e\u3002\u7528\u6237\u901a\u8fc7\u7b80\u5355\u7684Python\u63a5\u53e3\u4e0e\u6a21\u62df\u5668\u4ea4\u4e92\uff0c\u7b80\u5316\u4e86\u4ece\u6a21\u578b\u8bbe\u8ba1\u5230\u786c\u4ef6\u6267\u884c\u7684SNN\u90e8\u7f72\u6d41\u7a0b\u3002NeuroCoreX\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6\u53d1\u5e03\uff0c\u65e8\u5728\u52a0\u901f\u9ad8\u6548\u80fd\u751f\u7269\u542f\u53d1\u8ba1\u7b97\u7684\u7814\u7a76\u4e0e\u5f00\u53d1\u3002"}}
{"id": "2506.14159", "pdf": "https://arxiv.org/pdf/2506.14159", "abs": "https://arxiv.org/abs/2506.14159", "authors": ["Shayan Talaei", "Meijin Li", "Kanu Grover", "James Kent Hippler", "Diyi Yang", "Amin Saberi"], "title": "StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework", "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.", "AI": {"tldr": "StorySage\u662f\u4e00\u6b3e\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5bf9\u8bdd\u5f0f\u81ea\u4f20\u5199\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u7075\u6d3b\u5bf9\u8bdd\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\u5e2e\u52a9\u7528\u6237\u6574\u7406\u8bb0\u5fc6\u5e76\u5b8c\u6210\u81ea\u4f20\u3002\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "motivation": "\u4e2a\u4eba\u8bb0\u5fc6\u901a\u5e38\u5206\u6563\u4e14\u96be\u4ee5\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u81ea\u4f20\uff0c\u73b0\u6709\u5bf9\u8bdd\u5f0f\u5199\u4f5c\u52a9\u624b\u4f9d\u8d56\u901a\u7528\u4ea4\u4e92\u548c\u9884\u5b9a\u4e49\u6307\u5357\uff0c\u96be\u4ee5\u6355\u6349\u4e2a\u6027\u5316\u8bb0\u5fc6\u5e76\u5b8c\u6210\u5b8c\u6574\u4f20\u8bb0\u3002StorySage\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "StorySage\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u91c7\u8bbf\u8005\u3001\u4f1a\u8bdd\u8bb0\u5f55\u5458\u3001\u89c4\u5212\u5e08\u3001\u7ae0\u8282\u5199\u624b\u548c\u4f1a\u8bdd\u534f\u8c03\u5458\uff0c\u901a\u8fc7\u8fed\u4ee3\u6536\u96c6\u7528\u6237\u8bb0\u5fc6\u3001\u66f4\u65b0\u81ea\u4f20\u5e76\u89c4\u5212\u672a\u6765\u5bf9\u8bdd\u3002", "result": "\u5b9e\u9a8c\u6a21\u62df\u663e\u793aStorySage\u80fd\u5904\u7406\u591a\u8f6e\u4f1a\u8bdd\u5e76\u6355\u6349\u7528\u6237\u8bb0\u5fc6\uff1b\u7528\u6237\u7814\u7a76\uff08N=28\uff09\u8868\u660e\u5176\u5728\u5bf9\u8bdd\u6d41\u7545\u6027\u3001\u53d9\u4e8b\u5b8c\u6574\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "StorySage\u4e3a\u81ea\u4f20\u5199\u4f5c\u63d0\u4f9b\u4e86\u65b0\u9896\u67b6\u6784\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u589e\u5f3a\u4eba\u673a\u521b\u610f\u5408\u4f5c\u3002", "paper_title_zh": "StorySage\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5bf9\u8bdd\u5f0f\u81ea\u4f20\u5199\u4f5c\u7cfb\u7edf", "abstract_zh": "\u6bcf\u4e2a\u4eba\u90fd\u62e5\u6709\u7531\u8bb0\u5fc6\u548c\u7ecf\u5386\u5851\u9020\u7684\u72ec\u7279\u4eba\u751f\u6545\u4e8b\uff0c\u4f46\u8fd9\u4e9b\u8bb0\u5fc6\u901a\u5e38\u5206\u6563\u4e14\u96be\u4ee5\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u81ea\u4f20\u3002\u73b0\u6709\u7684\u5bf9\u8bdd\u5f0f\u5199\u4f5c\u52a9\u624b\u4f9d\u8d56\u901a\u7528\u4ea4\u4e92\u548c\u9884\u5b9a\u4e49\u6307\u5357\uff0c\u96be\u4ee5\u6355\u6349\u4e2a\u4eba\u8bb0\u5fc6\u5e76\u9010\u6b65\u5b8c\u6210\u4f20\u8bb0\u3002\u6211\u4eec\u63a8\u51faStorySage\uff0c\u8fd9\u662f\u4e00\u6b3e\u7528\u6237\u9a71\u52a8\u7684\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u5bf9\u8bdd\u548c\u7ed3\u6784\u5316\u81ea\u4f20\u5199\u4f5c\u65b9\u6cd5\u3002\u5176\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7531\u91c7\u8bbf\u8005\u3001\u4f1a\u8bdd\u8bb0\u5f55\u5458\u3001\u89c4\u5212\u5e08\u3001\u7ae0\u8282\u5199\u624b\u548c\u4f1a\u8bdd\u534f\u8c03\u5458\u7ec4\u6210\uff0c\u7cfb\u7edf\u901a\u8fc7\u8fed\u4ee3\u6536\u96c6\u7528\u6237\u8bb0\u5fc6\u3001\u66f4\u65b0\u81ea\u4f20\u5e76\u89c4\u5212\u672a\u6765\u5bf9\u8bdd\u3002\u5b9e\u9a8c\u6a21\u62df\u8868\u660e\uff0cStorySage\u80fd\u591f\u5904\u7406\u591a\u8f6e\u4f1a\u8bdd\u5e76\u6355\u6349\u7528\u6237\u8bb0\u5fc6\u3002\u7528\u6237\u7814\u7a76\uff08N=28\uff09\u663e\u793a\uff0c\u4e0e\u57fa\u7ebf\u7cfb\u7edf\u76f8\u6bd4\uff0cStorySage\u5728\u5bf9\u8bdd\u6d41\u7545\u6027\u3001\u53d9\u4e8b\u5b8c\u6574\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u603b\u4e4b\uff0cStorySage\u4e0d\u4ec5\u4e3a\u81ea\u4f20\u5199\u4f5c\u63d0\u4f9b\u4e86\u65b0\u9896\u67b6\u6784\uff0c\u8fd8\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u63d0\u5347\u4eba\u673a\u521b\u610f\u5408\u4f5c\u3002"}}
{"id": "2506.14196", "pdf": "https://arxiv.org/pdf/2506.14196", "abs": "https://arxiv.org/abs/2506.14196", "authors": ["Jiayue Melissa Shi", "Keran Wang", "Dong Whi Yoo", "Ravi Karkar", "Koustuv Saha"], "title": "Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u548c\u75f4\u5446\u75c7\u5bb6\u5ead\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\u9700\u6c42\uff0c\u5206\u6790\u4e86\u4ed6\u4eec\u5728\u7167\u987e\u8fc7\u7a0b\u4e2d\u7684\u5fc3\u7406\u6311\u6218\u3001\u5e94\u5bf9\u7b56\u7565\u53ca\u6280\u672f\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\u5e72\u9884\u7684\u5efa\u8bae\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u53ca\u76f8\u5173\u75f4\u5446\u75c7\uff08AD/ADRD\uff09\u60a3\u8005\u7684\u5bb6\u5ead\u7167\u987e\u8005\u56e0\u957f\u671f\u7167\u987e\u8d23\u4efb\u9762\u4e34\u4e25\u91cd\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u652f\u6301\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5173\u6ce8\u5176\u52a8\u6001\u53d8\u5316\u7684\u5fc3\u7406\u5065\u5eb7\u9700\u6c42\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf925\u540dAD/ADRD\u60a3\u8005\u5bb6\u5ead\u7167\u987e\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u7814\u7a76\u5206\u6790\u4e86\u5fc3\u7406\u5065\u5eb7\u6311\u6218\u7684\u6210\u56e0\u548c\u5f71\u54cd\uff0c\u5e76\u7ed8\u5236\u4e86\u7167\u987e\u8005\u5728\u4e09\u4e2a\u4e0d\u540c\u7167\u987e\u9636\u6bb5\u7684\u5fc3\u7406\u5065\u5eb7\u53d8\u5316\u65f6\u95f4\u7ebf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\u9700\u6c42\u968f\u7167\u987e\u9636\u6bb5\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u73b0\u6709\u5fc3\u7406\u5065\u5eb7\u6280\u672f\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u5f3a\u8c03\u9700\u8981\u53ef\u8bbf\u95ee\u3001\u53ef\u6269\u5c55\u4e14\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u52a8\u6001\u3001\u5206\u9636\u6bb5\u7684\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4ee5\u5168\u9762\u652f\u6301\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\uff0c\u4ece\u800c\u60e0\u53ca\u7167\u987e\u8005\u548c\u88ab\u7167\u987e\u8005\u3002", "paper_title_zh": "\u5e73\u8861\u7167\u987e\u4e0e\u81ea\u6211\u5173\u6000\uff1a\u63a2\u7d22\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u548c\u75f4\u5446\u75c7\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\u9700\u6c42", "abstract_zh": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u53ca\u76f8\u5173\u75f4\u5446\u75c7\uff08AD/ADRD\uff09\u662f\u4e00\u79cd\u8fdb\u884c\u6027\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u635f\u5bb3\u8bb0\u5fc6\u3001\u601d\u7ef4\u548c\u529f\u80fd\u3002AD/ADRD\u60a3\u8005\u7684\u5bb6\u5ead\u7167\u987e\u8005\u56e0\u957f\u671f\u7167\u987e\u8d23\u4efb\u9762\u4e34\u4e25\u91cd\u7684\u5fc3\u7406\u5065\u5eb7\u6311\u6218\uff0c\u4f46\u73b0\u6709\u652f\u6301\u7cfb\u7edf\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u5fc3\u7406\u5065\u5eb7\u9700\u6c42\u7684\u52a8\u6001\u53d8\u5316\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u4ed6\u4eec\u4e3a\u5e94\u5bf9\u7167\u987e\u8d1f\u62c5\u6240\u91c7\u53d6\u7684\u7b56\u7565\u53ca\u4f7f\u7528\u7684\u6280\u672f\u652f\u6301\u3002\u901a\u8fc7\u5bf925\u540dAD/ADRD\u60a3\u8005\u5bb6\u5ead\u7167\u987e\u8005\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5fc3\u7406\u5065\u5eb7\u6311\u6218\u7684\u4e3b\u8981\u6210\u56e0\u548c\u5f71\u54cd\uff0c\u5e76\u7ed8\u5236\u4e86\u7167\u987e\u8005\u5728\u4e09\u4e2a\u4e0d\u540c\u7167\u987e\u9636\u6bb5\u4e2d\u5fc3\u7406\u5065\u5eb7\u53d8\u5316\u7684\u65f6\u95f4\u7ebf\u3002\u6b64\u5916\uff0c\u53c2\u4e0e\u8005\u8fd8\u5206\u4eab\u4e86\u6539\u8fdb\u73b0\u6709\u5fc3\u7406\u5065\u5eb7\u6280\u672f\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u9700\u8981\u53ef\u8bbf\u95ee\u3001\u53ef\u6269\u5c55\u4e14\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u9002\u5e94\u7167\u987e\u8005\u968f\u65f6\u95f4\u53d8\u5316\u7684\u9700\u6c42\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bbe\u8ba1\u52a8\u6001\u3001\u5206\u9636\u6bb5\u7684\u5e72\u9884\u63aa\u65bd\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ee5\u5168\u9762\u652f\u6301\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\uff0c\u60e0\u53ca\u7167\u987e\u8005\u548c\u88ab\u7167\u987e\u8005\u3002"}}
{"id": "2506.14202", "pdf": "https://arxiv.org/pdf/2506.14202", "abs": "https://arxiv.org/abs/2506.14202", "authors": ["Makoto Shing", "Takuya Akiba"], "title": "DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To appear at TTODLer-FM Workshop of the 42nd International Conference on Machine Learning", "summary": "Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.", "AI": {"tldr": "\u63d0\u51faDiffusionBlocks\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u8bad\u7ec3\u548c\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5148\u8fdbAI\u7814\u7a76\u7684\u666e\u53ca\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u5757\u8bad\u7ec3\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u5212\u5206\u4e3a\u72ec\u7acb\u8bad\u7ec3\u5757\uff0c\u5c06\u5176\u89e3\u91ca\u4e3a\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u53bb\u566a\u64cd\u4f5c\uff0c\u5e76\u57fa\u4e8e\u7b49\u7d2f\u79ef\u6982\u7387\u8d28\u91cf\u4f18\u5316\u566a\u58f0\u6c34\u5e73\u5206\u914d\u3002", "result": "\u5728\u56fe\u50cf\u751f\u6210\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u5185\u5b58\u6d88\u8017\u4e0e\u5757\u6570\u6210\u6bd4\u4f8b\u51cf\u5c11\uff0c\u540c\u65f6\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\u3002", "conclusion": "DiffusionBlocks\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "paper_title_zh": "DiffusionBlocks\uff1a\u57fa\u4e8e\u5206\u6570\u6269\u6563\u7684\u5206\u5757\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5", "abstract_zh": "\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u4f1a\u5e26\u6765\u663e\u8457\u7684\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5148\u8fdbAI\u7814\u7a76\u7684\u666e\u53ca\u3002\u6211\u4eec\u63d0\u51faDiffusionBlocks\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u5757\u89e3\u91ca\u4e3a\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u53bb\u566a\u64cd\u4f5c\u3002\u901a\u8fc7\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u72ec\u7acb\u8bad\u7ec3\u5757\uff0c\u5e76\u57fa\u4e8e\u7b49\u7d2f\u79ef\u6982\u7387\u8d28\u91cf\u4f18\u5316\u566a\u58f0\u6c34\u5e73\u5206\u914d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u76f8\u5f53\u7684\u7ade\u4e89\u529b\u3002\u5728\u56fe\u50cf\u751f\u6210\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5185\u5b58\u6d88\u8017\u4e0e\u5757\u6570\u6210\u6bd4\u4f8b\u51cf\u5c11\uff0c\u540c\u65f6\u6027\u80fd\u8868\u73b0\u66f4\u4f18\u3002DiffusionBlocks\u4e3a\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8bad\u7ec3\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2506.14217", "pdf": "https://arxiv.org/pdf/2506.14217", "abs": "https://arxiv.org/abs/2506.14217", "authors": ["Dipesh Tharu Mahato", "Rohan Poudel", "Pramod Dhungana"], "title": "TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 6 tables, 6 figures", "summary": "Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.", "AI": {"tldr": "TriGuard\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9c81\u68d2\u6027\u9a8c\u8bc1\u3001\u5f52\u56e0\u71b5\u548c\u5f52\u56e0\u6f02\u79fb\u8bc4\u5206\uff0c\u63ed\u793a\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u5728\u5bf9\u6297\u6027\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002TriGuard\u65e8\u5728\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "TriGuard\u7ed3\u5408\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a(1) \u5f62\u5f0f\u5316\u9c81\u68d2\u6027\u9a8c\u8bc1\uff0c(2) \u5f52\u56e0\u71b5\u91cf\u5316\u663e\u8457\u6027\u96c6\u4e2d\u5ea6\uff0c(3) \u65b0\u9896\u7684\u5f52\u56e0\u6f02\u79fb\u8bc4\u5206\u8861\u91cf\u89e3\u91ca\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTriGuard\u80fd\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u7684\u5fae\u5999\u8106\u5f31\u6027\uff0c\u4e14\u71b5\u6b63\u5219\u5316\u8bad\u7ec3\u53ef\u51cf\u5c11\u89e3\u91ca\u6f02\u79fb\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "TriGuard\u4e3a\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "paper_title_zh": "TriGuard\uff1a\u57fa\u4e8e\u5f52\u56e0\u71b5\u3001\u9a8c\u8bc1\u4e0e\u6f02\u79fb\u7684\u6a21\u578b\u5b89\u5168\u6027\u6d4b\u8bd5", "abstract_zh": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5176\u5728\u5bf9\u6297\u6027\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u4ecd\u662f\u4e00\u4e2a\u7d27\u8feb\u7684\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86TriGuard\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\uff081\uff09\u5f62\u5f0f\u5316\u9c81\u68d2\u6027\u9a8c\u8bc1\uff0c\uff082\uff09\u5f52\u56e0\u71b5\u4ee5\u91cf\u5316\u663e\u8457\u6027\u96c6\u4e2d\u5ea6\uff0c\u4ee5\u53ca\uff083\uff09\u4e00\u79cd\u65b0\u9896\u7684\u5f52\u56e0\u6f02\u79fb\u8bc4\u5206\uff0c\u7528\u4e8e\u8861\u91cf\u89e3\u91ca\u7684\u7a33\u5b9a\u6027\u3002TriGuard\u63ed\u793a\u4e86\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5173\u952e\u4e0d\u5339\u914d\uff1a\u5df2\u9a8c\u8bc1\u7684\u6a21\u578b\u4ecd\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u7684\u63a8\u7406\uff0c\u800c\u5f52\u56e0\u4fe1\u53f7\u63d0\u4f9b\u4e86\u8d85\u8d8a\u5bf9\u6297\u51c6\u786e\u6027\u7684\u8865\u5145\u5b89\u5168\u6d1e\u5bdf\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u4e94\u79cd\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTriGuard\u5982\u4f55\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u7684\u5fae\u5999\u8106\u5f31\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\uff0c\u71b5\u6b63\u5219\u5316\u8bad\u7ec3\u53ef\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u89e3\u91ca\u6f02\u79fb\u3002TriGuard\u63a8\u52a8\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8bc4\u4f30\u7684\u524d\u6cbf\u3002"}}
{"id": "2506.14262", "pdf": "https://arxiv.org/pdf/2506.14262", "abs": "https://arxiv.org/abs/2506.14262", "authors": ["Mohammad Emtiyaz Khan"], "title": "Knowledge Adaptation as Posterior Correction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u77e5\u8bc6\u9002\u5e94\u89c6\u4e3a\u540e\u9a8c\u6821\u6b63\u7684\u673a\u5236\uff0c\u901a\u8fc7\u66f4\u51c6\u786e\u7684\u540e\u9a8c\u5206\u5e03\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u63ed\u793a\u4e86\u673a\u5668\u5982\u4f55\u50cf\u4eba\u7c7b\u548c\u52a8\u7269\u4e00\u6837\u81ea\u7136\u5b66\u4e60\u9002\u5e94\u7684\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5728\u6a21\u578b\u9002\u5e94\u65b9\u9762\u53d6\u5f97\u4e86\u8bb8\u591a\u8fdb\u5c55\uff08\u5982\u6301\u7eed\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u7b49\uff09\uff0c\u4f46\u673a\u5668\u5982\u4f55\u50cf\u4eba\u7c7b\u548c\u52a8\u7269\u4e00\u6837\u81ea\u7136\u5feb\u901f\u9002\u5e94\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u4e4b\u8c1c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u8fd9\u4e00\u673a\u5236\u3002", "method": "\u901a\u8fc7\u4f7f\u7528Khan\u548cRue\uff082023\uff09\u7684\u8d1d\u53f6\u65af\u5b66\u4e60\u89c4\u5219\u7684\u53cc\u91cd\u89c6\u89d2\uff0c\u5c06\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u5e72\u6270\u8868\u5f81\u4e3a\u8fc7\u53bb\u6570\u636e\u7684\u81ea\u7136\u68af\u5ea6\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u5c06\u5404\u79cd\u9002\u5e94\u65b9\u6cd5\u7edf\u4e00\u4e3a\u540e\u9a8c\u6821\u6b63\u7684\u4e0d\u540c\u5f62\u5f0f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u66f4\u51c6\u786e\u7684\u540e\u9a8c\u5206\u5e03\u53ef\u4ee5\u51cf\u5c0f\u6821\u6b63\u5e45\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5feb\u7684\u9002\u5e94\u3002\u901a\u8fc7\u591a\u4e2a\u793a\u4f8b\u9a8c\u8bc1\u4e86\u540e\u9a8c\u6821\u6b63\u4f5c\u4e3a\u673a\u5668\u5feb\u901f\u9002\u5e94\u81ea\u7136\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u540e\u9a8c\u6821\u6b63\u662f\u673a\u5668\u5feb\u901f\u9002\u5e94\u7684\u4e00\u79cd\u81ea\u7136\u673a\u5236\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7684\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u5b9e\u7528\u65b9\u6cd5\u3002", "paper_title_zh": "\u77e5\u8bc6\u9002\u5e94\u4f5c\u4e3a\u540e\u9a8c\u6821\u6b63", "abstract_zh": "\u9002\u5e94\u662f\u667a\u80fd\u7684\u7ec8\u6781\u76ee\u6807\uff0c\u4f46\u5373\u4f7f\u662f\u50cfGPT\u8fd9\u6837\u7684\u9876\u7ea7AI\u6a21\u578b\uff0c\u5176\u9002\u5e94\u6027\u4e5f\u4e0d\u53ca\u5e7c\u513f\u3002\u56e0\u6b64\uff0c\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff1a\u673a\u5668\u5982\u4f55\u5feb\u901f\u9002\u5e94\uff1f\u5c3d\u7ba1\u5728\u6a21\u578b\u9002\u5e94\u65b9\u9762\u53d6\u5f97\u4e86\u8bb8\u591a\u8fdb\u5c55\uff08\u5982\u6301\u7eed\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u3001\u6a21\u578b\u5408\u5e76\u3001\u7f16\u8f91\u3001\u9057\u5fd8\u7b49\uff09\uff0c\u4f46\u673a\u5668\u5982\u4f55\u50cf\u4eba\u7c7b\u548c\u52a8\u7269\u4e00\u6837\u81ea\u7136\u5b66\u4e60\u9002\u5e94\u7684\u673a\u5236\u4ecd\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u6587\u8868\u660e\uff0c\u6240\u6709\u8fd9\u4e9b\u9002\u5e94\u65b9\u6cd5\u90fd\u53ef\u4ee5\u89c6\u4e3a\u5bf9\u8fd1\u4f3c\u540e\u9a8c\u7684\u4e0d\u540c\u201c\u6821\u6b63\u201d\u65b9\u5f0f\u3002\u66f4\u51c6\u786e\u7684\u540e\u9a8c\u5206\u5e03\u4f1a\u5bfc\u81f4\u66f4\u5c0f\u7684\u6821\u6b63\uff0c\u4ece\u800c\u610f\u5473\u7740\u66f4\u5feb\u7684\u9002\u5e94\u3002\u8fd9\u4e00\u7ed3\u679c\u662f\u901a\u8fc7\u4f7f\u7528Khan\u548cRue\uff082023\uff09\u7684\u8d1d\u53f6\u65af\u5b66\u4e60\u89c4\u5219\u7684\u53cc\u91cd\u89c6\u89d2\u5f97\u51fa\u7684\uff0c\u5176\u4e2d\u9002\u5e94\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u5e72\u6270\u7531\u8fc7\u53bb\u6570\u636e\u7684\u81ea\u7136\u68af\u5ea6\u4e0d\u5339\u914d\u8868\u5f81\u3002\u6211\u4eec\u901a\u8fc7\u591a\u4e2a\u793a\u4f8b\u5c55\u793a\u4e86\u540e\u9a8c\u6821\u6b63\u4f5c\u4e3a\u673a\u5668\u5feb\u901f\u9002\u5e94\u81ea\u7136\u673a\u5236\u7684\u5e94\u7528\u3002"}}
{"id": "2506.14287", "pdf": "https://arxiv.org/pdf/2506.14287", "abs": "https://arxiv.org/abs/2506.14287", "authors": ["Yanwei Wang"], "title": "Steering Robots with Inference-Time Interactions", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": "MIT Robotics PhD Thesis", "summary": "Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u5f15\u5bfc\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u7ea0\u6b63\u7b56\u7565\u9519\u8bef\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5df2\u63a8\u52a8\u5f00\u53d1\u51fa\u80fd\u81ea\u4e3b\u89e3\u51b3\u591a\u4efb\u52a1\u7684\u901a\u7528\u7b56\u7565\uff0c\u4f46\u5728\u90e8\u7f72\u65f6\u82e5\u9884\u8bad\u7ec3\u7b56\u7565\u51fa\u9519\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u7528\u6237\u7ea0\u6b63\u673a\u5236\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u4e3a\u6bcf\u4e2a\u4e0b\u6e38\u7528\u4f8b\u6536\u96c6\u989d\u5916\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u6846\u67b6\uff1a(1) \u63a8\u7406\u65f6\u5f15\u5bfc\uff0c\u5229\u7528\u7528\u6237\u4ea4\u4e92\u5728\u79bb\u6563\u6280\u80fd\u95f4\u5207\u6362\uff1b(2) \u4efb\u52a1\u4e0e\u52a8\u4f5c\u6a21\u4eff\uff0c\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u8fde\u7eed\u52a8\u4f5c\uff0c\u540c\u65f6\u6ee1\u8db3\u79bb\u6563\u7b26\u53f7\u8ba1\u5212\u5b9a\u4e49\u7684\u4efb\u52a1\u7ea6\u675f\u3002", "result": "\u8fd9\u4e9b\u6846\u67b6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u7ea0\u6b63\u7b56\u7565\u9884\u6d4b\u504f\u5dee\uff0c\u6700\u5927\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u7528\uff0c\u540c\u65f6\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u7528\u6237\u76ee\u6807\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u65f6\u4ea4\u4e92\u5f15\u5bfc\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u6237\u53ef\u9ad8\u6548\u7ea0\u6b63\u6a21\u578b\u9519\u8bef\uff0c\u63d0\u5347\u90e8\u7f72\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u989d\u5916\u6570\u636e\u7684\u4f9d\u8d56\u3002", "paper_title_zh": "\u901a\u8fc7\u63a8\u7406\u65f6\u4ea4\u4e92\u5f15\u5bfc\u673a\u5668\u4eba", "abstract_zh": "\u6a21\u4eff\u5b66\u4e60\u63a8\u52a8\u4e86\u80fd\u591f\u81ea\u4e3b\u89e3\u51b3\u591a\u4efb\u52a1\u7684\u901a\u7528\u7b56\u7565\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5f53\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u51fa\u9519\u65f6\uff0c\u7528\u6237\u7ea0\u6b63\u5176\u884c\u4e3a\u7684\u673a\u5236\u6709\u9650\u3002\u867d\u7136\u6536\u96c6\u989d\u5916\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u4e0b\u6e38\u7528\u4f8b\u8fd9\u6837\u505a\u5728\u90e8\u7f72\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u6211\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff1a\u4fdd\u6301\u9884\u8bad\u7ec3\u7b56\u7565\u51bb\u7ed3\u4f5c\u4e3a\u56fa\u5b9a\u6280\u80fd\u5e93\uff0c\u540c\u65f6\u5141\u8bb8\u7528\u6237\u4ea4\u4e92\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u884c\u4e3a\u751f\u6210\u4ee5\u6ee1\u8db3\u7528\u6237\u504f\u597d\u3002\u901a\u8fc7\u4f7f\u9884\u8bad\u7ec3\u7b56\u7565\u53ef\u5f15\u5bfc\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u65f6\u5e2e\u52a9\u7ea0\u6b63\u7b56\u7565\u9519\u8bef\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u7b56\u7565\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u63d0\u51fa\u4e86\uff081\uff09\u63a8\u7406\u65f6\u5f15\u5bfc\uff0c\u5229\u7528\u7528\u6237\u4ea4\u4e92\u5728\u79bb\u6563\u6280\u80fd\u95f4\u5207\u6362\uff1b\uff082\uff09\u4efb\u52a1\u4e0e\u52a8\u4f5c\u6a21\u4eff\uff0c\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u8fde\u7eed\u52a8\u4f5c\uff0c\u540c\u65f6\u6ee1\u8db3\u7531\u79bb\u6563\u7b26\u53f7\u8ba1\u5212\u5b9a\u4e49\u7684\u4efb\u52a1\u7ea6\u675f\u3002\u8fd9\u4e9b\u6846\u67b6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u7ea0\u6b63\u7b56\u7565\u9884\u6d4b\u504f\u5dee\uff0c\u6700\u5927\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u7528\uff0c\u540c\u65f6\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u7528\u6237\u76ee\u6807\u3002"}}
{"id": "2506.14294", "pdf": "https://arxiv.org/pdf/2506.14294", "abs": "https://arxiv.org/abs/2506.14294", "authors": ["Prashant Kumar Rai", "Elham Kowsari", "Nataliya Strokina", "Reza Ghabcheloo"], "title": "Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation", "categories": ["cs.RO", "cs.AI", "eess.SP"], "comment": "This paper has been accepted for presentation at the 28th International Conference on Information Fusion (Fusion 2025)", "summary": "We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96f7\u8fbe-\u60ef\u6027\u878d\u5408\u7684\u77ac\u65f63D\u81ea\u8f66\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u96f7\u8fbe\u6570\u636e\u5e76\u7ed3\u5408\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u96f7\u8fbe\u81ea\u8fd0\u52a8\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u96f7\u8fbe\u6570\u636e\u548c\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u878d\u5408\u96f7\u8fbe\u4e0e\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u6570\u636e\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u5347\u81ea\u8f66\u901f\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u590d\u6570\u5f62\u5f0f\u7684\u539f\u59cb\u96f7\u8fbe\u6570\u636e\uff0c\u4f30\u8ba1\u77ac\u65f6\u7ebf\u6027\u81ea\u8f66\u901f\u5ea6\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff1b2) \u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5c06\u96f7\u8fbe\u4f30\u8ba1\u4e0e\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u6570\u636e\u878d\u5408\uff0c\u5229\u7528\u7f51\u7edc\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\u4f20\u611f\u5668\u566a\u58f0\u548c\u504f\u5dee\u53c2\u6570\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6ColoRadar\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u516c\u5f00\u65b9\u6cd5\uff0c\u4e14\u5728\u77ac\u65f6\u4f30\u8ba1\u548c\u626b\u63cf\u5339\u914d\u6280\u672f\u4e0a\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u96f7\u8fbe-\u60ef\u6027\u878d\u5408\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u8f66\u901f\u5ea6\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002", "paper_title_zh": "\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u96f7\u8fbe-\u60ef\u6027\u878d\u5408\u7528\u4e8e\u77ac\u65f63D\u81ea\u8f66\u901f\u5ea6\u4f30\u8ba1", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u96f7\u8fbe\u4e0e\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u6765\u4f30\u8ba1\u81ea\u4e3b\u5bfc\u822a\u4e2d\u81ea\u8f66\u901f\u5ea6\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u590d\u6570\u5f62\u5f0f\u7684\u539f\u59cb\u96f7\u8fbe\u6570\u636e\uff0c\u4f30\u8ba1\u77ac\u65f6\u7ebf\u6027\u81ea\u8f66\u901f\u5ea6\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u4f20\u7edf\u96f7\u8fbe\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6280\u672f\u7684\u5c40\u9650\u6027\u3002\u8fd9\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u901f\u5ea6\u4f30\u8ba1\u968f\u540e\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e0e\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u6570\u636e\u878d\u5408\u3002\u6ee4\u6ce2\u5668\u5229\u7528\u7f51\u7edc\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\u60ef\u6027\u4f20\u611f\u5668\u7684\u566a\u58f0\u548c\u504f\u5dee\u53c2\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u81ea\u8fd0\u52a8\u4f30\u8ba1\u7684\u6574\u4f53\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u6211\u4eec\u5728\u516c\u5f00\u7684ColoRadar\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u4e0e\u6700\u63a5\u8fd1\u7684\u516c\u5f00\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u8bef\u5dee\uff0c\u5e76\u4e14\u5728\u77ac\u65f6\u4f30\u8ba1\u548c\u626b\u63cf\u5339\u914d\u6280\u672f\u4e0a\u5747\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2506.14329", "pdf": "https://arxiv.org/pdf/2506.14329", "abs": "https://arxiv.org/abs/2506.14329", "authors": ["Rickmer Schulte", "David R\u00fcgamer", "Thomas Nagler"], "title": "Adjustment for Confounding using Pre-Trained Representations", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "comment": "Accepted at ICML 2025", "summary": "There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u7279\u5f81\u6765\u8c03\u6574\u6df7\u6742\u56e0\u7d20\uff0c\u4ee5\u6539\u8fdb\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u4f30\u8ba1\u3002\u7814\u7a76\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u9002\u5e94\u5b66\u4e60\u95ee\u9898\u7684\u5185\u5728\u7a00\u758f\u6027\u548c\u7ef4\u5ea6\uff0c\u4ece\u800c\u83b7\u5f97\u5feb\u901f\u6536\u655b\u901f\u7387\u3002", "motivation": "\u968f\u7740\u975e\u8868\u683c\u6570\u636e\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u5728\u6df7\u6742\u56e0\u7d20\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u5ffd\u7565\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u7ed3\u679c\u504f\u5dee\u548c\u79d1\u5b66\u7ed3\u8bba\u9519\u8bef\u3002\u7136\u800c\uff0c\u5c06\u8fd9\u4e9b\u6570\u636e\u7eb3\u5165ATE\u4f30\u8ba1\u9700\u8981\u590d\u6742\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u5e38\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u7279\u5f81\u6765\u8c03\u6574\u6df7\u6742\u56e0\u7d20\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5f62\u5f0f\u5316\u6761\u4ef6\uff0c\u63a2\u8ba8\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u7279\u5f81\u5982\u4f55\u7528\u4e8e\u8c03\u6574\u6df7\u6742\u56e0\u7d20\uff0c\u5e76\u5728ATE\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u7edf\u8ba1\u63a8\u65ad\u3002\u4ee5\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u7279\u5f81\u7684\u5e94\u7528\u3002\u540c\u65f6\u8ba8\u8bba\u4e86\u6f5c\u5728\u7279\u5f81\u5b66\u4e60\u548c\u9ad8\u7ef4\u975e\u53ef\u8bc6\u522b\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u7684\u52a0\u6027\u6216\u7a00\u758f\u7ebf\u6027\u6a21\u578b\u7684\u7ed3\u6784\u5047\u8bbe\u5bf9\u6f5c\u5728\u7279\u5f81\u4e0d\u73b0\u5b9e\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u9002\u5e94\u5b66\u4e60\u95ee\u9898\u7684\u5185\u5728\u7a00\u758f\u6027\u548c\u7ef4\u5ea6\uff0c\u4ece\u800c\u83b7\u5f97\u5feb\u901f\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u8c03\u6574\u6df7\u6742\u56e0\u7d20\uff0c\u5c3d\u7ba1\u5b58\u5728\u9ad8\u7ef4\u548c\u975e\u53ef\u8bc6\u522b\u6027\u7684\u6311\u6218\uff0c\u795e\u7ecf\u7f51\u7edc\u4ecd\u80fd\u901a\u8fc7\u9002\u5e94\u95ee\u9898\u7684\u5185\u5728\u7279\u6027\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u3002", "paper_title_zh": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8868\u793a\u8c03\u6574\u6df7\u6742\u56e0\u7d20", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u5c06\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u4f30\u8ba1\u6269\u5c55\u5230\u975e\u8868\u683c\u6570\u636e\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\uff0c\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u4f5c\u4e3a\u6df7\u6742\u56e0\u7d20\u7684\u6765\u6e90\u3002\u5ffd\u7565\u8fd9\u4e9b\u5f71\u54cd\u53ef\u80fd\u5bfc\u81f4\u7ed3\u679c\u504f\u5dee\u548c\u79d1\u5b66\u7ed3\u8bba\u9519\u8bef\u3002\u7136\u800c\uff0c\u7eb3\u5165\u975e\u8868\u683c\u6570\u636e\u9700\u8981\u590d\u6742\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u5e38\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u7684\u601d\u60f3\u3002\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u7279\u5f81\u6765\u8c03\u6574\u6df7\u6742\u56e0\u7d20\u3002\u6211\u4eec\u5f62\u5f0f\u5316\u4e86\u8fd9\u4e9b\u6f5c\u5728\u7279\u5f81\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u8c03\u6574\u548cATE\u4f30\u8ba1\u4e2d\u7edf\u8ba1\u63a8\u65ad\u7684\u6761\u4ef6\uff0c\u5e76\u4ee5\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u4e3a\u4f8b\u5c55\u793a\u4e86\u7ed3\u679c\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u6f5c\u5728\u7279\u5f81\u5b66\u4e60\u548c\u9ad8\u7ef4\u975e\u53ef\u8bc6\u522b\u6027\u5e26\u6765\u7684\u5173\u952e\u6311\u6218\uff0c\u4ee5\u53ca\u4e0b\u6e38\u53c2\u6570\u4f30\u8ba1\u7684\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u4f20\u7edf\u7684\u52a0\u6027\u6216\u7a00\u758f\u7ebf\u6027\u6a21\u578b\u7684\u7ed3\u6784\u5047\u8bbe\u5bf9\u6f5c\u5728\u7279\u5f81\u4e0d\u73b0\u5b9e\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u9002\u5e94\u5b66\u4e60\u95ee\u9898\u7684\u5185\u5728\u7a00\u758f\u6027\u548c\u7ef4\u5ea6\uff0c\u4ece\u800c\u83b7\u5f97\u5feb\u901f\u6536\u655b\u901f\u7387\u3002"}}
{"id": "2506.14337", "pdf": "https://arxiv.org/pdf/2506.14337", "abs": "https://arxiv.org/abs/2506.14337", "authors": ["Even Eilertsen", "Vasileios Mavroeidis", "Gudmund Grov"], "title": "LLM-Powered Intent-Based Categorization of Phishing Emails", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u57fa\u4e8e\u610f\u56fe\u68c0\u6d4b\u548c\u5206\u7c7b\u9493\u9c7c\u90ae\u4ef6\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LLMs\u5728\u6b64\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9493\u9c7c\u653b\u51fb\u5bf9\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u4f20\u7edf\u68c0\u6d4b\u7cfb\u7edf\u4f9d\u8d56\u7528\u6237\u4e0d\u53ef\u89c1\u7684\u90ae\u4ef6\u5143\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u5e94\u5bf9\u4ec5\u901a\u8fc7\u6587\u672c\u5373\u53ef\u8bc6\u522b\u7684\u9493\u9c7c\u90ae\u4ef6\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22LLMs\u5728\u68c0\u6d4b\u548c\u5206\u7c7b\u9493\u9c7c\u90ae\u4ef6\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5408\u6cd5\u4e0e\u9493\u9c7c\u90ae\u4ef6\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5229\u7528LLMs\u8fdb\u884c\u9493\u9c7c\u90ae\u4ef6\u7684\u4e8c\u5143\u5206\u7c7b\u548c\u610f\u56fe\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86LLMs\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709LLMs\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u5206\u7c7b\u9493\u9c7c\u90ae\u4ef6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "LLMs\u5728\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u5a01\u80c1\u4fe1\u606f\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9493\u9c7c\u90ae\u4ef6\u610f\u56fe\u5206\u7c7b", "abstract_zh": "\u9493\u9c7c\u653b\u51fb\u4ecd\u7136\u662f\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u7684\u91cd\u5927\u5a01\u80c1\uff0c\u5b83\u4eec\u6210\u529f\u6b3a\u9a97\u4e86\u4eba\u7c7b\u53ca\u5176\u9632\u5fa1\u673a\u5236\u3002\u4f20\u7edf\u68c0\u6d4b\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u7528\u6237\u65e0\u6cd5\u5728\u6536\u4ef6\u7bb1\u4e2d\u770b\u5230\u7684\u90ae\u4ef6\u5143\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u5e94\u5bf9\u4ec5\u901a\u8fc7\u6587\u672c\u5373\u53ef\u8bc6\u522b\u7684\u9493\u9c7c\u90ae\u4ef6\u3002\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u5173\u6ce8\u90ae\u4ef6\u610f\u56fe\u68c0\u6d4b\u9493\u9c7c\u90ae\u4ef6\u7684\u5b9e\u9645\u6f5c\u529b\u3002\u9664\u4e86\u9493\u9c7c\u90ae\u4ef6\u7684\u4e8c\u5143\u5206\u7c7b\u5916\uff0c\u672c\u6587\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u610f\u56fe\u7c7b\u578b\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7LLMs\u5c06\u90ae\u4ef6\u5206\u7c7b\u4e3a\u4e0d\u540c\u7c7b\u522b\uff0c\u4ece\u800c\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u5a01\u80c1\u4fe1\u606f\u3002\u4e3a\u652f\u6301\u7814\u7a76\uff0c\u6211\u4eec\u6574\u7406\u4e86\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5408\u6cd5\u4e0e\u9493\u9c7c\u90ae\u4ef6\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709LLMs\u80fd\u591f\u68c0\u6d4b\u548c\u5206\u7c7b\u9493\u9c7c\u90ae\u4ef6\uff0c\u51f8\u663e\u4e86\u5176\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14375", "pdf": "https://arxiv.org/pdf/2506.14375", "abs": "https://arxiv.org/abs/2506.14375", "authors": ["Muhammad Hamza Yousuf", "Jason Li", "Sahar Vahdati", "Raphael Theilen", "Jakob Wittenstein", "Jens Lehmann"], "title": "IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards", "categories": ["cs.LG", "cs.AI"], "comment": "under review, PAIS track @ ECAI 2025", "summary": "Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIntelliLung\uff0c\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u68b0\u901a\u6c14\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u548c\u4e34\u5e8a\u5bf9\u9f50\u7684\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u5347\u60a3\u8005\u5b89\u5168\u6027\u548c\u4e2a\u6027\u5316\u652f\u6301\u3002", "motivation": "\u673a\u68b0\u901a\u6c14\uff08MV\uff09\u662fICU\u4e2d\u7ef4\u6301\u91cd\u75c7\u60a3\u8005\u751f\u547d\u7684\u5173\u952e\u7597\u6cd5\uff0c\u4f46\u5176\u8bbe\u7f6e\u4f18\u5316\u590d\u6742\u4e14\u6613\u51fa\u9519\u3002\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406MV\u7684\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\uff08\u8fde\u7eed\u548c\u79bb\u6563\uff09\uff0c\u4e14\u79bb\u6563\u5316\u52a8\u4f5c\u7a7a\u95f4\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\u3002", "method": "\u672c\u6587\u4f18\u5316\u4e86\u52a8\u4f5c\u7a7a\u95f4\u7f29\u51cf\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08IQL\u548cEDAC\uff09\u4ee5\u76f4\u63a5\u5904\u7406\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e34\u5e8a\u76ee\u6807\uff08\u5982\u65e0\u547c\u5438\u673a\u5929\u6570\u548c\u751f\u7406\u6307\u6807\uff09\u7684\u5956\u52b1\u51fd\u6570\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u7a00\u758f\u6b7b\u4ea1\u7387\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u673a\u68b0\u901a\u6c14\u7684\u5b89\u5168\u6027\uff0c\u652f\u6301\u4e2a\u6027\u5316\u6cbb\u7597\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u91cd\u75c7\u76d1\u62a4\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "conclusion": "IntelliLung\u901a\u8fc7\u7ed3\u5408\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u548c\u4e34\u5e8a\u5956\u52b1\u51fd\u6570\uff0c\u4e3a\u667a\u80fd\u673a\u68b0\u901a\u6c14\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6709\u671b\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "paper_title_zh": "IntelliLung\uff1a\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u4e0e\u4e34\u5e8a\u5bf9\u9f50\u5956\u52b1\u7684\u5b89\u5168\u673a\u68b0\u901a\u6c14\u4f18\u5316", "abstract_zh": "\u4fb5\u5165\u6027\u673a\u68b0\u901a\u6c14\uff08MV\uff09\u662f\u91cd\u75c7\u76d1\u62a4\u75c5\u623f\uff08ICU\uff09\u4e2d\u7ef4\u6301\u5371\u91cd\u60a3\u8005\u751f\u547d\u7684\u7597\u6cd5\uff0c\u4f46\u5176\u8bbe\u7f6e\u4f18\u5316\u56e0\u60a3\u8005\u7279\u5f02\u6027\u800c\u590d\u6742\u4e14\u6613\u9519\u3002\u5c3d\u7ba1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728MV\u63a7\u5236\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406MV\u7684\u6df7\u5408\uff08\u8fde\u7eed\u548c\u79bb\u6563\uff09\u52a8\u4f5c\u7279\u6027\u3002\u79bb\u6563\u5316\u52a8\u4f5c\u7a7a\u95f4\u4f1a\u56e0\u7ec4\u5408\u7206\u70b8\u9650\u5236\u53ef\u7528\u52a8\u4f5c\u5e76\u5f15\u5165\u5206\u5e03\u504f\u79fb\uff0c\u53ef\u80fd\u5371\u53ca\u5b89\u5168\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u7f29\u51cf\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u79bb\u7ebfRL\u7b97\u6cd5\uff08IQL\u548cEDAC\uff09\u4ee5\u76f4\u63a5\u5904\u7406\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\uff0c\u907f\u514d\u79bb\u6563\u5316\u7f3a\u9677\u3002\u6b64\u5916\uff0c\u5f15\u5165\u57fa\u4e8e\u65e0\u547c\u5438\u673a\u5929\u6570\u548c\u751f\u7406\u76ee\u6807\u7684\u4e34\u5e8a\u5956\u52b1\u51fd\u6570\uff0c\u76f8\u6bd4\u4f20\u7edf\u7a00\u758f\u6b7b\u4ea1\u7387\u5956\u52b1\u66f4\u5177\u4f18\u5316\u610f\u4e49\u3002\u7ed3\u679c\u8868\u660e\uff0cAI\u8f85\u52a9\u7684MV\u4f18\u5316\u53ef\u63d0\u5347\u60a3\u8005\u5b89\u5168\u6027\u5e76\u5b9e\u73b0\u4e2a\u6027\u5316\u80ba\u652f\u6301\uff0c\u4e3a\u667a\u80fd\u6570\u636e\u9a71\u52a8\u7684\u91cd\u75c7\u76d1\u62a4\u65b9\u6848\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.14386", "pdf": "https://arxiv.org/pdf/2506.14386", "abs": "https://arxiv.org/abs/2506.14386", "authors": ["Christian H. X. Ali Mehmeti-G\u00f6pel", "Michael Wand"], "title": "ResNets Are Deeper Than You Think", "categories": ["cs.LG", "cs.AI"], "comment": "NeurIPS 2025 Submission", "summary": "Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.", "AI": {"tldr": "\u6b8b\u5dee\u7f51\u7edc\uff08ResNets\uff09\u7684\u6027\u80fd\u4f18\u52bf\u4e0d\u4ec5\u6e90\u4e8e\u4f18\u5316\uff0c\u8fd8\u56e0\u5176\u72ec\u7279\u7684\u51fd\u6570\u7a7a\u95f4\u4e0e\u81ea\u7136\u6570\u636e\u7ed3\u6784\u66f4\u5339\u914d\u3002", "motivation": "\u5c3d\u7ba1\u6b8b\u5dee\u8fde\u63a5\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6027\u80fd\u4f18\u52bf\u7684\u539f\u56e0\u5c1a\u672a\u5b8c\u5168\u660e\u786e\u3002\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u6b8b\u5dee\u7f51\u7edc\u4ec5\u901a\u8fc7\u4f18\u5316\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u5176\u53ef\u80fd\u5177\u6709\u66f4\u6df1\u5c42\u6b21\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540e\u8bad\u7ec3\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5206\u79bb\u6cdb\u5316\u6027\u80fd\u548c\u53ef\u8bad\u7ec3\u6027\uff0c\u6bd4\u8f83\u53ef\u53d8\u6df1\u5ea6\u67b6\u6784\uff08\u7c7b\u4f3cResNets\uff09\u4e0e\u56fa\u5b9a\u6df1\u5ea6\u7f51\u7edc\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u4f18\u5316\u5f71\u54cd\u8f83\u5c0f\uff0c\u53ef\u53d8\u6df1\u5ea6\u67b6\u6784\u4ecd\u80fd\u6301\u7eed\u4f18\u4e8e\u56fa\u5b9a\u6df1\u5ea6\u7f51\u7edc\uff0c\u8bf4\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u6027\u80fd\u4f18\u52bf\u4e0d\u4ec5\u6765\u81ea\u4f18\u5316\u3002", "conclusion": "\u6b8b\u5dee\u7f51\u7edc\u7684\u4f18\u52bf\u6e90\u4e8e\u5176\u72ec\u7279\u7684\u51fd\u6570\u7a7a\u95f4\uff0c\u4e0e\u81ea\u7136\u6570\u636e\u7684\u7ed3\u6784\u66f4\u5339\u914d\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u4f18\u5316\u3002", "paper_title_zh": "\u6b8b\u5dee\u7f51\u7edc\u6bd4\u4f60\u60f3\u8c61\u7684\u66f4\u6df1", "abstract_zh": "\u6b8b\u5dee\u8fde\u63a5\u5728\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u51e0\u4e4e\u65e0\u5904\u4e0d\u5728\uff0c\u5176\u5e7f\u6cdb\u5e94\u7528\u901a\u5e38\u5f52\u529f\u4e8e\u5176\u663e\u8457\u63d0\u5347\u7684\u53ef\u8bad\u7ec3\u6027\uff1a\u6b8b\u5dee\u7f51\u7edc\u8bad\u7ec3\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\uff0c\u4e14\u6bd4\u524d\u9988\u7f51\u7edc\u8fbe\u5230\u66f4\u9ad8\u7cbe\u5ea6\u3002\u5c3d\u7ba1\u63d0\u51fa\u4e86\u4ece\u6539\u8fdb\u521d\u59cb\u5316\u5230\u9ad8\u7ea7\u5b66\u4e60\u7387\u8c03\u5ea6\u7b49\u591a\u79cd\u6280\u672f\u4ee5\u7f29\u5c0f\u6b8b\u5dee\u7f51\u7edc\u4e0e\u524d\u9988\u7f51\u7edc\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u8fd9\u4e00\u5dee\u8ddd\u4ecd\u7136\u5b58\u5728\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u89e3\u91ca\uff1a\u6b8b\u5dee\u7f51\u7edc\u4e0d\u4ec5\u662f\u5bf9\u524d\u9988\u7f51\u7edc\u7684\u91cd\u65b0\u53c2\u6570\u5316\uff0c\u800c\u662f\u5b58\u5728\u4e8e\u4e0d\u540c\u7684\u51fd\u6570\u7a7a\u95f4\u4e2d\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540e\u8bad\u7ec3\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u4ee5\u5206\u79bb\u6cdb\u5316\u6027\u80fd\u4e0e\u53ef\u8bad\u7ec3\u6027\uff1b\u53d1\u73b0\u7c7b\u4f3cResNets\u7684\u53ef\u53d8\u6df1\u5ea6\u67b6\u6784\u5373\u4f7f\u5728\u4f18\u5316\u5f71\u54cd\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u6301\u7eed\u4f18\u4e8e\u56fa\u5b9a\u6df1\u5ea6\u7f51\u7edc\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u6b8b\u5dee\u8fde\u63a5\u7684\u6027\u80fd\u4f18\u52bf\u8d85\u8d8a\u4e86\u4f18\u5316\uff0c\u6307\u5411\u4e86\u4e00\u79cd\u4e0e\u81ea\u7136\u6570\u636e\u7ed3\u6784\u66f4\u5339\u914d\u7684\u66f4\u6df1\u5c42\u6b21\u7684\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2506.14391", "pdf": "https://arxiv.org/pdf/2506.14391", "abs": "https://arxiv.org/abs/2506.14391", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Geyong Min", "Man Luo"], "title": "HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHiLight\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e0e\u5168\u5c40\u5bf9\u6297\u6307\u5bfc\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u534f\u8c03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u96be\u4ee5\u517c\u987e\u5168\u5c40\u534f\u8c03\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\uff0c\u5206\u6563\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u76ee\u6807\u3002HiLight\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u4e0e\u5bf9\u6297\u8bad\u7ec3\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "HiLight\u91c7\u7528\u5206\u5c42\u7ed3\u6784\uff1a\u9ad8\u5c42Meta-Policy\u901a\u8fc7Transformer-LSTM\u5212\u5206\u4ea4\u901a\u7f51\u7edc\u5e76\u751f\u6210\u5b50\u76ee\u6807\uff1b\u4f4e\u5c42Sub-Policy\u63a7\u5236\u5355\u4e2a\u4ea4\u53c9\u53e3\u5e76\u5177\u5907\u5168\u5c40\u610f\u8bc6\u3002\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u673a\u5236\uff0cMeta-Policy\u751f\u6210\u6311\u6218\u6027\u5b50\u76ee\u6807\uff0cSub-Policy\u5b66\u4e60\u8d85\u8d8a\u76ee\u6807\u4ee5\u63d0\u5347\u534f\u8c03\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiLight\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "HiLight\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u6297\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u534f\u8c03\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "HiLight\uff1a\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e0e\u5168\u5c40\u5bf9\u6297\u6307\u5bfc\u7684\u5927\u89c4\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6846\u67b6", "abstract_zh": "\u9ad8\u6548\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08TSC\uff09\u5bf9\u7f13\u89e3\u57ce\u5e02\u62e5\u5835\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u96be\u4ee5\u517c\u987e\u5168\u5c40\u534f\u8c03\u4e0e\u53ef\u6269\u5c55\u6027\u3002\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\uff0c\u5206\u6563\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u76ee\u6807\uff0c\u5bfc\u81f4\u7f51\u7edc\u6548\u7387\u53d7\u9650\u3002\u672c\u6587\u63d0\u51faHiLight\uff0c\u4e00\u79cd\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e0e\u5168\u5c40\u5bf9\u6297\u6307\u5bfc\u7684\u5927\u89c4\u6a21TSC\u6846\u67b6\u3002HiLight\u5305\u542b\u9ad8\u5c42Meta-Policy\uff08\u901a\u8fc7Transformer-LSTM\u5212\u5206\u4ea4\u901a\u7f51\u7edc\u5e76\u751f\u6210\u5b50\u76ee\u6807\uff09\u548c\u4f4e\u5c42Sub-Policy\uff08\u63a7\u5236\u5355\u4e2a\u4ea4\u53c9\u53e3\u5e76\u5177\u5907\u5168\u5c40\u610f\u8bc6\uff09\u3002\u4e3a\u63d0\u5347\u5168\u5c40\u89c4\u5212\u4e0e\u5c40\u90e8\u6267\u884c\u7684\u534f\u540c\u6027\uff0c\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u673a\u5236\uff1aMeta-Policy\u751f\u6210\u6311\u6218\u6027\u5b50\u76ee\u6807\uff0cSub-Policy\u5b66\u4e60\u8d85\u8d8a\u76ee\u6807\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u534f\u8c03\u3002\u5b9e\u9a8c\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u9ad8\u5cf0\u8fc7\u6e21\u3001\u6076\u52a3\u5929\u6c14\u548c\u8282\u5047\u65e5\u6d41\u91cf\u7684\u5927\u89c4\u6a21\u66fc\u54c8\u987f\u7f51\u7edc\u3002\u7ed3\u679c\u8868\u660e\uff0cHiLight\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u4f18\u52bf\u663e\u8457\uff0c\u4e14\u5728\u4e0d\u540c\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.14411", "pdf": "https://arxiv.org/pdf/2506.14411", "abs": "https://arxiv.org/abs/2506.14411", "authors": ["John Wikman", "Alexandre Proutiere", "David Broman"], "title": "Adaptive Reinforcement Learning for Unobservable Random Delays", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u53ef\u89c2\u6d4b\u7684\u968f\u673a\u5ef6\u8fdf\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u4ea4\u4e92\u5c42\u548c\u52a8\u6001\u8c03\u6574\u7b97\u6cd5ACDA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5ef6\u8fdf\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\u65e0\u5ef6\u8fdf\uff0c\u4f46\u5728\u5b9e\u9645\u52a8\u6001\u73af\u5883\u4e2d\uff08\u5982\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff09\uff0c\u5ef6\u8fdf\u662f\u5e38\u89c1\u4e14\u4e0d\u53ef\u89c2\u6d4b\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4fdd\u5b88\u5730\u5047\u8bbe\u5ef6\u8fdf\u4e0a\u9650\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4ea4\u4e92\u5c42\u6846\u67b6\uff0c\u751f\u6210\u672a\u6765\u52a8\u4f5c\u77e9\u9635\u4ee5\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\u548c\u4e22\u5931\u7684\u52a8\u4f5c\u5305\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u6a21\u578b\u5316\u7b97\u6cd5ACDA\uff0c\u52a8\u6001\u9002\u5e94\u5ef6\u8fdf\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u8fd0\u52a8\u73af\u5883\u4e2d\uff0cACDA\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5904\u7406\u968f\u673a\u5ef6\u8fdf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6846\u67b6\u548cACDA\u7b97\u6cd5\u4e3a\u5904\u7406\u4e0d\u53ef\u89c2\u6d4b\u968f\u673a\u5ef6\u8fdf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u5e94\u5bf9\u4e0d\u53ef\u89c2\u6d4b\u968f\u673a\u5ef6\u8fdf", "abstract_zh": "\u5728\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u73af\u5883\u4e2d\uff0c\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u901a\u5e38\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5047\u8bbe\u667a\u80fd\u4f53\u80fd\u5373\u65f6\u89c2\u6d4b\u7cfb\u7edf\u72b6\u6001\u5e76\u7acb\u5373\u6267\u884c\u52a8\u4f5c\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u52a8\u6001\u73af\u5883\uff08\u5982\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff09\u4e2d\uff0c\u8fd9\u4e00\u5047\u8bbe\u5e38\u56e0\u4ea4\u4e92\u5ef6\u8fdf\u800c\u5931\u6548\u3002\u8fd9\u4e9b\u5ef6\u8fdf\u968f\u65f6\u95f4\u968f\u673a\u53d8\u5316\u4e14\u901a\u5e38\u4e0d\u53ef\u89c2\u6d4b\uff0c\u5bfc\u81f4\u52a8\u4f5c\u51b3\u7b56\u65f6\u672a\u77e5\u5ef6\u8fdf\u60c5\u51b5\u3002\u73b0\u6709\u65b9\u6cd5\u4fdd\u5b88\u5730\u5047\u8bbe\u5ef6\u8fdf\u4e0a\u9650\u5df2\u77e5\uff0c\u5373\u4f7f\u5b9e\u9645\u5ef6\u8fdf\u901a\u5e38\u66f4\u4f4e\u3002\u672c\u6587\u63d0\u51fa\u4ea4\u4e92\u5c42\u6846\u67b6\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u81ea\u9002\u5e94\u5904\u7406\u4e0d\u53ef\u89c2\u6d4b\u4e14\u65f6\u53d8\u7684\u5ef6\u8fdf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u667a\u80fd\u4f53\u751f\u6210\u672a\u6765\u52a8\u4f5c\u77e9\u9635\u4ee5\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\u548c\u7f51\u7edc\u52a8\u4f5c\u5305\u4e22\u5931\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u6a21\u578b\u5316\u7b97\u6cd5ACDA\uff08\u5e26\u5ef6\u8fdf\u9002\u5e94\u7684\u884c\u52a8\u8005-\u8bc4\u8bba\u8005\uff09\uff0c\u52a8\u6001\u9002\u5e94\u5ef6\u8fdf\u6a21\u5f0f\u3002\u5728\u591a\u79cd\u8fd0\u52a8\u57fa\u51c6\u73af\u5883\u4e2d\uff0cACDA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.14412", "pdf": "https://arxiv.org/pdf/2506.14412", "abs": "https://arxiv.org/abs/2506.14412", "authors": ["Tim Cofala", "Oleh Astappiev", "William Xion", "Hailay Teklehaymanot"], "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "4 pages, 5 figures. Report for SIGIR 2025 LiveRAG Challenge", "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728SIGIR LiveRAG\u7ade\u8d5b\u4e2d\u8bc4\u4f30RAG\u751f\u6210\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u7ed3\u5408InstructRAG\u3001Pinecone\u68c0\u7d22\u5668\u548cBGE\u91cd\u6392\u5668\uff0c\u6700\u7ec8\u65b9\u6848\u5728\u7ade\u8d5b\u4e2d\u6392\u540d\u7b2c\u56db\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u53c2\u4e0eSIGIR LiveRAG\u7ade\u8d5b\u4ee5\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86InstructRAG\u7ed3\u5408Pinecone\u68c0\u7d22\u5668\u548cBGE\u91cd\u6392\u5668\u7684\u65b9\u6cd5\uff0c\u5728\u7ade\u8d5b\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u4e86\u4e0d\u540c\u68c0\u7d22\u5668\u7ec4\u5408\u548cRAG\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6700\u7ec8\u65b9\u6848\u5728SIGIR 2025 LiveRAG\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4e861.13\u7684\u6b63\u786e\u6027\u5206\u6570\u548c0.55\u7684\u5fe0\u5b9e\u6027\u5206\u6570\uff0c\u6392\u540d\u7b2c\u56db\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cInstructRAG\u7ed3\u5408Pinecone\u68c0\u7d22\u5668\u548cBGE\u91cd\u6392\u5668\u662f\u4e00\u79cd\u6709\u6548\u7684RAG\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7ade\u8d5b\u6761\u4ef6\u4e0b\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "paper_title_zh": "RAGtifier\uff1a\u8bc4\u4f30SIGIR LiveRAG\u7ade\u8d5b\u4e2d\u6700\u5148\u8fdbRAG\u7cfb\u7edf\u7684\u751f\u6210\u65b9\u6cd5", "abstract_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u90e8\u53c2\u6570\u5316\u77e5\u8bc6\u4e0e\u5916\u90e8\u975e\u53c2\u6570\u5316\u6765\u6e90\u7ed3\u5408\uff0c\u65e8\u5728\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002LiveRAG 2025\u6311\u6218\u8d5b\u63a2\u7d22\u4e86RAG\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6700\u5927\u5316\u5bf9DataMorgana\u95ee\u7b54\u5bf9\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u4e9b\u95ee\u7b54\u5305\u62ec\u5355\u8df3\u548c\u591a\u8df3\u95ee\u9898\u3002\u6311\u6218\u8d5b\u63d0\u4f9b\u4e86\u5bf9Fineweb 10BT\u6570\u636e\u96c6\u7684\u7a00\u758fOpenSearch\u548c\u5bc6\u96c6Pinecone\u7d22\u5f15\u7684\u8bbf\u95ee\uff0c\u5e76\u9650\u5236\u4f7f\u7528\u53c2\u6570\u4e0d\u8d85\u8fc710B\u7684LLMs\uff0c\u6700\u7ec8\u7b54\u6848\u751f\u6210\u4f7f\u7528Falcon-3-10B\u3002\u8bc4\u59d4LLM\u548c\u4eba\u7c7b\u8bc4\u4f30\u8005\u5171\u540c\u8bc4\u4f30\u63d0\u4ea4\u7684\u7b54\u6848\u3002\u901a\u8fc7\u63a2\u7d22\u6311\u6218\u6761\u4ef6\u4e0b\u7684\u4e0d\u540c\u68c0\u7d22\u5668\u7ec4\u5408\u548cRAG\u89e3\u51b3\u65b9\u6848\uff0c\u6211\u4eec\u7684\u6700\u7ec8\u65b9\u6848\u91c7\u7528\u4e86InstructRAG\u7ed3\u5408Pinecone\u68c0\u7d22\u5668\u548cBGE\u91cd\u6392\u5668\u3002\u8be5\u65b9\u6848\u5728SIGIR 2025 LiveRAG\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4e861.13\u7684\u6b63\u786e\u6027\u5206\u6570\u548c0.55\u7684\u5fe0\u5b9e\u6027\u5206\u6570\uff0c\u6392\u540d\u7b2c\u56db\u3002"}}
{"id": "2506.14425", "pdf": "https://arxiv.org/pdf/2506.14425", "abs": "https://arxiv.org/abs/2506.14425", "authors": ["Tomofumi Kitamura", "Alex Fukunaga"], "title": "Is Selection All You Need in Differential Evolution?", "categories": ["cs.NE", "cs.AI"], "comment": "39 pages, 7 figures", "summary": "Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65e0\u754c\u5dee\u5206\u8fdb\u5316\uff08UDE\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b8c\u5168\u53d6\u6d88\u4f20\u7edf\u5dee\u5206\u8fdb\u5316\u4e2d\u7684\u4e2a\u4f53\u66ff\u6362\u673a\u5236\uff0c\u4ec5\u4f9d\u8d56\u9009\u62e9\u673a\u5236\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u5e76\u63d0\u5347\u4e86\u641c\u7d22\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u7b97\u6cd5\u5728\u56fa\u5b9a\u79cd\u7fa4\u5927\u5c0f\u4e0b\u5b58\u5728\u79cd\u7fa4\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5b58\u6863\u673a\u5236\u867d\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5f15\u5165\u4e86\u989d\u5916\u7684\u8bbe\u8ba1\u590d\u6742\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u65e0\u754c\u5dee\u5206\u8fdb\u5316\uff08UDE\uff09\u6846\u67b6\uff0c\u53d6\u6d88\u4f20\u7edfDE\u4e2d\u7684\u4e2a\u4f53\u66ff\u6362\u6b65\u9aa4\uff0c\u5c06\u6240\u6709\u751f\u6210\u7684\u5019\u9009\u4e2a\u4f53\u4fdd\u7559\u5728\u79cd\u7fa4\u4e2d\uff0c\u4ec5\u901a\u8fc7\u9009\u62e9\u673a\u5236\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "UDE\u901a\u8fc7\u53d6\u6d88\u66ff\u6362\u673a\u5236\u548c\u5b58\u6863\u7ba1\u7406\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u641c\u7d22\u80fd\u529b\uff0c\u4e3a\u5dee\u5206\u8fdb\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "conclusion": "UDE\u662f\u4e00\u79cd\u5168\u65b0\u7684\u5dee\u5206\u8fdb\u5316\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u9009\u62e9\u673a\u5236\uff0c\u907f\u514d\u4e86\u4f20\u7edfDE\u7684\u590d\u6742\u6027\uff0c\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u4e14\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5dee\u5206\u8fdb\u5316\u4e2d\uff0c\u9009\u62e9\u673a\u5236\u662f\u5426\u8db3\u591f\uff1f", "abstract_zh": "\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u662f\u4e00\u79cd\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u7684\u8fdb\u5316\u7b97\u6cd5\u3002\u7136\u800c\uff0c\u5728\u73b0\u4ee3DE\u5b9e\u73b0\u4e2d\uff0c\u56fa\u5b9a\u79cd\u7fa4\u5927\u5c0f\u5bfc\u81f4\u7684\u79cd\u7fa4\u591a\u6837\u6027\u4e0d\u8db3\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u79cd\u7fa4\u5927\u5c0f\u662f\u4e00\u4e2a\u5173\u952e\u63a7\u5236\u53c2\u6570\uff0c\u663e\u8457\u5f71\u54cdDE\u6027\u80fd\u3002\u8f83\u5927\u7684\u79cd\u7fa4\u5305\u542b\u66f4\u591a\u6837\u5316\u7684\u4e2a\u4f53\uff0c\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u5730\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\uff1b\u800c\u5728\u8bc4\u4f30\u9884\u7b97\u53d7\u9650\u65f6\uff0c\u8f83\u5c0f\u7684\u79cd\u7fa4\u53ef\u80fd\u66f4\u9002\u5408\u4e13\u6ce8\u4e8e\u5c11\u6570\u6709\u6f5c\u529b\u7684\u5019\u9009\u4e2a\u4f53\u3002\u8bb8\u591a\u5148\u8fdb\u7684DE\u53d8\u4f53\u91c7\u7528\u5b58\u6863\u673a\u5236\uff0c\u4fdd\u7559\u90e8\u5206\u88ab\u6dd8\u6c70\u4e2a\u4f53\u5e76\u5728\u53d8\u5f02\u64cd\u4f5c\u4e2d\u91cd\u7528\uff0c\u4f46\u8fd9\u5f15\u5165\u4e86\u989d\u5916\u7684\u8bbe\u8ba1\u590d\u6742\u6027\uff0c\u5982\u63d2\u5165\u3001\u5220\u9664\u548c\u5927\u5c0f\u8c03\u6574\u7b56\u7565\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65e0\u754c\u5dee\u5206\u8fdb\u5316\uff08UDE\uff09\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u6240\u6709\u751f\u6210\u7684\u5019\u9009\u4e2a\u4f53\u52a0\u5165\u79cd\u7fa4\uff0c\u4e0d\u57fa\u4e8e\u9002\u5e94\u5ea6\u6dd8\u6c70\u4efb\u4f55\u4e2a\u4f53\u3002\u4e0e\u4f20\u7edfDE\u4e0d\u540c\uff0cUDE\u5b8c\u5168\u53d6\u6d88\u4e86\u66ff\u6362\u673a\u5236\u53ca\u76f8\u5173\u590d\u6742\u6027\uff0c\u4ec5\u4f9d\u8d56\u9009\u62e9\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u5f3a\u5927\u7684\u641c\u7d22\u7b97\u6cd5\u3002"}}
{"id": "2506.14434", "pdf": "https://arxiv.org/pdf/2506.14434", "abs": "https://arxiv.org/abs/2506.14434", "authors": ["Bidisha Sharma", "Karthik Pandia Durai", "Shankar Venkatesan", "Jeena J Prakash", "Shashi Kumar", "Malolan Chetlur", "Andreas Stolcke"], "title": "Unifying Streaming and Non-streaming Zipformer-based ASR", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted in ACL2025 Industry track", "summary": "There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6d41\u5f0f\u548c\u975e\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53f3\u4e0a\u4e0b\u6587\u8bad\u7ec3Zipformer\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\u7684\u540c\u65f6\uff0c\u7528\u6237\u611f\u77e5\u5ef6\u8fdf\u4ec5\u8f7b\u5fae\u589e\u52a0\u3002", "motivation": "\u4e3a\u51cf\u5c11\u6d41\u5f0f\u548c\u975e\u6d41\u5f0fASR\u6a21\u578b\u7684\u5f00\u53d1\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u6210\u672c\uff0c\u7814\u7a76\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u52a8\u6001\u53f3\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8eZipformer\u7684ASR\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5757\u6ce8\u610f\u529b\u63a9\u7801\u52a8\u6001\u8c03\u6574\u53f3\u4e0a\u4e0b\u6587\u5e27\u6570\uff0c\u5206\u6790\u5176\u5bf9\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u4f7f\u7528Librispeech\u548c\u5185\u90e8\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5e76\u5728\u751f\u4ea7\u7ea7\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e7.9%\uff0c\u7528\u6237\u611f\u77e5\u5ef6\u8fdf\u7565\u6709\u589e\u52a0\u3002\u589e\u52a0\u53f3\u4e0a\u4e0b\u6587\u5e27\u6570\u53ef\u4f7f\u6d41\u5f0f\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u975e\u6d41\u5f0f\u6a21\u578b\uff0c\u4e14\u652f\u6301\u6839\u636e\u9700\u6c42\u7075\u6d3b\u8c03\u6574\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "\u7edf\u4e00\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86ASR\u6a21\u578b\u7684\u5f00\u53d1\u6210\u672c\uff0c\u52a8\u6001\u53f3\u4e0a\u4e0b\u6587\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u7684\u5ef6\u8fdf-\u51c6\u786e\u6027\u6743\u8861\uff0c\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u3002", "paper_title_zh": "\u57fa\u4e8eZipformer\u7684\u7edf\u4e00\u6d41\u5f0f\u4e0e\u975e\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u7edf\u4e00\u6d41\u5f0f\u548c\u975e\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u4ee5\u51cf\u5c11\u5f00\u53d1\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u6210\u672c\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6765\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8bad\u7ec3\u4e00\u4e2a\u7aef\u5230\u7aef\u7684ASR\u6a21\u578b\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u6d41\u5f0f\u548c\u975e\u6d41\u5f0f\u5e94\u7528\u3002\u5728Zipformer\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5206\u5757\u6ce8\u610f\u529b\u63a9\u7801\u52a8\u6001\u8c03\u6574\u53f3\u4e0a\u4e0b\u6587\u3002\u7814\u7a76\u8868\u660e\uff0c\u7531\u4e8e\u5176\u591a\u5c3a\u5ea6\u7279\u6027\uff0cZipformer\u6a21\u578b\u4e2d\u4f7f\u7528\u53f3\u4e0a\u4e0b\u6587\u6bd4\u5176\u4ed6Conformer\u6a21\u578b\u66f4\u6709\u6548\u3002\u6211\u4eec\u5206\u6790\u4e86\u4e0d\u540c\u53f3\u4e0a\u4e0b\u6587\u5e27\u6570\u5bf9\u6d41\u5f0fASR\u6a21\u578b\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u4f7f\u7528Librispeech\u548c\u5927\u578b\u5185\u90e8\u5bf9\u8bdd\u6570\u636e\u96c6\u8bad\u7ec3\u4e0d\u540c\u7248\u672c\u7684\u6d41\u5f0f\u548c\u975e\u6d41\u5f0f\u6a21\u578b\uff0c\u5e76\u5728\u751f\u4ea7\u7ea7\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\u8de8\u591a\u4e2a\u9886\u57df\u6d4b\u8bd5\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002\u6240\u63d0\u7b56\u7565\u5c06\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e7.9%\uff0c\u7528\u6237\u611f\u77e5\u5ef6\u8fdf\u4ec5\u8f7b\u5fae\u589e\u52a0\u3002\u901a\u8fc7\u589e\u52a0\u53f3\u4e0a\u4e0b\u6587\u5e27\u6570\uff0c\u6211\u4eec\u80fd\u591f\u4f7f\u6d41\u5f0f\u6a21\u578b\u7684\u6027\u80fd\u63a5\u8fd1\u975e\u6d41\u5f0f\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u8fd8\u652f\u6301\u6839\u636e\u5ba2\u6237\u9700\u6c42\u7075\u6d3b\u63a7\u5236\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002"}}
{"id": "2506.14438", "pdf": "https://arxiv.org/pdf/2506.14438", "abs": "https://arxiv.org/abs/2506.14438", "authors": ["Pol Ar\u00e9valo", "Alexis Molina", "\u00c1lvaro Ciudad"], "title": "sHGCN: Simplified hyperbolic graph convolutional neural networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u53cc\u66f2\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08sHGCN\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u4e2d\u66f4\u5177\u53ef\u884c\u6027\u3002", "motivation": "\u53cc\u66f2\u51e0\u4f55\u5728\u5efa\u6a21\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\uff08\u5c24\u5176\u662f\u5177\u6709\u5c42\u6b21\u6216\u6811\u72b6\u5173\u7cfb\u7684\u6570\u636e\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f20\u7edf\u7684\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5316\u64cd\u4f5c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7b80\u5316\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u64cd\u4f5c\uff0c\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\uff0c\u4ece\u800c\u63d0\u5347\u8fd0\u884c\u901f\u5ea6\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5316\u540e\u7684\u53cc\u66f2\u64cd\u4f5c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u7b80\u5316\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6548\u7387\uff0c\u8fd8\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u590d\u6742\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u4f18\u9009\u62e9\u3002", "paper_title_zh": "sHGCN\uff1a\u7b80\u5316\u7684\u53cc\u66f2\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "abstract_zh": "\u53cc\u66f2\u51e0\u4f55\u5df2\u6210\u4e3a\u5efa\u6a21\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u5c42\u6b21\u6216\u6811\u72b6\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7\u5b9e\u73b0\u66f4\u4f4e\u5931\u771f\u7684\u5d4c\u5165\uff0c\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u4e3a\u6355\u6349\u590d\u6742\u6570\u636e\u7ed3\u6784\u63d0\u4f9b\u4e86\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6027\u80fd\u6311\u6218\u3002\u672c\u6587\u901a\u8fc7\u7b80\u5316\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u65f6\u95f4\u548c\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u7684\u53cc\u66f2\u64cd\u4f5c\u53ef\u5927\u5e45\u63d0\u9ad8\u8ba1\u7b97\u901f\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f7f\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u5728\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u4e2d\u66f4\u5177\u53ef\u884c\u6027\u3002"}}
{"id": "2506.14456", "pdf": "https://arxiv.org/pdf/2506.14456", "abs": "https://arxiv.org/abs/2506.14456", "authors": ["Elija Perrier"], "title": "Hamiltonian Formalism for Comparing Quantum and Classical Intelligence", "categories": ["quant-ph", "cs.AI"], "comment": "This is the version accepted at AGI 25 (camera ready length limit of 10 pages plus references and appendices). Further work detailing bounds and limitations is in preparation. Comments and criticisms welcome", "summary": "The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u54c8\u5bc6\u987f\u5f62\u5f0f\u4e3b\u4e49\uff0c\u7528\u4e8e\u6bd4\u8f83\u91cf\u5b50\u4e0e\u7ecf\u5178\u667a\u80fd\u5728\u73af\u5883\u4ea4\u4e92\u4e2d\u7684\u5dee\u5f02\uff0c\u4e3a\u91cf\u5b50\u4e0e\u7ecf\u5178AGI\u4efb\u52a1\u63d0\u4f9b\u6570\u5b66\u6846\u67b6\u3002", "motivation": "\u91cf\u5b50\u57fa\u677f\u4e0a\u5b9e\u73b0AGI\u7684\u524d\u666f\u4fc3\u4f7f\u5f00\u53d1\u6570\u5b66\u6846\u67b6\uff0c\u4ee5\u76f4\u63a5\u6bd4\u8f83\u5176\u5728\u7ecf\u5178\u4e0e\u91cf\u5b50\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u5dee\u5f02\u3002", "method": "\u5f15\u5165\u54c8\u5bc6\u987f\u5f62\u5f0f\u4e3b\u4e49\uff0c\u5c06AGI\u52a8\u529b\u5b66\u5206\u89e3\u4e3a\u6838\u5fc3\u529f\u80fd\uff08\u5982\u5f52\u7eb3\u3001\u63a8\u7406\u3001\u9012\u5f52\u3001\u5b66\u4e60\u3001\u6d4b\u91cf\u548c\u8bb0\u5fc6\uff09\u7684\u54c8\u5bc6\u987f\u751f\u6210\u5668\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u8bed\u8a00\uff0c\u7528\u4e8e\u7cbe\u786e\u63cf\u8ff0\u91cf\u5b50\u4e0e\u7ecf\u5178\u667a\u80fd\u5728\u73af\u5883\u4ea4\u4e92\u4e2d\u7684\u5dee\u5f02\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u4e3b\u4e49\u4e3a\u91cf\u5b50\u4e0e\u7ecf\u5178AGI\u7684\u5bf9\u6bd4\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e24\u8005\u5dee\u5f02\u3002", "paper_title_zh": "\u6bd4\u8f83\u91cf\u5b50\u4e0e\u7ecf\u5178\u667a\u80fd\u7684\u54c8\u5bc6\u987f\u5f62\u5f0f\u4e3b\u4e49", "abstract_zh": "\u91cf\u5b50\u57fa\u677f\u4e0a\u5b9e\u73b0AGI\u7684\u524d\u666f\u4fc3\u4f7f\u5f00\u53d1\u6570\u5b66\u6846\u67b6\uff0c\u4ee5\u76f4\u63a5\u6bd4\u8f83\u5176\u5728\u7ecf\u5178\u4e0e\u91cf\u5b50\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u54c8\u5bc6\u987f\u5f62\u5f0f\u4e3b\u4e49\uff0c\u7528\u4e8e\u63cf\u8ff0\u7ecf\u5178\u4e0e\u91cf\u5b50AGI\u4efb\u52a1\uff0c\u4ee5\u5bf9\u6bd4\u5b83\u4eec\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u3002\u6211\u4eec\u63d0\u51fa\u5c06AGI\u52a8\u529b\u5b66\u5206\u89e3\u4e3a\u6838\u5fc3\u529f\u80fd\uff08\u5982\u5f52\u7eb3\u3001\u63a8\u7406\u3001\u9012\u5f52\u3001\u5b66\u4e60\u3001\u6d4b\u91cf\u548c\u8bb0\u5fc6\uff09\u7684\u54c8\u5bc6\u987f\u751f\u6210\u5668\u3002\u8fd9\u4e00\u5f62\u5f0f\u4e3b\u4e49\u65e8\u5728\u4e3a\u91cf\u5b50\u4e0e\u7ecf\u5178\u667a\u80fd\u5728\u73af\u5883\u4ea4\u4e92\u4e2d\u7684\u5dee\u5f02\u63d0\u4f9b\u7cbe\u786e\u7684\u6570\u5b66\u8bed\u8a00\u3002"}}
{"id": "2506.14464", "pdf": "https://arxiv.org/pdf/2506.14464", "abs": "https://arxiv.org/abs/2506.14464", "authors": ["Maximilian Baronig", "Yeganeh Bahariasl", "Ozan \u00d6zdenizci", "Robert Legenstein"], "title": "A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHYPR\u7684\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e76\u884c\u5316\u4e0e\u8fd1\u4f3c\u5728\u7ebf\u524d\u5411\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5faa\u73af\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08RSNN\uff09\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u6d88\u8017\u548c\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5728\u632f\u8361\u4e9a\u9608\u503c\u52a8\u6001\u795e\u7ecf\u5143\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5faa\u73af\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08RSNN\uff09\u5728\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u4e2d\u9ad8\u6548\u5b9e\u73b0\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u65b9\u6cd5\uff08\u5982BPTT\uff09\u5b58\u5728\u5185\u5b58\u6d88\u8017\u5927\u4e14\u4e0d\u652f\u6301\u5728\u7ebf\u5b66\u4e60\u7684\u9650\u5236\u3002\u524d\u5411\u68af\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u652f\u6301\u5728\u7ebf\u5b66\u4e60\uff0c\u4f46\u5728\u4f20\u7edf\u786c\u4ef6\u4e0a\u901f\u5ea6\u6162\u4e14\u6027\u80fd\u8f83\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u517c\u5177\u9ad8\u6548\u5e76\u884c\u5316\u548c\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faHYPR\uff08\u6df7\u5408\u4f20\u64ad\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u5e76\u884c\u5316\u4e0e\u8fd1\u4f3c\u5728\u7ebf\u524d\u5411\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5e76\u884c\u5316\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u7684\u6052\u5b9a\u5185\u5b58\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u51e0\u4e4e\u4efb\u610f\u975e\u7ebf\u6027\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u7684RSNN\u3002", "result": "HYPR\u5728\u632f\u8361\u4e9a\u9608\u503c\u52a8\u6001\u795e\u7ecf\u5143\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u8fd1\u4f3c\u524d\u5411\u68af\u5ea6\u5b66\u4e60\u4e0eBPTT\u4e4b\u95f4\u7684\u4efb\u52a1\u6027\u80fd\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u4f4e\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "HYPR\u4e3aRSNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5185\u5b58\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u548c\u590d\u6742\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "paper_title_zh": "\u5faa\u73af\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5", "abstract_zh": "\u5faa\u73af\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08RSNN\uff09\u5728\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8fd0\u884c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u8bad\u7ec3\u901a\u5e38\u91c7\u7528\u57fa\u4e8e\u65f6\u95f4\u7684\u53cd\u5411\u4f20\u64ad\uff08BPTT\uff09\u5728\u6807\u51c6\u6570\u5b57\u786c\u4ef6\u4e0a\u8fdb\u884c\uff0c\u4f46BPTT\u5b58\u5728\u663e\u8457\u9650\u5236\uff1a\u4e0d\u652f\u6301\u5728\u7ebf\u8bad\u7ec3\uff0c\u4e14\u5185\u5b58\u6d88\u8017\u968f\u8ba1\u7b97\u6b65\u9aa4\u7ebf\u6027\u589e\u52a0\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u4e8e\u524d\u5411\u68af\u5ea6\u4f20\u64ad\u7684\u5b66\u4e60\u65b9\u6cd5\u652f\u6301\u5728\u7ebf\u5b66\u4e60\uff0c\u4e14\u5185\u5b58\u6d88\u8017\u4e0e\u65f6\u95f4\u6b65\u957f\u65e0\u5173\uff0c\u4f7fSNN\u80fd\u591f\u4ece\u8fde\u7eed\u3001\u65e0\u9650\u957f\u7684\u8f93\u5165\u5e8f\u5217\u4e2d\u5b66\u4e60\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4f20\u7edf\u786c\u4ef6\u4e0a\u6267\u884c\u901f\u5ea6\u6162\u4e14\u6027\u80fd\u8f83\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u63d0\u51faHYPR\uff08\u6df7\u5408\u4f20\u64ad\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e76\u884c\u5316\u4e0e\u8fd1\u4f3c\u5728\u7ebf\u524d\u5411\u5b66\u4e60\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u5e76\u884c\u5316\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u7684\u6052\u5b9a\u5185\u5b58\u9700\u6c42\u3002HYPR\u652f\u6301\u5bf9\u51e0\u4e4e\u4efb\u610f\u975e\u7ebf\u6027\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u7684RSNN\u8fdb\u884c\u5b50\u5e8f\u5217\u53c2\u6570\u66f4\u65b0\u7684\u5e76\u884c\u8ba1\u7b97\u3002\u6211\u4eec\u5c06HYPR\u5e94\u7528\u4e8e\u5177\u6709\u632f\u8361\u4e9a\u9608\u503c\u52a8\u6001\u7684\u8109\u51b2\u795e\u7ecf\u5143\u7f51\u7edc\uff0c\u53d1\u73b0\u6b64\u7c7b\u795e\u7ecf\u5143\u6a21\u578b\u7279\u522b\u9002\u5408HYPR\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4f3c\u524d\u5411\u68af\u5ea6\u5b66\u4e60\u4e0eBPTT\u4e4b\u95f4\u524d\u6240\u672a\u6709\u7684\u4f4e\u4efb\u52a1\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2506.14472", "pdf": "https://arxiv.org/pdf/2506.14472", "abs": "https://arxiv.org/abs/2506.14472", "authors": ["Fabien Bernier", "Maxime Cordy", "Yves Le Traon"], "title": "Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks", "categories": ["cs.LG", "cs.AI"], "comment": "ECML PKDD 2025", "summary": "Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.\n  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u7535\u529b\u6d88\u8017\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u56e0\u7d20\uff08\u5982\u5929\u6c14\u6307\u6807\uff09\u63d0\u5347\u5168\u7403\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u57286000\u591a\u6237\u5362\u68ee\u5821\u5bb6\u5ead\u7684\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u7535\u529b\u6d88\u8017\u9884\u6d4b\u4e3b\u8981\u4f9d\u8d56\u5386\u53f2\u6570\u636e\uff0c\u800c\u5f15\u5165\u5916\u90e8\u56e0\u7d20\uff08\u5982\u5929\u6c14\u3001\u8282\u5047\u65e5\u7b49\uff09\u867d\u80fd\u63d0\u5347\u4e2a\u4f53\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5374\u53ef\u80fd\u964d\u4f4e\u5168\u7403\u6a21\u578b\u7684\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u5916\u90e8\u56e0\u7d20\u7684\u6709\u6548\u5229\u7528\u3002", "method": "\u91c7\u7528\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u6839\u636e\u5916\u90e8\u56e0\u7d20\u52a8\u6001\u8c03\u6574\u6a21\u578b\u6743\u91cd\uff0c\u7ed3\u54086000\u591a\u6237\u5362\u68ee\u5821\u5bb6\u5ead\u7684\u7535\u529b\u6d88\u8017\u6570\u636e\u53ca\u5929\u6c14\u3001\u8282\u5047\u65e5\u7b49\u5916\u90e8\u56e0\u7d20\u8fdb\u884c\u8bad\u7ec3\u4e0e\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8d85\u7f51\u7edc\u65b9\u6cd5\u5728\u7ed3\u5408\u5916\u90e8\u56e0\u7d20\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5168\u7403\u6a21\u578b\u7684\u4f18\u52bf\u3002", "conclusion": "\u8d85\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u6709\u6548\u5229\u7528\u5916\u90e8\u56e0\u7d20\u63d0\u5347\u7535\u529b\u6d88\u8017\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5168\u7403\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u4e2a\u4f53\u5dee\u5f02\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5229\u7528\u8d85\u7f51\u7edc\u7ed3\u5408\u5916\u90e8\u56e0\u7d20\u7684\u5bb6\u5ead\u7ea7\u7535\u529b\u6d88\u8017\u9884\u6d4b", "abstract_zh": "\u51c6\u786e\u7684\u7535\u529b\u6d88\u8017\u9884\u6d4b\u5bf9\u4e8e\u9ad8\u6548\u7684\u80fd\u6e90\u7ba1\u7406\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4f9d\u8d56\u4e8e\u5386\u53f2\u6a21\u5f0f\u548c\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u800c\u5f15\u5165\u5916\u90e8\u56e0\u7d20\uff08\u5982\u5929\u6c14\u6307\u6807\uff09\u5728\u590d\u6742\u73b0\u5b9e\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u989d\u5916\u7279\u5f81\u7684\u52a0\u5165\u5f80\u5f80\u4f1a\u964d\u4f4e\u57fa\u4e8e\u5168\u4f53\u6570\u636e\u8bad\u7ec3\u7684\u5168\u7403\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c3d\u7ba1\u80fd\u63d0\u5347\u4e2a\u4f53\u5bb6\u5ead\u7ea7\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u53d1\u73b0\u8d85\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u6709\u6548\u5229\u7528\u5916\u90e8\u56e0\u7d20\uff0c\u901a\u8fc7\u9488\u5bf9\u6bcf\u4e2a\u6d88\u8d39\u8005\u8c03\u6574\u6a21\u578b\u6743\u91cd\uff0c\u63d0\u5347\u5168\u7403\u7535\u529b\u6d88\u8017\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\n\n\u6211\u4eec\u6536\u96c6\u4e86\u6db5\u76d6\u4e24\u5e74\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5305\u62ec\u6765\u81ea6000\u591a\u6237\u5362\u68ee\u5821\u5bb6\u5ead\u7684\u7535\u529b\u6d88\u8017\u6570\u636e\u53ca\u76f8\u5e94\u7684\u5916\u90e8\u56e0\u7d20\uff08\u5982\u5929\u6c14\u6307\u6807\u3001\u8282\u5047\u65e5\u548c\u91cd\u5927\u672c\u5730\u4e8b\u4ef6\uff09\u3002\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u9884\u6d4b\u6a21\u578b\uff0c\u6211\u4eec\u8bc1\u660e\u8d85\u7f51\u7edc\u65b9\u6cd5\u5728\u7ed3\u5408\u5916\u90e8\u56e0\u7d20\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u9884\u6d4b\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5168\u7403\u6a21\u578b\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.14530", "pdf": "https://arxiv.org/pdf/2506.14530", "abs": "https://arxiv.org/abs/2506.14530", "authors": ["Anastasis Kratsios", "Tin Sum Cheng", "Aurelien Lucchi", "Haitz S\u00e1ez de Oc\u00e1riz Borde"], "title": "Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters", "categories": ["stat.ML", "cs.AI", "cs.LG", "cs.NE", "math.ST"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{r}}{\\sqrt{N}}\\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "paper_title_zh": "Error", "abstract_zh": "Error"}}
{"id": "2506.14534", "pdf": "https://arxiv.org/pdf/2506.14534", "abs": "https://arxiv.org/abs/2506.14534", "authors": ["Cl\u00e9ment Yvernes", "Emilie Devijver", "Eric Gaussier"], "title": "Complete Characterization for Adjustment in Summary Causal Graphs of Time Series", "categories": ["math.ST", "cs.AI"], "comment": "Accepted at the 41st Conference on Uncertainty in Artificial Intelligence (UAI)", "summary": "The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u4e2d\u57fa\u4e8e\u6458\u8981\u56e0\u679c\u56fe\u7684\u591a\u91cd\u5e72\u9884\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8c03\u6574\u51c6\u5219\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f2a\u7ebf\u6027\u7b97\u6cd5\u6765\u5224\u65ad\u67e5\u8be2\u662f\u5426\u53ef\u8bc6\u522b\u3002", "motivation": "\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\uff0c\u5f53\u4ec5\u80fd\u83b7\u5f97\u6458\u8981\u56e0\u679c\u56fe\u800c\u975e\u771f\u5b9e\u56e0\u679c\u56fe\u65f6\uff0c\u5982\u4f55\u8bc4\u4f30\u5e72\u9884\u7684\u603b\u56e0\u679c\u6548\u5e94\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u65e0\u5e72\u9884\u516c\u5f0f\u8868\u793a\u5e76\u4ec5\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\uff0c\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u7814\u7a76\u4e86\u591a\u91cd\u5e72\u9884\u4e0b\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8c03\u6574\u51c6\u5219\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u8be5\u8bbe\u5b9a\u4e0b\u7684\u5b8c\u5907\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f2a\u7ebf\u6027\u7b97\u6cd5\u7528\u4e8e\u5224\u65ad\u67e5\u8be2\u7684\u53ef\u8bc6\u522b\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8c03\u6574\u51c6\u5219\u5728\u6458\u8981\u56e0\u679c\u56fe\u7684\u8bbe\u5b9a\u4e0b\u662f\u5b8c\u5907\u7684\uff0c\u4e14\u4f2a\u7ebf\u6027\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u5224\u65ad\u67e5\u8be2\u7684\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u65f6\u95f4\u5e8f\u5217\u4e2d\u57fa\u4e8e\u6458\u8981\u56e0\u679c\u56fe\u7684\u5e72\u9884\u6548\u5e94\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53ef\u8bc6\u522b\u6027\u5224\u65ad\u3002", "paper_title_zh": "\u65f6\u95f4\u5e8f\u5217\u6458\u8981\u56e0\u679c\u56fe\u4e2d\u8c03\u6574\u7684\u5b8c\u5907\u6027\u8868\u5f81", "abstract_zh": "\u5e72\u9884\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\u65e8\u5728\u8bc4\u4f30\u603b\u56e0\u679c\u6548\u5e94\u662f\u5426\u53ef\u4ee5\u7528\u65e0\u5e72\u9884\u516c\u5f0f\u8868\u793a\uff0c\u4ece\u800c\u4ec5\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\u3002\u6211\u4eec\u5728\u65f6\u95f4\u5e8f\u5217\u7684\u80cc\u666f\u4e0b\u7814\u7a76\u8fd9\u4e00\u95ee\u9898\uff0c\u8003\u8651\u591a\u91cd\u5e72\u9884\uff0c\u4e14\u4ec5\u80fd\u83b7\u5f97\u6458\u8981\u56e0\u679c\u56fe\u5f62\u5f0f\u7684\u771f\u5b9e\u56e0\u679c\u56fe\u7684\u62bd\u8c61\u3002\u6211\u4eec\u7279\u522b\u63d0\u51fa\u4e86\u8c03\u6574\u51c6\u5219\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u8be5\u8bbe\u5b9a\u4e0b\u7684\u5b8c\u5907\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f2a\u7ebf\u6027\u7b97\u6cd5\u6765\u5224\u65ad\u67e5\u8be2\u662f\u5426\u53ef\u8bc6\u522b\u3002"}}
{"id": "2506.14535", "pdf": "https://arxiv.org/pdf/2506.14535", "abs": "https://arxiv.org/abs/2506.14535", "authors": ["Jos\u00e9 Manuel Su\u00e1rez", "Luis Mariano Bibb\u00f3", "Joaquin Bogado", "Alejandro Fernandez"], "title": "Automatic Qiskit Code Refactoring Using Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "Submitted for review to \"Taller Latinoamericano de Ingenier\u00eda de Software Cu\u00e1ntico\" (https://www.ripaisc.net/call-for-papers-tlisc-2025/)", "summary": "As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u91cd\u6784Qiskit\u4ee3\u7801\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u5b98\u65b9\u6587\u6863\u4e2d\u7684\u8fc1\u79fb\u573a\u666f\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408LLM\u751f\u6210\u91cd\u6784\u5efa\u8bae\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91cf\u5b50\u8f6f\u4ef6\u6846\u67b6\u5feb\u901f\u8fed\u4ee3\u5e26\u6765\u7684\u517c\u5bb9\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8f6f\u4ef6\u6846\u67b6\u7684\u5feb\u901f\u8fed\u4ee3\uff0c\u5f00\u53d1\u8005\u9762\u4e34API\u517c\u5bb9\u6027\u7ef4\u62a4\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u91cd\u6784Qiskit\u4ee3\u7801\uff0c\u51cf\u8f7b\u5f00\u53d1\u8005\u7684\u8fc1\u79fb\u8d1f\u62c5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4eceQiskit\u5b98\u65b9\u6587\u6863\uff08\u5982\u53d1\u5e03\u8bf4\u660e\uff09\u4e2d\u63d0\u53d6\u8fc1\u79fb\u573a\u666f\u5206\u7c7b\uff1b2) \u5c06\u5206\u7c7b\u548c\u539f\u59cbPython\u4ee3\u7801\u8f93\u5165LLM\uff0c\u8bc6\u522b\u8fc1\u79fb\u573a\u666f\u5e76\u751f\u6210\u91cd\u6784\u5efa\u8bae\uff1b3) \u8bbe\u8ba1\u9488\u5bf9\u6027\u8f93\u5165\u7ed3\u6784\u4ee5\u514b\u670dLLM\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684LLM\u80fd\u6709\u6548\u81ea\u52a8\u5316Qiskit\u4ee3\u7801\u8fc1\u79fb\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ece\u65e9\u671f\u7248\u672c\u8fc1\u79fb\u81f30.46\u7248\u7684\u5206\u7c7b\u548c\u63d0\u793a\u96c6\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86LLM\u5728\u91cf\u5b50\u4ee3\u7801\u8fc1\u79fb\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5206\u7c7b\u548c\u65b9\u6cd5\u8bba\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684Qiskit\u4ee3\u7801\u81ea\u52a8\u91cd\u6784\u65b9\u6cd5", "abstract_zh": "\u968f\u7740\u91cf\u5b50\u8f6f\u4ef6\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u5f00\u53d1\u8005\u9762\u4e34\u5feb\u901f\u53d8\u5316\u7684API\u5e26\u6765\u7684\u517c\u5bb9\u6027\u7ef4\u62a4\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u91cd\u6784Qiskit\u4ee3\u7801\u7684\u65b0\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u4eceQiskit\u5b98\u65b9\u6587\u6863\uff08\u5982\u53d1\u5e03\u8bf4\u660e\uff09\u4e2d\u63d0\u53d6\u8fc1\u79fb\u573a\u666f\u5206\u7c7b\uff0c\u6355\u83b7\u529f\u80fd\u8fc1\u79fb\u81f3\u4e0d\u540c\u6a21\u5757\u548c\u5e9f\u5f03\u7528\u6cd5\u7b49\u5e38\u89c1\u6a21\u5f0f\u3002\u8be5\u5206\u7c7b\u4e0e\u539f\u59cbPython\u4ee3\u7801\u4e00\u8d77\u4f5c\u4e3aLLM\u7684\u8f93\u5165\uff0cLLM\u7684\u4efb\u52a1\u662f\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684\u8fc1\u79fb\u573a\u666f\u5e76\u751f\u6210\u91cd\u6784\u5efa\u8bae\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u8f93\u5165\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u5f53\u524dLLM\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684LLM\u80fd\u6709\u6548\u8f85\u52a9\u81ea\u52a8\u5316Qiskit\u4ee3\u7801\u8fc1\u79fb\u3002\u672c\u6587\u8d21\u732e\u5305\u62ec\uff1a1) \u4ece\u65e9\u671f\u7248\u672c\u8fc1\u79fb\u81f30.46\u7248\u7684\u5206\u7c7b\u548c\u63d0\u793a\u96c6\uff1b2) \u8bc4\u4f30LLM\u5728\u91cf\u5b50\u4ee3\u7801\u8fc1\u79fb\u4e2d\u80fd\u529b\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2506.14540", "pdf": "https://arxiv.org/pdf/2506.14540", "abs": "https://arxiv.org/abs/2506.14540", "authors": ["Gerardo A. Flores", "Alyssa H. Smith", "Julia A. Fukuyama", "Ashia C. Wilson"], "title": "Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u6821\u51c6\u6027\u3001\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u6027\u548c\u9519\u8bef\u6210\u672c\u654f\u611f\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u4e34\u5e8a\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u4e34\u5e8a\u73af\u5883\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bc4\u5206\u89c4\u5219\uff08\u5982\u51c6\u786e\u7387\u548cAUC-ROC\uff09\u672a\u80fd\u5145\u5206\u53cd\u6620\u6821\u51c6\u6027\u3001\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u6027\u548c\u9519\u8bef\u6210\u672c\u4e0d\u5bf9\u79f0\u6027\u7b49\u5173\u952e\u4e34\u5e8a\u4f18\u5148\u7ea7\u3002", "method": "\u57fa\u4e8eSchervish\u8868\u793a\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8c03\u6574\u540e\u7684\u4ea4\u53c9\u71b5\u53d8\u4f53\uff0c\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u4e34\u5e8a\u76f8\u5173\u7c7b\u522b\u5e73\u8861\u8303\u56f4\u5185\u7684\u6027\u80fd\uff0c\u9009\u62e9\u6821\u51c6\u4e14\u9c81\u68d2\u7684\u9608\u503c\u5206\u7c7b\u5668\u3002", "result": "\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u7b80\u5355\u6613\u7528\uff0c\u5bf9\u4e34\u5e8a\u90e8\u7f72\u6761\u4ef6\u654f\u611f\uff0c\u80fd\u591f\u4f18\u5148\u9009\u62e9\u6821\u51c6\u6027\u597d\u4e14\u5bf9\u73b0\u5b9e\u53d8\u5316\u9c81\u68d2\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u6821\u51c6\u6027\u548c\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002", "paper_title_zh": "\u8bc4\u4f30\u4e0e\u4e34\u5e8a\u4f18\u5148\u4e8b\u9879\u7684\u5bf9\u9f50\uff1a\u6821\u51c6\u3001\u6807\u7b7e\u504f\u79fb\u548c\u9519\u8bef\u6210\u672c", "abstract_zh": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u4e34\u5e8a\u73af\u5883\uff0c\u5176\u4e2d\u6982\u7387\u8bc4\u5206\u51fd\u6570\u7528\u4e8e\u6307\u5bfc\u548c\u4f18\u5148\u5904\u7406\u60a3\u8005\u7ba1\u7406\u51b3\u7b56\u3002\u7136\u800c\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bc4\u5206\u89c4\u5219\uff08\u5982\u51c6\u786e\u7387\u548cAUC-ROC\uff09\u672a\u80fd\u5145\u5206\u53cd\u6620\u5173\u952e\u4e34\u5e8a\u4f18\u5148\u7ea7\uff0c\u5305\u62ec\u6821\u51c6\u6027\u3001\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u5bf9\u4e0d\u5bf9\u79f0\u9519\u8bef\u6210\u672c\u7684\u654f\u611f\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u4e14\u5b9e\u7528\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u9009\u62e9\u6821\u51c6\u7684\u9608\u503c\u5206\u7c7b\u5668\uff0c\u8be5\u6846\u67b6\u660e\u786e\u8003\u8651\u4e86\u7c7b\u522b\u6d41\u884c\u7387\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u5e38\u89c1\u7684\u9886\u57df\u7279\u5b9a\u6210\u672c\u4e0d\u5bf9\u79f0\u6027\u3002\u57fa\u4e8eSchervish\u8868\u793a\u7406\u8bba\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u4ea4\u53c9\u71b5\uff08\u5bf9\u6570\u8bc4\u5206\uff09\u7684\u8c03\u6574\u53d8\u4f53\uff0c\u8be5\u53d8\u4f53\u5728\u4e34\u5e8a\u76f8\u5173\u7c7b\u522b\u5e73\u8861\u8303\u56f4\u5185\u5bf9\u52a0\u6743\u6027\u80fd\u8fdb\u884c\u5e73\u5747\u3002\u6700\u7ec8\u7684\u8bc4\u4f30\u65b9\u6cd5\u7b80\u5355\u6613\u7528\uff0c\u5bf9\u4e34\u5e8a\u90e8\u7f72\u6761\u4ef6\u654f\u611f\uff0c\u5e76\u65e8\u5728\u4f18\u5148\u9009\u62e9\u65e2\u6821\u51c6\u53c8\u5bf9\u73b0\u5b9e\u53d8\u5316\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
{"id": "2506.14567", "pdf": "https://arxiv.org/pdf/2506.14567", "abs": "https://arxiv.org/abs/2506.14567", "authors": ["Emanuel Moss", "Elizabeth Watkins", "Christopher Persaud", "Passant Karunaratne", "Dawn Nafus"], "title": "Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u7b49\u9ad8\u7cbe\u5ea6\u9886\u57df\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u51c6\u786e\u6027\u4ecd\u5f15\u53d1\u5de5\u7a0b\u5e08\u5bf9\u9519\u8bef\u7684\u8b66\u60d5\u3002\u672c\u6587\u901a\u8fc7\u8bbf\u8c08\u786c\u4ef6\u548c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u4ea4\u4e92\u5f0f\u63a7\u5236\u4e0a\u4e0b\u6587\u6765\u7f13\u89e3\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u9ad8\u7cbe\u5ea6\u9886\u57df\uff08\u5982\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\uff09\u7684\u51c6\u786e\u6027\u6210\u4e3a\u5173\u6ce8\u7126\u70b9\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5de5\u7a0b\u5e08\u5982\u4f55\u5728\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u65f6\u4fdd\u6301\u5bf9\u9519\u8bef\u7684\u8b66\u60d5\uff0c\u5e76\u8bc6\u522b\u5176\u4ed6\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5bf9\u786c\u4ef6\u548c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u53ca\u5176\u5408\u4f5c\u8005\u7684\u8bbf\u8c08\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u9ad8\u7cbe\u5ea6\u9886\u57df\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u91cd\u70b9\u5173\u6ce8\u51c6\u786e\u6027\u53ca\u5176\u5e26\u6765\u7684\u5176\u4ed6\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5de5\u7a0b\u5e08\u5728\u4f7f\u7528\u751f\u6210\u5f0fAI\u5de5\u5177\u65f6\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u63a7\u5236\u4ea4\u4e92\u4e0a\u4e0b\u6587\u3002\u7814\u7a76\u8fd8\u5217\u4e3e\u4e86\u8fd9\u4e9b\u5de5\u5177\u5e26\u6765\u7684\u5176\u4ed6\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5177\u4f53\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u63a7\u5236\u5de5\u7a0b\u5e08\u4e0e\u751f\u6210\u5f0fAI\u5de5\u5177\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u662f\u6700\u5927\u6311\u6218\u4e4b\u4e00\u3002\u7814\u7a76\u5efa\u8bae\u901a\u8fc7\u589e\u5f3a\u4ea4\u4e92\u5f0f\u4e0a\u4e0b\u6587\u63a7\u5236\u80fd\u529b\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "paper_title_zh": "\u63a7\u5236\u4e0a\u4e0b\u6587\uff1a\u751f\u6210\u5f0fAI\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u7b49\u9ad8\u7cbe\u5ea6\u9886\u57df\u7684\u5e94\u7528", "abstract_zh": "\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u804a\u5929\u673a\u5668\u4eba\u548c\u4ee3\u7801\u52a9\u624b\u3002\u968f\u7740\u8fd9\u4e9b\u5de5\u5177\u51c6\u786e\u6027\u7684\u63d0\u5347\uff0c\u9ad8\u7cbe\u5ea6\u9886\u57df\u5de5\u4f5c\u8005\u5982\u4f55\u4fdd\u6301\u5bf9\u9519\u8bef\u7684\u8b66\u60d5\u4ee5\u53ca\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u53ef\u80fd\u5e26\u6765\u7684\u5176\u4ed6\u95ee\u9898\u6210\u4e3a\u5173\u6ce8\u7126\u70b9\u3002\u672c\u6587\u901a\u8fc7\u8bbf\u8c08\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u9886\u57df\u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u53ca\u5176\u5408\u4f5c\u8005\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u5de5\u5177\u4f7f\u7528\u4e2d\u51c6\u786e\u6027\u7684\u4f5c\u7528\u53ca\u5176\u5e26\u6765\u7684\u5176\u4ed6\u6311\u6218\u3002\u7814\u7a76\u5217\u4e3e\u4e86\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u6700\u7ec8\u6307\u51fa\u63a7\u5236\u5de5\u7a0b\u5e08\u4e0e\u751f\u6210\u5f0fAI\u5de5\u5177\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u662f\u6700\u5927\u6311\u6218\u4e4b\u4e00\u3002\u6587\u7ae0\u6700\u540e\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u4ea4\u4e92\u5f0f\u4e0a\u4e0b\u6587\u63a7\u5236\u80fd\u529b\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u7684\u5efa\u8bae\u3002"}}
{"id": "2506.14577", "pdf": "https://arxiv.org/pdf/2506.14577", "abs": "https://arxiv.org/abs/2506.14577", "authors": ["Abdul Rahman Jacob", "Avinash Kori", "Emanuele De Angelis", "Ben Glocker", "Maurizio Proietti", "Francesca Toni"], "title": "Object-Centric Neuro-Argumentative Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of Machine Learning Research, 2025 19th Conference on Neurosymbolic Learning and Reasoning", "summary": "Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u5b66\u4e60\u7684\u795e\u7ecf\u8bba\u8bc1\u5b66\u4e60\uff08NAL\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u6790\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u751f\u6210\u4e8b\u5b9e\u5e76\u5229\u7528\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u53ef\u4e0e\u5148\u8fdb\u65b9\u6cd5\u5ab2\u7f8e\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5176\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u5b66\u4e60\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u67b6\u6784\u4ee5\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u8bba\u8bc1\u5b66\u4e60\uff08NAL\uff09\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\uff08\u795e\u7ecf\u90e8\u5206\uff09\u548c\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff0c\u7b26\u53f7\u90e8\u5206\uff09\u3002\u795e\u7ecf\u90e8\u5206\u8d1f\u8d23\u56fe\u50cf\u5206\u5272\u4e0e\u7f16\u7801\u4e3a\u4e8b\u5b9e\uff0c\u7b26\u53f7\u90e8\u5206\u901a\u8fc7ABA\u5b66\u4e60\u751f\u6210\u9884\u6d4b\u6846\u67b6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNAL\u67b6\u6784\u7684\u6027\u80fd\u53ef\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "NAL\u67b6\u6784\u901a\u8fc7\u795e\u7ecf\u4e0e\u7b26\u53f7\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "paper_title_zh": "\u9762\u5411\u5bf9\u8c61\u4e2d\u5fc3\u7684\u795e\u7ecf\u8bba\u8bc1\u5b66\u4e60", "abstract_zh": "\u8fc7\u53bb\u5341\u5e74\u4e2d\uff0c\u968f\u7740\u6211\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u505a\u51fa\u5173\u952e\u51b3\u7b56\uff0c\u5176\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u9010\u6e10\u663e\u73b0\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u8bba\u8bc1\u5b66\u4e60\uff08NAL\uff09\u67b6\u6784\uff0c\u5c06\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u7528\u4e8e\u56fe\u50cf\u5206\u6790\u3002\u8be5\u67b6\u6784\u5305\u542b\u795e\u7ecf\u548c\u7b26\u53f7\u4e24\u90e8\u5206\uff1a\u524d\u8005\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u5c06\u56fe\u50cf\u5206\u5272\u5e76\u7f16\u7801\u4e3a\u4e8b\u5b9e\uff0c\u540e\u8005\u5e94\u7528ABA\u5b66\u4e60\u751f\u6210\u57fa\u4e8e\u56fe\u50cf\u7684\u9884\u6d4b\u6846\u67b6\u3002\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNAL\u67b6\u6784\u7684\u6027\u80fd\u53ef\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5ab2\u7f8e\u3002"}}
{"id": "2506.14623", "pdf": "https://arxiv.org/pdf/2506.14623", "abs": "https://arxiv.org/abs/2506.14623", "authors": ["Aaron Conrardy", "Armen Sulejmani", "Cindy Guerlain", "Daniele Pagani", "David Hick", "Matteo Satta", "Jordi Cabot"], "title": "Low-code to fight climate change: the Climaborough project", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "This paper was presented in the Research Projects Track of the 19th International Conference on Research Challenges in Information Science (RCIS 2025)", "summary": "The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.", "AI": {"tldr": "\u6b27\u76df\u8d44\u52a9\u7684Climaborough\u9879\u76ee\u901a\u8fc7\u4f4e\u4ee3\u7801/\u65e0\u4ee3\u7801\u7b56\u7565\uff0c\u5e2e\u52a9\u6b27\u6d32\u57ce\u5e02\u5feb\u901f\u90e8\u7f72\u6c14\u5019\u4eea\u8868\u76d8\uff0c\u4ee5\u5b9e\u73b02030\u5e74\u78b3\u4e2d\u548c\u76ee\u6807\u3002", "motivation": "\u9879\u76ee\u65e8\u5728\u652f\u6301\u6b27\u6d32\u57ce\u5e02\u57282030\u5e74\u524d\u5b9e\u73b0\u78b3\u4e2d\u548c\uff0c\u901a\u8fc7\u5b9e\u65f6\u548c\u5386\u53f2\u6570\u636e\u76d1\u6d4b\u6c14\u5019\u76ee\u6807\u8fdb\u5c55\uff0c\u5e76\u8bc4\u4f30\u672c\u5730\u5b9e\u9a8c\u6027\u4e3e\u63aa\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u4f4e\u4ee3\u7801\u7b56\u7565\u52a0\u901f\u4eea\u8868\u76d8\u5f00\u53d1\uff0c\u5d4c\u5165\u65e0\u4ee3\u7801\u7406\u5ff5\uff0c\u4f7f\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u6839\u636e\u9700\u6c42\u914d\u7f6e\u548c\u8c03\u6574\u4eea\u8868\u76d8\u3002", "result": "\u5f00\u53d1\u4e86Climaborough\u57ce\u5e02\u5e73\u53f0\uff0c\u901a\u8fc7\u7528\u6237\u53cb\u597d\u7684\u4eea\u8868\u76d8\u5c55\u793a\u6570\u636e\uff0c\u5e2e\u52a9\u57ce\u5e02\u8bc4\u4f30\u6c14\u5019\u4e3e\u63aa\u7684\u6210\u6548\u548c\u6269\u5c55\u6f5c\u529b\u3002", "conclusion": "\u4f4e\u4ee3\u7801/\u65e0\u4ee3\u7801\u7b56\u7565\u6709\u6548\u652f\u6301\u4e86\u6c14\u5019\u4eea\u8868\u76d8\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u52a9\u529b\u57ce\u5e02\u5b9e\u73b0\u78b3\u4e2d\u548c\u76ee\u6807\u3002", "paper_title_zh": "\u4f4e\u4ee3\u7801\u5bf9\u6297\u6c14\u5019\u53d8\u5316\uff1aClimaborough\u9879\u76ee", "abstract_zh": "\u6b27\u76df\u8d44\u52a9\u7684Climaborough\u9879\u76ee\u65e8\u5728\u652f\u6301\u6b27\u6d32\u57ce\u5e02\u57282030\u5e74\u524d\u5b9e\u73b0\u78b3\u4e2d\u548c\u3002\u6765\u81ea\u4e5d\u4e2a\u56fd\u5bb6\u768411\u4e2a\u57ce\u5e02\u5c06\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u90e8\u7f72\u4fc3\u8fdb\u6c14\u5019\u8f6c\u578b\u7684\u4ea7\u54c1\u548c\u670d\u52a1\u3002Climaborough\u57ce\u5e02\u5e73\u53f0\u6b63\u5728\u5f00\u53d1\u4e2d\uff0c\u901a\u8fc7\u805a\u5408\u5386\u53f2\u548c\u5b9e\u65f6\u6570\u636e\uff0c\u5e76\u5728\u7528\u6237\u53cb\u597d\u7684\u4eea\u8868\u76d8\u4e2d\u5c55\u793a\u7ed3\u679c\uff0c\u5e2e\u52a9\u975e\u6280\u672f\u4e13\u5bb6\u8bc4\u4f30\u672c\u5730\u5b9e\u9a8c\u6027\u4e3e\u63aa\u7684\u6709\u6548\u6027\uff0c\u8bc6\u522b\u5177\u6709\u663e\u8457\u5f71\u54cd\u7684\u4e3e\u63aa\uff0c\u5e76\u8bc4\u4f30\u5c06\u5176\u6269\u5c55\u5230\u66f4\u5e7f\u8303\u56f4\u7684\u6f5c\u5728\u540e\u679c\u3002\u672c\u6587\u89e3\u91ca\u4e86\u5982\u4f55\u901a\u8fc7\u4f4e\u4ee3\u7801/\u65e0\u4ee3\u7801\u7b56\u7565\u5feb\u901f\u90e8\u7f72\u6c14\u5019\u4eea\u8868\u76d8\u3002\u4f4e\u4ee3\u7801\u7b56\u7565\u7528\u4e8e\u52a0\u901f\u4eea\u8868\u76d8\u7684\u5f00\u53d1\uff0c\u800c\u4eea\u8868\u76d8\u5d4c\u5165\u7684\u65e0\u4ee3\u7801\u7406\u5ff5\u4f7f\u5404\u7c7b\u516c\u6c11\u80fd\u591f\u6839\u636e\u81ea\u8eab\u9700\u6c42\u914d\u7f6e\u548c\u8c03\u6574\u4eea\u8868\u76d8\u3002"}}
{"id": "2506.14627", "pdf": "https://arxiv.org/pdf/2506.14627", "abs": "https://arxiv.org/abs/2506.14627", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "ACM Survey Draft on Formalising Software Requirements with Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "22 pages. 6 summary tables", "summary": "This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:\n  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.\n  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u4efd\u5173\u4e8e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f62\u5f0f\u5316\u8f6f\u4ef6\u9700\u6c42\u7684ACM\u8c03\u67e5\u8349\u6848\uff0c\u603b\u7ed3\u4e8694\u7bc7\u8bba\u6587\uff0c\u5e76\u5305\u542b\u8f6f\u4ef6\u9700\u6c42\u53ef\u8ffd\u6eaf\u6027\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u53ca\u5176\u5de5\u5177\u3001\u7edf\u4e00\u7f16\u7a0b\u7406\u8bba\u548c\u673a\u6784\u7406\u8bba\u7b49\u7ae0\u8282\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u603b\u7ed3\u548c\u5206\u6790\u73b0\u6709\u7814\u7a76\uff0c\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5316\u8f6f\u4ef6\u9700\u6c42\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u603b\u7ed3\u4e8694\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u5e76\u5206\u7ae0\u8282\u8ba8\u8bba\u4e86\u8f6f\u4ef6\u9700\u6c42\u7684\u53ef\u8ffd\u6eaf\u6027\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u53ca\u5176\u5de5\u5177\u3001\u7edf\u4e00\u7f16\u7a0b\u7406\u8bba\u548c\u673a\u6784\u7406\u8bba\u7b49\u5185\u5bb9\u3002", "result": "\u8349\u6848\u63d0\u4f9b\u4e86\u5bf9\u73b0\u6709\u7814\u7a76\u7684\u5168\u9762\u603b\u7ed3\uff0c\u5e76\u6307\u51fa\u4e86\u4e0e\u5176\u4ed6\u7c7b\u4f3c\u6807\u9898\u8349\u6848\uff08\u5982AACS 2025\u548cSAIV 2025\uff09\u7684\u5173\u952e\u533a\u522b\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f62\u5f0f\u5316\u8f6f\u4ef6\u9700\u6c42\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7efc\u8ff0\uff0c\u5e76\u5f3a\u8c03\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f62\u5f0f\u5316\u8f6f\u4ef6\u9700\u6c42\u7684ACM\u8c03\u67e5\u8349\u6848", "abstract_zh": "\u672c\u8349\u6848\u662f\u4e00\u4efd\u5de5\u4f5c\u6587\u6863\uff0c\u603b\u7ed3\u4e8694\u7bc7\u8bba\u6587\uff0c\u5e76\u5305\u542b\u5173\u4e8e\u8f6f\u4ef6\u9700\u6c42\u53ef\u8ffd\u6eaf\u6027\uff08\u7b2c4\u8282\uff09\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u53ca\u5176\u5de5\u5177\uff08\u7b2c5\u8282\uff09\u3001\u7edf\u4e00\u7f16\u7a0b\u7406\u8bba\uff08UTP\uff09\u548c\u673a\u6784\u7406\u8bba\uff08\u7b2c6\u8282\uff09\u7684\u989d\u5916\u7ae0\u8282\u3002\u8bf7\u53c2\u8003[7,8]\u7684\u6458\u8981\u3002\u672c\u8349\u6848\u4e0e\u6211\u4eec\u8fd1\u671f\u7c7b\u4f3c\u6807\u9898\u7684\u9884\u671f\u8349\u6848\uff08\u5373AACS 2025 [7]\u548cSAIV 2025 [8]\uff09\u7684\u5173\u952e\u533a\u522b\u5728\u4e8e\uff1a\n[7]\u662f\u63d0\u4ea4\u7ed9\u7231\u5c14\u5170ADAPT\u5e74\u5ea6\u4f1a\u8bae\u7684\u4e24\u9875\u6295\u7a3f\uff0c\u4e8e2025\u5e743\u670818\u65e5\u63d0\u4ea4\uff0c\u7ecf\u8fc7\u8f7b\u91cf\u7ea7\u76f2\u5ba1\u5e76\u88ab\u63a5\u53d7\u4e3a\u6d77\u62a5\u5c55\u793a\u3002\u4f1a\u8bae\u4e8e2025\u5e745\u670815\u65e5\u4e3e\u884c\u3002\n[8]\u662f\u4e00\u7bc7\u4e5d\u9875\u7684\u8bba\u6587\uff0c\u9644\u6709\u4e5d\u9875\u7684\u53c2\u8003\u6587\u732e\u548c\u603b\u7ed3\u8868\u683c\uff0c\u4e8e2025\u5e744\u670824\u65e5\u63d0\u4ea4\u7ed9AI\u9a8c\u8bc1\u7814\u8ba8\u4f1a\uff08SAIV 2025\uff09\u3002\u5b83\u7ecf\u8fc7\u4e86\u4e25\u683c\u7684\u8bc4\u5ba1\u8fc7\u7a0b\u3002arXiv.org\u4e0a\u4e0a\u4f20\u7684\u7248\u672c[8]\u662f\u6539\u8fdb\u540e\u7684\u63d0\u4ea4\u7248\u672c\uff0c\u89e3\u51b3\u4e86\u7279\u5b9a\u7684\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2506.14640", "pdf": "https://arxiv.org/pdf/2506.14640", "abs": "https://arxiv.org/abs/2506.14640", "authors": ["Ina K. Schieferdecker"], "title": "Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey", "categories": ["cs.SE", "cs.AI"], "comment": "15 pages, 7 figures, 1 table, 2 listings (will be presented at FMICS 2025)", "summary": "In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u589e\u5f3a\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5ai4st\uff0c\u5e76\u7528\u4e8e\u5206\u7c7b\u8fd1\u671f\u7814\u7a76\u53ca\u8bc6\u522b\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u8f6f\u4ef6\u6d4b\u8bd5\u662f\u9a8c\u8bc1\u548c\u786e\u8ba4\u8f6f\u4ef6\u529f\u80fd\u3001\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u6d4b\u8bd5\u81ea\u52a8\u5316\u7684\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\u3002AI\u7684\u7a81\u7834\u4e3a\u8f6f\u4ef6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22AI\u5982\u4f55\u589e\u5f3a\u8f6f\u4ef6\u6d4b\u8bd5\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86AI\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u81ea\u52a8\u5316\u4e2d\u7684\u6700\u65b0\u7814\u7a76\uff0c\u4ece\u65e0\u81ea\u52a8\u5316\u5230\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5e76\u8ba8\u8bba\u4e86AI\u5e26\u6765\u7684\u65b0\u578b\u6d4b\u8bd5\u5f62\u5f0f\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5ai4st\uff0c\u7528\u4e8e\u5206\u7c7b\u7814\u7a76\u5e76\u8bc6\u522b\u5f00\u653e\u6027\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5ai4st\uff0c\u5e76\u6210\u529f\u5c06\u5176\u5e94\u7528\u4e8e\u8fd1\u671f\u7814\u7a76\u7684\u5206\u7c7b\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "AI\u4e3a\u8f6f\u4ef6\u6d4b\u8bd5\u5e26\u6765\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5206\u7c7b\u6cd5ai4st\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5f00\u653e\u6027\u95ee\u9898\u3002", "paper_title_zh": "\u63a2\u7d22AI\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u7814\u7a76\u9886\u57df\u2014\u2014AI\u589e\u5f3a\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u5206\u7c7b\u6cd5\u53ca\u672c\u4f53\u9a71\u52a8\u7684\u6587\u732e\u7efc\u8ff0", "abstract_zh": "\u5728\u5de5\u4e1a\u754c\uff0c\u8f6f\u4ef6\u6d4b\u8bd5\u662f\u9a8c\u8bc1\u548c\u786e\u8ba4\u57fa\u4e8e\u8f6f\u4ef6\u7cfb\u7edf\u7684\u529f\u80fd\u3001\u6027\u80fd\u3001\u5b89\u5168\u6027\u3001\u53ef\u7528\u6027\u7b49\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u8fc7\u53bb\u5341\u5e74\uff0c\u6d4b\u8bd5\u81ea\u52a8\u5316\u5728\u5de5\u4e1a\u754c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u800cAI\u5728\u8bb8\u591a\u5de5\u7a0b\u9886\u57df\u7684\u7a81\u7834\u4e3a\u8f6f\u4ef6\u6d4b\u8bd5\uff08\u5305\u62ec\u624b\u52a8\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u672c\u6587\u7efc\u8ff0\u4e86AI\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u81ea\u52a8\u5316\u4e2d\u7684\u6700\u65b0\u7814\u7a76\uff0c\u4ece\u65e0\u81ea\u52a8\u5316\u5230\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5e76\u8ba8\u8bba\u4e86AI\u5e26\u6765\u7684\u65b0\u578b\u6d4b\u8bd5\u5f62\u5f0f\u3002\u57fa\u4e8e\u6b64\uff0c\u65b0\u5f00\u53d1\u7684\u5206\u7c7b\u6cd5ai4st\u88ab\u63d0\u51fa\u5e76\u7528\u4e8e\u5206\u7c7b\u8fd1\u671f\u7814\u7a76\u53ca\u8bc6\u522b\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002"}}
{"id": "2506.14648", "pdf": "https://arxiv.org/pdf/2506.14648", "abs": "https://arxiv.org/abs/2506.14648", "authors": ["Hexian Ni", "Tao Lu", "Haoyuan Hu", "Yinghao Cai", "Shuo Wang"], "title": "SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 8 figures", "summary": "Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSENIOR\u7684\u9ad8\u6548\u67e5\u8be2\u9009\u62e9\u548c\u504f\u597d\u5f15\u5bfc\u63a2\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u504f\u597d\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\uff0c\u901a\u8fc7\u9009\u62e9\u6613\u4e8e\u6bd4\u8f83\u7684\u884c\u4e3a\u7247\u6bb5\u5bf9\u548c\u8bbe\u8ba1\u504f\u597d\u5f15\u5bfc\u7684\u5185\u5728\u5956\u52b1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u53cd\u9988\u6548\u7387\u548c\u7b56\u7565\u5b66\u4e60\u901f\u5ea6\u3002", "motivation": "\u504f\u597d\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u5956\u52b1\u6a21\u578b\uff0c\u907f\u514d\u4e86\u5956\u52b1\u5de5\u7a0b\u7684\u9700\u6c42\uff0c\u4f46\u5176\u53cd\u9988\u548c\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u9ad8\u6548\u67e5\u8be2\u9009\u62e9\u548c\u504f\u597d\u5f15\u5bfc\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u90e8\u5206\uff1a(1) \u57fa\u4e8e\u8fd0\u52a8\u5dee\u5f02\u7684\u9009\u62e9\u65b9\u6848\uff08MDS\uff09\uff0c\u901a\u8fc7\u72b6\u6001\u6838\u5bc6\u5ea6\u4f30\u8ba1\u9009\u62e9\u8fd0\u52a8\u660e\u663e\u4e14\u65b9\u5411\u4e0d\u540c\u7684\u7247\u6bb5\u5bf9\uff0c\u4fbf\u4e8e\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\uff1b(2) \u504f\u597d\u5f15\u5bfc\u63a2\u7d22\u65b9\u6cd5\uff08PGE\uff09\uff0c\u9f13\u52b1\u63a2\u7d22\u9ad8\u504f\u597d\u4e14\u4f4e\u8bbf\u95ee\u7684\u72b6\u6001\uff0c\u6301\u7eed\u5f15\u5bfc\u4ee3\u7406\u83b7\u53d6\u6709\u4ef7\u503c\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSENIOR\u5728\u516d\u9879\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08\u6a21\u62df\u548c\u73b0\u5b9e\uff09\u4e2d\uff0c\u53cd\u9988\u6548\u7387\u548c\u7b56\u7565\u6536\u655b\u901f\u5ea6\u5747\u4f18\u4e8e\u5176\u4ed6\u4e94\u79cd\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SENIOR\u901a\u8fc7\u7ed3\u5408MDS\u548cPGE\u673a\u5236\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5956\u52b1\u548c\u7b56\u7565\u5b66\u4e60\u8fdb\u7a0b\uff0c\u4e3a\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SENIOR\uff1a\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u67e5\u8be2\u9009\u62e9\u4e0e\u504f\u597d\u5f15\u5bfc\u63a2\u7d22", "abstract_zh": "\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\u65b9\u6cd5\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u5956\u52b1\u6a21\u578b\uff0c\u907f\u514d\u4e86\u5956\u52b1\u5de5\u7a0b\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u53cd\u9988\u548c\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u4ecd\u662f\u963b\u788dPbRL\u5e94\u7528\u7684\u4e3b\u8981\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSENIOR\u7684\u9ad8\u6548\u67e5\u8be2\u9009\u62e9\u548c\u504f\u597d\u5f15\u5bfc\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6709\u610f\u4e49\u4e14\u6613\u4e8e\u6bd4\u8f83\u7684\u884c\u4e3a\u7247\u6bb5\u5bf9\uff0c\u63d0\u9ad8\u4eba\u7c7b\u53cd\u9988\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u7684\u504f\u597d\u5f15\u5bfc\u5185\u5728\u5956\u52b1\u52a0\u901f\u7b56\u7565\u5b66\u4e60\u3002\u5176\u6838\u5fc3\u601d\u60f3\u5305\u62ec\uff1a(1) \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8fd0\u52a8\u5dee\u5f02\u7684\u9009\u62e9\u65b9\u6848\uff08MDS\uff09\uff0c\u901a\u8fc7\u72b6\u6001\u7684\u6838\u5bc6\u5ea6\u4f30\u8ba1\u9009\u62e9\u8fd0\u52a8\u660e\u663e\u4e14\u65b9\u5411\u4e0d\u540c\u7684\u7247\u6bb5\u5bf9\uff0c\u66f4\u6613\u4e8e\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\uff1b(2) \u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u597d\u5f15\u5bfc\u63a2\u7d22\u65b9\u6cd5\uff08PGE\uff09\uff0c\u9f13\u52b1\u63a2\u7d22\u9ad8\u504f\u597d\u4e14\u4f4e\u8bbf\u95ee\u7684\u72b6\u6001\uff0c\u6301\u7eed\u5f15\u5bfc\u4ee3\u7406\u83b7\u53d6\u6709\u4ef7\u503c\u6837\u672c\u3002\u8fd9\u4e24\u79cd\u673a\u5236\u7684\u534f\u540c\u4f5c\u7528\u663e\u8457\u52a0\u901f\u4e86\u5956\u52b1\u548c\u7b56\u7565\u5b66\u4e60\u8fdb\u7a0b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u516d\u9879\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08\u6a21\u62df\u548c\u73b0\u5b9e\uff09\u4e2d\uff0cSENIOR\u5728\u53cd\u9988\u6548\u7387\u548c\u7b56\u7565\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u4e94\u79cd\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.14652", "pdf": "https://arxiv.org/pdf/2506.14652", "abs": "https://arxiv.org/abs/2506.14652", "authors": ["Alexandra Olteanu", "Su Lin Blodgett", "Agathe Balayn", "Angelina Wang", "Fernando Diaz", "Flavio du Pin Calmon", "Margaret Mitchell", "Michael Ekstrand", "Reuben Binns", "Solon Barocas"], "title": "Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "20 pages, 1 figure, 1 table", "summary": "In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20AI\u7814\u7a76\u4e0e\u5b9e\u8df5\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u4e25\u8c28\u6027\u6982\u5ff5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u65b9\u6cd5\u8bba\u7684\u4e25\u8c28\u6027\uff0c\u4ee5\u89e3\u51b3\u8d1f\u8d23\u4efbAI\u793e\u533a\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u5982\u5938\u5927AI\u80fd\u529b\u7684\u58f0\u660e\u3002", "motivation": "\u5f53\u524dAI\u9886\u57df\u5bf9\u4e25\u8c28\u6027\u7684\u7406\u89e3\u8fc7\u4e8e\u72ed\u9698\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u65b9\u6cd5\u8bba\u4e0a\uff0c\u8fd9\u5bfc\u81f4\u4e86\u8d1f\u8d23\u4efbAI\u793e\u533a\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u5982\u5938\u5927AI\u80fd\u529b\u7684\u58f0\u660e\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u66f4\u5168\u9762\u7684\u4e25\u8c28\u6027\u6982\u5ff5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5e7f\u6cdb\u7684\u4e25\u8c28\u6027\u6846\u67b6\uff0c\u5305\u62ec\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u3001\u77e5\u8bc6\u80cc\u666f\u4e25\u8c28\u6027\u3001\u89c4\u8303\u6027\u4e25\u8c28\u6027\u3001\u6982\u5ff5\u4e25\u8c28\u6027\u3001\u62a5\u544a\u4e25\u8c28\u6027\u548c\u89e3\u91ca\u4e25\u8c28\u6027\u516d\u4e2a\u65b9\u9762\u3002", "result": "\u901a\u8fc7\u6269\u5c55\u4e25\u8c28\u6027\u7684\u5b9a\u4e49\uff0c\u4f5c\u8005\u4e3aAI\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u7814\u7a76\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u3001\u8bb0\u8005\u7b49\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002", "conclusion": "AI\u7814\u7a76\u4e0e\u5b9e\u8df5\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u4e25\u8c28\u6027\u6982\u5ff5\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u9886\u57df\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5de5\u4f5c\u63d0\u4f9b\u66f4\u575a\u5b9e\u7684\u57fa\u7840\u3002", "paper_title_zh": "AI\u4e2d\u7684\u4e25\u8c28\u6027\uff1a\u8fdb\u884c\u4e25\u8c28\u7684AI\u5de5\u4f5c\u9700\u8981\u66f4\u5e7f\u6cdb\u3001\u8d1f\u8d23\u4efb\u7684AI\u4e25\u8c28\u6027\u6982\u5ff5", "abstract_zh": "\u5728AI\u7814\u7a76\u548c\u5b9e\u8df5\u4e2d\uff0c\u4e25\u8c28\u6027\u901a\u5e38\u88ab\u7406\u89e3\u4e3a\u65b9\u6cd5\u8bba\u7684\u4e25\u8c28\u6027\uff0c\u4f8b\u5982\u6570\u5b66\u3001\u7edf\u8ba1\u6216\u8ba1\u7b97\u65b9\u6cd5\u662f\u5426\u6b63\u786e\u5e94\u7528\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u79cd\u72ed\u9698\u7684\u4e25\u8c28\u6027\u6982\u5ff5\u52a0\u5267\u4e86\u8d1f\u8d23\u4efbAI\u793e\u533a\u7684\u62c5\u5fe7\uff0c\u5305\u62ec\u5bf9AI\u80fd\u529b\u7684\u5938\u5927\u58f0\u660e\u3002\u6211\u4eec\u7684\u7acb\u573a\u662f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5e7f\u6cdb\u7684\u4e25\u8c28\u6027\u6982\u5ff5\u6765\u6307\u5bfcAI\u7814\u7a76\u548c\u5b9e\u8df5\u3002\u8fd9\u79cd\u6982\u5ff5\u9664\u4e86\u66f4\u5e7f\u6cdb\u7684\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u5916\uff0c\u8fd8\u5e94\u5305\u62ec\uff1a\uff081\uff09\u77e5\u8bc6\u80cc\u666f\u4e25\u8c28\u6027\uff08\u51b3\u5b9a\u7814\u7a76\u5185\u5bb9\uff09\uff1b\uff082\uff09\u89c4\u8303\u6027\u4e25\u8c28\u6027\uff08\u5b66\u79d1\u3001\u793e\u533a\u6216\u4e2a\u4eba\u89c4\u8303\u7684\u5f71\u54cd\uff09\uff1b\uff083\uff09\u6982\u5ff5\u4e25\u8c28\u6027\uff08\u7406\u8bba\u6784\u5efa\u7684\u6e05\u6670\u6027\uff09\uff1b\uff084\uff09\u62a5\u544a\u4e25\u8c28\u6027\uff08\u5185\u5bb9\u7684\u62a5\u544a\u65b9\u5f0f\uff09\uff1b\uff085\uff09\u89e3\u91ca\u4e25\u8c28\u6027\uff08\u4ece\u73b0\u6709\u8bc1\u636e\u4e2d\u63a8\u65ad\u7684\u652f\u6301\u7a0b\u5ea6\uff09\u3002\u901a\u8fc7\u8fd9\u4e00\u6846\u67b6\uff0c\u6211\u4eec\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u3001\u8bb0\u8005\u7b49\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u6709\u7528\u7684\u8bed\u8a00\u548c\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u5173\u4e8eAI\u793e\u533a\u5de5\u4f5c\u7684\u5fc5\u8981\u5bf9\u8bdd\u3002"}}
{"id": "2506.14665", "pdf": "https://arxiv.org/pdf/2506.14665", "abs": "https://arxiv.org/abs/2506.14665", "authors": ["Giulia Luise", "Chin-Wei Huang", "Thijs Vogels", "Derk P. Kooi", "Sebastian Ehlert", "Stephanie Lanius", "Klaas J. H. Giesbertz", "Amir Karton", "Deniz Gunceler", "Megan Stanley", "Wessel P. Bruinsma", "Lin Huang", "Xinran Wei", "Jos\u00e9 Garrido Torres", "Abylay Katbashev", "B\u00e1lint M\u00e1t\u00e9", "S\u00e9kou-Oumar Kaba", "Roberto Sordillo", "Yingrong Chen", "David B. Williams-Young", "Christopher M. Bishop", "Jan Hermann", "Rianne van den Berg", "Paola Gori-Giorgi"], "title": "Accurate and scalable exchange-correlation with deep learning", "categories": ["physics.chem-ph", "cs.AI", "cs.CE", "cs.LG", "physics.comp-ph"], "comment": "Main: 13 pages plus references, 11 figures and tables. Supplementary information: 19 pages, 12 figures and tables", "summary": "Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\u00f6dinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ea4\u6362\u76f8\u5173\u6cdb\u51fdSkala\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u5f81\uff0c\u907f\u514d\u4e86\u624b\u5de5\u8bbe\u8ba1\u7279\u5f81\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u5c0f\u5206\u5b50\u539f\u5b50\u5316\u80fd\u91cf\u7684\u5316\u5b66\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u534a\u5c40\u57dfDFT\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u662f\u9884\u6d4b\u5206\u5b50\u548c\u6750\u6599\u6027\u8d28\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u7535\u5b50\u7ed3\u6784\u65b9\u6cd5\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u4f9d\u8d56\u4e8e\u5bf9\u672a\u77e5\u4ea4\u6362\u76f8\u5173\uff08XC\uff09\u6cdb\u51fd\u7684\u8fd1\u4f3c\u3002\u73b0\u6709\u6cdb\u51fd\u901a\u5e38\u901a\u8fc7\u624b\u5de5\u8bbe\u8ba1\u7684\u590d\u6742\u7279\u5f81\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u727a\u7272\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e14\u65e0\u6cd5\u8fbe\u5230\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u7684\u5316\u5b66\u7cbe\u5ea6\uff08\u8bef\u5dee\u4f4e\u4e8e1 kcal/mol\uff09\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u53c8\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSkala\u7684\u6df1\u5ea6\u5b66\u4e60\u57faXC\u6cdb\u51fd\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u5927\u91cf\u9ad8\u7cbe\u5ea6\u53c2\u8003\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u5f81\uff0c\u907f\u514d\u4e86\u624b\u5de5\u8bbe\u8ba1\u7279\u5f81\u7684\u590d\u6742\u6027\u3002\u8fd9\u4e9b\u6570\u636e\u662f\u901a\u8fc7\u8ba1\u7b97\u5bc6\u96c6\u7684\u6ce2\u51fd\u6570\u65b9\u6cd5\u751f\u6210\u7684\uff0c\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u5316\u5b66\u9886\u57df\u3002", "result": "Skala\u5728\u5c0f\u5206\u5b50\u539f\u5b50\u5316\u80fd\u91cf\u4e0a\u5b9e\u73b0\u4e86\u5316\u5b66\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u534a\u5c40\u57dfDFT\u7684\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u5f15\u5165\u5c11\u91cf\u9488\u5bf9\u975e\u539f\u5b50\u5316\u80fd\u91cf\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\uff0cSkala\u5728\u4e00\u822c\u4e3b\u65cf\u5316\u5b66\u4e2d\u7684\u8868\u73b0\u4e0e\u6700\u4f73\u6742\u5316\u6cdb\u51fd\u76f8\u5f53\uff0c\u800c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u534a\u5c40\u57dfDFT\u6c34\u5e73\u3002\u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\uff0c\u5176\u9884\u6d4b\u80fd\u529b\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "Skala\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728XC\u6cdb\u51fd\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u8ba1\u7b97\u6548\u7387\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002\u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u6269\u5c55\uff0cSkala\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u5347\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u7684\u9884\u6d4b\u80fd\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7cbe\u786e\u4e14\u53ef\u6269\u5c55\u7684\u4ea4\u6362\u76f8\u5173\u6cdb\u51fd", "abstract_zh": "\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u662f\u9884\u6d4b\u5206\u5b50\u548c\u6750\u6599\u6027\u8d28\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u7535\u5b50\u7ed3\u6784\u65b9\u6cd5\u3002\u5c3d\u7ba1DFT\u5728\u539f\u7406\u4e0a\u662f\u859b\u5b9a\u8c14\u65b9\u7a0b\u7684\u7cbe\u786e\u91cd\u6784\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u4f9d\u8d56\u4e8e\u5bf9\u672a\u77e5\u4ea4\u6362\u76f8\u5173\uff08XC\uff09\u6cdb\u51fd\u7684\u8fd1\u4f3c\u3002\u5927\u591a\u6570\u73b0\u6709XC\u6cdb\u51fd\u901a\u8fc7\u6709\u9650\u4e14\u65e5\u76ca\u590d\u6742\u7684\u624b\u5de5\u8bbe\u8ba1\u7279\u5f81\u6784\u5efa\uff0c\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u727a\u7272\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u65e0\u8fd1\u4f3c\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u7684\u5316\u5b66\u7cbe\u5ea6\uff08\u901a\u5e38\u5b9a\u4e49\u4e3a\u8bef\u5dee\u4f4e\u4e8e1 kcal/mol\uff09\u3002\u672c\u6587\u63d0\u51fa\u4e86Skala\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u73b0\u4ee3XC\u6cdb\u51fd\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u5f81\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u624b\u5de5\u8bbe\u8ba1\u7279\u5f81\u3002Skala\u5728\u5c0f\u5206\u5b50\u539f\u5b50\u5316\u80fd\u91cf\u4e0a\u5b9e\u73b0\u4e86\u5316\u5b66\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u534a\u5c40\u57dfDFT\u7684\u8ba1\u7b97\u6548\u7387\u3002\u8fd9\u4e00\u6027\u80fd\u5f97\u76ca\u4e8e\u5bf9\u524d\u6240\u672a\u6709\u7684\u5927\u91cf\u9ad8\u7cbe\u5ea6\u53c2\u8003\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u6570\u636e\u662f\u901a\u8fc7\u8ba1\u7b97\u5bc6\u96c6\u7684\u6ce2\u51fd\u6570\u65b9\u6cd5\u751f\u6210\u7684\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSkala\u5728\u6db5\u76d6\u591a\u6837\u5316\u5316\u5b66\u7684\u989d\u5916\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u63d0\u5347\u3002\u901a\u8fc7\u5f15\u5165\u5c11\u91cf\u9488\u5bf9\u975e\u539f\u5b50\u5316\u80fd\u91cf\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\uff0cSkala\u5728\u4e00\u822c\u4e3b\u65cf\u5316\u5b66\u4e2d\u7684\u8868\u73b0\u4e0e\u6700\u4f73\u6742\u5316\u6cdb\u51fd\u76f8\u5f53\uff0c\u800c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u534a\u5c40\u57dfDFT\u6c34\u5e73\u3002\u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u6301\u7eed\u6269\u5c55\uff0cSkala\u6709\u671b\u8fdb\u4e00\u6b65\u589e\u5f3a\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.14670", "pdf": "https://arxiv.org/pdf/2506.14670", "abs": "https://arxiv.org/abs/2506.14670", "authors": ["Jina Kim", "Leeje Jang", "Yao-Yi Chiang", "Guanyu Wang", "Michelle Pasco"], "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86StreetLens\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4eba\u672c\u5316\u3001\u53ef\u914d\u7f6e\u7684\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u793e\u533a\u73af\u5883\u8bc4\u4f30\uff0c\u7ed3\u5408\u793e\u4f1a\u79d1\u5b66\u4e13\u4e1a\u77e5\u8bc6\uff0c\u63d0\u5347\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u793e\u533a\u7814\u7a76\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbf\u8c08\u3001\u8c03\u67e5\u548c\u56fe\u50cf\u6807\u6ce8\uff0c\u8017\u65f6\u4e14\u9700\u4e13\u5bb6\u5e72\u9884\u3002\u73b0\u6709\u6280\u672f\u867d\u90e8\u5206\u81ea\u52a8\u5316\uff0c\u4f46\u7f3a\u4e4f\u8de8\u8bbe\u8ba1\u548c\u5730\u7406\u73af\u5883\u7684\u9002\u5e94\u6027\u3002StreetLens\u65e8\u5728\u901a\u8fc7\u4eba\u672c\u5316AI\u7cfb\u7edf\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "StreetLens\u901a\u8fc7\u5d4c\u5165\u793e\u4f1a\u79d1\u5b66\u77e5\u8bc6\u7684VLM\uff0c\u6a21\u62df\u4eba\u5de5\u7f16\u7801\u8fc7\u7a0b\uff0c\u4ece\u8857\u666f\u56fe\u50cf\u4e2d\u751f\u6210\u4ece\u5ba2\u89c2\u7279\u5f81\u5230\u4e3b\u89c2\u611f\u77e5\u7684\u8bed\u4e49\u6807\u6ce8\uff0c\u652f\u6301\u7814\u7a76\u8005\u81ea\u5b9a\u4e49\u63d0\u793a\u4ee5\u589e\u5f3a\u5206\u6790\u7075\u6d3b\u6027\u3002", "result": "StreetLens\u5b9e\u73b0\u4e86\u793e\u533a\u73af\u5883\u7684\u9ad8\u6548\u8bc4\u4f30\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u8bed\u4e49\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u8c03\u67e5\u6570\u636e\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5730\u7406\u73af\u5883\u3002", "conclusion": "StreetLens\u4ee3\u8868\u4e86\u7075\u6d3b\u3001\u81ea\u4e3b\u7684AI\u7cfb\u7edf\u53d1\u5c55\u65b9\u5411\uff0c\u901a\u8fc7\u4e0e\u7814\u7a76\u8005\u534f\u4f5c\u52a0\u901f\u548c\u6269\u5c55\u793e\u533a\u7814\u7a76\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "paper_title_zh": "StreetLens\uff1a\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\u7684\u4eba\u672c\u5316AI\u4ee3\u7406\u793e\u533a\u8bc4\u4f30\u7cfb\u7edf", "abstract_zh": "\u4f20\u7edf\u793e\u533a\u7814\u7a76\u4f9d\u8d56\u8bbf\u8c08\u3001\u8c03\u67e5\u548c\u4eba\u5de5\u56fe\u50cf\u6807\u6ce8\uff0c\u8bc6\u522b\u73af\u5883\u7279\u5f81\uff08\u5982\u7269\u7406\u6df7\u4e71\u3001\u8857\u9053\u5b89\u5168\u7b49\uff09\u53ca\u5176\u5bf9\u5065\u5eb7\u548c\u53d1\u5c55\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u65b9\u6cd5\u867d\u6df1\u5165\u4f46\u8017\u65f6\u4e14\u9700\u4e13\u5bb6\u5e72\u9884\u3002\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5f00\u59cb\u90e8\u5206\u81ea\u52a8\u5316\u6b64\u8fc7\u7a0b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u8bbe\u8ba1\u548c\u5730\u7406\u73af\u5883\u7684\u9002\u5e94\u6027\u3002\u672c\u6587\u63d0\u51faStreetLens\uff0c\u4e00\u79cd\u4eba\u672c\u5316\u3001\u53ef\u914d\u7f6e\u7684\u5de5\u4f5c\u6d41\uff0c\u5c06\u793e\u4f1a\u79d1\u5b66\u77e5\u8bc6\u5d4c\u5165VLM\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u793e\u533a\u73af\u5883\u8bc4\u4f30\u3002StreetLens\u6a21\u62df\u4eba\u5de5\u7f16\u7801\u8fc7\u7a0b\uff0c\u57fa\u4e8e\u8bbf\u8c08\u534f\u8bae\u95ee\u9898\u5206\u6790\u8857\u666f\u56fe\u50cf\uff0c\u751f\u6210\u4ece\u5ba2\u89c2\u7279\u5f81\uff08\u5982\u8f66\u8f86\u6570\u91cf\uff09\u5230\u4e3b\u89c2\u611f\u77e5\uff08\u5982\u56fe\u50cf\u6df7\u4e71\u611f\uff09\u7684\u5e7f\u6cdb\u8bed\u4e49\u6807\u6ce8\u3002\u901a\u8fc7\u7814\u7a76\u8005\u81ea\u5b9a\u4e49\u63d0\u793a\uff0cStreetLens\u5c06\u9886\u57df\u77e5\u8bc6\u7f6e\u4e8e\u5206\u6790\u6838\u5fc3\uff0c\u5e76\u652f\u6301\u6574\u5408\u5148\u9a8c\u8c03\u67e5\u6570\u636e\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u8bc4\u4f30\u8303\u56f4\u3002\u6211\u4eec\u63d0\u4f9bGoogle Colab\u7b14\u8bb0\u672c\uff0c\u4fbf\u4e8e\u7814\u7a76\u8005\u4f7f\u7528\u516c\u5171\u6216\u81ea\u5b9a\u4e49\u8857\u666f\u6570\u636e\u96c6\u3002StreetLens\u6807\u5fd7\u7740\u5411\u7075\u6d3b\u3001\u81ea\u4e3b\u7684AI\u7cfb\u7edf\u8f6c\u53d8\uff0c\u4e0e\u7814\u7a76\u8005\u534f\u4f5c\u52a0\u901f\u548c\u6269\u5c55\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2506.14677", "pdf": "https://arxiv.org/pdf/2506.14677", "abs": "https://arxiv.org/abs/2506.14677", "authors": ["Yingchao Li"], "title": "Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u53ef\u7f16\u8f91\u8bed\u97f3\u5230\u624b\u8bed\u8f6c\u6362\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u53ef\u7f16\u8f91\u7684JSON\u4e2d\u95f4\u5c42\u548c\u5b9e\u65f6\u4f18\u5316\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u52a8\u753b\u7684\u81ea\u7136\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u7406\u89e3\u5ea6\u3001\u81ea\u7136\u6027\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u6280\u672f\u5b58\u5728\u7528\u6237\u65e0\u6cd5\u76f4\u63a5\u7f16\u8f91\u548c\u4f18\u5316\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u624b\u8bed\u52a8\u753b\u7684\u81ea\u7136\u6027\u548c\u8868\u8fbe\u529b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408Transformer\u6280\u672f\u548c\u7528\u6237\u53c2\u4e0e\u5f0f\u53cd\u9988\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u900f\u660e\u4e14\u7528\u6237\u53cb\u597d\u7684\u8bed\u97f3\u5230\u624b\u8bed\u8f6c\u6362\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u6d41\u5f0fConformer\u7f16\u7801\u5668\u548c\u81ea\u56de\u5f52Transformer-MDN\u89e3\u7801\u5668\uff0c\u5c06\u8bed\u97f3\u8f93\u5165\u540c\u6b65\u8f6c\u5316\u4e3a3D\u865a\u62df\u5f62\u8c61\u7684\u4e0a\u534a\u8eab\u548c\u9762\u90e8\u52a8\u4f5c\u3002\u901a\u8fc7\u7528\u6237\u53ef\u7f16\u8f91\u7684JSON\u4e2d\u95f4\u5c42\u548c\u5b9e\u65f6\u4f18\u5316\u5faa\u73af\uff0c\u5b9e\u73b0\u7528\u6237\u76f4\u63a5\u4fee\u6539\u548c\u53cd\u9988\u9a71\u52a8\u7684\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c20\u540d\u804b\u54d1\u4eba\u58eb\u548c5\u540d\u624b\u8bed\u7ffb\u8bd1\u8005\u7684\u53c2\u4e0e\u8868\u660e\uff0c\u53ef\u7f16\u8f91\u754c\u9762\u548c\u7528\u6237\u53cd\u9988\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u7406\u89e3\u5ea6\u3001\u81ea\u7136\u6027\u3001\u6613\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\u3002\u7cfb\u7edf\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u6bcf\u5e27\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e20\u6beb\u79d2\uff0c\u9002\u5408\u5b9e\u65f6\u901a\u4fe1\u548c\u6559\u80b2\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u6280\u672f\u4e0e\u7528\u6237\u53c2\u4e0e\u5f0f\u521b\u65b0\u76f8\u7ed3\u5408\u5982\u4f55\u5b9e\u73b0\u53ef\u8bbf\u95ee\u3001\u53ef\u89e3\u91ca\u4e14\u7528\u6237\u81ea\u9002\u5e94\u7684AI\u624b\u8bed\u6280\u672f\uff0c\u4e3a\u672a\u6765\u624b\u8bed\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "\u8bbe\u8ba1\u4e00\u79cd\u53ef\u7f16\u8f91\u7684\u8bed\u97f3\u5230\u624b\u8bed\u8f6c\u6362\u7cfb\u7edf\uff1a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u65b9\u6cd5", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5b9e\u65f6\u7528\u6237\u81ea\u9002\u5e94\u8bed\u97f3\u5230\u624b\u8bed\u52a8\u753b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u57fa\u4e8eTransformer\u7684\u52a8\u4f5c\u751f\u6210\u548c\u900f\u660e\u7684\u7528\u6237\u53ef\u7f16\u8f91JSON\u4e2d\u95f4\u5c42\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5141\u8bb8\u7528\u6237\u76f4\u63a5\u68c0\u67e5\u548c\u4fee\u6539\u624b\u8bed\u7247\u6bb5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u624b\u8bed\u6280\u672f\u7684\u5173\u952e\u9650\u5236\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u81ea\u7136\u6027\u3001\u8868\u8fbe\u529b\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002\u7cfb\u7edf\u5229\u7528\u6d41\u5f0fConformer\u7f16\u7801\u5668\u548c\u81ea\u56de\u5f52Transformer-MDN\u89e3\u7801\u5668\uff0c\u5c06\u8bed\u97f3\u8f93\u5165\u540c\u6b65\u8f6c\u5316\u4e3a3D\u865a\u62df\u5f62\u8c61\u7684\u4e0a\u534a\u8eab\u548c\u9762\u90e8\u52a8\u4f5c\u3002\u7528\u6237\u7f16\u8f91\u548c\u8bc4\u5206\u6570\u636e\u88ab\u7eb3\u5165\u5b9e\u65f6\u4f18\u5316\u5faa\u73af\u4ee5\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u300220\u540d\u804b\u54d1\u4eba\u58eb\u548c5\u540d\u624b\u8bed\u7ffb\u8bd1\u8005\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u53ef\u7f16\u8f91\u754c\u9762\u548c\u53c2\u4e0e\u5f0f\u53cd\u9988\u663e\u8457\u63d0\u5347\u4e86\u7406\u89e3\u5ea6\u3001\u81ea\u7136\u6027\u3001\u6613\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\u3002\u7cfb\u7edf\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u6bcf\u5e27\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e20\u6beb\u79d2\uff0c\u9002\u5408\u5b9e\u65f6\u901a\u4fe1\u548c\u6559\u80b2\u5e94\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u6280\u672f\u548c\u53c2\u4e0e\u5f0f\u521b\u65b0\u5982\u4f55\u5171\u540c\u63a8\u52a8\u624b\u8bed\u6280\u672f\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u81ea\u9002\u5e94\u6027\u3002"}}
{"id": "2506.14683", "pdf": "https://arxiv.org/pdf/2506.14683", "abs": "https://arxiv.org/abs/2506.14683", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "title": "Unified Software Engineering agent as AI Software Engineer", "categories": ["cs.SE", "cs.AI"], "comment": "Leonhard Applis and Yuntong Zhang contributed equally to this work", "summary": "The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff08USEagent\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8f6f\u4ef6\u5de5\u7a0b\u80fd\u529b\uff08\u5982\u7f16\u7801\u3001\u6d4b\u8bd5\u3001\u4fee\u590d\uff09\u6765\u6a21\u62df\u672a\u6765AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u89d2\u8272\u3002USEagent\u5728\u5305\u542b1,271\u4e2a\u4efb\u52a1\u7684USEbench\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u4ee3\u7406\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u67d0\u4e9b\u7f16\u7801\u4efb\u52a1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u867d\u5728\u81ea\u52a8\u5316\u7f16\u7801\u65b9\u9762\u6709\u8fdb\u5c55\uff0c\u4f46\u8f6f\u4ef6\u5de5\u7a0b\u6db5\u76d6\u66f4\u5e7f\uff0c\u5305\u62ec\u7ef4\u62a4\u548c\u9879\u76ee\u6f14\u8fdb\u7b49\u3002\u73b0\u6709\u7814\u7a76\u591a\u4e13\u6ce8\u4e8e\u7279\u5b9a\u4efb\u52a1\u4ee3\u7406\uff08\u5982\u6d4b\u8bd5\u3001\u4fee\u590d\uff09\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u4ee3\u7406\uff0c\u4ee5\u6a21\u62df\u672a\u6765AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u591a\u529f\u80fd\u89d2\u8272\u3002", "method": "\u5f00\u53d1\u4e86\u7edf\u4e00\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff08USEagent\uff09\uff0c\u6574\u5408\u591a\u79cd\u8f6f\u4ef6\u5de5\u7a0b\u80fd\u529b\uff08\u5982\u7f16\u7801\u3001\u6d4b\u8bd5\u3001\u4fee\u590d\uff09\uff0c\u5e76\u6784\u5efa\u4e86USEbench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6765\u81eaSWE-bench\u3001SWT-bench\u548cREPOCOD\u7b49\u4efb\u52a1\u76841,271\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002", "result": "\u5728USEbench\u6d4b\u8bd5\u4e2d\uff0cUSEagent\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u4ee3\u7406\uff08\u5982OpenHands CodeActAgent\uff09\uff0c\u4f46\u5728\u67d0\u4e9b\u7f16\u7801\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "USEagent\u4f5c\u4e3a\u672a\u6765AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u96cf\u5f62\uff0c\u5c55\u793a\u4e86\u6574\u5408\u591a\u4efb\u52a1\u80fd\u529b\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5b8c\u5168\u80dc\u4efb\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\u3002", "paper_title_zh": "\u7edf\u4e00\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff1a\u4f5c\u4e3aAI\u8f6f\u4ef6\u5de5\u7a0b\u5e08", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u53d1\u5c55\u63d0\u9ad8\u4e86\u5bf9\u81ea\u52a8\u5316\u7f16\u7801\u7684\u671f\u671b\uff0c\u4f46\u8f6f\u4ef6\u5de5\u7a0b\u4e0d\u4ec5\u9650\u4e8e\u7f16\u7801\uff0c\u8fd8\u5305\u62ec\u7ef4\u62a4\u548c\u9879\u76ee\u6f14\u8fdb\u7b49\u6d3b\u52a8\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0cLLM\u4ee3\u7406\u7684\u6982\u5ff5\u53d7\u5230\u5173\u6ce8\uff0c\u5176\u5229\u7528LLM\u4f5c\u4e3a\u63a8\u7406\u5f15\u64ce\u81ea\u4e3b\u8c03\u7528\u5916\u90e8\u5de5\u5177\u3002\u7136\u800c\uff0cLLM\u4ee3\u7406\u662f\u5426\u7b49\u540c\u4e8eAI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff1f\u672c\u6587\u901a\u8fc7\u5f00\u53d1\u7edf\u4e00\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff08USEagent\uff09\u63a2\u8ba8\u8fd9\u4e00\u95ee\u9898\u3002\u4e0e\u73b0\u6709\u4e13\u6ce8\u4e8e\u7279\u5b9a\u8f6f\u4ef6\u4efb\u52a1\uff08\u5982\u6d4b\u8bd5\u3001\u8c03\u8bd5\u3001\u4fee\u590d\uff09\u7684\u4ee3\u7406\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u534f\u8c03\u548c\u5904\u7406\u591a\u79cd\u80fd\u529b\u7684\u7edf\u4e00\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u5bf9\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\uff08\u5982\u4fee\u590d\u4e0d\u5b8c\u6574\u8865\u4e01\u3001\u6dfb\u52a0\u65b0\u529f\u80fd\u6216\u63a5\u7ba1\u4ed6\u4eba\u4ee3\u7801\uff09\u3002\u6211\u4eec\u8bbe\u60f3USEagent\u4f5c\u4e3a\u672a\u6765AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u521d\u7a3f\uff0c\u53ef\u6210\u4e3aAI\u4e0e\u4eba\u7c7b\u5171\u540c\u53c2\u4e0e\u7684\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u6210\u5458\u3002\u4e3a\u8bc4\u4f30USEagent\u7684\u6548\u679c\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7edf\u4e00\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\uff08USEbench\uff09\uff0c\u5305\u542b\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u4fee\u8865\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u6574\u5408\u4e86SWE-bench\u3001SWT-bench\u548cREPOCOD\u7b49\u73b0\u6709\u57fa\u51c6\u7684\u4efb\u52a1\u3002\u5728\u5305\u542b1,271\u4e2a\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684USEbench\u8bc4\u4f30\u4e2d\uff0cUSEagent\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u4ee3\u7406\uff08\u5982OpenHands CodeActAgent\uff09\u3002USEagent\u5728\u67d0\u4e9b\u7f16\u7801\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u80fd\u529b\u5dee\u8ddd\uff0c\u8fd9\u4e3a\u672a\u6765AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.14684", "pdf": "https://arxiv.org/pdf/2506.14684", "abs": "https://arxiv.org/abs/2506.14684", "authors": ["Aditya Bhattacharjee", "Ivan Meresman Higgs", "Mark Sandler", "Emmanouil Benetos"], "title": "Refining music sample identification with a self-supervised graph neural network", "categories": ["cs.SD", "cs.AI", "cs.IR"], "comment": "Accepted at International Conference for Music Information Retrieval (ISMIR) 2025", "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.\n  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.\n  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u67b6\u6784\uff0c\u7528\u4e8e\u6539\u8fdb\u97f3\u4e50\u6837\u672c\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u5728\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u4ec5\u4f7f\u75289%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u8fbe\u5230\u4e0e\u5f53\u524d\u6700\u4f18\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff08mAP 44.2%\uff09\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u81ea\u52a8\u6837\u672c\u8bc6\u522b\uff08ASID\uff09\u5728\u97f3\u9891\u68c0\u7d22\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\u97f3\u4e50\u5236\u4f5c\u4e2d\u7684\u5e38\u89c1\u53d8\u6362\uff08\u5982\u65f6\u95f4\u62c9\u4f38\u3001\u97f3\u9ad8\u53d8\u6362\u7b49\uff09\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u5bf9\u8fd9\u4e9b\u53d8\u6362\u9c81\u68d2\u7684\u7cfb\u7edf\u662f\u5f53\u524d\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7f16\u7801\u67b6\u6784\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\uff1a\u5148\u901a\u8fc7\u7c97\u7c92\u5ea6\u76f8\u4f3c\u6027\u641c\u7d22\u7b5b\u9009\u5019\u9009\u6837\u672c\uff0c\u518d\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5206\u7c7b\u5668\u5254\u9664\u65e0\u5173\u5339\u914d\u5e76\u4f18\u5316\u6392\u540d\u3002", "result": "\u6a21\u578b\u4ec5\u4f7f\u75289%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5373\u8fbe\u523044.2%\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\uff0c\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u4f18\u7cfb\u7edf\u76f8\u5f53\u3002\u540c\u65f6\uff0c\u9488\u5bf9\u77ed\u65f6\u67e5\u8be2\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u65b0\u6807\u6ce8\u7684Sample100\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8f7b\u91cf\u5316\u548c\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u97f3\u4e50\u6837\u672c\u8bc6\u522b\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u4e3a\u77ed\u65f6\u67e5\u8be2\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "paper_title_zh": "\u57fa\u4e8e\u81ea\u76d1\u7763\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u97f3\u4e50\u6837\u672c\u8bc6\u522b\u4f18\u5316", "abstract_zh": "\u81ea\u52a8\u6837\u672c\u8bc6\u522b\uff08ASID\uff09\u662f\u97f3\u9891\u68c0\u7d22\u9886\u57df\u7684\u4e00\u9879\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u76ee\u6807\u662f\u68c0\u6d4b\u5e76\u8bc6\u522b\u5728\u65b0\u97f3\u4e50\u4f5c\u54c1\u4e2d\u91cd\u590d\u4f7f\u7528\u7684\u97f3\u9891\u7247\u6bb5\u3002\u5c3d\u7ba1\u76f8\u5173\u4efb\u52a1\uff08\u5982\u97f3\u9891\u6307\u7eb9\u8bc6\u522b\uff09\u5728\u201c\u771f\u5b9e\u4e16\u754c\u201d\uff08\u566a\u58f0\u3001\u6df7\u54cd\uff09\u6761\u4ef6\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46ASID\u7cfb\u7edf\u4ecd\u96be\u4ee5\u8bc6\u522b\u7ecf\u8fc7\u97f3\u4e50\u4fee\u6539\u7684\u6837\u672c\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u5bf9\u65f6\u95f4\u62c9\u4f38\u3001\u97f3\u9ad8\u53d8\u6362\u3001\u6548\u679c\u5904\u7406\u53ca\u80cc\u666f\u97f3\u4e50\u7b49\u5e38\u89c1\u97f3\u4e50\u5236\u4f5c\u53d8\u6362\u9c81\u68d2\u7684\u7cfb\u7edf\u662f\u5f53\u524d\u7684\u91cd\u8981\u6311\u6218\u3002\n\n\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u6269\u5c55\u7684\u7f16\u7801\u67b6\u6784\uff0c\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002\u4e0e\u5f53\u524d\u6700\u4f18\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4ec5\u4f7f\u75289%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5374\u8fbe\u5230\u4e86\u76f8\u8fd1\u7684\u6027\u80fd\uff08\u5e73\u5747\u7cbe\u5ea644.2%\uff09\u3002\n\n\u4e3a\u63d0\u9ad8\u68c0\u7d22\u8d28\u91cf\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u7c97\u7c92\u5ea6\u76f8\u4f3c\u6027\u641c\u7d22\u7b5b\u9009\u5019\u9009\u6837\u672c\uff0c\u968f\u540e\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5206\u7c7b\u5668\u5254\u9664\u65e0\u5173\u5339\u914d\u5e76\u4f18\u5316\u5019\u9009\u6392\u540d\u2014\u2014\u8fd9\u662f\u73b0\u6709\u6a21\u578b\u6240\u7f3a\u4e4f\u7684\u5173\u952e\u80fd\u529b\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u67e5\u8be2\u65f6\u957f\u8f83\u77ed\u7684\u7279\u70b9\uff0c\u6211\u4eec\u4f7f\u7528\u65b0\u6807\u6ce8\u7684Sample100\u6570\u636e\u96c6\u5bf9\u7cfb\u7edf\u8fdb\u884c\u4e86\u77ed\u65f6\u67e5\u8be2\u6027\u80fd\u6d4b\u8bd5\uff0c\u5e76\u5c06\u8be5\u6570\u636e\u96c6\u4f5c\u4e3a\u672c\u6587\u7684\u4e00\u90e8\u5206\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.14723", "pdf": "https://arxiv.org/pdf/2506.14723", "abs": "https://arxiv.org/abs/2506.14723", "authors": ["Yusong Wu", "Tim Cooijmans", "Kyle Kastner", "Adam Roberts", "Ian Simon", "Alexander Scarlatos", "Chris Donahue", "Cassie Tarakajian", "Shayegan Omidshafiei", "Aaron Courville", "Pablo Samuel Castro", "Natasha Jaques", "Cheng-Zhi Anna Huang"], "title": "Adaptive Accompaniment with ReaLchords", "categories": ["cs.SD", "cs.AI"], "comment": "Accepted by ICML 2024", "summary": "Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \\emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReaLchords\u7684\u5728\u7ebf\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u4e3a\u7528\u6237\u65cb\u5f8b\u5373\u5174\u4f34\u594f\u3002\u901a\u8fc7\u7ed3\u5408\u6700\u5927\u4f3c\u7136\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u6a21\u578b\u80fd\u591f\u9002\u5e94\u964c\u751f\u8f93\u5165\u5e76\u751f\u6210\u548c\u8c10\u7684\u4f34\u594f\u3002", "motivation": "\u5f53\u524d\u97f3\u4e50\u751f\u6210\u6a21\u578b\u867d\u80fd\u4ea7\u751f\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8f93\u51fa\uff0c\u4f46\u65e0\u6cd5\u5728\u7ebf\uff08\u5b9e\u65f6\uff09\u4e0e\u5176\u4ed6\u97f3\u4e50\u5bb6\uff08\u4eba\u6216\u673a\u5668\uff09\u534f\u4f5c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5373\u5174\u4f34\u594f\u3002", "method": "\u9996\u5148\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u9884\u8bad\u7ec3\u5728\u7ebf\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002\u5fae\u8c03\u76ee\u6807\u5305\u62ec\u65b0\u9896\u7684\u5956\u52b1\u6a21\u578b\uff08\u8bc4\u4f30\u65cb\u5f8b\u4e0e\u548c\u5f26\u7684\u548c\u8c10\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff09\u548c\u4ece\u80fd\u9884\u89c1\u672a\u6765\u65cb\u5f8b\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u5dee\u5f02\u9879\u3002", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u548c\u542c\u89c9\u6d4b\u8bd5\u8868\u660e\uff0c\u6a21\u578b\u80fd\u826f\u597d\u9002\u5e94\u964c\u751f\u8f93\u5165\u5e76\u751f\u6210\u5408\u9002\u7684\u4f34\u594f\u3002", "conclusion": "ReaLchords\u4e3a\u5b9e\u65f6\u5373\u5174\u6f14\u594f\u548c\u5176\u4ed6\u6a21\u6001\u7684\u540c\u6b65\u5171\u521b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "paper_title_zh": "\u57fa\u4e8eReaLchords\u7684\u81ea\u9002\u5e94\u4f34\u594f", "abstract_zh": "\u5373\u5174\u6f14\u594f\u9700\u8981\u97f3\u4e50\u5bb6\u4e4b\u95f4\u7684\u534f\u8c03\u3001\u9884\u5224\u548c\u534f\u4f5c\u521b\u9020\u529b\u3002\u5f53\u524d\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u867d\u80fd\u4ea7\u751f\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8f93\u51fa\uff0c\u4f46\u65e0\u6cd5\u4ee5\u5728\u7ebf\u65b9\u5f0f\uff08\u5373\u4e0e\u5176\u4ed6\u97f3\u4e50\u5bb6\uff08\u4eba\u6216\u673a\u5668\uff09\u540c\u65f6\uff09\u751f\u6210\u97f3\u4e50\u3002\u6211\u4eec\u63d0\u51fa\u4e86ReaLchords\uff0c\u4e00\u79cd\u5728\u7ebf\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4e3a\u7528\u6237\u65cb\u5f8b\u5373\u5174\u4f34\u594f\u3002\u6211\u4eec\u4ece\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u9884\u8bad\u7ec3\u7684\u5728\u7ebf\u6a21\u578b\u51fa\u53d1\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u9002\u5e94\u5728\u7ebf\u4f7f\u7528\u3002\u5fae\u8c03\u76ee\u6807\u7ed3\u5408\u4e86\u65b0\u9896\u7684\u5956\u52b1\u6a21\u578b\uff08\u63d0\u4f9b\u65cb\u5f8b\u4e0e\u548c\u5f26\u5728\u548c\u8c10\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u53cd\u9988\uff09\u548c\u4ece\u80fd\u9884\u89c1\u672a\u6765\u65cb\u5f8b\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u5dee\u5f02\u9879\u3002\u901a\u8fc7\u5b9a\u91cf\u5b9e\u9a8c\u548c\u542c\u89c9\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc1\u660e\u8be5\u6a21\u578b\u80fd\u826f\u597d\u9002\u5e94\u964c\u751f\u8f93\u5165\u5e76\u751f\u6210\u5408\u9002\u7684\u4f34\u594f\u3002ReaLchords\u4e3a\u5b9e\u65f6\u5373\u5174\u6f14\u594f\u4ee5\u53ca\u5176\u4ed6\u6a21\u6001\u7684\u540c\u6b65\u5171\u521b\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2506.14727", "pdf": "https://arxiv.org/pdf/2506.14727", "abs": "https://arxiv.org/abs/2506.14727", "authors": ["Huihan Liu", "Rutav Shah", "Shuijing Liu", "Jack Pittenger", "Mingyo Seo", "Yuchen Cui", "Yonatan Bisk", "Roberto Mart\u00edn-Mart\u00edn", "Yuke Zhu"], "title": "Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.", "AI": {"tldr": "Casper\u662f\u4e00\u79cd\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u5b9e\u65f6\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u6267\u884c\u7075\u6d3b\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u5e76\u964d\u4f4e\u7528\u6237\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u673a\u5668\u4eba\u5982\u4f55\u4ece\u7528\u6237\u63a7\u5236\u8f93\u5165\u4e2d\u63a8\u65ad\u5e7f\u6cdb\u7684\u610f\u56fe\u5e76\u63d0\u4f9b\u6b63\u786e\u534f\u52a9\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7b80\u5355\u9884\u5b9a\u4e49\u573a\u666f\u6216\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u5206\u5e03\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "Casper\u7ed3\u5408\u5f00\u653e\u4e16\u754c\u611f\u77e5\u6a21\u5757\u3001\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u610f\u56fe\u63a8\u65ad\u673a\u5236\u548c\u6280\u80fd\u5e93\uff0c\u5b9e\u73b0\u5bf9\u65b0\u9896\u5bf9\u8c61\u548c\u573a\u666f\u7684\u901a\u7528\u7406\u89e3\uff0c\u5e76\u652f\u6301\u591a\u6837\u5316\u7684\u957f\u671f\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCasper\u5728\u4efb\u52a1\u6027\u80fd\u3001\u964d\u4f4e\u7528\u6237\u8ba4\u77e5\u8d1f\u8377\u548c\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u4f18\u4e8e\u76f4\u63a5\u8fdc\u7a0b\u64cd\u4f5c\u548c\u5176\u4ed6\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Casper\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "Casper\uff1a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u591a\u6837\u5316\u610f\u56fe\u7684\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf", "abstract_zh": "\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u901a\u8fc7\u4eba\u4e0e\u673a\u5668\u4eba\u5171\u4eab\u63a7\u5236\uff0c\u80fd\u591f\u5728\u591a\u6837\u5316\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u76f4\u89c2\u7684\u4eba\u673a\u534f\u4f5c\u3002\u73b0\u5b9e\u4e16\u754c\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u673a\u5668\u4eba\u5982\u4f55\u4ece\u7528\u6237\u63a7\u5236\u8f93\u5165\u4e2d\u63a8\u65ad\u5e7f\u6cdb\u7684\u610f\u56fe\u5e76\u63d0\u4f9b\u6b63\u786e\u534f\u52a9\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u7b80\u5355\u9884\u5b9a\u4e49\u573a\u666f\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u8bad\u7ec3\u65f6\u7684\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u5206\u5e03\uff0c\u96be\u4ee5\u652f\u6301\u5b9e\u9645\u5e94\u7528\u3002\u6211\u4eec\u63d0\u51faCasper\uff0c\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u5d4c\u5165\u7684\u5e38\u8bc6\u77e5\u8bc6\u8fdb\u884c\u5b9e\u65f6\u610f\u56fe\u63a8\u65ad\u548c\u7075\u6d3b\u6280\u80fd\u6267\u884c\u7684\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u3002Casper\u5305\u542b\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u611f\u77e5\u6a21\u5757\uff0c\u7528\u4e8e\u5bf9\u65b0\u5bf9\u8c61\u548c\u573a\u666f\u7684\u901a\u7528\u7406\u89e3\uff1b\u4e00\u4e2a\u57fa\u4e8eVLM\u7684\u610f\u56fe\u63a8\u65ad\u673a\u5236\uff0c\u5229\u7528\u5e38\u8bc6\u63a8\u7406\u89e3\u91ca\u7528\u6237\u8f93\u5165\u7684\u7247\u6bb5\uff1b\u4ee5\u53ca\u4e00\u4e2a\u6280\u80fd\u5e93\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u957f\u671f\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002\u901a\u8fc7\u5305\u62ec\u4eba\u7c7b\u7814\u7a76\u548c\u7cfb\u7edf\u6d88\u878d\u5b9e\u9a8c\u5728\u5185\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\uff0cCasper\u5728\u4efb\u52a1\u6027\u80fd\u3001\u964d\u4f4e\u7528\u6237\u8ba4\u77e5\u8d1f\u8377\u548c\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u4f18\u4e8e\u76f4\u63a5\u8fdc\u7a0b\u64cd\u4f5c\u548c\u5176\u4ed6\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2506.14750", "pdf": "https://arxiv.org/pdf/2506.14750", "abs": "https://arxiv.org/abs/2506.14750", "authors": ["Gaobin Yang", "Maokui He", "Shutong Niu", "Ruoyu Wang", "Hang Chen", "Jun Du"], "title": "Exploring Speaker Diarization with Mixture of Experts", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u8bf4\u8bdd\u4eba\u5206\u5272\u7cfb\u7edfNSD-MS2S\uff0c\u7ed3\u5408\u8bb0\u5fc6\u611f\u77e5\u591a\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6a21\u5757\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u5171\u4eab\u4e0e\u8f6f\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\uff08SS-MoE\uff09\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u8bf4\u8bdd\u4eba\u5206\u5272\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u58f0\u5b66\u573a\u666f\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8bb0\u5fc6\u611f\u77e5\u5d4c\u5165\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u67b6\u6784\uff0c\u4ee5\u53ca\u5f15\u5165\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faNSD-MS2S\u7cfb\u7edf\uff0c\u6574\u5408\u8bb0\u5fc6\u611f\u77e5\u591a\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6a21\u5757\u4e0e\u5e8f\u5217\u5230\u5e8f\u5217\u67b6\u6784\uff0c\u5e76\u5f15\u5165SS-MoE\u6a21\u5757\u4ee5\u51cf\u5c11\u6a21\u578b\u504f\u5dee\u3002\u6269\u5c55\u6a21\u578b\u4e3aNSD-MS2S-SSMoE\u3002", "result": "\u5728CHiME-6\u3001DiPCo\u3001Mixer 6\u548cDIHARD-III\u7b49\u590d\u6742\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "NSD-MS2S-SSMoE\u7cfb\u7edf\u5728\u590d\u6742\u58f0\u5b66\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u611f\u77e5\u5d4c\u5165\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bf4\u8bdd\u4eba\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u7684\u8bf4\u8bdd\u4eba\u5206\u5272\u63a2\u7d22", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u8bf4\u8bdd\u4eba\u5206\u5272\u7cfb\u7edfNSD-MS2S\uff0c\u8be5\u7cfb\u7edf\u5c06\u8bb0\u5fc6\u611f\u77e5\u591a\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6a21\u5757\u4e0e\u5e8f\u5217\u5230\u5e8f\u5217\u67b6\u6784\u76f8\u7ed3\u5408\u3002\u7cfb\u7edf\u5229\u7528\u8bb0\u5fc6\u6a21\u5757\u589e\u5f3a\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u5e76\u91c7\u7528Seq2Seq\u6846\u67b6\u5c06\u58f0\u5b66\u7279\u5f81\u9ad8\u6548\u6620\u5c04\u5230\u8bf4\u8bdd\u4eba\u6807\u7b7e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6df7\u5408\u4e13\u5bb6\u5728\u8bf4\u8bdd\u4eba\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f15\u5165\u5171\u4eab\u4e0e\u8f6f\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\uff08SS-MoE\uff09\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u6a21\u578b\u504f\u5dee\u5e76\u63d0\u5347\u6027\u80fd\u3002\u7ed3\u5408SS-MoE\u540e\uff0c\u6269\u5c55\u6a21\u578b\u4e3aNSD-MS2S-SSMoE\u3002\u5728CHiME-6\u3001DiPCo\u3001Mixer 6\u548cDIHARD-III\u7b49\u591a\u4e2a\u590d\u6742\u58f0\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u6240\u63d0\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
