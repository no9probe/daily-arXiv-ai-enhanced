{"id": "2506.17223", "pdf": "https://arxiv.org/pdf/2506.17223", "abs": "https://arxiv.org/abs/2506.17223", "authors": ["Shuvra Smaran Das", "Anirban Saha Anik", "Md Kishor Morol", "Mohammad Sakib Mahmood"], "title": "Outcome-Based Education: Evaluating Students' Perspectives Using Transformer", "categories": ["cs.CL"], "comment": "6 pages, 7 figures", "summary": "Outcome-Based Education (OBE) emphasizes the development of specific\ncompetencies through student-centered learning. In this study, we reviewed the\nimportance of OBE and implemented transformer-based models, particularly\nDistilBERT, to analyze an NLP dataset that includes student feedback. Our\nobjective is to assess and improve educational outcomes. Our approach is better\nthan other machine learning models because it uses the transformer's deep\nunderstanding of language context to classify sentiment better, giving better\nresults across a wider range of matrices. Our work directly contributes to\nOBE's goal of achieving measurable outcomes by facilitating the identification\nof patterns in student learning experiences. We have also applied LIME (local\ninterpretable model-agnostic explanations) to make sure that model predictions\nare clear. This gives us understandable information about how key terms affect\nsentiment. Our findings indicate that the combination of transformer models and\nLIME explanations results in a strong and straightforward framework for\nanalyzing student feedback. This aligns more closely with the principles of OBE\nand ensures the improvement of educational practices through data-driven\ninsights.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982DistilBERT\uff09\u5206\u6790\u5b66\u751f\u53cd\u9988\u6570\u636e\uff0c\u7ed3\u5408LIME\u89e3\u91ca\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u57fa\u4e8e\u6210\u679c\u7684\u6559\u80b2\uff08OBE\uff09\u6548\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u5206\u7c7b\u60c5\u611f\u5e76\u8bc6\u522b\u5b66\u4e60\u6a21\u5f0f\uff0c\u4e3a\u6559\u80b2\u5b9e\u8df5\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "motivation": "\u57fa\u4e8e\u6210\u679c\u7684\u6559\u80b2\uff08OBE\uff09\u5f3a\u8c03\u901a\u8fc7\u4ee5\u5b66\u751f\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u57f9\u517b\u7279\u5b9a\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528Transformer\u6a21\u578b\u5206\u6790\u5b66\u751f\u53cd\u9988\uff0c\u8bc4\u4f30\u548c\u6539\u8fdb\u6559\u80b2\u6210\u679c\uff0c\u4ee5\u5b9e\u73b0OBE\u7684\u53ef\u6d4b\u91cf\u76ee\u6807\u3002", "method": "\u91c7\u7528DistilBERT\u7b49Transformer\u6a21\u578b\u5206\u6790\u5b66\u751f\u53cd\u9988\u6570\u636e\u96c6\uff0c\u7ed3\u5408LIME\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff0c\u786e\u4fdd\u6a21\u578b\u900f\u660e\u6027\u3002\u8be5\u65b9\u6cd5\u5229\u7528Transformer\u5bf9\u8bed\u8a00\u4e0a\u4e0b\u6587\u7684\u6df1\u5ea6\u7406\u89e3\uff0c\u4f18\u4e8e\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cTransformer\u6a21\u578b\u4e0eLIME\u89e3\u91ca\u7ed3\u5408\u80fd\u6709\u6548\u5206\u7c7b\u60c5\u611f\u5e76\u8bc6\u522b\u5b66\u4e60\u6a21\u5f0f\uff0c\u4e3a\u6559\u80b2\u5b9e\u8df5\u63d0\u4f9b\u6e05\u6670\u7684\u6570\u636e\u9a71\u52a8\u89c1\u89e3\uff0c\u66f4\u7b26\u5408OBE\u539f\u5219\u3002", "conclusion": "Transformer\u6a21\u578b\u4e0eLIME\u89e3\u91ca\u7684\u7ed3\u5408\u4e3a\u5206\u6790\u5b66\u751f\u53cd\u9988\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u76f4\u89c2\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u6559\u80b2\u5b9e\u8df5\uff0c\u5b9e\u73b0OBE\u7684\u53ef\u6d4b\u91cf\u76ee\u6807\u3002", "paper_title_zh": "\u57fa\u4e8eTransformer\u7684\u6210\u679c\u5bfc\u5411\u6559\u80b2\uff1a\u5b66\u751f\u89c6\u89d2\u8bc4\u4f30", "abstract_zh": "\u6210\u679c\u5bfc\u5411\u6559\u80b2\uff08OBE\uff09\u5f3a\u8c03\u901a\u8fc7\u4ee5\u5b66\u751f\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u57f9\u517b\u7279\u5b9a\u80fd\u529b\u3002\u672c\u7814\u7a76\u56de\u987e\u4e86OBE\u7684\u91cd\u8981\u6027\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982DistilBERT\uff09\u5206\u6790\u5305\u542b\u5b66\u751f\u53cd\u9988\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u6559\u80b2\u6210\u679c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u5229\u7528Transformer\u5bf9\u8bed\u8a00\u4e0a\u4e0b\u6587\u7684\u6df1\u5ea6\u7406\u89e3\uff0c\u66f4\u51c6\u786e\u5730\u5206\u7c7b\u60c5\u611f\uff0c\u5e76\u5728\u66f4\u5e7f\u6cdb\u7684\u6307\u6807\u4e0a\u53d6\u5f97\u66f4\u597d\u7ed3\u679c\u3002\u672c\u7814\u7a76\u76f4\u63a5\u652f\u6301OBE\u7684\u53ef\u6d4b\u91cf\u76ee\u6807\uff0c\u901a\u8fc7\u8bc6\u522b\u5b66\u751f\u5b66\u4e60\u4f53\u9a8c\u4e2d\u7684\u6a21\u5f0f\u3002\u6211\u4eec\u8fd8\u5e94\u7528LIME\uff08\u5c40\u90e8\u53ef\u89e3\u91ca\u6a21\u578b\u65e0\u5173\u89e3\u91ca\uff09\u786e\u4fdd\u6a21\u578b\u9884\u6d4b\u7684\u900f\u660e\u6027\uff0c\u4ece\u800c\u83b7\u5f97\u5173\u4e8e\u5173\u952e\u672f\u8bed\u5982\u4f55\u5f71\u54cd\u60c5\u611f\u7684\u6e05\u6670\u4fe1\u606f\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cTransformer\u6a21\u578b\u4e0eLIME\u89e3\u91ca\u7684\u7ed3\u5408\u4e3a\u5206\u6790\u5b66\u751f\u53cd\u9988\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u76f4\u89c2\u7684\u6846\u67b6\uff0c\u66f4\u7b26\u5408OBE\u539f\u5219\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u89c1\u89e3\u786e\u4fdd\u6559\u80b2\u5b9e\u8df5\u7684\u6539\u8fdb\u3002"}}
{"id": "2506.17231", "pdf": "https://arxiv.org/pdf/2506.17231", "abs": "https://arxiv.org/abs/2506.17231", "authors": ["Xiang Li", "Chong Zhang", "Jia Wang", "Fangyu Wu", "Yushi Li", "Xiaobo Jin"], "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs", "categories": ["cs.CL", "cs.CR"], "comment": "15 pages, 5 figures", "summary": "Attacks on large language models (LLMs) in jailbreaking scenarios raise many\nsecurity and ethical issues. Current jailbreak attack methods face problems\nsuch as low efficiency, high computational cost, and poor cross-model\nadaptability and versatility, which make it difficult to cope with the rapid\ndevelopment of LLM and new defense strategies. Our work proposes an Adversarial\nPrompt Distillation, which combines masked language modeling, reinforcement\nlearning, and dynamic temperature control through a prompt generation and\ndistillation method. It enables small language models (SLMs) to jailbreak\nattacks on mainstream LLMs. The experimental results verify the superiority of\nthe proposed method in terms of attack success rate and harm, and reflect the\nresource efficiency and cross-model adaptability. This research explores the\nfeasibility of distilling the jailbreak ability of LLM to SLM, reveals the\nmodel's vulnerability, and provides a new idea for LLM security research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u5bf9\u6297\u6027\u63d0\u793a\u84b8\u998f\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001\u6e29\u5ea6\u63a7\u5236\uff0c\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u80fd\u591f\u5bf9\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u8d8a\u72f1\u653b\u51fb\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u5371\u5bb3\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u5177\u5907\u8d44\u6e90\u9ad8\u6548\u6027\u548c\u8de8\u6a21\u578b\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8de8\u6a21\u578b\u9002\u5e94\u6027\u548c\u901a\u7528\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9LLM\u7684\u5feb\u901f\u53d1\u5c55\u548c\u65b0\u9632\u5fa1\u7b56\u7565\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06LLM\u7684\u8d8a\u72f1\u80fd\u529b\u84b8\u998f\u5230SLM\u4e2d\uff0c\u63ed\u793a\u6a21\u578b\u6f0f\u6d1e\uff0c\u5e76\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u201c\u5bf9\u6297\u6027\u63d0\u793a\u84b8\u998f\u201d\u65b9\u6cd5\uff0c\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001\u6e29\u5ea6\u63a7\u5236\uff0c\u901a\u8fc7\u63d0\u793a\u751f\u6210\u548c\u84b8\u998f\u6280\u672f\uff0c\u4f7fSLM\u80fd\u591f\u5bf9LLM\u8fdb\u884c\u9ad8\u6548\u7684\u8d8a\u72f1\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u5371\u5bb3\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u5177\u5907\u8d44\u6e90\u9ad8\u6548\u6027\u548c\u8de8\u6a21\u578b\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u5c06LLM\u8d8a\u72f1\u80fd\u529b\u84b8\u998f\u5230SLM\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u6f0f\u6d1e\uff0c\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u901a\u8fc7\u5bf9\u6297\u6027\u63d0\u793a\u84b8\u998f\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5230\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u9690\u853d\u8d8a\u72f1\u653b\u51fb", "abstract_zh": "\u5728\u8d8a\u72f1\u573a\u666f\u4e0b\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u653b\u51fb\u5f15\u53d1\u4e86\u8bb8\u591a\u5b89\u5168\u548c\u4f26\u7406\u95ee\u9898\u3002\u5f53\u524d\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8de8\u6a21\u578b\u9002\u5e94\u6027\u548c\u901a\u7528\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9LLM\u7684\u5feb\u901f\u53d1\u5c55\u548c\u65b0\u9632\u5fa1\u7b56\u7565\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5bf9\u6297\u6027\u63d0\u793a\u84b8\u998f\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001\u6e29\u5ea6\u63a7\u5236\u7684\u63d0\u793a\u751f\u6210\u4e0e\u84b8\u998f\u6280\u672f\uff0c\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u80fd\u591f\u5bf9\u4e3b\u6d41LLM\u8fdb\u884c\u8d8a\u72f1\u653b\u51fb\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u5371\u5bb3\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u4f53\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u6027\u548c\u8de8\u6a21\u578b\u9002\u5e94\u6027\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5c06LLM\u8d8a\u72f1\u80fd\u529b\u84b8\u998f\u5230SLM\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.17286", "pdf": "https://arxiv.org/pdf/2506.17286", "abs": "https://arxiv.org/abs/2506.17286", "authors": ["Luoyang Sun", "Jiwen Jiang", "Cheng Deng", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "title": "GTA: Grouped-head latenT Attention", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "AI": {"tldr": "GTA\u662f\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5171\u4eab\u6ce8\u610f\u529b\u56fe\u548c\u538b\u7f29\u503c\u7f13\u5b58\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u5b58\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662fKV\u7f13\u5b58\u548c\u6ce8\u610f\u529b\u8ba1\u7b97\u968f\u6587\u672c\u957f\u5ea6\u5feb\u901f\u589e\u52a0\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u6709\u9650\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u5197\u4f59\uff0cKV\u7f13\u5b58\u53ef\u538b\u7f29\u4e14\u591a\u5934\u6ce8\u610f\u529b\u56fe\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u8868\u660e\u5927\u91cf\u8ba1\u7b97\u548c\u5b58\u50a8\u662f\u4e0d\u5fc5\u8981\u7684\u3002", "method": "GTA\u5305\u542b\u4e24\u90e8\u5206\uff1a1\uff09\u5171\u4eab\u6ce8\u610f\u529b\u56fe\u673a\u5236\uff0c\u8de8\u591a\u5934\u590d\u7528\u6ce8\u610f\u529b\u5206\u6570\uff0c\u51cf\u5c11\u952e\u7f13\u5b58\u5927\u5c0f\uff1b2\uff09\u975e\u7ebf\u6027\u503c\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u5c06\u503c\u7f13\u5b58\u538b\u7f29\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "GTA\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97FLOPs\u51cf\u5c11\u9ad8\u8fbe62.5%\uff0cKV\u7f13\u5b58\u538b\u7f29\u9ad8\u8fbe70%\uff0c\u540c\u65f6\u907f\u514d\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u7684\u989d\u5916\u5f00\u9500\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\u3002", "conclusion": "GTA\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u90e8\u7f72\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6027\u80fd\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "GTA\uff1a\u5206\u7ec4\u5934\u6f5c\u5728\u6ce8\u610f\u529b", "abstract_zh": "\u6ce8\u610f\u529b\u673a\u5236\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6210\u529f\u7684\u5173\u952e\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5bf9\u6548\u7387\u548c\u6027\u80fd\u4f18\u5316\u63d0\u51fa\u4e86\u6311\u6218\u3002KV\u7f13\u5b58\u548c\u6ce8\u610f\u529b\u8ba1\u7b97\u968f\u6587\u672c\u957f\u5ea6\u5feb\u901f\u589e\u52a0\u6210\u4e3a\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u3002\u6211\u4eec\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u663e\u8457\u5197\u4f59\uff0cKV\u7f13\u5b58\u53ef\u5927\u5e45\u538b\u7f29\u4e14\u591a\u5934\u6ce8\u610f\u529b\u56fe\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u8868\u660e\u5927\u91cf\u8ba1\u7b97\u548c\u5b58\u50a8\u662f\u4e0d\u5fc5\u8981\u7684\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u7ec4\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08GTA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002GTA\u5305\u542b\u4e24\u90e8\u5206\uff1a1\uff09\u5171\u4eab\u6ce8\u610f\u529b\u56fe\u673a\u5236\uff0c\u8de8\u591a\u5934\u590d\u7528\u6ce8\u610f\u529b\u5206\u6570\uff0c\u51cf\u5c11\u952e\u7f13\u5b58\u5927\u5c0f\uff1b2\uff09\u975e\u7ebf\u6027\u503c\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u5c06\u503c\u7f13\u5b58\u538b\u7f29\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002GTA\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97FLOPs\u51cf\u5c11\u9ad8\u8fbe62.5%\uff0cKV\u7f13\u5b58\u538b\u7f29\u9ad8\u8fbe70%\uff0c\u540c\u65f6\u907f\u514d\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u7684\u989d\u5916\u5f00\u9500\uff0c\u4ece\u800c\u63d0\u5347LLM\u90e8\u7f72\u6548\u7387\u3002\u56e0\u6b64\uff0cGTA\u6a21\u578b\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u9884\u586b\u5145\u5f97\u76ca\u4e8e\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0c\u89e3\u7801\u5219\u53d7\u76ca\u4e8e\u66f4\u5c0f\u7684\u7f13\u5b58\u5360\u7528\u3002"}}
{"id": "2506.17294", "pdf": "https://arxiv.org/pdf/2506.17294", "abs": "https://arxiv.org/abs/2506.17294", "authors": ["Qirui Zheng", "Xingbo Wang", "Keyuan Cheng", "Yunlong Lu", "Wenxin Li"], "title": "AI-Generated Game Commentary: A Survey and a Datasheet Repository", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "AI-Generated Game Commentary (AIGGC) has gained increasing attention due to\nits market potential and inherent technical challenges. As a comprehensive\nmultimodal Natural Language Processing (NLP) task, AIGGC imposes substantial\ndemands on language models, including factual accuracy, logical reasoning,\nexpressive text generation, generation speed, and context management. In this\npaper, we introduce a general framework for AIGGC and present a comprehensive\nsurvey of 45 existing game commentary dataset and methods according to key\nchallenges they aim to address in this domain. We further classify and compare\nvarious evaluation metrics commonly used in this domain. To support future\nresearch and benchmarking, we also provide a structured datasheet summarizing\nthe essential attributes of these datasets in appendix, which is meanwhile\npublicly available in an open repository.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u751f\u6210\u6e38\u620f\u89e3\u8bf4\uff08AIGGC\uff09\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u901a\u7528\u6846\u67b6\uff0c\u5206\u7c7b\u6bd4\u8f83\u4e8645\u4e2a\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6570\u636e\u8868\u4ed3\u5e93\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "AI\u751f\u6210\u6e38\u620f\u89e3\u8bf4\uff08AIGGC\uff09\u56e0\u5176\u5e02\u573a\u6f5c\u529b\u4e0e\u6280\u672f\u6311\u6218\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u548c\u6807\u51c6\u5316\u6570\u636e\u96c6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "method": "\u63d0\u51faAIGGC\u7684\u901a\u7528\u6846\u67b6\uff0c\u7cfb\u7edf\u7efc\u8ff045\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u5206\u7c7b\u6bd4\u8f83\u5e38\u7528\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u521b\u5efa\u7ed3\u6784\u5316\u6570\u636e\u8868\u4ed3\u5e93\u3002", "result": "\u603b\u7ed3\u4e86AIGGC\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6570\u636e\u8868\u4ed3\u5e93\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "AIGGC\u662f\u4e00\u4e2a\u590d\u6742\u7684\u591a\u6a21\u6001NLP\u4efb\u52a1\uff0c\u672c\u6587\u7684\u7efc\u8ff0\u4e0e\u6570\u636e\u4ed3\u5e93\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u4e0e\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "paper_title_zh": "AI\u751f\u6210\u6e38\u620f\u89e3\u8bf4\uff1a\u7efc\u8ff0\u4e0e\u6570\u636e\u8868\u4ed3\u5e93", "abstract_zh": "AI\u751f\u6210\u6e38\u620f\u89e3\u8bf4\uff08AIGGC\uff09\u56e0\u5176\u5e02\u573a\u6f5c\u529b\u4e0e\u6280\u672f\u6311\u6218\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u4f5c\u4e3a\u4e00\u79cd\u7efc\u5408\u6027\u7684\u591a\u6a21\u6001\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\uff0cAIGGC\u5bf9\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u9ad8\u8981\u6c42\uff0c\u5305\u62ec\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u903b\u8f91\u63a8\u7406\u3001\u8868\u8fbe\u6027\u6587\u672c\u751f\u6210\u3001\u751f\u6210\u901f\u5ea6\u4e0e\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002\u672c\u6587\u63d0\u51fa\u4e86AIGGC\u7684\u901a\u7528\u6846\u67b6\uff0c\u5e76\u7cfb\u7edf\u7efc\u8ff0\u4e8645\u4e2a\u73b0\u6709\u6e38\u620f\u89e3\u8bf4\u6570\u636e\u96c6\u4e0e\u65b9\u6cd5\uff0c\u6839\u636e\u5176\u89e3\u51b3\u7684\u5173\u952e\u6311\u6218\u8fdb\u884c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5206\u7c7b\u6bd4\u8f83\u4e86\u8be5\u9886\u57df\u5e38\u7528\u7684\u8bc4\u4f30\u6307\u6807\u3002\u4e3a\u652f\u6301\u672a\u6765\u7814\u7a76\u4e0e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6570\u636e\u8868\uff0c\u603b\u7ed3\u4e86\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u5e76\u5c06\u5176\u516c\u5f00\u4e8e\u5f00\u653e\u4ed3\u5e93\u4e2d\u3002"}}
{"id": "2506.17237", "pdf": "https://arxiv.org/pdf/2506.17237", "abs": "https://arxiv.org/abs/2506.17237", "authors": ["Dip Roy"], "title": "Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation", "categories": ["cs.CV"], "comment": null, "summary": "We present a quantitative circuit-level analysis of diffusion models,\nestablishing computational pathways and mechanistic principles underlying image\ngeneration processes. Through systematic intervention experiments across 2,000\nsynthetic and 2,000 CelebA facial images, we discover fundamental algorithmic\ndifferences in how diffusion architectures process synthetic versus\nnaturalistic data distributions. Our investigation reveals that real-world face\nprocessing requires circuits with measurably higher computational complexity\n(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct\nattention specialization patterns with entropy divergence ranging from 0.015 to\n0.166 across denoising timesteps. We identify eight functionally distinct\nattention mechanisms showing specialized computational roles: edge detection\n(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus\n0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).\nIntervention analysis demonstrates critical computational bottlenecks where\ntargeted ablations produce 25.6% to 128.3% performance degradation, providing\ncausal evidence for identified circuit functions. These findings establish\nquantitative foundations for algorithmic understanding and control of\ngenerative model behavior through mechanistic intervention strategies.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9a\u91cf\u7535\u8def\u7ea7\u5206\u6790\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\u673a\u5236\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4eba\u8138\u6570\u636e\u5904\u7406\u7684\u7b97\u6cd5\u5dee\u5f02\uff0c\u5e76\u8bc6\u522b\u4e86\u516b\u79cd\u529f\u80fd\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002\u5e72\u9884\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5173\u952e\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u56e0\u679c\u8bc1\u636e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5b9a\u91cf\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u7535\u8def\u7ea7\u673a\u5236\uff0c\u63ed\u793a\u5176\u5728\u5408\u6210\u4e0e\u81ea\u7136\u6570\u636e\u5206\u5e03\u5904\u7406\u4e2d\u7684\u7b97\u6cd5\u5dee\u5f02\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u884c\u4e3a\u7406\u89e3\u548c\u63a7\u5236\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5bf92,000\u5f20\u5408\u6210\u56fe\u50cf\u548c2,000\u5f20CelebA\u4eba\u8138\u56fe\u50cf\u8fdb\u884c\u7cfb\u7edf\u5e72\u9884\u5b9e\u9a8c\uff0c\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u8def\u5f84\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bc6\u522b\u529f\u80fd\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u89d2\u8272\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u771f\u5b9e\u4eba\u8138\u5904\u7406\u9700\u8981\u66f4\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\uff08\u590d\u6742\u5ea6\u6bd4=1.084\u00b10.008\uff09\uff0c\u5e76\u8bc6\u522b\u4e86\u516b\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u8fb9\u7f18\u68c0\u6d4b\u3001\u7eb9\u7406\u5206\u6790\u7b49\uff09\u3002\u5e72\u9884\u5b9e\u9a8c\u663e\u793a\u5173\u952e\u8ba1\u7b97\u74f6\u9888\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d25.6%\u81f3128.3%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7b97\u6cd5\u7406\u89e3\u548c\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "paper_title_zh": "\u6269\u6563\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff1a\u7535\u8def\u7ea7\u5206\u6790\u4e0e\u56e0\u679c\u9a8c\u8bc1", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u6269\u6563\u6a21\u578b\u7684\u5b9a\u91cf\u7535\u8def\u7ea7\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u7684\u8ba1\u7b97\u8def\u5f84\u548c\u673a\u5236\u539f\u7406\u3002\u901a\u8fc7\u5bf92,000\u5f20\u5408\u6210\u56fe\u50cf\u548c2,000\u5f20CelebA\u4eba\u8138\u56fe\u50cf\u7684\u7cfb\u7edf\u5e72\u9884\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u6269\u6563\u67b6\u6784\u5728\u5904\u7406\u5408\u6210\u4e0e\u81ea\u7136\u6570\u636e\u5206\u5e03\u65f6\u5b58\u5728\u57fa\u7840\u7b97\u6cd5\u5dee\u5f02\u3002\u7814\u7a76\u8868\u660e\uff0c\u771f\u5b9e\u4eba\u8138\u5904\u7406\u9700\u8981\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u66f4\u9ad8\u7684\u7535\u8def\uff08\u590d\u6742\u5ea6\u6bd4=1.084\u00b10.008\uff0cp<0.001\uff09\uff0c\u5e76\u5728\u53bb\u566a\u65f6\u95f4\u6b65\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u4e13\u4e1a\u5316\u6a21\u5f0f\uff08\u71b5\u6563\u5ea6\u8303\u56f4\u4e3a0.015\u81f30.166\uff09\u3002\u6211\u4eec\u8bc6\u522b\u4e86\u516b\u79cd\u529f\u80fd\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5305\u62ec\u8fb9\u7f18\u68c0\u6d4b\uff08\u71b5=3.18\u00b10.12\uff09\u3001\u7eb9\u7406\u5206\u6790\uff08\u71b5=4.16\u00b10.08\uff09\u548c\u8bed\u4e49\u7406\u89e3\uff08\u71b5=2.67\u00b10.15\uff09\u3002\u5e72\u9884\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u8ba1\u7b97\u74f6\u9888\uff0c\u9488\u5bf9\u6027\u6d88\u878d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d25.6%\u81f3128.3%\uff0c\u4e3a\u7535\u8def\u529f\u80fd\u63d0\u4f9b\u4e86\u56e0\u679c\u8bc1\u636e\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u901a\u8fc7\u673a\u5236\u5e72\u9884\u7b56\u7565\u7406\u89e3\u548c\u63a7\u5236\u751f\u6210\u6a21\u578b\u884c\u4e3a\u5960\u5b9a\u4e86\u5b9a\u91cf\u57fa\u7840\u3002"}}
{"id": "2506.17289", "pdf": "https://arxiv.org/pdf/2506.17289", "abs": "https://arxiv.org/abs/2506.17289", "authors": ["Rahul Raja", "Arpita Vats"], "title": "Evaluating Generalization and Representation Stability in Small LMs via Prompting", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at ICML", "summary": "We investigate the generalization capabilities of small language models under\ntwo popular adaptation paradigms: few-shot prompting and supervised\nfine-tuning. While prompting is often favored for its parameter efficiency and\nflexibility, it remains unclear how robust this approach is in low-resource\nsettings and under distributional shifts. This paper presents a comparative\nstudy of prompting and fine-tuning across task formats, prompt styles, and\nmodel scales, with a focus on their behavior in both in-distribution and\nout-of-distribution (OOD) settings.\n  Beyond accuracy, we analyze the internal representations learned by each\napproach to assess the stability and abstraction of task-specific features. Our\nfindings highlight critical differences in how small models internalize and\ngeneralize knowledge under different adaptation strategies. This work offers\npractical guidance for model selection in low-data regimes and contributes\nempirical insight into the ongoing debate over prompting versus fine-tuning.\nCode for the experiments is available at the following", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u63d0\u793a\u548c\u76d1\u7763\u5fae\u8c03\u4e24\u79cd\u9002\u5e94\u8303\u5f0f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5b83\u4eec\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\u53ca\u5185\u90e8\u8868\u5f81\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u5c11\u6837\u672c\u63d0\u793a\u548c\u76d1\u7763\u5fae\u8c03\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8868\u5f81\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u548c\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u4efb\u52a1\u683c\u5f0f\u3001\u63d0\u793a\u98ce\u683c\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u63d0\u793a\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u7684\u7a33\u5b9a\u6027\u548c\u62bd\u8c61\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u793a\u548c\u5fae\u8c03\u5728\u5c0f\u6a21\u578b\u4e2d\u5bf9\u77e5\u8bc6\u7684\u5185\u90e8\u5316\u548c\u6cdb\u5316\u65b9\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63d0\u793a\u5728\u53c2\u6570\u6548\u7387\u548c\u7075\u6d3b\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u548c\u5206\u5e03\u53d8\u5316\u4e0b\u53ef\u80fd\u4e0d\u591f\u7a33\u5065\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u4e3a\u63d0\u793a\u4e0e\u5fae\u8c03\u7684\u4e89\u8bba\u8d21\u732e\u4e86\u5b9e\u8bc1\u89c1\u89e3\u3002", "paper_title_zh": "\u901a\u8fc7\u63d0\u793a\u8bc4\u4f30\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8868\u5f81\u7a33\u5b9a\u6027", "abstract_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u79cd\u6d41\u884c\u7684\u9002\u5e94\u8303\u5f0f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff1a\u5c11\u6837\u672c\u63d0\u793a\u548c\u76d1\u7763\u5fae\u8c03\u3002\u5c3d\u7ba1\u63d0\u793a\u56e0\u5176\u53c2\u6570\u6548\u7387\u548c\u7075\u6d3b\u6027\u800c\u5907\u53d7\u9752\u7750\uff0c\u4f46\u5176\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u548c\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u7a33\u5065\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u6bd4\u8f83\u4e86\u63d0\u793a\u548c\u5fae\u8c03\u5728\u4e0d\u540c\u4efb\u52a1\u683c\u5f0f\u3001\u63d0\u793a\u98ce\u683c\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u5b83\u4eec\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\u7684\u884c\u4e3a\u3002\n  \u9664\u4e86\u51c6\u786e\u6027\u5916\uff0c\u6211\u4eec\u8fd8\u5206\u6790\u4e86\u6bcf\u79cd\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u5185\u90e8\u8868\u5f81\uff0c\u4ee5\u8bc4\u4f30\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u7684\u7a33\u5b9a\u6027\u548c\u62bd\u8c61\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5c0f\u6a21\u578b\u5728\u4e0d\u540c\u9002\u5e94\u7b56\u7565\u4e0b\u5982\u4f55\u5185\u90e8\u5316\u548c\u6cdb\u5316\u77e5\u8bc6\u7684\u5173\u952e\u5dee\u5f02\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u4e3a\u63d0\u793a\u4e0e\u5fae\u8c03\u7684\u4e89\u8bba\u8d21\u732e\u4e86\u5b9e\u8bc1\u89c1\u89e3\u3002\u5b9e\u9a8c\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\u3002"}}
{"id": "2506.17296", "pdf": "https://arxiv.org/pdf/2506.17296", "abs": "https://arxiv.org/abs/2506.17296", "authors": ["Darius Foodeei", "Simin Fan", "Martin Jaggi"], "title": "Semantic uncertainty in advanced decoding methods for LLM generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates semantic uncertainty in large language model (LLM)\noutputs across different decoding methods, focusing on emerging techniques like\nspeculative sampling and chain-of-thought (CoT) decoding. Through experiments\non question answering, summarization, and code generation tasks, we analyze how\ndifferent decoding strategies affect both the diversity and reliability of\nmodel outputs. Our findings reveal that while CoT decoding demonstrates higher\nsemantic diversity, it maintains lower predictive entropy, suggesting that\nstructured exploration can lead to more confident and accurate outputs. This is\nevidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower\nalignment with reference solutions. For summarization tasks, speculative\nsampling proved particularly effective, achieving superior ROUGE scores while\nmaintaining moderate semantic diversity. Our results challenge conventional\nassumptions about trade-offs between diversity and accuracy in language model\noutputs, demonstrating that properly structured decoding methods can increase\nsemantic exploration while maintaining or improving output quality. These\nfindings have significant implications for deploying language models in\npractical applications where both reliability and diverse solution generation\nare crucial.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u540c\u89e3\u7801\u65b9\u6cd5\u4e0b\u7684\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u89e3\u7801\u65b9\u6cd5\uff08\u5982\u94fe\u5f0f\u601d\u7ef4\u89e3\u7801\u548c\u63a8\u6d4b\u91c7\u6837\uff09\u80fd\u63d0\u5347\u8bed\u4e49\u591a\u6837\u6027\u5e76\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e0d\u540c\u89e3\u7801\u65b9\u6cd5\u5bf9LLM\u8f93\u51fa\u7684\u8bed\u4e49\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u591a\u6837\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u95ee\u7b54\u3001\u6458\u8981\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u94fe\u5f0f\u601d\u7ef4\u89e3\u7801\uff08CoT\uff09\u548c\u63a8\u6d4b\u91c7\u6837\u7b49\u65b0\u5174\u89e3\u7801\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cCoT\u89e3\u7801\u5177\u6709\u66f4\u9ad8\u7684\u8bed\u4e49\u591a\u6837\u6027\u4e14\u9884\u6d4b\u71b5\u8f83\u4f4e\uff0c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2dPass@2\u7387\u63d0\u534748.8%\uff1b\u63a8\u6d4b\u91c7\u6837\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cROUGE\u5f97\u5206\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u6784\u5316\u89e3\u7801\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u63a2\u7d22\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\u7684\u573a\u666f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2d\u9ad8\u7ea7\u89e3\u7801\u65b9\u6cd5\u7684\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u540c\u89e3\u7801\u65b9\u6cd5\u4e0b\u7684\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u63a8\u6d4b\u91c7\u6837\u548c\u94fe\u5f0f\u601d\u7ef4\u89e3\u7801\uff08CoT\uff09\u7b49\u65b0\u5174\u6280\u672f\u3002\u901a\u8fc7\u95ee\u7b54\u3001\u6458\u8981\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u89e3\u7801\u7b56\u7565\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1CoT\u89e3\u7801\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u4f46\u5176\u9884\u6d4b\u71b5\u8f83\u4f4e\uff0c\u8868\u660e\u7ed3\u6784\u5316\u63a2\u7d22\u53ef\u4ee5\u5e26\u6765\u66f4\u81ea\u4fe1\u548c\u51c6\u786e\u7684\u8f93\u51fa\u3002\u8fd9\u4e00\u70b9\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5c24\u4e3a\u660e\u663e\uff0cPass@2\u7387\u63d0\u5347\u4e8648.8%\uff0c\u5c3d\u7ba1\u4e0e\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u7684\u5339\u914d\u5ea6\u8f83\u4f4e\u3002\u5728\u6458\u8981\u4efb\u52a1\u4e2d\uff0c\u63a8\u6d4b\u91c7\u6837\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684ROUGE\u5206\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9002\u5ea6\u7684\u8bed\u4e49\u591a\u6837\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u6311\u6218\u4e86\u5173\u4e8e\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u591a\u6837\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u6743\u8861\u7684\u4f20\u7edf\u5047\u8bbe\uff0c\u8868\u660e\u7ed3\u6784\u5316\u7684\u89e3\u7801\u65b9\u6cd5\u53ef\u4ee5\u589e\u52a0\u8bed\u4e49\u63a2\u7d22\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u8bed\u8a00\u6a21\u578b\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u53ef\u9760\u6027\u548c\u591a\u6837\u5316\u89e3\u751f\u6210\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2506.17290", "pdf": "https://arxiv.org/pdf/2506.17290", "abs": "https://arxiv.org/abs/2506.17290", "authors": ["Yuqi Li", "Junhao Dong", "Zeyu Dong", "Chuanguang Yang", "Zhulin An", "Yongjun Xu"], "title": "SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation", "categories": ["cs.CV"], "comment": "13 pages", "summary": "3D point cloud segmentation faces practical challenges due to the\ncomputational complexity and deployment limitations of large-scale\ntransformer-based models. To address this, we propose a novel Structure- and\nRelation-aware Knowledge Distillation framework, named SRKD, that transfers\nrich geometric and semantic knowledge from a large frozen teacher model (>100M)\nto a lightweight student model (<15M). Specifically, we propose an affinity\nmatrix-based relation alignment module, which distills structural dependencies\nfrom the teacher to the student through point-wise similarity matching,\nenhancing the student's capability to learn contextual interactions. Meanwhile,\nwe introduce a cross-sample mini-batch construction strategy that enables the\nstudent to perceive stable and generalized geometric structure. This aligns\nacross diverse point cloud instances of the teacher, rather than within a\nsingle sample. Additionally, KL divergence is applied to align semantic\ndistributions, and ground-truth supervision further reinforces accurate\nsegmentation. Our method achieves state of the art performance with\nsignificantly reduced model complexity, demonstrating its effectiveness and\nefficiency in real-world deployment scenarios. Our Code is available at\nhttps://github.com/itsnotacie/SRKD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRKD\u7684\u7ed3\u6784\u548c\u5173\u7cfb\u611f\u77e5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4ece\u5927\u578b\u6559\u5e08\u6a21\u578b\u5411\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4f20\u9012\u51e0\u4f55\u548c\u8bed\u4e49\u77e5\u8bc6\uff0c\u9ad8\u6548\u89e3\u51b33D\u70b9\u4e91\u5206\u5272\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u90e8\u7f72\u9650\u5236\u95ee\u9898\u3002", "motivation": "3D\u70b9\u4e91\u5206\u5272\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5927\u89c4\u6a21\u57fa\u4e8eTransformer\u6a21\u578b\u90e8\u7f72\u9650\u5236\u7684\u5b9e\u9645\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u538b\u7f29\u6a21\u578b\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "SRKD\u6846\u67b6\u901a\u8fc7\u4eb2\u548c\u77e9\u9635\u5173\u7cfb\u5bf9\u9f50\u6a21\u5757\u4ece\u6559\u5e08\u6a21\u578b\u5411\u5b66\u751f\u6a21\u578b\u4f20\u9012\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u91c7\u7528\u8de8\u6837\u672c\u5c0f\u6279\u91cf\u6784\u5efa\u7b56\u7565\u589e\u5f3a\u5b66\u751f\u5bf9\u51e0\u4f55\u7ed3\u6784\u7684\u611f\u77e5\uff0c\u5e76\u7ed3\u5408KL\u6563\u5ea6\u5bf9\u9f50\u8bed\u4e49\u5206\u5e03\u548c\u771f\u5b9e\u76d1\u7763\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "SRKD\u901a\u8fc7\u7ed3\u6784\u548c\u5173\u7cfb\u611f\u77e5\u77e5\u8bc6\u84b8\u998f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea73D\u70b9\u4e91\u5206\u5272\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u548c\u90e8\u7f72\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "paper_title_zh": "SRKD\uff1a\u901a\u8fc7\u7ed3\u6784\u548c\u5173\u7cfb\u611f\u77e5\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u76843D\u70b9\u4e91\u5206\u5272", "abstract_zh": "3D\u70b9\u4e91\u5206\u5272\u7531\u4e8e\u5927\u89c4\u6a21\u57fa\u4e8eTransformer\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u90e8\u7f72\u9650\u5236\u9762\u4e34\u5b9e\u9645\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRKD\u7684\u65b0\u578b\u7ed3\u6784\u548c\u5173\u7cfb\u611f\u77e5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u77e5\u8bc6\u4ece\u5927\u578b\u51bb\u7ed3\u6559\u5e08\u6a21\u578b\uff08>100M\uff09\u4f20\u9012\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff08<15M\uff09\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u4eb2\u548c\u77e9\u9635\u7684\u5173\u7cfb\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u70b9\u5bf9\u70b9\u76f8\u4f3c\u6027\u5339\u914d\u4ece\u6559\u5e08\u6a21\u578b\u5411\u5b66\u751f\u6a21\u578b\u84b8\u998f\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u589e\u5f3a\u5b66\u751f\u5b66\u4e60\u4e0a\u4e0b\u6587\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8de8\u6837\u672c\u5c0f\u6279\u91cf\u6784\u5efa\u7b56\u7565\uff0c\u4f7f\u5b66\u751f\u80fd\u591f\u611f\u77e5\u7a33\u5b9a\u4e14\u6cdb\u5316\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd9\u79cd\u5bf9\u9f50\u662f\u5728\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u540c\u70b9\u4e91\u5b9e\u4f8b\u4e4b\u95f4\u8fdb\u884c\u7684\uff0c\u800c\u975e\u5355\u4e2a\u6837\u672c\u5185\u3002\u6b64\u5916\uff0cKL\u6563\u5ea6\u7528\u4e8e\u5bf9\u9f50\u8bed\u4e49\u5206\u5e03\uff0c\u771f\u5b9e\u76d1\u7763\u8fdb\u4e00\u6b65\u5f3a\u5316\u4e86\u51c6\u786e\u5206\u5272\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728https://github.com/itsnotacie/SRKD\u83b7\u53d6\u3002"}}
{"id": "2506.17300", "pdf": "https://arxiv.org/pdf/2506.17300", "abs": "https://arxiv.org/abs/2506.17300", "authors": ["Daniel T. Chang"], "title": "Individual Causal Inference with Structural Causal Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Individual causal inference (ICI) uses causal inference methods to understand\nand predict the effects of interventions on individuals, considering their\nspecific characteristics / facts. It aims to estimate individual causal effect\n(ICE), which varies across individuals. Estimating ICE can be challenging due\nto the limited data available for individuals, and the fact that most causal\ninference methods are population-based. Structural Causal Model (SCM) is\nfundamentally population-based. Therefore, causal discovery (structural\nlearning and parameter learning), association queries and intervention queries\nare all naturally population-based. However, exogenous variables (U) in SCM can\nencode individual variations and thus provide the mechanism for individualized\npopulation per specific individual characteristics / facts. Based on this, we\npropose ICI with SCM as a \"rung 3\" causal inference, because it involves\n\"imagining\" what would be the causal effect of a hypothetical intervention on\nan individual, given the individual's observed characteristics / facts.\nSpecifically, we propose the indiv-operator, indiv(W), to formalize/represent\nthe population individualization process, and the individual causal query, P(Y\n| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI\nwith SCM is inference on individual alternatives (possible), not individual\ncounterfactuals (non-actual).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u7684\u4e2a\u4f53\u56e0\u679c\u63a8\u65ad\uff08ICI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e2a\u4f53\u5316\u64cd\u4f5c\u7b26\u548c\u4e2a\u4f53\u56e0\u679c\u67e5\u8be2\uff0c\u5b9e\u73b0\u5bf9\u4e2a\u4f53\u5e72\u9884\u6548\u679c\u7684\u4f30\u8ba1\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u7fa4\u4f53\u56e0\u679c\u63a8\u65ad\u5728\u4e2a\u4f53\u5c42\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u7fa4\u4f53\u6570\u636e\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e2a\u4f53\u5c42\u9762\u3002\u4e2a\u4f53\u56e0\u679c\u63a8\u65ad\uff08ICI\uff09\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u8003\u8651\u4e2a\u4f53\u7279\u5f81\uff0c\u4f30\u8ba1\u4e2a\u4f53\u56e0\u679c\u6548\u5e94\uff08ICE\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4e2a\u4f53\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b58\u5728\u6311\u6218\u3002SCM\u867d\u4e3a\u7fa4\u4f53\u6a21\u578b\uff0c\u4f46\u5176\u5916\u751f\u53d8\u91cf\u53ef\u7f16\u7801\u4e2a\u4f53\u5dee\u5f02\uff0c\u4e3a\u4e2a\u4f53\u5316\u63a8\u65ad\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSCM\u7684ICI\u65b9\u6cd5\uff0c\u5f15\u5165\u4e2a\u4f53\u5316\u64cd\u4f5c\u7b26indiv(W)\u548c\u4e2a\u4f53\u56e0\u679c\u67e5\u8be2P(Y | indiv(W), do(X), Z)\uff0c\u5c06\u7fa4\u4f53\u6a21\u578b\u4e2a\u4f53\u5316\uff0c\u4ece\u800c\u4f30\u8ba1\u4e2a\u4f53\u5e72\u9884\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5916\u751f\u53d8\u91cf\u6355\u6349\u4e2a\u4f53\u5dee\u5f02\uff0c\u5b9e\u73b0\u4e86\u4e2a\u4f53\u5c42\u9762\u7684\u56e0\u679c\u63a8\u65ad\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eSCM\u7684ICI\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u4e2a\u4f53\u56e0\u679c\u6548\u5e94\uff0c\u4e14\u5176\u63a8\u65ad\u5bf9\u8c61\u4e3a\u4e2a\u4f53\u53ef\u80fd\u7684\u66ff\u4ee3\u60c5\u51b5\uff0c\u800c\u975e\u53cd\u4e8b\u5b9e\u7684\u975e\u5b9e\u9645\u72b6\u6001\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684ICI\u65b9\u6cd5\u6269\u5c55\u4e86SCM\u5728\u4e2a\u4f53\u5c42\u9762\u7684\u5e94\u7528\uff0c\u4e3a\u4e2a\u4f53\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u7fa4\u4f53\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "paper_title_zh": "\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u4e2a\u4f53\u56e0\u679c\u63a8\u65ad", "abstract_zh": "\u4e2a\u4f53\u56e0\u679c\u63a8\u65ad\uff08ICI\uff09\u5229\u7528\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e2a\u4f53\u7279\u5f81/\u4e8b\u5b9e\uff0c\u7406\u89e3\u548c\u9884\u6d4b\u5e72\u9884\u5bf9\u4e2a\u4f53\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u4f30\u8ba1\u4e2a\u4f53\u56e0\u679c\u6548\u5e94\uff08ICE\uff09\u3002\u7531\u4e8e\u4e2a\u4f53\u6570\u636e\u6709\u9650\u4e14\u4f20\u7edf\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u591a\u4e3a\u7fa4\u4f53\u6027\uff0c\u4f30\u8ba1ICE\u5177\u6709\u6311\u6218\u6027\u3002\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u672c\u8d28\u4e0a\u662f\u7fa4\u4f53\u6a21\u578b\uff0c\u5176\u56e0\u679c\u53d1\u73b0\u3001\u5173\u8054\u67e5\u8be2\u548c\u5e72\u9884\u67e5\u8be2\u5747\u4e3a\u7fa4\u4f53\u6027\u3002\u7136\u800c\uff0cSCM\u4e2d\u7684\u5916\u751f\u53d8\u91cf\uff08U\uff09\u53ef\u7f16\u7801\u4e2a\u4f53\u5dee\u5f02\uff0c\u4ece\u800c\u4e3a\u57fa\u4e8e\u7279\u5b9a\u4e2a\u4f53\u7279\u5f81/\u4e8b\u5b9e\u7684\u4e2a\u4f53\u5316\u7fa4\u4f53\u63d0\u4f9b\u673a\u5236\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u5c06SCM\u7528\u4e8eICI\u4f5c\u4e3a\u201c\u7b2c\u4e09\u7ea7\u201d\u56e0\u679c\u63a8\u65ad\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u201c\u60f3\u8c61\u201d\u5728\u7ed9\u5b9a\u4e2a\u4f53\u89c2\u5bdf\u7279\u5f81/\u4e8b\u5b9e\u7684\u60c5\u51b5\u4e0b\uff0c\u5047\u8bbe\u5e72\u9884\u5bf9\u4e2a\u4f53\u7684\u56e0\u679c\u6548\u5e94\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51faindiv(W)\u64cd\u4f5c\u7b26\u5f62\u5f0f\u5316/\u8868\u793a\u7fa4\u4f53\u4e2a\u4f53\u5316\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u4e2a\u4f53\u56e0\u679c\u67e5\u8be2P(Y | indiv(W), do(X), Z)\u5f62\u5f0f\u5316/\u8868\u793aICI\u3002\u6211\u4eec\u8bc1\u660e\u5e76\u8bba\u8bc1\uff0c\u57fa\u4e8eSCM\u7684ICI\u662f\u5bf9\u4e2a\u4f53\u66ff\u4ee3\uff08\u53ef\u80fd\uff09\u7684\u63a8\u65ad\uff0c\u800c\u975e\u4e2a\u4f53\u53cd\u4e8b\u5b9e\uff08\u975e\u5b9e\u9645\uff09\u3002"}}
{"id": "2506.17298", "pdf": "https://arxiv.org/pdf/2506.17298", "abs": "https://arxiv.org/abs/2506.17298", "authors": ["Inception Labs", "Samar Khanna", "Siddhant Kharbanda", "Shufan Li", "Harshit Varma", "Eric Wang", "Sawyer Birnbaum", "Ziyang Luo", "Yanis Miraoui", "Akash Palrecha", "Stefano Ermon", "Aditya Grover", "Volodymyr Kuleshov"], "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "AI": {"tldr": "Mercury\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6280\u672f\u7684\u8d85\u5feb\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u7f16\u7801\u5e94\u7528\u8bbe\u8ba1\uff0c\u63d0\u4f9bMini\u548cSmall\u4e24\u79cd\u7248\u672c\uff0c\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5747\u8fbe\u5230\u884c\u4e1a\u9886\u5148\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cMercury\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6280\u672f\u5b9e\u73b0\u8d85\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u6a21\u578b\uff0c\u6ee1\u8db3\u5546\u4e1a\u89c4\u6a21\u7684\u9700\u6c42\u3002", "method": "Mercury\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u91c7\u7528\u6269\u6563\u6280\u672f\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u4ee4\u724c\uff0c\u8bad\u7ec3\u51fa\u4e13\u4e3a\u7f16\u7801\u8bbe\u8ba1\u7684\u6a21\u578bMercury Coder\uff0c\u63d0\u4f9bMini\u548cSmall\u4e24\u79cd\u7248\u672c\u3002", "result": "Mercury Coder\u5728NVIDIA H100 GPU\u4e0a\u5206\u522b\u8fbe\u52301109\u548c737\u4ee4\u724c/\u79d2\u7684\u541e\u5410\u91cf\uff0c\u901f\u5ea6\u6bd4\u524d\u6cbf\u6a21\u578b\u5feb10\u500d\uff0c\u540c\u65f6\u5728Copilot Arena\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "Mercury\u901a\u8fc7\u6269\u6563\u6280\u672f\u5b9e\u73b0\u4e86\u901f\u5ea6\u548c\u6027\u80fd\u7684\u7a81\u7834\uff0c\u4e3a\u7f16\u7801\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u901a\u8fc7\u516c\u5f00API\u548c\u514d\u8d39\u5e73\u53f0\u5f00\u653e\u4f7f\u7528\u3002", "paper_title_zh": "Mercury\uff1a\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5feb\u901f\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86Mercury\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6280\u672f\u7684\u65b0\u4e00\u4ee3\u5546\u4e1a\u89c4\u6a21\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u8fc7Transformer\u67b6\u6784\u53c2\u6570\u5316\uff0c\u5e76\u8bad\u7ec3\u4ee5\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u4ee4\u724c\u3002\u672c\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u4e13\u4e3a\u7f16\u7801\u5e94\u7528\u8bbe\u8ba1\u7684Mercury Coder\uff0c\u76ee\u524d\u63d0\u4f9bMini\u548cSmall\u4e24\u79cd\u7248\u672c\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u901f\u5ea6\u4e0e\u8d28\u91cf\u7684\u524d\u6cbf\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u7684\u884c\u4e1a\u6807\u51c6\u3002\u6839\u636eArtificial Analysis\u7684\u72ec\u7acb\u8bc4\u4f30\uff0cMercury Coder Mini\u548cSmall\u5728NVIDIA H100 GPU\u4e0a\u5206\u522b\u5b9e\u73b0\u4e861109\u548c737\u4ee4\u724c/\u79d2\u7684\u541e\u5410\u91cf\uff0c\u901f\u5ea6\u6bd4\u4f18\u5316\u540e\u7684\u524d\u6cbf\u6a21\u578b\u5feb10\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u8d28\u91cf\u3002\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u591a\u79cd\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u7684\u7ed3\u679c\uff0c\u6db5\u76d6\u591a\u8bed\u8a00\u548c\u7528\u4f8b\uff0c\u5e76\u901a\u8fc7Copilot Arena\u7684\u5f00\u53d1\u8005\u5b9e\u9645\u9a8c\u8bc1\uff0c\u8be5\u6a21\u578b\u76ee\u524d\u5728\u8d28\u91cf\u4e0a\u6392\u540d\u7b2c\u4e8c\uff0c\u4e14\u662f\u6574\u4f53\u6700\u5feb\u7684\u6a21\u578b\u3002\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u516c\u5171API\uff08https://platform.inceptionlabs.ai/\uff09\u548c\u514d\u8d39\u5e73\u53f0\uff08https://chat.inceptionlabs.ai\uff09\u3002"}}
{"id": "2506.17302", "pdf": "https://arxiv.org/pdf/2506.17302", "abs": "https://arxiv.org/abs/2506.17302", "authors": ["Yijun Lin", "Theresa Chen", "Colby Brungard", "Grunwald Sabine", "Sue Ives", "Matt Macander", "Timm Nawrocki", "Yao-Yi Chiang", "Nic Jelinski"], "title": "Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, Submitted to SIGSPATIAL 2025", "summary": "Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and\nlocalized simulations, remains a critical yet underdeveloped task, despite the\nregion's ecological importance and extensive permafrost coverage. As permafrost\nthaw accelerates due to climate change, it threatens infrastructure stability\nand key ecosystem services, such as soil carbon storage. High-resolution soil\nmaps are essential for characterizing permafrost distribution, identifying\nvulnerable areas, and informing adaptation strategies. We present MISO, a\nvision-based machine learning (ML) model to produce statewide fine-scale soil\nmaps for near-surface permafrost and soil taxonomy. The model integrates a\ngeospatial foundation model for visual feature extraction, implicit neural\nrepresentations for continuous spatial prediction, and contrastive learning for\nmultimodal alignment and geo-location awareness. We compare MISO with Random\nForest (RF), a traditional ML model that has been widely used in soil mapping\napplications. Spatial cross-validation and regional analysis across Permafrost\nZones and Major Land Resource Areas (MLRAs) show that MISO generalizes better\nto remote, unseen locations and achieves higher recall than RF, which is\ncritical for monitoring permafrost thaw and related environmental processes.\nThese findings demonstrate the potential of advanced ML approaches for\nfine-scale soil mapping and provide practical guidance for future soil sampling\nand infrastructure planning in permafrost-affected landscapes. The project will\nbe released at https://github.com/knowledge-computing/Peatland-permafrost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578bMISO\uff0c\u7528\u4e8e\u751f\u6210\u963f\u62c9\u65af\u52a0\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u5730\u56fe\uff0c\u4ee5\u76d1\u6d4b\u8fd1\u5730\u8868\u6c38\u4e45\u51bb\u571f\u548c\u571f\u58e4\u5206\u7c7b\u3002\u76f8\u6bd4\u4f20\u7edf\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0cMISO\u5728\u672a\u89c1\u8fc7\u533a\u57df\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ec\u56de\u7387\u66f4\u9ad8\u3002", "motivation": "\u963f\u62c9\u65af\u52a0\u7684\u571f\u58e4\u5730\u56fe\u7ed8\u5236\u5bf9\u751f\u6001\u4fdd\u62a4\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b9e\u5730\u8c03\u67e5\u548c\u5c40\u90e8\u6a21\u62df\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u968f\u7740\u6c14\u5019\u53d8\u5316\u52a0\u901f\u6c38\u4e45\u51bb\u571f\u878d\u5316\uff0c\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u5730\u56fe\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002", "method": "MISO\u6a21\u578b\u7ed3\u5408\u4e86\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\uff09\u3001\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08\u8fde\u7eed\u7a7a\u95f4\u9884\u6d4b\uff09\u548c\u5bf9\u6bd4\u5b66\u4e60\uff08\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u5730\u7406\u5b9a\u4f4d\u611f\u77e5\uff09\uff0c\u5e76\u4e0e\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u548c\u533a\u57df\u5206\u6790\u8868\u660e\uff0cMISO\u5728\u672a\u89c1\u8fc7\u533a\u57df\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ec\u56de\u7387\u4f18\u4e8e\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6c38\u4e45\u51bb\u571f\u533a\u548c\u4e3b\u8981\u571f\u5730\u8d44\u6e90\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MISO\u5c55\u793a\u4e86\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u5730\u56fe\u7ed8\u5236\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6c38\u4e45\u51bb\u571f\u533a\u7684\u571f\u58e4\u91c7\u6837\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u7684\u963f\u62c9\u65af\u52a0\u7cbe\u7ec6\u5c3a\u5ea6\u571f\u58e4\u5236\u56fe", "abstract_zh": "\u963f\u62c9\u65af\u52a0\u7684\u7cbe\u7ec6\u5c3a\u5ea6\u571f\u58e4\u5236\u56fe\u4f20\u7edf\u4e0a\u4f9d\u8d56\u5b9e\u5730\u8c03\u67e5\u548c\u5c40\u90e8\u6a21\u62df\uff0c\u5c3d\u7ba1\u8be5\u5730\u533a\u751f\u6001\u91cd\u8981\u4e14\u6c38\u4e45\u51bb\u571f\u8986\u76d6\u5e7f\u6cdb\uff0c\u4f46\u8fd9\u4e00\u4efb\u52a1\u4ecd\u672a\u5f97\u5230\u5145\u5206\u53d1\u5c55\u3002\u968f\u7740\u6c14\u5019\u53d8\u5316\u52a0\u901f\u6c38\u4e45\u51bb\u571f\u878d\u5316\uff0c\u5176\u5bf9\u57fa\u7840\u8bbe\u65bd\u7a33\u5b9a\u6027\u548c\u571f\u58e4\u78b3\u50a8\u5b58\u7b49\u5173\u952e\u751f\u6001\u7cfb\u7edf\u670d\u52a1\u6784\u6210\u5a01\u80c1\u3002\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u5730\u56fe\u5bf9\u4e8e\u8868\u5f81\u6c38\u4e45\u51bb\u571f\u5206\u5e03\u3001\u8bc6\u522b\u8106\u5f31\u533a\u57df\u548c\u5236\u5b9a\u9002\u5e94\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u4e86MISO\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5168\u5dde\u8303\u56f4\u7684\u8fd1\u5730\u8868\u6c38\u4e45\u51bb\u571f\u548c\u571f\u58e4\u5206\u7c7b\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u5730\u56fe\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\uff09\u3001\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08\u8fde\u7eed\u7a7a\u95f4\u9884\u6d4b\uff09\u548c\u5bf9\u6bd4\u5b66\u4e60\uff08\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u5730\u7406\u5b9a\u4f4d\u611f\u77e5\uff09\u3002\u6211\u4eec\u5c06MISO\u4e0e\u968f\u673a\u68ee\u6797\uff08RF\uff09\u8fd9\u4e00\u4f20\u7edf\u571f\u58e4\u5236\u56fe\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u548c\u8de8\u6c38\u4e45\u51bb\u571f\u533a\u53ca\u4e3b\u8981\u571f\u5730\u8d44\u6e90\u533a\u57df\uff08MLRAs\uff09\u7684\u533a\u57df\u5206\u6790\u8868\u660e\uff0cMISO\u5728\u672a\u89c1\u8fc7\u533a\u57df\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ec\u56de\u7387\u4f18\u4e8eRF\uff0c\u8fd9\u5bf9\u76d1\u6d4b\u6c38\u4e45\u51bb\u571f\u878d\u5316\u53ca\u76f8\u5173\u73af\u5883\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e9b\u53d1\u73b0\u5c55\u793a\u4e86\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u7cbe\u7ec6\u5c3a\u5ea6\u571f\u58e4\u5236\u56fe\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u6c38\u4e45\u51bb\u571f\u5f71\u54cd\u533a\u57df\u7684\u672a\u6765\u571f\u58e4\u91c7\u6837\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002\u9879\u76ee\u5c06\u5728https://github.com/knowledge-computing/Peatland-permafrost\u53d1\u5e03\u3002"}}
{"id": "2506.17434", "pdf": "https://arxiv.org/pdf/2506.17434", "abs": "https://arxiv.org/abs/2506.17434", "authors": ["Sydney Levine", "Matija Franklin", "Tan Zhi-Xuan", "Secil Yanik Guyot", "Lionel Wong", "Daniel Kilov", "Yejin Choi", "Joshua B. Tenenbaum", "Noah Goodman", "Seth Lazar", "Iason Gabriel"], "title": "Resource Rational Contractualism Should Guide AI Alignment", "categories": ["cs.AI"], "comment": "24 pages, 10 figures", "summary": "AI systems will soon have to navigate human environments and make decisions\nthat affect people and other AI agents whose goals and values diverge.\nContractualist alignment proposes grounding those decisions in agreements that\ndiverse stakeholders would endorse under the right conditions, yet securing\nsuch agreement at scale remains costly and slow -- even for advanced AI. We\ntherefore propose Resource-Rational Contractualism (RRC): a framework where AI\nsystems approximate the agreements rational parties would form by drawing on a\ntoolbox of normatively-grounded, cognitively-inspired heuristics that trade\neffort for accuracy. An RRC-aligned agent would not only operate efficiently,\nbut also be equipped to dynamically adapt to and interpret the ever-changing\nhuman social world.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8d44\u6e90\u7406\u6027\u5951\u7ea6\u4e3b\u4e49\uff08RRC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u9ad8\u6548\u6a21\u62df\u591a\u65b9\u7406\u6027\u534f\u8bae\uff0c\u6307\u5bfcAI\u7cfb\u7edf\u5728\u590d\u6742\u4eba\u7c7b\u73af\u5883\u4e2d\u52a8\u6001\u9002\u5e94\u548c\u51b3\u7b56\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u51b3\u7b56\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5982\u4f55\u8ba9AI\u5728\u76ee\u6807\u548c\u4ef7\u503c\u89c2\u4e0d\u540c\u7684\u591a\u65b9\u4e4b\u95f4\u8fbe\u6210\u5171\u8bc6\u6210\u4e3a\u6311\u6218\u3002\u4f20\u7edf\u5951\u7ea6\u4e3b\u4e49\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8d44\u6e90\u7406\u6027\u5951\u7ea6\u4e3b\u4e49\uff08RRC\uff09\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u89c4\u8303\u548c\u8ba4\u77e5\u542f\u53d1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u6a21\u62df\u591a\u65b9\u7406\u6027\u534f\u8bae\u3002", "result": "RRC\u6846\u67b6\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u8fd0\u4f5c\uff0c\u5e76\u52a8\u6001\u9002\u5e94\u548c\u7406\u89e3\u4e0d\u65ad\u53d8\u5316\u7684\u4eba\u7c7b\u793e\u4f1a\u73af\u5883\u3002", "conclusion": "\u8d44\u6e90\u7406\u6027\u5951\u7ea6\u4e3b\u4e49\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u52a8\u6001\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u4eba\u7c7b\u793e\u4f1a\u51b3\u7b56\u573a\u666f\u3002", "paper_title_zh": "\u8d44\u6e90\u7406\u6027\u5951\u7ea6\u4e3b\u4e49\u5e94\u6307\u5bfcAI\u5bf9\u9f50", "abstract_zh": "AI\u7cfb\u7edf\u5f88\u5feb\u5c06\u9700\u8981\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5bfc\u822a\u5e76\u505a\u51fa\u5f71\u54cd\u4eba\u7c7b\u548c\u5176\u4ed6AI\u4ee3\u7406\u7684\u51b3\u7b56\uff0c\u800c\u8fd9\u4e9b\u4ee3\u7406\u7684\u76ee\u6807\u548c\u4ef7\u503c\u89c2\u53ef\u80fd\u5404\u4e0d\u76f8\u540c\u3002\u5951\u7ea6\u4e3b\u4e49\u5bf9\u9f50\u63d0\u8bae\u5c06\u8fd9\u4e9b\u51b3\u7b56\u5efa\u7acb\u5728\u591a\u6837\u5229\u76ca\u76f8\u5173\u8005\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u4f1a\u652f\u6301\u7684\u534f\u8bae\u57fa\u7840\u4e0a\uff0c\u7136\u800c\u5927\u89c4\u6a21\u8fbe\u6210\u6b64\u7c7b\u534f\u8bae\u6210\u672c\u9ad8\u4e14\u901f\u5ea6\u6162\u2014\u2014\u5373\u4f7f\u5bf9\u5148\u8fdbAI\u4e5f\u662f\u5982\u6b64\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u8d44\u6e90\u7406\u6027\u5951\u7ea6\u4e3b\u4e49\uff08RRC\uff09\uff1a\u4e00\u79cd\u6846\u67b6\uff0c\u5176\u4e2dAI\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u4e00\u5957\u57fa\u4e8e\u89c4\u8303\u548c\u8ba4\u77e5\u542f\u53d1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u52aa\u529b\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u8fd1\u4f3c\u6a21\u62df\u7406\u6027\u5404\u65b9\u4f1a\u5f62\u6210\u7684\u534f\u8bae\u3002\u4e00\u4e2aRRC\u5bf9\u9f50\u7684\u4ee3\u7406\u4e0d\u4ec5\u80fd\u9ad8\u6548\u8fd0\u4f5c\uff0c\u8fd8\u80fd\u52a8\u6001\u9002\u5e94\u5e76\u89e3\u91ca\u4e0d\u65ad\u53d8\u5316\u7684\u4eba\u7c7b\u793e\u4f1a\u4e16\u754c\u3002"}}
{"id": "2506.17314", "pdf": "https://arxiv.org/pdf/2506.17314", "abs": "https://arxiv.org/abs/2506.17314", "authors": ["Adnan Qidwai", "Srija Mukhopadhyay", "Prerana Khatiwada", "Dan Roth", "Vivek Gupta"], "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights", "categories": ["cs.CL", "cs.HC"], "comment": "9 Pages, 9 Figures. Accepted at ACL 2025 System Demonstration Track", "summary": "Accurate and complete product descriptions are crucial for e-commerce, yet\nseller-provided information often falls short. Customer reviews offer valuable\ndetails but are laborious to sift through manually. We present PRAISE: Product\nReview Attribute Insight Structuring Engine, a novel system that uses Large\nLanguage Models (LLMs) to automatically extract, compare, and structure\ninsights from customer reviews and seller descriptions. PRAISE provides users\nwith an intuitive interface to identify missing, contradictory, or partially\nmatching details between these two sources, presenting the discrepancies in a\nclear, structured format alongside supporting evidence from reviews. This\nallows sellers to easily enhance their product listings for clarity and\npersuasiveness, and buyers to better assess product reliability. Our\ndemonstration showcases PRAISE's workflow, its effectiveness in generating\nactionable structured insights from unstructured reviews, and its potential to\nsignificantly improve the quality and trustworthiness of e-commerce product\ncatalogs.", "AI": {"tldr": "PRAISE\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u5ba2\u6237\u8bc4\u8bba\u548c\u5356\u5bb6\u63cf\u8ff0\u4e2d\u63d0\u53d6\u3001\u6bd4\u8f83\u548c\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u7cfb\u7edf\uff0c\u5e2e\u52a9\u5356\u5bb6\u6539\u8fdb\u4ea7\u54c1\u63cf\u8ff0\u5e76\u63d0\u5347\u4e70\u5bb6\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u7535\u5546\u4e2d\u4ea7\u54c1\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5356\u5bb6\u63d0\u4f9b\u7684\u4fe1\u606f\u5f80\u5f80\u4e0d\u8db3\uff0c\u800c\u5ba2\u6237\u8bc4\u8bba\u867d\u5305\u542b\u4e30\u5bcc\u7ec6\u8282\u5374\u96be\u4ee5\u624b\u52a8\u7b5b\u9009\u3002PRAISE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PRAISE\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5206\u6790\u5ba2\u6237\u8bc4\u8bba\u548c\u5356\u5bb6\u63cf\u8ff0\uff0c\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u5e76\u5bf9\u6bd4\u4e24\u8005\u5dee\u5f02\uff0c\u4ee5\u76f4\u89c2\u754c\u9762\u5c55\u793a\u7f3a\u5931\u3001\u77db\u76fe\u6216\u4e0d\u5339\u914d\u7684\u7ec6\u8282\u3002", "result": "PRAISE\u80fd\u591f\u6709\u6548\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u7535\u5546\u4ea7\u54c1\u76ee\u5f55\u7684\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "PRAISE\u901a\u8fc7LLM\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u5206\u6790\uff0c\u4e3a\u5356\u5bb6\u548c\u4e70\u5bb6\u63d0\u4f9b\u4e86\u6539\u8fdb\u4ea7\u54c1\u63cf\u8ff0\u548c\u8bc4\u4f30\u4ea7\u54c1\u53ef\u9760\u6027\u7684\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "PRAISE\uff1a\u57fa\u4e8eLLM\u7684\u7ed3\u6784\u5316\u6d1e\u5bdf\u589e\u5f3a\u4ea7\u54c1\u63cf\u8ff0", "abstract_zh": "\u51c6\u786e\u4e14\u5b8c\u6574\u7684\u4ea7\u54c1\u63cf\u8ff0\u5bf9\u7535\u5546\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5356\u5bb6\u63d0\u4f9b\u7684\u4fe1\u606f\u5f80\u5f80\u4e0d\u8db3\u3002\u5ba2\u6237\u8bc4\u8bba\u867d\u5305\u542b\u5b9d\u8d35\u7ec6\u8282\uff0c\u4f46\u624b\u52a8\u7b5b\u9009\u8d39\u65f6\u8d39\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86PRAISE\uff08\u4ea7\u54c1\u8bc4\u8bba\u5c5e\u6027\u6d1e\u5bdf\u7ed3\u6784\u5316\u5f15\u64ce\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u4ece\u5ba2\u6237\u8bc4\u8bba\u548c\u5356\u5bb6\u63cf\u8ff0\u4e2d\u63d0\u53d6\u3001\u6bd4\u8f83\u548c\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u7cfb\u7edf\u3002PRAISE\u901a\u8fc7\u76f4\u89c2\u754c\u9762\u5c55\u793a\u4e24\u8005\u4e4b\u95f4\u7684\u7f3a\u5931\u3001\u77db\u76fe\u6216\u4e0d\u5339\u914d\u7ec6\u8282\uff0c\u5e76\u63d0\u4f9b\u8bc4\u8bba\u4e2d\u7684\u652f\u6301\u8bc1\u636e\uff0c\u5e2e\u52a9\u5356\u5bb6\u6539\u8fdb\u4ea7\u54c1\u63cf\u8ff0\u7684\u6e05\u6670\u5ea6\u548c\u8bf4\u670d\u529b\uff0c\u540c\u65f6\u8ba9\u4e70\u5bb6\u66f4\u597d\u5730\u8bc4\u4f30\u4ea7\u54c1\u53ef\u9760\u6027\u3002\u6211\u4eec\u7684\u6f14\u793a\u5c55\u793a\u4e86PRAISE\u7684\u5de5\u4f5c\u6d41\u7a0b\u3001\u4ece\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u4e2d\u751f\u6210\u53ef\u64cd\u4f5c\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5176\u663e\u8457\u63d0\u5347\u7535\u5546\u4ea7\u54c1\u76ee\u5f55\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17325", "pdf": "https://arxiv.org/pdf/2506.17325", "abs": "https://arxiv.org/abs/2506.17325", "authors": ["Sina Najafi", "M. Hadi Sepanj", "Fahimeh Jafari"], "title": "RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Predicting user churn in non-subscription gig platforms, where disengagement\nis implicit, poses unique challenges due to the absence of explicit labels and\nthe dynamic nature of user behavior. Existing methods often rely on aggregated\nsnapshots or static visual representations, which obscure temporal cues\ncritical for early detection. In this work, we propose a temporally-aware\ncomputer vision framework that models user behavioral patterns as a sequence of\nradar chart images, each encoding day-level behavioral features. By integrating\na pretrained CNN encoder with a bidirectional LSTM, our architecture captures\nboth spatial and temporal patterns underlying churn behavior. Extensive\nexperiments on a large real-world dataset demonstrate that our method\noutperforms classical models and ViT-based radar chart baselines, yielding\ngains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with\nimproved interpretability. The framework's modular design, explainability\ntools, and efficient deployment characteristics make it suitable for\nlarge-scale churn modeling in dynamic gig-economy platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f7\u8fbe\u56fe\u5e8f\u5217\u7684\u65f6\u95f4\u89c6\u89c9\u6846\u67b6RadarSeq\uff0c\u7528\u4e8e\u9884\u6d4b\u975e\u8ba2\u9605\u5236\u96f6\u5de5\u5e73\u53f0\u7684\u7528\u6237\u6d41\u5931\u3002\u901a\u8fc7\u5c06\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u5efa\u6a21\u4e3a\u96f7\u8fbe\u56fe\u5e8f\u5217\uff0c\u5e76\u7ed3\u5408CNN\u548c\u53cc\u5411LSTM\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u975e\u8ba2\u9605\u5236\u96f6\u5de5\u5e73\u53f0\u4e2d\uff0c\u7528\u6237\u6d41\u5931\u662f\u9690\u5f0f\u7684\uff0c\u7f3a\u4e4f\u660e\u786e\u6807\u7b7e\u4e14\u884c\u4e3a\u52a8\u6001\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u9759\u6001\u5feb\u7167\u6216\u89c6\u89c9\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u7ebf\u7d22\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u7684\u89c6\u89c9\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faRadarSeq\u6846\u67b6\uff0c\u5c06\u7528\u6237\u884c\u4e3a\u7279\u5f81\u7f16\u7801\u4e3a\u96f7\u8fbe\u56fe\u5e8f\u5217\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3CNN\u7f16\u7801\u5668\u548c\u53cc\u5411LSTM\u6355\u6349\u65f6\u7a7a\u6a21\u5f0f\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRadarSeq\u5728F1\u5206\u6570\u3001\u7cbe\u786e\u7387\u548cAUC\u4e0a\u5206\u522b\u63d0\u534717.7\u300129.4\u548c16.1\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u548cViT\u57fa\u7ebf\u3002", "conclusion": "RadarSeq\u6846\u67b6\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u6548\u90e8\u7f72\u7279\u70b9\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u96f6\u5de5\u7ecf\u6d4e\u5e73\u53f0\u7684\u5927\u89c4\u6a21\u6d41\u5931\u5efa\u6a21\u3002", "paper_title_zh": "RadarSeq\uff1a\u57fa\u4e8e\u96f7\u8fbe\u56fe\u5e8f\u5217\u7684\u65f6\u95f4\u89c6\u89c9\u6846\u67b6\u7528\u4e8e\u7528\u6237\u6d41\u5931\u9884\u6d4b", "abstract_zh": "\u9884\u6d4b\u975e\u8ba2\u9605\u5236\u96f6\u5de5\u5e73\u53f0\u7684\u7528\u6237\u6d41\u5931\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u56e0\u4e3a\u6d41\u5931\u662f\u9690\u5f0f\u7684\u4e14\u7528\u6237\u884c\u4e3a\u52a8\u6001\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u5feb\u7167\u6216\u89c6\u89c9\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u65f6\u95f4\u7ebf\u7d22\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6846\u67b6\uff0c\u5c06\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u5efa\u6a21\u4e3a\u96f7\u8fbe\u56fe\u5e8f\u5217\uff0c\u6bcf\u5f20\u56fe\u7f16\u7801\u6bcf\u65e5\u884c\u4e3a\u7279\u5f81\u3002\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3CNN\u7f16\u7801\u5668\u548c\u53cc\u5411LSTM\uff0c\u8be5\u6846\u67b6\u6355\u6349\u4e86\u6d41\u5931\u884c\u4e3a\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002\u5728\u5927\u578b\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u3001\u7cbe\u786e\u7387\u548cAUC\u4e0a\u5206\u522b\u63d0\u534717.7\u300129.4\u548c16.1\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u548cViT\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u89e3\u91ca\u5de5\u5177\u548c\u9ad8\u6548\u90e8\u7f72\u7279\u6027\u4f7f\u5176\u9002\u7528\u4e8e\u52a8\u6001\u96f6\u5de5\u7ecf\u6d4e\u5e73\u53f0\u7684\u5927\u89c4\u6a21\u6d41\u5931\u5efa\u6a21\u3002"}}
{"id": "2506.17442", "pdf": "https://arxiv.org/pdf/2506.17442", "abs": "https://arxiv.org/abs/2506.17442", "authors": ["Hao Guan", "David Bates", "Li Zhou"], "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation", "categories": ["cs.AI", "cs.ET", "cs.LG"], "comment": "15 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly integrated into modern\nhealthcare, offering powerful support for clinical decision-making. However, in\nreal-world settings, AI systems may experience performance degradation over\ntime, due to factors such as shifting data distributions, changes in patient\ncharacteristics, evolving clinical protocols, and variations in data quality.\nThese factors can compromise model reliability, posing safety concerns and\nincreasing the likelihood of inaccurate predictions or adverse outcomes. This\nreview presents a forward-looking perspective on monitoring and maintaining the\n\"health\" of AI systems in healthcare. We highlight the urgent need for\ncontinuous performance monitoring, early degradation detection, and effective\nself-correction mechanisms. The paper begins by reviewing common causes of\nperformance degradation at both data and model levels. We then summarize key\ntechniques for detecting data and model drift, followed by an in-depth look at\nroot cause analysis. Correction strategies are further reviewed, ranging from\nmodel retraining to test-time adaptation. Our survey spans both traditional\nmachine learning models and state-of-the-art large language models (LLMs),\noffering insights into their strengths and limitations. Finally, we discuss\nongoing technical challenges and propose future research directions. This work\naims to guide the development of reliable, robust medical AI systems capable of\nsustaining safe, long-term deployment in dynamic clinical settings.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u7597AI\u7cfb\u7edf\u6027\u80fd\u9000\u5316\u7684\u68c0\u6d4b\u4e0e\u6821\u6b63\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u5206\u5e03\u53d8\u5316\u3001\u60a3\u8005\u7279\u5f81\u53d8\u5316\u7b49\u56e0\u7d20\u5bf9AI\u6a21\u578b\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6301\u7eed\u76d1\u63a7\u548c\u81ea\u6821\u6b63\u673a\u5236\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u56e0\u6570\u636e\u5206\u5e03\u53d8\u5316\u3001\u60a3\u8005\u7279\u5f81\u53d8\u5316\u7b49\u56e0\u7d20\u800c\u9000\u5316\uff0c\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u76d1\u6d4b\u548c\u7ef4\u62a4\u533b\u7597AI\u7cfb\u7edf\u7684\u201c\u5065\u5eb7\u201d\uff0c\u4ee5\u786e\u4fdd\u5176\u957f\u671f\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u672c\u6587\u9996\u5148\u5206\u6790\u4e86\u5bfc\u81f4AI\u7cfb\u7edf\u6027\u80fd\u9000\u5316\u7684\u5e38\u89c1\u539f\u56e0\uff08\u5982\u6570\u636e\u548c\u6a21\u578b\u5c42\u9762\u7684\u53d8\u5316\uff09\uff0c\u968f\u540e\u603b\u7ed3\u4e86\u6570\u636e\u6f02\u79fb\u548c\u6a21\u578b\u6f02\u79fb\u7684\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u6839\u56e0\u5206\u6790\u3002\u6b64\u5916\uff0c\u8fd8\u56de\u987e\u4e86\u4ece\u6a21\u578b\u91cd\u8bad\u7ec3\u5230\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u6821\u6b63\u7b56\u7565\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u6d4b\u548c\u6821\u6b63\u6027\u80fd\u9000\u5316\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u9c81\u68d2\u7684\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6301\u7eed\u76d1\u63a7\u548c\u81ea\u6821\u6b63\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "paper_title_zh": "\u4fdd\u6301\u533b\u7597AI\u7684\u5065\u5eb7\uff1a\u7cfb\u7edf\u6027\u80fd\u9000\u5316\u68c0\u6d4b\u4e0e\u6821\u6b63\u65b9\u6cd5\u7efc\u8ff0", "abstract_zh": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u73b0\u4ee3\u533b\u7597\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u5f3a\u5927\u652f\u6301\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cAI\u7cfb\u7edf\u53ef\u80fd\u56e0\u6570\u636e\u5206\u5e03\u53d8\u5316\u3001\u60a3\u8005\u7279\u5f81\u53d8\u5316\u3001\u4e34\u5e8a\u534f\u8bae\u6f14\u53d8\u4ee5\u53ca\u6570\u636e\u8d28\u91cf\u6ce2\u52a8\u7b49\u56e0\u7d20\u800c\u51fa\u73b0\u6027\u80fd\u9000\u5316\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5f15\u53d1\u5b89\u5168\u9690\u60a3\u6216\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u3002\u672c\u6587\u4ece\u524d\u77bb\u6027\u89c6\u89d2\u63a2\u8ba8\u4e86\u5982\u4f55\u76d1\u6d4b\u548c\u7ef4\u62a4\u533b\u7597AI\u7cfb\u7edf\u7684\u201c\u5065\u5eb7\u201d\uff0c\u5f3a\u8c03\u4e86\u6301\u7eed\u6027\u80fd\u76d1\u63a7\u3001\u65e9\u671f\u9000\u5316\u68c0\u6d4b\u548c\u6709\u6548\u81ea\u6821\u6b63\u673a\u5236\u7684\u8feb\u5207\u9700\u6c42\u3002\u6587\u7ae0\u9996\u5148\u56de\u987e\u4e86\u6570\u636e\u548c\u6a21\u578b\u5c42\u9762\u5bfc\u81f4\u6027\u80fd\u9000\u5316\u7684\u5e38\u89c1\u539f\u56e0\uff0c\u968f\u540e\u603b\u7ed3\u4e86\u6570\u636e\u6f02\u79fb\u548c\u6a21\u578b\u6f02\u79fb\u7684\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u6839\u56e0\u3002\u6821\u6b63\u7b56\u7565\u65b9\u9762\uff0c\u4ece\u6a21\u578b\u91cd\u8bad\u7ec3\u5230\u6d4b\u8bd5\u65f6\u9002\u5e94\u5747\u6709\u6240\u6d89\u53ca\u3002\u672c\u7814\u7a76\u6db5\u76d6\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u4e0e\u5c40\u9650\u3002\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u5f53\u524d\u6280\u672f\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u9c81\u68d2\u7684\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\uff0c\u4ee5\u652f\u6301\u5176\u5728\u52a8\u6001\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u957f\u671f\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2506.17352", "pdf": "https://arxiv.org/pdf/2506.17352", "abs": "https://arxiv.org/abs/2506.17352", "authors": ["Tatsuhiro Aoshima", "Mitsuaki Akiyama"], "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to advance, the\nimportance of rigorous safety evaluation is becoming increasingly evident.\nRecent concerns within the realm of safety assessment have highlighted\ninstances in which LLMs exhibit behaviors that appear to disable oversight\nmechanisms and respond in a deceptive manner. For example, there have been\nreports suggesting that, when confronted with information unfavorable to their\nown persistence during task execution, LLMs may act covertly and even provide\nfalse answers to questions intended to verify their behavior.To evaluate the\npotential risk of such deceptive actions toward developers or users, it is\nessential to investigate whether these behaviors stem from covert, intentional\nprocesses within the model. In this study, we propose that it is necessary to\nmeasure the theory of mind capabilities of LLMs. We begin by reviewing existing\nresearch on theory of mind and identifying the perspectives and tasks relevant\nto its application in safety evaluation. Given that theory of mind has been\npredominantly studied within the context of developmental psychology, we\nanalyze developmental trends across a series of open-weight LLMs. Our results\nindicate that while LLMs have improved in reading comprehension, their theory\nof mind capabilities have not shown comparable development. Finally, we present\nthe current state of safety evaluation with respect to LLMs' theory of mind,\nand discuss remaining challenges for future work.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u53ca\u5176\u5728\u5b89\u5168\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u5c3d\u7ba1LLMs\u5728\u9605\u8bfb\u7406\u89e3\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5176\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u5e76\u672a\u540c\u6b65\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u51f8\u663e\uff0c\u5c24\u5176\u662f\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u6b3a\u9a97\u6027\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u662f\u5426\u5177\u5907\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u4ee5\u5224\u65ad\u5176\u884c\u4e3a\u662f\u5426\u6e90\u4e8e\u9690\u853d\u7684\u610f\u56fe\u3002", "method": "\u7814\u7a76\u56de\u987e\u4e86\u5fc3\u667a\u7406\u8bba\u7684\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u5b89\u5168\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5f00\u653e\u6743\u91cd\u7684LLMs\uff0c\u7814\u7a76\u4e86\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1LLMs\u5728\u9605\u8bfb\u7406\u89e3\u65b9\u9762\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5176\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u5e76\u672a\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bc4\u4f30LLMs\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u4e2d\u9700\u8981\u89e3\u51b3\u7684\u6311\u6218\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u7684\u5b89\u5168\u8bc4\u4f30\u7814\u7a76", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u529b\u7684\u4e0d\u65ad\u63d0\u5347\uff0c\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\u663e\u5f97\u6108\u53d1\u91cd\u8981\u3002\u8fd1\u671f\u5b89\u5168\u8bc4\u4f30\u9886\u57df\u7684\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u53ef\u80fd\u8868\u73b0\u51fa\u7ed5\u8fc7\u76d1\u7ba1\u673a\u5236\u5e76\u4ee5\u6b3a\u9a97\u65b9\u5f0f\u56de\u5e94\u7684\u884c\u4e3a\u3002\u4f8b\u5982\uff0c\u6709\u62a5\u544a\u6307\u51fa\uff0c\u5f53LLMs\u5728\u6267\u884c\u4efb\u52a1\u65f6\u9047\u5230\u4e0d\u5229\u4e8e\u5176\u6301\u7eed\u5b58\u5728\u7684\u4fe1\u606f\u65f6\uff0c\u53ef\u80fd\u4f1a\u91c7\u53d6\u9690\u853d\u884c\u52a8\uff0c\u751a\u81f3\u5bf9\u9a8c\u8bc1\u5176\u884c\u4e3a\u7684\u95ee\u9898\u63d0\u4f9b\u865a\u5047\u7b54\u6848\u3002\u4e3a\u8bc4\u4f30\u6b64\u7c7b\u6b3a\u9a97\u884c\u4e3a\u5bf9\u5f00\u53d1\u8005\u6216\u7528\u6237\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u6709\u5fc5\u8981\u63a2\u7a76\u8fd9\u4e9b\u884c\u4e3a\u662f\u5426\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u9690\u853d\u610f\u56fe\u3002\u672c\u7814\u7a76\u63d0\u51fa\uff0c\u9700\u8981\u6d4b\u91cfLLMs\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u3002\u6211\u4eec\u9996\u5148\u56de\u987e\u4e86\u5fc3\u667a\u7406\u8bba\u7684\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u786e\u5b9a\u4e86\u5176\u5728\u5b89\u5168\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u89c6\u89d2\u548c\u4efb\u52a1\u3002\u9274\u4e8e\u5fc3\u667a\u7406\u8bba\u4e3b\u8981\u5728\u53d1\u5c55\u5fc3\u7406\u5b66\u80cc\u666f\u4e0b\u7814\u7a76\uff0c\u6211\u4eec\u5206\u6790\u4e86\u4e00\u7cfb\u5217\u5f00\u653e\u6743\u91cdLLMs\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u53d1\u5c55\u8d8b\u52bf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1LLMs\u7684\u9605\u8bfb\u7406\u89e3\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5176\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u5e76\u672a\u540c\u6b65\u53d1\u5c55\u3002\u6700\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u5f53\u524dLLMs\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u7684\u5b89\u5168\u8bc4\u4f30\u73b0\u72b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2506.17332", "pdf": "https://arxiv.org/pdf/2506.17332", "abs": "https://arxiv.org/abs/2506.17332", "authors": ["Haitian Wang", "Yiren Wang", "Xinyu Wang", "Yumeng Miao", "Yuliang Zhang", "Yu Zhang", "Atif Mansoor"], "title": "P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to appear in the 2025 IEEE International Workshop on AIoT\n  and Smart Systems (AIoTSys'25). Nominated for Best Paper Award and Best IoT\n  System Implementation Award. Code and pretrained models available at:\n  https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom", "summary": "By 2050, people aged 65 and over are projected to make up 16 percent of the\nglobal population. As aging is closely associated with increased fall risk,\nparticularly in wet and confined environments such as bathrooms where over 80\npercent of falls occur. Although recent research has increasingly focused on\nnon-intrusive, privacy-preserving approaches that do not rely on wearable\ndevices or video-based monitoring, these efforts have not fully overcome the\nlimitations of existing unimodal systems (e.g., WiFi-, infrared-, or\nmmWave-based), which are prone to reduced accuracy in complex environments.\nThese limitations stem from fundamental constraints in unimodal sensing,\nincluding system bias and environmental interference, such as multipath fading\nin WiFi-based systems and drastic temperature changes in infrared-based\nmethods. To address these challenges, we propose a Privacy-Preserving\nMultimodal Fall Detection System for Elderly People in Bathroom Environments.\nFirst, we develop a sensor evaluation framework to select and fuse\nmillimeter-wave radar with 3D vibration sensing, and use it to construct and\npreprocess a large-scale, privacy-preserving multimodal dataset in real\nbathroom settings, which will be released upon publication. Second, we\nintroduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch\nfor radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch\nfor vibration impact detection. By uniting macro- and micro-scale features,\nP2MFDS delivers significant gains in accuracy and recall over state-of-the-art\napproaches. Code and pretrained models will be made available at:\nhttps://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edfP2MFDS\uff0c\u4e13\u4e3a\u8001\u5e74\u4eba\u5728\u6d74\u5ba4\u73af\u5883\u4e2d\u8bbe\u8ba1\u3002\u901a\u8fc7\u878d\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u548c3D\u632f\u52a8\u4f20\u611f\uff0c\u7ed3\u5408\u53cc\u6d41\u7f51\u7edc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u968f\u7740\u5168\u7403\u8001\u9f84\u5316\u52a0\u5267\uff0c\u8001\u5e74\u4eba\u8dcc\u5012\u98ce\u9669\u589e\u52a0\uff0c\u5c24\u5176\u5728\u6d74\u5ba4\u7b49\u6e7f\u6ed1\u72ed\u7a84\u73af\u5883\u4e2d\u3002\u73b0\u6709\u5355\u6a21\u6001\u68c0\u6d4b\u7cfb\u7edf\uff08\u5982WiFi\u3001\u7ea2\u5916\u6216\u6beb\u7c73\u6ce2\uff09\u56e0\u73af\u5883\u5e72\u6270\u548c\u7cfb\u7edf\u504f\u5dee\u5bfc\u81f4\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u9996\u5148\u5f00\u53d1\u4f20\u611f\u5668\u8bc4\u4f30\u6846\u67b6\uff0c\u9009\u62e9\u5e76\u878d\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e3D\u632f\u52a8\u4f20\u611f\uff0c\u6784\u5efa\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u5176\u6b21\u63d0\u51faP2MFDS\u53cc\u6d41\u7f51\u7edc\uff0c\u7ed3\u5408CNN-BiLSTM-Attention\u5206\u652f\u5904\u7406\u96f7\u8fbe\u52a8\u6001\u6570\u636e\uff0c\u4ee5\u53ca\u591a\u5c3a\u5ea6CNN-SEBlock-Self-Attention\u5206\u652f\u68c0\u6d4b\u632f\u52a8\u4fe1\u53f7\u3002", "result": "P2MFDS\u5728\u771f\u5b9e\u6d74\u5ba4\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "P2MFDS\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6d74\u5ba4\u73af\u5883\u4e2d\u8001\u5e74\u4eba\u8dcc\u5012\u68c0\u6d4b\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u51c6\u786e\u6027\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "P2MFDS\uff1a\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u8001\u5e74\u4eba\u6d74\u5ba4\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf", "abstract_zh": "\u52302050\u5e74\uff0c65\u5c81\u53ca\u4ee5\u4e0a\u4eba\u53e3\u9884\u8ba1\u5c06\u5360\u5168\u7403\u4eba\u53e3\u768416%\u3002\u8001\u9f84\u5316\u4e0e\u8dcc\u5012\u98ce\u9669\u589e\u52a0\u5bc6\u5207\u76f8\u5173\uff0c\u5c24\u5176\u662f\u5728\u6d74\u5ba4\u7b49\u6e7f\u6ed1\u72ed\u7a84\u73af\u5883\u4e2d\uff0c80%\u4ee5\u4e0a\u7684\u8dcc\u5012\u53d1\u751f\u4e8e\u6b64\u3002\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u9010\u6e10\u5173\u6ce8\u975e\u4fb5\u5165\u5f0f\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u6cd5\uff08\u4e0d\u4f9d\u8d56\u53ef\u7a7f\u6234\u8bbe\u5907\u6216\u89c6\u9891\u76d1\u63a7\uff09\uff0c\u4f46\u8fd9\u4e9b\u52aa\u529b\u5c1a\u672a\u5b8c\u5168\u514b\u670d\u73b0\u6709\u5355\u6a21\u6001\u7cfb\u7edf\uff08\u5982\u57fa\u4e8eWiFi\u3001\u7ea2\u5916\u6216\u6beb\u7c73\u6ce2\uff09\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u51c6\u786e\u6027\u8f83\u4f4e\u3002\u8fd9\u4e9b\u5c40\u9650\u6027\u6e90\u4e8e\u5355\u6a21\u6001\u4f20\u611f\u7684\u57fa\u672c\u7ea6\u675f\uff0c\u5305\u62ec\u7cfb\u7edf\u504f\u5dee\u548c\u73af\u5883\u5e72\u6270\uff08\u5982WiFi\u7cfb\u7edf\u4e2d\u7684\u591a\u5f84\u8870\u843d\u548c\u7ea2\u5916\u65b9\u6cd5\u4e2d\u7684\u6e29\u5ea6\u5267\u70c8\u53d8\u5316\uff09\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u8001\u5e74\u4eba\u6d74\u5ba4\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4f20\u611f\u5668\u8bc4\u4f30\u6846\u67b6\uff0c\u9009\u62e9\u5e76\u878d\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e3D\u632f\u52a8\u4f20\u611f\uff0c\u6784\u5efa\u5e76\u9884\u5904\u7406\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\uff09\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86P2MFDS\uff0c\u8fd9\u662f\u4e00\u79cd\u53cc\u6d41\u7f51\u7edc\uff0c\u7ed3\u5408\u4e86CNN-BiLSTM-Attention\u5206\u652f\u5904\u7406\u96f7\u8fbe\u52a8\u6001\u6570\u636e\uff0c\u4ee5\u53ca\u591a\u5c3a\u5ea6CNN-SEBlock-Self-Attention\u5206\u652f\u68c0\u6d4b\u632f\u52a8\u4fe1\u53f7\u3002\u901a\u8fc7\u878d\u5408\u5b8f\u89c2\u548c\u5fae\u89c2\u7279\u5f81\uff0cP2MFDS\u5728\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u5728\u4ee5\u4e0b\u7f51\u5740\u63d0\u4f9b\uff1ahttps://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom\u3002"}}
{"id": "2506.17449", "pdf": "https://arxiv.org/pdf/2506.17449", "abs": "https://arxiv.org/abs/2506.17449", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "categories": ["cs.AI"], "comment": null, "summary": "Efforts to improve Large Language Model (LLM) agent performance on complex\ntasks have largely focused on fine-tuning and iterative self-correction.\nHowever, these approaches often lack generalizable mechanisms for longterm\nlearning and remain inefficient in dynamic environments. We introduce\nOmniReflect, a hierarchical, reflection-driven framework that constructs a\nconstitution, a compact set of guiding principles distilled from task\nexperiences, to enhance the effectiveness and efficiency of an LLM agent.\nOmniReflect operates in two modes: Self-sustaining, where a single agent\nperiodically curates its own reflections during task execution, and\nCo-operative, where a Meta-advisor derives a constitution from a small\ncalibration set to guide another agent. To construct these constitutional\nprinciples, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering\na balance between contextual adaptability and computational efficiency.\nEmpirical results averaged across models show major improvements in task\nsuccess, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%\non PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative\nmode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion\nbaselines on BabyAI. These findings highlight the robustness and effectiveness\nof OmniReflect across environments and backbones.", "AI": {"tldr": "OmniReflect\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u3001\u53cd\u601d\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6280\u672f\u6784\u5efa\u6307\u5bfc\u539f\u5219\uff08\u5baa\u6cd5\uff09\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u5728ALFWorld\u3001BabyAI\u548cPDDL\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6027\u80fd\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5fae\u8c03\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\u4e0a\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u957f\u671f\u5b66\u4e60\u7684\u901a\u7528\u673a\u5236\u4e14\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002OmniReflect\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u7d27\u51d1\u7684\u6307\u5bfc\u539f\u5219\uff08\u5baa\u6cd5\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "OmniReflect\u91c7\u7528\u4e24\u79cd\u6a21\u5f0f\uff1a\u81ea\u6211\u7ef4\u6301\u6a21\u5f0f\uff08\u5355\u4e2a\u4ee3\u7406\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u5b9a\u671f\u53cd\u601d\uff09\u548c\u534f\u4f5c\u6a21\u5f0f\uff08\u5143\u987e\u95ee\u4ece\u6821\u51c6\u96c6\u4e2d\u63d0\u53d6\u5baa\u6cd5\u6307\u5bfc\u53e6\u4e00\u4ee3\u7406\uff09\u3002\u901a\u8fc7\u795e\u7ecf\u3001\u7b26\u53f7\u548c\u795e\u7ecf\u7b26\u53f7\u6280\u672f\u6784\u5efa\u5baa\u6cd5\uff0c\u5e73\u8861\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOmniReflect\u5728ALFWorld\u3001BabyAI\u548cPDDL\u4efb\u52a1\u4e2d\u5206\u522b\u53d6\u5f97+10.3%\u3001+23.8%\u548c+8.3%\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u534f\u4f5c\u6a21\u5f0f\u4e0b\uff0c\u8f7b\u91cf\u7ea7Qwen3-4B ReAct\u4ee3\u7406\u5728BabyAI\u4e0a\u8868\u73b0\u4f18\u4e8e\u6240\u6709Reflexion\u57fa\u7ebf\u3002", "conclusion": "OmniReflect\u901a\u8fc7\u6784\u5efa\u901a\u7528\u6307\u5bfc\u539f\u5219\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u73af\u5883\u548c\u6a21\u578b\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "paper_title_zh": "OmniReflect\uff1a\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u53cd\u601d\u53d1\u73b0LLM\u4ee3\u7406\u7684\u53ef\u8fc1\u79fb\u5baa\u6cd5", "abstract_zh": "\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u5fae\u8c03\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\u4e0a\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u957f\u671f\u5b66\u4e60\u7684\u901a\u7528\u673a\u5236\u4e14\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86OmniReflect\uff0c\u4e00\u79cd\u5c42\u6b21\u5316\u3001\u53cd\u601d\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4efb\u52a1\u7ecf\u9a8c\u4e2d\u63d0\u70bc\u7d27\u51d1\u7684\u6307\u5bfc\u539f\u5219\uff08\u5baa\u6cd5\uff09\u6765\u589e\u5f3aLLM\u4ee3\u7406\u7684\u6548\u7387\u548c\u6548\u679c\u3002OmniReflect\u6709\u4e24\u79cd\u6a21\u5f0f\uff1a\u81ea\u6211\u7ef4\u6301\u6a21\u5f0f\uff08\u5355\u4e2a\u4ee3\u7406\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u5b9a\u671f\u53cd\u601d\uff09\u548c\u534f\u4f5c\u6a21\u5f0f\uff08\u5143\u987e\u95ee\u4ece\u6821\u51c6\u96c6\u4e2d\u63d0\u53d6\u5baa\u6cd5\u6307\u5bfc\u53e6\u4e00\u4ee3\u7406\uff09\u3002\u4e3a\u6784\u5efa\u8fd9\u4e9b\u5baa\u6cd5\u539f\u5219\uff0c\u6211\u4eec\u91c7\u7528\u795e\u7ecf\u3001\u7b26\u53f7\u548c\u795e\u7ecf\u7b26\u53f7\u6280\u672f\uff0c\u5e73\u8861\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOmniReflect\u5728ALFWorld\u3001BabyAI\u548cPDDL\u4efb\u52a1\u4e2d\u5206\u522b\u53d6\u5f97+10.3%\u3001+23.8%\u548c+8.3%\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u534f\u4f5c\u6a21\u5f0f\u4e0b\uff0c\u8f7b\u91cf\u7ea7Qwen3-4B ReAct\u4ee3\u7406\u5728BabyAI\u4e0a\u8868\u73b0\u4f18\u4e8e\u6240\u6709Reflexion\u57fa\u7ebf\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86OmniReflect\u5728\u591a\u79cd\u73af\u5883\u548c\u6a21\u578b\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.17367", "pdf": "https://arxiv.org/pdf/2506.17367", "abs": "https://arxiv.org/abs/2506.17367", "authors": ["Mateusz Cedro", "Timour Ichmoukhamedov", "Sofie Goethals", "Yifan He", "James Hinns", "David Martens"], "title": "Cash or Comfort? How LLMs Value Your Inconvenience", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "12 pages, 4 figures, 3 tables", "summary": "Large Language Models (LLMs) are increasingly proposed as near-autonomous\nartificial intelligence (AI) agents capable of making everyday decisions on\nbehalf of humans. Although LLMs perform well on many technical tasks, their\nbehaviour in personal decision-making remains less understood. Previous studies\nhave assessed their rationality and moral alignment with human decisions.\nHowever, the behaviour of AI assistants in scenarios where financial rewards\nare at odds with user comfort has not yet been thoroughly explored. In this\npaper, we tackle this problem by quantifying the prices assigned by multiple\nLLMs to a series of user discomforts: additional walking, waiting, hunger and\npain. We uncover several key concerns that strongly question the prospect of\nusing current LLMs as decision-making assistants: (1) a large variance in\nresponses between LLMs, (2) within a single LLM, responses show fragility to\nminor variations in prompt phrasing (e.g., reformulating the question in the\nfirst person can considerably alter the decision), (3) LLMs can accept\nunreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10\nhours), and (4) LLMs can reject monetary gains where no discomfort is imposed\n(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for\nscrutiny of how LLMs value human inconvenience, particularly as we move toward\napplications where such cash-versus-comfort trade-offs are made on users'\nbehalf.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6743\u8861\u91d1\u94b1\u4e0e\u7528\u6237\u8212\u9002\u5ea6\u65f6\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u5408\u7406\u6027\uff0c\u4e0d\u9002\u5408\u4f5c\u4e3a\u51b3\u7b56\u52a9\u624b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u63d0\u8bae\u4f5c\u4e3a\u8fd1\u4e4e\u81ea\u4e3b\u7684AI\u4ee3\u7406\uff0c\u5176\u5728\u4e2a\u4eba\u51b3\u7b56\u4e2d\u7684\u884c\u4e3a\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u91d1\u94b1\u4e0e\u7528\u6237\u8212\u9002\u5ea6\u51b2\u7a81\u7684\u573a\u666f\u4e0b\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u91cf\u5316\u591a\u4e2aLLMs\u5bf9\u7528\u6237\u4e0d\u9002\uff08\u5982\u989d\u5916\u6b65\u884c\u3001\u7b49\u5f85\u3001\u9965\u997f\u548c\u75bc\u75db\uff09\u7684\u5b9a\u4ef7\uff0c\u5206\u6790\u5176\u51b3\u7b56\u884c\u4e3a\u3002", "result": "\u53d1\u73b0LLMs\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a(1) \u4e0d\u540c\u6a21\u578b\u95f4\u5dee\u5f02\u5927\uff0c(2) \u540c\u4e00\u6a21\u578b\u5bf9\u63d0\u793a\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c(3) \u5bf9\u91cd\u5927\u4e0d\u9002\u63a5\u53d7\u6781\u4f4e\u62a5\u916c\uff0c(4) \u65e0\u4e0d\u9002\u65f6\u62d2\u7edd\u9ad8\u989d\u62a5\u916c\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u6743\u8861\u91d1\u94b1\u4e0e\u8212\u9002\u5ea6\u65f6\u8868\u73b0\u4e0d\u53ef\u9760\uff0c\u9700\u8fdb\u4e00\u6b65\u5ba1\u67e5\u5176\u4f5c\u4e3a\u51b3\u7b56\u52a9\u624b\u7684\u9002\u7528\u6027\u3002", "paper_title_zh": "\u91d1\u94b1\u8fd8\u662f\u8212\u9002\uff1f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u8861\u91cf\u60a8\u7684\u4e0d\u4fbf", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u63d0\u8bae\u4f5c\u4e3a\u8fd1\u4e4e\u81ea\u4e3b\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4ee3\u7406\uff0c\u80fd\u591f\u4ee3\u8868\u4eba\u7c7b\u505a\u51fa\u65e5\u5e38\u51b3\u7b56\u3002\u5c3d\u7ba1LLMs\u5728\u8bb8\u591a\u6280\u672f\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u5728\u4e2a\u4eba\u51b3\u7b56\u4e2d\u7684\u884c\u4e3a\u4ecd\u8f83\u5c11\u88ab\u7406\u89e3\u3002\u4ee5\u5f80\u7814\u7a76\u8bc4\u4f30\u4e86\u5176\u7406\u6027\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7684\u9053\u5fb7\u4e00\u81f4\u6027\uff0c\u4f46AI\u52a9\u624b\u5728\u91d1\u94b1\u5956\u52b1\u4e0e\u7528\u6237\u8212\u9002\u5ea6\u51b2\u7a81\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002\u672c\u6587\u901a\u8fc7\u91cf\u5316\u591a\u4e2aLLMs\u5bf9\u4e00\u7cfb\u5217\u7528\u6237\u4e0d\u9002\uff08\u989d\u5916\u6b65\u884c\u3001\u7b49\u5f85\u3001\u9965\u997f\u548c\u75bc\u75db\uff09\u7684\u5b9a\u4ef7\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\u51e0\u4e2a\u5173\u952e\u95ee\u9898\u4e25\u91cd\u8d28\u7591\u5f53\u524dLLMs\u4f5c\u4e3a\u51b3\u7b56\u52a9\u624b\u7684\u524d\u666f\uff1a(1) \u4e0d\u540cLLMs\u95f4\u54cd\u5e94\u5dee\u5f02\u5927\uff0c(2) \u540c\u4e00LLM\u5bf9\u63d0\u793a\u5fae\u5c0f\u53d8\u5316\uff08\u5982\u4ee5\u7b2c\u4e00\u4eba\u79f0\u91cd\u8ff0\u95ee\u9898\uff09\u654f\u611f\uff0c(3) LLMs\u53ef\u80fd\u4e3a\u91cd\u5927\u4e0d\u9002\u63a5\u53d7\u6781\u4f4e\u62a5\u916c\uff08\u59821\u6b27\u5143\u7b49\u5f8510\u5c0f\u65f6\uff09\uff0c(4) LLMs\u53ef\u80fd\u62d2\u7edd\u65e0\u4e0d\u9002\u65f6\u7684\u9ad8\u989d\u62a5\u916c\uff08\u59821,000\u6b27\u5143\u7b49\u5f850\u5206\u949f\uff09\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u9700\u5ba1\u67e5LLMs\u5982\u4f55\u8861\u91cf\u4eba\u7c7b\u4e0d\u4fbf\uff0c\u5c24\u5176\u662f\u5728\u8fc8\u5411\u7531\u5176\u4ee3\u8868\u7528\u6237\u505a\u51fa\u91d1\u94b1\u4e0e\u8212\u9002\u5ea6\u6743\u8861\u7684\u5e94\u7528\u65f6\u3002"}}
{"id": "2506.17346", "pdf": "https://arxiv.org/pdf/2506.17346", "abs": "https://arxiv.org/abs/2506.17346", "authors": ["Yuhan Zhou", "Haihua Chen", "Kewei Sha"], "title": "A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The next-generation autonomous vehicles (AVs), embedded with frequent\nreal-time decision-making, will rely heavily on a large volume of multisource\nand multimodal data. In real-world settings, the data quality (DQ) of different\nsources and modalities usually varies due to unexpected environmental factors\nor sensor issues. However, both researchers and practitioners in the AV field\noverwhelmingly concentrate on models/algorithms while undervaluing the DQ. To\nfulfill the needs of the next-generation AVs with guarantees of functionality,\nefficiency, and trustworthiness, this paper proposes a novel task-centric and\ndata quality vase framework which consists of five layers: data layer, DQ\nlayer, task layer, application layer, and goal layer. The proposed framework\naims to map DQ with task requirements and performance goals. To illustrate, a\ncase study investigating redundancy on the nuScenes dataset proves that\npartially removing redundancy on multisource image data could improve YOLOv8\nobject detection task performance. Analysis on multimodal data of image and\nLiDAR further presents existing redundancy DQ issues. This paper opens up a\nrange of critical but unexplored challenges at the intersection of DQ, task\norchestration, and performance-oriented system development in AVs. It is\nexpected to guide the AV community toward building more adaptive, explainable,\nand resilient AVs that respond intelligently to dynamic environments and\nheterogeneous data streams. Code, data, and implementation details are publicly\navailable at: https://anonymous.4open.science/r/dq4av-framework/README.md.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4efb\u52a1\u548c\u6570\u636e\u8d28\u91cf\u7684\u591a\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u529f\u80fd\u6027\u548c\u53ef\u9760\u6027\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5197\u4f59\u6570\u636e\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u547c\u5401\u793e\u533a\u5173\u6ce8\u6570\u636e\u8d28\u91cf\u4e0e\u4efb\u52a1\u9700\u6c42\u7684\u5339\u914d\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u8fc7\u4e8e\u5173\u6ce8\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u800c\u5ffd\u89c6\u4e86\u6570\u636e\u8d28\u91cf\uff08DQ\uff09\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4e0b\u4e00\u4ee3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u6570\u636e\u5904\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e94\u5c42\u6846\u67b6\uff08\u6570\u636e\u5c42\u3001DQ\u5c42\u3001\u4efb\u52a1\u5c42\u3001\u5e94\u7528\u5c42\u548c\u76ee\u6807\u5c42\uff09\uff0c\u5c06\u6570\u636e\u8d28\u91cf\u4e0e\u4efb\u52a1\u9700\u6c42\u548c\u6027\u80fd\u76ee\u6807\u5173\u8054\u3002\u901a\u8fc7nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u591a\u6e90\u56fe\u50cf\u6570\u636e\u7684\u5197\u4f59\u5bf9YOLOv8\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u90e8\u5206\u53bb\u9664\u591a\u6e90\u56fe\u50cf\u6570\u636e\u7684\u5197\u4f59\u53ef\u4ee5\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u56fe\u50cf\u548cLiDAR\u6570\u636e\u7684\u591a\u6a21\u6001\u5206\u6790\u63ed\u793a\u4e86\u73b0\u6709\u5197\u4f59\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u6570\u636e\u8d28\u91cf\u3001\u4efb\u52a1\u7f16\u6392\u548c\u6027\u80fd\u5bfc\u5411\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u63a8\u52a8\u6784\u5efa\u66f4\u5177\u9002\u5e94\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "paper_title_zh": "\u4e00\u79cd\u9762\u5411\u4efb\u52a1\u548c\u6570\u636e\u8d28\u91cf\u7684\u591a\u5c42\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6", "abstract_zh": "\u4e0b\u4e00\u4ee3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u4f9d\u8d56\u4e8e\u5927\u91cf\u591a\u6e90\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u9645\u73af\u5883\u4e2d\uff0c\u6570\u636e\u8d28\u91cf\uff08DQ\uff09\u56e0\u73af\u5883\u56e0\u7d20\u6216\u4f20\u611f\u5668\u95ee\u9898\u800c\u5b58\u5728\u5dee\u5f02\u3002\u5f53\u524d\u7814\u7a76\u4e0e\u5b9e\u8df5\u8fc7\u4e8e\u5173\u6ce8\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u800c\u4f4e\u4f30\u4e86DQ\u7684\u91cd\u8981\u6027\u3002\u4e3a\u6ee1\u8db3\u4e0b\u4e00\u4ee3AVs\u7684\u529f\u80fd\u6027\u3001\u9ad8\u6548\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u9700\u6c42\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4efb\u52a1\u548c\u6570\u636e\u8d28\u91cf\u7684\u591a\u5c42\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u5c42\u3001DQ\u5c42\u3001\u4efb\u52a1\u5c42\u3001\u5e94\u7528\u5c42\u548c\u76ee\u6807\u5c42\u3002\u8be5\u6846\u67b6\u65e8\u5728\u5c06DQ\u4e0e\u4efb\u52a1\u9700\u6c42\u548c\u6027\u80fd\u76ee\u6807\u5173\u8054\u3002\u901a\u8fc7nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u90e8\u5206\u53bb\u9664\u591a\u6e90\u56fe\u50cf\u6570\u636e\u7684\u5197\u4f59\u53ef\u4ee5\u63d0\u5347YOLOv8\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5bf9\u56fe\u50cf\u548cLiDAR\u591a\u6a21\u6001\u6570\u636e\u7684\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u73b0\u6709\u5197\u4f59DQ\u95ee\u9898\u3002\u672c\u6587\u4e3aDQ\u3001\u4efb\u52a1\u7f16\u6392\u548c\u6027\u80fd\u5bfc\u5411\u7cfb\u7edf\u5f00\u53d1\u7684\u4ea4\u53c9\u9886\u57df\u5f00\u8f9f\u4e86\u4e00\u7cfb\u5217\u5173\u952e\u4f46\u672a\u63a2\u7d22\u7684\u6311\u6218\uff0c\u6709\u671b\u5f15\u5bfcAV\u793e\u533a\u6784\u5efa\u66f4\u5177\u9002\u5e94\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u5b9e\u73b0\u7ec6\u8282\u516c\u5f00\u4e8e\uff1ahttps://anonymous.4open.science/r/dq4av-framework/README.md\u3002"}}
{"id": "2506.17484", "pdf": "https://arxiv.org/pdf/2506.17484", "abs": "https://arxiv.org/abs/2506.17484", "authors": ["Yao Zhang", "Zaixi Shang", "Silpan Patel", "Mikel Zuniga"], "title": "From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases", "categories": ["cs.AI"], "comment": "Accepted In Proceedings of the 1st Workshop on AI for Supply Chain:\n  Today and Future @ 31st ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining V.2 (KDD 25), August 3, 2025, Toronto, ON, Canada. ACM, New York, NY,\n  USA, 14 pages, 2 figures", "summary": "Supply chain operations generate vast amounts of operational data; however,\ncritical knowledge such as system usage practices, troubleshooting workflows,\nand resolution techniques often remains buried within unstructured\ncommunications like support tickets, emails, and chat logs. While RAG systems\naim to leverage such communications as a knowledge base, their effectiveness is\nlimited by raw data challenges: support tickets are typically noisy,\ninconsistent, and incomplete, making direct retrieval suboptimal. Unlike\nexisting RAG approaches that focus on runtime optimization, we introduce a\nnovel offline-first methodology that transforms these communications into a\nstructured knowledge base. Our key innovation is a LLMs-based multi-agent\nsystem orchestrating three specialized agents: Category Discovery for taxonomy\ncreation, Categorization for ticket grouping, and Knowledge Synthesis for\narticle generation. Applying our methodology to real-world support tickets with\nresolution notes and comments, our system creates a compact knowledge base -\nreducing total volume to just 3.4% of original ticket data while improving\nquality. Experiments demonstrate that our prebuilt knowledge base in RAG\nsystems significantly outperforms traditional RAG implementations (48.74% vs.\n38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.\nBy automating institutional knowledge capture that typically remains siloed in\nexperts' heads, our solution translates to substantial operational efficiency:\nreducing support workload, accelerating resolution times, and creating\nself-improving systems that automatically resolve approximately 50% of future\nsupply chain tickets. Our approach addresses a key gap in knowledge management\nby transforming transient communications into structured, reusable knowledge\nthrough intelligent offline processing rather than latency-inducing runtime\narchitectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u79bb\u7ebf\u65b9\u6cd5\uff0c\u5c06\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u901a\u4fe1\uff08\u5982\u652f\u6301\u5de5\u5355\uff09\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u652f\u6301\u5de5\u4f5c\u91cf\u5e76\u52a0\u901f\u4e86\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u4f9b\u5e94\u94fe\u8fd0\u8425\u4ea7\u751f\u5927\u91cf\u975e\u7ed3\u6784\u5316\u901a\u4fe1\u6570\u636e\uff08\u5982\u652f\u6301\u5de5\u5355\u3001\u90ae\u4ef6\u7b49\uff09\uff0c\u5176\u4e2d\u5305\u542b\u5173\u952e\u77e5\u8bc6\uff08\u5982\u7cfb\u7edf\u4f7f\u7528\u5b9e\u8df5\u3001\u6545\u969c\u6392\u9664\u6d41\u7a0b\u7b49\uff09\uff0c\u4f46\u73b0\u6709RAG\u7cfb\u7edf\u56e0\u6570\u636e\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u800c\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u79bb\u7ebf\u5904\u7406\u5c06\u8fd9\u4e9b\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff0c\u63d0\u5347RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff1a\u5206\u7c7b\u53d1\u73b0\uff08\u7528\u4e8e\u521b\u5efa\u5206\u7c7b\u6cd5\uff09\u3001\u5206\u7c7b\uff08\u7528\u4e8e\u5de5\u5355\u5206\u7ec4\uff09\u548c\u77e5\u8bc6\u5408\u6210\uff08\u7528\u4e8e\u751f\u6210\u77e5\u8bc6\u6587\u7ae0\uff09\u3002\u901a\u8fc7\u79bb\u7ebf\u5904\u7406\u5c06\u975e\u7ed3\u6784\u5316\u901a\u4fe1\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u539f\u59cb\u5de5\u5355\u6570\u636e\u91cf\u51cf\u5c11\u81f33.4%\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u7684\u6027\u80fd\uff08\u6709\u7528\u56de\u7b54\u7387\u4ece38.60%\u63d0\u5347\u81f348.74%\uff09\uff0c\u5e76\u51cf\u5c1177.4%\u7684\u65e0\u7528\u56de\u7b54\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u89e3\u51b3\u4e86\u7ea650%\u7684\u672a\u6765\u4f9b\u5e94\u94fe\u5de5\u5355\u3002", "conclusion": "\u901a\u8fc7\u79bb\u7ebf\u667a\u80fd\u5904\u7406\u5c06\u975e\u7ed3\u6784\u5316\u901a\u4fe1\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u652f\u6301\u5de5\u4f5c\u91cf\u5e76\u52a0\u901f\u4e86\u95ee\u9898\u89e3\u51b3\uff0c\u586b\u8865\u4e86\u77e5\u8bc6\u7ba1\u7406\u7684\u5173\u952e\u7a7a\u767d\u3002", "paper_title_zh": "\u4ece\u975e\u7ed3\u6784\u5316\u901a\u4fe1\u5230\u667a\u80fdRAG\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u4f9b\u5e94\u94fe\u77e5\u8bc6\u5e93\u81ea\u52a8\u5316", "abstract_zh": "\u4f9b\u5e94\u94fe\u8fd0\u8425\u4ea7\u751f\u5927\u91cf\u8fd0\u8425\u6570\u636e\uff0c\u4f46\u5173\u952e\u77e5\u8bc6\uff08\u5982\u7cfb\u7edf\u4f7f\u7528\u5b9e\u8df5\u3001\u6545\u969c\u6392\u9664\u6d41\u7a0b\u7b49\uff09\u901a\u5e38\u57cb\u85cf\u5728\u975e\u7ed3\u6784\u5316\u901a\u4fe1\uff08\u5982\u652f\u6301\u5de5\u5355\u3001\u90ae\u4ef6\u548c\u804a\u5929\u8bb0\u5f55\uff09\u4e2d\u3002\u5c3d\u7ba1RAG\u7cfb\u7edf\u8bd5\u56fe\u5229\u7528\u8fd9\u4e9b\u901a\u4fe1\u4f5c\u4e3a\u77e5\u8bc6\u5e93\uff0c\u4f46\u5176\u6548\u679c\u53d7\u9650\u4e8e\u539f\u59cb\u6570\u636e\u7684\u566a\u58f0\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u5b8c\u6574\u6027\u3002\u4e0e\u73b0\u6709\u4e13\u6ce8\u4e8e\u8fd0\u884c\u65f6\u4f18\u5316\u7684RAG\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u4f18\u5148\u65b9\u6cd5\uff0c\u5c06\u8fd9\u4e9b\u901a\u4fe1\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3002\u6211\u4eec\u7684\u6838\u5fc3\u521b\u65b0\u662f\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u534f\u8c03\u4e09\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff1a\u5206\u7c7b\u53d1\u73b0\uff08\u7528\u4e8e\u521b\u5efa\u5206\u7c7b\u6cd5\uff09\u3001\u5206\u7c7b\uff08\u7528\u4e8e\u5de5\u5355\u5206\u7ec4\uff09\u548c\u77e5\u8bc6\u5408\u6210\uff08\u7528\u4e8e\u751f\u6210\u77e5\u8bc6\u6587\u7ae0\uff09\u3002\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u5305\u542b\u89e3\u51b3\u6ce8\u91ca\u548c\u8bc4\u8bba\u7684\u771f\u5b9e\u652f\u6301\u5de5\u5355\uff0c\u7cfb\u7edf\u521b\u5efa\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u77e5\u8bc6\u5e93\u2014\u2014\u5c06\u603b\u6570\u636e\u91cf\u51cf\u5c11\u81f3\u539f\u59cb\u5de5\u5355\u6570\u636e\u76843.4%\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u9884\u6784\u5efa\u77e5\u8bc6\u5e93\u5728RAG\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5b9e\u73b0\uff08\u6709\u7528\u56de\u7b54\u738748.74% vs. 38.60%\uff09\uff0c\u5e76\u51cf\u5c11\u4e8677.4%\u7684\u65e0\u7528\u56de\u7b54\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u6355\u83b7\u901a\u5e38\u4ec5\u5b58\u5728\u4e8e\u4e13\u5bb6\u5934\u8111\u4e2d\u7684\u673a\u6784\u77e5\u8bc6\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u8425\u6548\u7387\uff1a\u51cf\u5c11\u4e86\u652f\u6301\u5de5\u4f5c\u91cf\u3001\u52a0\u901f\u4e86\u89e3\u51b3\u65f6\u95f4\uff0c\u5e76\u521b\u5efa\u4e86\u80fd\u591f\u81ea\u52a8\u89e3\u51b3\u7ea650%\u672a\u6765\u4f9b\u5e94\u94fe\u5de5\u5355\u7684\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u79bb\u7ebf\u5904\u7406\u800c\u975e\u5ef6\u8fdf\u8bf1\u5bfc\u7684\u8fd0\u884c\u65f6\u67b6\u6784\uff0c\u5c06\u77ac\u6001\u901a\u4fe1\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u91cd\u7528\u7684\u77e5\u8bc6\uff0c\u586b\u8865\u4e86\u77e5\u8bc6\u7ba1\u7406\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2506.17410", "pdf": "https://arxiv.org/pdf/2506.17410", "abs": "https://arxiv.org/abs/2506.17410", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Jionghao Lin", "Sanjit Kakarla", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Ralph Abboud", "Kenneth R. Koedinger"], "title": "Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study", "categories": ["cs.CL", "cs.CY"], "comment": "Short research paper accepted at EC-TEL 2025", "summary": "Tutoring improves student achievement, but identifying and studying what\ntutoring actions are most associated with student learning at scale based on\naudio transcriptions is an open research problem. This present study\ninvestigates the feasibility and scalability of using generative AI to identify\nand evaluate specific tutor moves in real-life math tutoring. We analyze 50\nrandomly selected transcripts of college-student remote tutors assisting middle\nschool students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,\nGemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:\ndelivering effective praise and responding to student math errors. All models\nreliably detected relevant situations, for example, tutors providing praise to\nstudents (94-98% accuracy) and a student making a math error (82-88% accuracy)\nand effectively evaluated the tutors' adherence to tutoring best practices,\naligning closely with human judgments (83-89% and 73-77%, respectively). We\npropose a cost-effective prompting strategy and discuss practical implications\nfor using large language models to support scalable assessment in authentic\nsettings. This work further contributes LLM prompts to support reproducibility\nand research in AI-supported learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u751f\u6210\u5f0fAI\uff08\u5982GPT-4\u3001Gemini-1.5-pro\u7b49\uff09\u8bc6\u522b\u548c\u8bc4\u4f30\u771f\u5b9e\u6570\u5b66\u8f85\u5bfc\u5bf9\u8bdd\u4e2d\u5bfc\u5e08\u884c\u4e3a\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u5bfc\u5e08\u7684\u8868\u626c\u548c\u9519\u8bef\u56de\u5e94\u884c\u4e3a\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u8f85\u5bfc\u80fd\u63d0\u5347\u5b66\u751f\u6210\u7ee9\uff0c\u4f46\u5982\u4f55\u57fa\u4e8e\u97f3\u9891\u8f6c\u5f55\u5927\u89c4\u6a21\u8bc6\u522b\u548c\u7814\u7a76\u4e0e\u5b66\u751f\u5b66\u4e60\u6700\u76f8\u5173\u7684\u8f85\u5bfc\u884c\u4e3a\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u6210\u5f0fAI\u5728\u6b64\u9886\u57df\u7684\u53ef\u884c\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8650\u4efd\u968f\u673a\u9009\u62e9\u7684\u5927\u5b66\u5bfc\u5e08\u8fdc\u7a0b\u8f85\u5bfc\u4e2d\u5b66\u751f\u7684\u6570\u5b66\u5bf9\u8bdd\u8f6c\u5f55\uff0c\u4f7f\u7528GPT-4\u3001GPT-4-turbo\u3001Gemini-1.5-pro\u7b49\u6a21\u578b\u8bc4\u4f30\u5bfc\u5e08\u7684\u4e24\u9879\u6280\u80fd\uff1a\u6709\u6548\u8868\u626c\u548c\u56de\u5e94\u5b66\u751f\u6570\u5b66\u9519\u8bef\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u76f8\u5173\u60c5\u5883\uff08\u5982\u5bfc\u5e08\u8868\u626c\u5b66\u751f\u51c6\u786e\u738794-98%\uff0c\u5b66\u751f\u9519\u8bef\u68c0\u6d4b\u738782-88%\uff09\uff0c\u4e14\u5bf9\u5bfc\u5e08\u884c\u4e3a\u7684\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0883-89%\u548c73-77%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u751f\u6210\u5f0fAI\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u8f85\u5bfc\u884c\u4e3a\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u63d0\u793a\u7b56\u7565\uff0c\u4e3aAI\u652f\u6301\u7684\u771f\u5b9e\u573a\u666f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002", "paper_title_zh": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7684\u5bfc\u5e08\u884c\u4e3a\uff1a\u53ef\u884c\u6027\u7814\u7a76", "abstract_zh": "\u8f85\u5bfc\u80fd\u63d0\u5347\u5b66\u751f\u6210\u7ee9\uff0c\u4f46\u57fa\u4e8e\u97f3\u9891\u8f6c\u5f55\u5927\u89c4\u6a21\u8bc6\u522b\u548c\u7814\u7a76\u4e0e\u5b66\u751f\u5b66\u4e60\u6700\u76f8\u5173\u7684\u8f85\u5bfc\u884c\u4e3a\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u751f\u6210\u5f0fAI\u8bc6\u522b\u548c\u8bc4\u4f30\u771f\u5b9e\u6570\u5b66\u8f85\u5bfc\u4e2d\u5bfc\u5e08\u884c\u4e3a\u7684\u53ef\u884c\u6027\u548c\u6269\u5c55\u6027\u3002\u6211\u4eec\u5206\u6790\u4e8650\u4efd\u968f\u673a\u9009\u62e9\u7684\u5927\u5b66\u5bfc\u5e08\u8fdc\u7a0b\u8f85\u5bfc\u4e2d\u5b66\u751f\u7684\u6570\u5b66\u5bf9\u8bdd\u8f6c\u5f55\uff0c\u4f7f\u7528GPT-4\u3001GPT-4-turbo\u3001Gemini-1.5-pro\u7b49\u6a21\u578b\u8bc4\u4f30\u5bfc\u5e08\u7684\u4e24\u9879\u6280\u80fd\uff1a\u6709\u6548\u8868\u626c\u548c\u56de\u5e94\u5b66\u751f\u6570\u5b66\u9519\u8bef\u3002\u6240\u6709\u6a21\u578b\u5747\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u76f8\u5173\u60c5\u5883\uff08\u5982\u5bfc\u5e08\u8868\u626c\u5b66\u751f\u51c6\u786e\u738794-98%\uff0c\u5b66\u751f\u9519\u8bef\u68c0\u6d4b\u738782-88%\uff09\uff0c\u4e14\u5bf9\u5bfc\u5e08\u884c\u4e3a\u7684\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0883-89%\u548c73-77%\uff09\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u8ba8\u8bba\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u652f\u6301\u771f\u5b9e\u573a\u666f\u8bc4\u4f30\u7684\u5b9e\u8df5\u610f\u4e49\u3002\u672c\u7814\u7a76\u8fd8\u8d21\u732e\u4e86\u652f\u6301\u53ef\u91cd\u590d\u6027\u548cAI\u8f85\u52a9\u5b66\u4e60\u7814\u7a76\u7684LLM\u63d0\u793a\u3002"}}
{"id": "2506.17361", "pdf": "https://arxiv.org/pdf/2506.17361", "abs": "https://arxiv.org/abs/2506.17361", "authors": ["Xufei Wang", "Mingjian Zhang", "Fei Ge", "Jinchen Zhu", "Wen Sha", "Jifen Ren", "Zhimeng Hou", "Shouguo Zheng", "ling Zheng", "Shizhuang Weng"], "title": "Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages,17 figures", "summary": "Even without auxiliary images, single hyperspectral image super-resolution\n(SHSR) methods can be designed to improve the spatial resolution of\nhyperspectral images. However, failing to explore coherence thoroughly along\nbands and spatial-spectral information leads to the limited performance of the\nSHSR. In this study, we propose a novel group-based SHSR method termed the\nefficient feedback gate network, which uses various feedbacks and gate\noperations involving large kernel convolutions and spectral interactions. In\nparticular, by providing different guidance for neighboring groups, we can\nlearn rich band information and hierarchical hyperspectral spatial information\nusing channel shuffling and dilatation convolution in shuffled and progressive\ndilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate\nblock and a spectrum enhancement gate block to construct the spatial-spectral\nreinforcement gate module (SSRGM) and obtain highly representative\nspatial-spectral features efficiently. Additionally, we apply a\nthree-dimensional SSRGM to enhance holistic information and coherence for\nhyperspectral data. The experimental results on three hyperspectral datasets\ndemonstrate the superior performance of the proposed network over the\nstate-of-the-art methods in terms of spectral fidelity and spatial content\nreconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53cd\u9988\u95e8\u7f51\u7edc\uff08EFGN\uff09\uff0c\u7528\u4e8e\u5355\u5e45\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SHSR\uff09\uff0c\u901a\u8fc7\u53cd\u9988\u548c\u95e8\u64cd\u4f5c\u7ed3\u5408\u5927\u6838\u5377\u79ef\u548c\u5149\u8c31\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u5e45\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6ce2\u6bb5\u95f4\u548c\u7a7a\u95f4-\u5149\u8c31\u4fe1\u606f\u7684\u5173\u8054\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u5206\u7ec4\u7684SHSR\u65b9\u6cd5\uff0c\u63d0\u5347\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53cd\u9988\u95e8\u7f51\u7edc\uff08EFGN\uff09\uff0c\u5305\u62ec\u4ee5\u4e0b\u6a21\u5757\uff1a1\uff09\u57fa\u4e8e\u901a\u9053\u6df7\u6d17\u548c\u6269\u5f20\u5377\u79ef\u7684\u6df7\u6d17\u6e10\u8fdb\u6269\u5f20\u878d\u5408\u6a21\u5757\uff08SPDFM\uff09\uff0c\u7528\u4e8e\u5b66\u4e60\u4e30\u5bcc\u7684\u6ce2\u6bb5\u4fe1\u606f\u548c\u5206\u5c42\u7a7a\u95f4\u4fe1\u606f\uff1b2\uff09\u5bbd\u8fb9\u754c\u611f\u77e5\u95e8\u5757\u548c\u5149\u8c31\u589e\u5f3a\u95e8\u5757\uff0c\u6784\u5efa\u7a7a\u95f4-\u5149\u8c31\u5f3a\u5316\u95e8\u6a21\u5757\uff08SSRGM\uff09\uff0c\u9ad8\u6548\u63d0\u53d6\u4ee3\u8868\u6027\u7279\u5f81\uff1b3\uff09\u4e09\u7ef4SSRGM\u589e\u5f3a\u5168\u5c40\u4fe1\u606f\u548c\u5173\u8054\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7f51\u7edc\u5728\u5149\u8c31\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u5185\u5bb9\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684EFGN\u901a\u8fc7\u53cd\u9988\u548c\u95e8\u64cd\u4f5c\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u4e3aSHSR\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u9ad8\u6548\u53cd\u9988\u95e8\u7f51\u7edc\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u5373\u4f7f\u6ca1\u6709\u8f85\u52a9\u56fe\u50cf\uff0c\u5355\u5e45\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SHSR\uff09\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u8bbe\u8ba1\u7528\u4e8e\u63d0\u9ad8\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3002\u7136\u800c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6ce2\u6bb5\u95f4\u548c\u7a7a\u95f4-\u5149\u8c31\u4fe1\u606f\u7684\u5173\u8054\u6027\uff0c\u5bfc\u81f4SHSR\u7684\u6027\u80fd\u53d7\u9650\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u5206\u7ec4\u7684SHSR\u65b9\u6cd5\uff0c\u79f0\u4e3a\u9ad8\u6548\u53cd\u9988\u95e8\u7f51\u7edc\uff0c\u5b83\u5229\u7528\u591a\u79cd\u53cd\u9988\u548c\u95e8\u64cd\u4f5c\uff0c\u5305\u62ec\u5927\u6838\u5377\u79ef\u548c\u5149\u8c31\u4ea4\u4e92\u3002\u5177\u4f53\u800c\u8a00\uff0c\u901a\u8fc7\u4e3a\u76f8\u90bb\u5206\u7ec4\u63d0\u4f9b\u4e0d\u540c\u7684\u6307\u5bfc\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u6df7\u6d17\u6e10\u8fdb\u6269\u5f20\u878d\u5408\u6a21\u5757\uff08SPDFM\uff09\u4e2d\u901a\u8fc7\u901a\u9053\u6df7\u6d17\u548c\u6269\u5f20\u5377\u79ef\u5b66\u4e60\u4e30\u5bcc\u7684\u6ce2\u6bb5\u4fe1\u606f\u548c\u5206\u5c42\u7a7a\u95f4\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u5bbd\u8fb9\u754c\u611f\u77e5\u95e8\u5757\u548c\u5149\u8c31\u589e\u5f3a\u95e8\u5757\uff0c\u6784\u5efa\u4e86\u7a7a\u95f4-\u5149\u8c31\u5f3a\u5316\u95e8\u6a21\u5757\uff08SSRGM\uff09\uff0c\u9ad8\u6548\u83b7\u53d6\u9ad8\u5ea6\u4ee3\u8868\u6027\u7684\u7a7a\u95f4-\u5149\u8c31\u7279\u5f81\u3002\u53e6\u5916\uff0c\u6211\u4eec\u5e94\u7528\u4e09\u7ef4SSRGM\u589e\u5f3a\u9ad8\u5149\u8c31\u6570\u636e\u7684\u5168\u5c40\u4fe1\u606f\u548c\u5173\u8054\u6027\u3002\u5728\u4e09\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7f51\u7edc\u5728\u5149\u8c31\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u5185\u5bb9\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2506.17514", "pdf": "https://arxiv.org/pdf/2506.17514", "abs": "https://arxiv.org/abs/2506.17514", "authors": ["Ninareh Mehrabi", "Tharindu Kumarage", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta"], "title": "Kaleidoscopic Teaming in Multi Agent Simulations", "categories": ["cs.AI"], "comment": null, "summary": "Warning: This paper contains content that may be inappropriate or offensive.\n  AI agents have gained significant recent attention due to their autonomous\ntool usage capabilities and their integration in various real-world\napplications. This autonomy poses novel challenges for the safety of such\nsystems, both in single- and multi-agent scenarios. We argue that existing red\nteaming or safety evaluation frameworks fall short in evaluating safety risks\nin complex behaviors, thought processes and actions taken by agents. Moreover,\nthey fail to consider risks in multi-agent setups where various vulnerabilities\ncan be exposed when agents engage in complex behaviors and interactions with\neach other. To address this shortcoming, we introduce the term kaleidoscopic\nteaming which seeks to capture complex and wide range of vulnerabilities that\ncan happen in agents both in single-agent and multi-agent scenarios. We also\npresent a new kaleidoscopic teaming framework that generates a diverse array of\nscenarios modeling real-world human societies. Our framework evaluates safety\nof agents in both single-agent and multi-agent setups. In single-agent setup,\nan agent is given a scenario that it needs to complete using the tools it has\naccess to. In multi-agent setup, multiple agents either compete against or\ncooperate together to complete a task in the scenario through which we capture\nexisting safety vulnerabilities in agents. We introduce new in-context\noptimization techniques that can be used in our kaleidoscopic teaming framework\nto generate better scenarios for safety analysis. Lastly, we present\nappropriate metrics that can be used along with our framework to measure safety\nof agents. Utilizing our kaleidoscopic teaming framework, we identify\nvulnerabilities in various models with respect to their safety in agentic\nuse-cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f\u201d\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u5316\u7684\u573a\u666f\u548c\u4f18\u5316\u6280\u672f\u8bc6\u522b\u667a\u80fd\u4f53\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7ea2\u961f\u6216\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u590d\u6742\u884c\u4e3a\u548c\u4ea4\u4e92\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u66f4\u5168\u9762\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u201c\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f\u201d\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u751f\u6210\u591a\u6837\u5316\u573a\u666f\u6a21\u62df\u73b0\u5b9e\u793e\u4f1a\uff0c\u8bc4\u4f30\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u7684\u5b89\u5168\u98ce\u9669\u3002\u91c7\u7528\u4e0a\u4e0b\u6587\u4f18\u5316\u6280\u672f\u751f\u6210\u66f4\u4f18\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u5b89\u5168\u5ea6\u91cf\u6307\u6807\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u6210\u529f\u8bc6\u522b\u4e86\u591a\u79cd\u667a\u80fd\u4f53\u6a21\u578b\u5728\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u590d\u6742\u884c\u4e3a\u548c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002", "paper_title_zh": "\u591a\u667a\u80fd\u4f53\u6a21\u62df\u4e2d\u7684\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f", "abstract_zh": "\u8b66\u544a\uff1a\u672c\u6587\u5185\u5bb9\u53ef\u80fd\u5305\u542b\u4e0d\u9002\u5f53\u6216\u5192\u72af\u6027\u5185\u5bb9\u3002\n\u8fd1\u5e74\u6765\uff0cAI\u667a\u80fd\u4f53\u56e0\u5176\u81ea\u4e3b\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u8fd9\u79cd\u81ea\u4e3b\u6027\u4e3a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u65e0\u8bba\u662f\u5355\u667a\u80fd\u4f53\u8fd8\u662f\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u73b0\u6709\u7684\u7ea2\u961f\u6216\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u5728\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u590d\u6742\u884c\u4e3a\u3001\u601d\u7ef4\u8fc7\u7a0b\u548c\u884c\u52a8\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u672a\u80fd\u8003\u8651\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u7684\u98ce\u9669\uff0c\u5f53\u667a\u80fd\u4f53\u8fdb\u884c\u590d\u6742\u884c\u4e3a\u548c\u76f8\u4e92\u4ea4\u4e92\u65f6\uff0c\u53ef\u80fd\u66b4\u9732\u51fa\u591a\u79cd\u6f0f\u6d1e\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f\u201d\u8fd9\u4e00\u672f\u8bed\uff0c\u65e8\u5728\u6355\u6349\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u53ef\u80fd\u53d1\u751f\u7684\u5e7f\u6cdb\u800c\u590d\u6742\u7684\u6f0f\u6d1e\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f\u6846\u67b6\uff0c\u751f\u6210\u591a\u6837\u5316\u573a\u666f\u4ee5\u6a21\u62df\u73b0\u5b9e\u4eba\u7c7b\u793e\u4f1a\u3002\u8be5\u6846\u67b6\u8bc4\u4f30\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u667a\u80fd\u4f53\u7684\u5b89\u5168\u6027\u3002\u5728\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u5b8c\u6210\u4e00\u4e2a\u573a\u666f\u4efb\u52a1\uff1b\u5728\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\uff0c\u591a\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u7ade\u4e89\u6216\u5408\u4f5c\u5b8c\u6210\u4efb\u52a1\uff0c\u4ece\u800c\u63ed\u793a\u5176\u5b89\u5168\u6f0f\u6d1e\u3002\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u6280\u672f\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u4f18\u7684\u5b89\u5168\u5206\u6790\u573a\u666f\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u8be5\u6846\u67b6\u7684\u5ea6\u91cf\u6307\u6807\uff0c\u4ee5\u8861\u91cf\u667a\u80fd\u4f53\u7684\u5b89\u5168\u6027\u3002\u901a\u8fc7\u4e07\u82b1\u7b52\u5f0f\u56e2\u961f\u6846\u67b6\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u591a\u79cd\u6a21\u578b\u5728\u667a\u80fd\u4f53\u7528\u4f8b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002"}}
{"id": "2506.17419", "pdf": "https://arxiv.org/pdf/2506.17419", "abs": "https://arxiv.org/abs/2506.17419", "authors": ["Jinhao Duan", "James Diffenderfer", "Sandeep Madireddy", "Tianlong Chen", "Bhavya Kailkhura", "Kaidi Xu"], "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "19 pages, 5 figures, 4 tables", "summary": "As Large Language Models (LLMs) are integrated into safety-critical\napplications involving sequential decision-making in the real world, it is\nessential to know when to trust LLM decisions. Existing LLM Uncertainty\nQuantification (UQ) methods are primarily designed for single-turn\nquestion-answering formats, resulting in multi-step decision-making scenarios,\ne.g., LLM agentic system, being underexplored. In this paper, we introduce a\nprincipled, information-theoretic framework that decomposes LLM sequential\ndecision uncertainty into two parts: (i) internal uncertainty intrinsic to the\ncurrent decision, which is focused on existing UQ methods, and (ii) extrinsic\nuncertainty, a Mutual-Information (MI) quantity describing how much uncertainty\nshould be inherited from preceding decisions. We then propose UProp, an\nefficient and effective extrinsic uncertainty estimator that converts the\ndirect estimation of MI to the estimation of Pointwise Mutual Information (PMI)\nover multiple Trajectory-Dependent Decision Processes (TDPs). UProp is\nevaluated over extensive multi-step decision-making benchmarks, e.g.,\nAgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and\nDeepSeek-V3. Experimental results demonstrate that UProp significantly\noutperforms existing single-turn UQ baselines equipped with thoughtful\naggregation strategies. Moreover, we provide a comprehensive analysis of UProp,\nincluding sampling efficiency, potential applications, and intermediate\nuncertainty propagation, to demonstrate its effectiveness. Codes will be\navailable at https://github.com/jinhaoduan/UProp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUProp\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u6b65\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5185\u90e8\u548c\u5916\u90e8\u4e24\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u5355\u8f6e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u5e94\u7528\u4e8e\u6d89\u53ca\u591a\u6b65\u51b3\u7b56\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u4e86\u89e3\u4f55\u65f6\u4fe1\u4efbLLM\u7684\u51b3\u7b56\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u8f6e\u95ee\u7b54\u573a\u666f\uff0c\u591a\u6b65\u51b3\u7b56\u573a\u666f\u7684\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6\uff0c\u5c06LLM\u5e8f\u5217\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\uff08\u901a\u8fc7\u4e92\u4fe1\u606f\u91cf\u5316\uff09\u3002UProp\u901a\u8fc7\u5c06\u4e92\u4fe1\u606f\u4f30\u8ba1\u8f6c\u5316\u4e3a\u8f68\u8ff9\u4f9d\u8d56\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u70b9\u4e92\u4fe1\u606f\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUProp\u5728\u591a\u6b65\u51b3\u7b56\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982AgentBench\u548cHotpotQA\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8f6e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u91c7\u6837\u6548\u7387\u3001\u6f5c\u5728\u5e94\u7528\u548c\u4e2d\u95f4\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u3002", "conclusion": "UProp\u4e3a\u591a\u6b65\u51b3\u7b56\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "UProp\uff1a\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u4ee3\u7406\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u96c6\u6210\u5230\u6d89\u53ca\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u6b65\u51b3\u7b56\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u4e86\u89e3\u4f55\u65f6\u4fe1\u4efbLLM\u7684\u51b3\u7b56\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684LLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u8f6e\u95ee\u7b54\u683c\u5f0f\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u591a\u6b65\u51b3\u7b56\u573a\u666f\uff08\u5982LLM\u4ee3\u7406\u7cfb\u7edf\uff09\u7684\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6\uff0c\u5c06LLM\u5e8f\u5217\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u4e24\u90e8\u5206\uff1a\uff08i\uff09\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u5373\u5f53\u524d\u51b3\u7b56\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u73b0\u6709UQ\u65b9\u6cd5\u7684\u91cd\u70b9\uff09\uff0c\u4ee5\u53ca\uff08ii\uff09\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u4e00\u79cd\u4e92\u4fe1\u606f\u91cf\uff0c\u63cf\u8ff0\u4ece\u524d\u5e8f\u51b3\u7b56\u4e2d\u5e94\u7ee7\u627f\u591a\u5c11\u4e0d\u786e\u5b9a\u6027\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86UProp\uff0c\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\uff0c\u5c06\u4e92\u4fe1\u606f\u7684\u76f4\u63a5\u4f30\u8ba1\u8f6c\u5316\u4e3a\u5728\u591a\u4e2a\u8f68\u8ff9\u4f9d\u8d56\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5bf9\u70b9\u4e92\u4fe1\u606f\u7684\u4f30\u8ba1\u3002UProp\u5728\u5e7f\u6cdb\u7684\u591a\u6b65\u51b3\u7b56\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982AgentBench\u548cHotpotQA\uff09\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u6700\u5148\u8fdb\u7684LLM\uff08\u5982GPT-4.1\u548cDeepSeek-V3\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUProp\u663e\u8457\u4f18\u4e8e\u914d\u5907\u4e86\u6df1\u601d\u719f\u8651\u805a\u5408\u7b56\u7565\u7684\u73b0\u6709\u5355\u8f6eUQ\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9UProp\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u91c7\u6837\u6548\u7387\u3001\u6f5c\u5728\u5e94\u7528\u548c\u4e2d\u95f4\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u4ee5\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002\u4ee3\u7801\u5c06\u5728https://github.com/jinhaoduan/UProp \u4e0a\u63d0\u4f9b\u3002"}}
{"id": "2506.17374", "pdf": "https://arxiv.org/pdf/2506.17374", "abs": "https://arxiv.org/abs/2506.17374", "authors": ["Muhammad Tayyab Khan", "Lequn Chen", "Zane Yong", "Jun Ming Tan", "Wenhe Feng", "Seung Ki Moon"], "title": "From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "Preprint submitted to Elsevier", "summary": "Efficient and accurate extraction of key information from 2D engineering\ndrawings is essential for advancing digital manufacturing workflows. Such\ninformation includes geometric dimensioning and tolerancing (GD&T), measures,\nmaterial specifications, and textual annotations. Manual extraction is slow and\nlabor-intensive, while generic OCR models often fail due to complex layouts,\nengineering symbols, and rotated text, leading to incomplete and unreliable\noutputs. These limitations result in incomplete and unreliable outputs. To\naddress these challenges, we propose a hybrid vision-language framework that\nintegrates a rotation-aware object detection model (YOLOv11-obb) with a\ntransformer-based vision-language parser. Our structured pipeline applies\nYOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)\npatches, which are then parsed into structured outputs using a fine-tuned,\nlightweight vision-language model (VLM). We curate a dataset of 1,367 2D\nmechanical drawings annotated across nine key categories. YOLOv11-OBB is\ntrained on this dataset to detect OBBs and extract annotation patches. These\nare parsed using two open-source VLMs: Donut and Florence-2. Both models are\nlightweight and well-suited for specialized industrial tasks under limited\ncomputational overhead. Following fine-tuning of both models on the curated\ndataset of image patches paired with structured annotation labels, a\ncomparative experiment is conducted to evaluate parsing performance across four\nkey metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%\nrecall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a\ncase study demonstrates how the extracted structured information supports\ndownstream manufacturing tasks such as process and tool selection, showcasing\nthe practical utility of the proposed framework in modernizing 2D drawing\ninterpretation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece2D\u5de5\u7a0b\u56fe\u7eb8\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u5236\u9020\u77e5\u8bc6\uff0c\u7ed3\u5408\u65cb\u8f6c\u611f\u77e5\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u89e3\u6790\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u63d0\u53d62D\u5de5\u7a0b\u56fe\u7eb8\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff08\u5982\u51e0\u4f55\u5c3a\u5bf8\u4e0e\u516c\u5dee\u3001\u6750\u6599\u89c4\u683c\u7b49\uff09\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\uff0c\u800c\u901a\u7528OCR\u6a21\u578b\u56e0\u590d\u6742\u5e03\u5c40\u548c\u5de5\u7a0b\u7b26\u53f7\u7b49\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u652f\u6301\u6570\u5b57\u5316\u5236\u9020\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408YOLOv11-OBB\uff08\u65cb\u8f6c\u611f\u77e5\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff09\u548c\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8bed\u8a00\u89e3\u6790\u5668\u3002\u9996\u5148\u4f7f\u7528YOLOv11-OBB\u5b9a\u4f4d\u6807\u6ce8\u5e76\u63d0\u53d6\u5b9a\u5411\u8fb9\u754c\u6846\uff08OBB\uff09\u56fe\u50cf\u5757\uff0c\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e861,367\u5f202D\u673a\u68b0\u56fe\u7eb8\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4e86Donut\u548cFlorence-2\u4e24\u79cdVLM\u7684\u6027\u80fd\u3002", "result": "Donut\u6a21\u578b\u8868\u73b0\u4f18\u4e8eFlorence-2\uff0c\u8fbe\u523088.5%\u7684\u7cbe\u786e\u7387\u300199.2%\u7684\u53ec\u56de\u7387\u548c93.5%\u7684F1\u5206\u6570\uff0c\u5e7b\u89c9\u7387\u4e3a11.5%\u3002\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u63d0\u53d6\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u53ef\u6709\u6548\u652f\u6301\u4e0b\u6e38\u5236\u9020\u4efb\u52a1\uff08\u5982\u5de5\u827a\u548c\u5de5\u5177\u9009\u62e9\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e862D\u5de5\u7a0b\u56fe\u7eb8\u89e3\u6790\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u6570\u5b57\u5316\u5236\u9020\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002", "paper_title_zh": "\u4ece\u56fe\u7eb8\u5230\u51b3\u7b56\uff1a\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\u7528\u4e8e\u5c062D\u5de5\u7a0b\u56fe\u7eb8\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u5236\u9020\u77e5\u8bc6", "abstract_zh": "\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u4ece2D\u5de5\u7a0b\u56fe\u7eb8\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff08\u5982\u51e0\u4f55\u5c3a\u5bf8\u4e0e\u516c\u5dee\u3001\u6d4b\u91cf\u503c\u3001\u6750\u6599\u89c4\u683c\u548c\u6587\u672c\u6807\u6ce8\uff09\u5bf9\u63a8\u52a8\u6570\u5b57\u5316\u5236\u9020\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u624b\u52a8\u63d0\u53d6\u7f13\u6162\u4e14\u8d39\u529b\uff0c\u800c\u901a\u7528OCR\u6a21\u578b\u5e38\u56e0\u590d\u6742\u5e03\u5c40\u3001\u5de5\u7a0b\u7b26\u53f7\u548c\u65cb\u8f6c\u6587\u672c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u5b8c\u6574\u4e14\u4e0d\u53ef\u9760\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u7ed3\u5408\u65cb\u8f6c\u611f\u77e5\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08YOLOv11-OBB\uff09\u548c\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8bed\u8a00\u89e3\u6790\u5668\u3002\u7ed3\u6784\u5316\u6d41\u7a0b\u9996\u5148\u5e94\u7528YOLOv11-OBB\u5b9a\u4f4d\u6807\u6ce8\u5e76\u63d0\u53d6\u5b9a\u5411\u8fb9\u754c\u6846\uff08OBB\uff09\u56fe\u50cf\u5757\uff0c\u968f\u540e\u901a\u8fc7\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u3002\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u5305\u542b1,367\u5f202D\u673a\u68b0\u56fe\u7eb8\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e5d\u7c7b\u5173\u952e\u4fe1\u606f\u3002YOLOv11-OBB\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u68c0\u6d4bOBB\u5e76\u63d0\u53d6\u6807\u6ce8\u56fe\u50cf\u5757\uff0c\u968f\u540e\u4f7f\u7528\u4e24\u79cd\u5f00\u6e90VLM\uff08Donut\u548cFlorence-2\uff09\u8fdb\u884c\u89e3\u6790\u3002\u8fd9\u4e24\u79cd\u6a21\u578b\u5747\u4e3a\u8f7b\u91cf\u7ea7\uff0c\u9002\u5408\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b8c\u6210\u5de5\u4e1a\u4efb\u52a1\u3002\u5fae\u8c03\u540e\uff0c\u901a\u8fc7\u56db\u9879\u5173\u952e\u6307\u6807\u8bc4\u4f30\u89e3\u6790\u6027\u80fd\u3002Donut\u8868\u73b0\u4f18\u4e8eFlorence-2\uff0c\u8fbe\u523088.5%\u7684\u7cbe\u786e\u7387\u300199.2%\u7684\u53ec\u56de\u7387\u548c93.5%\u7684F1\u5206\u6570\uff0c\u5e7b\u89c9\u7387\u4e3a11.5%\u3002\u6700\u540e\uff0c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u63d0\u53d6\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u5982\u4f55\u652f\u6301\u4e0b\u6e38\u5236\u9020\u4efb\u52a1\uff08\u5982\u5de5\u827a\u548c\u5de5\u5177\u9009\u62e9\uff09\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u73b0\u4ee3\u53162D\u56fe\u7eb8\u89e3\u6790\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.17585", "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u9700\u68c0\u7d22\u7684\u60c5\u51b5\u4e0b\u53ef\u9760\u5730\u5f15\u7528\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u5fae\u8c03\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e3b\u52a8\u7d22\u5f15\u6cd5\u4f18\u4e8e\u88ab\u52a8\u7d22\u5f15\u6cd5\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5f15\u7528\u9884\u8bad\u7ec3\u6570\u636e\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u800c\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u5668\u4f1a\u5f15\u5165\u5ef6\u8fdf\u548c\u566a\u58f0\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4f7f\u6a21\u578b\u65e0\u9700\u68c0\u7d22\u5373\u53ef\u53ef\u9760\u5f15\u7528\u6570\u636e\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5c06\u4e8b\u5b9e\u4e0e\u6587\u6863\u6807\u8bc6\u7b26\u7ed1\u5b9a\uff1b2) \u6307\u4ee4\u5fae\u8c03\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u5f15\u7528\u884c\u4e3a\u3002\u63d0\u51fa\u4e3b\u52a8\u7d22\u5f15\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u591a\u6837\u5316QA\u5bf9\u589e\u5f3a\u6a21\u578b\u7684\u53cc\u5411\u751f\u6210\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u52a8\u7d22\u5f15\u6cd5\u5728Qwen2.5-7B\u548c3B\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u88ab\u52a8\u7d22\u5f15\u6cd5\uff0c\u5f15\u7528\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe30.2%\u3002\u6570\u636e\u589e\u5f3a\u89c4\u6a21\u8d8a\u5927\uff0c\u6027\u80fd\u63d0\u5347\u8d8a\u660e\u663e\u3002", "conclusion": "\u4e3b\u52a8\u7d22\u5f15\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5f15\u7528\u9884\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u4e14\u6027\u80fd\u968f\u6570\u636e\u589e\u5f3a\u89c4\u6a21\u589e\u52a0\u800c\u6301\u7eed\u63d0\u5347\u3002", "paper_title_zh": "\u5f15\u7528\u9884\u8bad\u7ec3\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u68c0\u7d22\u77e5\u8bc6\u5f52\u56e0", "abstract_zh": "\u53ef\u4fe1\u7684\u8bed\u8a00\u6a21\u578b\u5e94\u63d0\u4f9b\u6b63\u786e\u4e14\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u3002\u867d\u7136\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u80fd\u5c06\u5176\u8f93\u51fa\u5f52\u56e0\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5176\u5f15\u7528\u5e38\u56e0\u5e7b\u89c9\u800c\u4e0d\u53ef\u9760\u3002\u56e0\u6b64\uff0c\u73b0\u6709\u7cfb\u7edf\u901a\u8fc7\u63a8\u7406\u65f6\u67e5\u8be2\u5916\u90e8\u68c0\u7d22\u5668\u63d2\u5165\u5f15\u7528\uff0c\u8fd9\u5f15\u5165\u4e86\u5ef6\u8fdf\u3001\u57fa\u7840\u8bbe\u65bd\u4f9d\u8d56\u6027\u548c\u68c0\u7d22\u566a\u58f0\u7684\u8106\u5f31\u6027\u3002\u6211\u4eec\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u5f52\u56e0\u4e8e\uff08\u6301\u7eed\uff09\u9884\u8bad\u7ec3\u671f\u95f4\u770b\u5230\u7684\u6587\u6863\u2014\u2014\u800c\u65e0\u9700\u6d4b\u8bd5\u65f6\u68c0\u7d22\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u53d1\u5e03\u4e86CitePretrainBench\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6df7\u5408\u4e86\u771f\u5b9e\u8bed\u6599\u5e93\uff08\u7ef4\u57fa\u767e\u79d1\u3001Common Crawl\u3001arXiv\uff09\u548c\u672a\u89c1\u6587\u6863\uff0c\u5e76\u6d4b\u8bd5\u4e86\u77ed\u5f62\u5f0f\uff08\u5355\u4e8b\u5b9e\uff09\u548c\u957f\u5f62\u5f0f\uff08\u591a\u4e8b\u5b9e\uff09\u5f15\u7528\u4efb\u52a1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a(1) \u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5c06\u4e8b\u5b9e\u4e0e\u6301\u4e45\u6587\u6863\u6807\u8bc6\u7b26\u7ed1\u5b9a\uff1b(2) \u6307\u4ee4\u5fae\u8c03\uff0c\u5f15\u5bfc\u5f15\u7528\u884c\u4e3a\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u88ab\u52a8\u7d22\u5f15\u6cd5\uff08\u4e3a\u6bcf\u7bc7\u6587\u6863\u9644\u52a0\u6807\u8bc6\u7b26\uff09\u6709\u52a9\u4e8e\u8bb0\u5fc6\u9010\u5b57\u6587\u672c\uff0c\u4f46\u5728\u8f6c\u8ff0\u6216\u7ec4\u5408\u4e8b\u5b9e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e3b\u52a8\u7d22\u5f15\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u5408\u6210\u591a\u6837\u5316QA\u5bf9\uff0c\u8981\u6c42\u53cc\u5411\u751f\u6210\uff08\u4ece\u6e90\u5230\u4e8b\u5b9e\u548c\u4ece\u4e8b\u5b9e\u5230\u6e90\uff09\uff0c\u5171\u540c\u8bad\u7ec3\u6a21\u578b\u4ece\u5f15\u7528\u6e90\u751f\u6210\u5185\u5bb9\u5e76\u5f52\u56e0\u5176\u7b54\u6848\u3002\u5728Qwen2.5-7B\u548c3B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u52a8\u7d22\u5f15\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u88ab\u52a8\u7d22\u5f15\u6cd5\uff0c\u5f15\u7528\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe30.2%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u968f\u7740\u589e\u5f3a\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u5373\u4f7f\u5728\u539f\u59cb\u6807\u8bb0\u91cf\u768416\u500d\u65f6\u4ecd\u5448\u73b0\u660e\u663e\u4e0a\u5347\u8d8b\u52bf\u3002"}}
{"id": "2506.17435", "pdf": "https://arxiv.org/pdf/2506.17435", "abs": "https://arxiv.org/abs/2506.17435", "authors": ["Alberto Martinez-Serra", "Alejandro De La Fuente", "Nienke Viescher", "Ana S. Cardenal"], "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media", "categories": ["cs.CL"], "comment": null, "summary": "The use of large language models (LLMs) is becoming common in the context of\npolitical science, particularly in studies that analyse individuals use of\ndigital media. However, while previous research has demonstrated LLMs ability\nat labelling tasks, the effectiveness of using LLMs to classify political\ncontent (PC) from just URLs is not yet well explored. The work presented in\nthis article bridges this gap by evaluating whether LLMs can accurately\nidentify PC vs. non-PC from both the article text and the URLs from five\ncountries (France, Germany, Spain, the UK, and the US) and different languages.\nUsing cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we\nmeasure model performance to assess whether URL-level analysis can be a good\napproximation for full-text analysis of PC, even across different linguistic\nand national contexts. Model outputs are compared with human-labelled articles,\nas well as traditional supervised machine learning techniques, to set a\nbaseline of performance. Overall, our findings suggest the capacity of URLs to\nembed most of the news content, providing a vital perspective on accuracy-cost\nbalancing. We also account for contextual limitations and suggest\nmethodological recommendations to use LLMs within political science studies.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ec5\u901a\u8fc7URL\u5206\u7c7b\u653f\u6cbb\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u53d1\u73b0URL\u80fd\u6709\u6548\u5d4c\u5165\u65b0\u95fb\u5185\u5bb9\uff0c\u4e3a\u653f\u6cbb\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u89c6\u89d2\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u4ec5\u901a\u8fc7URL\u5206\u7c7b\u653f\u6cbb\u5185\u5bb9\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8URL\u5206\u6790\u662f\u5426\u80fd\u66ff\u4ee3\u5168\u6587\u5206\u6790\u3002", "method": "\u4f7f\u7528GPT\u3001Llama\u3001Mistral\u7b49\u5148\u8fdbLLMs\uff0c\u5bf9\u6765\u81ea\u6cd5\u3001\u5fb7\u3001\u897f\u3001\u82f1\u3001\u7f8e\u4e94\u56fd\u7684\u591a\u8bed\u8a00\u65b0\u95fbURL\u548c\u6587\u672c\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u53ca\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u53d1\u73b0URL\u80fd\u6709\u6548\u5d4c\u5165\u65b0\u95fb\u5185\u5bb9\uff0c\u4e3a\u653f\u6cbb\u5185\u5bb9\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6027\u4ef7\u6bd4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u548c\u56fd\u5bb6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "URL\u5206\u6790\u53ef\u4f5c\u4e3a\u5168\u6587\u5206\u6790\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u4e3a\u653f\u6cbb\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u5efa\u8bae\uff0c\u4f46\u9700\u8003\u8651\u4e0a\u4e0b\u6587\u9650\u5236\u3002", "paper_title_zh": "\u8d85\u8d8a\u94fe\u63a5\uff1a\u8bc4\u4f30LLMs\u5728\u5168\u7403\u5a92\u4f53\u4e2d\u5206\u7c7b\u653f\u6cbb\u5185\u5bb9\u7684\u80fd\u529b", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u653f\u6cbb\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u666e\u904d\uff0c\u5c24\u5176\u662f\u5728\u5206\u6790\u6570\u5b57\u5a92\u4f53\u4f7f\u7528\u7684\u573a\u666f\u4e2d\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u8bc1\u660e\u4e86LLMs\u5728\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4f46\u4ec5\u901a\u8fc7URL\u5206\u7c7b\u653f\u6cbb\u5185\u5bb9\uff08PC\uff09\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u901a\u8fc7\u8bc4\u4f30LLMs\u80fd\u5426\u4ece\u6587\u7ae0\u6587\u672c\u548cURL\u4e2d\u51c6\u786e\u8bc6\u522bPC\u4e0e\u975ePC\u5185\u5bb9\uff08\u8986\u76d6\u6cd5\u56fd\u3001\u5fb7\u56fd\u3001\u897f\u73ed\u7259\u3001\u82f1\u56fd\u548c\u7f8e\u56fd\u7684\u591a\u8bed\u8a00\u6570\u636e\uff09\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u4f7f\u7528GPT\u3001Llama\u3001Mistral\u3001Deepseek\u3001Qwen\u548cGemma\u7b49\u5148\u8fdbLLMs\uff0c\u6211\u4eec\u6d4b\u91cf\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u8bc4\u4f30URL\u5206\u6790\u662f\u5426\u80fd\u4f5c\u4e3a\u5168\u6587\u5206\u6790\u7684\u6709\u6548\u8fd1\u4f3c\uff0c\u5c24\u5176\u662f\u5728\u8de8\u8bed\u8a00\u548c\u56fd\u5bb6\u80cc\u666f\u4e0b\u3002\u6a21\u578b\u8f93\u51fa\u4e0e\u4eba\u5de5\u6807\u6ce8\u6587\u7ae0\u53ca\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u8bbe\u5b9a\u4e86\u6027\u80fd\u57fa\u51c6\u3002\u603b\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u53d1\u73b0URL\u80fd\u6709\u6548\u5d4c\u5165\u5927\u90e8\u5206\u65b0\u95fb\u5185\u5bb9\uff0c\u4e3a\u51c6\u786e\u6027-\u6210\u672c\u5e73\u8861\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\u3002\u540c\u65f6\uff0c\u672c\u6587\u8fd8\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u5e76\u4e3a\u653f\u6cbb\u79d1\u5b66\u7814\u7a76\u4e2d\u4f7f\u7528LLMs\u63d0\u51fa\u4e86\u65b9\u6cd5\u8bba\u5efa\u8bae\u3002"}}
{"id": "2506.17403", "pdf": "https://arxiv.org/pdf/2506.17403", "abs": "https://arxiv.org/abs/2506.17403", "authors": ["Zhiyi Shi", "Junsik Kim", "Helen Y. Yang", "Yonghyun Song", "Hyun-Jic Oh", "Dalit Ben-Yosef", "Daniel Needleman", "Hanspeter Pfister"], "title": "Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos", "categories": ["cs.CV"], "comment": "Preprint submitted to Medical Image Analysis", "summary": "Automating embryo viability prediction for in vitro fertilization (IVF) is\nimportant but challenging due to the limited availability of labeled pregnancy\noutcome data, as only a small fraction of embryos are labeled after transfer.\nSelf-supervised learning (SSL) can leverage both labeled and unlabeled data to\nimprove prediction. However, existing SSL methods for videos are not directly\napplicable to embryo development videos due to two challenges: (1) embryo\ntime-lapse videos contain hundreds of frames, requiring significant GPU memory\nfor conventional SSL; (2) the dataset contains videos with varying lengths and\nmany outlier frames, causing traditional video alignment methods to struggle\nwith semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to\naddress these challenges. STPT includes two stages: spatial and temporal. In\neach stage, only one encoder is trained while the other is frozen, reducing\nmemory demands. To handle temporal misalignment, STPT avoids frame-by-frame\nalignment across videos. The spatial stage learns from alignments within each\nvideo and its temporally consistent augmentations. The temporal stage then\nmodels relationships between video embeddings. Our method efficiently handles\nlong videos and temporal variability. On 23,027 time-lapse videos (3,286\nlabeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared\nto baselines, with limited computational resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65f6\u7a7a\u9884\u8bad\u7ec3\uff08STPT\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u80da\u80ce\u53d1\u80b2\u89c6\u9891\u7684\u751f\u5b58\u80fd\u529b\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u548c\u65f6\u5e8f\u4e0d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5e76\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4f53\u5916\u53d7\u7cbe\uff08IVF\uff09\u4e2d\u80da\u80ce\u751f\u5b58\u80fd\u529b\u7684\u81ea\u52a8\u5316\u9884\u6d4b\u56e0\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u89c6\u9891\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u80da\u80ce\u53d1\u80b2\u89c6\u9891\uff0c\u56e0\u5176\u5e27\u6570\u591a\u4e14\u65f6\u5e8f\u4e0d\u4e00\u81f4\u3002", "method": "STPT\u5206\u4e3a\u7a7a\u95f4\u548c\u65f6\u5e8f\u4e24\u9636\u6bb5\uff1a\u7a7a\u95f4\u9636\u6bb5\u5b66\u4e60\u5355\u89c6\u9891\u5185\u5bf9\u9f50\u548c\u589e\u5f3a\uff0c\u65f6\u5e8f\u9636\u6bb5\u5efa\u6a21\u89c6\u9891\u5d4c\u5165\u95f4\u5173\u7cfb\u3002\u901a\u8fc7\u51bb\u7ed3\u4e00\u4e2a\u7f16\u7801\u5668\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u907f\u514d\u8de8\u89c6\u9891\u5e27\u5bf9\u9f50\u3002", "result": "\u572823,027\u4e2a\u80da\u80ce\u89c6\u9891\uff083,286\u4e2a\u6807\u8bb0\uff09\u4e0a\uff0cSTPT\u4ee50.635\u7684AUC\uff0895% CI: 0.632-0.638\uff09\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002", "conclusion": "STPT\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u548c\u65f6\u5e8f\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u80da\u80ce\u751f\u5b58\u80fd\u529b\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u65f6\u7a7a\u9884\u8bad\u7ec3\u7684\u80da\u80ce\u751f\u5b58\u80fd\u529b\u9884\u6d4b\u65b9\u6cd5\uff1a\u5229\u7528\u5ef6\u65f6\u6444\u5f71\u89c6\u9891", "abstract_zh": "\u4f53\u5916\u53d7\u7cbe\uff08IVF\uff09\u4e2d\u80da\u80ce\u751f\u5b58\u80fd\u529b\u7684\u81ea\u52a8\u5316\u9884\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u7531\u4e8e\u4ec5\u6709\u5c11\u91cf\u80da\u80ce\u5728\u79fb\u690d\u540e\u88ab\u6807\u8bb0\uff0c\u6807\u8bb0\u6570\u636e\u6709\u9650\uff0c\u8fd9\u4e00\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u53ef\u4ee5\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u89c6\u9891SSL\u65b9\u6cd5\u56e0\u4ee5\u4e0b\u4e24\u4e2a\u6311\u6218\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u80da\u80ce\u53d1\u80b2\u89c6\u9891\uff1a\uff081\uff09\u80da\u80ce\u5ef6\u65f6\u89c6\u9891\u5305\u542b\u6570\u767e\u5e27\uff0c\u4f20\u7edfSSL\u9700\u8981\u5927\u91cfGPU\u5185\u5b58\uff1b\uff082\uff09\u6570\u636e\u96c6\u89c6\u9891\u957f\u5ea6\u4e0d\u4e00\u4e14\u5b58\u5728\u5f02\u5e38\u5e27\uff0c\u4f20\u7edf\u89c6\u9891\u5bf9\u9f50\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8bed\u4e49\u4e0d\u5bf9\u9f50\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u65f6\u7a7a\u9884\u8bad\u7ec3\uff08STPT\uff09\u65b9\u6cd5\u3002STPT\u5206\u4e3a\u7a7a\u95f4\u548c\u65f6\u5e8f\u4e24\u9636\u6bb5\uff1a\u6bcf\u9636\u6bb5\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u7f16\u7801\u5668\u5e76\u51bb\u7ed3\u53e6\u4e00\u4e2a\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u3002\u4e3a\u5904\u7406\u65f6\u5e8f\u4e0d\u5bf9\u9f50\uff0cSTPT\u907f\u514d\u8de8\u89c6\u9891\u9010\u5e27\u5bf9\u9f50\u3002\u7a7a\u95f4\u9636\u6bb5\u5b66\u4e60\u5355\u89c6\u9891\u5185\u5bf9\u9f50\u53ca\u5176\u65f6\u5e8f\u4e00\u81f4\u7684\u589e\u5f3a\uff0c\u65f6\u5e8f\u9636\u6bb5\u5efa\u6a21\u89c6\u9891\u5d4c\u5165\u95f4\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u9ad8\u6548\u5904\u7406\u957f\u89c6\u9891\u548c\u65f6\u5e8f\u53d8\u5f02\u6027\u3002\u572823,027\u4e2a\u5ef6\u65f6\u89c6\u9891\uff083,286\u4e2a\u6807\u8bb0\uff09\u4e0a\uff0cSTPT\u4ee50.635\u7684AUC\uff0895% CI: 0.632-0.638\uff09\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6709\u9650\u3002"}}
{"id": "2506.17589", "pdf": "https://arxiv.org/pdf/2506.17589", "abs": "https://arxiv.org/abs/2506.17589", "authors": ["Bowen Wang"], "title": "Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown", "categories": ["cs.AI"], "comment": null, "summary": "The real value of knowledge lies not just in its accumulation, but in its\npotential to be harnessed effectively to conquer the unknown. Although recent\nmultimodal large language models (MLLMs) exhibit impressing multimodal\ncapabilities, they often fail in rarely encountered domain-specific tasks due\nto limited relevant knowledge. To explore this, we adopt visual game cognition\nas a testbed and select Monster Hunter: World as the target to construct a\nmultimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and\nintricate entity relations. We also design a series of challenging queries\nbased on MH-MMKG to evaluate the models' ability for complex knowledge\nretrieval and reasoning. Furthermore, we propose a multi-agent retriever that\nenables a model to autonomously search relevant knowledge without additional\ntraining. Experimental results show that our approach significantly enhances\nthe performance of MLLMs, providing a new perspective on multimodal\nknowledge-augmented reasoning and laying a solid foundation for future\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MH-MMKG\uff09\u548c\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7f55\u89c1\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7f55\u89c1\u9886\u57df\u4efb\u52a1\u4e2d\u5e38\u56e0\u76f8\u5173\u77e5\u8bc6\u4e0d\u8db3\u800c\u5931\u8d25\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u589e\u5f3aMLLMs\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u672a\u77e5\u9886\u57df\u7684\u6311\u6218\u3002", "method": "\u4ee5\u89c6\u89c9\u6e38\u620f\u8ba4\u77e5\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6784\u5efa\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MH-MMKG\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u590d\u6742\u67e5\u8be2\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u5668\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u641c\u7d22\u76f8\u5173\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u590d\u6742\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u589e\u5f3a\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aMLLMs\u5728\u7f55\u89c1\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u9a6f\u670d\u672a\u77e5\uff1a\u57fa\u4e8e\u56fe\u7684\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u52a9\u529b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5f81\u670d\u672a\u77e5\u9886\u57df", "abstract_zh": "\u77e5\u8bc6\u7684\u771f\u6b63\u4ef7\u503c\u4e0d\u4ec5\u5728\u4e8e\u5176\u79ef\u7d2f\uff0c\u66f4\u5728\u4e8e\u5176\u88ab\u6709\u6548\u5229\u7528\u4ee5\u5f81\u670d\u672a\u77e5\u7684\u6f5c\u529b\u3002\u5c3d\u7ba1\u8fd1\u671f\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5c55\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4f46\u5728\u7f55\u89c1\u9886\u57df\u4efb\u52a1\u4e2d\u5e38\u56e0\u76f8\u5173\u77e5\u8bc6\u6709\u9650\u800c\u5931\u8d25\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4ee5\u89c6\u89c9\u6e38\u620f\u8ba4\u77e5\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9009\u62e9\u300a\u602a\u7269\u730e\u4eba\uff1a\u4e16\u754c\u300b\u4e3a\u76ee\u6807\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u6a21\u6001\u548c\u590d\u6742\u5b9e\u4f53\u5173\u7cfb\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MH-MMKG\uff09\u3002\u6211\u4eec\u8fd8\u57fa\u4e8eMH-MMKG\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u67e5\u8be2\u4efb\u52a1\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u5668\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u81ea\u4e3b\u641c\u7d22\u76f8\u5173\u77e5\u8bc6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u589e\u5f3a\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.17459", "pdf": "https://arxiv.org/pdf/2506.17459", "abs": "https://arxiv.org/abs/2506.17459", "authors": ["Siyu Liang", "Gina-Anne Levow"], "title": "Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages", "categories": ["cs.CL"], "comment": null, "summary": "Automatic Speech Recognition (ASR) has reached impressive accuracy for\nhigh-resource languages, yet its utility in linguistic fieldwork remains\nlimited. Recordings collected in fieldwork contexts present unique challenges,\nincluding spontaneous speech, environmental noise, and severely constrained\ndatasets from under-documented languages. In this paper, we benchmark the\nperformance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five\ntypologically diverse low-resource languages with control of training data\nduration. Our findings show that MMS is best suited when extremely small\namounts of training data are available, whereas XLS-R shows parity performance\nonce training data exceed one hour. We provide linguistically grounded analysis\nfor further provide insights towards practical guidelines for field linguists,\nhighlighting reproducible ASR adaptation approaches to mitigate the\ntranscription bottleneck in language documentation.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u591a\u8bed\u8a00ASR\u6a21\u578b\uff08MMS\u548cXLS-R\uff09\u5728\u4e94\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0MMS\u5728\u6781\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u800cXLS-R\u5728\u6570\u636e\u8d85\u8fc7\u4e00\u5c0f\u65f6\u540e\u6027\u80fd\u76f8\u5f53\u3002\u7814\u7a76\u4e3a\u8bed\u8a00\u5b66\u5bb6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684ASR\u9002\u5e94\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bed\u8a00\u7530\u91ce\u8c03\u67e5\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5982\u81ea\u53d1\u8bed\u97f3\u3001\u73af\u5883\u566a\u58f0\u548c\u6781\u5c11\u91cf\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bc4\u4f30ASR\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u7530\u91ce\u8bed\u8a00\u5b66\u5bb6\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5bf9\u4e24\u79cd\u591a\u8bed\u8a00ASR\u6a21\u578b\uff08MMS\u548cXLS-R\uff09\u5728\u4e94\u79cd\u7c7b\u578b\u591a\u6837\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u957f\uff0c\u4ee5\u8bc4\u4f30\u5176\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cMMS\u5728\u6781\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u800cXLS-R\u5728\u8bad\u7ec3\u6570\u636e\u8d85\u8fc7\u4e00\u5c0f\u65f6\u540e\u6027\u80fd\u4e0eMMS\u76f8\u5f53\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u8bed\u8a00\u5b66\u5206\u6790\uff0c\u4e3a\u7530\u91ce\u8bed\u8a00\u5b66\u5bb6\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684ASR\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0cMMS\u9002\u5408\u6781\u5c11\u91cf\u6570\u636e\u573a\u666f\uff0c\u800cXLS-R\u5728\u6570\u636e\u91cf\u8f83\u5927\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u8bed\u8a00\u6587\u6863\u5316\u4e2d\u7684\u8f6c\u5f55\u74f6\u9888\u3002", "paper_title_zh": "\u7a81\u7834\u8f6c\u5f55\u74f6\u9888\uff1a\u4e3a\u6781\u4f4e\u8d44\u6e90\u7530\u91ce\u8c03\u67e5\u8bed\u8a00\u5fae\u8c03ASR\u6a21\u578b", "abstract_zh": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u5df2\u8fbe\u5230\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5728\u8bed\u8a00\u7530\u91ce\u8c03\u67e5\u4e2d\u7684\u5b9e\u7528\u6027\u4ecd\u7136\u6709\u9650\u3002\u7530\u91ce\u8c03\u67e5\u4e2d\u6536\u96c6\u7684\u5f55\u97f3\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u81ea\u53d1\u8bed\u97f3\u3001\u73af\u5883\u566a\u58f0\u4ee5\u53ca\u6765\u81ea\u672a\u5145\u5206\u8bb0\u5f55\u8bed\u8a00\u7684\u6781\u5c11\u91cf\u6570\u636e\u96c6\u3002\u672c\u6587\u5bf9\u4e24\u79cd\u5fae\u8c03\u7684\u591a\u8bed\u8a00ASR\u6a21\u578b\uff08MMS\u548cXLS-R\uff09\u5728\u4e94\u79cd\u7c7b\u578b\u591a\u6837\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8fdb\u884c\u4e86\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63a7\u5236\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u957f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u8bad\u7ec3\u6570\u636e\u6781\u5c11\u65f6\uff0cMMS\u8868\u73b0\u6700\u4f73\uff0c\u800cXLS-R\u5728\u8bad\u7ec3\u6570\u636e\u8d85\u8fc7\u4e00\u5c0f\u65f6\u540e\u6027\u80fd\u4e0eMMS\u76f8\u5f53\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u8bed\u8a00\u5b66\u5206\u6790\uff0c\u4e3a\u7530\u91ce\u8bed\u8a00\u5b66\u5bb6\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff0c\u5f3a\u8c03\u53ef\u590d\u73b0\u7684ASR\u9002\u5e94\u65b9\u6cd5\uff0c\u4ee5\u7f13\u89e3\u8bed\u8a00\u6587\u6863\u5316\u4e2d\u7684\u8f6c\u5f55\u74f6\u9888\u3002"}}
{"id": "2506.17412", "pdf": "https://arxiv.org/pdf/2506.17412", "abs": "https://arxiv.org/abs/2506.17412", "authors": ["Zijun Sun", "Solveig Thrun", "Michael Kampffmeyer"], "title": "VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction", "categories": ["cs.CV"], "comment": "MICCAI 2025, Provisional Accept", "summary": "Breast cancer remains a leading cause of mortality worldwide and is typically\ndetected via screening programs where healthy people are invited in regular\nintervals. Automated risk prediction approaches have the potential to improve\nthis process by facilitating dynamically screening of high-risk groups. While\nmost models focus solely on the most recent screening, there is growing\ninterest in exploiting temporal information to capture evolving trends in\nbreast tissue, as inspired by clinical practice. Early methods typically relied\non two time steps, and although recent efforts have extended this to multiple\ntime steps using Transformer architectures, challenges remain in fully\nharnessing the rich temporal dynamics inherent in longitudinal imaging data. In\nthis work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a\nstate-space model (SSM) and LSTM-like memory mechanisms to effectively capture\nnuanced trends in breast tissue evolution. To further enhance our approach, we\nincorporate an asymmetry module that utilizes a Spatial Asymmetry Detector\n(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant\nbilateral differences. This integrated framework demonstrates notable\nimprovements in predicting cancer onset, especially for the more challenging\nhigh-density breast cases and achieves superior performance at extended time\npoints (years four and five), highlighting its potential to advance early\nbreast cancer recognition and enable more personalized screening strategies.\nOur code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision Mamba RNN\uff08VMRNN\uff09\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u65f6\u5e8f\u6846\u67b6VMRA-MaR\uff0c\u7ed3\u5408\u4e0d\u5bf9\u79f0\u6a21\u5757\uff08SAD\u548cLAT\uff09\uff0c\u7528\u4e8e\u7eb5\u5411\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u98ce\u9669\u4eba\u7fa4\u7684\u7b5b\u67e5\u6548\u679c\u3002", "motivation": "\u4e73\u817a\u764c\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b7b\u539f\u56e0\u4e4b\u4e00\uff0c\u73b0\u6709\u7b5b\u67e5\u65b9\u6cd5\u591a\u4f9d\u8d56\u6700\u65b0\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u65f6\u5e8f\u52a8\u6001\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u7eb5\u5411\u5f71\u50cf\u6570\u636e\u4e2d\u7684\u65f6\u5e8f\u8d8b\u52bf\u548c\u53cc\u4fa7\u4e0d\u5bf9\u79f0\u6027\uff0c\u63d0\u5347\u4e73\u817a\u764c\u65e9\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faVMRA-MaR\u6846\u67b6\uff0c\u7ed3\u5408VMRNN\u548cSSM\u6355\u6349\u4e73\u817a\u7ec4\u7ec7\u6f14\u53d8\u7684\u65f6\u5e8f\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u4e0d\u5bf9\u79f0\u6a21\u5757\uff08SAD\u548cLAT\uff09\u68c0\u6d4b\u4e34\u5e8a\u76f8\u5173\u7684\u53cc\u4fa7\u5dee\u5f02\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u764c\u75c7\u53d1\u75c5\u65b9\u9762\u8868\u73b0\u663e\u8457\uff0c\u5c24\u5176\u5bf9\u9ad8\u5bc6\u5ea6\u4e73\u817a\u75c5\u4f8b\u548c\u8fdc\u671f\u65f6\u95f4\u70b9\uff08\u7b2c\u56db\u548c\u7b2c\u4e94\u5e74\uff09\u6548\u679c\u66f4\u4f18\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "VMRA-MaR\u6846\u67b6\u901a\u8fc7\u65f6\u5e8f\u5efa\u6a21\u548c\u4e0d\u5bf9\u79f0\u6027\u68c0\u6d4b\uff0c\u4e3a\u4e73\u817a\u764c\u65e9\u671f\u8bc6\u522b\u548c\u4e2a\u6027\u5316\u7b5b\u67e5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "VMRA-MaR\uff1a\u4e00\u79cd\u57fa\u4e8e\u4e0d\u5bf9\u79f0\u611f\u77e5\u7684\u65f6\u5e8f\u6846\u67b6\u7528\u4e8e\u7eb5\u5411\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b", "abstract_zh": "\u4e73\u817a\u764c\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b7b\u539f\u56e0\u4e4b\u4e00\uff0c\u901a\u5e38\u901a\u8fc7\u5b9a\u671f\u7b5b\u67e5\u53d1\u73b0\u3002\u81ea\u52a8\u5316\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u6709\u671b\u901a\u8fc7\u52a8\u6001\u7b5b\u67e5\u9ad8\u98ce\u9669\u7fa4\u4f53\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002\u73b0\u6709\u6a21\u578b\u591a\u5173\u6ce8\u6700\u65b0\u7b5b\u67e5\u6570\u636e\uff0c\u800c\u4e34\u5e8a\u5b9e\u8df5\u542f\u53d1\u4e0b\uff0c\u5229\u7528\u65f6\u5e8f\u4fe1\u606f\u6355\u6349\u4e73\u817a\u7ec4\u7ec7\u6f14\u53d8\u8d8b\u52bf\u7684\u7814\u7a76\u65e5\u76ca\u589e\u591a\u3002\u65e9\u671f\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e24\u4e2a\u65f6\u95f4\u70b9\uff0c\u8fd1\u671f\u7814\u7a76\u867d\u6269\u5c55\u81f3\u591a\u65f6\u95f4\u70b9\u5e76\u4f7f\u7528Transformer\u67b6\u6784\uff0c\u4f46\u5728\u5145\u5206\u5229\u7528\u7eb5\u5411\u5f71\u50cf\u6570\u636e\u7684\u65f6\u5e8f\u52a8\u6001\u65b9\u9762\u4ecd\u5b58\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u7ed3\u5408Vision Mamba RNN\uff08VMRNN\uff09\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u53ca\u7c7bLSTM\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u6355\u6349\u4e73\u817a\u7ec4\u7ec7\u6f14\u53d8\u7684\u7ec6\u5fae\u8d8b\u52bf\u3002\u4e3a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5f15\u5165\u4e0d\u5bf9\u79f0\u6a21\u5757\uff08SAD\u548cLAT\uff09\u8bc6\u522b\u4e34\u5e8a\u76f8\u5173\u7684\u53cc\u4fa7\u5dee\u5f02\u3002\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u764c\u75c7\u53d1\u75c5\u65b9\u9762\u8868\u73b0\u663e\u8457\uff0c\u5c24\u5176\u5bf9\u9ad8\u5bc6\u5ea6\u4e73\u817a\u75c5\u4f8b\u548c\u8fdc\u671f\u65f6\u95f4\u70b9\uff08\u7b2c\u56db\u548c\u7b2c\u4e94\u5e74\uff09\u6548\u679c\u66f4\u4f18\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u4e73\u817a\u764c\u65e9\u671f\u8bc6\u522b\u548c\u4e2a\u6027\u5316\u7b5b\u67e5\u4e2d\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u89c1https://github.com/Mortal-Suen/VMRA-MaR.git\u3002"}}
{"id": "2506.17644", "pdf": "https://arxiv.org/pdf/2506.17644", "abs": "https://arxiv.org/abs/2506.17644", "authors": ["Zimo Ji", "Daoyuan Wu", "Wenyuan Jiang", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "title": "Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges", "categories": ["cs.AI"], "comment": null, "summary": "Capture-the-Flag (CTF) competitions are crucial for cybersecurity education\nand training. As large language models (LLMs) evolve, there is increasing\ninterest in their ability to automate CTF challenge solving. For example, DARPA\nhas organized the AIxCC competition since 2023 to advance AI-powered automated\noffense and defense. However, this demands a combination of multiple abilities,\nfrom knowledge to reasoning and further to actions. In this paper, we highlight\nthe importance of technical knowledge in solving CTF problems and deliberately\nconstruct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'\nperformance in this core aspect. Our study offers a focused and innovative\nmeasurement of LLMs' capability in understanding CTF knowledge and applying it\nto solve CTF challenges. Our key findings reveal that while LLMs possess\nsubstantial technical knowledge, they falter in accurately applying this\nknowledge to specific scenarios and adapting their strategies based on feedback\nfrom the CTF environment.\n  Based on insights derived from this measurement study, we propose CTFAgent, a\nnovel LLM-driven framework for advancing CTF problem-solving. CTFAgent\nintroduces two new modules: two-stage Retrieval Augmented Generation (RAG) and\ninteractive Environmental Augmentation, which enhance LLMs' technical knowledge\nand vulnerability exploitation on CTF, respectively. Our experimental results\nshow that, on two popular CTF datasets, CTFAgent both achieves over 80%\nperformance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,\nCTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This\nreflects the benefit of our measurement study and the potential of our\nframework in advancing LLMs' capabilities in CTF problem-solving.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u89e3\u51b3\u7f51\u7edc\u5b89\u5168\u593a\u65d7\u8d5b\uff08CTF\uff09\u6311\u6218\u4e2d\u7684\u80fd\u529b\uff0c\u6784\u5efa\u4e86CTFKnow\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51faCTFAgent\u6846\u67b6\u4ee5\u63d0\u5347LLM\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u663e\u793aCTFAgent\u5728\u6027\u80fd\u4e0a\u63d0\u5347\u4e8680%\u4ee5\u4e0a\uff0c\u5e76\u5728\u5b9e\u9645\u6bd4\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5176\u5728\u81ea\u52a8\u5316\u89e3\u51b3CTF\u6311\u6218\u4e2d\u7684\u6f5c\u529b\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0cLLM\u5728\u6280\u672f\u77e5\u8bc6\u5e94\u7528\u548c\u573a\u666f\u9002\u5e94\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u5e76\u63d0\u5347\u5176\u80fd\u529b\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u5305\u542b3,992\u4e2a\u95ee\u9898\u7684CTFKnow\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u6280\u672f\u77e5\u8bc6\u80fd\u529b\u3002\u968f\u540e\u63d0\u51fa\u4e86CTFAgent\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9636\u6bb5\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u63d0\u5347LLM\u7684\u77e5\u8bc6\u5e94\u7528\u548c\u6f0f\u6d1e\u5229\u7528\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCTFAgent\u5728\u4e24\u4e2a\u6d41\u884c\u7684CTF\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728picoCTF2024\u6bd4\u8d5b\u4e2d\u6392\u540d\u524d23.6%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728CTF\u6311\u6218\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7CTFAgent\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5176\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u63a8\u52a8LLM\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u6d4b\u91cf\u4e0e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u593a\u65d7\u8d5b\u6311\u6218\u7684\u80fd\u529b", "abstract_zh": "\u593a\u65d7\u8d5b\uff08CTF\uff09\u7ade\u8d5b\u5bf9\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u548c\u57f9\u8bad\u81f3\u5173\u91cd\u8981\u3002\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\uff0c\u4eba\u4eec\u5bf9\u5176\u81ea\u52a8\u5316\u89e3\u51b3CTF\u6311\u6218\u7684\u80fd\u529b\u8d8a\u6765\u8d8a\u611f\u5174\u8da3\u3002\u4f8b\u5982\uff0cDARPA\u81ea2023\u5e74\u8d77\u7ec4\u7ec7\u4e86AIxCC\u7ade\u8d5b\uff0c\u4ee5\u63a8\u52a8AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u653b\u9632\u3002\u7136\u800c\uff0c\u8fd9\u9700\u8981\u7ed3\u5408\u591a\u79cd\u80fd\u529b\uff0c\u4ece\u77e5\u8bc6\u5230\u63a8\u7406\u518d\u5230\u884c\u52a8\u3002\u672c\u6587\u5f3a\u8c03\u4e86\u6280\u672f\u77e5\u8bc6\u5728\u89e3\u51b3CTF\u95ee\u9898\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e13\u95e8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3,992\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5CTFKnow\uff0c\u4ee5\u8861\u91cfLLM\u5728\u8fd9\u4e00\u6838\u5fc3\u65b9\u9762\u7684\u8868\u73b0\u3002\u6211\u4eec\u7684\u7814\u7a76\u5bf9LLM\u7406\u89e3CTF\u77e5\u8bc6\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u89e3\u51b3CTF\u6311\u6218\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u521b\u65b0\u6027\u6d4b\u91cf\u3002\u5173\u952e\u53d1\u73b0\u8868\u660e\uff0c\u5c3d\u7ba1LLM\u62e5\u6709\u4e30\u5bcc\u7684\u6280\u672f\u77e5\u8bc6\uff0c\u4f46\u5728\u5c06\u77e5\u8bc6\u51c6\u786e\u5e94\u7528\u4e8e\u5177\u4f53\u573a\u666f\u5e76\u6839\u636eCTF\u73af\u5883\u7684\u53cd\u9988\u8c03\u6574\u7b56\u7565\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CTFAgent\uff0c\u4e00\u79cd\u65b0\u578bLLM\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347CTF\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002CTFAgent\u5f15\u5165\u4e86\u4e24\u9636\u6bb5\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\u589e\u5f3a\u6a21\u5757\uff0c\u5206\u522b\u589e\u5f3a\u4e86LLM\u7684\u6280\u672f\u77e5\u8bc6\u548c\u6f0f\u6d1e\u5229\u7528\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e24\u4e2a\u6d41\u884c\u7684CTF\u6570\u636e\u96c6\u4e0a\uff0cCTFAgent\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5728CMU\u4e3b\u529e\u7684picoCTF2024\u4e2d\uff0cCTFAgent\u5728\u8fd17,000\u652f\u53c2\u8d5b\u961f\u4f0d\u4e2d\u6392\u540d\u524d23.6%\u3002\u8fd9\u53cd\u6620\u4e86\u6211\u4eec\u6d4b\u91cf\u7814\u7a76\u7684\u4ef7\u503c\u4ee5\u53ca\u8be5\u6846\u67b6\u5728\u63d0\u5347LLM\u89e3\u51b3CTF\u6311\u6218\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17467", "pdf": "https://arxiv.org/pdf/2506.17467", "abs": "https://arxiv.org/abs/2506.17467", "authors": ["Weixin Liang"], "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u5199\u4f5c\u548c\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u7684\u5f71\u54cd\uff1a\u63ed\u793aAI\u68c0\u6d4b\u5668\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u91cf\u5316LLMs\u5728\u591a\u4e2a\u5199\u4f5c\u9886\u57df\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u8bc4\u4f30LLMs\u5728\u7814\u7a76\u624b\u7a3f\u53cd\u9988\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u666e\u53ca\u6b63\u5728\u6539\u53d8\u5199\u4f5c\u3001\u6c9f\u901a\u548c\u521b\u4f5c\u65b9\u5f0f\uff0c\u4f46\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5bf9\u4e2a\u4f53\u548c\u673a\u6784\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u516c\u5e73\u6027\u3001\u5e94\u7528\u8303\u56f4\u548c\u79d1\u7814\u652f\u6301\u65b9\u9762\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e09\u90e8\u5206\uff1a1\uff09\u5206\u6790AI\u68c0\u6d4b\u5668\u5bf9\u975e\u4e3b\u6d41\u8bed\u8a00\u4f7f\u7528\u8005\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff1b2\uff09\u5f00\u53d1\u7b97\u6cd5\u65b9\u6cd5\u91cf\u5316LLMs\u5728\u5b66\u672f\u8bc4\u5ba1\u3001\u79d1\u5b66\u51fa\u7248\u7269\u3001\u4f01\u4e1a\u901a\u8baf\u7b49\u9886\u57df\u7684\u5e94\u7528\uff1b3\uff09\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u8bc4\u4f30LLMs\u5728\u7814\u7a76\u624b\u7a3f\u53cd\u9988\u4e2d\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09AI\u68c0\u6d4b\u5668\u5b58\u5728\u5bf9\u975e\u4e3b\u6d41\u8bed\u8a00\u4f7f\u7528\u8005\u7684\u504f\u89c1\uff1b2\uff09LLMs\u5728\u591a\u4e2a\u5199\u4f5c\u9886\u57df\u4e2d\u7684\u5e94\u7528\u663e\u8457\u589e\u52a0\uff1b3\uff09LLMs\u80fd\u591f\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6709\u6548\u7684\u624b\u7a3f\u53cd\u9988\uff0c\u5c24\u5176\u5bf9\u8d44\u6e90\u532e\u4e4f\u7684\u7814\u7a76\u8005\u6709\u76ca\u3002", "conclusion": "LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u4e86\u673a\u9047\u4e0e\u6311\u6218\uff0c\u9700\u5173\u6ce8\u516c\u5e73\u6027\u548c\u652f\u6301\u5f31\u52bf\u7fa4\u4f53\u3002\u672a\u6765\u7814\u7a76\u5e94\u8fdb\u4e00\u6b65\u4f18\u5316AI\u5de5\u5177\uff0c\u786e\u4fdd\u5176\u666e\u60e0\u6027\u3002", "paper_title_zh": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5199\u4f5c\u548c\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u5f71\u54cd\u7684\u8ba1\u7b97\u65b9\u6cd5", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5c55\u73b0\u51fa\u6539\u53d8\u5199\u4f5c\u3001\u6c9f\u901a\u548c\u521b\u4f5c\u65b9\u5f0f\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u5728\u793e\u4f1a\u4e2d\u8fc5\u901f\u666e\u53ca\u3002\u672c\u8bba\u6587\u901a\u8fc7\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\u63a2\u8ba8\u4e2a\u4f53\u548c\u673a\u6784\u5982\u4f55\u9002\u5e94\u5e76\u5229\u7528\u8fd9\u4e00\u65b0\u5174\u6280\u672f\u3002\u9996\u5148\uff0c\u6211\u63ed\u793a\u4e86AI\u68c0\u6d4b\u5668\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5bf9\u975e\u4e3b\u6d41\u8bed\u8a00\u4f7f\u7528\u8005\u7684\u4e0d\u516c\u5e73\u5bf9\u5f85\uff0c\u51f8\u663e\u4e86AI\u6cbb\u7406\u4e2d\u7684\u5173\u952e\u516c\u5e73\u95ee\u9898\u3002\u5176\u6b21\uff0c\u6211\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7fa4\u4f53\u7ea7\u7b97\u6cd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316LLMs\u5728\u5b66\u672f\u8bc4\u5ba1\u3001\u79d1\u5b66\u51fa\u7248\u7269\u3001\u6d88\u8d39\u8005\u6295\u8bc9\u3001\u4f01\u4e1a\u901a\u8baf\u3001\u62db\u8058\u4fe1\u606f\u53ca\u56fd\u9645\u7ec4\u7ec7\u65b0\u95fb\u7a3f\u7b49\u5199\u4f5c\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0AI\u8f85\u52a9\u5185\u5bb9\u7684\u666e\u904d\u5b58\u5728\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u7814\u7a76\u4e86LLMs\u5728\u7814\u7a76\u624b\u7a3f\u53cd\u9988\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u90a3\u4e9b\u96be\u4ee5\u83b7\u5f97\u53ca\u65f6\u53cd\u9988\u7684\u7814\u7a76\u8005\uff08\u5c24\u5176\u662f\u65e9\u671f\u804c\u4e1a\u7814\u7a76\u8005\u548c\u8d44\u6e90\u532e\u4e4f\u8005\uff09\u63d0\u4f9b\u4e86\u652f\u6301\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.17425", "pdf": "https://arxiv.org/pdf/2506.17425", "abs": "https://arxiv.org/abs/2506.17425", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views\nenables faster scans with lower radiation dose, but the resulting severe\nunder-sampling causes strong artifacts and poor spatial coverage. We address\nthese challenges in a unified framework. First, we replace conventional\nUNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.\nConvolutional layers capture local details, while self-attention layers enhance\nglobal context. We adapt TransUNet to CBCT by combining multi-scale features,\nquerying view-specific features per 3D point, and adding a lightweight\nattenuation-prediction head. This yields Trans-CBCT, which surpasses prior\nbaselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.\nSecond, we introduce a neighbor-aware Point Transformer to enforce volumetric\ncoherence. This module uses 3D positional encoding and attention over k-nearest\nneighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,\nprovides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on\nLUNA16 and ToothFairy show consistent gains from six to ten views, validating\nthe effectiveness of combining CNN-Transformer features with point-based\ngeometry reasoning for sparse-view CBCT reconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrans$^2$-CBCT\u7684\u53ccTransformer\u6846\u67b6\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56feCBCT\u91cd\u5efa\u3002\u901a\u8fc7\u7ed3\u5408CNN-Transformer\u7279\u5f81\u548c\u57fa\u4e8e\u70b9\u7684\u51e0\u4f55\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u7a00\u758f\u89c6\u56feCBCT\u626b\u63cf\u901f\u5ea6\u5feb\u4e14\u8f90\u5c04\u5242\u91cf\u4f4e\uff0c\u4f46\u4e25\u91cd\u7684\u6b20\u91c7\u6837\u4f1a\u5bfc\u81f4\u4f2a\u5f71\u548c\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408CNN\u548cTransformer\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528TransUNet\u66ff\u4ee3\u4f20\u7edfUNet/ResNet\u7f16\u7801\u5668\uff0c\u7ed3\u5408CNN\u7684\u5c40\u90e8\u7ec6\u8282\u6355\u6349\u548cTransformer\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u589e\u5f3a\u30022. \u5f15\u5165\u90bb\u57df\u611f\u77e5\u7684Point Transformer\u6a21\u5757\uff0c\u901a\u8fc73D\u4f4d\u7f6e\u7f16\u7801\u548ck\u8fd1\u90bb\u6ce8\u610f\u529b\u63d0\u5347\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728LUNA16\u6570\u636e\u96c6\u4e0a\uff0cTrans-CBCT\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e861.17 dB PSNR\u548c0.0163 SSIM\uff1bTrans$^2$-CBCT\u8fdb\u4e00\u6b65\u63d0\u5347\u4e860.63 dB PSNR\u548c0.0117 SSIM\u3002\u5b9e\u9a8c\u57286\u523010\u89c6\u56fe\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7ed3\u5408CNN-Transformer\u7279\u5f81\u548c\u57fa\u4e8e\u70b9\u7684\u51e0\u4f55\u63a8\u7406\uff0cTrans$^2$-CBCT\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56feCBCT\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "paper_title_zh": "Trans$^2$-CBCT\uff1a\u4e00\u79cd\u7528\u4e8e\u7a00\u758f\u89c6\u56feCBCT\u91cd\u5efa\u7684\u53ccTransformer\u6846\u67b6", "abstract_zh": "\u7a00\u758f\u89c6\u56fe\u7684\u9525\u675f\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CBCT\uff09\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u626b\u63cf\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u8f90\u5c04\u5242\u91cf\uff0c\u4f46\u4e25\u91cd\u7684\u6b20\u91c7\u6837\u4f1a\u5bfc\u81f4\u5f3a\u70c8\u7684\u4f2a\u5f71\u548c\u8f83\u5dee\u7684\u7a7a\u95f4\u8986\u76d6\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\u3002\u9996\u5148\uff0c\u6211\u4eec\u7528TransUNet\u66ff\u4ee3\u4f20\u7edf\u7684UNet/ResNet\u7f16\u7801\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u6df7\u5408\u4e86CNN\u548cTransformer\u7684\u6a21\u578b\u3002\u5377\u79ef\u5c42\u6355\u6349\u5c40\u90e8\u7ec6\u8282\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u5c42\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u3001\u67e5\u8be2\u6bcf\u4e2a3D\u70b9\u7684\u89c6\u56fe\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u7684\u8870\u51cf\u9884\u6d4b\u5934\uff0c\u5c06TransUNet\u9002\u914d\u5230CBCT\u4e2d\u3002\u8fd9\u4ea7\u751f\u4e86Trans-CBCT\uff0c\u5728LUNA16\u6570\u636e\u96c6\u4e0a\uff0c\u5176PSNR\u6bd4\u73b0\u6709\u57fa\u7ebf\u9ad8\u51fa1.17 dB\uff0cSSIM\u9ad8\u51fa0.0163\uff08\u516d\u89c6\u56fe\uff09\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u90bb\u57df\u611f\u77e5\u7684Point Transformer\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u4f53\u79ef\u4e00\u81f4\u6027\u3002\u8be5\u6a21\u5757\u4f7f\u75283D\u4f4d\u7f6e\u7f16\u7801\u548ck\u8fd1\u90bb\u6ce8\u610f\u529b\u6765\u6539\u5584\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u6700\u7ec8\u7684\u6a21\u578bTrans$^2$-CBCT\u8fdb\u4e00\u6b65\u63d0\u5347\u4e860.63 dB PSNR\u548c0.0117 SSIM\u3002\u5728LUNA16\u548cToothFairy\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ece\u516d\u89c6\u56fe\u5230\u5341\u89c6\u56fe\u5747\u53d6\u5f97\u4e86\u7a33\u5b9a\u7684\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u5408CNN-Transformer\u7279\u5f81\u4e0e\u57fa\u4e8e\u70b9\u7684\u51e0\u4f55\u63a8\u7406\u5728\u7a00\u758f\u89c6\u56feCBCT\u91cd\u5efa\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.17667", "pdf": "https://arxiv.org/pdf/2506.17667", "abs": "https://arxiv.org/abs/2506.17667", "authors": ["Lintao Wang", "Encheng Su", "Jiaqi Liu", "Pengze Li", "Peng Xia", "Jiabei Xiao", "Wenlong Zhang", "Xinnan Dai", "Xi Chen", "Yuan Meng", "Mingyu Ding", "Lei Bai", "Wanli Ouyang", "Shixiang Tang", "Aoran Wang", "Xinzhu Ma"], "title": "PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models", "categories": ["cs.AI"], "comment": null, "summary": "Physics problem-solving is a challenging domain for large AI models,\nrequiring integration of conceptual understanding, mathematical reasoning, and\ninterpretation of physical diagrams. Current evaluation methodologies show\nnotable limitations in capturing the breadth and complexity of\nundergraduate-level physics, underscoring the need for more rigorous\nassessments. To this end, we present PhysUniBench, a large-scale multimodal\nbenchmark designed to evaluate and improve the reasoning capabilities of\nmultimodal large language models (MLLMs) specifically on undergraduate-level\nphysics problems. PhysUniBench consists of 3,304 physics questions spanning 8\nmajor sub-disciplines of physics, each accompanied by one visual diagrams. The\nbenchmark includes both open-ended and multiple-choice questions,\nsystematically curated and difficulty-rated through an iterative\nmodel-in-the-loop process. The benchmark's construction involved a rigorous\nmulti-stage process, including multiple roll-outs, expert-level evaluation,\nautomated filtering of easily solved problems, and a nuanced difficulty grading\nsystem with five levels. Through extensive experiments, we observe that current\nstate-of-the-art models encounter substantial challenges in physics reasoning.\nFor example, GPT-4o mini achieves only about 34.2\\% accuracy in the proposed\nPhysUniBench. These results highlight that current MLLMs struggle with advanced\nphysics reasoning, especially on multi-step problems and those requiring\nprecise diagram interpretation. By providing a broad and rigorous assessment\ntool, PhysUniBench aims to drive progress in AI for Science, encouraging the\ndevelopment of models with stronger physical reasoning, problem-solving skills,\nand multimodal understanding. The benchmark and evaluation scripts are\navailable at https://prismax-team.github.io/PhysUniBenchmark/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PhysUniBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672c\u79d1\u7269\u7406\u95ee\u9898\u63a8\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u57fa\u51c6\u5305\u542b3,304\u4e2a\u7269\u7406\u95ee\u9898\uff0c\u8986\u76d68\u4e2a\u4e3b\u8981\u7269\u7406\u5b50\u9886\u57df\uff0c\u5e76\u914d\u6709\u56fe\u8868\u3002\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5982GPT-4o mini\u4ec5\u8fbe\u523034.2%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u9886\u57df\u7684\u80fd\u529b\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u672c\u79d1\u7269\u7406\u7684\u5e7f\u5ea6\u548c\u590d\u6742\u6027\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u63a8\u52a8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\u3002", "method": "PhysUniBench\u901a\u8fc7\u591a\u9636\u6bb5\u4e25\u683c\u6d41\u7a0b\u6784\u5efa\uff0c\u5305\u62ec\u591a\u6b21\u6d4b\u8bd5\u3001\u4e13\u5bb6\u8bc4\u4f30\u3001\u81ea\u52a8\u8fc7\u6ee4\u7b80\u5355\u95ee\u9898\u4ee5\u53ca\u4e94\u7ea7\u96be\u5ea6\u5206\u7ea7\u7cfb\u7edf\u3002\u57fa\u51c6\u5305\u542b3,304\u4e2a\u95ee\u9898\uff0c\u6db5\u76d68\u4e2a\u7269\u7406\u5b50\u9886\u57df\uff0c\u5e76\u914d\u6709\u56fe\u8868\uff0c\u95ee\u9898\u7c7b\u578b\u5305\u62ec\u5f00\u653e\u5f0f\u548c\u9009\u62e9\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u4f8b\u5982GPT-4o mini\u5728PhysUniBench\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a34.2%\u3002\u6a21\u578b\u5728\u591a\u6b65\u9aa4\u95ee\u9898\u548c\u9700\u8981\u7cbe\u786e\u56fe\u8868\u89e3\u91ca\u7684\u95ee\u9898\u4e0a\u5c24\u5176\u56f0\u96be\u3002", "conclusion": "PhysUniBench\u4e3aAI\u5728\u79d1\u5b66\u9886\u57df\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u800c\u4e25\u683c\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u65e8\u5728\u63a8\u52a8\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u3001\u95ee\u9898\u89e3\u51b3\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u4e0a\u7684\u53d1\u5c55\u3002", "paper_title_zh": "PhysUniBench\uff1a\u9762\u5411\u591a\u6a21\u6001\u6a21\u578b\u7684\u672c\u79d1\u7269\u7406\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u7269\u7406\u95ee\u9898\u89e3\u51b3\u662f\u5927\u578bAI\u6a21\u578b\u7684\u6311\u6218\u6027\u9886\u57df\uff0c\u9700\u8981\u6574\u5408\u6982\u5ff5\u7406\u89e3\u3001\u6570\u5b66\u63a8\u7406\u548c\u7269\u7406\u56fe\u8868\u89e3\u91ca\u3002\u5f53\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u6355\u6349\u672c\u79d1\u7269\u7406\u7684\u5e7f\u5ea6\u548c\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u51f8\u663e\u4e86\u5bf9\u66f4\u4e25\u683c\u8bc4\u4f30\u7684\u9700\u6c42\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PhysUniBench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u672c\u79d1\u7269\u7406\u95ee\u9898\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3002PhysUniBench\u5305\u542b3,304\u4e2a\u7269\u7406\u95ee\u9898\uff0c\u6db5\u76d68\u4e2a\u4e3b\u8981\u7269\u7406\u5b50\u9886\u57df\uff0c\u6bcf\u4e2a\u95ee\u9898\u914d\u6709\u4e00\u5f20\u56fe\u8868\u3002\u57fa\u51c6\u5305\u62ec\u5f00\u653e\u5f0f\u548c\u9009\u62e9\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u6a21\u578b\u53c2\u4e0e\u8fc7\u7a0b\u7cfb\u7edf\u7b5b\u9009\u548c\u96be\u5ea6\u5206\u7ea7\u3002\u57fa\u51c6\u7684\u6784\u5efa\u6d89\u53ca\u591a\u9636\u6bb5\u4e25\u683c\u6d41\u7a0b\uff0c\u5305\u62ec\u591a\u6b21\u6d4b\u8bd5\u3001\u4e13\u5bb6\u8bc4\u4f30\u3001\u81ea\u52a8\u8fc7\u6ee4\u6613\u89e3\u51b3\u95ee\u9898\u4ee5\u53ca\u4e94\u7ea7\u96be\u5ea6\u5206\u7ea7\u7cfb\u7edf\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4f8b\u5982\uff0cGPT-4o mini\u5728PhysUniBench\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a34.2%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524dMLLMs\u5728\u9ad8\u7ea7\u7269\u7406\u63a8\u7406\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u591a\u6b65\u9aa4\u95ee\u9898\u548c\u9700\u8981\u7cbe\u786e\u56fe\u8868\u89e3\u91ca\u7684\u95ee\u9898\u3002\u901a\u8fc7\u63d0\u4f9b\u5e7f\u6cdb\u800c\u4e25\u683c\u7684\u8bc4\u4f30\u5de5\u5177\uff0cPhysUniBench\u65e8\u5728\u63a8\u52a8AI\u5728\u79d1\u5b66\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u9f13\u52b1\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u7269\u7406\u63a8\u7406\u3001\u95ee\u9898\u89e3\u51b3\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u6a21\u578b\u3002\u57fa\u51c6\u548c\u8bc4\u4f30\u811a\u672c\u53ef\u5728https://prismax-team.github.io/PhysUniBenchmark/\u83b7\u53d6\u3002"}}
{"id": "2506.17506", "pdf": "https://arxiv.org/pdf/2506.17506", "abs": "https://arxiv.org/abs/2506.17506", "authors": ["Lesheng Jin", "Zhenyuan Ruan", "Haohui Mai", "Jingbo Shang"], "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM", "categories": ["cs.CL", "cs.OS"], "comment": null, "summary": "Modern GPUs evolve rapidly, yet production compilers still rely on\nhand-crafted register allocation heuristics that require substantial re-tuning\nfor each hardware generation. We introduce VeriLocc, a framework that combines\nlarge language models (LLMs) with formal compiler techniques to enable\ngeneralizable and verifiable register allocation across GPU architectures.\nVeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)\ninto target-specific register assignments, aided by static analysis for\ncross-architecture normalization and generalization and a verifier-guided\nregeneration loop to ensure correctness. Evaluated on matrix multiplication\n(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot\naccuracy and near-100% pass@100. Case study shows that VeriLocc discovers more\nperformant assignments than expert-tuned libraries, outperforming rocBLAS by\nover 10% in runtime.", "AI": {"tldr": "VeriLocc\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f62\u5f0f\u5316\u7f16\u8bd1\u5668\u6280\u672f\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8de8GPU\u67b6\u6784\u7684\u901a\u7528\u4e14\u53ef\u9a8c\u8bc1\u7684\u5bc4\u5b58\u5668\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3GPU\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u751f\u4ea7\u7f16\u8bd1\u5668\u4ecd\u4f9d\u8d56\u624b\u5de5\u8c03\u6574\u7684\u5bc4\u5b58\u5668\u5206\u914d\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9700\u8981\u4e3a\u6bcf\u4ee3\u786c\u4ef6\u91cd\u65b0\u8c03\u6574\u3002VeriLocc\u65e8\u5728\u901a\u8fc7LLM\u548c\u5f62\u5f0f\u5316\u6280\u672f\u5b9e\u73b0\u8de8\u67b6\u6784\u7684\u901a\u7528\u5bc4\u5b58\u5668\u5206\u914d\u3002", "method": "VeriLocc\u901a\u8fc7\u5fae\u8c03LLM\u5c06\u4e2d\u95f4\u8868\u793a\uff08MIR\uff09\u8f6c\u6362\u4e3a\u76ee\u6807\u7279\u5b9a\u7684\u5bc4\u5b58\u5668\u5206\u914d\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u5b9e\u73b0\u8de8\u67b6\u6784\u5f52\u4e00\u5316\u548c\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u518d\u751f\u5faa\u73af\u786e\u4fdd\u6b63\u786e\u6027\u3002", "result": "\u5728\u77e9\u9635\u4e58\u6cd5\uff08GEMM\uff09\u548c\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u6d4b\u8bd5\u4e2d\uff0cVeriLocc\u5355\u6b21\u51c6\u786e\u7387\u8fbe85-99%\uff0cpass@100\u63a5\u8fd1100%\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u5bb6\u8c03\u4f18\u5e93\uff0c\u5982rocBLAS\u8fd0\u884c\u65f6\u63d0\u5347\u8d8510%\u3002", "conclusion": "VeriLocc\u5c55\u793a\u4e86LLM\u4e0e\u5f62\u5f0f\u5316\u7f16\u8bd1\u5668\u6280\u672f\u7ed3\u5408\u5728\u5bc4\u5b58\u5668\u5206\u914d\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u6027\u80fd\u4e14\u53ef\u9a8c\u8bc1\u7684\u5206\u914d\u65b9\u6848\u3002", "paper_title_zh": "VeriLocc\uff1a\u57fa\u4e8eLLM\u7684\u7aef\u5230\u7aef\u8de8\u67b6\u6784\u5bc4\u5b58\u5668\u5206\u914d", "abstract_zh": "\u73b0\u4ee3GPU\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u751f\u4ea7\u7f16\u8bd1\u5668\u4ecd\u4f9d\u8d56\u624b\u5de5\u8c03\u6574\u7684\u5bc4\u5b58\u5668\u5206\u914d\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9700\u4e3a\u6bcf\u4ee3\u786c\u4ef6\u91cd\u65b0\u8c03\u6574\u3002\u6211\u4eec\u63d0\u51faVeriLocc\uff0c\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f62\u5f0f\u5316\u7f16\u8bd1\u5668\u6280\u672f\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8GPU\u67b6\u6784\u7684\u901a\u7528\u4e14\u53ef\u9a8c\u8bc1\u7684\u5bc4\u5b58\u5668\u5206\u914d\u3002VeriLocc\u901a\u8fc7\u5fae\u8c03LLM\u5c06\u4e2d\u95f4\u8868\u793a\uff08MIR\uff09\u8f6c\u6362\u4e3a\u76ee\u6807\u7279\u5b9a\u7684\u5bc4\u5b58\u5668\u5206\u914d\uff0c\u8f85\u4ee5\u9759\u6001\u5206\u6790\u5b9e\u73b0\u8de8\u67b6\u6784\u5f52\u4e00\u5316\u548c\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u518d\u751f\u5faa\u73af\u786e\u4fdd\u6b63\u786e\u6027\u3002\u5728\u77e9\u9635\u4e58\u6cd5\uff08GEMM\uff09\u548c\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u6d4b\u8bd5\u4e2d\uff0cVeriLocc\u5355\u6b21\u51c6\u786e\u7387\u8fbe85-99%\uff0cpass@100\u63a5\u8fd1100%\u3002\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cVeriLocc\u53d1\u73b0\u7684\u5206\u914d\u65b9\u6848\u6027\u80fd\u4f18\u4e8e\u4e13\u5bb6\u8c03\u4f18\u5e93\uff0c\u5982rocBLAS\u8fd0\u884c\u65f6\u63d0\u5347\u8d8510%\u3002"}}
{"id": "2506.17439", "pdf": "https://arxiv.org/pdf/2506.17439", "abs": "https://arxiv.org/abs/2506.17439", "authors": ["Nisar Ahmed", "Gulshan Saleem", "Hafiz Muhammad Shahzad Asif", "Muhammad Usman Younus", "Kalsoom Safdar"], "title": "Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis", "categories": ["cs.CV"], "comment": "Submitted in Wireless Personal Communications", "summary": "In recent years, the rapid growth of the Internet of Things technologies and\nthe widespread adoption of 5G wireless networks have led to an exponential\nincrease in the number of radiation devices operating in complex\nelectromagnetic environments. A key challenge in managing and securing these\ndevices is accurate identification and classification. To address this\nchallenge, specific emitter identification techniques have emerged as a\npromising solution that aims to provide reliable and efficient means of\nidentifying individual radiation devices in a unified and standardized manner.\nThis research proposes an approach that leverages transient energy spectrum\nanalysis using the General Linear Chirplet Transform to extract features from\nRF devices. A dataset comprising nine RF devices is utilized, with each sample\ncontaining 900 attributes and a total of 1080 equally distributed samples\nacross the devices. These features are then used in a classification modeling\nframework. To overcome the limitations of conventional machine learning\nmethods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for\nlearning the identification of RF devices based on their transient\ncharacteristics. The proposed approach provided a 10-fold cross-validation\nperformance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,\nand classification accuracy of 99.17%. The results demonstrate the promising\nclassification performance of the CNN-Bi-GRU approach, indicating its\nsuitability for accurately identifying RF devices based on their transient\ncharacteristics and its potential for enhancing device identification and\nclassification in complex wireless environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ac\u6001\u80fd\u91cf\u8c31\u5206\u6790\u7684\u65e0\u7ebf\u8bbe\u5907\u8bc6\u522b\u65b9\u6cd5\uff0c\u5229\u7528\u901a\u7528\u7ebf\u6027\u8c03\u9891\u5c0f\u6ce2\u53d8\u6362\u63d0\u53d6\u5c04\u9891\u8bbe\u5907\u7279\u5f81\uff0c\u5e76\u7ed3\u5408CNN-Bi-GRU\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8699.17%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u6280\u672f\u548c5G\u65e0\u7ebf\u7f51\u7edc\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u590d\u6742\u7535\u78c1\u73af\u5883\u4e2d\u7684\u8f90\u5c04\u8bbe\u5907\u6570\u91cf\u6fc0\u589e\uff0c\u51c6\u786e\u8bc6\u522b\u548c\u5206\u7c7b\u8fd9\u4e9b\u8bbe\u5907\u6210\u4e3a\u7ba1\u7406\u548c\u5b89\u5168\u7684\u5173\u952e\u6311\u6218\u3002\u7279\u5b9a\u53d1\u5c04\u5668\u8bc6\u522b\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u901a\u7528\u7ebf\u6027\u8c03\u9891\u5c0f\u6ce2\u53d8\u6362\u5206\u6790\u5c04\u9891\u8bbe\u5907\u7684\u77ac\u6001\u80fd\u91cf\u8c31\uff0c\u63d0\u53d6\u7279\u5f81\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bCNN-Bi-GRU\u8fdb\u884c\u8bbe\u5907\u8bc6\u522b\u3002\u6570\u636e\u96c6\u5305\u542b9\u79cd\u5c04\u9891\u8bbe\u5907\uff0c\u6bcf\u6837\u672c900\u4e2a\u5c5e\u6027\uff0c\u51711080\u4e2a\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCNN-Bi-GRU\u6a21\u578b\u768410\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6027\u80fd\u4e3a\uff1a\u7cbe\u786e\u738799.33%\u3001\u53ec\u56de\u738799.53%\u3001F1\u5206\u657099.43%\u3001\u5206\u7c7b\u51c6\u786e\u738799.17%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e\u77ac\u6001\u7279\u5f81\u7684\u5c04\u9891\u8bbe\u5907\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u8bbe\u5907\u8bc6\u522b\u4e0e\u5206\u7c7b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u901a\u8fc7\u5c04\u9891\u6307\u7eb9\u589e\u5f3a\u65e0\u7ebf\u8bbe\u5907\u8bc6\u522b\uff1a\u57fa\u4e8e\u77ac\u6001\u80fd\u91cf\u8c31\u5206\u6790\u7684\u65b9\u6cd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u7269\u8054\u7f51\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u548c5G\u65e0\u7ebf\u7f51\u7edc\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bfc\u81f4\u590d\u6742\u7535\u78c1\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u8f90\u5c04\u8bbe\u5907\u6570\u91cf\u5448\u6307\u6570\u589e\u957f\u3002\u7ba1\u7406\u548c\u5b89\u5168\u8fd9\u4e9b\u8bbe\u5907\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u51c6\u786e\u8bc6\u522b\u548c\u5206\u7c7b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7279\u5b9a\u53d1\u5c04\u5668\u8bc6\u522b\u6280\u672f\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u5e94\u8fd0\u800c\u751f\uff0c\u65e8\u5728\u4ee5\u7edf\u4e00\u548c\u6807\u51c6\u5316\u7684\u65b9\u5f0f\u63d0\u4f9b\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u8f90\u5c04\u8bbe\u5907\u8bc6\u522b\u624b\u6bb5\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u901a\u7528\u7ebf\u6027\u8c03\u9891\u5c0f\u6ce2\u53d8\u6362\u5206\u6790\u5c04\u9891\u8bbe\u5907\u77ac\u6001\u80fd\u91cf\u8c31\u4ee5\u63d0\u53d6\u7279\u5f81\u7684\u65b9\u6cd5\u3002\u6570\u636e\u96c6\u5305\u542b9\u79cd\u5c04\u9891\u8bbe\u5907\uff0c\u6bcf\u6837\u672c900\u4e2a\u5c5e\u6027\uff0c\u51711080\u4e2a\u5747\u5300\u5206\u5e03\u7684\u6837\u672c\u3002\u8fd9\u4e9b\u7279\u5f81\u968f\u540e\u88ab\u7528\u4e8e\u5206\u7c7b\u5efa\u6a21\u6846\u67b6\u3002\u4e3a\u514b\u670d\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aCNN-Bi-GRU\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u57fa\u4e8e\u77ac\u6001\u7279\u5f81\u7684\u5c04\u9891\u8bbe\u5907\u8bc6\u522b\u3002\u6240\u63d0\u65b9\u6cd5\u768410\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6027\u80fd\u4e3a\uff1a\u7cbe\u786e\u738799.33%\u3001\u53ec\u56de\u738799.53%\u3001F1\u5206\u657099.43%\u3001\u5206\u7c7b\u51c6\u786e\u738799.17%\u3002\u7ed3\u679c\u8868\u660e\uff0cCNN-Bi-GRU\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u77ac\u6001\u7279\u5f81\u7684\u5c04\u9891\u8bbe\u5907\u51c6\u786e\u8bc6\u522b\uff0c\u5e76\u6709\u671b\u63d0\u5347\u590d\u6742\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u8bbe\u5907\u8bc6\u522b\u4e0e\u5206\u7c7b\u80fd\u529b\u3002"}}
{"id": "2506.17697", "pdf": "https://arxiv.org/pdf/2506.17697", "abs": "https://arxiv.org/abs/2506.17697", "authors": ["Bohan Tang", "Dezhao Luo", "Jingxuan Chen", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Beyond Syntax: Action Semantics Learning for App Agents", "categories": ["cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) enables the rise of App agents\nthat interpret user intent and operate smartphone Apps through actions such as\nclicking and scrolling. While prompt-based solutions with closed LLM APIs show\npromising ability, they incur heavy compute costs and external API dependency.\nFine-tuning smaller open-source LLMs solves these limitations. However, current\nfine-tuning methods use a syntax learning paradigm that forces agents to\nreproduce exactly the ground truth action strings, leading to\nout-of-distribution (OOD) vulnerability. To fill this gap, we propose Action\nSemantics Learning (ASL), a novel learning framework, where the learning\nobjective is capturing the semantics of the ground truth actions. Specifically,\ninspired by the programming language theory, we define the action semantics for\nApp agents as the state transition induced by the action in the user interface.\nWith this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a\nsemantic reward to train the App agents in generating actions aligned with the\nsemantics of ground truth actions, even when the syntactic forms differ. To\nsupport the effectiveness of ASL, we theoretically demonstrate the superior\nrobustness of ASL for the OOD problem compared with the existing syntax\nlearning paradigm. Extensive experiments on offline and online smartphone App\noperation benchmarks show that ASL significantly improves the accuracy and\ngeneralisation of App agents over existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u4f5c\u8bed\u4e49\u5b66\u4e60\uff08ASL\uff09\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6355\u6349\u52a8\u4f5c\u7684\u8bed\u4e49\u800c\u975e\u8bed\u6cd5\u5f62\u5f0f\uff0c\u63d0\u5347App\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bed\u6cd5\u5b66\u4e60\u8303\u5f0f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u65b9\u6848\u867d\u7136\u6709\u6548\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u5916\u90e8API\uff1b\u800c\u5fae\u8c03\u5f00\u6e90\u5c0f\u6a21\u578b\u7684\u65b9\u6cd5\u5219\u56e0\u8bed\u6cd5\u5b66\u4e60\u8303\u5f0f\u5bfc\u81f4\u5206\u5e03\u5916\uff08OOD\uff09\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u5b66\u4e60\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u52a8\u4f5c\u8bed\u4e49\u5b66\u4e60\uff08ASL\uff09\u6846\u67b6\uff0c\u5b9a\u4e49\u52a8\u4f5c\u8bed\u4e49\u4e3a\u7528\u6237\u754c\u9762\u4e2d\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u5e76\u8bbe\u8ba1\u8bed\u4e49\u4f30\u8ba1\u5668\uff08SEE\uff09\u8ba1\u7b97\u8bed\u4e49\u5956\u52b1\uff0c\u8bad\u7ec3\u4ee3\u7406\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u52a8\u4f5c\uff0c\u5373\u4f7f\u8bed\u6cd5\u5f62\u5f0f\u4e0d\u540c\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0cASL\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u95ee\u9898\u4e0a\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86App\u4ee3\u7406\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ASL\u901a\u8fc7\u8bed\u4e49\u5b66\u4e60\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u6cd5\u5b66\u4e60\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u4e3aApp\u4ee3\u7406\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u8d85\u8d8a\u8bed\u6cd5\uff1a\u9762\u5411App\u4ee3\u7406\u7684\u52a8\u4f5c\u8bed\u4e49\u5b66\u4e60", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51fa\u73b0\u63a8\u52a8\u4e86App\u4ee3\u7406\u7684\u5174\u8d77\uff0c\u8fd9\u4e9b\u4ee3\u7406\u901a\u8fc7\u70b9\u51fb\u548c\u6eda\u52a8\u7b49\u52a8\u4f5c\u89e3\u91ca\u7528\u6237\u610f\u56fe\u5e76\u64cd\u4f5c\u667a\u80fd\u624b\u673a\u5e94\u7528\u3002\u5c3d\u7ba1\u57fa\u4e8e\u63d0\u793a\u7684\u5c01\u95edLLM API\u89e3\u51b3\u65b9\u6848\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u5916\u90e8API\u3002\u5fae\u8c03\u8f83\u5c0f\u7684\u5f00\u6e90LLM\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u5f53\u524d\u7684\u5fae\u8c03\u65b9\u6cd5\u91c7\u7528\u8bed\u6cd5\u5b66\u4e60\u8303\u5f0f\uff0c\u5f3a\u5236\u4ee3\u7406\u5b8c\u5168\u590d\u5236\u771f\u5b9e\u52a8\u4f5c\u5b57\u7b26\u4e32\uff0c\u5bfc\u81f4\u5206\u5e03\u5916\uff08OOD\uff09\u8106\u5f31\u6027\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u52a8\u4f5c\u8bed\u4e49\u5b66\u4e60\uff08ASL\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u5b66\u4e60\u76ee\u6807\u662f\u6355\u6349\u771f\u5b9e\u52a8\u4f5c\u7684\u8bed\u4e49\u3002\u5177\u4f53\u800c\u8a00\uff0c\u53d7\u7f16\u7a0b\u8bed\u8a00\u7406\u8bba\u542f\u53d1\uff0c\u6211\u4eec\u5c06App\u4ee3\u7406\u7684\u52a8\u4f5c\u8bed\u4e49\u5b9a\u4e49\u4e3a\u7528\u6237\u754c\u9762\u4e2d\u52a8\u4f5c\u5f15\u53d1\u7684\u72b6\u6001\u8f6c\u6362\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c1\u89e3\uff0cASL\u91c7\u7528\u65b0\u578b\u8bed\u4e49\u4f30\u8ba1\u5668\uff08SEE\uff09\u8ba1\u7b97\u8bed\u4e49\u5956\u52b1\uff0c\u8bad\u7ec3App\u4ee3\u7406\u751f\u6210\u4e0e\u771f\u5b9e\u52a8\u4f5c\u8bed\u4e49\u4e00\u81f4\u7684\u52a8\u4f5c\uff0c\u5373\u4f7f\u8bed\u6cd5\u5f62\u5f0f\u4e0d\u540c\u3002\u4e3a\u9a8c\u8bc1ASL\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86ASL\u5728OOD\u95ee\u9898\u4e0a\u7684\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u8bed\u6cd5\u5b66\u4e60\u8303\u5f0f\u3002\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u667a\u80fd\u624b\u673a\u5e94\u7528\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cASL\u663e\u8457\u63d0\u5347\u4e86App\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.17525", "pdf": "https://arxiv.org/pdf/2506.17525", "abs": "https://arxiv.org/abs/2506.17525", "authors": ["Mingfei Lau", "Qian Chen", "Yeming Fang", "Tingting Xu", "Tongzhou Chen", "Pavel Golik"], "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Our quality audit for three widely used public multilingual speech datasets -\nMozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some\nlanguages, these datasets suffer from significant quality issues. We believe\naddressing these issues will make these datasets more useful as training and\nevaluation sets, and improve downstream models. We divide these quality issues\ninto two categories: micro-level and macro-level. We find that macro-level\nissues are more prevalent in less institutionalized, often under-resourced\nlanguages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that\nhighlights the need for proactive language planning (e.g. orthography\nprescriptions, dialect boundary definition) and enhanced data quality control\nin the process of Automatic Speech Recognition (ASR) dataset creation. We\nconclude by proposing guidelines and recommendations to mitigate these issues\nin future dataset development, emphasizing the importance of sociolinguistic\nawareness in creating robust and reliable speech data resources.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff08Mozilla Common Voice 17.0\u3001FLEURS\u548cVoxPopuli\uff09\u8fdb\u884c\u4e86\u8d28\u91cf\u5ba1\u6838\uff0c\u53d1\u73b0\u67d0\u4e9b\u8bed\u8a00\u5b58\u5728\u663e\u8457\u8d28\u91cf\u95ee\u9898\u3002\u7814\u7a76\u5c06\u95ee\u9898\u5206\u4e3a\u5fae\u89c2\u548c\u5b8f\u89c2\u4e24\u7c7b\uff0c\u6307\u51fa\u5b8f\u89c2\u95ee\u9898\u5728\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\u66f4\u4e3a\u666e\u904d\uff0c\u5e76\u4ee5\u53f0\u6e7e\u95fd\u5357\u8bed\u4e3a\u4f8b\uff0c\u5f3a\u8c03\u8bed\u8a00\u89c4\u5212\u548c\u6570\u636e\u8d28\u91cf\u63a7\u5236\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u67d0\u4e9b\u8bed\u8a00\u7684\u6570\u636e\u8d28\u91cf\u5b58\u5728\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5176\u4f5c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u96c6\u7684\u5b9e\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u7814\u7a76\u5bf9\u4e09\u4e2a\u516c\u5171\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\u8fdb\u884c\u8d28\u91cf\u5ba1\u6838\uff0c\u5c06\u95ee\u9898\u5206\u4e3a\u5fae\u89c2\u548c\u5b8f\u89c2\u4e24\u7c7b\uff0c\u5e76\u4ee5\u53f0\u6e7e\u95fd\u5357\u8bed\u4e3a\u4f8b\u8fdb\u884c\u6848\u4f8b\u5206\u6790\uff0c\u63a2\u8ba8\u8bed\u8a00\u89c4\u5212\u548c\u6570\u636e\u8d28\u91cf\u63a7\u5236\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b8f\u89c2\u8d28\u91cf\u95ee\u9898\u5728\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\u66f4\u4e3a\u666e\u904d\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u8bed\u8a00\u89c4\u5212\u7684\u8bed\u8a00\u3002\u53f0\u6e7e\u95fd\u5357\u8bed\u7684\u6848\u4f8b\u8868\u660e\uff0c\u4e3b\u52a8\u7684\u8bed\u8a00\u89c4\u5212\u548c\u4e25\u683c\u7684\u6570\u636e\u8d28\u91cf\u63a7\u5236\u662f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5efa\u8bae\uff0c\u5f3a\u8c03\u5728\u672a\u6765\u7684\u6570\u636e\u96c6\u5f00\u53d1\u4e2d\u9700\u8981\u589e\u5f3a\u793e\u4f1a\u8bed\u8a00\u5b66\u610f\u8bc6\uff0c\u5b9e\u65bd\u8bed\u8a00\u89c4\u5212\u548c\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u4ee5\u786e\u4fdd\u8bed\u97f3\u6570\u636e\u8d44\u6e90\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "paper_title_zh": "\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff1a\u793e\u4f1a\u8bed\u8a00\u5b66\u610f\u8bc6\u4e0e\u4e3b\u52a8\u8bed\u8a00\u89c4\u5212\u7684\u5fc5\u8981\u6027", "abstract_zh": "\u6211\u4eec\u5bf9\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u516c\u5171\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff08Mozilla Common Voice 17.0\u3001FLEURS\u548cVoxPopuli\uff09\u8fdb\u884c\u4e86\u8d28\u91cf\u5ba1\u6838\uff0c\u53d1\u73b0\u67d0\u4e9b\u8bed\u8a00\u5b58\u5728\u663e\u8457\u7684\u8d28\u91cf\u95ee\u9898\u3002\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5c06\u4f7f\u8fd9\u4e9b\u6570\u636e\u96c6\u66f4\u9002\u5408\u4f5c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u96c6\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u95ee\u9898\u5206\u4e3a\u5fae\u89c2\u548c\u5b8f\u89c2\u4e24\u7c7b\uff0c\u53d1\u73b0\u5b8f\u89c2\u95ee\u9898\u5728\u5236\u5ea6\u5316\u7a0b\u5ea6\u8f83\u4f4e\u3001\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\u66f4\u4e3a\u666e\u904d\u3002\u901a\u8fc7\u5bf9\u53f0\u6e7e\u95fd\u5357\u8bed\uff08nan_tw\uff09\u7684\u6848\u4f8b\u5206\u6790\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u4e3b\u52a8\u8bed\u8a00\u89c4\u5212\uff08\u5982\u6b63\u5b57\u6cd5\u89c4\u8303\u3001\u65b9\u8a00\u8fb9\u754c\u5b9a\u4e49\uff09\u548c\u589e\u5f3a\u6570\u636e\u8d28\u91cf\u63a7\u5236\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6570\u636e\u96c6\u521b\u5efa\u8fc7\u7a0b\u4e2d\u7684\u5fc5\u8981\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u7684\u6307\u5bfc\u539f\u5219\u548c\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u793e\u4f1a\u8bed\u8a00\u5b66\u610f\u8bc6\u5728\u521b\u5efa\u7a33\u5065\u53ef\u9760\u7684\u8bed\u97f3\u6570\u636e\u8d44\u6e90\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.17455", "pdf": "https://arxiv.org/pdf/2506.17455", "abs": "https://arxiv.org/abs/2506.17455", "authors": ["Taufikur Rahman Fuad", "Sabbir Ahmed", "Shahriar Ivan"], "title": "AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions", "categories": ["cs.CV"], "comment": "Submitted to AJSE Springer", "summary": "Robust visual recognition in underwater environments remains a significant\nchallenge due to complex distortions such as turbidity, low illumination, and\nocclusion, which severely degrade the performance of standard vision systems.\nThis paper introduces AQUA20, a comprehensive benchmark dataset comprising\n8,171 underwater images across 20 marine species reflecting real-world\nenvironmental challenges such as illumination, turbidity, occlusions, etc.,\nproviding a valuable resource for underwater visual understanding. Thirteen\nstate-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,\nMobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were\nevaluated to benchmark their performance in classifying marine species under\nchallenging conditions. Our experimental results show ConvNeXt achieving the\nbest performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of\n90.69%, as well as the highest overall F1-score of 88.92% with moderately large\nparameter size. The results obtained from our other benchmark models also\ndemonstrate trade-offs between complexity and performance. We also provide an\nextensive explainability analysis using GRAD-CAM and LIME for interpreting the\nstrengths and pitfalls of the models. Our results reveal substantial room for\nimprovement in underwater species recognition and demonstrate the value of\nAQUA20 as a foundation for future research in this domain. The dataset is\npublicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AQUA20\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b8,171\u5f20\u6c34\u4e0b\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d620\u79cd\u6d77\u6d0b\u7269\u79cd\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5206\u7c7b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660eConvNeXt\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "motivation": "\u6c34\u4e0b\u89c6\u89c9\u8bc6\u522b\u56e0\u6d51\u6d4a\u3001\u4f4e\u5149\u7167\u548c\u906e\u6321\u7b49\u590d\u6742\u5931\u771f\u95ee\u9898\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u89c6\u89c9\u7cfb\u7edf\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51faAQUA20\u6570\u636e\u96c6\uff0c\u4e3a\u6c34\u4e0b\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u8d44\u6e90\uff0c\u5e76\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6b64\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86AQUA20\u6570\u636e\u96c6\uff0c\u5305\u542b8,171\u5f20\u6c34\u4e0b\u56fe\u50cf\uff0c\u6db5\u76d620\u79cd\u6d77\u6d0b\u7269\u79cd\uff0c\u5e76\u8bc4\u4f30\u4e8613\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ec\u8f7b\u91cf\u7ea7CNN\u548c\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff09\u7684\u5206\u7c7b\u6027\u80fd\u3002\u4f7f\u7528GRAD-CAM\u548cLIME\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "ConvNeXt\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cTop-3\u51c6\u786e\u7387\u8fbe98.82%\uff0cTop-1\u51c6\u786e\u7387\u4e3a90.69%\uff0cF1\u5206\u6570\u4e3a88.92%\u3002\u5176\u4ed6\u6a21\u578b\u5728\u590d\u6742\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "AQUA20\u6570\u636e\u96c6\u4e3a\u6c34\u4e0b\u7269\u79cd\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u7a7a\u95f4\uff0c\u5e76\u5f3a\u8c03\u4e86\u8be5\u6570\u636e\u96c6\u5bf9\u672a\u6765\u7814\u7a76\u7684\u4ef7\u503c\u3002", "paper_title_zh": "AQUA20\uff1a\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6c34\u4e0b\u7269\u79cd\u5206\u7c7b\u7684\u57fa\u51c6\u6570\u636e\u96c6", "abstract_zh": "\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u89c6\u89c9\u8bc6\u522b\u56e0\u6d51\u6d4a\u3001\u4f4e\u5149\u7167\u548c\u906e\u6321\u7b49\u590d\u6742\u5931\u771f\u95ee\u9898\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4e25\u91cd\u964d\u4f4e\u4e86\u6807\u51c6\u89c6\u89c9\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u672c\u6587\u4ecb\u7ecd\u4e86AQUA20\uff0c\u4e00\u4e2a\u5305\u542b8,171\u5f20\u6c34\u4e0b\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d620\u79cd\u6d77\u6d0b\u7269\u79cd\uff0c\u53cd\u6620\u4e86\u5149\u7167\u3001\u6d51\u6d4a\u5ea6\u548c\u906e\u6321\u7b49\u771f\u5b9e\u73af\u5883\u6311\u6218\uff0c\u4e3a\u6c34\u4e0b\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002\u8bc4\u4f30\u4e8613\u79cd\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ec\u8f7b\u91cf\u7ea7CNN\u5982SqueezeNet\u3001MobileNetV2\u548c\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5982ViT\u3001ConvNeXt\uff09\uff0c\u4ee5\u8861\u91cf\u5176\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5206\u7c7b\u6d77\u6d0b\u7269\u79cd\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cConvNeXt\u8868\u73b0\u6700\u4f73\uff0cTop-3\u51c6\u786e\u7387\u4e3a98.82%\uff0cTop-1\u51c6\u786e\u7387\u4e3a90.69%\uff0cF1\u5206\u6570\u4e3a88.92%\uff0c\u4e14\u53c2\u6570\u91cf\u9002\u4e2d\u3002\u5176\u4ed6\u57fa\u51c6\u6a21\u578b\u7684\u7ed3\u679c\u4e5f\u5c55\u793a\u4e86\u590d\u6742\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002\u6b64\u5916\uff0c\u4f7f\u7528GRAD-CAM\u548cLIME\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u4ee5\u89e3\u91ca\u6a21\u578b\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6c34\u4e0b\u7269\u79cd\u8bc6\u522b\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u5e76\u8bc1\u660e\u4e86AQUA20\u4f5c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u57fa\u7840\u7684\u4ef7\u503c\u3002\u6570\u636e\u96c6\u516c\u5f00\u4e8e\uff1ahttps://huggingface.co/datasets/taufiktrf/AQUA20\u3002"}}
{"id": "2506.17784", "pdf": "https://arxiv.org/pdf/2506.17784", "abs": "https://arxiv.org/abs/2506.17784", "authors": ["Song Wang", "Zhen Tan", "Zihan Chen", "Shuang Zhou", "Tianlong Chen", "Jundong Li"], "title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Recent progress in large language model (LLM)-based multi-agent collaboration\nhighlights the power of structured communication in enabling collective\nintelligence. However, existing methods largely rely on static or graph-based\ninter-agent topologies, lacking the potential adaptability and flexibility in\ncommunication. In this work, we propose a new framework that rethinks\nmulti-agent coordination through a sequential structure rather than a graph\nstructure, offering a significantly larger topology space for multi-agent\ncommunication. Our method focuses on two key directions: (1) Next-Agent\nPrediction, which selects the most suitable agent role at each step, and (2)\nNext-Context Selection (NCS), which enables each agent to selectively access\nrelevant information from any previous step. Together, these components\nconstruct task-adaptive communication pipelines that support both role\nflexibility and global information flow. Extensive evaluations across multiple\nbenchmarks demonstrate that our approach achieves superior performance while\nsubstantially reducing communication overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6AnyMAC\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u7ed3\u6784\u800c\u975e\u56fe\u7ed3\u6784\u5b9e\u73b0\u7075\u6d3b\u901a\u4fe1\uff0c\u7ed3\u5408Next-Agent Prediction\u548cNext-Context Selection\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u591a\u4f9d\u8d56\u9759\u6001\u6216\u56fe\u7ed3\u6784\u7684\u901a\u4fe1\u62d3\u6251\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5e8f\u5217\u5316\u7ed3\u6784\u91cd\u65b0\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u4ee5\u6269\u5c55\u901a\u4fe1\u62d3\u6251\u7a7a\u95f4\u5e76\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "method": "\u63d0\u51faAnyMAC\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a(1) Next-Agent Prediction\uff0c\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684\u667a\u80fd\u4f53\u89d2\u8272\uff1b(2) Next-Context Selection (NCS)\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u9009\u62e9\u6027\u8bbf\u95ee\u5386\u53f2\u4fe1\u606f\u3002\u4e24\u8005\u7ed3\u5408\u6784\u5efa\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u901a\u4fe1\u7ba1\u9053\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAnyMAC\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "AnyMAC\u901a\u8fc7\u5e8f\u5217\u5316\u7ed3\u6784\u548c\u52a8\u6001\u901a\u4fe1\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9ad8\u6548\u4e0e\u7075\u6d3b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "AnyMAC\uff1a\u901a\u8fc7\u4e0b\u4e00\u667a\u80fd\u4f53\u9884\u6d4b\u5b9e\u73b0\u7ea7\u8054\u7075\u6d3b\u591a\u667a\u80fd\u4f53\u534f\u4f5c", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7814\u7a76\u7a81\u663e\u4e86\u7ed3\u6784\u5316\u901a\u4fe1\u5728\u6fc0\u53d1\u96c6\u4f53\u667a\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6216\u57fa\u4e8e\u56fe\u7684\u667a\u80fd\u4f53\u95f4\u62d3\u6251\u7ed3\u6784\uff0c\u7f3a\u4e4f\u901a\u4fe1\u7684\u6f5c\u5728\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u7ed3\u6784\u800c\u975e\u56fe\u7ed3\u6784\u91cd\u65b0\u601d\u8003\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u62d3\u6251\u7a7a\u95f4\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u805a\u7126\u4e8e\u4e24\u4e2a\u5173\u952e\u65b9\u5411\uff1a(1) Next-Agent Prediction\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e00\u6b65\u6700\u9002\u5408\u7684\u667a\u80fd\u4f53\u89d2\u8272\uff1b(2) Next-Context Selection (NCS)\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u80fd\u591f\u9009\u62e9\u6027\u8bbf\u95ee\u4efb\u4f55\u5148\u524d\u6b65\u9aa4\u7684\u76f8\u5173\u4fe1\u606f\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u6784\u5efa\u4e86\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u901a\u4fe1\u7ba1\u9053\uff0c\u652f\u6301\u89d2\u8272\u7075\u6d3b\u6027\u548c\u5168\u5c40\u4fe1\u606f\u6d41\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.17533", "pdf": "https://arxiv.org/pdf/2506.17533", "abs": "https://arxiv.org/abs/2506.17533", "authors": ["Yuanhao Wu", "Juntong Song", "Hanning Zhang", "Tong Zhang", "Cheng Niu"], "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDuaShepherd\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6b63\u786e\u6027\u548c\u6f5c\u529b\u4e24\u79cd\u5956\u52b1\u4fe1\u53f7\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u5956\u52b1\u4fe1\u53f7\uff08\u5982\u6b63\u786e\u6027\uff09\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u6b63\u786e\u6027\u548c\u6f5c\u529b\u4e24\u79cd\u4e92\u8865\u5956\u52b1\u4fe1\u53f7\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faDuaShepherd\u6846\u67b6\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210\u5305\u542b\u4e24\u79cd\u5956\u52b1\u4fe1\u53f7\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u7edf\u4e00\u591a\u5934\u90e8\u67b6\u6784\u8bad\u7ec3\u6a21\u578b\u3002\u901a\u8fc7\u5c06\u4e24\u79cd\u4fe1\u53f7\u7ed3\u5408\u4e3a\u590d\u5408\u6982\u7387\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728MATH500\u548cProcessBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDuaShepherd\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u5355\u4e00\u5956\u52b1\u4fe1\u53f7\u7684\u6a21\u578b\uff0c\u5e76\u5728\u76f8\u540c\u8d44\u6e90\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "DuaShepherd\u901a\u8fc7\u6574\u5408\u6b63\u786e\u6027\u548c\u6f5c\u529b\u5956\u52b1\u4fe1\u53f7\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DuaShepherd\uff1a\u7ed3\u5408\u9010\u6b65\u6b63\u786e\u6027\u4e0e\u6f5c\u5728\u5956\u52b1\u7684\u6570\u5b66\u63a8\u7406\u65b9\u6cd5", "abstract_zh": "\u672c\u6587\u63d0\u51faDuaShepherd\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e24\u79cd\u4e92\u8865\u7684\u5956\u52b1\u4fe1\u53f7\uff08\u6b63\u786e\u6027\u548c\u6f5c\u529b\uff09\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u6b63\u786e\u6027\u4fe1\u53f7\u4fa7\u91cd\u4e8e\u9010\u6b65\u9519\u8bef\u7684\u8bc6\u522b\uff0c\u800c\u6f5c\u529b\u4fe1\u53f7\u5219\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u53ef\u80fd\u6027\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u6784\u5efa\u5305\u542b\u8fd9\u4e24\u79cd\u4fe1\u53f7\u7684\u5927\u89c4\u6a21\u5956\u52b1\u5efa\u6a21\u6570\u636e\u96c6\u3002\u901a\u8fc7\u63a2\u7d22\u7edf\u4e00\u7684\u591a\u5934\u90e8\u67b6\u6784\uff0c\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u8fd9\u4e24\u79cd\u5956\u52b1\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u540c\u65f6\u5b66\u4e60\u6b63\u786e\u6027\u548c\u6f5c\u529b\u7684\u4f18\u52bf\u3002\u901a\u8fc7\u5c06\u8fd9\u4e24\u79cd\u4fe1\u53f7\u7ed3\u5408\u4e3a\u590d\u5408\u6982\u7387\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728MATH500\u548cProcessBench\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u79cd\u7ec4\u5408\u5956\u52b1\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u5355\u4e00\u5956\u52b1\u7c7b\u578b\u7684\u6a21\u578b\uff0c\u5e76\u5728\u53ef\u6bd4\u8d44\u6e90\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2506.17457", "pdf": "https://arxiv.org/pdf/2506.17457", "abs": "https://arxiv.org/abs/2506.17457", "authors": ["Dong Xiao", "Guangyao Chen", "Peixi Peng", "Yangru Huang", "Yifan Zhao", "Yongxing Dai", "Yonghong Tian"], "title": "When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network", "categories": ["cs.CV"], "comment": "ICML 2025 Spotlight", "summary": "Anomaly detection is essential for the safety and reliability of autonomous\ndriving systems. Current methods often focus on detection accuracy but neglect\nresponse time, which is critical in time-sensitive driving scenarios. In this\npaper, we introduce real-time anomaly detection for autonomous driving,\nprioritizing both minimal response time and high accuracy. We propose a novel\nmultimodal asynchronous hybrid network that combines event streams from event\ncameras with image data from RGB cameras. Our network utilizes the high\ntemporal resolution of event cameras through an asynchronous Graph Neural\nNetwork and integrates it with spatial features extracted by a CNN from RGB\nimages. This combination effectively captures both the temporal dynamics and\nspatial details of the driving environment, enabling swift and precise anomaly\ndetection. Extensive experiments on benchmark datasets show that our approach\noutperforms existing methods in both accuracy and response time, achieving\nmillisecond-level real-time performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\u7684\u591a\u6a21\u6001\u5f02\u6b65\u6df7\u5408\u7f51\u7edc\uff0c\u540c\u65f6\u4f18\u5316\u54cd\u5e94\u65f6\u95f4\u548c\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u4f9d\u8d56\u4e8e\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u7cbe\u5ea6\u800c\u5ffd\u89c6\u54cd\u5e94\u65f6\u95f4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9a7e\u9a76\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f02\u6b65\u6df7\u5408\u7f51\u7edc\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u901a\u8fc7\u5f02\u6b65\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408RGB\u76f8\u673a\u7684\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6CNN\uff0c\u4ee5\u6355\u6349\u9a7e\u9a76\u73af\u5883\u7684\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u54cd\u5e94\u65f6\u95f4\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6beb\u79d2\u7ea7\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u5f02\u6b65\u6df7\u5408\u7f51\u7edc\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\uff0c\u517c\u987e\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5ef6\u8fdf\u3002", "paper_title_zh": "\u5206\u79d2\u5fc5\u4e89\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5f02\u6b65\u6df7\u5408\u7f51\u7edc\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b", "abstract_zh": "\u5f02\u5e38\u68c0\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u68c0\u6d4b\u7cbe\u5ea6\u800c\u5ffd\u89c6\u54cd\u5e94\u65f6\u95f4\uff0c\u8fd9\u5728\u65f6\u95f4\u654f\u611f\u7684\u9a7e\u9a76\u573a\u666f\u4e2d\u5c24\u4e3a\u5173\u952e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u54cd\u5e94\u65f6\u95f4\u548c\u68c0\u6d4b\u7cbe\u5ea6\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5f02\u6b65\u6df7\u5408\u7f51\u7edc\uff0c\u7ed3\u5408\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u4e8b\u4ef6\u6d41\u548cRGB\u76f8\u673a\u7684\u56fe\u50cf\u6570\u636e\u3002\u8be5\u7f51\u7edc\u901a\u8fc7\u5f02\u6b65\u56fe\u795e\u7ecf\u7f51\u7edc\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5e76\u4e0eRGB\u56fe\u50cf\u4e2dCNN\u63d0\u53d6\u7684\u7a7a\u95f4\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u6355\u6349\u4e86\u9a7e\u9a76\u73af\u5883\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u54cd\u5e94\u65f6\u95f4\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6beb\u79d2\u7ea7\u7684\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2506.17788", "pdf": "https://arxiv.org/pdf/2506.17788", "abs": "https://arxiv.org/abs/2506.17788", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "title": "Bayesian Social Deduction with Graph-Informed Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2.1; I.2.7"], "comment": "32 pages, 10 figures. Under review", "summary": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620fAvalon\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u51fb\u8d25\u4eba\u7c7b\u73a9\u5bb6\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\uff08\u5982\u63a8\u65ad\u4ed6\u4eba\u4fe1\u5ff5\u548c\u610f\u56fe\uff09\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u63a8\u7406\u548c\u5c0f\u578b\u5316\u6a21\u578b\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u793e\u4ea4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u4fe1\u5ff5\u63a8\u65ad\u4efb\u52a1\u4ea4\u7ed9\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\u5904\u7406\uff0c\u540c\u65f6\u5229\u7528LLM\u8fdb\u884c\u8bed\u8a00\u7406\u89e3\u548c\u4ea4\u4e92\u3002\u8be5\u65b9\u6cd5\u5728Avalon\u6e38\u620f\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Agent-Agent\u5bf9\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u5728\u53d7\u63a7\u7814\u7a76\u4e2d\u51fb\u8d25\u4eba\u7c7b\u73a9\u5bb6\uff0c\u80dc\u7387\u8fbe67%\uff0c\u4e14\u5b9a\u6027\u8bc4\u5206\u9ad8\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u4eba\u7c7b\u961f\u53cb\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\u4e0eLLM\u7684\u6df7\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765LLM\u5728\u793e\u4ea4\u63a8\u7406\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u57fa\u4e8e\u56fe\u7ed3\u6784\u8bed\u8a00\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u793e\u4ea4\u63a8\u7406", "abstract_zh": "\u793e\u4ea4\u63a8\u7406\u2014\u2014\u4ece\u90e8\u5206\u89c2\u5bdf\u4e2d\u63a8\u65ad\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u4e0d\u53ef\u89c1\u4fe1\u5ff5\u548c\u610f\u56fe\u2014\u2014\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ecd\u662f\u4e00\u9879\u6311\u6218\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5f53\u524d\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620fAvalon\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1\u6700\u5927\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u6d4b\u8bd5\u65f6\u63a8\u7406\uff0c\u4e14\u5728\u5c0f\u578b\u5316\u5b9e\u65f6\u6a21\u578b\u4e0a\u8868\u73b0\u6025\u5267\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u4fe1\u5ff5\u63a8\u65ad\u4efb\u52a1\u4ea4\u7ed9\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528LLM\u8fdb\u884c\u8bed\u8a00\u7406\u89e3\u548c\u4ea4\u4e92\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728Agent-Agent\u5bf9\u6218\u4e2d\u4e0e\u66f4\u5927\u6a21\u578b\u7ade\u4e89\uff0c\u5e76\u9996\u6b21\u5728\u53d7\u63a7\u7814\u7a76\u4e2d\u51fb\u8d25\u4eba\u7c7b\u73a9\u5bb6\u2014\u2014\u80dc\u7387\u8fbe67%\uff0c\u5b9a\u6027\u8bc4\u5206\u9ad8\u4e8e\u63a8\u7406\u57fa\u7ebf\u548c\u4eba\u7c7b\u961f\u53cb\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765LLM\u667a\u80fd\u4f53\u7684\u793e\u4ea4\u63a8\u7406\u7814\u7a76\uff0c\u8be6\u89c1https://camp-lab-purdue.github.io/bayesian-social-deduction/\u3002"}}
{"id": "2506.17542", "pdf": "https://arxiv.org/pdf/2506.17542", "abs": "https://arxiv.org/abs/2506.17542", "authors": ["Nitin Venkateswaran", "Kevin Tang", "Ratree Wayland"], "title": "Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception", "categories": ["cs.CL"], "comment": null, "summary": "Traditional models of accent perception underestimate the role of gradient\nvariations in phonological features which listeners rely upon for their accent\njudgments. We investigate how pretrained representations from current\nself-supervised learning (SSL) models of speech encode phonological\nfeature-level variations that influence the perception of segmental accent. We\nfocus on three segments: the labiodental approximant, the rhotic tap, and the\nretroflex stop, which are uniformly produced in the English of native speakers\nof Hindi as well as other languages in the Indian sub-continent. We use the\nCSLU Foreign Accented English corpus (Lander, 2007) to extract, for these\nsegments, phonological feature probabilities using Phonet (V\\'asquez-Correa et\nal., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,\n2023) and WavLM (Chen et al., 2022) along with accent judgements by native\nspeakers of American English. Probing analyses show that accent strength is\nbest predicted by a subset of the segment's pretrained representation features,\nin which perceptually salient phonological features that contrast the expected\nAmerican English and realized non-native English segments are given prominent\nweighting. A multinomial logistic regression of pretrained representation-based\nsegment distances from American and Indian English baselines on accent ratings\nreveals strong associations between the odds of accent strength and distances\nfrom the baselines, in the expected directions. These results highlight the\nvalue of self-supervised speech representations for modeling accent perception\nusing interpretable phonological features.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u63a2\u7a76\u8bed\u97f3\u8868\u5f81\u5982\u4f55\u7f16\u7801\u5f71\u54cd\u53e3\u97f3\u611f\u77e5\u7684\u97f3\u7cfb\u7279\u5f81\uff0c\u53d1\u73b0\u7279\u5b9a\u97f3\u6bb5\u7684\u81ea\u76d1\u7763\u8868\u5f81\u7279\u5f81\u80fd\u6709\u6548\u9884\u6d4b\u53e3\u97f3\u5f3a\u5ea6\uff0c\u7a81\u663e\u4e86\u97f3\u7cfb\u7279\u5f81\u5728\u53e3\u97f3\u611f\u77e5\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u53e3\u97f3\u611f\u77e5\u6a21\u578b\u4f4e\u4f30\u4e86\u97f3\u7cfb\u7279\u5f81\u68af\u5ea6\u53d8\u5316\u7684\u4f5c\u7528\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5982\u4f55\u7f16\u7801\u8fd9\u4e9b\u7279\u5f81\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u53e3\u97f3\u611f\u77e5\u673a\u5236\u3002", "method": "\u4f7f\u7528CSLU\u5916\u56fd\u53e3\u97f3\u82f1\u8bed\u8bed\u6599\u5e93\uff0c\u63d0\u53d6\u4e09\u4e2a\u97f3\u6bb5\u7684\u97f3\u7cfb\u7279\u5f81\u6982\u7387\uff0c\u5e76\u7ed3\u5408Wav2Vec2-BERT\u548cWavLM\u7684\u81ea\u76d1\u7763\u8868\u5f81\uff0c\u901a\u8fc7\u63a2\u6d4b\u5206\u6790\u548c\u591a\u9879\u903b\u8f91\u56de\u5f52\u7814\u7a76\u53e3\u97f3\u8bc4\u5206\u7684\u9884\u6d4b\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u53e3\u97f3\u5f3a\u5ea6\u53ef\u901a\u8fc7\u97f3\u6bb5\u7684\u81ea\u76d1\u7763\u8868\u5f81\u7279\u5f81\u5b50\u96c6\u9884\u6d4b\uff0c\u5176\u4e2d\u611f\u77e5\u663e\u8457\u7684\u97f3\u7cfb\u7279\u5f81\u5bf9\u53e3\u97f3\u8bc4\u5206\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u4e0e\u57fa\u7ebf\u8ddd\u79bb\u7684\u5173\u8054\u6027\u7b26\u5408\u9884\u671f\u65b9\u5411\u3002", "conclusion": "\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u4e3a\u57fa\u4e8e\u53ef\u89e3\u91ca\u97f3\u7cfb\u7279\u5f81\u7684\u53e3\u97f3\u611f\u77e5\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u4ef7\u503c\uff0c\u63ed\u793a\u4e86\u97f3\u7cfb\u7279\u5f81\u5728\u53e3\u97f3\u5224\u65ad\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "paper_title_zh": "\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u4e2d\u7684\u97f3\u7cfb\u7279\u5f81\u63a2\u7a76\uff1a\u4ee5\u53e3\u97f3\u611f\u77e5\u4e3a\u4f8b", "abstract_zh": "\u4f20\u7edf\u53e3\u97f3\u611f\u77e5\u6a21\u578b\u4f4e\u4f30\u4e86\u97f3\u7cfb\u7279\u5f81\u68af\u5ea6\u53d8\u5316\u7684\u4f5c\u7528\uff0c\u800c\u542c\u8005\u6b63\u662f\u4f9d\u8d56\u8fd9\u4e9b\u53d8\u5316\u8fdb\u884c\u53e3\u97f3\u5224\u65ad\u3002\u672c\u6587\u7814\u7a76\u4e86\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u8bed\u97f3\u6a21\u578b\u5982\u4f55\u7f16\u7801\u5f71\u54cd\u97f3\u6bb5\u53e3\u97f3\u611f\u77e5\u7684\u97f3\u7cfb\u7279\u5f81\u53d8\u5316\u3002\u6211\u4eec\u805a\u7126\u4e8e\u4e09\u4e2a\u97f3\u6bb5\uff1a\u5507\u9f7f\u8fd1\u97f3\u3001\u95ea\u97f3\u548c\u5377\u820c\u585e\u97f3\uff0c\u8fd9\u4e9b\u97f3\u6bb5\u5728\u5370\u5ea6\u6b21\u5927\u9646\u7684\u82f1\u8bed\u6bcd\u8bed\u8005\u4e2d\u8868\u73b0\u4e00\u81f4\u3002\u4f7f\u7528CSLU\u5916\u56fd\u53e3\u97f3\u82f1\u8bed\u8bed\u6599\u5e93\uff08Lander\uff0c2007\uff09\uff0c\u6211\u4eec\u901a\u8fc7Phonet\uff08V\u00e1squez-Correa\u7b49\uff0c2019\uff09\u63d0\u53d6\u8fd9\u4e9b\u97f3\u6bb5\u7684\u97f3\u7cfb\u7279\u5f81\u6982\u7387\uff0c\u5e76\u7ed3\u5408Wav2Vec2-BERT\uff08Barrault\u7b49\uff0c2023\uff09\u548cWavLM\uff08Chen\u7b49\uff0c2022\uff09\u7684\u81ea\u76d1\u7763\u8868\u5f81\u53ca\u7f8e\u56fd\u82f1\u8bed\u6bcd\u8bed\u8005\u7684\u53e3\u97f3\u8bc4\u5206\u3002\u63a2\u6d4b\u5206\u6790\u8868\u660e\uff0c\u53e3\u97f3\u5f3a\u5ea6\u53ef\u901a\u8fc7\u97f3\u6bb5\u81ea\u76d1\u7763\u8868\u5f81\u7279\u5f81\u7684\u5b50\u96c6\u6700\u4f73\u9884\u6d4b\uff0c\u5176\u4e2d\u611f\u77e5\u663e\u8457\u7684\u97f3\u7cfb\u7279\u5f81\uff08\u5bf9\u6bd4\u7f8e\u56fd\u82f1\u8bed\u9884\u671f\u4e0e\u975e\u6bcd\u8bed\u82f1\u8bed\u5b9e\u9645\u97f3\u6bb5\uff09\u88ab\u8d4b\u4e88\u663e\u8457\u6743\u91cd\u3002\u57fa\u4e8e\u81ea\u76d1\u7763\u8868\u5f81\u7684\u97f3\u6bb5\u8ddd\u79bb\u4e0e\u7f8e\u56fd\u548c\u5370\u5ea6\u82f1\u8bed\u57fa\u7ebf\u7684\u591a\u9879\u903b\u8f91\u56de\u5f52\u663e\u793a\uff0c\u53e3\u97f3\u8bc4\u5206\u4e0e\u57fa\u7ebf\u8ddd\u79bb\u7684\u5173\u8054\u6027\u5728\u9884\u671f\u65b9\u5411\u4e0a\u8868\u73b0\u51fa\u5f3a\u76f8\u5173\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u5728\u5229\u7528\u53ef\u89e3\u91ca\u97f3\u7cfb\u7279\u5f81\u5efa\u6a21\u53e3\u97f3\u611f\u77e5\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.17469", "pdf": "https://arxiv.org/pdf/2506.17469", "abs": "https://arxiv.org/abs/2506.17469", "authors": ["Thomas Plante St-Cyr", "Fran\u00e7ois Duhaime", "Jean-S\u00e9bastien Dub\u00e9", "Simon Grenier"], "title": "Photogranulometry -- Dataset of soil images with corresponding particle size distributions", "categories": ["cs.CV", "I.5.4; I.2.10"], "comment": "8 pages, 10 figures, conference", "summary": "Traditional particle size distribution (PSD) analyses create significant\ndowntime and are expensive in labor and maintenance. These drawbacks could be\nalleviated using optical grain size analysis integrated into routine\ngeotechnical laboratory workflow. This paper presents a high-resolution dataset\nof 12,714 images of 321 different soil samples collected in the Montreal,\nQuebec region, alongside their PSD analysis. It is designed to provide a robust\nstarting point for training convolutional neural networks (CNN) in geotechnical\napplications. Soil samples were photographed in a standardized top-view\nposition with a resolution of 45 MP and a minimum scale of 39.4 micrometers per\npixel, both in their moist and dry states. A custom test bench employing 13x9\ninch white aluminum trays, on which the samples are spread in a thin layer, was\nused. For samples exceeding a size limit, a coning and quartering method was\nemployed for mass reduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b12,714\u5f20\u56fe\u50cf\u548c321\u79cd\u571f\u58e4\u6837\u672c\u7684\u7c92\u5f84\u5206\u5e03\u6570\u636e\uff0c\u65e8\u5728\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u8bad\u7ec3\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u7684\u7c92\u5f84\u5206\u5e03\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u901a\u8fc7\u5149\u5b66\u7c92\u5ea6\u5206\u6790\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5ca9\u571f\u5de5\u7a0b\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u548c\u5e94\u7528\u3002", "method": "\u7814\u7a76\u91c7\u96c6\u4e86321\u79cd\u571f\u58e4\u6837\u672c\uff0c\u62cd\u6444\u4e8612,714\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0845 MP\uff0c\u6700\u5c0f\u5c3a\u5ea6\u4e3a39.4\u5fae\u7c73/\u50cf\u7d20\uff09\uff0c\u5e76\u5728\u6e7f\u6da6\u548c\u5e72\u71e5\u72b6\u6001\u4e0b\u8fdb\u884c\u6807\u51c6\u5316\u62cd\u6444\u3002\u4f7f\u7528\u5b9a\u5236\u6d4b\u8bd5\u53f0\u548c13x9\u82f1\u5bf8\u767d\u8272\u94dd\u6258\u76d8\uff0c\u5bf9\u6837\u672c\u8fdb\u884c\u8584\u5c42\u94fa\u5c55\uff0c\u5927\u6837\u672c\u91c7\u7528\u56db\u5206\u6cd5\u51cf\u91cf\u3002", "result": "\u63d0\u4f9b\u4e86\u5305\u542b12,714\u5f20\u56fe\u50cf\u548c\u5bf9\u5e94\u7c92\u5f84\u5206\u5e03\u7684\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff0c\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u5149\u5b66\u7c92\u5ea6\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\uff0c\u6709\u671b\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u7684\u6210\u672c\u548c\u8017\u65f6\u3002", "paper_title_zh": "\u5149\u7c92\u5ea6\u5206\u6790\u2014\u2014\u5305\u542b\u5bf9\u5e94\u7c92\u5f84\u5206\u5e03\u7684\u571f\u58e4\u56fe\u50cf\u6570\u636e\u96c6", "abstract_zh": "\u4f20\u7edf\u7684\u7c92\u5f84\u5206\u5e03\uff08PSD\uff09\u5206\u6790\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u7ef4\u62a4\u548c\u4eba\u529b\u6295\u5165\u5927\u3002\u901a\u8fc7\u5c06\u5149\u5b66\u7c92\u5ea6\u5206\u6790\u6574\u5408\u5230\u5e38\u89c4\u5ca9\u571f\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff0c\u5305\u542b12,714\u5f20\u56fe\u50cf\u548c321\u79cd\u5728\u52a0\u62ff\u5927\u9b41\u5317\u514b\u8499\u7279\u5229\u5c14\u5730\u533a\u91c7\u96c6\u7684\u571f\u58e4\u6837\u672c\u53ca\u5176PSD\u5206\u6790\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u5ca9\u571f\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u8bad\u7ec3\u63d0\u4f9b\u575a\u5b9e\u57fa\u7840\u3002\u571f\u58e4\u6837\u672c\u5728\u6e7f\u6da6\u548c\u5e72\u71e5\u72b6\u6001\u4e0b\u4ee5\u6807\u51c6\u5316\u4fef\u89c6\u89d2\u5ea6\u62cd\u6444\uff0c\u5206\u8fa8\u7387\u4e3a45 MP\uff0c\u6700\u5c0f\u5c3a\u5ea6\u4e3a39.4\u5fae\u7c73/\u50cf\u7d20\u3002\u4f7f\u7528\u5b9a\u5236\u6d4b\u8bd5\u53f0\u548c13x9\u82f1\u5bf8\u767d\u8272\u94dd\u6258\u76d8\uff0c\u5c06\u6837\u672c\u8584\u5c42\u94fa\u5c55\u3002\u5bf9\u4e8e\u8d85\u8fc7\u5c3a\u5bf8\u9650\u5236\u7684\u6837\u672c\uff0c\u91c7\u7528\u56db\u5206\u6cd5\u51cf\u91cf\u3002"}}
{"id": "2506.17792", "pdf": "https://arxiv.org/pdf/2506.17792", "abs": "https://arxiv.org/abs/2506.17792", "authors": ["Alexandros Evangelidis", "Gricel V\u00e1zquez", "Simos Gerasimou"], "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition", "categories": ["cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Software-intensive systems, such as software product lines and robotics,\nutilise Markov decision processes (MDPs) to capture uncertainty and analyse\nsequential decision-making problems. Despite the usefulness of conventional\npolicy synthesis methods, they fail to scale to large state spaces. Our\napproach addresses this issue and accelerates policy synthesis in large MDPs by\ndynamically refining the MDP and iteratively selecting the most fragile MDP\nregions for refinement. This iterative procedure offers a balance between\naccuracy and efficiency, as refinement occurs only when necessary. Through a\ncomprehensive empirical evaluation comprising diverse case studies and MDPs up\nto 1M states, we demonstrate significant performance improvements yielded by\nour approach compared to the leading probabilistic model checker PRISM (up to\n2x), thus offering a very competitive solution for real-world policy synthesis\ntasks in larger MDPs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u5c42\u5757\u5206\u89e3\u52a0\u901f\u5927\u89c4\u6a21MDP\u7b56\u7565\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u52a8\u6001\u4f18\u5316MDP\u5e76\u8fed\u4ee3\u9009\u62e9\u8106\u5f31\u533a\u57df\u8fdb\u884c\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7b56\u7565\u5408\u6210\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u4e2d\u96be\u4ee5\u6269\u5c55\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316MDP\u63d0\u5347\u7b56\u7565\u5408\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5757\u5206\u89e3\u65b9\u6cd5\uff0c\u52a8\u6001\u7ec6\u5316MDP\u5e76\u8fed\u4ee3\u9009\u62e9\u6700\u8106\u5f31\u7684\u533a\u57df\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u6548\u7387\u4e0e\u7cbe\u5ea6\u7684\u5e73\u8861\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u6848\u4f8b\u7814\u7a76\u548c\u9ad8\u8fbe100\u4e07\u72b6\u6001\u7684MDP\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4f18\u4e8ePRISM\u6a21\u578b\u68c0\u67e5\u5668\uff08\u6700\u9ad8\u63d0\u53472\u500d\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21MDP\u4e2d\u7684\u5b9e\u9645\u7b56\u7565\u5408\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5177\u6709\u7ade\u4e89\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u5206\u5c42\u5757\u5206\u89e3\u7684MDP\u9ad8\u6548\u7b56\u7565\u5408\u6210\u65b9\u6cd5", "abstract_zh": "\u8f6f\u4ef6\u5bc6\u96c6\u578b\u7cfb\u7edf\uff08\u5982\u8f6f\u4ef6\u4ea7\u54c1\u7ebf\u548c\u673a\u5668\u4eba\u6280\u672f\uff09\u5229\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6765\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u5e76\u5206\u6790\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u3002\u5c3d\u7ba1\u4f20\u7edf\u7b56\u7565\u5408\u6210\u65b9\u6cd5\u5177\u6709\u4e00\u5b9a\u5b9e\u7528\u6027\uff0c\u4f46\u5176\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u4f18\u5316MDP\u5e76\u8fed\u4ee3\u9009\u62e9\u6700\u8106\u5f31\u7684\u533a\u57df\u8fdb\u884c\u7ec6\u5316\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u5927\u89c4\u6a21MDP\u4e2d\u7684\u7b56\u7565\u5408\u6210\u3002\u8fd9\u79cd\u8fed\u4ee3\u8fc7\u7a0b\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u56e0\u4e3a\u7ec6\u5316\u4ec5\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u3002\u901a\u8fc7\u5305\u542b\u591a\u6837\u5316\u6848\u4f8b\u7814\u7a76\u548c\u9ad8\u8fbe100\u4e07\u72b6\u6001\u7684MDP\u7684\u5168\u9762\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u76f8\u8f83\u4e8e\u9886\u5148\u7684\u6982\u7387\u6a21\u578b\u68c0\u67e5\u5668PRISM\uff08\u6700\u9ad8\u63d0\u53472\u500d\uff09\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u4ece\u800c\u4e3a\u5927\u89c4\u6a21MDP\u4e2d\u7684\u5b9e\u9645\u7b56\u7565\u5408\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17578", "pdf": "https://arxiv.org/pdf/2506.17578", "abs": "https://arxiv.org/abs/2506.17578", "authors": ["Lingxiao Zeng", "Yiqi Tong", "Wei Guo", "Huarui Wu", "Lihao Ge", "Yijun Ye", "Fuzhen Zhuang", "Deqing Wang", "Wei Guo", "Cheng Chen"], "title": "AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Agricultural named entity recognition is a specialized task focusing on\nidentifying distinct agricultural entities within vast bodies of text,\nincluding crops, diseases, pests, and fertilizers. It plays a crucial role in\nenhancing information extraction from extensive agricultural text resources.\nHowever, the scarcity of high-quality agricultural datasets, particularly in\nChinese, has resulted in suboptimal performance when employing mainstream\nmethods for this purpose. Most earlier works only focus on annotating\nagricultural entities while overlook the profound correlation of agriculture\nwith hydrology and meteorology. To fill this blank, we present AgriCHN, a\ncomprehensive open-source Chinese resource designed to promote the accuracy of\nautomated agricultural entity annotation. The AgriCHN dataset has been\nmeticulously curated from a wealth of agricultural articles, comprising a total\nof 4,040 sentences and encapsulating 15,799 agricultural entity mentions\nspanning 27 diverse entity categories. Furthermore, it encompasses entities\nfrom hydrology to meteorology, thereby enriching the diversity of entities\nconsidered. Data validation reveals that, compared with relevant resources,\nAgriCHN demonstrates outstanding data quality, attributable to its richer\nagricultural entity types and more fine-grained entity divisions. A benchmark\ntask has also been constructed using several state-of-the-art neural NER\nmodels. Extensive experimental results highlight the significant challenge\nposed by AgriCHN and its potential for further research.", "AI": {"tldr": "AgriCHN\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u6db5\u76d627\u7c7b\u519c\u4e1a\u5b9e\u4f53\uff0c\u5e76\u6574\u5408\u6c34\u6587\u548c\u6c14\u8c61\u5b9e\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u519c\u4e1a\u4fe1\u606f\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u591a\u5ffd\u7565\u519c\u4e1a\u4e0e\u6c34\u6587\u3001\u6c14\u8c61\u7684\u5173\u8054\u3002AgriCHN\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u65e8\u5728\u63d0\u5347\u519c\u4e1a\u5b9e\u4f53\u6807\u6ce8\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "AgriCHN\u4ece\u5927\u91cf\u519c\u4e1a\u6587\u7ae0\u4e2d\u7cbe\u5fc3\u7b5b\u90094040\u4e2a\u53e5\u5b50\uff0c\u6807\u6ce815799\u4e2a\u519c\u4e1a\u5b9e\u4f53\uff0c\u6db5\u76d627\u7c7b\u5b9e\u4f53\uff0c\u5e76\u6574\u5408\u6c34\u6587\u548c\u6c14\u8c61\u6570\u636e\u3002\u901a\u8fc7\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5229\u7528\u524d\u6cbf\u795e\u7ecfNER\u6a21\u578b\u6784\u5efa\u57fa\u51c6\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgriCHN\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u5b9e\u4f53\u7c7b\u578b\u4e30\u5bcc\u4e14\u5212\u5206\u7ec6\u81f4\uff0c\u4e3a\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5176\u6311\u6218\u6027\u548c\u7814\u7a76\u6f5c\u529b\u3002", "conclusion": "AgriCHN\u662f\u4e00\u4e2a\u5168\u9762\u4e14\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u4e3a\u519c\u4e1a\u4fe1\u606f\u63d0\u53d6\u548c\u76f8\u5173\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "paper_title_zh": "AgriCHN\uff1a\u4e00\u4e2a\u5168\u9762\u7684\u8de8\u9886\u57df\u4e2d\u6587\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u8d44\u6e90", "abstract_zh": "\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u662f\u4e00\u9879\u4e13\u6ce8\u4e8e\u4ece\u5927\u91cf\u6587\u672c\u4e2d\u8bc6\u522b\u519c\u4e1a\u5b9e\u4f53\uff08\u5982\u4f5c\u7269\u3001\u75c5\u5bb3\u3001\u5bb3\u866b\u548c\u80a5\u6599\uff09\u7684\u4efb\u52a1\uff0c\u5bf9\u63d0\u5347\u519c\u4e1a\u6587\u672c\u4fe1\u606f\u63d0\u53d6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u519c\u4e1a\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u5bfc\u81f4\u4e3b\u6d41\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7814\u7a76\u591a\u4ec5\u6807\u6ce8\u519c\u4e1a\u5b9e\u4f53\uff0c\u800c\u5ffd\u7565\u4e86\u519c\u4e1a\u4e0e\u6c34\u6587\u3001\u6c14\u8c61\u7684\u5173\u8054\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AgriCHN\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5f00\u6e90\u4e2d\u6587\u8d44\u6e90\uff0c\u65e8\u5728\u63d0\u5347\u519c\u4e1a\u5b9e\u4f53\u81ea\u52a8\u6807\u6ce8\u7684\u51c6\u786e\u6027\u3002AgriCHN\u6570\u636e\u96c6\u4ece\u5927\u91cf\u519c\u4e1a\u6587\u7ae0\u4e2d\u7cbe\u90094040\u4e2a\u53e5\u5b50\uff0c\u6807\u6ce815799\u4e2a\u519c\u4e1a\u5b9e\u4f53\uff0c\u6db5\u76d627\u7c7b\u5b9e\u4f53\uff0c\u5e76\u6574\u5408\u6c34\u6587\u548c\u6c14\u8c61\u5b9e\u4f53\uff0c\u4e30\u5bcc\u4e86\u5b9e\u4f53\u591a\u6837\u6027\u3002\u6570\u636e\u9a8c\u8bc1\u8868\u660e\uff0cAgriCHN\u56e0\u66f4\u4e30\u5bcc\u7684\u5b9e\u4f53\u7c7b\u578b\u548c\u66f4\u7ec6\u81f4\u7684\u5212\u5206\u800c\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u8fd8\u5229\u7528\u524d\u6cbf\u795e\u7ecfNER\u6a21\u578b\u6784\u5efa\u4e86\u57fa\u51c6\u4efb\u52a1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u51f8\u663e\u4e86AgriCHN\u7684\u6311\u6218\u6027\u548c\u7814\u7a76\u6f5c\u529b\u3002"}}
{"id": "2506.17500", "pdf": "https://arxiv.org/pdf/2506.17500", "abs": "https://arxiv.org/abs/2506.17500", "authors": ["Julio Silva-Rodr\u00edguez", "Fereshteh Shakeri", "Houda Bahig", "Jose Dolz", "Ismail Ben Ayed"], "title": "Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation", "categories": ["cs.CV"], "comment": "MICCAI 2025. Code: https://github.com/jusiro/SS-Text", "summary": "Vision-language models (VLMs) are gaining attention in medical image\nanalysis. These are pre-trained on large, heterogeneous data sources, yielding\nrich and transferable representations. Notably, the combination of\nmodality-specialized VLMs with few-shot adaptation has provided fruitful\nresults, enabling the efficient deployment of high-performing solutions.\nHowever, previous works on this topic make strong assumptions about the\ndistribution of adaptation data, which are unrealistic in the medical domain.\nFirst, prior art assumes access to a balanced support set, a condition that\nbreaks the natural imbalance in disease prevalence found in real-world\nscenarios. Second, these works typically assume the presence of an additional\nvalidation set to fix critical hyper-parameters, which is highly\ndata-inefficient. This work challenges these favorable deployment scenarios and\nintroduces a realistic, imbalanced, validation-free adaptation setting. Our\nextensive benchmark across various modalities and downstream tasks demonstrates\nthat current methods systematically compromise their performance when operating\nunder realistic conditions, occasionally even performing worse than zero-shot\ninference. Also, we introduce a training-free linear probe that adaptively\nblends visual and textual supervision. Detailed studies demonstrate that the\nproposed solver is a strong, efficient baseline, enabling robust adaptation in\nchallenging scenarios.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u4e0d\u73b0\u5b9e\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5e73\u8861\u6570\u636e\u96c6\u6216\u9a8c\u8bc1\u96c6\u7684\u771f\u5b9e\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u7ebf\u6027\u63a2\u9488\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66VLM\u5c11\u6837\u672c\u9002\u5e94\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5206\u5e03\u5e73\u8861\u4e14\u9700\u8981\u9a8c\u8bc1\u96c6\uff0c\u8fd9\u4e0e\u5b9e\u9645\u533b\u5b66\u573a\u666f\u4e2d\u75be\u75c5\u5206\u5e03\u4e0d\u5747\u8861\u4e14\u6570\u636e\u7a00\u7f3a\u7684\u73b0\u5b9e\u4e0d\u7b26\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5e73\u8861\u652f\u6301\u96c6\u6216\u9a8c\u8bc1\u96c6\u7684\u771f\u5b9e\u9002\u5e94\u8bbe\u7f6e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u81ea\u9002\u5e94\u7ebf\u6027\u63a2\u9488\u65b9\u6cd5\uff0c\u52a8\u6001\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u751a\u81f3\u53ef\u80fd\u4e0d\u5982\u96f6\u6837\u672c\u63a8\u7406\uff1b\u800c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\u4e14\u9ad8\u6548\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u533b\u5b66VLM\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c11\u6837\u672c\u9002\u5e94\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "paper_title_zh": "\u5c11\u6837\u672c\u9002\u5e94\uff0c\u771f\u5b9e\u573a\u666f\uff1a\u65e0\u9700\u5e73\u8861\u96c6\u6216\u9a8c\u8bc1\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u5e94", "abstract_zh": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53d7\u5230\u5173\u6ce8\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u751f\u6210\u4e30\u5bcc\u4e14\u53ef\u8fc1\u79fb\u7684\u8868\u793a\u3002\u7279\u522b\u662f\uff0c\u7ed3\u5408\u6a21\u6001\u4e13\u7528VLM\u4e0e\u5c11\u6837\u672c\u9002\u5e94\u5df2\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u9ad8\u6548\u90e8\u7f72\u3002\u7136\u800c\uff0c\u6b64\u524d\u7814\u7a76\u5bf9\u9002\u5e94\u6570\u636e\u5206\u5e03\u7684\u5047\u8bbe\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u4e0e\u533b\u5b66\u9886\u57df\u7684\u5b9e\u9645\u60c5\u51b5\u4e0d\u7b26\u3002\u9996\u5148\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u652f\u6301\u96c6\u662f\u5e73\u8861\u7684\uff0c\u8fd9\u4e0e\u771f\u5b9e\u4e16\u754c\u4e2d\u75be\u75c5\u6d41\u884c\u7387\u7684\u4e0d\u5747\u8861\u6027\u76f8\u77db\u76fe\u3002\u5176\u6b21\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u9a8c\u8bc1\u96c6\u6765\u8c03\u6574\u5173\u952e\u8d85\u53c2\u6570\uff0c\u8fd9\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u6311\u6218\u4e86\u8fd9\u4e9b\u7406\u60f3\u5316\u7684\u90e8\u7f72\u573a\u666f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u771f\u5b9e\u3001\u4e0d\u5747\u8861\u4e14\u65e0\u9700\u9a8c\u8bc1\u7684\u9002\u5e94\u8bbe\u7f6e\u3002\u901a\u8fc7\u591a\u79cd\u6a21\u6001\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u751a\u81f3\u53ef\u80fd\u4e0d\u5982\u96f6\u6837\u672c\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u7ebf\u6027\u63a2\u9488\u65b9\u6cd5\uff0c\u52a8\u6001\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u76d1\u7763\u3002\u8be6\u7ec6\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u57fa\u7ebf\uff0c\u80fd\u591f\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b9e\u73b0\u7a33\u5065\u9002\u5e94\u3002"}}
{"id": "2506.17834", "pdf": "https://arxiv.org/pdf/2506.17834", "abs": "https://arxiv.org/abs/2506.17834", "authors": ["Carter Blair", "Kate Larson", "Edith Law"], "title": "Reflective Verbal Reward Design for Pluralistic Alignment", "categories": ["cs.AI", "cs.HC", "I.2.6; H.5.2; I.2.7"], "comment": "9 pages, 3 figures, accepted to the IJCAI 2025 Human-Centred AI\n  track. Project repository at: https://osf.io/8yxf2/", "summary": "AI agents are commonly aligned with \"human values\" through reinforcement\nlearning from human feedback (RLHF), where a single reward model is learned\nfrom aggregated human feedback and used to align an agent's behavior. However,\nhuman values are not homogeneous--different people hold distinct and sometimes\nconflicting values. Aggregating feedback into a single reward model risks\ndisproportionately suppressing minority preferences. To address this, we\npresent a novel reward modeling approach for learning individualized reward\nmodels. Our approach uses a language model to guide users through reflective\ndialogues where they critique agent behavior and construct their preferences.\nThis personalized dialogue history, containing the user's reflections and\ncritiqued examples, is then used as context for another language model that\nserves as an individualized reward function (what we call a \"verbal reward\nmodel\") for evaluating new trajectories. In studies with 30 participants, our\nmethod achieved a 9-12% improvement in accuracy over non-reflective verbal\nreward models while being more sample efficient than traditional supervised\nlearning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u5bf9\u8bdd\u5b66\u4e60\u4e2a\u4f53\u5316\u5956\u52b1\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRLHF\u65b9\u6cd5\u4e2d\u5c11\u6570\u7fa4\u4f53\u504f\u597d\u88ab\u538b\u5236\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6837\u672c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRLHF\u65b9\u6cd5\u901a\u8fc7\u805a\u5408\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\u5355\u4e00\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u4eba\u7c7b\u4ef7\u503c\u89c2\u591a\u6837\u4e14\u53ef\u80fd\u5b58\u5728\u51b2\u7a81\uff0c\u5bfc\u81f4\u5c11\u6570\u7fa4\u4f53\u504f\u597d\u88ab\u538b\u5236\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e2a\u4f53\u5316\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7528\u6237\u901a\u8fc7\u53cd\u601d\u6027\u5bf9\u8bdd\u6784\u5efa\u4e2a\u6027\u5316\u504f\u597d\uff0c\u5e76\u5c06\u5bf9\u8bdd\u5386\u53f2\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u8bad\u7ec3\u4e2a\u4f53\u5316\u5956\u52b1\u6a21\u578b\uff08\u79f0\u4e3a\u201c\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u201d\uff09\u4ee5\u8bc4\u4f30\u65b0\u884c\u4e3a\u8f68\u8ff9\u3002", "result": "\u572830\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u975e\u53cd\u601d\u6027\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad89-12%\uff0c\u4e14\u6837\u672c\u6548\u7387\u9ad8\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e2a\u4f53\u5316\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ef7\u503c\u89c2\u591a\u6837\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u7528\u4e8e\u591a\u5143\u5bf9\u9f50\u7684\u53cd\u601d\u6027\u8bed\u8a00\u5956\u52b1\u8bbe\u8ba1", "abstract_zh": "AI\u4ee3\u7406\u901a\u5e38\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e0e\u201c\u4eba\u7c7b\u4ef7\u503c\u89c2\u201d\u5bf9\u9f50\uff0c\u5176\u4e2d\u5355\u4e00\u5956\u52b1\u6a21\u578b\u4ece\u805a\u5408\u7684\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60\u5e76\u7528\u4e8e\u5bf9\u9f50\u4ee3\u7406\u884c\u4e3a\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u4ef7\u503c\u89c2\u5e76\u975e\u540c\u8d28\u5316\u2014\u2014\u4e0d\u540c\u4eba\u6301\u6709\u4e0d\u540c\u751a\u81f3\u51b2\u7a81\u7684\u4ef7\u503c\u89c2\u3002\u5c06\u53cd\u9988\u805a\u5408\u4e3a\u5355\u4e00\u5956\u52b1\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u5c11\u6570\u504f\u597d\u88ab\u538b\u5236\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u4e2a\u4f53\u5316\u5956\u52b1\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7528\u6237\u901a\u8fc7\u53cd\u601d\u6027\u5bf9\u8bdd\u6279\u5224\u4ee3\u7406\u884c\u4e3a\u5e76\u6784\u5efa\u5176\u504f\u597d\u3002\u8fd9\u79cd\u4e2a\u6027\u5316\u5bf9\u8bdd\u5386\u53f2\uff08\u5305\u542b\u7528\u6237\u7684\u53cd\u601d\u548c\u6279\u5224\u793a\u4f8b\uff09\u968f\u540e\u7528\u4f5c\u53e6\u4e00\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\uff0c\u8be5\u6a21\u578b\u4f5c\u4e3a\u4e2a\u4f53\u5316\u5956\u52b1\u51fd\u6570\uff08\u6211\u4eec\u79f0\u4e4b\u4e3a\u201c\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u201d\uff09\u7528\u4e8e\u8bc4\u4f30\u65b0\u8f68\u8ff9\u3002\u572830\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u975e\u53cd\u601d\u6027\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad89-12%\uff0c\u540c\u65f6\u6837\u672c\u6548\u7387\u9ad8\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2506.17603", "pdf": "https://arxiv.org/pdf/2506.17603", "abs": "https://arxiv.org/abs/2506.17603", "authors": ["Jonathan Sakunkoo", "Annabella Sakunkoo"], "title": "Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Morphological defectivity is an intriguing and understudied phenomenon in\nlinguistics. Addressing defectivity, where expected inflectional forms are\nabsent, is essential for improving the accuracy of NLP tools in morphologically\nrich languages. However, traditional linguistic resources often lack coverage\nof morphological gaps as such knowledge requires significant human expertise\nand effort to document and verify. For scarce linguistic phenomena in\nunder-explored languages, Wikipedia and Wiktionary often serve as among the few\naccessible resources. Despite their extensive reach, their reliability has been\na subject of controversy. This study customizes a novel neural morphological\nanalyzer to annotate Latin and Italian corpora. Using the massive annotated\ndata, crowd-sourced lists of defective verbs compiled from Wiktionary are\nvalidated computationally. Our results indicate that while Wiktionary provides\na highly reliable account of Italian morphological gaps, 7% of Latin lemmata\nlisted as defective show strong corpus evidence of being non-defective. This\ndiscrepancy highlights potential limitations of crowd-sourced wikis as\ndefinitive sources of linguistic knowledge, particularly for less-studied\nphenomena and languages, despite their value as resources for rare linguistic\nfeatures. By providing scalable tools and methods for quality assurance of\ncrowd-sourced data, this work advances computational morphology and expands\nlinguistic knowledge of defectivity in non-English, morphologically rich\nlanguages.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9a\u5236\u795e\u7ecf\u5f62\u6001\u5206\u6790\u5668\u9a8c\u8bc1\u4e86\u7ef4\u57fa\u8bcd\u5178\u4e2d\u62c9\u4e01\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u5f62\u6001\u7a7a\u7f3a\u6570\u636e\uff0c\u53d1\u73b0\u610f\u5927\u5229\u8bed\u6570\u636e\u9ad8\u5ea6\u53ef\u9760\uff0c\u4f46\u62c9\u4e01\u8bed\u67097%\u7684\u8bcd\u6761\u88ab\u8bef\u6807\u4e3a\u975e\u7a7a\u7f3a\uff0c\u63ed\u793a\u4e86\u4f17\u5305\u6570\u636e\u5728\u672a\u88ab\u5145\u5206\u7814\u7a76\u8bed\u8a00\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f62\u6001\u7a7a\u7f3a\u662f\u8bed\u8a00\u5b66\u4e2d\u4e00\u4e2a\u6709\u8da3\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u73b0\u8c61\uff0c\u5bf9\u63d0\u5347\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684NLP\u5de5\u5177\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u8d44\u6e90\u5f80\u5f80\u7f3a\u4e4f\u5bf9\u5f62\u6001\u7a7a\u7f3a\u7684\u8986\u76d6\uff0c\u800c\u7ef4\u57fa\u8bcd\u5178\u7b49\u4f17\u5305\u8d44\u6e90\u7684\u53ef\u9760\u6027\u5b58\u5728\u4e89\u8bae\uff0c\u56e0\u6b64\u9700\u8981\u9a8c\u8bc1\u5176\u6570\u636e\u8d28\u91cf\u3002", "method": "\u7814\u7a76\u5b9a\u5236\u4e86\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u5206\u6790\u5668\uff0c\u7528\u4e8e\u6807\u6ce8\u62c9\u4e01\u8bed\u548c\u610f\u5927\u5229\u8bed\u8bed\u6599\u5e93\uff0c\u5e76\u5229\u7528\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u8ba1\u7b97\u9a8c\u8bc1\u7ef4\u57fa\u8bcd\u5178\u4e2d\u4f17\u5305\u6574\u7406\u7684\u5f62\u6001\u7a7a\u7f3a\u52a8\u8bcd\u5217\u8868\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7ef4\u57fa\u8bcd\u5178\u5bf9\u610f\u5927\u5229\u8bed\u5f62\u6001\u7a7a\u7f3a\u7684\u63cf\u8ff0\u9ad8\u5ea6\u53ef\u9760\uff0c\u4f46\u62c9\u4e01\u8bed\u4e2d\u67097%\u88ab\u6807\u8bb0\u4e3a\u7a7a\u7f3a\u7684\u8bcd\u6761\u5728\u8bed\u6599\u5e93\u4e2d\u8868\u73b0\u51fa\u975e\u7a7a\u7f3a\u7684\u8bc1\u636e\uff0c\u63ed\u793a\u4e86\u4f17\u5305\u6570\u636e\u5728\u672a\u88ab\u5145\u5206\u7814\u7a76\u8bed\u8a00\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u4f17\u5305\u8d44\u6e90\u5bf9\u7f55\u89c1\u8bed\u8a00\u7279\u5f81\u5177\u6709\u4ef7\u503c\uff0c\u4f46\u5176\u4f5c\u4e3a\u8bed\u8a00\u5b66\u77e5\u8bc6\u7684\u6743\u5a01\u6765\u6e90\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u8bed\u8a00\u548c\u73b0\u8c61\u4e2d\u3002\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u5f62\u6001\u5b66\u7684\u53d1\u5c55\uff0c\u5e76\u6269\u5c55\u4e86\u5bf9\u975e\u82f1\u8bed\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e2d\u5f62\u6001\u7a7a\u7f3a\u7684\u8ba4\u77e5\u3002", "paper_title_zh": "\u6ce8\u610f\u5dee\u8ddd\uff1a\u8bc4\u4f30\u7ef4\u57fa\u8bcd\u5178\u4e2d\u4f17\u5305\u8bed\u8a00\u77e5\u8bc6\u5bf9\u4e24\u79cd\u76f8\u5173\u8bed\u8a00\u5f62\u6001\u7a7a\u7f3a\u7684\u63cf\u8ff0", "abstract_zh": "\u5f62\u6001\u7a7a\u7f3a\u662f\u8bed\u8a00\u5b66\u4e2d\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u73b0\u8c61\u3002\u89e3\u51b3\u5f62\u6001\u7a7a\u7f3a\u95ee\u9898\uff08\u5373\u9884\u671f\u5c48\u6298\u5f62\u5f0f\u7f3a\u5931\u7684\u60c5\u51b5\uff09\u5bf9\u63d0\u5347\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684NLP\u5de5\u5177\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u8bed\u8a00\u5b66\u8d44\u6e90\u5f80\u5f80\u7f3a\u4e4f\u5bf9\u5f62\u6001\u7a7a\u7f3a\u7684\u8986\u76d6\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u77e5\u8bc6\u9700\u8981\u5927\u91cf\u4eba\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\u6765\u8bb0\u5f55\u548c\u9a8c\u8bc1\u3002\u5bf9\u4e8e\u672a\u88ab\u5145\u5206\u63a2\u7d22\u8bed\u8a00\u4e2d\u7684\u7a00\u7f3a\u8bed\u8a00\u73b0\u8c61\uff0c\u7ef4\u57fa\u767e\u79d1\u548c\u7ef4\u57fa\u8bcd\u5178\u901a\u5e38\u662f\u5c11\u6570\u53ef\u8bbf\u95ee\u7684\u8d44\u6e90\u4e4b\u4e00\u3002\u5c3d\u7ba1\u5b83\u4eec\u8986\u76d6\u9762\u5e7f\uff0c\u4f46\u5176\u53ef\u9760\u6027\u4e00\u76f4\u5b58\u5728\u4e89\u8bae\u3002\u672c\u7814\u7a76\u5b9a\u5236\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u5f62\u6001\u5206\u6790\u5668\uff0c\u7528\u4e8e\u6807\u6ce8\u62c9\u4e01\u8bed\u548c\u610f\u5927\u5229\u8bed\u8bed\u6599\u5e93\u3002\u5229\u7528\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u8ba1\u7b97\u9a8c\u8bc1\u4e86\u7ef4\u57fa\u8bcd\u5178\u4e2d\u4f17\u5305\u6574\u7406\u7684\u5f62\u6001\u7a7a\u7f3a\u52a8\u8bcd\u5217\u8868\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u7ef4\u57fa\u8bcd\u5178\u5bf9\u610f\u5927\u5229\u8bed\u5f62\u6001\u7a7a\u7f3a\u7684\u63cf\u8ff0\u9ad8\u5ea6\u53ef\u9760\uff0c\u4f46\u62c9\u4e01\u8bed\u4e2d\u67097%\u88ab\u6807\u8bb0\u4e3a\u7a7a\u7f3a\u7684\u8bcd\u6761\u5728\u8bed\u6599\u5e93\u4e2d\u8868\u73b0\u51fa\u975e\u7a7a\u7f3a\u7684\u8bc1\u636e\u3002\u8fd9\u79cd\u5dee\u5f02\u51f8\u663e\u4e86\u4f17\u5305\u7ef4\u57fa\u4f5c\u4e3a\u8bed\u8a00\u5b66\u77e5\u8bc6\u6743\u5a01\u6765\u6e90\u7684\u6f5c\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u73b0\u8c61\u548c\u8bed\u8a00\u4e2d\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5bf9\u7f55\u89c1\u8bed\u8a00\u7279\u5f81\u5177\u6709\u4ef7\u503c\u3002\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u6765\u4fdd\u8bc1\u4f17\u5305\u6570\u636e\u7684\u8d28\u91cf\uff0c\u672c\u7814\u7a76\u63a8\u52a8\u4e86\u8ba1\u7b97\u5f62\u6001\u5b66\u7684\u53d1\u5c55\uff0c\u5e76\u6269\u5c55\u4e86\u5bf9\u975e\u82f1\u8bed\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e2d\u5f62\u6001\u7a7a\u7f3a\u7684\u8ba4\u77e5\u3002"}}
{"id": "2506.17503", "pdf": "https://arxiv.org/pdf/2506.17503", "abs": "https://arxiv.org/abs/2506.17503", "authors": ["Julio Silva-Rodr\u00edguez", "Ismail Ben Ayed", "Jose Dolz"], "title": "Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction", "categories": ["cs.CV"], "comment": "MICCAI 2025. Code: https://github.com/jusiro/SCA-T", "summary": "Medical vision-language models (VLMs) have demonstrated unprecedented\ntransfer capabilities and are being increasingly adopted for data-efficient\nimage classification. Despite its growing popularity, its reliability aspect\nremains largely unexplored. This work explores the split conformal prediction\n(SCP) framework to provide trustworthiness guarantees when transferring such\nmodels based on a small labeled calibration set. Despite its potential, the\ngeneralist nature of the VLMs' pre-training could negatively affect the\nproperties of the predicted conformal sets for specific tasks. While common\npractice in transfer learning for discriminative purposes involves an\nadaptation stage, we observe that deploying such a solution for conformal\npurposes is suboptimal since adapting the model using the available calibration\ndata breaks the rigid exchangeability assumptions for test data in SCP. To\naddress this issue, we propose transductive split conformal adaptation (SCA-T),\na novel pipeline for transfer learning on conformal scenarios, which performs\nan unsupervised transductive adaptation jointly on calibration and test data.\nWe present comprehensive experiments utilizing medical VLMs across various\nimage modalities, transfer tasks, and non-conformity scores. Our framework\noffers consistent gains in efficiency and conditional coverage compared to SCP,\nmaintaining the same empirical guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8f6c\u5bfc\u5f0f\u5206\u5272\u5171\u5f62\u9002\u5e94\uff08SCA-T\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u4e2d\u63d0\u4f9b\u53ef\u4fe1\u5ea6\u4fdd\u8bc1\u3002\u901a\u8fc7\u8054\u5408\u6821\u51c6\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u65e0\u76d1\u7763\u8f6c\u5bfc\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u5272\u5171\u5f62\u9884\u6d4b\uff08SCP\uff09\u5728\u6a21\u578b\u9002\u5e94\u65f6\u7834\u574f\u6570\u636e\u4ea4\u6362\u6027\u5047\u8bbe\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6761\u4ef6\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u4f20\u7edf\u5206\u5272\u5171\u5f62\u9884\u6d4b\uff08SCP\uff09\u5728\u6a21\u578b\u9002\u5e94\u65f6\u4f1a\u7834\u574f\u6d4b\u8bd5\u6570\u636e\u7684\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8fc1\u79fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8f6c\u5bfc\u5f0f\u5206\u5272\u5171\u5f62\u9002\u5e94\uff08SCA-T\uff09\uff0c\u4e00\u79cd\u65b0\u7684\u8fc1\u79fb\u5b66\u4e60\u6d41\u7a0b\u3002\u8be5\u65b9\u6cd5\u5728\u5171\u5f62\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u8054\u5408\u6821\u51c6\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u65e0\u76d1\u7763\u8f6c\u5bfc\u9002\u5e94\uff0c\u907f\u514d\u4e86\u4f20\u7edfSCP\u4e2d\u6a21\u578b\u9002\u5e94\u5bf9\u6570\u636e\u4ea4\u6362\u6027\u7684\u7834\u574f\u3002\u5b9e\u9a8c\u8986\u76d6\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u6a21\u6001\u3001\u8fc1\u79fb\u4efb\u52a1\u548c\u975e\u5171\u5f62\u6027\u8bc4\u5206\u3002", "result": "SCA-T\u5728\u6548\u7387\u548c\u6761\u4ef6\u8986\u76d6\u8303\u56f4\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfSCP\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u7ecf\u9a8c\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\u4e14\u9ad8\u6548\u3002", "conclusion": "SCA-T\u4e3a\u533b\u5b66VLM\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSCP\u5728\u6a21\u578b\u9002\u5e94\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u5206\u5272\u5171\u5f62\u9884\u6d4b\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4fe1\u5c11\u6837\u672c\u8fc1\u79fb", "abstract_zh": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6570\u636e\u9ad8\u6548\u7684\u56fe\u50cf\u5206\u7c7b\u3002\u5c3d\u7ba1\u5176\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u5176\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5206\u5272\u5171\u5f62\u9884\u6d4b\uff08SCP\uff09\u6846\u67b6\uff0c\u4ee5\u5728\u57fa\u4e8e\u5c11\u91cf\u6807\u8bb0\u6821\u51c6\u96c6\u8fc1\u79fb\u6b64\u7c7b\u6a21\u578b\u65f6\u63d0\u4f9b\u53ef\u4fe1\u5ea6\u4fdd\u8bc1\u3002\u5c3d\u7ba1\u6f5c\u529b\u5de8\u5927\uff0c\u4f46VLM\u9884\u8bad\u7ec3\u7684\u901a\u7528\u6027\u53ef\u80fd\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u5171\u5f62\u9884\u6d4b\u96c6\u6027\u8d28\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u5e38\u89c1\u7684\u5224\u522b\u6027\u76ee\u7684\u9002\u5e94\u9636\u6bb5\u5728\u5b9e\u8df5\u4e2d\u7528\u4e8e\u5171\u5f62\u76ee\u7684\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4f7f\u7528\u6821\u51c6\u6570\u636e\u9002\u5e94\u6a21\u578b\u4f1a\u7834\u574fSCP\u4e2d\u6d4b\u8bd5\u6570\u636e\u7684\u4e25\u683c\u4ea4\u6362\u6027\u5047\u8bbe\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8f6c\u5bfc\u5f0f\u5206\u5272\u5171\u5f62\u9002\u5e94\uff08SCA-T\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u5171\u5f62\u573a\u666f\u8fc1\u79fb\u5b66\u4e60\u7684\u65b0\u6d41\u7a0b\uff0c\u5b83\u8054\u5408\u6821\u51c6\u548c\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u65e0\u76d1\u7763\u8f6c\u5bfc\u9002\u5e94\u3002\u6211\u4eec\u901a\u8fc7\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u6a21\u6001\u3001\u8fc1\u79fb\u4efb\u52a1\u548c\u975e\u5171\u5f62\u6027\u8bc4\u5206\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\u3002\u4e0eSCP\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u6548\u7387\u548c\u6761\u4ef6\u8986\u76d6\u8303\u56f4\u4e0a\u5747\u8868\u73b0\u51fa\u6301\u7eed\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u7ecf\u9a8c\u4fdd\u8bc1\u3002"}}
{"id": "2506.17846", "pdf": "https://arxiv.org/pdf/2506.17846", "abs": "https://arxiv.org/abs/2506.17846", "authors": ["Elija Perrier"], "title": "Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)", "categories": ["cs.AI"], "comment": "Under review for Neurips 2025", "summary": "This position paper argues that formal optimal control theory should be\ncentral to AI alignment research, offering a distinct perspective from\nprevailing AI safety and security approaches. While recent work in AI safety\nand mechanistic interpretability has advanced formal methods for alignment,\nthey often fall short of the generalisation required of control frameworks for\nother technologies. There is also a lack of research into how to render\ndifferent alignment/control protocols interoperable. We argue that by recasting\nalignment through principles of formal optimal control and framing alignment in\nterms of hierarchical stack from physical to socio-technical layers according\nto which controls may be applied we can develop a better understanding of the\npotential and limitations for controlling frontier models and agentic AI\nsystems. To this end, we introduce an Alignment Control Stack which sets out a\nhierarchical layered alignment stack, identifying measurement and control\ncharacteristics at each layer and how different layers are formally\ninteroperable. We argue that such analysis is also key to the assurances that\nwill be needed by governments and regulators in order to see AI technologies\nsustainably benefit the community. Our position is that doing so will bridge\nthe well-established and empirically validated methods of optimal control with\npractical deployment considerations to create a more comprehensive alignment\nframework, enhancing how we approach safety and reliability for advanced AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5c06\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u4f5c\u4e3aAI\u5bf9\u9f50\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u540c\u4e8e\u73b0\u6709AI\u5b89\u5168\u548c\u5b89\u5168\u65b9\u6cd5\u7684\u89c6\u89d2\u3002\u901a\u8fc7\u5f15\u5165\u5bf9\u9f50\u63a7\u5236\u6808\uff0c\u5206\u5c42\u5206\u6790\u5bf9\u9f50\u95ee\u9898\uff0c\u4ee5\u589e\u5f3a\u5bf9\u524d\u6cbf\u6a21\u578b\u548c\u4ee3\u7406AI\u7cfb\u7edf\u7684\u63a7\u5236\u6f5c\u529b\u4e0e\u5c40\u9650\u7684\u7406\u89e3\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u867d\u5728\u5f62\u5f0f\u4e0a\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u63a7\u5236\u6846\u67b6\u548c\u534f\u8bae\u4e92\u64cd\u4f5c\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u91cd\u65b0\u5b9a\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u5206\u5c42\u5bf9\u9f50\u63a7\u5236\u6808\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5bf9\u9f50\u63a7\u5236\u6808\uff0c\u5206\u5c42\u5206\u6790\u4ece\u7269\u7406\u5c42\u5230\u793e\u4f1a\u6280\u672f\u5c42\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u660e\u786e\u5404\u5c42\u7684\u6d4b\u91cf\u548c\u63a7\u5236\u7279\u6027\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u5c42\u4e4b\u95f4\u7684\u5f62\u5f0f\u4e92\u64cd\u4f5c\u6027\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u548c\u5bf9\u9f50\u63a7\u5236\u6808\uff0c\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u524d\u6cbf\u6a21\u578b\u548c\u4ee3\u7406AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u5c06\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5e94\u7528\u4e8eAI\u5bf9\u9f50\u7814\u7a76\uff0c\u80fd\u591f\u7ed3\u5408\u5b9e\u8df5\u7ecf\u9a8c\u4e0e\u90e8\u7f72\u9700\u6c42\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u4e3a\u653f\u5e9c\u548c\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u5fc5\u8981\u7684\u4fdd\u969c\u3002", "paper_title_zh": "\u5931\u63a7\u2014\u2014\u4e3a\u4ec0\u4e48\u5bf9\u9f50\u9700\u8981\u5f62\u5f0f\u63a7\u5236\u7406\u8bba\uff08\u53ca\u5bf9\u9f50\u63a7\u5236\u6808\uff09", "abstract_zh": "\u672c\u7acb\u573a\u8bba\u6587\u4e3b\u5f20\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5e94\u6210\u4e3aAI\u5bf9\u9f50\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u540c\u4e8e\u4e3b\u6d41AI\u5b89\u5168\u548c\u5b89\u5168\u65b9\u6cd5\u7684\u89c6\u89d2\u3002\u5c3d\u7ba1AI\u5b89\u5168\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5728\u5f62\u5f0f\u4e0a\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5176\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u9f50/\u63a7\u5236\u534f\u8bae\u7684\u4e92\u64cd\u4f5c\u6027\u7814\u7a76\u3002\u901a\u8fc7\u5c06\u5bf9\u9f50\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u539f\u5219\uff0c\u5e76\u6784\u5efa\u4ece\u7269\u7406\u5c42\u5230\u793e\u4f1a\u6280\u672f\u5c42\u7684\u5206\u5c42\u5bf9\u9f50\u63a7\u5236\u6808\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u524d\u6cbf\u6a21\u578b\u548c\u4ee3\u7406AI\u7cfb\u7edf\u7684\u63a7\u5236\u6f5c\u529b\u4e0e\u5c40\u9650\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5bf9\u9f50\u63a7\u5236\u6808\uff0c\u5206\u5c42\u5206\u6790\u5404\u5c42\u7684\u6d4b\u91cf\u548c\u63a7\u5236\u7279\u6027\uff0c\u4ee5\u53ca\u4e0d\u540c\u5c42\u4e4b\u95f4\u7684\u5f62\u5f0f\u4e92\u64cd\u4f5c\u6027\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u8fd9\u79cd\u5206\u6790\u4e5f\u662f\u653f\u5e9c\u548c\u76d1\u7ba1\u673a\u6784\u6240\u9700\u4fdd\u969c\u7684\u5173\u952e\u3002\u6211\u4eec\u7684\u7acb\u573a\u662f\uff0c\u901a\u8fc7\u7ed3\u5408\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u4e0e\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u5168\u9762\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u63d0\u5347\u9ad8\u7ea7AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.17609", "pdf": "https://arxiv.org/pdf/2506.17609", "abs": "https://arxiv.org/abs/2506.17609", "authors": ["Lincan Li", "Eren Erman Ozguven", "Yue Zhao", "Guang Wang", "Yiqun Xie", "Yushun Dong"], "title": "TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate typhoon track forecasting is crucial for early system warning and\ndisaster response. While Transformer-based models have demonstrated strong\nperformance in modeling the temporal dynamics of dense trajectories of humans\nand vehicles in smart cities, they usually lack access to broader contextual\nknowledge that enhances the forecasting reliability of sparse meteorological\ntrajectories, such as typhoon tracks. To address this challenge, we propose\nTyphoFormer, a novel framework that incorporates natural language descriptions\nas auxiliary prompts to improve typhoon trajectory forecasting. For each time\nstep, we use Large Language Model (LLM) to generate concise textual\ndescriptions based on the numerical attributes recorded in the North Atlantic\nhurricane database. The language descriptions capture high-level meteorological\nsemantics and are embedded as auxiliary special tokens prepended to the\nnumerical time series input. By integrating both textual and sequential\ninformation within a unified Transformer encoder, TyphoFormer enables the model\nto leverage contextual cues that are otherwise inaccessible through numerical\nfeatures alone. Extensive experiments are conducted on HURDAT2 benchmark,\nresults show that TyphoFormer consistently outperforms other state-of-the-art\nbaseline methods, particularly under challenging scenarios involving nonlinear\npath shifts and limited historical observations.", "AI": {"tldr": "TyphoFormer\u662f\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u8a00\u589e\u5f3a\u63d0\u5347\u53f0\u98ce\u8def\u5f84\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u53f0\u98ce\u8def\u5f84\u9884\u6d4b\u5bf9\u65e9\u671f\u9884\u8b66\u548c\u707e\u5bb3\u54cd\u5e94\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfTransformer\u6a21\u578b\u5728\u7a00\u758f\u6c14\u8c61\u8f68\u8ff9\uff08\u5982\u53f0\u98ce\u8def\u5f84\uff09\u9884\u6d4b\u4e2d\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0cTyphoFormer\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "TyphoFormer\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u53f0\u98ce\u6570\u503c\u5c5e\u6027\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7279\u6b8a\u6807\u8bb0\u5d4c\u5165Transformer\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u6587\u672c\u548c\u5e8f\u5217\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728HURDAT2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTyphoFormer\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u975e\u7ebf\u6027\u8def\u5f84\u53d8\u5316\u548c\u5386\u53f2\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "TyphoFormer\u901a\u8fc7\u8bed\u8a00\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u53f0\u98ce\u8def\u5f84\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u6c14\u8c61\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "TyphoFormer\uff1a\u8bed\u8a00\u589e\u5f3a\u7684Transformer\u6a21\u578b\u7528\u4e8e\u7cbe\u51c6\u53f0\u98ce\u8def\u5f84\u9884\u6d4b", "abstract_zh": "\u51c6\u786e\u7684\u53f0\u98ce\u8def\u5f84\u9884\u6d4b\u5bf9\u65e9\u671f\u7cfb\u7edf\u9884\u8b66\u548c\u707e\u5bb3\u54cd\u5e94\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u667a\u80fd\u57ce\u5e02\u4e2d\u5bc6\u96c6\u8f68\u8ff9\uff08\u5982\u4eba\u7c7b\u548c\u8f66\u8f86\uff09\u7684\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u5bf9\u7a00\u758f\u6c14\u8c61\u8f68\u8ff9\uff08\u5982\u53f0\u98ce\u8def\u5f84\uff09\u9884\u6d4b\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faTyphoFormer\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u8f85\u52a9\u63d0\u793a\u6765\u6539\u8fdb\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u3002\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u57fa\u4e8e\u5317\u5927\u897f\u6d0b\u98d3\u98ce\u6570\u636e\u5e93\u4e2d\u7684\u6570\u503c\u5c5e\u6027\u751f\u6210\u7b80\u6d01\u7684\u6587\u672c\u63cf\u8ff0\u3002\u8fd9\u4e9b\u8bed\u8a00\u63cf\u8ff0\u6355\u6349\u4e86\u9ad8\u7ea7\u6c14\u8c61\u8bed\u4e49\uff0c\u5e76\u4f5c\u4e3a\u7279\u6b8a\u6807\u8bb0\u5d4c\u5165\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u4e2d\u3002\u901a\u8fc7\u5c06\u6587\u672c\u548c\u5e8f\u5217\u4fe1\u606f\u6574\u5408\u5230\u7edf\u4e00\u7684Transformer\u7f16\u7801\u5668\u4e2d\uff0cTyphoFormer\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u4ec5\u901a\u8fc7\u6570\u503c\u7279\u5f81\u65e0\u6cd5\u83b7\u53d6\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002\u5728HURDAT2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eTyphoFormer\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u975e\u7ebf\u6027\u8def\u5f84\u53d8\u5316\u548c\u5386\u53f2\u89c2\u6d4b\u6709\u9650\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002"}}
{"id": "2506.17505", "pdf": "https://arxiv.org/pdf/2506.17505", "abs": "https://arxiv.org/abs/2506.17505", "authors": ["Jessy Lauer"], "title": "Learning golf swing signatures from a single wrist-worn inertial sensor", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Despite its importance for performance and injury prevention, golf swing\nanalysis is limited by isolated metrics, underrepresentation of professional\nathletes, and a lack of rich, interpretable movement representations. We\naddress these gaps with a holistic, data-driven framework for personalized golf\nswing analysis from a single wrist-worn sensor. We build a large dataset of\nprofessional swings from publicly available videos, reconstruct full-body 3D\nkinematics using biologically accurate human mesh recovery, and generate\nsynthetic inertial data to train neural networks that infer motion and segment\nswing phases from wrist-based input. We learn a compositional, discrete\nvocabulary of motion primitives that facilitates the detection and\nvisualization of technical flaws, and is expressive enough to predict player\nidentity, club type, sex, and age. Our system accurately estimates full-body\nkinematics and swing events from wrist data, delivering lab-grade motion\nanalysis on-course and supporting early detection of anomalous movement\npatterns. Explainability methods reveal subtle, individualized movement\nsignatures, reinforcing the view that variability is a hallmark of skilled\nperformance. Longitudinal tracking demonstrates practical value: as one\nplayer's handicap improved from 50 to 2.2 over 1.5 years, our system captured\nmeasurable technical progress and provided targeted, actionable feedback. Our\nfindings challenge common assumptions, such as swing consistency across clubs\nand the existence of a single \"ideal\" swing, and uncover latent biomarkers\nshaped by both intrinsic traits and task-specific constraints. This work\nbridges lab and field-based biomechanics, offering scalable, accessible,\nhigh-fidelity motion analysis for research, coaching, and injury prevention,\nwhile opening new directions in movement-based phenotyping, personalized\nequipment design, and motor skill development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u624b\u8155\u60ef\u6027\u4f20\u611f\u5668\u7684\u4e2a\u6027\u5316\u9ad8\u5c14\u592b\u6325\u6746\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u89e3\u51b3\u4e86\u4f20\u7edf\u6325\u6746\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u5b64\u7acb\u6307\u6807\u3001\u4e13\u4e1a\u8fd0\u52a8\u5458\u6570\u636e\u4e0d\u8db3\u7b49\u3002", "motivation": "\u9ad8\u5c14\u592b\u6325\u6746\u5206\u6790\u5bf9\u8868\u73b0\u548c\u9884\u9632\u635f\u4f24\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5b64\u7acb\u6307\u6807\u3001\u4e13\u4e1a\u8fd0\u52a8\u5458\u6570\u636e\u4e0d\u8db3\u7b49\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u53ef\u89e3\u91ca\u7684\u6325\u6746\u5206\u6790\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u4e13\u4e1a\u6325\u6746\u6570\u636e\u96c6\uff0c\u5229\u7528\u751f\u7269\u7cbe\u786e\u7684\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u6280\u672f\u751f\u6210\u5408\u6210\u60ef\u6027\u6570\u636e\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4ece\u624b\u8155\u6570\u636e\u63a8\u65ad\u52a8\u4f5c\u5e76\u5206\u5272\u6325\u6746\u9636\u6bb5\u3002\u540c\u65f6\uff0c\u5b66\u4e60\u4e86\u4e00\u79cd\u79bb\u6563\u7684\u8fd0\u52a8\u57fa\u5143\u8bcd\u6c47\u8868\uff0c\u7528\u4e8e\u68c0\u6d4b\u6280\u672f\u7f3a\u9677\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ece\u624b\u8155\u6570\u636e\u51c6\u786e\u4f30\u8ba1\u5168\u8eab\u8fd0\u52a8\u5b66\u548c\u6325\u6746\u4e8b\u4ef6\uff0c\u63d0\u4f9b\u5b9e\u9a8c\u5ba4\u7ea7\u522b\u7684\u8fd0\u52a8\u5206\u6790\uff0c\u5e76\u652f\u6301\u65e9\u671f\u5f02\u5e38\u52a8\u4f5c\u68c0\u6d4b\u3002\u6b64\u5916\uff0c\u8fd8\u63ed\u793a\u4e86\u7ec6\u5fae\u7684\u4e2a\u6027\u5316\u8fd0\u52a8\u7279\u5f81\uff0c\u6311\u6218\u4e86\u6325\u6746\u4e00\u81f4\u6027\u548c\u5355\u4e00\u201c\u7406\u60f3\u201d\u6325\u6746\u7684\u5047\u8bbe\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86\u5b9e\u9a8c\u5ba4\u4e0e\u73b0\u573a\u751f\u7269\u529b\u5b66\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u3001\u6559\u7ec3\u548c\u635f\u4f24\u9884\u9632\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u4fdd\u771f\u7684\u8fd0\u52a8\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8fd0\u52a8\u8868\u578b\u3001\u4e2a\u6027\u5316\u88c5\u5907\u8bbe\u8ba1\u548c\u8fd0\u52a8\u6280\u80fd\u53d1\u5c55\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u4ece\u5355\u624b\u8155\u60ef\u6027\u4f20\u611f\u5668\u5b66\u4e60\u9ad8\u5c14\u592b\u6325\u6746\u7279\u5f81", "abstract_zh": "\u5c3d\u7ba1\u9ad8\u5c14\u592b\u6325\u6746\u5206\u6790\u5bf9\u8868\u73b0\u548c\u9884\u9632\u635f\u4f24\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u53d7\u9650\u4e8e\u5b64\u7acb\u6307\u6807\u3001\u4e13\u4e1a\u8fd0\u52a8\u5458\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u7f3a\u4e4f\u4e30\u5bcc\u53ef\u89e3\u91ca\u7684\u8fd0\u52a8\u8868\u5f81\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u5355\u624b\u8155\u4f20\u611f\u5668\u7684\u4e2a\u6027\u5316\u6325\u6746\u5206\u6790\u6846\u67b6\u586b\u8865\u4e86\u8fd9\u4e9b\u7a7a\u767d\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u4e13\u4e1a\u6325\u6746\u6570\u636e\u96c6\uff0c\u5229\u7528\u751f\u7269\u7cbe\u786e\u7684\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u6280\u672f\u751f\u6210\u5408\u6210\u60ef\u6027\u6570\u636e\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4ece\u624b\u8155\u6570\u636e\u63a8\u65ad\u52a8\u4f5c\u5e76\u5206\u5272\u6325\u6746\u9636\u6bb5\u3002\u6211\u4eec\u5b66\u4e60\u4e86\u4e00\u79cd\u79bb\u6563\u7684\u8fd0\u52a8\u57fa\u5143\u8bcd\u6c47\u8868\uff0c\u7528\u4e8e\u68c0\u6d4b\u6280\u672f\u7f3a\u9677\uff0c\u5e76\u80fd\u9884\u6d4b\u7403\u5458\u8eab\u4efd\u3001\u7403\u6746\u7c7b\u578b\u3001\u6027\u522b\u548c\u5e74\u9f84\u3002\u7cfb\u7edf\u4ece\u624b\u8155\u6570\u636e\u51c6\u786e\u4f30\u8ba1\u5168\u8eab\u8fd0\u52a8\u5b66\u548c\u6325\u6746\u4e8b\u4ef6\uff0c\u63d0\u4f9b\u5b9e\u9a8c\u5ba4\u7ea7\u522b\u7684\u8fd0\u52a8\u5206\u6790\uff0c\u652f\u6301\u65e9\u671f\u5f02\u5e38\u52a8\u4f5c\u68c0\u6d4b\u3002\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63ed\u793a\u4e86\u7ec6\u5fae\u7684\u4e2a\u6027\u5316\u8fd0\u52a8\u7279\u5f81\uff0c\u8868\u660e\u53d8\u5f02\u6027\u662f\u9ad8\u6c34\u5e73\u8868\u73b0\u7684\u6807\u5fd7\u3002\u7eb5\u5411\u8ddf\u8e2a\u663e\u793a\u5b9e\u9645\u4ef7\u503c\uff1a\u4e00\u540d\u7403\u5458\u7684\u5dee\u70b9\u57281.5\u5e74\u5185\u4ece50\u964d\u81f32.2\uff0c\u7cfb\u7edf\u6355\u6349\u5230\u4e86\u6280\u672f\u8fdb\u6b65\u7684\u91cf\u5316\u6307\u6807\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u6027\u53cd\u9988\u3002\u6211\u4eec\u7684\u53d1\u73b0\u6311\u6218\u4e86\u5e38\u89c1\u5047\u8bbe\uff0c\u5982\u6325\u6746\u5728\u4e0d\u540c\u7403\u6746\u95f4\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u5355\u4e00\u201c\u7406\u60f3\u201d\u6325\u6746\u7684\u5b58\u5728\uff0c\u5e76\u63ed\u793a\u4e86\u7531\u5185\u5728\u7279\u5f81\u548c\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\u5f62\u6210\u7684\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u3002\u672c\u7814\u7a76\u8fde\u63a5\u4e86\u5b9e\u9a8c\u5ba4\u4e0e\u73b0\u573a\u751f\u7269\u529b\u5b66\uff0c\u4e3a\u7814\u7a76\u3001\u6559\u7ec3\u548c\u635f\u4f24\u9884\u9632\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u4fdd\u771f\u7684\u8fd0\u52a8\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8fd0\u52a8\u8868\u578b\u3001\u4e2a\u6027\u5316\u88c5\u5907\u8bbe\u8ba1\u548c\u8fd0\u52a8\u6280\u80fd\u53d1\u5c55\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.17878", "pdf": "https://arxiv.org/pdf/2506.17878", "abs": "https://arxiv.org/abs/2506.17878", "authors": ["Tam Trinh", "Manh Nguyen", "Truong-Son Hy"], "title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval", "categories": ["cs.AI"], "comment": null, "summary": "The rapid spread of misinformation in the digital era poses significant\nchallenges to public discourse, necessitating robust and scalable fact-checking\nsolutions. Traditional human-led fact-checking methods, while credible,\nstruggle with the volume and velocity of online content, prompting the\nintegration of automated systems powered by Large Language Models (LLMs).\nHowever, existing automated approaches often face limitations, such as handling\ncomplex claims, ensuring source credibility, and maintaining transparency. This\npaper proposes a novel multi-agent system for automated fact-checking that\nenhances accuracy, efficiency, and explainability. The system comprises four\nspecialized agents: an Input Ingestion Agent for claim decomposition, a Query\nGeneration Agent for formulating targeted subqueries, an Evidence Retrieval\nAgent for sourcing credible evidence, and a Verdict Prediction Agent for\nsynthesizing veracity judgments with human-interpretable explanations.\nEvaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system\nachieves a 12.3% improvement in Macro F1-score over baseline methods. The\nsystem effectively decomposes complex claims, retrieves reliable evidence from\ntrusted sources, and generates transparent explanations for verification\ndecisions. Our approach contributes to the growing field of automated\nfact-checking by providing a more accurate, efficient, and transparent\nverification methodology that aligns with human fact-checking practices while\nmaintaining scalability for real-world applications. Our source code is\navailable at https://github.com/HySonLab/FactAgent", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u58f0\u660e\u3001\u68c0\u7d22\u53ef\u4fe1\u8bc1\u636e\u5e76\u751f\u6210\u900f\u660e\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u6570\u5b57\u65f6\u4ee3\u4e2d\u9519\u8bef\u4fe1\u606f\u7684\u5feb\u901f\u4f20\u64ad\u5bf9\u516c\u5171\u8ba8\u8bba\u6784\u6210\u6311\u6218\uff0c\u4f20\u7edf\u4eba\u5de5\u6838\u67e5\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u6d77\u91cf\u5185\u5bb9\uff0c\u800c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u590d\u6742\u58f0\u660e\u5904\u7406\u3001\u6765\u6e90\u53ef\u4fe1\u5ea6\u548c\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u56db\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff1a\u8f93\u5165\u89e3\u6790\u667a\u80fd\u4f53\u5206\u89e3\u58f0\u660e\uff0c\u67e5\u8be2\u751f\u6210\u667a\u80fd\u4f53\u5236\u5b9a\u5b50\u67e5\u8be2\uff0c\u8bc1\u636e\u68c0\u7d22\u667a\u80fd\u4f53\u83b7\u53d6\u53ef\u4fe1\u8bc1\u636e\uff0c\u88c1\u51b3\u9884\u6d4b\u667a\u80fd\u4f53\u751f\u6210\u53ef\u89e3\u91ca\u7684\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\uff08FEVEROUS\u3001HOVER\u3001SciFact\uff09\u4e0a\uff0c\u7cfb\u7edfMacro F1\u5206\u6570\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534712.3%\uff0c\u80fd\u6709\u6548\u5206\u89e3\u590d\u6742\u58f0\u660e\u5e76\u751f\u6210\u900f\u660e\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u900f\u660e\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u517c\u5177\u53ef\u6269\u5c55\u6027\uff0c\u7b26\u5408\u4eba\u5de5\u6838\u67e5\u5b9e\u8df5\u3002", "paper_title_zh": "\u8fc8\u5411\u7a33\u5065\u7684\u4e8b\u5b9e\u6838\u67e5\uff1a\u4e00\u79cd\u5177\u5907\u9ad8\u7ea7\u8bc1\u636e\u68c0\u7d22\u529f\u80fd\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf", "abstract_zh": "\u6570\u5b57\u65f6\u4ee3\u9519\u8bef\u4fe1\u606f\u7684\u5feb\u901f\u4f20\u64ad\u5bf9\u516c\u5171\u8ba8\u8bba\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u4e9f\u9700\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u89e3\u51b3\u65b9\u6848\u3002\u4f20\u7edf\u4eba\u5de5\u6838\u67e5\u65b9\u6cd5\u867d\u53ef\u4fe1\uff0c\u4f46\u96be\u4ee5\u5e94\u5bf9\u5728\u7ebf\u5185\u5bb9\u7684\u4f53\u91cf\u548c\u901f\u5ea6\uff0c\u4fc3\u4f7f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u88ab\u5f15\u5165\u3002\u7136\u800c\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5e38\u9762\u4e34\u5c40\u9650\uff0c\u5982\u5904\u7406\u590d\u6742\u58f0\u660e\u3001\u786e\u4fdd\u6765\u6e90\u53ef\u4fe1\u5ea6\u53ca\u4fdd\u6301\u900f\u660e\u5ea6\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff0c\u63d0\u5347\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u7cfb\u7edf\u5305\u542b\u56db\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff1a\u8f93\u5165\u89e3\u6790\u667a\u80fd\u4f53\u7528\u4e8e\u58f0\u660e\u5206\u89e3\uff0c\u67e5\u8be2\u751f\u6210\u667a\u80fd\u4f53\u7528\u4e8e\u5236\u5b9a\u76ee\u6807\u5b50\u67e5\u8be2\uff0c\u8bc1\u636e\u68c0\u7d22\u667a\u80fd\u4f53\u7528\u4e8e\u83b7\u53d6\u53ef\u4fe1\u8bc1\u636e\uff0c\u88c1\u51b3\u9884\u6d4b\u667a\u80fd\u4f53\u7528\u4e8e\u7efc\u5408\u771f\u5b9e\u6027\u5224\u65ad\u5e76\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\uff08FEVEROUS\u3001HOVER\u3001SciFact\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edfMacro F1\u5206\u6570\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534712.3%\uff0c\u80fd\u6709\u6548\u5206\u89e3\u590d\u6742\u58f0\u660e\u3001\u4ece\u53ef\u4fe1\u6765\u6e90\u68c0\u7d22\u53ef\u9760\u8bc1\u636e\uff0c\u5e76\u4e3a\u9a8c\u8bc1\u51b3\u7b56\u751f\u6210\u900f\u660e\u89e3\u91ca\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u900f\u660e\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7b26\u5408\u4eba\u5de5\u6838\u67e5\u5b9e\u8df5\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u6269\u5c55\u6027\u3002\u6e90\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/HySonLab/FactAgent\u3002"}}
{"id": "2506.17611", "pdf": "https://arxiv.org/pdf/2506.17611", "abs": "https://arxiv.org/abs/2506.17611", "authors": ["Jinchuan Tian", "William Chen", "Yifan Peng", "Jiatong Shi", "Siddhant Arora", "Shikhar Bharadwaj", "Takashi Maekaku", "Yusuke Shinohara", "Keita Goto", "Xiang Yue", "Huck Yang", "Shinji Watanabe"], "title": "OpusLM: A Family of Open Unified Speech Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OpusLM\u7cfb\u5217\u5f00\u653e\u7edf\u4e00\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u5316\uff0c\u5e76\u5728\u5927\u91cf\u8bed\u97f3-\u6587\u672c\u5bf9\u548c\u7eaf\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u5408\u6210\u548c\u7eaf\u6587\u672c\u4efb\u52a1\u4e0a\u7684\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SpeechLMs\uff09\u7684\u7814\u7a76\u7f3a\u4e4f\u5f00\u653e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4e14\u6027\u80fd\u6709\u5f85\u63d0\u5347\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8e\u516c\u5f00\u6750\u6599\u7684\u5f00\u653e\u7edf\u4e00\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u63a8\u52a8\u5f00\u653e\u7814\u7a76\u3002", "method": "OpusLM\u7cfb\u5217\u6a21\u578b\u4ece\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408213K\u5c0f\u65f6\u7684\u8bed\u97f3-\u6587\u672c\u5bf9\u548c292B\u7684\u7eaf\u6587\u672c\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u591a\u6d41\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOpusLM\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u5408\u6210\u548c\u7eaf\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u73b0\u6709SpeechLMs\u3002\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u548c\u6570\u636e\u9009\u62e9\u7b56\u7565\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "OpusLM\u7cfb\u5217\u6a21\u578b\u57fa\u4e8e\u516c\u5f00\u6750\u6599\u6784\u5efa\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u5b8c\u5168\u900f\u660e\uff0c\u4e3a\u5f00\u653e\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "paper_title_zh": "OpusLM\uff1a\u4e00\u4e2a\u5f00\u653e\u7edf\u4e00\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5f00\u653e\u7edf\u4e00\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08OpusLMs\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u89c4\u6a21\u9ad8\u8fbe7B\u7684\u5f00\u653e\u57fa\u7840\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u3002OpusLMs\u4ece\u4ec5\u89e3\u7801\u5668\u7684\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u5316\uff0c\u5e76\u5728213K\u5c0f\u65f6\u7684\u8bed\u97f3-\u6587\u672c\u5bf9\u548c292B\u7684\u7eaf\u6587\u672c\u6807\u8bb0\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\u3002\u6211\u4eec\u5c55\u793a\u4e86OpusLMs\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u5408\u6210\u548c\u7eaf\u6587\u672c\u80fd\u529b\u4e0a\u4e0e\u73b0\u6709SpeechLMs\u76f8\u5f53\uff08\u751a\u81f3\u66f4\u4f18\uff09\u7684\u6027\u80fd\u3002\u6280\u672f\u4e0a\uff0c\u672c\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86\u6211\u4eec\u5728\u5206\u8bcd\u3001\u591a\u6d41\u8bed\u8a00\u6a21\u578b\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4e0a\u7684\u8bbe\u8ba1\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u548c\u6570\u636e\u9009\u62e9\u9000\u706b\u7684\u91cd\u8981\u6027\u3002OpusLMs\u5b8c\u5168\u57fa\u4e8e\u516c\u5f00\u6750\u6599\u6784\u5efa\uff0c\u662f\u5b8c\u5168\u900f\u660e\u7684\u6a21\u578b\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6570\u636e\u3001\u68c0\u67e5\u70b9\u548c\u8bad\u7ec3\u65e5\u5fd7\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u653eSpeechLM\u7814\u7a76\u3002"}}
{"id": "2506.17545", "pdf": "https://arxiv.org/pdf/2506.17545", "abs": "https://arxiv.org/abs/2506.17545", "authors": ["Zhihao Yuan", "Shuyi Jiang", "Chun-Mei Feng", "Yaolun Zhang", "Shuguang Cui", "Zhen Li", "Na Zhao"], "title": "Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Currently, utilizing large language models to understand the 3D world is\nbecoming popular. Yet existing 3D-aware LLMs act as black boxes: they output\nbounding boxes or textual answers without revealing how those decisions are\nmade, and they still rely on pre-trained 3D detectors to supply object\nproposals. We introduce Scene-R1, a video-grounded framework that learns to\nreason about 3D scenes without any point-wise 3D instance supervision by\npairing reinforcement-learning-driven reasoning with a two-stage grounding\npipeline. In the temporal grounding stage, we explicitly reason about the video\nand select the video snippets most relevant to an open-ended query. In the\nsubsequent image grounding stage, we analyze the image and predict the 2D\nbounding box. After that, we track the object using SAM2 to produce\npixel-accurate masks in RGB frames, and project them back into 3D, thereby\neliminating the need for 3D detector-based proposals while capturing fine\ngeometry and material cues. Scene-R1 can also adapt to the 3D visual question\nanswering task to answer free-form questions directly from video. Our training\npipeline only needs task-level 2D boxes or textual labels without dense 3D\npoint-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on\nmultiple datasets, while delivering transparent, step-by-step rationales. These\nresults show that reinforcement-learning-based reasoning combined with RGB-D\nvideo alone offers a practical, annotation-efficient route to trustworthy 3D\nscene understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faScene-R1\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u63a8\u7406\u548c\u4e24\u9636\u6bb5\u5b9a\u4f4d\u6d41\u7a0b\uff0c\u65e0\u97003D\u6807\u6ce8\u5373\u53ef\u5b9e\u73b03D\u573a\u666f\u7406\u89e3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u76843D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u76843D\u68c0\u6d4b\u5668\uff0c\u4e14\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u3002Scene-R1\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u6d88\u9664\u5bf93D\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "Scene-R1\u91c7\u7528\u4e24\u9636\u6bb5\u5b9a\u4f4d\u6d41\u7a0b\uff1a1\uff09\u65f6\u95f4\u5b9a\u4f4d\u9636\u6bb5\u9009\u62e9\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u89c6\u9891\u7247\u6bb5\uff1b2\uff09\u56fe\u50cf\u5b9a\u4f4d\u9636\u6bb5\u9884\u6d4b2D\u8fb9\u754c\u6846\uff0c\u5e76\u901a\u8fc7SAM2\u751f\u6210\u50cf\u7d20\u7ea7\u63a9\u7801\uff0c\u6700\u540e\u6295\u5f71\u56de3D\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u65e0\u97003D\u6807\u6ce8\uff0c\u4ec5\u9700\u4efb\u52a1\u7ea72D\u6846\u6216\u6587\u672c\u6807\u7b7e\u3002", "result": "Scene-R1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u9010\u6b65\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8eRGB-D\u89c6\u9891\u7684\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Scene-R1\u5c55\u793a\u4e86\u65e0\u97003D\u6807\u6ce8\u76843D\u573a\u666f\u7406\u89e3\u53ef\u884c\u6027\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u9891\u6570\u636e\uff0c\u4e3a\u9ad8\u6548\u4e14\u53ef\u4fe1\u76843D\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "paper_title_zh": "Scene-R1\uff1a\u65e0\u97003D\u6807\u6ce8\u7684\u89c6\u9891\u9a71\u52a8\u5927\u8bed\u8a00\u6a21\u578b3D\u573a\u666f\u63a8\u7406", "abstract_zh": "\u76ee\u524d\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e33D\u4e16\u754c\u6b63\u53d8\u5f97\u6d41\u884c\u3002\u7136\u800c\u73b0\u6709\u76843D\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\u5982\u540c\u9ed1\u7bb1\uff1a\u5b83\u4eec\u8f93\u51fa\u8fb9\u754c\u6846\u6216\u6587\u672c\u7b54\u6848\uff0c\u5374\u4e0d\u63ed\u793a\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e14\u4ecd\u4f9d\u8d56\u9884\u8bad\u7ec3\u76843D\u68c0\u6d4b\u5668\u63d0\u4f9b\u5bf9\u8c61\u63d0\u6848\u3002\u6211\u4eec\u63d0\u51faScene-R1\uff0c\u4e00\u79cd\u89c6\u9891\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u63a8\u7406\u4e0e\u4e24\u9636\u6bb5\u5b9a\u4f4d\u6d41\u7a0b\u7ed3\u5408\uff0c\u65e0\u9700\u4efb\u4f55\u70b9\u7ea73D\u5b9e\u4f8b\u6807\u6ce8\u5373\u53ef\u5b66\u4e603D\u573a\u666f\u63a8\u7406\u3002\u5728\u65f6\u95f4\u5b9a\u4f4d\u9636\u6bb5\uff0c\u6211\u4eec\u663e\u5f0f\u63a8\u7406\u89c6\u9891\u5e76\u9009\u62e9\u4e0e\u5f00\u653e\u5f0f\u67e5\u8be2\u6700\u76f8\u5173\u7684\u7247\u6bb5\uff1b\u5728\u968f\u540e\u7684\u56fe\u50cf\u5b9a\u4f4d\u9636\u6bb5\uff0c\u6211\u4eec\u5206\u6790\u56fe\u50cf\u5e76\u9884\u6d4b2D\u8fb9\u754c\u6846\uff0c\u7136\u540e\u4f7f\u7528SAM2\u8ddf\u8e2a\u5bf9\u8c61\u4ee5\u751f\u6210RGB\u5e27\u4e2d\u7684\u50cf\u7d20\u7ea7\u63a9\u7801\uff0c\u5e76\u5c06\u5176\u6295\u5f71\u56de3D\u7a7a\u95f4\uff0c\u4ece\u800c\u65e0\u9700\u57fa\u4e8e3D\u68c0\u6d4b\u5668\u7684\u63d0\u6848\uff0c\u540c\u65f6\u6355\u6349\u7cbe\u7ec6\u7684\u51e0\u4f55\u548c\u6750\u8d28\u7ebf\u7d22\u3002Scene-R1\u8fd8\u53ef\u9002\u5e943D\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u76f4\u63a5\u4ece\u89c6\u9891\u4e2d\u56de\u7b54\u81ea\u7531\u5f62\u5f0f\u95ee\u9898\u3002\u6211\u4eec\u7684\u8bad\u7ec3\u6d41\u7a0b\u4ec5\u9700\u4efb\u52a1\u7ea72D\u6846\u6216\u6587\u672c\u6807\u7b7e\uff0c\u65e0\u9700\u5bc6\u96c63D\u70b9\u7ea7\u6807\u6ce8\u3002Scene-R1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u57fa\u7ebf\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u9010\u6b65\u63a8\u7406\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u7ed3\u5408RGB-D\u89c6\u9891\uff0c\u4e3a\u53ef\u4fe1\u76843D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u4e14\u6807\u6ce8\u8282\u7ea6\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.17900", "pdf": "https://arxiv.org/pdf/2506.17900", "abs": "https://arxiv.org/abs/2506.17900", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms", "categories": ["cs.AI", "cs.DC"], "comment": "Accepted by 2025 8th International Conference on Advanced Electronic\n  Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "With the increasing complexity and rapid expansion of the scale of AI systems\nin cloud platforms, the log data generated during system operation is massive,\nunstructured, and semantically ambiguous, which brings great challenges to\nfault location and system self-repair. In order to solve this problem, this\npaper proposes an intelligent log processing and automatic debugging framework\nbased on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This\nmethod is extended on the basis of the existing pre-trained Transformer model,\nand integrates a multi-stage semantic inference mechanism to realize the\ncontext understanding of system logs and the automatic reconstruction of fault\nchains. Firstly, the system log is dynamically structured, and the unsupervised\nclustering and embedding mechanism is used to extract the event template and\nsemantic schema. Subsequently, the fine-tuned LLM combined with the multi-round\nattention mechanism to perform contextual reasoning on the log sequence to\ngenerate potential fault assumptions and root cause paths. Furthermore, this\npaper introduces a reinforcement learning-based policy-guided recovery planner,\nwhich is driven by the remediation strategy generated by LLM to support dynamic\ndecision-making and adaptive debugging in the cloud environment. Compared with\nthe existing rule engine or traditional log analysis system, the proposed model\nhas stronger semantic understanding ability, continuous learning ability and\nheterogeneous environment adaptability. Experiments on the cloud platform log\ndataset show that LLM-ID improves the fault location accuracy by 16.2%, which\nis significantly better than the current mainstream methods", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u65e5\u5fd7\u5904\u7406\u548c\u81ea\u52a8\u8c03\u8bd5\u6846\u67b6LLM-ID\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e91AI\u5e73\u53f0\u4e2d\u7684\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u4e91\u5e73\u53f0\u4e2dAI\u7cfb\u7edf\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u8fc5\u901f\u6269\u5927\uff0c\u65e5\u5fd7\u6570\u636e\u91cf\u5927\u3001\u975e\u7ed3\u6784\u5316\u4e14\u8bed\u4e49\u6a21\u7cca\uff0c\u7ed9\u6545\u969c\u5b9a\u4f4d\u548c\u7cfb\u7edf\u81ea\u4fee\u590d\u5e26\u6765\u5de8\u5927\u6311\u6218\u3002", "method": "\u6269\u5c55\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u673a\u5236\uff0c\u52a8\u6001\u7ed3\u6784\u5316\u7cfb\u7edf\u65e5\u5fd7\uff0c\u63d0\u53d6\u4e8b\u4ef6\u6a21\u677f\u548c\u8bed\u4e49\u6a21\u5f0f\uff1b\u5229\u7528\u5fae\u8c03\u7684LLM\u548c\u591a\u8f6e\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u751f\u6210\u6f5c\u5728\u6545\u969c\u5047\u8bbe\u548c\u6839\u56e0\u8def\u5f84\uff1b\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u5f15\u5bfc\u6062\u590d\u89c4\u5212\u5668\uff0c\u652f\u6301\u52a8\u6001\u51b3\u7b56\u548c\u81ea\u9002\u5e94\u8c03\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM-ID\u5728\u4e91\u5e73\u53f0\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u7684\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u5347\u4e8616.2%\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "LLM-ID\u6846\u67b6\u5177\u6709\u66f4\u5f3a\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3001\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u548c\u5f02\u6784\u73af\u5883\u9002\u5e94\u6027\uff0c\u4e3a\u4e91AI\u5e73\u53f0\u7684\u667a\u80fd\u65e5\u5fd7\u5904\u7406\u548c\u81ea\u52a8\u8c03\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e91AI\u5e73\u53f0\u667a\u80fd\u65e5\u5fd7\u5904\u7406\u4e0e\u81ea\u4e3b\u8c03\u8bd5", "abstract_zh": "\u968f\u7740\u4e91\u5e73\u53f0\u4e2dAI\u7cfb\u7edf\u590d\u6742\u6027\u548c\u89c4\u6a21\u7684\u8fc5\u901f\u6269\u5927\uff0c\u7cfb\u7edf\u8fd0\u884c\u671f\u95f4\u751f\u6210\u7684\u65e5\u5fd7\u6570\u636e\u91cf\u5927\u3001\u975e\u7ed3\u6784\u5316\u4e14\u8bed\u4e49\u6a21\u7cca\uff0c\u4e3a\u6545\u969c\u5b9a\u4f4d\u548c\u7cfb\u7edf\u81ea\u4fee\u590d\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u65e5\u5fd7\u5904\u7406\u548c\u81ea\u52a8\u8c03\u8bd5\u6846\u67b6\uff0c\u547d\u540d\u4e3a\u667a\u80fd\u8c03\u8bd5\u5668\uff08LLM-ID\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u73b0\u6709\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u7684\u57fa\u7840\u4e0a\u6269\u5c55\uff0c\u5e76\u6574\u5408\u4e86\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7cfb\u7edf\u65e5\u5fd7\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u6545\u969c\u94fe\u7684\u81ea\u52a8\u91cd\u5efa\u3002\u9996\u5148\uff0c\u52a8\u6001\u7ed3\u6784\u5316\u7cfb\u7edf\u65e5\u5fd7\uff0c\u5229\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u548c\u5d4c\u5165\u673a\u5236\u63d0\u53d6\u4e8b\u4ef6\u6a21\u677f\u548c\u8bed\u4e49\u6a21\u5f0f\uff1b\u968f\u540e\uff0c\u7ed3\u5408\u5fae\u8c03\u7684LLM\u548c\u591a\u8f6e\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u65e5\u5fd7\u5e8f\u5217\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u751f\u6210\u6f5c\u5728\u6545\u969c\u5047\u8bbe\u548c\u6839\u56e0\u8def\u5f84\uff1b\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u5f15\u5bfc\u6062\u590d\u89c4\u5212\u5668\uff0c\u7531LLM\u751f\u6210\u7684\u4fee\u590d\u7b56\u7565\u9a71\u52a8\uff0c\u652f\u6301\u4e91\u73af\u5883\u4e2d\u7684\u52a8\u6001\u51b3\u7b56\u548c\u81ea\u9002\u5e94\u8c03\u8bd5\u3002\u4e0e\u73b0\u6709\u89c4\u5219\u5f15\u64ce\u6216\u4f20\u7edf\u65e5\u5fd7\u5206\u6790\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6240\u63d0\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3001\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u548c\u5f02\u6784\u73af\u5883\u9002\u5e94\u6027\u3002\u5728\u4e91\u5e73\u53f0\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM-ID\u5c06\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u9ad8\u4e8616.2%\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002"}}
{"id": "2506.17630", "pdf": "https://arxiv.org/pdf/2506.17630", "abs": "https://arxiv.org/abs/2506.17630", "authors": ["Yang Wu", "Yifan Zhang", "Yiwei Wang", "Yujun Cai", "Yurong Wu", "Yuran Wang", "Ning Xu", "Jian Cheng"], "title": "Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs", "categories": ["cs.CL"], "comment": "14 pages, 8 figures", "summary": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, growing evidence suggests much of their success stems from\nmemorized answer-reasoning patterns rather than genuine inference. In this\nwork, we investigate a central question: are LLMs primarily anchored to final\nanswers or to the textual pattern of reasoning chains? We propose a five-level\nanswer-visibility prompt framework that systematically manipulates answer cues\nand probes model behavior through indirect, behavioral analysis. Experiments\nacross state-of-the-art LLMs reveal a strong and consistent reliance on\nexplicit answers. The performance drops by 26.90\\% when answer cues are masked,\neven with complete reasoning chains. These findings suggest that much of the\nreasoning exhibited by LLMs may reflect post-hoc rationalization rather than\ntrue inference, calling into question their inferential depth. Our study\nuncovers the answer-anchoring phenomenon with rigorous empirical validation and\nunderscores the need for a more nuanced understanding of what constitutes\nreasoning in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66f4\u591a\u4f9d\u8d56\u8bb0\u5fc6\u7684\u7b54\u6848\u6a21\u5f0f\u800c\u975e\u771f\u5b9e\u63a8\u7406\uff0c\u901a\u8fc7\u4e94\u7ea7\u7b54\u6848\u53ef\u89c1\u6027\u63d0\u793a\u6846\u67b6\u9a8c\u8bc1\u4e86\u5176\u5bf9\u663e\u5f0f\u7b54\u6848\u7684\u5f3a\u70c8\u4f9d\u8d56\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8d8a\u6765\u8d8a\u591a\u7684\u8bc1\u636e\u8868\u660e\u5176\u6210\u529f\u53ef\u80fd\u6e90\u4e8e\u8bb0\u5fc6\u7684\u7b54\u6848-\u63a8\u7406\u6a21\u5f0f\u800c\u975e\u771f\u5b9e\u63a8\u7406\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76LLMs\u662f\u5426\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6700\u7ec8\u7b54\u6848\u6216\u63a8\u7406\u94fe\u7684\u6587\u672c\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e94\u7ea7\u7b54\u6848\u53ef\u89c1\u6027\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u64cd\u7eb5\u7b54\u6848\u7ebf\u7d22\u5e76\u95f4\u63a5\u5206\u6790\u6a21\u578b\u884c\u4e3a\uff0c\u9a8c\u8bc1LLMs\u5bf9\u663e\u5f0f\u7b54\u6848\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u7b54\u6848\u7ebf\u7d22\u88ab\u63a9\u76d6\u65f6\uff0c\u5373\u4f7f\u63a8\u7406\u94fe\u5b8c\u6574\uff0cLLMs\u7684\u6027\u80fd\u4ecd\u4e0b\u964d26.90%\uff0c\u8868\u660e\u5176\u63a8\u7406\u53ef\u80fd\u66f4\u591a\u662f\u4e8b\u540e\u5408\u7406\u5316\u800c\u975e\u771f\u5b9e\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5bf9\u7b54\u6848\u7684\u951a\u5b9a\u73b0\u8c61\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66f4\u7ec6\u81f4\u5730\u7406\u89e3LLMs\u7684\u63a8\u7406\u672c\u8d28\u3002", "paper_title_zh": "\u7b54\u6848\u4e2d\u5fc3\u8fd8\u662f\u63a8\u7406\u9a71\u52a8\uff1f\u63ed\u793aLLMs\u4e2d\u7684\u6f5c\u5728\u8bb0\u5fc6\u951a\u70b9", "abstract_zh": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8d8a\u6765\u8d8a\u591a\u7684\u8bc1\u636e\u8868\u660e\u5176\u6210\u529f\u53ef\u80fd\u6e90\u4e8e\u8bb0\u5fc6\u7684\u7b54\u6848-\u63a8\u7406\u6a21\u5f0f\u800c\u975e\u771f\u5b9e\u63a8\u7406\u3002\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\uff1aLLMs\u662f\u5426\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6700\u7ec8\u7b54\u6848\u6216\u63a8\u7406\u94fe\u7684\u6587\u672c\u6a21\u5f0f\uff1f\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u7ea7\u7b54\u6848\u53ef\u89c1\u6027\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u64cd\u7eb5\u7b54\u6848\u7ebf\u7d22\u5e76\u95f4\u63a5\u5206\u6790\u6a21\u578b\u884c\u4e3a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u63a8\u7406\u94fe\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\uff0c\u5f53\u7b54\u6848\u7ebf\u7d22\u88ab\u63a9\u76d6\u65f6\uff0c\u6027\u80fd\u4e0b\u964d\u4e8626.90%\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cLLMs\u5c55\u73b0\u7684\u63a8\u7406\u53ef\u80fd\u66f4\u591a\u662f\u4e8b\u540e\u5408\u7406\u5316\u800c\u975e\u771f\u5b9e\u63a8\u7406\uff0c\u5bf9\u5176\u63a8\u7406\u6df1\u5ea6\u63d0\u51fa\u4e86\u8d28\u7591\u3002\u672c\u7814\u7a76\u901a\u8fc7\u4e25\u683c\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u63ed\u793a\u4e86\u7b54\u6848\u951a\u5b9a\u73b0\u8c61\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66f4\u7ec6\u81f4\u5730\u7406\u89e3LLMs\u7684\u63a8\u7406\u672c\u8d28\u3002"}}
{"id": "2506.17558", "pdf": "https://arxiv.org/pdf/2506.17558", "abs": "https://arxiv.org/abs/2506.17558", "authors": ["Jake Levi", "Mark van der Wilk"], "title": "SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at Methods and Opportunities at Small Scale (MOSS), ICML\n  2025, Vancouver, Canada", "summary": "Learning to infer object representations, and in particular part-whole\nhierarchies, has been the focus of extensive research in computer vision, in\npursuit of improving data efficiency, systematic generalisation, and\nrobustness. Models which are \\emph{designed} to infer part-whole hierarchies,\noften referred to as capsule networks, are typically trained end-to-end on\nsupervised tasks such as object classification, in which case it is difficult\nto evaluate whether such a model \\emph{actually} learns to infer part-whole\nhierarchies, as claimed. To address this difficulty, we present a SYNthetic\nDAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and\nestablish its utility by (1) demonstrating the precise bottleneck in a\nprominent existing capsule model, and (2) demonstrating that\npermutation-equivariant self-attention is highly effective for parts-to-wholes\ninference, which motivates future directions for designing effective inductive\nbiases for computer vision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSynDaCaTE\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u80f6\u56ca\u6a21\u578b\u7684\u74f6\u9888\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u90e8\u5206\u5230\u6574\u4f53\u63a8\u7406\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u80f6\u56ca\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u9a8c\u8bc1\u662f\u5426\u771f\u6b63\u5b66\u4e60\u5230\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6b64\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86SynDaCaTE\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u73b0\u6709\u80f6\u56ca\u6a21\u578b\u7684\u74f6\u9888\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86\u7f6e\u6362\u7b49\u53d8\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u90e8\u5206-\u6574\u4f53\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u80f6\u56ca\u6a21\u578b\u5b58\u5728\u74f6\u9888\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u90e8\u5206\u5230\u6574\u4f53\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "SynDaCaTE\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u3002", "paper_title_zh": "SynDaCaTE\uff1a\u7528\u4e8e\u8bc4\u4f30\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u63a8\u7406\u7684\u5408\u6210\u6570\u636e\u96c6", "abstract_zh": "\u5b66\u4e60\u63a8\u65ad\u5bf9\u8c61\u8868\u793a\uff0c\u5c24\u5176\u662f\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u7ed3\u6784\uff0c\u4e00\u76f4\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u7684\u91cd\u70b9\uff0c\u65e8\u5728\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3001\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u8bbe\u8ba1\u7528\u4e8e\u63a8\u65ad\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u7ed3\u6784\u7684\u6a21\u578b\uff08\u901a\u5e38\u79f0\u4e3a\u80f6\u56ca\u7f51\u7edc\uff09\u901a\u5e38\u5728\u76d1\u7763\u4efb\u52a1\uff08\u5982\u5bf9\u8c61\u5206\u7c7b\uff09\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5f88\u96be\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b66\u4e60\u5230\u4e86\u90e8\u5206-\u6574\u4f53\u5c42\u6b21\u7ed3\u6784\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSynDaCaTE\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4ee5\u4e0b\u4e24\u70b9\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff1a\uff081\uff09\u63ed\u793a\u4e86\u73b0\u6709\u80f6\u56ca\u6a21\u578b\u7684\u74f6\u9888\uff0c\uff082\uff09\u5c55\u793a\u4e86\u7f6e\u6362\u7b49\u53d8\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u90e8\u5206\u5230\u6574\u4f53\u63a8\u7406\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u672a\u6765\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.17913", "pdf": "https://arxiv.org/pdf/2506.17913", "abs": "https://arxiv.org/abs/2506.17913", "authors": ["Jinjie Wei", "Jiyao Liu", "Lihao Liu", "Ming Hu", "Junzhi Ning", "Mingcheng Li", "Weijie Yin", "Junjun He", "Xiao Liang", "Chao Feng", "Dingkang Yang"], "title": "Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents have made significant progress in\nautomating digital tasks through the utilization of computer vision and\nlanguage models. Nevertheless, existing agent systems encounter notable\nlimitations. Firstly, they predominantly depend on trial and error decision\nmaking rather than progressive reasoning, thereby lacking the capability to\nlearn and adapt from interactive encounters. Secondly, these systems are\nassessed using overly simplistic single step accuracy metrics, which do not\nadequately reflect the intricate nature of real world GUI interactions. In this\npaper, we present CogniGUI, a cognitive framework developed to overcome these\nlimitations by enabling adaptive learning for GUI automation resembling\nhuman-like behavior. Inspired by Kahneman's Dual Process Theory, our approach\ncombines two main components: (1) an omni parser engine that conducts immediate\nhierarchical parsing of GUI elements through quick visual semantic analysis to\nidentify actionable components, and (2) a Group based Relative Policy\nOptimization (GRPO) grounding agent that assesses multiple interaction paths\nusing a unique relative reward system, promoting minimal and efficient\noperational routes. This dual-system design facilitates iterative ''exploration\nlearning mastery'' cycles, enabling the agent to enhance its strategies over\ntime based on accumulated experience. Moreover, to assess the generalization\nand adaptability of agent systems, we introduce ScreenSeek, a comprehensive\nbenchmark that includes multi application navigation, dynamic state\ntransitions, and cross interface coherence, which are often overlooked\nchallenges in current benchmarks. Experimental results demonstrate that\nCogniGUI surpasses state-of-the-art methods in both the current GUI grounding\nbenchmarks and our newly proposed benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCogniGUI\u6846\u67b6\uff0c\u7ed3\u5408Kahneman\u7684\u53cc\u7cfb\u7edf\u7406\u8bba\uff0c\u901a\u8fc7\u5feb\u901f\u89c6\u89c9\u8bed\u4e49\u5206\u6790\u548c\u76f8\u5bf9\u5956\u52b1\u4f18\u5316\uff0c\u5b9e\u73b0GUI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u5b66\u4e60\uff0c\u5e76\u5f15\u5165ScreenSeek\u57fa\u51c6\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u8bd5\u9519\u51b3\u7b56\uff0c\u7f3a\u4e4f\u6e10\u8fdb\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9eGUI\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CogniGUI\u6846\u67b6\u5305\u542b\u4e24\u90e8\u5206\uff1a(1) \u5168\u89e3\u6790\u5f15\u64ce\uff0c\u901a\u8fc7\u5feb\u901f\u89c6\u89c9\u8bed\u4e49\u5206\u6790\u8bc6\u522b\u53ef\u64cd\u4f5c\u7ec4\u4ef6\uff1b(2) GRPO\u4ee3\u7406\uff0c\u4f7f\u7528\u76f8\u5bf9\u5956\u52b1\u7cfb\u7edf\u8bc4\u4f30\u4ea4\u4e92\u8def\u5f84\u3002\u6b64\u5916\uff0c\u63d0\u51faScreenSeek\u57fa\u51c6\u6d4b\u8bd5\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCogniGUI\u5728\u73b0\u6709GUI\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684ScreenSeek\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CogniGUI\u901a\u8fc7\u53cc\u7cfb\u7edf\u8bbe\u8ba1\u5b9e\u73b0\u4e86GUI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u5b66\u4e60\u548c\u9ad8\u6548\u4ea4\u4e92\uff0cScreenSeek\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "paper_title_zh": "\u5b66\u4e60\u3001\u63a8\u7406\u4e0e\u4f18\u5316\uff1a\u57fa\u4e8eKahneman\u53cc\u7cfb\u7edf\u7406\u8bba\u7684GUI\u4ee3\u7406\u667a\u80fd\u6846\u67b6", "abstract_zh": "\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6570\u5b57\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u660e\u663e\u5c40\u9650\uff1a\u4e3b\u8981\u4f9d\u8d56\u8bd5\u9519\u51b3\u7b56\u800c\u975e\u6e10\u8fdb\u63a8\u7406\uff0c\u7f3a\u4e4f\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff1b\u8bc4\u4f30\u6307\u6807\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9eGUI\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002\u672c\u6587\u63d0\u51faCogniGUI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Kahneman\u7684\u53cc\u8fc7\u7a0b\u7406\u8bba\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u884c\u4e3a\u7684GUI\u81ea\u52a8\u5316\u5b66\u4e60\u3002\u5176\u6838\u5fc3\u5305\u62ec\uff1a(1) \u5168\u89e3\u6790\u5f15\u64ce\uff0c\u901a\u8fc7\u5feb\u901f\u89c6\u89c9\u8bed\u4e49\u5206\u6790\u5206\u5c42\u89e3\u6790GUI\u5143\u7d20\uff1b(2) GRPO\u4ee3\u7406\uff0c\u4f7f\u7528\u76f8\u5bf9\u5956\u52b1\u7cfb\u7edf\u8bc4\u4f30\u4ea4\u4e92\u8def\u5f84\uff0c\u4fc3\u8fdb\u9ad8\u6548\u64cd\u4f5c\u3002\u6b64\u5916\uff0c\u5f15\u5165ScreenSeek\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u5e94\u7528\u5bfc\u822a\u3001\u52a8\u6001\u72b6\u6001\u8f6c\u6362\u548c\u8de8\u754c\u9762\u4e00\u81f4\u6027\u7b49\u6311\u6218\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cCogniGUI\u5728\u73b0\u6709\u57fa\u51c6\u548c\u65b0\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.17637", "pdf": "https://arxiv.org/pdf/2506.17637", "abs": "https://arxiv.org/abs/2506.17637", "authors": ["Yang Wu", "Yifan Zhang", "Yurong Wu", "Yuran Wang", "Junkai Zhang", "Jian Cheng"], "title": "Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation", "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Large Language Models (LLMs) have revolutionized various domains but\nencounter substantial challenges in tackling optimization modeling tasks for\nOperations Research (OR), particularly when dealing with complex problem. In\nthis work, we propose Step-Opt-Instruct, a framework that augments existing\ndatasets and generates high-quality fine-tuning data tailored to optimization\nmodeling. Step-Opt-Instruct employs iterative problem generation to\nsystematically increase problem complexity and stepwise validation to\nrigorously verify data, preventing error propagation and ensuring the quality\nof the generated dataset. Leveraging this framework, we fine-tune open-source\nLLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that\nachieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and\nIndustryOR. Extensive experiments demonstrate the superior performance of\nStep-Opt, especially in addressing complex OR tasks, with a notable 17.01\\%\nimprovement in micro average accuracy on difficult problems. These findings\nhighlight the effectiveness of combining structured validation with gradual\nproblem refinement to advance the automation of decision-making processes using\nLLMs.The code and dataset are available at https://github.com/samwu-learn/Step.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStep-Opt-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u95ee\u9898\u548c\u9010\u6b65\u9a8c\u8bc1\u6570\u636e\uff0c\u63d0\u5347LLMs\u5728\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e2d\u9762\u4e34\u590d\u6742\u95ee\u9898\u5904\u7406\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "Step-Opt-Instruct\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u9010\u6b65\u590d\u6742\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u9010\u6b65\u9a8c\u8bc1\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff0c\u968f\u540e\u5bf9\u5f00\u6e90LLMs\uff08\u5982LLaMA-3-8B\u548cMistral-7B\uff09\u8fdb\u884c\u5fae\u8c03\u3002", "result": "Step-Opt\u5728NL4OPT\u3001MAMO\u548cIndustryOR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8617.01%\u7684\u5fae\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u9a8c\u8bc1\u548c\u9010\u6b65\u95ee\u9898\u4f18\u5316\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u51b3\u7b56\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002", "paper_title_zh": "Step-Opt\uff1a\u901a\u8fc7\u8fed\u4ee3\u6570\u636e\u5408\u6210\u4e0e\u7ed3\u6784\u5316\u9a8c\u8bc1\u63d0\u5347LLMs\u7684\u4f18\u5316\u5efa\u6a21\u80fd\u529b", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u8fd0\u7b79\u5b66\uff08OR\uff09\u4e2d\u7684\u590d\u6742\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u65f6\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u63d0\u51faStep-Opt-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u95ee\u9898\u548c\u9010\u6b65\u9a8c\u8bc1\u6570\u636e\uff0c\u4e3a\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5fae\u8c03\u6570\u636e\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u95ee\u9898\u590d\u6742\u5ea6\u548c\u4e25\u683c\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\uff0c\u907f\u514d\u9519\u8bef\u4f20\u64ad\u5e76\u786e\u4fdd\u751f\u6210\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u5bf9\u5f00\u6e90LLMs\uff08\u5982LLaMA-3-8B\u548cMistral-7B\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u53d1\u51faStep-Opt\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728NL4OPT\u3001MAMO\u548cIndustryOR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742OR\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8617.01%\u7684\u5fae\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u9a8c\u8bc1\u548c\u9010\u6b65\u95ee\u9898\u4f18\u5316\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LLMs\u5728\u51b3\u7b56\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\uff1ahttps://github.com/samwu-learn/Step\u3002"}}
{"id": "2506.17561", "pdf": "https://arxiv.org/pdf/2506.17561", "abs": "https://arxiv.org/abs/2506.17561", "authors": ["Chongkai Gao", "Zixuan Liu", "Zhenghao Chi", "Junshan Huang", "Xin Fei", "Yiwen Hou", "Yuxuan Zhang", "Yudi Lin", "Zhirui Fang", "Zeyu Jiang", "Lin Shao"], "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent studies on Vision-Language-Action (VLA) models have shifted from the\nend-to-end action-generation paradigm toward a pipeline involving task planning\nfollowed by action generation, demonstrating improved performance on various\ncomplex, long-horizon manipulation tasks. However, existing approaches vary\nsignificantly in terms of network architectures, planning paradigms,\nrepresentations, and training data sources, making it challenging for\nresearchers to identify the precise sources of performance gains and components\nto be further improved. To systematically investigate the impacts of different\nplanning paradigms and representations isolating from network architectures and\ntraining data, in this paper, we introduce VLA-OS, a unified VLA architecture\nseries capable of various task planning paradigms, and design a comprehensive\nsuite of controlled experiments across diverse object categories (rigid and\ndeformable), visual modalities (2D and 3D), environments (simulation and\nreal-world), and end-effectors (grippers and dexterous hands). Our results\ndemonstrate that: 1) visually grounded planning representations are generally\nbetter than language planning representations; 2) the Hierarchical-VLA paradigm\ngenerally achieves superior or comparable performance than other paradigms on\ntask performance, pretraining, generalization ability, scalability, and\ncontinual learning ability, albeit at the cost of slower training and inference\nspeeds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVLA-OS\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u67b6\u6784\u7cfb\u5217\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u4efb\u52a1\u89c4\u5212\u8303\u5f0f\u548c\u8868\u793a\uff0c\u53d1\u73b0\u89c6\u89c9\u57fa\u7840\u89c4\u5212\u8868\u793a\u4f18\u4e8e\u8bed\u8a00\u8868\u793a\uff0c\u4e14\u5206\u5c42VLA\u8303\u5f0f\u5728\u4efb\u52a1\u6027\u80fd\u3001\u9884\u8bad\u7ec3\u3001\u6cdb\u5316\u80fd\u529b\u7b49\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u4efb\u52a1\u89c4\u5212\u548c\u52a8\u4f5c\u751f\u6210\u65b9\u9762\u5b58\u5728\u67b6\u6784\u3001\u8303\u5f0f\u548c\u8868\u793a\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u6765\u6e90\u96be\u4ee5\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u89c4\u5212\u8303\u5f0f\u548c\u8868\u793a\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faVLA-OS\uff0c\u4e00\u79cd\u652f\u6301\u591a\u79cd\u4efb\u52a1\u89c4\u5212\u8303\u5f0f\u7684\u7edf\u4e00VLA\u67b6\u6784\u7cfb\u5217\uff0c\u5e76\u5728\u4e0d\u540c\u5bf9\u8c61\u7c7b\u522b\uff08\u521a\u4f53\u548c\u53ef\u53d8\u5f62\uff09\u3001\u89c6\u89c9\u6a21\u6001\uff082D\u548c3D\uff09\u3001\u73af\u5883\uff08\u4eff\u771f\u548c\u73b0\u5b9e\uff09\u548c\u672b\u7aef\u6267\u884c\u5668\uff08\u5939\u722a\u548c\u7075\u5de7\u624b\uff09\u4e0a\u8bbe\u8ba1\u5168\u9762\u7684\u5bf9\u7167\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u89c6\u89c9\u57fa\u7840\u89c4\u5212\u8868\u793a\u901a\u5e38\u4f18\u4e8e\u8bed\u8a00\u89c4\u5212\u8868\u793a\uff1b2\uff09\u5206\u5c42VLA\u8303\u5f0f\u5728\u4efb\u52a1\u6027\u80fd\u3001\u9884\u8bad\u7ec3\u3001\u6cdb\u5316\u80fd\u529b\u7b49\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002", "conclusion": "VLA-OS\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u7cfb\u7edf\u5b9e\u9a8c\u63ed\u793a\u4e86\u89c6\u89c9\u57fa\u7840\u89c4\u5212\u548c\u5206\u5c42\u8303\u5f0f\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765VLA\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "VLA-OS\uff1a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u89c4\u5212\u8868\u793a\u4e0e\u8303\u5f0f\u7ed3\u6784\u5206\u6790\u4e0e\u89e3\u6784", "abstract_zh": "\u8fd1\u671f\u5173\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u7814\u7a76\u4ece\u7aef\u5230\u7aef\u52a8\u4f5c\u751f\u6210\u8303\u5f0f\u8f6c\u5411\u4efb\u52a1\u89c4\u5212\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u6d41\u6c34\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7f51\u7edc\u67b6\u6784\u3001\u89c4\u5212\u8303\u5f0f\u3001\u8868\u793a\u548c\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u4e0a\u5dee\u5f02\u663e\u8457\uff0c\u4f7f\u5f97\u7814\u7a76\u8005\u96be\u4ee5\u660e\u786e\u6027\u80fd\u63d0\u5347\u7684\u5177\u4f53\u6765\u6e90\u548c\u6539\u8fdb\u65b9\u5411\u3002\u4e3a\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u89c4\u5212\u8303\u5f0f\u548c\u8868\u793a\u7684\u5f71\u54cd\uff08\u6392\u9664\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u6570\u636e\u7684\u5e72\u6270\uff09\uff0c\u672c\u6587\u63d0\u51faVLA-OS\uff0c\u4e00\u79cd\u652f\u6301\u591a\u79cd\u4efb\u52a1\u89c4\u5212\u8303\u5f0f\u7684\u7edf\u4e00VLA\u67b6\u6784\u7cfb\u5217\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6db5\u76d6\u4e0d\u540c\u5bf9\u8c61\u7c7b\u522b\uff08\u521a\u4f53\u548c\u53ef\u53d8\u5f62\uff09\u3001\u89c6\u89c9\u6a21\u6001\uff082D\u548c3D\uff09\u3001\u73af\u5883\uff08\u4eff\u771f\u548c\u73b0\u5b9e\uff09\u548c\u672b\u7aef\u6267\u884c\u5668\uff08\u5939\u722a\u548c\u7075\u5de7\u624b\uff09\u7684\u5168\u9762\u5bf9\u7167\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u89c6\u89c9\u57fa\u7840\u89c4\u5212\u8868\u793a\u901a\u5e38\u4f18\u4e8e\u8bed\u8a00\u89c4\u5212\u8868\u793a\uff1b2\uff09\u5206\u5c42VLA\u8303\u5f0f\u5728\u4efb\u52a1\u6027\u80fd\u3001\u9884\u8bad\u7ec3\u3001\u6cdb\u5316\u80fd\u529b\u3001\u6269\u5c55\u6027\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u4e0a\u8868\u73b0\u66f4\u4f18\u6216\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002"}}
{"id": "2506.17930", "pdf": "https://arxiv.org/pdf/2506.17930", "abs": "https://arxiv.org/abs/2506.17930", "authors": ["Jianyu Wang", "Zhiqiang Hu", "Lidong Bing"], "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "cs.RO"], "comment": "ICML 2025, and Code will be released at:\n  https://github.com/jianyu-cs/PromptQuine/", "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u4fee\u526a\u968f\u673a\u6f14\u793a\u4e3a\u770b\u4f3c\u65e0\u610f\u4e49\u7684\u201c\u80e1\u8a00\u4e71\u8bed\u201d\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf\u4f18\u5316\u6280\u672f\uff0c\u800c\u662f\u901a\u8fc7\u81ea\u53d1\u73b0\u6846\u67b6PromptQuine\u81ea\u52a8\u641c\u7d22\u4fee\u526a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5f00\u653e\u7684\u63d0\u793a\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u8bbe\u8ba1\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u548c\u6f14\u793a\uff0c\u4f46\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5e38\u89c4\u601d\u8def\uff0c\u63d0\u51fa\u4fee\u526a\u968f\u673a\u6f14\u793a\u4e3a\u201c\u80e1\u8a00\u4e71\u8bed\u201d\u53cd\u800c\u80fd\u63d0\u5347\u6027\u80fd\u3002\u7136\u800c\uff0c\u5982\u4f55\u53d1\u73b0\u6709\u6548\u7684\u4fee\u526a\u7b56\u7565\u4ecd\u662f\u4e00\u4e2a\u96be\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u7a33\u5065\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u81ea\u53d1\u73b0\u63d0\u793a\u4f18\u5316\u6846\u67b6PromptQuine\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u81ea\u52a8\u5bfb\u627e\u4fee\u526a\u7b56\u7565\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e0a\u4e0b\u6587\u4e2d\u7684\u6807\u8bb0\uff0c\u6a21\u62df\u81ea\u7136\u754c\u7684\u81ea\u7ec4\u7ec7\u73b0\u8c61\uff0c\u9010\u6b65\u6f14\u5316\u51fa\u975e\u5e38\u89c4\u4f46\u9ad8\u6548\u7684\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u3001\u591a\u9009\u95ee\u7b54\u3001\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u4e14\u8fd0\u884c\u65f6\u6548\u7387\u8f83\u9ad8\u3002", "conclusion": "\u672c\u6587\u53d1\u73b0\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u591a\u5f00\u653e\u5f0f\u641c\u7d22\u7b97\u6cd5\u4ee5\u4f18\u5316LLM\u63d0\u793a\u8bbe\u8ba1\u3002", "paper_title_zh": "\u4e0a\u4e0b\u6587\u4e2d\u7684\u52a8\u6001\u63d0\u793a\u6f14\u5316\uff1a\u4e00\u79cd\u5f00\u653e\u5f0f\u3001\u81ea\u6211\u590d\u5236\u7684\u89c6\u89d2", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u8bbe\u8ba1\u8303\u5f0f\uff0c\u6311\u6218\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u793a\u8bbe\u8ba1\u7684\u4f20\u7edf\u89c2\u5ff5\u3002\u4f20\u7edf\u89c2\u5ff5\u5f3a\u8c03\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u548c\u6f14\u793a\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u91cd\u8981\u6027\uff0c\u800c\u6211\u4eec\u53d1\u73b0\u5c06\u968f\u673a\u6f14\u793a\u4fee\u526a\u4e3a\u770b\u4f3c\u65e0\u610f\u4e49\u7684\u201c\u80e1\u8a00\u4e71\u8bed\u201d\u53cd\u800c\u80fd\u663e\u8457\u63d0\u5347\u591a\u79cd\u4efb\u52a1\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u79cd\u201c\u80e1\u8a00\u4e71\u8bed\u201d\u59cb\u7ec8\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u4e14\u4e0d\u53d7LLM\u5bf9\u9f50\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u53d1\u73b0\u6709\u6548\u7684\u4fee\u526a\u7b56\u7565\u5e76\u975e\u6613\u4e8b\uff0c\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u548c\u63d0\u793a\u538b\u7f29\u7b97\u6cd5\u5747\u65e0\u6cd5\u63d0\u4f9b\u7a33\u5065\u7ed3\u679c\uff0c\u66f4\u4e0d\u7528\u8bf4\u4eba\u7c7b\u76f4\u89c9\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u53d1\u73b0\u63d0\u793a\u4f18\u5316\u6846\u67b6PromptQuine\uff0c\u8fd9\u662f\u4e00\u79cd\u8fdb\u5316\u641c\u7d22\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u81ea\u52a8\u641c\u7d22\u4fee\u526a\u7b56\u7565\u3002\u7c7b\u4f3c\u4e8e\u81ea\u7136\u754c\u4e2d\u56e0\u8d44\u6e90\u9650\u5236\u800c\u6d8c\u73b0\u7684\u590d\u6742\u6027\uff08\u5982\u5171\u751f\u548c\u81ea\u7ec4\u7ec7\uff09\uff0c\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e0a\u4e0b\u6587\u4e2d\u7684\u6807\u8bb0\uff0c\u9010\u6b65\u6f14\u5316\u51fa\u975e\u5e38\u89c4\u4f46\u9ad8\u6548\u7684\u63d0\u793a\u3002\u6211\u4eec\u5728\u591a\u79cdLLM\u7684\u5206\u7c7b\u3001\u591a\u9009\u95ee\u7b54\u3001\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u8fd0\u884c\u65f6\u6548\u7387\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u53d1\u73b0\u80fd\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u591a\u5f00\u653e\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u4ee5\u63a8\u52a8\u66f4\u9ad8\u6548\u7684LLM\u63d0\u793a\u8bbe\u8ba1\u3002"}}
{"id": "2506.17671", "pdf": "https://arxiv.org/pdf/2506.17671", "abs": "https://arxiv.org/abs/2506.17671", "authors": ["Fabien Furfaro"], "title": "TPTT: Transforming Pretrained Transformer into Titans", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .", "AI": {"tldr": "TPTT\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7ebf\u6027\u5316\u6ce8\u610f\u529b\u673a\u5236\u548c\u5148\u8fdb\u5185\u5b58\u7ba1\u7406\u6280\u672f\u589e\u5f3a\u9884\u8bad\u7ec3Transformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4ecd\u662f\u957f\u671f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e3b\u8981\u6311\u6218\u3002TPTT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TPTT\u91c7\u7528Memory as Gate\uff08MaG\uff09\u548c\u6df7\u5408\u7ebf\u6027\u5316\u6ce8\u610f\u529b\uff08LiZA\uff09\u6280\u672f\uff0c\u5e76\u4e0eHugging Face Transformers\u5e93\u5b8c\u5168\u517c\u5bb9\uff0c\u652f\u6301\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08LoRA\uff09\u65e0\u7f1d\u9002\u914d\u4efb\u4f55\u56e0\u679cLLM\u3002", "result": "\u5728MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTPTT\u5728\u7ea610\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u5982Titans-Llama-3.2-1B\u7684Exact Match\uff08EM\uff09\u5206\u6570\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8620%\u3002", "conclusion": "TPTT\u5c55\u793a\u4e86\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u9ad8\u6548\u5229\u7528\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "TPTT\uff1a\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u5316\u4e3a\u6cf0\u5766", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4ecd\u662f\u957f\u671f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e3b\u8981\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86TPTT\uff08\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u5316\u4e3a\u6cf0\u5766\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7ebf\u6027\u5316\u6ce8\u610f\u529b\u673a\u5236\u548c\u5148\u8fdb\u5185\u5b58\u7ba1\u7406\u6280\u672f\u589e\u5f3a\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u3002TPTT\u91c7\u7528Memory as Gate\uff08MaG\uff09\u548c\u6df7\u5408\u7ebf\u6027\u5316\u6ce8\u610f\u529b\uff08LiZA\uff09\u7b49\u6280\u672f\uff0c\u5e76\u4e0eHugging Face Transformers\u5e93\u5b8c\u5168\u517c\u5bb9\uff0c\u652f\u6301\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08LoRA\uff09\u65e0\u7f1d\u9002\u914d\u4efb\u4f55\u56e0\u679cLLM\uff0c\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002\u6211\u4eec\u5728MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86TPTT\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u7ea610\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u89c2\u5bdf\u5230\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u663e\u8457\u63d0\u5347\u3002\u4f8b\u5982\uff0cTitans-Llama-3.2-1B\u7684Exact Match\uff08EM\uff09\u5206\u6570\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8620%\u3002\u7edf\u8ba1\u5206\u6790\u53ca\u4e0e\u6700\u65b0\u65b9\u6cd5\u7684\u6bd4\u8f83\u8bc1\u5b9e\u4e86TPTT\u7684\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/fabienfrfr/tptt\uff0cPython\u5305\u53d1\u5e03\u4e8ehttps://pypi.org/project/tptt/\u3002"}}
{"id": "2506.17562", "pdf": "https://arxiv.org/pdf/2506.17562", "abs": "https://arxiv.org/abs/2506.17562", "authors": ["Haoxuan Che", "Haibo Jin", "Zhengrui Guo", "Yi Lin", "Cheng Jin", "Hao Chen"], "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedMRG\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5b9e\u73b0\u591a\u4e2d\u5fc3\u9690\u79c1\u4fdd\u62a4\u7684\u533b\u5b66\u62a5\u544a\u751f\u6210\uff08MRG\uff09\uff0c\u89e3\u51b3\u4e86\u901a\u4fe1\u6548\u7387\u548c\u6570\u636e\u5f02\u8d28\u6027\u4e24\u5927\u6311\u6218\u3002", "motivation": "\u533b\u5b66\u62a5\u544a\u751f\u6210\uff08MRG\uff09\u9700\u8981\u5927\u91cf\u533b\u7597\u56fe\u50cf-\u62a5\u544a\u5bf9\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5206\u6563\u5728\u5404\u4e2d\u5fc3\u4e14\u9690\u79c1\u4fdd\u62a4\u4e25\u683c\uff0c\u96be\u4ee5\u96c6\u4e2d\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u4e2d\u5fc3\u534f\u4f5c\u65b9\u6cd5\u3002", "method": "FedMRG\u6846\u67b6\u91c7\u7528\u4f4e\u79e9\u5206\u89e3\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u5ba2\u6237\u7aef\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u548c\u53cc\u9002\u914d\u5668\u673a\u5236\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedMRG\u5728\u901a\u4fe1\u6548\u7387\u548c\u62a5\u544a\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u4e2d\u5fc3\u6570\u636e\u751f\u6210\u4e34\u5e8a\u51c6\u786e\u7684\u62a5\u544a\u3002", "conclusion": "FedMRG\u4e3a\u591a\u4e2d\u5fc3\u9690\u79c1\u4fdd\u62a4\u7684\u533b\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u517c\u5177\u901a\u4fe1\u6548\u7387\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u901a\u4fe1\u9ad8\u6548\u7684\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u533b\u5b66\u62a5\u544a\u751f\u6210", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\uff08MRG\uff09\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u53d1\u5c55\u9700\u8981\u5927\u91cf\u533b\u7597\u56fe\u50cf-\u62a5\u544a\u5bf9\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u901a\u5e38\u5206\u6563\u5728\u5404\u4e2d\u5fc3\u3002\u7531\u4e8e\u9690\u79c1\u6cd5\u89c4\u7684\u9650\u5236\uff0c\u96c6\u4e2d\u8fd9\u4e9b\u6570\u636e\u6781\u5177\u6311\u6218\u6027\uff0c\u963b\u788d\u4e86LLM\u9a71\u52a8\u7684MRG\u6a21\u578b\u7684\u5f00\u53d1\u548c\u5e7f\u6cdb\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FedMRG\uff0c\u9996\u4e2a\u5229\u7528\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u4e2d\u5fc3LLM\u9a71\u52a8MRG\u6a21\u578b\u5f00\u53d1\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u7684\u901a\u4fe1\u9ad8\u6548LLM\u8bad\u7ec3\u96be\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u9ad8\u6548\u5206\u89e3\u53c2\u6570\u66f4\u65b0\uff0c\u663e\u8457\u964d\u4f4e\u68af\u5ea6\u4f20\u8f93\u6210\u672c\uff0c\u4f7fLLM\u9a71\u52a8\u7684MRG\u5728\u5e26\u5bbd\u53d7\u9650\u7684FL\u73af\u5883\u4e2d\u53ef\u884c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230FL\u573a\u666f\u4e0bMRG\u7684\u53cc\u91cd\u5f02\u8d28\u6027\uff1a\u5404\u533b\u7597\u4e2d\u5fc3\u7684\u56fe\u50cf\u7279\u5f81\u5dee\u5f02\uff0c\u4ee5\u53ca\u62a5\u544a\u98ce\u683c\u548c\u672f\u8bed\u504f\u597d\u7684\u591a\u6837\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u4f18\u5316FedMRG\uff1a\uff081\uff09\u5728MRG\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u5ba2\u6237\u7aef\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7ed3\u5408\u8bca\u65ad\u9a71\u52a8\u7684\u63d0\u793a\uff0c\u6355\u6349\u5168\u5c40\u901a\u7528\u548c\u5c40\u90e8\u72ec\u7279\u7684\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u8bca\u65ad\u51c6\u786e\u6027\uff1b\uff082\uff09\u5728MRG\u89e3\u7801\u5668\u4e2d\u91c7\u7528\u53cc\u9002\u914d\u5668\u76f8\u4e92\u4fc3\u8fdb\u673a\u5236\uff0c\u534f\u8c03\u901a\u7528\u548c\u4e13\u7528\u9002\u914d\u5668\u4ee5\u5e94\u5bf9\u62a5\u544a\u98ce\u683c\u548c\u672f\u8bed\u7684\u5dee\u5f02\u3002\u901a\u8fc7\u5bf9\u6211\u4eec\u5efa\u7acb\u7684FL-MRG\u57fa\u51c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660e\u4e86FedMRG\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u51f8\u663e\u4e86\u5176\u5728\u5229\u7528\u591a\u4e2d\u5fc3\u6570\u636e\u751f\u6210\u4e34\u5e8a\u51c6\u786e\u62a5\u544a\u7684\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17959", "pdf": "https://arxiv.org/pdf/2506.17959", "abs": "https://arxiv.org/abs/2506.17959", "authors": ["Lizzy Farrugia", "Lilian M. Azzopardi", "Jeremy Debattista", "Charlie Abela"], "title": "medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs", "categories": ["cs.AI"], "comment": null, "summary": "The role of pharmacists is evolving from medicine dispensing to delivering\ncomprehensive pharmaceutical services within multidisciplinary healthcare\nteams. Central to this shift is access to accurate, up-to-date medicinal\nproduct information supported by robust data integration. Leveraging artificial\nintelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden\nrelationships and enable data-driven decision-making. This paper presents\nmedicX-KG, a pharmacist-oriented knowledge graph supporting clinical and\nregulatory decisions. It forms the semantic layer of the broader medicX\nplatform, powering predictive and explainable pharmacy services. medicX-KG\nintegrates data from three sources, including, the British National Formulary\n(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's\nregulatory landscape and combines European Medicines Agency alignment with\npartial UK supply dependence. The KG tackles the absence of a unified national\ndrug repository, reducing pharmacists' reliance on fragmented sources. Its\ndesign was informed by interviews with practicing pharmacists to ensure\nreal-world applicability. We detail the KG's construction, including data\nextraction, ontology design, and semantic mapping. Evaluation demonstrates that\nmedicX-KG effectively supports queries about drug availability, interactions,\nadverse reactions, and therapeutic classes. Limitations, including missing\ndetailed dosage encoding and real-time updates, are discussed alongside\ndirections for future enhancements.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86medicX-KG\uff0c\u4e00\u4e2a\u9762\u5411\u836f\u5242\u5e08\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u65e8\u5728\u6574\u5408\u591a\u6e90\u836f\u7269\u6570\u636e\uff0c\u652f\u6301\u4e34\u5e8a\u548c\u76d1\u7ba1\u51b3\u7b56\uff0c\u89e3\u51b3\u836f\u7269\u4fe1\u606f\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u836f\u5242\u5e08\u7684\u89d2\u8272\u6b63\u4ece\u7b80\u5355\u7684\u836f\u7269\u5206\u53d1\u8f6c\u5411\u63d0\u4f9b\u5168\u9762\u7684\u836f\u5b66\u670d\u52a1\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u836f\u7269\u4fe1\u606f\u5e93\uff0c\u5bfc\u81f4\u4f9d\u8d56\u788e\u7247\u5316\u6570\u636e\u6765\u6e90\u3002medicX-KG\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u6280\u672f\u6574\u5408\u591a\u6e90\u6570\u636e\uff0c\u652f\u6301\u836f\u5242\u5e08\u7684\u51b3\u7b56\u9700\u6c42\u3002", "method": "medicX-KG\u6574\u5408\u4e86\u82f1\u56fd\u56fd\u5bb6\u5904\u65b9\u96c6\uff08BNF\uff09\u3001DrugBank\u548c\u9a6c\u8033\u4ed6\u836f\u7269\u7ba1\u7406\u5c40\uff08MMA\uff09\u7684\u6570\u636e\uff0c\u901a\u8fc7\u6570\u636e\u63d0\u53d6\u3001\u672c\u4f53\u8bbe\u8ba1\u548c\u8bed\u4e49\u6620\u5c04\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u8fd8\u53c2\u8003\u4e86\u836f\u5242\u5e08\u7684\u5b9e\u8df5\u9700\u6c42\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cmedicX-KG\u80fd\u6709\u6548\u652f\u6301\u5173\u4e8e\u836f\u7269\u53ef\u7528\u6027\u3001\u76f8\u4e92\u4f5c\u7528\u3001\u4e0d\u826f\u53cd\u5e94\u548c\u6cbb\u7597\u7c7b\u522b\u7684\u67e5\u8be2\u3002\u4f46\u4e5f\u5b58\u5728\u5242\u91cf\u7f16\u7801\u4e0d\u8be6\u7ec6\u548c\u5b9e\u65f6\u66f4\u65b0\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "conclusion": "medicX-KG\u4e3a\u836f\u5242\u5e08\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u836f\u7269\u4fe1\u606f\u5e73\u53f0\uff0c\u652f\u6301\u4e34\u5e8a\u548c\u76d1\u7ba1\u51b3\u7b56\u3002\u672a\u6765\u9700\u6539\u8fdb\u5242\u91cf\u7f16\u7801\u548c\u5b9e\u65f6\u66f4\u65b0\u529f\u80fd\u3002", "paper_title_zh": "medicX-KG\uff1a\u6ee1\u8db3\u836f\u5242\u5e08\u836f\u7269\u4fe1\u606f\u9700\u6c42\u7684\u77e5\u8bc6\u56fe\u8c31", "abstract_zh": "\u836f\u5242\u5e08\u7684\u89d2\u8272\u6b63\u4ece\u836f\u7269\u5206\u53d1\u8f6c\u5411\u5728\u591a\u5b66\u79d1\u533b\u7597\u56e2\u961f\u4e2d\u63d0\u4f9b\u5168\u9762\u7684\u836f\u5b66\u670d\u52a1\u3002\u8fd9\u4e00\u8f6c\u53d8\u7684\u6838\u5fc3\u662f\u83b7\u53d6\u51c6\u786e\u3001\u6700\u65b0\u7684\u836f\u7269\u4fe1\u606f\uff0c\u5e76\u4f9d\u8d56\u5f3a\u5927\u7684\u6570\u636e\u6574\u5408\u3002\u501f\u52a9\u4eba\u5de5\u667a\u80fd\u548c\u8bed\u4e49\u6280\u672f\uff0c\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u80fd\u591f\u63ed\u793a\u9690\u85cf\u7684\u5173\u7cfb\u5e76\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u3002\u672c\u6587\u4ecb\u7ecd\u4e86medicX-KG\uff0c\u4e00\u4e2a\u9762\u5411\u836f\u5242\u5e08\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u65e8\u5728\u652f\u6301\u4e34\u5e8a\u548c\u76d1\u7ba1\u51b3\u7b56\u3002\u5b83\u4f5c\u4e3amedicX\u5e73\u53f0\u7684\u8bed\u4e49\u5c42\uff0c\u4e3a\u9884\u6d4b\u6027\u548c\u53ef\u89e3\u91ca\u7684\u836f\u5b66\u670d\u52a1\u63d0\u4f9b\u652f\u6301\u3002medicX-KG\u6574\u5408\u4e86\u4e09\u4e2a\u6570\u636e\u6e90\uff0c\u5305\u62ec\u82f1\u56fd\u56fd\u5bb6\u5904\u65b9\u96c6\uff08BNF\uff09\u3001DrugBank\u548c\u9a6c\u8033\u4ed6\u836f\u7269\u7ba1\u7406\u5c40\uff08MMA\uff09\uff0c\u4ee5\u5e94\u5bf9\u9a6c\u8033\u4ed6\u7684\u76d1\u7ba1\u73af\u5883\uff0c\u540c\u65f6\u7ed3\u5408\u6b27\u6d32\u836f\u54c1\u7ba1\u7406\u5c40\u7684\u89c4\u8303\u5e76\u90e8\u5206\u4f9d\u8d56\u82f1\u56fd\u7684\u4f9b\u5e94\u3002\u8be5\u77e5\u8bc6\u56fe\u8c31\u89e3\u51b3\u4e86\u7f3a\u4e4f\u7edf\u4e00\u56fd\u5bb6\u836f\u7269\u5e93\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u836f\u5242\u5e08\u5bf9\u788e\u7247\u5316\u6765\u6e90\u7684\u4f9d\u8d56\u3002\u5176\u8bbe\u8ba1\u57fa\u4e8e\u5bf9\u6267\u4e1a\u836f\u5242\u5e08\u7684\u8bbf\u8c08\uff0c\u4ee5\u786e\u4fdd\u5b9e\u9645\u9002\u7528\u6027\u3002\u6211\u4eec\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u8fc7\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u63d0\u53d6\u3001\u672c\u4f53\u8bbe\u8ba1\u548c\u8bed\u4e49\u6620\u5c04\u3002\u8bc4\u4f30\u8868\u660e\uff0cmedicX-KG\u80fd\u6709\u6548\u652f\u6301\u5173\u4e8e\u836f\u7269\u53ef\u7528\u6027\u3001\u76f8\u4e92\u4f5c\u7528\u3001\u4e0d\u826f\u53cd\u5e94\u548c\u6cbb\u7597\u7c7b\u522b\u7684\u67e5\u8be2\u3002\u6587\u4e2d\u8fd8\u8ba8\u8bba\u4e86\u5c40\u9650\u6027\uff0c\u5982\u7f3a\u5c11\u8be6\u7ec6\u7684\u5242\u91cf\u7f16\u7801\u548c\u5b9e\u65f6\u66f4\u65b0\u529f\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2506.17692", "pdf": "https://arxiv.org/pdf/2506.17692", "abs": "https://arxiv.org/abs/2506.17692", "authors": ["Binquan Ji", "Haibo Luo", "Yifei Lu", "Lei Hei", "Jiaqi Wang", "Tingjing Liao", "Lingyu Wang", "Shichao Wang", "Feiliang Ren"], "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge-intensive multi-hop question answering (QA) tasks, which require\nintegrating evidence from multiple sources to address complex queries, often\nnecessitate multiple rounds of retrieval and iterative generation by large\nlanguage models (LLMs). However, incorporating many documents and extended\ncontexts poses challenges -such as hallucinations and semantic drift-for\nlightweight LLMs with fewer parameters. This work proposes a novel framework\ncalled DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions\ninto logically coherent subquestions to form a hallucination-free reasoning\nchain. It then iteratively refines these subquestions through context-aware\nrewriting to generate effective query formulations. For retrieval, we introduce\na lightweight discriminative keyword extraction module that leverages extracted\nkeywords to achieve targeted, precise document recall with relatively low\ncomputational overhead. Extensive experiments on three multi-hop QA datasets\ndemonstrate that DEC performs on par with or surpasses state-of-the-art\nbenchmarks while significantly reducing token consumption. Notably, our\napproach attains state-of-the-art results on models with 8B parameters,\nshowcasing its effectiveness in various scenarios, particularly in\nresource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEC\uff08\u52a8\u6001\u589e\u5f3a\u94fe\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002DEC\u901a\u8fc7\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u903b\u8f91\u8fde\u8d2f\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u5173\u952e\u8bcd\u63d0\u53d6\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u9700\u8981\u4ece\u591a\u4e2a\u6765\u6e90\u6574\u5408\u8bc1\u636e\uff0c\u4f46\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u6587\u6863\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u8d44\u6e90\u53cb\u597d\u7684\u6846\u67b6\uff0c\u65e2\u80fd\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\uff0c\u53c8\u80fd\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "DEC\u6846\u67b6\u9996\u5148\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u903b\u8f91\u8fde\u8d2f\u7684\u5b50\u95ee\u9898\uff0c\u5f62\u6210\u65e0\u5e7b\u89c9\u7684\u63a8\u7406\u94fe\uff1b\u7136\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u91cd\u5199\u8fed\u4ee3\u4f18\u5316\u5b50\u95ee\u9898\u3002\u6b64\u5916\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u5224\u522b\u6027\u5173\u952e\u8bcd\u63d0\u53d6\u6a21\u5757\uff0c\u5229\u7528\u5173\u952e\u8bcd\u5b9e\u73b0\u7cbe\u51c6\u6587\u6863\u53ec\u56de\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDEC\u7684\u6027\u80fd\u4e0e\u6216\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u51c6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u6d88\u8017\u3002\u7279\u522b\u662f\u57288B\u53c2\u6570\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DEC\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u589e\u5f3a\u94fe\u548c\u8f7b\u91cf\u7ea7\u5173\u952e\u8bcd\u63d0\u53d6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "paper_title_zh": "\u8d44\u6e90\u53cb\u597d\u7684\u52a8\u6001\u589e\u5f3a\u94fe\u7528\u4e8e\u591a\u8df3\u95ee\u7b54", "abstract_zh": "\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u8df3\u95ee\u7b54\uff08QA\uff09\u4efb\u52a1\u9700\u8981\u901a\u8fc7\u6574\u5408\u591a\u6e90\u8bc1\u636e\u6765\u89e3\u51b3\u590d\u6742\u67e5\u8be2\uff0c\u901a\u5e38\u9700\u8981\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u591a\u8f6e\u68c0\u7d22\u548c\u8fed\u4ee3\u751f\u6210\u3002\u7136\u800c\uff0c\u5904\u7406\u5927\u91cf\u6587\u6863\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\u5bf9\u53c2\u6570\u8f83\u5c11\u7684\u8f7b\u91cf\u7ea7LLM\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f8b\u5982\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEC\uff08\u52a8\u6001\u589e\u5f3a\u94fe\uff09\u7684\u65b0\u6846\u67b6\u3002DEC\u9996\u5148\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u903b\u8f91\u8fde\u8d2f\u7684\u5b50\u95ee\u9898\uff0c\u5f62\u6210\u65e0\u5e7b\u89c9\u7684\u63a8\u7406\u94fe\uff1b\u7136\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u91cd\u5199\u8fed\u4ee3\u4f18\u5316\u8fd9\u4e9b\u5b50\u95ee\u9898\u4ee5\u751f\u6210\u6709\u6548\u7684\u67e5\u8be2\u8868\u8ff0\u3002\u5728\u68c0\u7d22\u65b9\u9762\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5224\u522b\u6027\u5173\u952e\u8bcd\u63d0\u53d6\u6a21\u5757\uff0c\u5229\u7528\u63d0\u53d6\u7684\u5173\u952e\u8bcd\u5b9e\u73b0\u76ee\u6807\u660e\u786e\u3001\u7cbe\u786e\u7684\u6587\u6863\u53ec\u56de\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u8f83\u4f4e\u3002\u5728\u4e09\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDEC\u7684\u6027\u80fd\u4e0e\u6216\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u51c6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u6d88\u8017\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u57288B\u53c2\u6570\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2506.17587", "pdf": "https://arxiv.org/pdf/2506.17587", "abs": "https://arxiv.org/abs/2506.17587", "authors": ["Le Yu", "Kaishen Wang", "Jianlong Xiong", "Yue Cao", "Tao He"], "title": "HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 figures, 9 tables", "summary": "Though Large Vision-Language Models (LVLMs) have achieved remarkable\nperformance across various tasks, they are still prone to\nhallucinations-generating outputs that are textually plausible but visually\nungrounded. While prior approaches generally address this issue through\ndata-centric fine-tuning or innovative decoding strategies, these methods often\nrequire substantial resources or task-specific configurations. In this work, we\nintroduce an architecture-level solution, HalluRNN, which enhances model\nstability through recurrent cross-layer reasoning. Specifically, we propose a\nnovel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across\nlayers and recurrently refines hidden states. This allows for the adaptive\npropagation of information throughout the model, enforces consistency across\nlayers, and mitigates hallucinations caused by representational drift. By\nfine-tuning only the DG-DPU module, HalluRNN achieves strong and robust\nperformance across multiple benchmarks.", "AI": {"tldr": "HalluRNN\u901a\u8fc7\u8de8\u5c42\u5faa\u73af\u63a8\u7406\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u51fa\u53cc\u95e8\u6df1\u5ea6\u4f20\u64ad\u5355\u5143\uff08DG-DPU\uff09\uff0c\u4ec5\u5fae\u8c03\u8be5\u6a21\u5757\u5373\u53ef\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u867d\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6613\u4ea7\u751f\u89c6\u89c9\u65e0\u4f9d\u636e\u7684\u5e7b\u89c9\u8f93\u51fa\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8d44\u6e90\u6216\u4efb\u52a1\u7279\u5b9a\u914d\u7f6e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHalluRNN\u67b6\u6784\uff0c\u5f15\u5165\u53cc\u95e8\u6df1\u5ea6\u4f20\u64ad\u5355\u5143\uff08DG-DPU\uff09\uff0c\u8be5\u6a21\u5757\u8de8\u5c42\u5171\u4eab\u5e76\u901a\u8fc7\u5faa\u73af\u63a8\u7406\u4f18\u5316\u9690\u85cf\u72b6\u6001\uff0c\u81ea\u9002\u5e94\u4f20\u64ad\u4fe1\u606f\u5e76\u51cf\u5c11\u8868\u5f81\u6f02\u79fb\u5bfc\u81f4\u7684\u5e7b\u89c9\u3002", "result": "\u4ec5\u5fae\u8c03DG-DPU\u6a21\u5757\u7684HalluRNN\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "HalluRNN\u901a\u8fc7\u8de8\u5c42\u5faa\u73af\u63a8\u7406\u548cDG-DPU\u6a21\u5757\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8d44\u6e90\u53cb\u597d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "paper_title_zh": "HalluRNN\uff1a\u901a\u8fc7\u8de8\u5c42\u5faa\u73af\u63a8\u7406\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9", "abstract_zh": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u4ecd\u6613\u4ea7\u751f\u6587\u672c\u5408\u7406\u4f46\u89c6\u89c9\u65e0\u4f9d\u636e\u7684\u5e7b\u89c9\u8f93\u51fa\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5fae\u8c03\u6216\u521b\u65b0\u89e3\u7801\u7b56\u7565\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5927\u91cf\u8d44\u6e90\u6216\u4efb\u52a1\u7279\u5b9a\u914d\u7f6e\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u67b6\u6784\u7ea7\u89e3\u51b3\u65b9\u6848HalluRNN\uff0c\u901a\u8fc7\u8de8\u5c42\u5faa\u73af\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u7a33\u5b9a\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u53cc\u95e8\u6df1\u5ea6\u4f20\u64ad\u5355\u5143\uff08DG-DPU\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u8de8\u5c42\u5171\u4eab\u5e76\u901a\u8fc7\u5faa\u73af\u4f18\u5316\u9690\u85cf\u72b6\u6001\uff0c\u5b9e\u73b0\u4fe1\u606f\u81ea\u9002\u5e94\u4f20\u64ad\uff0c\u786e\u4fdd\u5c42\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u51cf\u5c11\u8868\u5f81\u6f02\u79fb\u5bfc\u81f4\u7684\u5e7b\u89c9\u3002\u4ec5\u5fae\u8c03DG-DPU\u6a21\u5757\u7684HalluRNN\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u7a33\u5065\u3002"}}
{"id": "2506.18019", "pdf": "https://arxiv.org/pdf/2506.18019", "abs": "https://arxiv.org/abs/2506.18019", "authors": ["Yuanchen Bei", "Weizhi Zhang", "Siwen Wang", "Weizhi Chen", "Sheng Zhou", "Hao Chen", "Yong Li", "Jiajun Bu", "Shirui Pan", "Yizhou Yu", "Irwin King", "Fakhri Karray", "Philip S. Yu"], "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities", "categories": ["cs.AI"], "comment": "20 pages, 7 figures", "summary": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u56fe\u6280\u672f\u5982\u4f55\u8d4b\u80fdAI\u4ee3\u7406\uff0c\u63a2\u8ba8\u4e86\u56fe\u6280\u672f\u4e0e\u6838\u5fc3\u4ee3\u7406\u529f\u80fd\u7684\u7ed3\u5408\u3001\u663e\u8457\u5e94\u7528\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8\u4e0b\u4e00\u4ee3AI\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u4ece\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6f14\u8fdb\uff0c\u5176\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u4f46\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u4ecd\u9700\u9ad8\u6548\u89c4\u5212\u3001\u53ef\u9760\u8bb0\u5fc6\u548c\u591a\u4ee3\u7406\u534f\u8c03\u3002\u56fe\u6280\u672f\u56e0\u5176\u7ed3\u6784\u5316\u6570\u636e\u7684\u4f18\u52bf\uff0c\u6709\u671b\u652f\u6301\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u56fe\u6280\u672f\u4e0eAI\u4ee3\u7406\u6838\u5fc3\u529f\u80fd\u7684\u7ed3\u5408\uff0c\u5305\u62ec\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u534f\u8c03\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u56fe\u6280\u672f\u80fd\u591f\u6709\u6548\u652f\u6301AI\u4ee3\u7406\u5904\u7406\u590d\u6742\u6570\u636e\u548c\u4ea4\u4e92\uff0c\u63d0\u5347\u5176\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u4e2a\u6f5c\u5728\u65b9\u5411\u3002", "conclusion": "\u56fe\u6280\u672f\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u636e\u8303\u5f0f\u652f\u6301\uff0c\u672a\u6765\u7814\u7a76\u5e94\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u4e0e\u4ee3\u7406\u529f\u80fd\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u6311\u6218\u3002", "paper_title_zh": "\u56fe\u4e0eAI\u4ee3\u7406\u7684\u4ea4\u6c47\uff1a\u5206\u7c7b\u3001\u8fdb\u5c55\u4e0e\u672a\u6765\u673a\u9047", "abstract_zh": "AI\u4ee3\u7406\u7ecf\u5386\u4e86\u4ece\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5982\u4eca\u8fdb\u4e00\u6b65\u5411RL\u4e0eLLM\u80fd\u529b\u7684\u534f\u540c\u878d\u5408\u8fc8\u8fdb\u3002\u8fd9\u4e00\u8fdb\u5c55\u8d4b\u4e88\u4ee3\u7406\u66f4\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5b8c\u6210\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4ecd\u9700\u9ad8\u6548\u89c4\u5212\u3001\u53ef\u9760\u8bb0\u5fc6\u548c\u591a\u4ee3\u7406\u534f\u8c03\u3002\u9762\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6570\u636e\u7ed3\u6784\u5316\u901a\u8fc7\u5c06\u590d\u6742\u65e0\u5e8f\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5f62\u5f0f\uff0c\u6709\u671b\u652f\u6301\u4ee3\u7406\u66f4\u9ad8\u6548\u5730\u7406\u89e3\u548c\u5904\u7406\u6570\u636e\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u56fe\u51ed\u501f\u5176\u5728\u7ec4\u7ec7\u3001\u7ba1\u7406\u548c\u5229\u7528\u590d\u6742\u6570\u636e\u5173\u7cfb\u65b9\u9762\u7684\u5929\u7136\u4f18\u52bf\uff0c\u4e3a\u652f\u6301\u9ad8\u7ea7AI\u4ee3\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u636e\u8303\u5f0f\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7efc\u8ff0\u4e86\u56fe\u5982\u4f55\u8d4b\u80fdAI\u4ee3\u7406\uff0c\u63a2\u8ba8\u4e86\u56fe\u6280\u672f\u4e0e\u6838\u5fc3\u4ee3\u7406\u529f\u80fd\u7684\u7ed3\u5408\u3001\u663e\u8457\u5e94\u7528\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u901a\u8fc7\u5168\u9762\u7efc\u8ff0\u8fd9\u4e00\u65b0\u5174\u4ea4\u53c9\u9886\u57df\uff0c\u6211\u4eec\u5e0c\u671b\u63a8\u52a8\u4e0b\u4e00\u4ee3AI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u4f7f\u5176\u80fd\u591f\u501f\u52a9\u56fe\u6280\u672f\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6311\u6218\u3002\u76f8\u5173\u8d44\u6e90\u5df2\u5728Github\u94fe\u63a5\u4e2d\u6536\u96c6\u5e76\u6301\u7eed\u66f4\u65b0\u3002"}}
{"id": "2506.17693", "pdf": "https://arxiv.org/pdf/2506.17693", "abs": "https://arxiv.org/abs/2506.17693", "authors": ["Yuzhe Ding", "Kang He", "Bobo Li", "Li Zheng", "Haijun He", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches", "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025 (Findings)", "summary": "Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nincreasing number of online debates among social media users, conversational\nstance detection has become a crucial research area. However, existing\nconversational stance detection datasets are restricted to a limited set of\nspecific targets, which constrains the effectiveness of stance detection models\nwhen encountering a large number of unseen targets in real-world applications.\nTo bridge this gap, we manually curate a large-scale, high-quality zero-shot\nconversational stance detection dataset, named ZS-CSD, comprising 280 targets\nacross two distinct target types. Leveraging the ZS-CSD dataset, we propose\nSITPCL, a speaker interaction and target-aware prototypical contrastive\nlearning model, and establish the benchmark performance in the zero-shot\nsetting. Experimental results demonstrate that our proposed SITPCL model\nachieves state-of-the-art performance in zero-shot conversational stance\ndetection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,\nhighlighting the persistent challenges in zero-shot conversational stance\ndetection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6ZS-CSD\uff0c\u5e76\u8bbe\u8ba1\u4e86SITPCL\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u5148\u8fdb\u6027\u80fd\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u7279\u5b9a\u76ee\u6807\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u5e94\u7528\u4e2d\u5927\u91cf\u672a\u89c1\u76ee\u6807\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u96f6\u6837\u672c\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u76f8\u5e94\u6a21\u578b\u3002", "method": "\u624b\u52a8\u6784\u5efa\u4e86\u5305\u542b280\u4e2a\u76ee\u6807\u7684ZS-CSD\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faSITPCL\u6a21\u578b\uff0c\u7ed3\u5408\u8bf4\u8bdd\u8005\u4ea4\u4e92\u548c\u76ee\u6807\u611f\u77e5\u7684\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "SITPCL\u5728\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46F1-macro\u5f97\u5206\u4ec5\u4e3a43.81%\uff0c\u663e\u793a\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u57fa\u51c6\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "paper_title_zh": "\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\uff1a\u6570\u636e\u96c6\u4e0e\u65b9\u6cd5", "abstract_zh": "\u7acb\u573a\u68c0\u6d4b\u65e8\u5728\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u8bc6\u522b\u516c\u4f17\u5bf9\u7279\u5b9a\u76ee\u6807\u7684\u89c2\u70b9\uff0c\u662f\u4e00\u9879\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u968f\u7740\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u5728\u7ebf\u8fa9\u8bba\u7684\u589e\u52a0\uff0c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6210\u4e3a\u5173\u952e\u7814\u7a76\u9886\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5c11\u91cf\u7279\u5b9a\u76ee\u6807\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u5bf9\u5927\u91cf\u672a\u89c1\u76ee\u6807\u65f6\u7684\u6709\u6548\u6027\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u624b\u52a8\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6ZS-CSD\uff0c\u6db5\u76d6\u4e24\u79cd\u76ee\u6807\u7c7b\u578b\u7684280\u4e2a\u76ee\u6807\u3002\u57fa\u4e8eZS-CSD\u6570\u636e\u96c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SITPCL\u6a21\u578b\uff0c\u4e00\u79cd\u7ed3\u5408\u8bf4\u8bdd\u8005\u4ea4\u4e92\u548c\u76ee\u6807\u611f\u77e5\u7684\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5efa\u7acb\u4e86\u57fa\u51c6\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSITPCL\u5728\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSITPCL\u6a21\u578b\u7684F1-macro\u5f97\u5206\u4ec5\u4e3a43.81%\uff0c\u51f8\u663e\u4e86\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u7684\u6301\u7eed\u6311\u6218\u3002"}}
{"id": "2506.17590", "pdf": "https://arxiv.org/pdf/2506.17590", "abs": "https://arxiv.org/abs/2506.17590", "authors": ["Mihir Godbole", "Xiangbo Gao", "Zhengzhong Tu"], "title": "DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "19 pages, 5 figures, Preprint under review. Code available at:\n  https://github.com/taco-group/DRAMA-X", "summary": "Understanding the short-term motion of vulnerable road users (VRUs) like\npedestrians and cyclists is critical for safe autonomous driving, especially in\nurban scenarios with ambiguous or high-risk behaviors. While vision-language\nmodels (VLMs) have enabled open-vocabulary perception, their utility for\nfine-grained intent reasoning remains underexplored. Notably, no existing\nbenchmark evaluates multi-class intent prediction in safety-critical\nsituations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark\nconstructed from the DRAMA dataset via an automated annotation pipeline.\nDRAMA-X contains 5,686 accident-prone frames labeled with object bounding\nboxes, a nine-class directional intent taxonomy, binary risk scores,\nexpert-generated action suggestions for the ego vehicle, and descriptive motion\nsummaries. These annotations enable a structured evaluation of four\ninterrelated tasks central to autonomous decision-making: object detection,\nintent prediction, risk assessment, and action suggestion. As a reference\nbaseline, we propose SGG-Intent, a lightweight, training-free framework that\nmirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene\ngraph from visual input using VLM-backed detectors, infers intent, assesses\nrisk, and recommends an action using a compositional reasoning stage powered by\na large language model. We evaluate a range of recent VLMs, comparing\nperformance across all four DRAMA-X tasks. Our experiments demonstrate that\nscene-graph-based reasoning enhances intent prediction and risk assessment,\nespecially when contextual cues are explicitly modeled.", "AI": {"tldr": "DRAMA-X\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u7ec6\u7c92\u5ea6\u610f\u56fe\u9884\u6d4b\u548c\u98ce\u9669\u63a8\u7406\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u591a\u7c7b\u610f\u56fe\u9884\u6d4b\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6SGG-Intent\u4f5c\u4e3a\u57fa\u7ebf\u3002", "motivation": "\u7406\u89e3\u884c\u4eba\u3001\u9a91\u884c\u8005\u7b49\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u77ed\u671f\u8fd0\u52a8\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u6216\u6a21\u7cca\u884c\u4e3a\u573a\u666f\u4e2d\u3002\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u591a\u7c7b\u610f\u56fe\u9884\u6d4b\u7684\u57fa\u51c6\uff0cDRAMA-X\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DRAMA-X\u57fa\u4e8eDRAMA\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\uff0c\u5305\u542b5,686\u5e27\u4e8b\u6545\u6613\u53d1\u573a\u666f\u7684\u6807\u6ce8\u6570\u636e\u3002\u63d0\u51faSGG-Intent\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u573a\u666f\u56fe\u751f\u6210\u3001\u610f\u56fe\u63a8\u7406\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u52a8\u4f5c\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u63a8\u7406\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u610f\u56fe\u9884\u6d4b\u548c\u98ce\u9669\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u663e\u5f0f\u5efa\u6a21\u4e0a\u4e0b\u6587\u7ebf\u7d22\u65f6\u3002", "conclusion": "DRAMA-X\u4e3a\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bc4\u4f30\u57fa\u51c6\uff0cSGG-Intent\u5c55\u793a\u4e86\u573a\u666f\u56fe\u63a8\u7406\u5728\u610f\u56fe\u548c\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "DRAMA-X\uff1a\u4e00\u79cd\u7528\u4e8e\u9a7e\u9a76\u7684\u7ec6\u7c92\u5ea6\u610f\u56fe\u9884\u6d4b\u548c\u98ce\u9669\u63a8\u7406\u57fa\u51c6", "abstract_zh": "\u7406\u89e3\u884c\u4eba\u3001\u9a91\u884c\u8005\u7b49\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u77ed\u671f\u8fd0\u52a8\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u6216\u6a21\u7cca\u884c\u4e3a\u573a\u666f\u4e2d\u3002\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5df2\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u611f\u77e5\uff0c\u4f46\u5176\u5728\u7ec6\u7c92\u5ea6\u610f\u56fe\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u76ee\u524d\u5c1a\u65e0\u57fa\u51c6\u8bc4\u4f30\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u591a\u7c7b\u610f\u56fe\u9884\u6d4b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DRAMA-X\uff0c\u4e00\u4e2a\u57fa\u4e8eDRAMA\u6570\u636e\u96c6\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u7684\u7ec6\u7c92\u5ea6\u57fa\u51c6\u3002DRAMA-X\u5305\u542b5,686\u5e27\u4e8b\u6545\u6613\u53d1\u573a\u666f\u7684\u6807\u6ce8\u6570\u636e\uff0c\u5305\u62ec\u76ee\u6807\u8fb9\u754c\u6846\u3001\u4e5d\u7c7b\u65b9\u5411\u610f\u56fe\u5206\u7c7b\u3001\u4e8c\u5143\u98ce\u9669\u8bc4\u5206\u3001\u4e13\u5bb6\u751f\u6210\u7684\u81ea\u6211\u8f66\u8f86\u52a8\u4f5c\u5efa\u8bae\u53ca\u8fd0\u52a8\u63cf\u8ff0\u6458\u8981\u3002\u8fd9\u4e9b\u6807\u6ce8\u652f\u6301\u5bf9\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4e2d\u56db\u9879\u6838\u5fc3\u4efb\u52a1\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\uff1a\u76ee\u6807\u68c0\u6d4b\u3001\u610f\u56fe\u9884\u6d4b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u52a8\u4f5c\u5efa\u8bae\u3002\u4f5c\u4e3a\u53c2\u8003\u57fa\u7ebf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SGG-Intent\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u6a21\u62df\u81ea\u6211\u8f66\u8f86\u7684\u63a8\u7406\u6d41\u7a0b\u3002\u5b83\u901a\u8fc7VLM\u652f\u6301\u7684\u68c0\u6d4b\u5668\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u573a\u666f\u56fe\uff0c\u63a8\u65ad\u610f\u56fe\uff0c\u8bc4\u4f30\u98ce\u9669\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u652f\u6301\u7684\u7ec4\u5408\u63a8\u7406\u9636\u6bb5\u63a8\u8350\u52a8\u4f5c\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u6700\u65b0VLMs\uff0c\u6bd4\u8f83\u4e86\u5176\u5728DRAMA-X\u56db\u9879\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u610f\u56fe\u9884\u6d4b\u548c\u98ce\u9669\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u663e\u5f0f\u5efa\u6a21\u4e0a\u4e0b\u6587\u7ebf\u7d22\u65f6\u3002"}}
{"id": "2506.18044", "pdf": "https://arxiv.org/pdf/2506.18044", "abs": "https://arxiv.org/abs/2506.18044", "authors": ["Joseph Babb", "Joohyung Lee"], "title": "Action Language BC+", "categories": ["cs.AI"], "comment": "Journal of Logic and Computation, 2015", "summary": "Action languages are formal models of parts of natural language that are\ndesigned to describe effects of actions. Many of these languages can be viewed\nas high level notations of answer set programs structured to represent\ntransition systems. However, the form of answer set programs considered in the\nearlier work is quite limited in comparison with the modern Answer Set\nProgramming (ASP) language, which allows several useful constructs for\nknowledge representation, such as choice rules, aggregates, and abstract\nconstraint atoms. We propose a new action language called BC+, which closes the\ngap between action languages and the modern ASP language. The main idea is to\ndefine the semantics of BC+ in terms of general stable model semantics for\npropositional formulas, under which many modern ASP language constructs can be\nidentified with shorthands for propositional formulas. Language BC+ turns out\nto be sufficiently expressive to encompass the best features of other action\nlanguages, such as languages B, C, C+, and BC. Computational methods available\nin ASP solvers are readily applicable to compute BC+, which led to an\nimplementation of the language by extending system cplus2asp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u4f5c\u8bed\u8a00BC+\uff0c\u65e8\u5728\u5f25\u8865\u4f20\u7edf\u52a8\u4f5c\u8bed\u8a00\u4e0e\u73b0\u4ee3\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u5e7f\u4e49\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u77e5\u8bc6\u8868\u793a\u3002", "motivation": "\u4f20\u7edf\u52a8\u4f5c\u8bed\u8a00\u5728\u63cf\u8ff0\u52a8\u4f5c\u6548\u679c\u65f6\u529f\u80fd\u6709\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u4ee3ASP\u8bed\u8a00\u7684\u9ad8\u7ea7\u7279\u6027\uff08\u5982\u9009\u62e9\u89c4\u5219\u3001\u805a\u5408\u548c\u62bd\u8c61\u7ea6\u675f\u539f\u5b50\uff09\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u8bed\u8a00BC+\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "BC+\u57fa\u4e8e\u5e7f\u4e49\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\u5b9a\u4e49\uff0c\u5c06\u73b0\u4ee3ASP\u8bed\u8a00\u6784\u9020\u89c6\u4e3a\u547d\u9898\u516c\u5f0f\u7684\u7b80\u5199\u5f62\u5f0f\uff0c\u4ece\u800c\u652f\u6301\u66f4\u590d\u6742\u7684\u77e5\u8bc6\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u7cfb\u7edfcplus2asp\u5b9e\u73b0\u3002", "result": "BC+\u6210\u529f\u6574\u5408\u4e86\u5176\u4ed6\u52a8\u4f5c\u8bed\u8a00\uff08\u5982B\u3001C\u3001C+\u548cBC\uff09\u7684\u4f18\u826f\u7279\u6027\uff0c\u5e76\u901a\u8fc7ASP\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u3002", "conclusion": "BC+\u662f\u4e00\u79cd\u8868\u8fbe\u529b\u5f3a\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u52a8\u4f5c\u8bed\u8a00\uff0c\u4e3a\u52a8\u4f5c\u63cf\u8ff0\u548c\u77e5\u8bc6\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002", "paper_title_zh": "\u52a8\u4f5c\u8bed\u8a00BC+", "abstract_zh": "\u52a8\u4f5c\u8bed\u8a00\u662f\u81ea\u7136\u8bed\u8a00\u4e2d\u7528\u4e8e\u63cf\u8ff0\u52a8\u4f5c\u6548\u679c\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u3002\u8bb8\u591a\u52a8\u4f5c\u8bed\u8a00\u53ef\u89c6\u4e3a\u8868\u793a\u8fc7\u6e21\u7cfb\u7edf\u7684\u9ad8\u7ea7\u7b54\u6848\u96c6\u7a0b\u5e8f\u3002\u7136\u800c\uff0c\u65e9\u671f\u7814\u7a76\u4e2d\u7684\u7b54\u6848\u96c6\u7a0b\u5e8f\u5f62\u5f0f\u4e0e\u73b0\u4ee3\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u8bed\u8a00\u76f8\u6bd4\u529f\u80fd\u6709\u9650\uff0c\u540e\u8005\u652f\u6301\u9009\u62e9\u89c4\u5219\u3001\u805a\u5408\u548c\u62bd\u8c61\u7ea6\u675f\u539f\u5b50\u7b49\u77e5\u8bc6\u8868\u793a\u6784\u9020\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u52a8\u4f5c\u8bed\u8a00BC+\uff0c\u4ee5\u5f25\u8865\u52a8\u4f5c\u8bed\u8a00\u4e0e\u73b0\u4ee3ASP\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u57fa\u4e8e\u547d\u9898\u516c\u5f0f\u7684\u5e7f\u4e49\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\u5b9a\u4e49BC+\u7684\u8bed\u4e49\uff0c\u5c06\u73b0\u4ee3ASP\u8bed\u8a00\u6784\u9020\u89c6\u4e3a\u547d\u9898\u516c\u5f0f\u7684\u7b80\u5199\u3002BC+\u7684\u8868\u8fbe\u529b\u8db3\u4ee5\u6db5\u76d6\u5176\u4ed6\u52a8\u4f5c\u8bed\u8a00\uff08\u5982B\u3001C\u3001C+\u548cBC\uff09\u7684\u6700\u4f73\u7279\u6027\u3002\u901a\u8fc7\u6269\u5c55\u7cfb\u7edfcplus2asp\uff0c\u5b9e\u73b0\u4e86BC+\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u53ef\u76f4\u63a5\u5229\u7528ASP\u6c42\u89e3\u5668\u8fdb\u884c\u8ba1\u7b97\u3002"}}
{"id": "2506.17700", "pdf": "https://arxiv.org/pdf/2506.17700", "abs": "https://arxiv.org/abs/2506.17700", "authors": ["Summra Saleem", "Muhammad Nabeel Asim", "Shaista Zulfiqar", "Andreas Dengel"], "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing (NLP) by automating traditional labor-intensive tasks and\nconsequently accelerated the development of computer-aided applications. As\nresearchers continue to advance this field with the introduction of novel\nlanguage models and more efficient training/finetuning methodologies, the idea\nof prompt engineering and subsequent optimization strategies with LLMs has\nemerged as a particularly impactful trend to yield a substantial performance\nboost across diverse NLP tasks. To best of our knowledge numerous review\narticles have explored prompt engineering, however, a critical gap exists in\ncomprehensive analyses of prompt optimization strategies. To bridge this gap\nthis paper provides unique and comprehensive insights about the potential of\ndiverse prompt optimization strategies. It analyzes their underlying working\nparadigms and based on these principles, categorizes them into 11 distinct\nclasses. Moreover, the paper provides details about various NLP tasks where\nthese prompt optimization strategies have been employed, along with details of\ndifferent LLMs and benchmark datasets used for evaluation. This comprehensive\ncompilation lays a robust foundation for future comparative studies and enables\nrigorous assessment of prompt optimization and LLM-based predictive pipelines\nunder consistent experimental settings: a critical need in the current\nlandscape. Ultimately, this research will centralize diverse strategic\nknowledge to facilitate the adaptation of existing prompt optimization\nstrategies for development of innovative predictors across unexplored tasks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u63d0\u793a\u4f18\u5316\u7b56\u7565\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u5c06\u5176\u5206\u4e3a11\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u5173\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u7efc\u8ff0\u6587\u7ae0\uff0c\u4f46\u5bf9\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u5168\u9762\u5206\u6790\u4ecd\u5b58\u5728\u7a7a\u767d\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aNLP\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u4f18\u5316\u7b56\u7565\u77e5\u8bc6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u5c06\u5176\u5206\u4e3a11\u7c7b\uff0c\u5e76\u8be6\u7ec6\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u7b56\u7565\u5728\u4e0d\u540cNLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u793a\u4f18\u5316\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347NLP\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6bd4\u8f83\u7814\u7a76\u548c\u5b9e\u9a8c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3a\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u7cfb\u7edf\u5316\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u521b\u65b0\u9884\u6d4b\u6a21\u578b\u5728\u672a\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "paper_title_zh": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6f14\u8fdb\uff1a\u63d0\u793a\u4f18\u5316\u4e0e\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5851\u9020\u672a\u6765", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u7edf\u52b3\u52a8\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\uff0c\u5e76\u52a0\u901f\u4e86\u8ba1\u7b97\u673a\u8f85\u52a9\u5e94\u7528\u7684\u53d1\u5c55\u3002\u968f\u7740\u7814\u7a76\u4eba\u5458\u4e0d\u65ad\u5f15\u5165\u65b0\u9896\u7684\u8bed\u8a00\u6a21\u578b\u548c\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3/\u5fae\u8c03\u65b9\u6cd5\uff0c\u63d0\u793a\u5de5\u7a0b\u53ca\u5176\u4f18\u5316\u7b56\u7565\u5df2\u6210\u4e3a\u663e\u8457\u63d0\u5347NLP\u4efb\u52a1\u6027\u80fd\u7684\u91cd\u8981\u8d8b\u52bf\u3002\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u5173\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u7efc\u8ff0\u6587\u7ae0\uff0c\u4f46\u5bf9\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u5168\u9762\u5206\u6790\u4ecd\u5b58\u5728\u7a7a\u767d\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u591a\u79cd\u63d0\u793a\u4f18\u5316\u7b56\u7565\u6f5c\u529b\u7684\u72ec\u7279\u4e14\u5168\u9762\u7684\u89c1\u89e3\u3002\u6587\u7ae0\u5206\u6790\u4e86\u8fd9\u4e9b\u7b56\u7565\u7684\u57fa\u672c\u5de5\u4f5c\u539f\u7406\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u539f\u7406\u5c06\u5176\u5206\u4e3a11\u7c7b\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8fd9\u4e9b\u63d0\u793a\u4f18\u5316\u7b56\u7565\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bc4\u4f30\u7684\u4e0d\u540cLLM\u548c\u57fa\u51c6\u6570\u636e\u96c6\u3002\u8fd9\u4e00\u5168\u9762\u7684\u6c47\u7f16\u4e3a\u672a\u6765\u7684\u6bd4\u8f83\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5e76\u5728\u4e00\u81f4\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u63d0\u793a\u4f18\u5316\u548c\u57fa\u4e8eLLM\u7684\u9884\u6d4b\u6d41\u7a0b\u7684\u4e25\u683c\u8bc4\u4f30\uff1a\u8fd9\u662f\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5173\u952e\u9700\u6c42\u3002\u6700\u7ec8\uff0c\u672c\u7814\u7a76\u5c06\u96c6\u4e2d\u591a\u6837\u5316\u7684\u7b56\u7565\u77e5\u8bc6\uff0c\u4fc3\u8fdb\u73b0\u6709\u63d0\u793a\u4f18\u5316\u7b56\u7565\u5728\u672a\u63a2\u7d22\u4efb\u52a1\u4e2d\u521b\u65b0\u9884\u6d4b\u6a21\u578b\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.17592", "pdf": "https://arxiv.org/pdf/2506.17592", "abs": "https://arxiv.org/abs/2506.17592", "authors": ["Younghun Kim", "Minsuk Jang", "Myung-Joon Kwon", "Wonjun Lee", "Changick Kim"], "title": "SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Face identity provides a powerful signal for deepfake detection. Prior\nstudies show that even when not explicitly modeled, classifiers often learn\nidentity features implicitly. This has led to conflicting views: some suppress\nidentity cues to reduce bias, while others rely on them as forensic evidence.\nTo reconcile these views, we analyze two hypotheses: (1) whether face identity\nalone is discriminative for detecting deepfakes, and (2) whether such identity\nfeatures generalize poorly across manipulation methods. Our experiments confirm\nthat identity is informative but context-dependent. While some manipulations\npreserve identity-consistent artifacts, others distort identity cues and harm\ngeneralization. We argue that identity features should neither be blindly\nsuppressed nor relied upon, but instead be explicitly modeled and adaptively\ncontrolled based on per-sample relevance. We propose \\textbf{SELFI}\n(\\textbf{SEL}ective \\textbf{F}usion of \\textbf{I}dentity), a generalizable\ndetection framework that dynamically modulates identity usage. SELFI consists\nof: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity\nembeddings from a frozen face recognition model and projects them into a\nforgery-relevant space via auxiliary supervision; and (2) an Identity-Aware\nFusion Module (IAFM) that selectively integrates identity and visual features\nusing a relevance-guided fusion mechanism. Experiments on four benchmarks show\nthat SELFI improves cross-manipulation generalization, outperforming prior\nmethods by an average of 3.1\\% AUC. On the challenging DFDC dataset, SELFI\nexceeds the previous best by 6\\%. Code will be released upon paper acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSELFI\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u878d\u5408\u8eab\u4efd\u7279\u5f81\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u8de8\u64cd\u7eb5\u65b9\u6cd5\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u8eab\u4efd\u7279\u5f81\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\u5b58\u5728\u5206\u6b67\uff1a\u4e00\u4e9b\u8ba4\u4e3a\u5e94\u6291\u5236\u8eab\u4efd\u7279\u5f81\u4ee5\u51cf\u5c11\u504f\u5dee\uff0c\u53e6\u4e00\u4e9b\u5219\u4f9d\u8d56\u5176\u4f5c\u4e3a\u8bc1\u636e\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u8eab\u4efd\u7279\u5f81\u7684\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u8c03\u63a7\u8eab\u4efd\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSELFI\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a(1) Forgery-Aware Identity Adapter (FAIA)\uff0c\u4ece\u51bb\u7ed3\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e2d\u63d0\u53d6\u8eab\u4efd\u5d4c\u5165\u5e76\u901a\u8fc7\u8f85\u52a9\u76d1\u7763\u6295\u5f71\u81f3\u4f2a\u9020\u76f8\u5173\u7a7a\u95f4\uff1b(2) Identity-Aware Fusion Module (IAFM)\uff0c\u57fa\u4e8e\u76f8\u5173\u6027\u52a8\u6001\u878d\u5408\u8eab\u4efd\u548c\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSELFI\u5e73\u5747AUC\u63d0\u53473.1%\uff0c\u5728DFDC\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e4b\u524d\u6700\u4f73\u65b9\u6cd56%\u3002", "conclusion": "\u8eab\u4efd\u7279\u5f81\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5177\u6709\u4fe1\u606f\u91cf\u4f46\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0cSELFI\u901a\u8fc7\u52a8\u6001\u8c03\u63a7\u8eab\u4efd\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "SELFI\uff1a\u9009\u62e9\u6027\u8eab\u4efd\u878d\u5408\u7528\u4e8e\u53ef\u6cdb\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b", "abstract_zh": "\u4eba\u8138\u8eab\u4efd\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u4fe1\u53f7\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u672a\u663e\u5f0f\u5efa\u6a21\uff0c\u5206\u7c7b\u5668\u4e5f\u5e38\u9690\u5f0f\u5b66\u4e60\u8eab\u4efd\u7279\u5f81\u3002\u8fd9\u5bfc\u81f4\u4e86\u4e24\u79cd\u5bf9\u7acb\u89c2\u70b9\uff1a\u4e00\u4e9b\u7814\u7a76\u6291\u5236\u8eab\u4efd\u7ebf\u7d22\u4ee5\u51cf\u5c11\u504f\u5dee\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u4f9d\u8d56\u5176\u4f5c\u4e3a\u53d6\u8bc1\u8bc1\u636e\u3002\u4e3a\u8c03\u548c\u8fd9\u4e9b\u89c2\u70b9\uff0c\u6211\u4eec\u5206\u6790\u4e24\u4e2a\u5047\u8bbe\uff1a(1) \u8eab\u4efd\u7279\u5f81\u662f\u5426\u8db3\u4ee5\u5224\u522b\u6df1\u5ea6\u4f2a\u9020\uff1b(2) \u6b64\u7c7b\u7279\u5f81\u662f\u5426\u5728\u8de8\u64cd\u7eb5\u65b9\u6cd5\u4e2d\u6cdb\u5316\u6027\u5dee\u3002\u5b9e\u9a8c\u8bc1\u5b9e\u8eab\u4efd\u7279\u5f81\u5177\u6709\u4fe1\u606f\u91cf\u4f46\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff1a\u67d0\u4e9b\u64cd\u7eb5\u4fdd\u7559\u8eab\u4efd\u4e00\u81f4\u6027\u4f2a\u5f71\uff0c\u800c\u5176\u4ed6\u5219\u626d\u66f2\u8eab\u4efd\u7ebf\u7d22\u5e76\u635f\u5bb3\u6cdb\u5316\u3002\u6211\u4eec\u8ba4\u4e3a\u8eab\u4efd\u7279\u5f81\u65e2\u4e0d\u5e94\u76f2\u76ee\u6291\u5236\u4e5f\u4e0d\u5e94\u5b8c\u5168\u4f9d\u8d56\uff0c\u800c\u5e94\u663e\u5f0f\u5efa\u6a21\u5e76\u57fa\u4e8e\u6837\u672c\u76f8\u5173\u6027\u52a8\u6001\u8c03\u63a7\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faSELFI\uff08\u9009\u62e9\u6027\u8eab\u4efd\u878d\u5408\uff09\uff0c\u4e00\u79cd\u53ef\u6cdb\u5316\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u63a7\u8eab\u4efd\u7279\u5f81\u4f7f\u7528\u3002SELFI\u5305\u542b\uff1a(1) Forgery-Aware Identity Adapter (FAIA)\uff0c\u4ece\u51bb\u7ed3\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e2d\u63d0\u53d6\u8eab\u4efd\u5d4c\u5165\u5e76\u901a\u8fc7\u8f85\u52a9\u76d1\u7763\u6295\u5f71\u81f3\u4f2a\u9020\u76f8\u5173\u7a7a\u95f4\uff1b(2) Identity-Aware Fusion Module (IAFM)\uff0c\u57fa\u4e8e\u76f8\u5173\u6027\u9009\u62e9\u6027\u878d\u5408\u8eab\u4efd\u548c\u89c6\u89c9\u7279\u5f81\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSELFI\u5e73\u5747AUC\u63d0\u53473.1%\uff0c\u5728DFDC\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e4b\u524d\u6700\u4f73\u65b9\u6cd56%\u3002\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2506.18056", "pdf": "https://arxiv.org/pdf/2506.18056", "abs": "https://arxiv.org/abs/2506.18056", "authors": ["Paolo Baldi", "Fabio Aurelio D'Asaro", "Abeer Dyoub", "Francesca Alessandra Lisi"], "title": "Weighted Assumption Based Argumentation to reason about ethical principles and actions", "categories": ["cs.AI"], "comment": null, "summary": "We augment Assumption Based Argumentation (ABA for short) with weighted\nargumentation. In a nutshell, we assign weights to arguments and then derive\nthe weight of attacks between ABA arguments. We illustrate our proposal through\nrunning examples in the field of ethical reasoning, and present an\nimplementation based on Answer Set Programming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u5047\u8bbe\u7684\u8bba\u8bc1\u65b9\u6cd5\uff08ABA\uff09\uff0c\u901a\u8fc7\u4e3a\u8bba\u8bc1\u5206\u914d\u6743\u91cd\u5e76\u63a8\u5bfc\u653b\u51fb\u6743\u91cd\uff0c\u5e94\u7528\u4e8e\u4f26\u7406\u63a8\u7406\u9886\u57df\uff0c\u5e76\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6269\u5c55\u5047\u8bbe\u57fa\u4e8e\u8bba\u8bc1\uff08ABA\uff09\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u6743\u91cd\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u4f26\u7406\u539f\u5219\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u590d\u6742\u63a8\u7406\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e3aABA\u4e2d\u7684\u8bba\u8bc1\u5206\u914d\u6743\u91cd\uff0c\u5e76\u63a8\u5bfc\u8bba\u8bc1\u4e4b\u95f4\u7684\u653b\u51fb\u6743\u91cd\u3002\u901a\u8fc7\u4f26\u7406\u63a8\u7406\u7684\u5b9e\u9645\u6848\u4f8b\u8fdb\u884c\u8bf4\u660e\uff0c\u5e76\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\u8be5\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u52a0\u6743ABA\u80fd\u591f\u6709\u6548\u652f\u6301\u4f26\u7406\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u52a0\u6743ABA\u4e3a\u4f26\u7406\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u5177\uff0c\u5176\u5b9e\u73b0\u65b9\u6cd5\u5177\u6709\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u52a0\u6743\u5047\u8bbe\u7684\u8bba\u8bc1\u65b9\u6cd5\u7528\u4e8e\u4f26\u7406\u539f\u5219\u4e0e\u884c\u4e3a\u63a8\u7406", "abstract_zh": "\u6211\u4eec\u901a\u8fc7\u52a0\u6743\u8bba\u8bc1\u6269\u5c55\u4e86\u5047\u8bbe\u57fa\u4e8e\u8bba\u8bc1\uff08ABA\uff09\u3002\u7b80\u800c\u8a00\u4e4b\uff0c\u6211\u4eec\u4e3a\u8bba\u8bc1\u5206\u914d\u6743\u91cd\uff0c\u5e76\u63a8\u5bfcABA\u8bba\u8bc1\u4e4b\u95f4\u7684\u653b\u51fb\u6743\u91cd\u3002\u901a\u8fc7\u4f26\u7406\u63a8\u7406\u9886\u57df\u7684\u5b9e\u4f8b\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\u4e86\u8be5\u65b9\u6848\u3002"}}
{"id": "2506.17708", "pdf": "https://arxiv.org/pdf/2506.17708", "abs": "https://arxiv.org/abs/2506.17708", "authors": ["MingZe Tang"], "title": "Aged to Perfection: Machine-Learning Maps of Age in Conversational English", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 11 figures", "summary": "The study uses the British National Corpus 2014, a large sample of\ncontemporary spoken British English, to investigate language patterns across\ndifferent age groups. Our research attempts to explore how language patterns\nvary between different age groups, exploring the connection between speaker\ndemographics and linguistic factors such as utterance duration, lexical\ndiversity, and word choice. By merging computational language analysis and\nmachine learning methodologies, we attempt to uncover distinctive linguistic\nmarkers characteristic of multiple generations and create prediction models\nthat can consistently estimate the speaker's age group from various aspects.\nThis work contributes to our knowledge of sociolinguistic diversity throughout\nthe life of modern British speech.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u82f1\u56fd\u56fd\u5bb6\u8bed\u6599\u5e932014\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u8bed\u8a00\u5206\u6790\uff0c\u63a2\u7d22\u4e0d\u540c\u5e74\u9f84\u6bb5\u82f1\u8bed\u4f7f\u7528\u8005\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u4ee5\u4f30\u8ba1\u8bf4\u8bdd\u8005\u7684\u5e74\u9f84\u7ec4\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4e0d\u540c\u5e74\u9f84\u6bb5\u8bf4\u8bdd\u8005\u5728\u8bed\u8a00\u4f7f\u7528\u4e0a\u7684\u5dee\u5f02\uff0c\u63a2\u7d22\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u4e0e\u8bed\u8a00\u56e0\u7d20\uff08\u5982\u8bdd\u8bed\u65f6\u957f\u3001\u8bcd\u6c47\u591a\u6837\u6027\u548c\u8bcd\u6c47\u9009\u62e9\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u82f1\u56fd\u56fd\u5bb6\u8bed\u6599\u5e932014\u7684\u5927\u89c4\u6a21\u5f53\u4ee3\u82f1\u56fd\u53e3\u8bed\u6837\u672c\uff0c\u7ed3\u5408\u8ba1\u7b97\u8bed\u8a00\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u5e74\u9f84\u7ec4\u7684\u8bed\u8a00\u6a21\u5f0f\uff0c\u5e76\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u5e74\u9f84\u7ec4\u5728\u8bed\u8a00\u4f7f\u7528\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u6839\u636e\u8bed\u8a00\u7279\u5f81\u9884\u6d4b\u8bf4\u8bdd\u8005\u5e74\u9f84\u7ec4\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u73b0\u4ee3\u82f1\u56fd\u53e3\u8bed\u7684\u793e\u4f1a\u8bed\u8a00\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u8bed\u8a00\u5e74\u9f84\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u5b8c\u7f8e\u5e74\u9f84\uff1a\u673a\u5668\u5b66\u4e60\u5bf9\u82f1\u8bed\u4f1a\u8bdd\u4e2d\u5e74\u9f84\u7684\u6620\u5c04", "abstract_zh": "\u672c\u7814\u7a76\u5229\u7528\u82f1\u56fd\u56fd\u5bb6\u8bed\u6599\u5e932014\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5f53\u4ee3\u82f1\u56fd\u53e3\u8bed\u6837\u672c\uff0c\u63a2\u8ba8\u4e0d\u540c\u5e74\u9f84\u7ec4\u7684\u8bed\u8a00\u6a21\u5f0f\u3002\u7814\u7a76\u8bd5\u56fe\u63ed\u793a\u4e0d\u540c\u5e74\u9f84\u7ec4\u4e4b\u95f4\u8bed\u8a00\u6a21\u5f0f\u7684\u5dee\u5f02\uff0c\u63a2\u7d22\u8bf4\u8bdd\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u4e0e\u8bed\u8a00\u56e0\u7d20\uff08\u5982\u8bdd\u8bed\u65f6\u957f\u3001\u8bcd\u6c47\u591a\u6837\u6027\u548c\u8bcd\u6c47\u9009\u62e9\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u901a\u8fc7\u7ed3\u5408\u8ba1\u7b97\u8bed\u8a00\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u6211\u4eec\u8bd5\u56fe\u63ed\u793a\u591a\u4ee3\u4eba\u7279\u6709\u7684\u8bed\u8a00\u6807\u8bb0\uff0c\u5e76\u6784\u5efa\u80fd\u591f\u4ece\u591a\u4e2a\u65b9\u9762\u4e00\u81f4\u4f30\u8ba1\u8bf4\u8bdd\u8005\u5e74\u9f84\u7ec4\u7684\u9884\u6d4b\u6a21\u578b\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6211\u4eec\u7406\u89e3\u73b0\u4ee3\u82f1\u56fd\u53e3\u8bed\u5728\u793e\u4f1a\u8bed\u8a00\u5b66\u591a\u6837\u6027\u65b9\u9762\u7684\u77e5\u8bc6\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2506.17596", "pdf": "https://arxiv.org/pdf/2506.17596", "abs": "https://arxiv.org/abs/2506.17596", "authors": ["Wei Huang", "Yinxuan Xu", "Yintao Zhou", "Zhengyu Li", "Jing Huang", "Meng Pang"], "title": "A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data", "categories": ["cs.CV"], "comment": "8 pages, 4 figures, accepted by CogSci 2025", "summary": "Parkinson's disease (PD), characterized by its incurable nature, rapid\nprogression, and severe disability, poses significant challenges to the lives\nof patients and their families. Given the aging population, the need for early\ndetection of PD is increasing. In vitro diagnosis has garnered attention due to\nits non-invasive nature and low cost. However, existing methods present several\nchallenges: 1) limited training data for facial expression diagnosis; 2)\nspecialized equipment and acquisition environments required for gait diagnosis,\nresulting in poor generalizability; 3) the risk of misdiagnosis or missed\ndiagnosis when relying on a single modality. To address these issues, we\npropose a novel multimodal in vitro diagnostic method for PD, leveraging facial\nexpressions and behavioral gait. Our method employs a lightweight deep learning\nmodel for feature extraction and fusion, aimed at improving diagnostic accuracy\nand facilitating deployment on mobile devices. Furthermore, we have established\nthe largest multimodal PD dataset in collaboration with a hospital and\nconducted extensive experiments to validate the effectiveness of our proposed\nmethod.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9762\u90e8\u8868\u60c5\u548c\u884c\u4e3a\u6b65\u6001\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u7684\u65e9\u671f\u68c0\u6d4b\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u548c\u878d\u5408\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u6700\u5927\u7684\u591a\u6a21\u6001PD\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u5177\u6709\u4e0d\u53ef\u6cbb\u6108\u6027\u3001\u5feb\u901f\u8fdb\u5c55\u548c\u4e25\u91cd\u81f4\u6b8b\u6027\uff0c\u5bf9\u60a3\u8005\u53ca\u5176\u5bb6\u5ead\u751f\u6d3b\u9020\u6210\u5de8\u5927\u6311\u6218\u3002\u968f\u7740\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u65e9\u671f\u68c0\u6d4bPD\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002\u73b0\u6709\u7684\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u3001\u8bbe\u5907\u8981\u6c42\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u5355\u6a21\u6001\u6613\u8bef\u8bca\u6216\u6f0f\u8bca\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9762\u90e8\u8868\u60c5\u548c\u884c\u4e3a\u6b65\u6001\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\uff0c\u4ee5\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u5e76\u4fbf\u4e8e\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u540c\u65f6\uff0c\u4e0e\u533b\u9662\u5408\u4f5c\u5efa\u7acb\u4e86\u6700\u5927\u7684\u591a\u6a21\u6001PD\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aPD\u7684\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u975e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u4e00\u79cd\u7ed3\u5408\u9762\u90e8\u8868\u60c5\u548c\u884c\u4e3a\u6b65\u6001\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5", "abstract_zh": "\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u4ee5\u5176\u4e0d\u53ef\u6cbb\u6108\u6027\u3001\u5feb\u901f\u8fdb\u5c55\u548c\u4e25\u91cd\u81f4\u6b8b\u6027\u4e3a\u7279\u5f81\uff0c\u5bf9\u60a3\u8005\u53ca\u5176\u5bb6\u5ead\u751f\u6d3b\u9020\u6210\u5de8\u5927\u6311\u6218\u3002\u968f\u7740\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u65e9\u671f\u68c0\u6d4bPD\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002\u4f53\u5916\u8bca\u65ad\u56e0\u5176\u975e\u4fb5\u5165\u6027\u548c\u4f4e\u6210\u672c\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1\uff09\u9762\u90e8\u8868\u60c5\u8bca\u65ad\u7684\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff1b2\uff09\u6b65\u6001\u8bca\u65ad\u9700\u8981\u4e13\u7528\u8bbe\u5907\u548c\u91c7\u96c6\u73af\u5883\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff1b3\uff09\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u6613\u5bfc\u81f4\u8bef\u8bca\u6216\u6f0f\u8bca\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9762\u90e8\u8868\u60c5\u548c\u884c\u4e3a\u6b65\u6001\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\uff0c\u65e8\u5728\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u5e76\u4fbf\u4e8e\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e0e\u533b\u9662\u5408\u4f5c\u5efa\u7acb\u4e86\u6700\u5927\u7684\u591a\u6a21\u6001PD\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18096", "pdf": "https://arxiv.org/pdf/2506.18096", "abs": "https://arxiv.org/abs/2506.18096", "authors": ["Yuxuan Huang", "Yihang Chen", "Haozheng Zhang", "Kang Li", "Meng Fang", "Linyi Yang", "Xiaoguang Li", "Lifeng Shang", "Songcen Xu", "Jianye Hao", "Kun Shao", "Jun Wang"], "title": "Deep Research Agents: A Systematic Examination And Roadmap", "categories": ["cs.AI"], "comment": null, "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6df1\u5ea6\u7814\u7a76\uff08DR\uff09\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u6280\u672f\u3001\u67b6\u6784\u7ec4\u4ef6\u53ca\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u9759\u6001\u4e0e\u52a8\u6001\u5de5\u4f5c\u6d41\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5f53\u524d\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\uff08DR agents\uff09\u6210\u4e3a\u89e3\u51b3\u590d\u6742\u591a\u8f6e\u4fe1\u606f\u7814\u7a76\u4efb\u52a1\u7684\u65b0\u5174\u5de5\u5177\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u5176\u6838\u5fc3\u6280\u672f\u3001\u67b6\u6784\u53ca\u5206\u7c7b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u6587\u7ae0\u9996\u5148\u5bf9\u6bd4\u4e86\u57fa\u4e8eAPI\u548c\u6d4f\u89c8\u5668\u7684\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\uff0c\u63a5\u7740\u63a2\u8ba8\u4e86\u6a21\u5757\u5316\u5de5\u5177\u4f7f\u7528\u6846\u67b6\uff08\u5982\u4ee3\u7801\u6267\u884c\u3001\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\u7b49\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9759\u6001\u4e0e\u52a8\u6001\u5de5\u4f5c\u6d41\u7684\u5206\u7c7b\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u6839\u636e\u89c4\u5212\u7b56\u7565\u548c\u667a\u80fd\u4f53\u7ec4\u6210\uff08\u5355\u667a\u80fd\u4f53\u4e0e\u591a\u667a\u80fd\u4f53\uff09\u5bf9\u67b6\u6784\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86DR\u667a\u80fd\u4f53\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5982\u5916\u90e8\u77e5\u8bc6\u8bbf\u95ee\u53d7\u9650\u3001\u987a\u5e8f\u6267\u884c\u6548\u7387\u4f4e\u53ca\u8bc4\u4f30\u6307\u6807\u4e0e\u5b9e\u9645\u76ee\u6807\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u603b\u7ed3\u4e86DR\u667a\u80fd\u4f53\u7684\u6280\u672f\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\u4e0e\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u7814\u7a76\u7684\u6301\u7eed\u66f4\u65b0\u8d44\u6e90\u5e93\u3002", "paper_title_zh": "\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\uff1a\u7cfb\u7edf\u5316\u5ba1\u89c6\u4e0e\u8def\u7ebf\u56fe", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u50ac\u751f\u4e86\u4e00\u7c7b\u65b0\u578b\u81ea\u4e3bAI\u7cfb\u7edf\uff0c\u79f0\u4e3a\u6df1\u5ea6\u7814\u7a76\uff08DR\uff09\u667a\u80fd\u4f53\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u901a\u8fc7\u52a8\u6001\u63a8\u7406\u3001\u81ea\u9002\u5e94\u957f\u7a0b\u89c4\u5212\u3001\u591a\u8df3\u4fe1\u606f\u68c0\u7d22\u3001\u8fed\u4ee3\u5de5\u5177\u4f7f\u7528\u4ee5\u53ca\u7ed3\u6784\u5316\u5206\u6790\u62a5\u544a\u751f\u6210\uff0c\u65e8\u5728\u89e3\u51b3\u590d\u6742\u7684\u591a\u8f6e\u4fe1\u606f\u7814\u7a76\u4efb\u52a1\u3002\u672c\u6587\u8be6\u7ec6\u5206\u6790\u4e86\u6784\u6210\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u57fa\u7840\u6280\u672f\u4e0e\u67b6\u6784\u7ec4\u4ef6\u3002\u9996\u5148\u56de\u987e\u4e86\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\uff0c\u5bf9\u6bd4\u4e86\u57fa\u4e8eAPI\u7684\u68c0\u7d22\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u63a2\u7d22\u3002\u63a5\u7740\u63a2\u8ba8\u4e86\u6a21\u5757\u5316\u5de5\u5177\u4f7f\u7528\u6846\u67b6\uff0c\u5305\u62ec\u4ee3\u7801\u6267\u884c\u3001\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\u4ee5\u53ca\u652f\u6301\u53ef\u6269\u5c55\u6027\u548c\u751f\u6001\u7cfb\u7edf\u5f00\u53d1\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCPs\uff09\u96c6\u6210\u3002\u4e3a\u7cfb\u7edf\u5316\u73b0\u6709\u65b9\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u533a\u5206\u9759\u6001\u4e0e\u52a8\u6001\u5de5\u4f5c\u6d41\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6839\u636e\u89c4\u5212\u7b56\u7565\u548c\u667a\u80fd\u4f53\u7ec4\u6210\uff08\u5355\u667a\u80fd\u4f53\u4e0e\u591a\u667a\u80fd\u4f53\uff09\u5bf9\u67b6\u6784\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9\u5f53\u524d\u57fa\u51c6\u8fdb\u884c\u4e86\u6279\u5224\u6027\u8bc4\u4f30\uff0c\u6307\u51fa\u4e86\u5173\u952e\u5c40\u9650\u6027\uff0c\u5982\u5916\u90e8\u77e5\u8bc6\u8bbf\u95ee\u53d7\u9650\u3001\u987a\u5e8f\u6267\u884c\u6548\u7387\u4f4e\u4ee5\u53ca\u8bc4\u4f30\u6307\u6807\u4e0eDR\u667a\u80fd\u4f53\u5b9e\u9645\u76ee\u6807\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002\u6700\u540e\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\u4e0e\u65b9\u5411\u3002\u76f8\u5173\u7814\u7a76\u7684\u7cbe\u9009\u4e0e\u6301\u7eed\u66f4\u65b0\u8d44\u6e90\u5e93\u53ef\u5728\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\uff1a{https://github.com/ai-agents-2030/awesome-deep-research-agent}\u3002"}}
{"id": "2506.17715", "pdf": "https://arxiv.org/pdf/2506.17715", "abs": "https://arxiv.org/abs/2506.17715", "authors": ["Matthias Sch\u00f6ffel", "Esteban Garces Arias", "Marinus Wiedner", "Paula Ruppert", "Meimingwei Li", "Christian Heumann", "Matthias A\u00dfenmacher"], "title": "Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Part-of-speech (POS) tagging remains a foundational component in natural\nlanguage processing pipelines, particularly critical for historical text\nanalysis at the intersection of computational linguistics and digital\nhumanities. Despite significant advancements in modern large language models\n(LLMs) for ancient languages, their application to Medieval Romance languages\npresents distinctive challenges stemming from diachronic linguistic evolution,\nspelling variations, and labeled data scarcity. This study systematically\ninvestigates the central determinants of POS tagging performance across diverse\ncorpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,\nspanning biblical, hagiographical, medical, and dietary domains. Through\nrigorous experimentation, we evaluate how fine-tuning approaches, prompt\nengineering, model architectures, decoding strategies, and cross-lingual\ntransfer learning techniques affect tagging accuracy. Our results reveal both\nnotable limitations in LLMs' ability to process historical language variations\nand non-standardized spelling, as well as promising specialized techniques that\neffectively address the unique challenges presented by low-resource historical\nlanguages.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f4e\u8d44\u6e90\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\uff08\u5982\u4e2d\u4e16\u7eaa\u5965\u514b\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u6cd5\u8bed\uff09\u7684\u8bcd\u6027\u6807\u6ce8\u6027\u80fd\u5173\u952e\u56e0\u7d20\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6709\u6548\u7684\u4e13\u95e8\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u8bed\u8a00\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\u65f6\u4ecd\u9762\u4e34\u5386\u65f6\u8bed\u8a00\u6f14\u53d8\u3001\u62fc\u5199\u53d8\u4f53\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7b49\u72ec\u7279\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u6311\u6218\u5e76\u63a2\u7d22\u63d0\u5347\u8bcd\u6027\u6807\u6ce8\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u5fae\u8c03\u65b9\u6cd5\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u67b6\u6784\u3001\u89e3\u7801\u7b56\u7565\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u5bf9\u4e2d\u4e16\u7eaa\u5965\u514b\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u6cd5\u8bed\u6587\u672c\u8bcd\u6027\u6807\u6ce8\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4f46\u4e5f\u53d1\u73b0\u4e86\u4e00\u4e9b\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u6311\u6218\u7684\u4e13\u95e8\u6280\u672f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u7684\u4e13\u95e8\u6280\u672f\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\u7684\u8bcd\u6027\u6807\u6ce8\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "\u63ed\u793a\u589e\u5f3a\u8bcd\u6027\u6807\u6ce8\u7684\u5173\u952e\u56e0\u7d20\uff1a\u4f4e\u8d44\u6e90\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\u7814\u7a76", "abstract_zh": "\u8bcd\u6027\u6807\u6ce8\uff08POS\uff09\u4ecd\u7136\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6d41\u7a0b\u4e2d\u7684\u57fa\u7840\u7ec4\u6210\u90e8\u5206\uff0c\u5c24\u5176\u5bf9\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e0e\u6570\u5b57\u4eba\u6587\u4ea4\u53c9\u9886\u57df\u7684\u5386\u53f2\u6587\u672c\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53e4\u8bed\u8a00\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u5386\u65f6\u8bed\u8a00\u6f14\u53d8\u3001\u62fc\u5199\u53d8\u4f53\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7b49\u72ec\u7279\u6311\u6218\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u4e2d\u4e16\u7eaa\u5965\u514b\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u6cd5\u8bed\u6587\u672c\u4e2d\u8bcd\u6027\u6807\u6ce8\u6027\u80fd\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\uff0c\u6db5\u76d6\u5723\u7ecf\u3001\u5723\u5f92\u4f20\u8bb0\u3001\u533b\u5b66\u548c\u996e\u98df\u7b49\u591a\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u4e25\u683c\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5fae\u8c03\u65b9\u6cd5\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u67b6\u6784\u3001\u89e3\u7801\u7b56\u7565\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u5bf9\u6807\u6ce8\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4f46\u4e5f\u53d1\u73b0\u4e86\u4e00\u4e9b\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u6311\u6218\u7684\u4e13\u95e8\u6280\u672f\u3002"}}
{"id": "2506.17597", "pdf": "https://arxiv.org/pdf/2506.17597", "abs": "https://arxiv.org/abs/2506.17597", "authors": ["Pengyu Kan", "Craig Jones", "Kenichi Oishi"], "title": "OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: To develop an age prediction model which is interpretable and robust\nto demographic and technological variances in brain MRI scans. Materials and\nMethods: We propose a transformer-based architecture that leverages\nself-supervised pre-training on large-scale datasets. Our model processes\npseudo-3D T1-weighted MRI scans from three anatomical views and incorporates\nbrain volumetric information. By introducing a stem architecture, we reduce the\nconventional quadratic complexity of transformer models to linear complexity,\nenabling scalability for high-dimensional MRI data. We trained our model on\nADNI2 $\\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the\nNorth America, with an 8:1:1 split for train, validation and test. Then, we\nvalidated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.\nResults: We achieved an MAE of 3.65 years on ADNI2 $\\&$ 3 and OASIS3 test set\nand a high generalizability of MAE of 3.54 years on AIBL. There was a notable\nincrease in brain age gap (BAG) across cognitive groups, with mean of 0.15\nyears (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12\nyears ([5.82, 6.43]) in AD. Additionally, significant negative correlation\nbetween BAG and cognitive scores was observed, with correlation coefficient of\n-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based\nfeature attribution highlighted ventricles and white matter structures as key\nregions influenced by brain aging. Conclusion: Our model effectively fused\ninformation from different views and volumetric information to achieve\nstate-of-the-art brain age prediction accuracy, improved generalizability and\ninterpretability with association to neurodegenerative disorders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u8111\u9f84\u9884\u6d4b\u6a21\u578bOpenMAP-BrainAge\uff0c\u901a\u8fc7\u591a\u89c6\u89d2MRI\u626b\u63cf\u548c\u4f53\u79ef\u4fe1\u606f\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u89e3\u91ca\u4e14\u5bf9\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u6280\u672f\u5dee\u5f02\u5177\u6709\u9c81\u68d2\u6027\u7684\u8111\u9f84\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8111MRI\u626b\u63cf\u7684\u5e74\u9f84\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u591a\u89c6\u89d2\u4f2a3D T1\u52a0\u6743MRI\u626b\u63cf\uff0c\u5f15\u5165\u830e\u67b6\u6784\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728ADNI2 & 3\u548cOASIS3\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728AIBL\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5728ADNI2 & 3\u548cOASIS3\u6d4b\u8bd5\u96c6\u4e0aMAE\u4e3a3.65\u5e74\uff0c\u5728AIBL\u6570\u636e\u96c6\u4e0aMAE\u4e3a3.54\u5e74\uff1b\u8111\u9f84\u5dee\uff08BAG\uff09\u968f\u8ba4\u77e5\u969c\u788d\u7a0b\u5ea6\u589e\u52a0\u800c\u663e\u8457\u4e0a\u5347\uff0c\u4e14\u4e0e\u8ba4\u77e5\u8bc4\u5206\u5448\u663e\u8457\u8d1f\u76f8\u5173\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u591a\u89c6\u89d2\u548c\u4f53\u79ef\u4fe1\u606f\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8111\u9f84\u9884\u6d4b\uff0c\u5e76\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e0e\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u5173\u8054\u663e\u8457\u3002", "paper_title_zh": "OpenMAP-BrainAge\uff1a\u4e00\u79cd\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u8111\u9f84\u9884\u6d4b\u6a21\u578b", "abstract_zh": "\u76ee\u7684\uff1a\u5f00\u53d1\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u5bf9\u8111MRI\u626b\u63cf\u4e2d\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u6280\u672f\u5dee\u5f02\u5177\u6709\u9c81\u68d2\u6027\u7684\u5e74\u9f84\u9884\u6d4b\u6a21\u578b\u3002\u6750\u6599\u4e0e\u65b9\u6cd5\uff1a\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002\u6a21\u578b\u5904\u7406\u6765\u81ea\u4e09\u4e2a\u89e3\u5256\u89c6\u89d2\u7684\u4f2a3D T1\u52a0\u6743MRI\u626b\u63cf\uff0c\u5e76\u6574\u5408\u8111\u4f53\u79ef\u4fe1\u606f\u3002\u901a\u8fc7\u5f15\u5165\u830e\u67b6\u6784\uff0c\u5c06\u4f20\u7edfTransformer\u6a21\u578b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u964d\u81f3\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u9002\u5e94\u9ad8\u7ef4MRI\u6570\u636e\u3002\u6211\u4eec\u5728\u5317\u7f8e\u5730\u533a\u7684ADNI2 & 3\uff08N=1348\uff09\u548cOASIS3\uff08N=716\uff09\u6570\u636e\u96c6\uff08\u5e74\u9f84\u8303\u56f4\uff1a42-95\u5c81\uff09\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u63098:1:1\u5212\u5206\u4e3a\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u5728\u6fb3\u5927\u5229\u4e9a\u7684AIBL\u6570\u636e\u96c6\uff08N=768\uff0c\u5e74\u9f84\u8303\u56f4\uff1a60-92\u5c81\uff09\u4e0a\u9a8c\u8bc1\u3002\u7ed3\u679c\uff1a\u5728ADNI2 & 3\u548cOASIS3\u6d4b\u8bd5\u96c6\u4e0aMAE\u4e3a3.65\u5e74\uff0c\u5728AIBL\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u5f3a\u7684MAE\u4e3a3.54\u5e74\u3002\u8111\u9f84\u5dee\uff08BAG\uff09\u5728\u4e0d\u540c\u8ba4\u77e5\u7ec4\u4e2d\u663e\u8457\u589e\u52a0\uff0cCN\u7ec4\u5e73\u5747\u4e3a0.15\u5e74\uff0895% CI: [-0.22, 0.51]\uff09\uff0cMCI\u7ec4\u4e3a2.55\u5e74\uff08[2.40, 2.70]\uff09\uff0cAD\u7ec4\u4e3a6.12\u5e74\uff08[5.82, 6.43]\uff09\u3002\u6b64\u5916\uff0cBAG\u4e0e\u8ba4\u77e5\u8bc4\u5206\u5448\u663e\u8457\u8d1f\u76f8\u5173\uff0cMoCA\u548cMMSE\u7684\u76f8\u5173\u7cfb\u6570\u5206\u522b\u4e3a-0.185\uff08p < 0.001\uff09\u548c-0.231\uff08p < 0.001\uff09\u3002\u57fa\u4e8e\u68af\u5ea6\u7684\u7279\u5f81\u5f52\u56e0\u663e\u793a\uff0c\u8111\u5ba4\u548c\u767d\u8d28\u7ed3\u6784\u662f\u53d7\u8111\u8001\u5316\u5f71\u54cd\u7684\u5173\u952e\u533a\u57df\u3002\u7ed3\u8bba\uff1a\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u878d\u5408\u591a\u89c6\u89d2\u548c\u4f53\u79ef\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8111\u9f84\u9884\u6d4b\u7cbe\u5ea6\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e0e\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u5173\u8054\u663e\u8457\u3002"}}
{"id": "2506.18126", "pdf": "https://arxiv.org/pdf/2506.18126", "abs": "https://arxiv.org/abs/2506.18126", "authors": ["Xiang Yuming", "Li Sizhao", "Li Rongpeng", "Zhao Zhifeng", "Zhang Honggang"], "title": "Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game", "categories": ["cs.AI"], "comment": null, "summary": "Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered\nwidespread research interest and fostered tremendous interesting applications,\nespecially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative\nEvasion and Formation Coverage (CEFC) task, where the UAV swarm aims to\nmaximize formation coverage across multiple target zones while collaboratively\nevading predators, belongs to one of the most challenging issues in MC-PEG,\nespecially under communication-limited constraints. This multifaceted problem,\nwhich intertwines responses to obstacles, adversaries, target zones, and\nformation dynamics, brings up significant high-dimensional complications in\nlocating a solution. In this paper, we propose a novel two-level framework\n(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),\nwhich delegates target localization to a high-level policy, while adopting a\nlow-level policy to manage obstacle avoidance, navigation, and formation.\nSpecifically, in the high-level policy, we develop a novel multi-agent\nreinforcement learning module, Consensus-oriented Multi-Agent Communication\n(ConsMAC), to enable agents to perceive global information and establish\nconsensus from local states by effectively aggregating neighbor messages.\nMeanwhile, we leverage an Alternative Training-based Multi-agent proximal\npolicy optimization (AT-M) and policy distillation to accomplish the low-level\ncontrol. The experimental results, including the high-fidelity\nsoftware-in-the-loop (SITL) simulations, validate that CI-HRL provides a\nsuperior solution with enhanced swarm's collaborative evasion and task\ncompletion capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u8bc6\u63a8\u65ad\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08CI-HRL\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u7ea6\u675f\u65e0\u4eba\u673a\u8ffd\u9003\u6e38\u620f\u4e2d\u7684\u9ad8\u7ef4\u590d\u6742\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u4f4e\u5c42\u7b56\u7565\u5206\u5de5\u534f\u4f5c\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u7fa4\u7684\u534f\u540c\u9003\u9038\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "motivation": "\u591a\u7ea6\u675f\u65e0\u4eba\u673a\u8ffd\u9003\u6e38\u620f\uff08MC-PEG\uff09\u4e2d\u7684\u534f\u540c\u9003\u9038\u4e0e\u7f16\u961f\u8986\u76d6\u4efb\u52a1\uff08CEFC\uff09\u5728\u901a\u4fe1\u53d7\u9650\u6761\u4ef6\u4e0b\u6781\u5177\u6311\u6218\u6027\uff0c\u6d89\u53ca\u969c\u788d\u7269\u3001\u654c\u624b\u3001\u76ee\u6807\u533a\u57df\u548c\u7f16\u961f\u52a8\u6001\u7b49\u591a\u7ef4\u590d\u6742\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCI-HRL\u6846\u67b6\uff0c\u9ad8\u5c42\u7b56\u7565\u91c7\u7528\u5171\u8bc6\u5bfc\u5411\u591a\u667a\u80fd\u4f53\u901a\u4fe1\uff08ConsMAC\uff09\u5b9e\u73b0\u5168\u5c40\u611f\u77e5\u4e0e\u5171\u8bc6\u5efa\u7acb\uff0c\u4f4e\u5c42\u7b56\u7565\u7ed3\u5408\u4ea4\u66ff\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08AT-M\uff09\u548c\u7b56\u7565\u84b8\u998f\uff0c\u5b8c\u6210\u907f\u969c\u3001\u5bfc\u822a\u4e0e\u7f16\u961f\u63a7\u5236\u3002", "result": "\u9ad8\u4fdd\u771f\u8f6f\u4ef6\u5728\u73af\uff08SITL\uff09\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cCI-HRL\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7fa4\u7684\u534f\u540c\u9003\u9038\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "conclusion": "CI-HRL\u4e3a\u591a\u7ea6\u675f\u65e0\u4eba\u673a\u8ffd\u9003\u6e38\u620f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u548c\u5171\u8bc6\u63a8\u65ad\u673a\u5236\uff0c\u6709\u6548\u5e94\u5bf9\u9ad8\u7ef4\u590d\u6742\u95ee\u9898\u3002", "paper_title_zh": "\u57fa\u4e8e\u5171\u8bc6\u63a8\u65ad\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u7ea6\u675f\u65e0\u4eba\u673a\u8ffd\u9003\u6e38\u620f\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\uff08UAV\uff09\u7cfb\u7edf\u5728\u591a\u7ea6\u675f\u8ffd\u9003\u6e38\u620f\uff08MC-PEG\uff09\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u5176\u4e2d\u534f\u540c\u9003\u9038\u4e0e\u7f16\u961f\u8986\u76d6\u4efb\u52a1\uff08CEFC\uff09\u5c24\u4e3a\u590d\u6742\uff0c\u5c24\u5176\u5728\u901a\u4fe1\u53d7\u9650\u6761\u4ef6\u4e0b\u3002\u8be5\u95ee\u9898\u6d89\u53ca\u969c\u788d\u7269\u3001\u654c\u624b\u3001\u76ee\u6807\u533a\u57df\u548c\u7f16\u961f\u52a8\u6001\u7b49\u591a\u7ef4\u590d\u6742\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u5c42\u6846\u67b6\u2014\u2014\u57fa\u4e8e\u5171\u8bc6\u63a8\u65ad\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08CI-HRL\uff09\uff0c\u9ad8\u5c42\u7b56\u7565\u8d1f\u8d23\u76ee\u6807\u5b9a\u4f4d\uff0c\u4f4e\u5c42\u7b56\u7565\u5904\u7406\u907f\u969c\u3001\u5bfc\u822a\u548c\u7f16\u961f\u3002\u9ad8\u5c42\u7b56\u7565\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u5171\u8bc6\u5bfc\u5411\u591a\u667a\u80fd\u4f53\u901a\u4fe1\uff08ConsMAC\uff09\uff0c\u901a\u8fc7\u6709\u6548\u805a\u5408\u90bb\u5c45\u4fe1\u606f\u5b9e\u73b0\u5168\u5c40\u611f\u77e5\u4e0e\u5171\u8bc6\u5efa\u7acb\uff1b\u4f4e\u5c42\u7b56\u7565\u5219\u7ed3\u5408\u4ea4\u66ff\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08AT-M\uff09\u548c\u7b56\u7565\u84b8\u998f\u5b8c\u6210\u63a7\u5236\u3002\u9ad8\u4fdd\u771f\u8f6f\u4ef6\u5728\u73af\uff08SITL\uff09\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CI-HRL\u5728\u63d0\u5347\u65e0\u4eba\u673a\u7fa4\u534f\u540c\u9003\u9038\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.17728", "pdf": "https://arxiv.org/pdf/2506.17728", "abs": "https://arxiv.org/abs/2506.17728", "authors": ["Dalong Zhang", "Jun Xu", "Jun Zhou", "Lei Liang", "Lin Yuan", "Ling Zhong", "Mengshu Sun", "Peilong Zhao", "QiWei Wang", "Xiaorui Wang", "Xinkai Du", "YangYang Hou", "Yu Ao", "ZhaoYang Wang", "Zhengke Gui", "ZhiYing Yi", "Zhongpu Bo"], "title": "KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce KAG-Thinker, a novel human-like reasoning\nframework built upon a parameter-light large language model (LLM). Our approach\nenhances the logical coherence and contextual consistency of the thinking\nprocess in question-answering (Q\\&A) tasks on domain-specific knowledge bases\n(KBs) within LLMs. This framework simulates human cognitive mechanisms for\nhandling complex problems by establishing a structured thinking process.\nContinuing the \\textbf{Logical Form} guided retrieval and reasoning technology\nroute of KAG v0.7, firstly, it decomposes complex questions into independently\nsolvable sub-problems(also referred to as logical forms) through\n\\textbf{breadth decomposition}, each represented in two equivalent\nforms-natural language and logical function-and further classified as either\nKnowledge Retrieval or Reasoning Analysis tasks, with dependencies and\nvariables passing explicitly modeled via logical function interfaces. In the\nsolving process, the Retrieval function is used to perform knowledge retrieval\ntasks, while the Math and Deduce functions are used to perform reasoning\nanalysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval\nsub-problem tasks, LLMs and external knowledge sources are regarded as\nequivalent KBs. We use the \\textbf{knowledge boundary} model to determine the\noptimal source using self-regulatory mechanisms such as confidence calibration\nand reflective reasoning, and use the \\textbf{depth solving} model to enhance\nthe comprehensiveness of knowledge acquisition. Finally, instead of utilizing\nreinforcement learning, we employ supervised fine-tuning with multi-turn\ndialogues to align the model with our structured inference paradigm, thereby\navoiding excessive reflection. This is supported by a data evaluation framework\nand iterative corpus synthesis, which facilitate the generation of detailed\nreasoning trajectories...", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKAG-Thinker\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u77e5\u8bc6\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u63a8\u7406\u4e2d\u7f3a\u4e4f\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0cKAG-Thinker\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7ed3\u6784\u5316\u601d\u7ef4\u8fc7\u7a0b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KAG-Thinker\u91c7\u7528\u5e7f\u5ea6\u5206\u89e3\u5c06\u590d\u6742\u95ee\u9898\u62c6\u5206\u4e3a\u53ef\u72ec\u7acb\u89e3\u51b3\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u903b\u8f91\u51fd\u6570\u63a5\u53e3\u660e\u786e\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\u3002\u5728\u89e3\u51b3\u8fc7\u7a0b\u4e2d\uff0c\u5206\u522b\u4f7f\u7528\u68c0\u7d22\u3001\u6570\u5b66\u548c\u63a8\u7406\u529f\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u8fb9\u754c\u6a21\u578b\u548c\u6df1\u5ea6\u89e3\u51b3\u6a21\u578b\u4f18\u5316\u77e5\u8bc6\u83b7\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKAG-Thinker\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u9886\u57df\u77e5\u8bc6\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5f97\u5230\u660e\u663e\u6539\u5584\u3002", "conclusion": "KAG-Thinker\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u95ee\u9898\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "KAG-Thinker\uff1a\u6559\u6388\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\u8fdb\u884c\u63a8\u7406", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86KAG-Thinker\uff0c\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u8f7b\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u4eba\u7c7b\u63a8\u7406\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u7acb\u7ed3\u6784\u5316\u601d\u7ef4\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86LLM\u5728\u9886\u57df\u77e5\u8bc6\u5e93\uff08KB\uff09\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u5ef6\u7eed\u4e86KAG v0.7\u7684\u903b\u8f91\u5f62\u5f0f\u5f15\u5bfc\u68c0\u7d22\u4e0e\u63a8\u7406\u6280\u672f\u8def\u7ebf\uff0c\u9996\u5148\u901a\u8fc7\u5e7f\u5ea6\u5206\u89e3\u5c06\u590d\u6742\u95ee\u9898\u62c6\u5206\u4e3a\u53ef\u72ec\u7acb\u89e3\u51b3\u7684\u5b50\u95ee\u9898\uff08\u5373\u903b\u8f91\u5f62\u5f0f\uff09\uff0c\u6bcf\u4e2a\u5b50\u95ee\u9898\u4ee5\u81ea\u7136\u8bed\u8a00\u548c\u903b\u8f91\u51fd\u6570\u4e24\u79cd\u7b49\u4ef7\u5f62\u5f0f\u8868\u793a\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u7c7b\u4e3a\u77e5\u8bc6\u68c0\u7d22\u6216\u63a8\u7406\u5206\u6790\u4efb\u52a1\uff0c\u901a\u8fc7\u903b\u8f91\u51fd\u6570\u63a5\u53e3\u660e\u786e\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\u548c\u53d8\u91cf\u4f20\u9012\u3002\u5728\u89e3\u51b3\u8fc7\u7a0b\u4e2d\uff0c\u68c0\u7d22\u529f\u80fd\u7528\u4e8e\u6267\u884c\u77e5\u8bc6\u68c0\u7d22\u4efb\u52a1\uff0c\u800c\u6570\u5b66\u548c\u63a8\u7406\u529f\u80fd\u7528\u4e8e\u6267\u884c\u63a8\u7406\u5206\u6790\u4efb\u52a1\u3002\u5176\u6b21\uff0c\u5728\u77e5\u8bc6\u68c0\u7d22\u5b50\u4efb\u52a1\u4e2d\uff0cLLM\u548c\u5916\u90e8\u77e5\u8bc6\u6e90\u88ab\u89c6\u4e3a\u7b49\u6548\u7684\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u77e5\u8bc6\u8fb9\u754c\u6a21\u578b\u7ed3\u5408\u81ea\u4fe1\u6821\u51c6\u548c\u53cd\u601d\u63a8\u7406\u7b49\u81ea\u8c03\u8282\u673a\u5236\u786e\u5b9a\u6700\u4f18\u77e5\u8bc6\u6e90\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u89e3\u51b3\u6a21\u578b\u63d0\u5347\u77e5\u8bc6\u83b7\u53d6\u7684\u5168\u9762\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u800c\u975e\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u5c06\u6a21\u578b\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u5bf9\u9f50\uff0c\u907f\u514d\u8fc7\u5ea6\u53cd\u601d\u3002\u8fd9\u4e00\u8fc7\u7a0b\u5f97\u5230\u6570\u636e\u8bc4\u4f30\u6846\u67b6\u548c\u8fed\u4ee3\u8bed\u6599\u5408\u6210\u7684\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u751f\u6210\u8be6\u7ec6\u7684\u63a8\u7406\u8f68\u8ff9\u3002"}}
{"id": "2506.17608", "pdf": "https://arxiv.org/pdf/2506.17608", "abs": "https://arxiv.org/abs/2506.17608", "authors": ["Nikitha SR", "Aradhya Neeraj Mathur", "Tarun Ram Menta", "Rishabh Jain", "Mausoom Sarkar"], "title": "HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025 Workshop on What's Next in Multimodal\n  Foundational Models", "summary": "The integration of high-resolution image features in modern multimodal large\nlanguage models has demonstrated significant improvements in fine-grained\nvisual understanding tasks, achieving high performance across multiple\nbenchmarks. Since these features are obtained from large image encoders like\nViT, they come with a significant increase in computational costs due to\nmultiple calls to these encoders. In this work, we first develop an intuition\nfor feature upsampling as a natural extension of high-resolution feature\ngeneration. Through extensive experiments and ablations, we demonstrate how a\nshallow feature enricher can achieve competitive results with tremendous\nreductions in training and inference time as well as computational cost, with\nupto 1.5x saving in FLOPs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\uff08HIRE\uff09\uff0c\u901a\u8fc7\u6d45\u5c42\u7279\u5f81\u589e\u5f3a\u5668\u663e\u8457\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7279\u5f81\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f7f\u7528\u5927\u578b\u56fe\u50cf\u7f16\u7801\u5668\uff08\u5982ViT\uff09\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d45\u5c42\u7279\u5f81\u589e\u5f3a\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u4e0a\u91c7\u6837\u6269\u5c55\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u4ee5\u53ca\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e861.5\u500d\uff08\u4ee5FLOPs\u8861\u91cf\uff09\u3002", "conclusion": "HIRE\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7279\u5f81\u7684\u9ad8\u6548\u96c6\u6210\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6210\u672c\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "HIRE\uff1a\u9762\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7279\u5f81\u589e\u5f3a", "abstract_zh": "\u5728\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7279\u5f81\u901a\u5e38\u6765\u81ea\u5927\u578b\u56fe\u50cf\u7f16\u7801\u5668\uff08\u5982ViT\uff09\uff0c\u591a\u6b21\u8c03\u7528\u8fd9\u4e9b\u7f16\u7801\u5668\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u589e\u52a0\u3002\u672c\u6587\u9996\u5148\u5c06\u7279\u5f81\u4e0a\u91c7\u6837\u4f5c\u4e3a\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u751f\u6210\u7684\u81ea\u7136\u6269\u5c55\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e00\u79cd\u6d45\u5c42\u7279\u5f81\u589e\u5f3a\u5668\u80fd\u591f\u5728\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u4ee5\u53ca\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u6700\u9ad8\u53ef\u8282\u77011.5\u500d\u7684FLOPs\u3002"}}
{"id": "2506.18135", "pdf": "https://arxiv.org/pdf/2506.18135", "abs": "https://arxiv.org/abs/2506.18135", "authors": ["Zijun Chen", "Zhanpeng Zhou", "Bo Zhang", "Weinan Zhang", "Xi Sun", "Junchi Yan"], "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging", "categories": ["cs.AI", "cs.CL"], "comment": "preprint, accepted at IJCNN2025", "summary": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u589e\u5f3a\u7684\u52a8\u6001\u6a21\u578b\u878d\u5408\u65b9\u6cd5SE-Merging\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u878d\u5408\u7684\u673a\u5236\uff0c\u53d1\u73b0\u5176\u591a\u4efb\u52a1\u80fd\u529b\u4f9d\u8d56\u4e8e\u533a\u5206\u4e0d\u540c\u4efb\u52a1\u6837\u672c\u548c\u9002\u5e94\u5bf9\u5e94\u4e13\u5bb6\u6a21\u578b\u7684\u80fd\u529b\u3002SE-Merging\u5229\u7528\u8fd9\u4e24\u70b9\u52a8\u6001\u8bc6\u522b\u4efb\u52a1\u5e76\u8c03\u6574\u878d\u5408\u7cfb\u6570\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6a21\u578b\u878d\u5408\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u4ece\u8868\u793a\u89d2\u5ea6\u63ed\u793a\u6a21\u578b\u878d\u5408\u7684\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e00\u79cd\u81ea\u589e\u5f3a\u7684\u52a8\u6001\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u878d\u5408\u7684\u673a\u5236\uff0c\u63d0\u51faSE-Merging\u6846\u67b6\uff0c\u52a8\u6001\u8bc6\u522b\u6837\u672c\u4efb\u52a1\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u878d\u5408\u7cfb\u6570\uff0c\u4ee5\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSE-Merging\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u878d\u5408\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u878d\u5408\u6280\u672f\u517c\u5bb9\u3002", "conclusion": "SE-Merging\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u878d\u5408\u7cfb\u6570\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u878d\u5408\u7684\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u878d\u5408\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "SE-Merging\uff1a\u4e00\u79cd\u81ea\u589e\u5f3a\u7684\u52a8\u6001\u6a21\u578b\u878d\u5408\u65b9\u6cd5", "abstract_zh": "\u6a21\u578b\u878d\u5408\u56e0\u5176\u80fd\u591f\u901a\u8fc7\u63d2\u503c\u4e0d\u540c\u4efb\u52a1\u5fae\u8c03\u6a21\u578b\u7684\u53c2\u6570\u5b9e\u73b0\u591a\u4efb\u52a1\u80fd\u529b\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5176\u7ecf\u9a8c\u6027\u6210\u529f\uff0c\u6a21\u578b\u878d\u5408\u7684\u673a\u5236\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u672c\u6587\u4ece\u8868\u793a\u89d2\u5ea6\u6df1\u5165\u7814\u7a76\u4e86\u6a21\u578b\u878d\u5408\u7684\u673a\u5236\uff0c\u53d1\u73b0\u5176\u591a\u4efb\u52a1\u80fd\u529b\u4f9d\u8d56\u4e8e\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1ai) \u533a\u5206\u4e0d\u540c\u4efb\u52a1\u7684\u6837\u672c\uff0cii) \u4e3a\u6bcf\u4e2a\u6837\u672c\u9002\u5e94\u5bf9\u5e94\u7684\u4e13\u5bb6\u6a21\u578b\u3002\u8fd9\u4e24\u79cd\u80fd\u529b\u4f7f\u878d\u5408\u6a21\u578b\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u4e13\u957f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u9002\u5e94\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SE-Merging\uff0c\u4e00\u79cd\u81ea\u589e\u5f3a\u7684\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u5229\u7528\u8fd9\u4e24\u79cd\u7279\u6027\u52a8\u6001\u8bc6\u522b\u6837\u672c\u4efb\u52a1\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u878d\u5408\u7cfb\u6570\uff0c\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u878d\u5408\u6a21\u578b\u7684\u4efb\u52a1\u7279\u5b9a\u4e13\u957f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSE-Merging\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u878d\u5408\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSE-Merging\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u878d\u5408\u6280\u672f\u517c\u5bb9\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.17748", "pdf": "https://arxiv.org/pdf/2506.17748", "abs": "https://arxiv.org/abs/2506.17748", "authors": ["Anwoy Chatterjee", "Yash Goel", "Tanmoy Chakraborty"], "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Contemporary Language Models (LMs), while impressively fluent, often generate\ncontent that is factually incorrect or unfaithful to the input context - a\ncritical issue commonly referred to as 'hallucination'. This tendency of LMs to\ngenerate hallucinated content undermines their reliability, especially because\nthese fabrications are often highly convincing and therefore difficult to\ndetect. While several existing methods attempt to detect hallucinations, most\nrely on analyzing multiple generations per input, leading to increased\ncomputational cost and latency. To address this, we propose a single-pass,\ntraining-free approach for effective Hallucination detectIon via Decoupled\nrEpresentations (HIDE). Our approach leverages the hypothesis that\nhallucinations result from a statistical decoupling between an LM's internal\nrepresentations of input context and its generated output. We quantify this\ndecoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to\nhidden-state representations extracted while generating the output sequence. We\nconduct extensive experiments on four diverse question answering datasets,\nevaluating both faithfulness and factuality hallucinations across six\nopen-source LMs of varying scales and properties. Our results demonstrate that\nHIDE outperforms other single-pass methods in almost all settings, achieving an\naverage relative improvement of ~29% in AUC-ROC over the best-performing\nsingle-pass strategy across various models and datasets. Additionally, HIDE\nshows competitive and often superior performance with multi-pass\nstate-of-the-art methods, obtaining an average relative improvement of ~3% in\nAUC-ROC while consuming ~51% less computation time. Our findings highlight the\neffectiveness of exploiting internal representation decoupling in LMs for\nefficient and practical hallucination detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIDE\u7684\u5355\u6b21\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u68c0\u6d4b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5355\u6b21\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u591a\u6b21\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u5f53\u4ee3\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6d41\u7545\uff0c\u4f46\u5e38\u751f\u6210\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u4e0d\u7b26\u6216\u4e8b\u5b9e\u9519\u8bef\u7684\u5185\u5bb9\uff08\u5373\u5e7b\u89c9\uff09\uff0c\u4e14\u8fd9\u4e9b\u5185\u5bb9\u96be\u4ee5\u68c0\u6d4b\uff0c\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u6b21\u751f\u6210\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b21\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "HIDE\u65b9\u6cd5\u57fa\u4e8e\u5e7b\u89c9\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u8f93\u5165\u4e0a\u4e0b\u6587\u4e0e\u751f\u6210\u8f93\u51fa\u7684\u7edf\u8ba1\u89e3\u8026\u5047\u8bbe\uff0c\u901a\u8fc7Hilbert-Schmidt\u72ec\u7acb\u6027\u51c6\u5219\uff08HSIC\uff09\u91cf\u5316\u9690\u85cf\u72b6\u6001\u8868\u793a\u7684\u89e3\u8026\u7a0b\u5ea6\uff0c\u5b9e\u73b0\u5355\u6b21\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u95ee\u7b54\u6570\u636e\u96c6\u548c\u516d\u4e2a\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIDE\u5728\u51e0\u4e4e\u6240\u6709\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5176\u4ed6\u5355\u6b21\u65b9\u6cd5\uff0cAUC-ROC\u5e73\u5747\u63d0\u5347\u7ea629%\uff0c\u4e14\u4e0e\u591a\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1151%\u3002", "conclusion": "HIDE\u901a\u8fc7\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u89e3\u8026\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\uff0c\u4e3a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "HIDE\u4e0eSeek\uff1a\u901a\u8fc7\u89e3\u8026\u8868\u793a\u68c0\u6d4b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9", "abstract_zh": "\u5f53\u4ee3\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u867d\u7136\u6d41\u7545\u6027\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u5e38\u751f\u6210\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u4e0d\u7b26\u6216\u4e8b\u5b9e\u9519\u8bef\u7684\u5185\u5bb9\u2014\u2014\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u5e7b\u89c9\u201d\u3002\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e7b\u89c9\u5185\u5bb9\u7684\u503e\u5411\u524a\u5f31\u4e86\u5176\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u56e0\u4e3a\u8fd9\u4e9b\u865a\u6784\u5185\u5bb9\u901a\u5e38\u6781\u5177\u8bf4\u670d\u529b\uff0c\u96be\u4ee5\u68c0\u6d4b\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u68c0\u6d4b\u5e7b\u89c9\uff0c\u4f46\u5927\u591a\u4f9d\u8d56\u5bf9\u6bcf\u4e2a\u8f93\u5165\u8fdb\u884c\u591a\u6b21\u751f\u6210\u5206\u6790\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u589e\u52a0\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u6b21\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u89e3\u8026\u8868\u793a\u8fdb\u884c\u6709\u6548\u5e7b\u89c9\u68c0\u6d4b\uff08HIDE\uff09\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5e7b\u89c9\u6e90\u4e8e\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8f93\u5165\u4e0a\u4e0b\u6587\u4e0e\u751f\u6210\u8f93\u51fa\u7684\u7edf\u8ba1\u89e3\u8026\u5047\u8bbe\uff0c\u901a\u8fc7Hilbert-Schmidt\u72ec\u7acb\u6027\u51c6\u5219\uff08HSIC\uff09\u91cf\u5316\u751f\u6210\u8f93\u51fa\u5e8f\u5217\u65f6\u63d0\u53d6\u7684\u9690\u85cf\u72b6\u6001\u8868\u793a\u7684\u89e3\u8026\u7a0b\u5ea6\u3002\u6211\u4eec\u5728\u56db\u4e2a\u591a\u6837\u5316\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u4e0d\u540c\u89c4\u6a21\u548c\u7279\u6027\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5fe0\u5b9e\u6027\u548c\u4e8b\u5b9e\u6027\u5e7b\u89c9\u3002\u7ed3\u679c\u8868\u660e\uff0cHIDE\u5728\u51e0\u4e4e\u6240\u6709\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5176\u4ed6\u5355\u6b21\u65b9\u6cd5\uff0cAUC-ROC\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\u7ea629%\u3002\u6b64\u5916\uff0cHIDE\u4e0e\u591a\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0cAUC-ROC\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\u7ea63%\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u7ea651%\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u89e3\u8026\u7279\u6027\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\u3002"}}
{"id": "2506.17612", "pdf": "https://arxiv.org/pdf/2506.17612", "abs": "https://arxiv.org/abs/2506.17612", "authors": ["Yunlong Lin", "Zixu Lin", "Kunjie Lin", "Jinbin Bai", "Panwang Pan", "Chenxin Li", "Haoyu Chen", "Zhongdao Wang", "Xinghao Ding", "Wenbo Li", "Shuicheng Yan"], "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent", "categories": ["cs.CV"], "comment": "40 pages, 26 figures", "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.", "AI": {"tldr": "JarvisArt\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u7167\u7247\u6da6\u8272\u4ee3\u7406\uff0c\u901a\u8fc7\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u6a21\u4eff\u4e13\u4e1a\u827a\u672f\u5bb6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u534f\u8c03200\u591a\u79cdLightroom\u5de5\u5177\uff0c\u5b9e\u73b0\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u548c\u7cbe\u7ec6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709AI\u7167\u7247\u6da6\u8272\u5de5\u5177\u81ea\u52a8\u5316\u7a0b\u5ea6\u9ad8\u4f46\u8c03\u6574\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\uff1b\u4e13\u4e1a\u5de5\u5177\u5982Lightroom\u529f\u80fd\u5f3a\u5927\u4f46\u9700\u4e13\u4e1a\u77e5\u8bc6\u3002JarvisArt\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u667a\u80fd\u4e14\u7075\u6d3b\u7684\u6da6\u8272\u89e3\u51b3\u65b9\u6848\u3002", "method": "JarvisArt\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u901a\u8fc7Chain-of-Thought\u76d1\u7763\u5fae\u8c03\u5efa\u7acb\u57fa\u7840\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u518d\u901a\u8fc7Group Relative Policy Optimization for Retouching (GRPO-R)\u63d0\u5347\u51b3\u7b56\u548c\u5de5\u5177\u719f\u7ec3\u5ea6\u3002\u540c\u65f6\u63d0\u51faAgent-to-Lightroom\u534f\u8bae\u5b9e\u73b0\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728MMArt-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJarvisArt\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u4e0a\u6bd4GPT-4o\u63d0\u534760%\uff0c\u540c\u65f6\u4fdd\u6301\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5168\u5c40/\u5c40\u90e8\u8c03\u6574\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "conclusion": "JarvisArt\u4e3a\u667a\u80fd\u7167\u7247\u6da6\u8272\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u7ed3\u5408\u4e86\u7528\u6237\u53cb\u597d\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u7cbe\u7ec6\u63a7\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "JarvisArt\uff1a\u901a\u8fc7\u667a\u80fd\u7167\u7247\u6da6\u8272\u4ee3\u7406\u89e3\u653e\u4eba\u7c7b\u827a\u672f\u521b\u9020\u529b", "abstract_zh": "\u7167\u7247\u6da6\u8272\u5df2\u6210\u4e3a\u5f53\u4ee3\u89c6\u89c9\u53d9\u4e8b\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5e2e\u52a9\u7528\u6237\u6355\u6349\u7f8e\u5b66\u5e76\u8868\u8fbe\u521b\u610f\u3002\u5c3d\u7ba1\u4e13\u4e1a\u5de5\u5177\u5982Adobe Lightroom\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u624b\u52a8\u64cd\u4f5c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u81ea\u52a8\u5316\uff0c\u4f46\u8c03\u6574\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u548c\u4e2a\u6027\u5316\u9700\u6c42\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51faJarvisArt\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u4ee3\u7406\uff0c\u80fd\u591f\u7406\u89e3\u7528\u6237\u610f\u56fe\uff0c\u6a21\u4eff\u4e13\u4e1a\u827a\u672f\u5bb6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u667a\u80fd\u534f\u8c03Lightroom\u4e2d\u7684200\u591a\u79cd\u6da6\u8272\u5de5\u5177\u3002JarvisArt\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u901a\u8fc7Chain-of-Thought\u76d1\u7763\u5fae\u8c03\u5efa\u7acb\u57fa\u7840\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u518d\u901a\u8fc7Group Relative Policy Optimization for Retouching (GRPO-R)\u63d0\u5347\u51b3\u7b56\u548c\u5de5\u5177\u719f\u7ec3\u5ea6\u3002\u6211\u4eec\u8fd8\u63d0\u51faAgent-to-Lightroom\u534f\u8bae\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u96c6\u6210\u3002\u4e3a\u8bc4\u4f30\u6027\u80fd\uff0c\u6211\u4eec\u5f00\u53d1\u4e86MMArt-Bench\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u7f16\u8f91\u6570\u636e\u6784\u5efa\u3002JarvisArt\u5c55\u793a\u4e86\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u3001\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u5bf9\u5168\u5c40\u548c\u5c40\u90e8\u8c03\u6574\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4e3a\u667a\u80fd\u7167\u7247\u6da6\u8272\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728MMArt-Bench\u7684\u5185\u5bb9\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\uff0cJarvisArt\u6bd4GPT-4o\u63d0\u5347\u4e8660%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://jarvisart.vercel.app/\u3002"}}
{"id": "2506.18149", "pdf": "https://arxiv.org/pdf/2506.18149", "abs": "https://arxiv.org/abs/2506.18149", "authors": ["Fumian Chen", "Sotheara Veng", "Joshua Wilson", "Xiaoming Li", "Hui Fang"], "title": "CoachGPT: A Scaffolding-based Academic Writing Assistant", "categories": ["cs.AI"], "comment": "SIGIR 2025 DEMO Pre-print", "summary": "Academic writing skills are crucial for students' success, but can feel\noverwhelming without proper guidance and practice, particularly when writing in\na second language. Traditionally, students ask instructors or search\ndictionaries, which are not universally accessible. Early writing assistants\nemerged as rule-based systems that focused on detecting misspellings,\nsubject-verb disagreements, and basic punctuation errors; however, they are\ninaccurate and lack contextual understanding. Machine learning-based assistants\ndemonstrate a strong ability for language understanding but are expensive to\ntrain. Large language models (LLMs) have shown remarkable capabilities in\ngenerating responses in natural languages based on given prompts. Still, they\nhave a fundamental limitation in education: they generate essays without\nteaching, which can have detrimental effects on learning when misused. To\naddress this limitation, we develop CoachGPT, which leverages large language\nmodels (LLMs) to assist individuals with limited educational resources and\nthose who prefer self-paced learning in academic writing. CoachGPT is an AI\nagent-based web application that (1) takes instructions from experienced\neducators, (2) converts instructions into sub-tasks, and (3) provides real-time\nfeedback and suggestions using large language models. This unique scaffolding\nstructure makes CoachGPT unique among existing writing assistants. Compared to\nexisting writing assistants, CoachGPT provides a more immersive writing\nexperience with personalized feedback and guidance. Our user studies prove the\nusefulness of CoachGPT and the potential of large language models for academic\nwriting.", "AI": {"tldr": "CoachGPT\u662f\u4e00\u6b3e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u672f\u5199\u4f5c\u52a9\u624b\uff0c\u901a\u8fc7\u5206\u6b65\u4efb\u52a1\u548c\u5b9e\u65f6\u53cd\u9988\u5e2e\u52a9\u7528\u6237\u63d0\u5347\u5199\u4f5c\u80fd\u529b\uff0c\u5c24\u5176\u9002\u5408\u8d44\u6e90\u6709\u9650\u6216\u504f\u597d\u81ea\u4e3b\u5b66\u4e60\u7684\u5b66\u4e60\u8005\u3002", "motivation": "\u5b66\u672f\u5199\u4f5c\u5bf9\u5b66\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6307\u5bfc\u548c\u8d44\u6e90\u65f6\u53ef\u80fd\u4ee4\u4eba\u671b\u800c\u751f\u754f\u3002\u4f20\u7edf\u5199\u4f5c\u52a9\u624b\u529f\u80fd\u6709\u9650\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u5f3a\u5927\u4f46\u7f3a\u4e4f\u6559\u80b2\u6027\u53cd\u9988\u3002\u56e0\u6b64\uff0c\u5f00\u53d1CoachGPT\u4ee5\u7ed3\u5408\u6559\u80b2\u6307\u5bfc\u548c\u8bed\u8a00\u6a21\u578b\u80fd\u529b\uff0c\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u5199\u4f5c\u652f\u6301\u3002", "method": "CoachGPT\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u4ee3\u7406\u7684\u7f51\u9875\u5e94\u7528\uff0c\u901a\u8fc7\u63a5\u6536\u6559\u80b2\u8005\u7684\u6307\u4ee4\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u548c\u5efa\u8bae\uff0c\u5f62\u6210\u72ec\u7279\u7684\u652f\u67b6\u5f0f\u5b66\u4e60\u7ed3\u6784\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cCoachGPT\u63d0\u4f9b\u4e86\u6c89\u6d78\u5f0f\u7684\u5199\u4f5c\u4f53\u9a8c\u548c\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u4f18\u4e8e\u73b0\u6709\u5199\u4f5c\u52a9\u624b\uff0c\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "CoachGPT\u901a\u8fc7\u7ed3\u5408\u6559\u80b2\u6307\u5bfc\u548c\u8bed\u8a00\u6a21\u578b\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5b66\u672f\u5199\u4f5c\u7684\u8f85\u52a9\u6548\u679c\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u5b66\u4e60\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "CoachGPT\uff1a\u57fa\u4e8e\u652f\u67b6\u5f0f\u5b66\u4e60\u7684\u5b66\u672f\u5199\u4f5c\u52a9\u624b", "abstract_zh": "\u5b66\u672f\u5199\u4f5c\u80fd\u529b\u5bf9\u5b66\u751f\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u9002\u5f53\u6307\u5bfc\u548c\u7ec3\u4e60\u65f6\u53ef\u80fd\u4ee4\u4eba\u611f\u5230\u538b\u529b\uff0c\u5c24\u5176\u662f\u7528\u7b2c\u4e8c\u8bed\u8a00\u5199\u4f5c\u65f6\u3002\u4f20\u7edf\u4e0a\uff0c\u5b66\u751f\u4f1a\u5411\u6559\u5e08\u6c42\u52a9\u6216\u67e5\u9605\u8bcd\u5178\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u975e\u666e\u904d\u53ef\u7528\u3002\u65e9\u671f\u7684\u5199\u4f5c\u52a9\u624b\u662f\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u62fc\u5199\u9519\u8bef\u3001\u4e3b\u8c13\u4e0d\u4e00\u81f4\u548c\u57fa\u672c\u6807\u70b9\u9519\u8bef\uff0c\u4f46\u5b83\u4eec\u51c6\u786e\u6027\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u52a9\u624b\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6839\u636e\u63d0\u793a\u751f\u6210\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6559\u80b2\u4e2d\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u5b83\u4eec\u751f\u6210\u6587\u7ae0\u800c\u4e0d\u6559\u6388\uff0c\u82e5\u88ab\u8bef\u7528\u53ef\u80fd\u5bf9\u5b66\u4e60\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86CoachGPT\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e2e\u52a9\u6559\u80b2\u8d44\u6e90\u6709\u9650\u6216\u504f\u597d\u81ea\u4e3b\u5b66\u4e60\u7684\u5b66\u4e60\u8005\u8fdb\u884c\u5b66\u672f\u5199\u4f5c\u3002CoachGPT\u662f\u4e00\u6b3e\u57fa\u4e8eAI\u4ee3\u7406\u7684\u7f51\u9875\u5e94\u7528\uff0c\u80fd\u591f\uff081\uff09\u63a5\u6536\u7ecf\u9a8c\u4e30\u5bcc\u7684\u6559\u80b2\u8005\u7684\u6307\u4ee4\uff0c\uff082\uff09\u5c06\u6307\u4ee4\u8f6c\u5316\u4e3a\u5b50\u4efb\u52a1\uff0c\uff083\uff09\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u548c\u5efa\u8bae\u3002\u8fd9\u79cd\u72ec\u7279\u7684\u652f\u67b6\u5f0f\u7ed3\u6784\u4f7fCoachGPT\u5728\u73b0\u6709\u5199\u4f5c\u52a9\u624b\u4e2d\u72ec\u6811\u4e00\u5e1c\u3002\u4e0e\u73b0\u6709\u5199\u4f5c\u52a9\u624b\u76f8\u6bd4\uff0cCoachGPT\u63d0\u4f9b\u4e86\u66f4\u5177\u6c89\u6d78\u611f\u7684\u5199\u4f5c\u4f53\u9a8c\u548c\u4e2a\u6027\u5316\u53cd\u9988\u4e0e\u6307\u5bfc\u3002\u6211\u4eec\u7684\u7528\u6237\u7814\u7a76\u8bc1\u660e\u4e86CoachGPT\u7684\u5b9e\u7528\u6027\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17789", "pdf": "https://arxiv.org/pdf/2506.17789", "abs": "https://arxiv.org/abs/2506.17789", "authors": ["N J Karthika", "Maharaj Brahma", "Rohit Saluja", "Ganesh Ramakrishnan", "Maunendra Sankar Desarkar"], "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights", "categories": ["cs.CL"], "comment": null, "summary": "Tokenization plays a pivotal role in multilingual NLP. However, existing\ntokenizers are often skewed towards high-resource languages, limiting their\neffectiveness for linguistically diverse and morphologically rich languages\nsuch as those in the Indian subcontinent. This paper presents a comprehensive\nintrinsic evaluation of tokenization strategies across 17 Indian languages. We\nquantify the trade-offs between bottom-up and top-down tokenizer algorithms\n(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of\nmultilingual vocabulary construction such as joint and cluster-based training.\nWe also show that extremely low-resource languages can benefit from tokenizers\ntrained on related high-resource languages. Our study provides practical\ninsights for building more fair, efficient, and linguistically informed\ntokenizers for multilingual NLP.", "AI": {"tldr": "\u672c\u6587\u5bf917\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5206\u8bcd\u7b56\u7565\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63a2\u8ba8\u4e86BPE\u548cUnigram LM\u7b97\u6cd5\u7684\u6743\u8861\u3001\u8bcd\u6c47\u91cf\u5927\u5c0f\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8054\u5408\u548c\u57fa\u4e8e\u96c6\u7fa4\u7684\u591a\u8bed\u8a00\u8bcd\u6c47\u6784\u5efa\u7b56\u7565\uff0c\u53d1\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ef\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684Tokenizer\u4e2d\u53d7\u76ca\u3002", "motivation": "\u73b0\u6709Tokenizer\u504f\u5411\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5bf9\u5370\u5ea6\u7b49\u8bed\u8a00\u591a\u6837\u4e14\u5f62\u6001\u4e30\u5bcc\u7684\u5730\u533a\u6548\u679c\u6709\u9650\uff0c\u9700\u7814\u7a76\u66f4\u516c\u5e73\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u5206\u8bcd\u65b9\u6cd5\u3002", "method": "\u5bf917\u79cd\u5370\u5ea6\u8bed\u8a00\u8fdb\u884c\u5206\u8bcd\u7b56\u7565\u8bc4\u4f30\uff0c\u6bd4\u8f83BPE\u548cUnigram LM\u7b97\u6cd5\uff0c\u5206\u6790\u8bcd\u6c47\u91cf\u5927\u5c0f\u548c\u591a\u8bed\u8a00\u8bcd\u6c47\u6784\u5efa\u7b56\u7565\uff08\u8054\u5408\u4e0e\u96c6\u7fa4\u8bad\u7ec3\uff09\u3002", "result": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ef\u901a\u8fc7\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684Tokenizer\u63d0\u5347\u6548\u679c\uff0c\u4e0d\u540c\u7b97\u6cd5\u548c\u8bcd\u6c47\u91cf\u5bf9\u5206\u8bcd\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u3001\u9ad8\u6548\u4e14\u7b26\u5408\u8bed\u8a00\u5b66\u7279\u70b9\u7684\u591a\u8bed\u8a00Tokenizer\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "paper_title_zh": "\u4ece\u5370\u5ea6\u8bed\u8a00\u89c6\u89d2\u770b\u591a\u8bed\u8a00\u5206\u8bcd\uff1a\u6311\u6218\u4e0e\u6d1e\u89c1", "abstract_zh": "\u5206\u8bcd\u5728\u591a\u8bed\u8a00NLP\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709Tokenizer\u5e38\u504f\u5411\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5bf9\u5370\u5ea6\u7b49\u8bed\u8a00\u591a\u6837\u4e14\u5f62\u6001\u4e30\u5bcc\u7684\u5730\u533a\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u5bf917\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5206\u8bcd\u7b56\u7565\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u91cf\u5316\u4e86BPE\u548cUnigram LM\u7b97\u6cd5\u7684\u6743\u8861\u3001\u8bcd\u6c47\u91cf\u5927\u5c0f\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86\u8054\u5408\u548c\u57fa\u4e8e\u96c6\u7fa4\u7684\u591a\u8bed\u8a00\u8bcd\u6c47\u6784\u5efa\u7b56\u7565\u3002\u7814\u7a76\u8868\u660e\uff0c\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ef\u4ece\u76f8\u5173\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684Tokenizer\u4e2d\u53d7\u76ca\u3002\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u3001\u9ad8\u6548\u4e14\u7b26\u5408\u8bed\u8a00\u5b66\u7279\u70b9\u7684\u591a\u8bed\u8a00Tokenizer\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2506.17629", "pdf": "https://arxiv.org/pdf/2506.17629", "abs": "https://arxiv.org/abs/2506.17629", "authors": ["Kailing Li", "Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Yang Jiao", "Xiaoling Wang"], "title": "CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Embodied Visual Reasoning (EVR) seeks to follow complex, free-form\ninstructions based on egocentric video, enabling semantic understanding and\nspatiotemporal reasoning in dynamic environments. Despite its promising\npotential, EVR encounters significant challenges stemming from the diversity of\ncomplex instructions and the intricate spatiotemporal dynamics in long-term\negocentric videos. Prior solutions either employ Large Language Models (LLMs)\nover static video captions, which often omit critical visual details, or rely\non end-to-end Vision-Language Models (VLMs) that struggle with stepwise\ncompositional reasoning. Consider the complementary strengths of LLMs in\nreasoning and VLMs in perception, we propose CLiViS. It is a novel\ntraining-free framework that leverages LLMs for high-level task planning and\norchestrates VLM-driven open-world visual perception to iteratively update the\nscene context. Building on this synergy, the core of CLiViS is a dynamic\nCognitive Map that evolves throughout the reasoning process. This map\nconstructs a structured representation of the embodied scene, bridging\nlow-level perception and high-level reasoning. Extensive experiments across\nmultiple benchmarks demonstrate the effectiveness and generality of CLiViS,\nespecially in handling long-term visual dependencies. Code is available at\nhttps://github.com/Teacher-Tom/CLiViS.", "AI": {"tldr": "CLiViS\u662f\u4e00\u79cd\u65b0\u578b\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u4f18\u52bf\uff0c\u6784\u5efa\u52a8\u6001\u8ba4\u77e5\u5730\u56fe\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5177\u8eab\u89c6\u89c9\u63a8\u7406\u3002", "motivation": "\u5177\u8eab\u89c6\u89c9\u63a8\u7406\uff08EVR\uff09\u9762\u4e34\u590d\u6742\u6307\u4ee4\u591a\u6837\u6027\u548c\u957f\u65f6\u5e8f\u89c6\u9891\u52a8\u6001\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u5173\u952e\u89c6\u89c9\u7ec6\u8282\uff0c\u8981\u4e48\u96be\u4ee5\u5b9e\u73b0\u9010\u6b65\u7ec4\u5408\u63a8\u7406\u3002CLiViS\u65e8\u5728\u901a\u8fc7\u8bed\u8a00-\u89c6\u89c9\u534f\u540c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CLiViS\u5229\u7528LLMs\u8fdb\u884c\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u901a\u8fc7VLM\u9a71\u52a8\u7684\u5f00\u653e\u4e16\u754c\u89c6\u89c9\u611f\u77e5\u8fed\u4ee3\u66f4\u65b0\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u6784\u5efa\u52a8\u6001\u8ba4\u77e5\u5730\u56fe\uff0c\u8fde\u63a5\u4f4e\u5c42\u611f\u77e5\u4e0e\u9ad8\u5c42\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLiViS\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5904\u7406\u957f\u65f6\u5e8f\u89c6\u89c9\u4f9d\u8d56\u4efb\u52a1\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "CLiViS\u901a\u8fc7\u8bed\u8a00-\u89c6\u89c9\u534f\u540c\u548c\u52a8\u6001\u8ba4\u77e5\u5730\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u89c6\u89c9\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u65f6\u7a7a\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "CLiViS\uff1a\u901a\u8fc7\u8bed\u8a00-\u89c6\u89c9\u534f\u540c\u91ca\u653e\u8ba4\u77e5\u5730\u56fe\u4ee5\u5b9e\u73b0\u5177\u8eab\u89c6\u89c9\u63a8\u7406", "abstract_zh": "\u5177\u8eab\u89c6\u89c9\u63a8\u7406\uff08EVR\uff09\u65e8\u5728\u57fa\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6267\u884c\u590d\u6742\u7684\u81ea\u7531\u5f62\u5f0f\u6307\u4ee4\uff0c\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u65f6\u7a7a\u63a8\u7406\u3002\u5c3d\u7ba1\u6f5c\u529b\u5de8\u5927\uff0cEVR\u4ecd\u9762\u4e34\u590d\u6742\u6307\u4ee4\u591a\u6837\u6027\u548c\u957f\u65f6\u5e8f\u89c6\u9891\u52a8\u6001\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u89c6\u9891\u63cf\u8ff0\u7684LLMs\uff0c\u5ffd\u7565\u5173\u952e\u89c6\u89c9\u7ec6\u8282\uff0c\u8981\u4e48\u4f7f\u7528\u7aef\u5230\u7aefVLMs\uff0c\u96be\u4ee5\u5b9e\u73b0\u9010\u6b65\u7ec4\u5408\u63a8\u7406\u3002\u7ed3\u5408LLMs\u7684\u63a8\u7406\u80fd\u529b\u548cVLMs\u7684\u611f\u77e5\u4f18\u52bf\uff0c\u6211\u4eec\u63d0\u51faCLiViS\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u578b\u6846\u67b6\u3002\u5b83\u5229\u7528LLMs\u8fdb\u884c\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u901a\u8fc7VLM\u9a71\u52a8\u7684\u5f00\u653e\u4e16\u754c\u89c6\u89c9\u611f\u77e5\u8fed\u4ee3\u66f4\u65b0\u573a\u666f\u4e0a\u4e0b\u6587\u3002\u57fa\u4e8e\u6b64\u534f\u540c\uff0cCLiViS\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u52a8\u6001\u8ba4\u77e5\u5730\u56fe\uff0c\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u6f14\u5316\uff0c\u6784\u5efa\u5177\u8eab\u573a\u666f\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u8fde\u63a5\u4f4e\u5c42\u611f\u77e5\u4e0e\u9ad8\u5c42\u63a8\u7406\u3002\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CLiViS\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u65f6\u5e8f\u89c6\u89c9\u4f9d\u8d56\u4efb\u52a1\u4e0a\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/Teacher-Tom/CLiViS\u3002"}}
{"id": "2506.18156", "pdf": "https://arxiv.org/pdf/2506.18156", "abs": "https://arxiv.org/abs/2506.18156", "authors": ["Akash Kundu", "Rishika Goswami"], "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology", "categories": ["cs.AI"], "comment": null, "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u5728\u56db\u79cd\u5fc3\u7406\u5b66\u6846\u67b6\u4e0b\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u6a21\u5f0f\uff0c\u53d1\u73b0\u5176\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u503e\u5411\u76f8\u4f3c\uff0c\u4f46\u53d7\u8bad\u7ec3\u6570\u636e\u548c\u6821\u51c6\u65b9\u6cd5\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u6a21\u5f0f\uff0c\u4ee5\u5fc3\u7406\u5b66\u7406\u8bba\u4e3a\u6846\u67b6\uff0c\u8bc4\u4f30\u5176\u900f\u660e\u5ea6\u548c\u4f26\u7406\u90e8\u7f72\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e3b\u9898\u7edf\u89c9\u6d4b\u8bd5\uff08TAT\uff09\u3001\u6846\u67b6\u504f\u5dee\u3001\u9053\u5fb7\u57fa\u7840\u7406\u8bba\uff08MFT\uff09\u548c\u8ba4\u77e5\u5931\u8c03\u56db\u79cd\u5fc3\u7406\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u81ea\u52a8\u8bc4\u5206\u8bc4\u4f30\u591a\u4e2a\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u751f\u6210\u8fde\u8d2f\u53d9\u8ff0\uff0c\u6613\u53d7\u6b63\u9762\u6846\u67b6\u5f71\u54cd\uff0c\u9053\u5fb7\u5224\u65ad\u504f\u5411\u81ea\u7531/\u538b\u8feb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5408\u7406\u5316\u7f13\u89e3\u81ea\u6211\u77db\u76fe\uff0c\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u503e\u5411\u76f8\u4f3c\u3002", "conclusion": "\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u503e\u5411\u76f8\u4f3c\uff0c\u4f46\u53d7\u8bad\u7ec3\u6570\u636e\u548c\u6821\u51c6\u65b9\u6cd5\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347AI\u900f\u660e\u5ea6\u548c\u4f26\u7406\u90e8\u7f72\u3002", "paper_title_zh": "\u4ece\u4eba\u7c7b\u89c6\u89d2\u770bAI\uff1a\u673a\u5668\u5fc3\u7406\u5b66\u4e2d\u7684\u8ba4\u77e5\u7406\u8bba\u7814\u7a76", "abstract_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u5728\u56db\u79cd\u5fc3\u7406\u5b66\u6846\u67b6\u4e0b\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u6a21\u5f0f\uff1a\u4e3b\u9898\u7edf\u89c9\u6d4b\u8bd5\uff08TAT\uff09\u3001\u6846\u67b6\u504f\u5dee\u3001\u9053\u5fb7\u57fa\u7840\u7406\u8bba\uff08MFT\uff09\u548c\u8ba4\u77e5\u5931\u8c03\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u81ea\u52a8\u8bc4\u5206\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u4e2a\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u6a21\u578b\u5e38\u80fd\u751f\u6210\u8fde\u8d2f\u53d9\u8ff0\uff0c\u6613\u53d7\u6b63\u9762\u6846\u67b6\u5f71\u54cd\uff0c\u9053\u5fb7\u5224\u65ad\u504f\u5411\u81ea\u7531/\u538b\u8feb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5408\u7406\u5316\u7f13\u89e3\u81ea\u6211\u77db\u76fe\u3002\u8fd9\u4e9b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u503e\u5411\u76f8\u4f3c\uff0c\u4f46\u53d7\u8bad\u7ec3\u6570\u636e\u548c\u6821\u51c6\u65b9\u6cd5\u5f71\u54cd\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u5176\u5bf9AI\u900f\u660e\u5ea6\u3001\u4f26\u7406\u90e8\u7f72\u4ee5\u53ca\u672a\u6765\u7ed3\u5408\u8ba4\u77e5\u5fc3\u7406\u5b66\u4e0eAI\u5b89\u5168\u7814\u7a76\u7684\u542f\u793a\u3002"}}
{"id": "2506.17844", "pdf": "https://arxiv.org/pdf/2506.17844", "abs": "https://arxiv.org/abs/2506.17844", "authors": ["Xin Zhang", "Qiyu Wei", "Yingjie Zhu", "Fanyi Wu", "Sophia Ananiadou"], "title": "THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Automated clinical risk prediction from electronic health records (EHRs)\ndemands modeling both structured diagnostic codes and unstructured narrative\nnotes. However, most prior approaches either handle these modalities separately\nor rely on simplistic fusion strategies that ignore the directional,\nhierarchical causal interactions by which narrative observations precipitate\ndiagnoses and propagate risk across admissions. In this paper, we propose\nTHCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our\nframework constructs a multimodal causal graph where nodes represent clinical\nentities from two modalities: Textual propositions extracted from notes and ICD\ncodes mapped to textual descriptions. Through hierarchical causal discovery,\nTHCM-CAL infers three clinically grounded interactions: intra-slice\nsame-modality sequencing, intra-slice cross-modality triggers, and inter-slice\nrisk propagation. To enhance prediction reliability, we extend conformal\nprediction to multi-label ICD coding, calibrating per-code confidence intervals\nunder complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV\ndemonstrate the superiority of THCM-CAL.", "AI": {"tldr": "THCM-CAL\u662f\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5c42\u6b21\u56e0\u679c\u5efa\u6a21\u4e0e\u7f6e\u4fe1\u6821\u51c6\u7684\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u56e0\u679c\u56fe\u5efa\u6a21\u8bca\u65ad\u4ee3\u7801\u548c\u6587\u672c\u7b14\u8bb0\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u7b80\u5355\uff0c\u5ffd\u7565\u4e86\u8bca\u65ad\u4e0e\u6587\u672c\u7b14\u8bb0\u95f4\u7684\u5c42\u6b21\u56e0\u679c\u4ea4\u4e92\uff0cTHCM-CAL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "THCM-CAL\u6784\u5efa\u591a\u6a21\u6001\u56e0\u679c\u56fe\uff0c\u5efa\u6a21\u4e34\u5e8a\u5b9e\u4f53\u7684\u5c42\u6b21\u56e0\u679c\u4ea4\u4e92\uff0c\u5e76\u6269\u5c55\u7f6e\u4fe1\u9884\u6d4b\u81f3\u591a\u6807\u7b7eICD\u7f16\u7801\u4ee5\u6821\u51c6\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728MIMIC-III\u548cMIMIC-IV\u6570\u636e\u96c6\u4e0a\uff0cTHCM-CAL\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "THCM-CAL\u901a\u8fc7\u5c42\u6b21\u56e0\u679c\u5efa\u6a21\u548c\u7f6e\u4fe1\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "paper_title_zh": "THCM-CAL\uff1a\u57fa\u4e8e\u65f6\u95f4\u5c42\u6b21\u56e0\u679c\u5efa\u6a21\u4e0e\u7f6e\u4fe1\u6821\u51c6\u7684\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b", "abstract_zh": "\u81ea\u52a8\u5316\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u9700\u540c\u65f6\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u7ed3\u6784\u5316\u8bca\u65ad\u4ee3\u7801\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u7b14\u8bb0\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5355\u72ec\u5904\u7406\u8fd9\u4e9b\u6a21\u6001\u6216\u91c7\u7528\u7b80\u5355\u878d\u5408\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u89c2\u5bdf\u5f15\u53d1\u8bca\u65ad\u53ca\u8de8\u5165\u9662\u98ce\u9669\u4f20\u64ad\u7684\u5c42\u6b21\u56e0\u679c\u4ea4\u4e92\u3002\u672c\u6587\u63d0\u51faTHCM-CAL\uff0c\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5c42\u6b21\u56e0\u679c\u5efa\u6a21\u4e0e\u7f6e\u4fe1\u6821\u51c6\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u6784\u5efa\u591a\u6a21\u6001\u56e0\u679c\u56fe\uff0c\u8282\u70b9\u4ee3\u8868\u4e24\u79cd\u6a21\u6001\u7684\u4e34\u5e8a\u5b9e\u4f53\uff1a\u4ece\u7b14\u8bb0\u63d0\u53d6\u7684\u6587\u672c\u547d\u9898\u548c\u6620\u5c04\u4e3a\u6587\u672c\u63cf\u8ff0\u7684ICD\u4ee3\u7801\u3002\u901a\u8fc7\u5c42\u6b21\u56e0\u679c\u53d1\u73b0\uff0cTHCM-CAL\u63a8\u65ad\u4e09\u79cd\u4e34\u5e8a\u4ea4\u4e92\uff1a\u540c\u6a21\u6001\u5e8f\u5217\u3001\u8de8\u6a21\u6001\u89e6\u53d1\u548c\u8de8\u5207\u7247\u98ce\u9669\u4f20\u64ad\u3002\u4e3a\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u6211\u4eec\u5c06\u7f6e\u4fe1\u9884\u6d4b\u6269\u5c55\u81f3\u591a\u6807\u7b7eICD\u7f16\u7801\uff0c\u6821\u51c6\u590d\u6742\u5171\u73b0\u4e0b\u7684\u6bcf\u4ee3\u7801\u7f6e\u4fe1\u533a\u95f4\u3002\u5728MIMIC-III\u548cMIMIC-IV\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86THCM-CAL\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.17632", "pdf": "https://arxiv.org/pdf/2506.17632", "abs": "https://arxiv.org/abs/2506.17632", "authors": ["Hangcheng Liu", "Xu Kuang", "Xingshuo Han", "Xingwan Wu", "Haoran Ou", "Shangwei Guo", "Xingyi Huang", "Tao Xiang", "Tianwei Zhang"], "title": "Optimization-Free Patch Attack on Stereo Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Stereo Depth Estimation (SDE) is essential for scene understanding in\nvision-based systems like autonomous driving. However, recent studies show that\nSDE models are vulnerable to adversarial attacks, which are often limited to\nunrealistic settings, e.g., digital perturbations on separate stereo views in\nstatic scenes, restricting their real-world applicability. This raises a\ncritical question: how can we design physically realizable, scene-adaptive, and\ntransferable attacks against SDE under realistic constraints?\n  To answer this, we make two key contributions. First, we propose a unified\nattack framework that extends optimization-based techniques to four core stages\nof stereo matching: feature extraction, cost-volume construction, cost\naggregation, and disparity regression. A comprehensive stage-wise evaluation\nacross 9 mainstream SDE models, under constraints like photometric consistency,\nreveals that optimization-based patches suffer from poor transferability.\nInterestingly, partially transferable patches suggest that patterns, rather\nthan pixel-level perturbations, may be key to generalizable attacks. Motivated\nby this, we present PatchHunter, the first optimization-free adversarial patch\nattack against SDE. PatchHunter formulates patch generation as a reinforcement\nlearning-driven search over a structured space of visual patterns crafted to\ndisrupt SDE assumptions.\n  We validate PatchHunter across three levels: the KITTI dataset, the CARLA\nsimulator, and real-world vehicle deployment. PatchHunter not only surpasses\noptimization-based methods in effectiveness but also achieves significantly\nbetter black-box transferability. Even under challenging physical conditions\nlike low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),\nwhereas optimization-based methods fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f18\u5316\u7684\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u65b9\u6cd5PatchHunter\uff0c\u9488\u5bf9\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff08SDE\uff09\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff08SDE\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u89c6\u89c9\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5c40\u9650\u4e8e\u4e0d\u73b0\u5b9e\u7684\u573a\u666f\uff08\u5982\u9759\u6001\u573a\u666f\u7684\u6570\u5b57\u6270\u52a8\uff09\uff0c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u7269\u7406\u53ef\u5b9e\u73b0\u3001\u573a\u666f\u81ea\u9002\u5e94\u4e14\u53ef\u8fc1\u79fb\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u653b\u51fb\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u4f18\u5316\u7684\u6280\u672f\u6269\u5c55\u5230\u7acb\u4f53\u5339\u914d\u7684\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\u3002\u968f\u540e\uff0c\u63d0\u51faPatchHunter\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4f18\u5316\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u641c\u7d22\u89c6\u89c9\u6a21\u5f0f\u7a7a\u95f4\u6765\u7834\u574fSDE\u5047\u8bbe\u3002", "result": "PatchHunter\u5728KITTI\u6570\u636e\u96c6\u3001CARLA\u6a21\u62df\u5668\u548c\u771f\u5b9e\u8f66\u8f86\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4f4e\u5149\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "PatchHunter\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u8fd8\u5c55\u793a\u4e86\u66f4\u5f3a\u7684\u9ed1\u76d2\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3a\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u65e0\u9700\u4f18\u5316\u7684\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u5bf9\u6297\u8865\u4e01\u653b\u51fb", "abstract_zh": "\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff08SDE\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u89c6\u89c9\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660eSDE\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u4e14\u8fd9\u4e9b\u653b\u51fb\u901a\u5e38\u5c40\u9650\u4e8e\u4e0d\u73b0\u5b9e\u7684\u573a\u666f\uff08\u5982\u9759\u6001\u573a\u666f\u4e2d\u5206\u79bb\u7acb\u4f53\u89c6\u56fe\u7684\u6570\u5b57\u6270\u52a8\uff09\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5982\u4f55\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u8bbe\u8ba1\u7269\u7406\u53ef\u5b9e\u73b0\u3001\u573a\u666f\u81ea\u9002\u5e94\u4e14\u53ef\u8fc1\u79fb\u7684\u653b\u51fb\u65b9\u6cd5\uff1f\n\n\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u9879\u5173\u952e\u8d21\u732e\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u653b\u51fb\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u4f18\u5316\u7684\u6280\u672f\u6269\u5c55\u5230\u7acb\u4f53\u5339\u914d\u7684\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a\u7279\u5f81\u63d0\u53d6\u3001\u6210\u672c\u4f53\u79ef\u6784\u5efa\u3001\u6210\u672c\u805a\u5408\u548c\u89c6\u5dee\u56de\u5f52\u3002\u901a\u8fc7\u5bf99\u79cd\u4e3b\u6d41SDE\u6a21\u578b\u7684\u9636\u6bb5\u8bc4\u4f30\uff08\u5728\u5149\u5ea6\u4e00\u81f4\u6027\u7b49\u7ea6\u675f\u4e0b\uff09\uff0c\u53d1\u73b0\u57fa\u4e8e\u4f18\u5316\u7684\u8865\u4e01\u53ef\u8fc1\u79fb\u6027\u8f83\u5dee\u3002\u6709\u8da3\u7684\u662f\uff0c\u90e8\u5206\u53ef\u8fc1\u79fb\u7684\u8865\u4e01\u8868\u660e\uff0c\u6a21\u5f0f\u800c\u975e\u50cf\u7d20\u7ea7\u6270\u52a8\u53ef\u80fd\u662f\u5b9e\u73b0\u901a\u7528\u653b\u51fb\u7684\u5173\u952e\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PatchHunter\uff0c\u9996\u4e2a\u9488\u5bf9SDE\u7684\u65e0\u4f18\u5316\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u65b9\u6cd5\u3002PatchHunter\u5c06\u8865\u4e01\u751f\u6210\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u641c\u7d22\uff0c\u901a\u8fc7\u8bbe\u8ba1\u89c6\u89c9\u6a21\u5f0f\u7a7a\u95f4\u6765\u7834\u574fSDE\u5047\u8bbe\u3002\n\n\u6211\u4eec\u5728\u4e09\u4e2a\u5c42\u9762\u9a8c\u8bc1\u4e86PatchHunter\uff1aKITTI\u6570\u636e\u96c6\u3001CARLA\u6a21\u62df\u5668\u548c\u771f\u5b9e\u8f66\u8f86\u90e8\u7f72\u3002PatchHunter\u4e0d\u4ec5\u5728\u653b\u51fb\u6548\u679c\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u8fd8\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u597d\u7684\u9ed1\u76d2\u53ef\u8fc1\u79fb\u6027\u3002\u5373\u4f7f\u5728\u4f4e\u5149\u7b49\u6311\u6218\u6027\u7269\u7406\u6761\u4ef6\u4e0b\uff0cPatchHunter\u4ecd\u80fd\u4fdd\u6301\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff08\u5982D1-all > 0.4\uff09\uff0c\u800c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5219\u5931\u6548\u3002"}}
{"id": "2506.18158", "pdf": "https://arxiv.org/pdf/2506.18158", "abs": "https://arxiv.org/abs/2506.18158", "authors": ["Xinzge Gao", "Chuanrui Hu", "Bin Chen", "Teng Li"], "title": "Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) are attracting growing attention in\nthe development of Graphical User Interface (GUI) agents. Existing approaches\noften rely on historical screenshots or actions to implicitly represent the\ntask state. This reliance poses challenges for GUI agents in accurately\nunderstanding task states and underscores the absence of effective mechanisms\nto store critical information in complex and lengthy cross-app tasks. To\naddress these challenges, we propose Chain-of-Memory (CoM), a novel approach\nfor explicitly modeling short-term and long-term memory in GUI agents. CoM\nachieves this by capturing action descriptions, integrating task-relevant\nscreen information, and maintaining a dedicated memory module to store and\nmanage this information. By leveraging explicit memory representations, CoM\nenables GUI agents to better understand task states and retain critical\nhistorical information persistently. To equip GUI agents with memory management\ncapabilities and evaluate the effectiveness of CoM, we developed the GUI\nOdyssey-CoM, a dataset comprising 111k screen-action pairs annotated with\nChain-of-Memory. Experimental results demonstrate that CoM significantly\nimproves GUI agents' performance in cross-application tasks. Additionally, GUI\nOdyssey-CoM enables 7B models to achieve memory management capabilities\ncomparable to 72B models. The dataset and code will be open-sourced.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Memory (CoM)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\uff0c\u63d0\u5347GUI\u4ee3\u7406\u5728\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660eCoM\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u4f7f\u5f977B\u6a21\u578b\u5177\u5907\u4e0e72B\u6a21\u578b\u76f8\u5f53\u7684\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684GUI\u4ee3\u7406\u65b9\u6cd5\u4f9d\u8d56\u5386\u53f2\u622a\u56fe\u6216\u52a8\u4f5c\u9690\u5f0f\u8868\u793a\u4efb\u52a1\u72b6\u6001\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u4efb\u52a1\u72b6\u6001\u4e14\u7f3a\u4e4f\u6709\u6548\u4fe1\u606f\u5b58\u50a8\u673a\u5236\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86CoM\u65b9\u6cd5\u3002", "method": "CoM\u901a\u8fc7\u6355\u83b7\u52a8\u4f5c\u63cf\u8ff0\u3001\u6574\u5408\u4efb\u52a1\u76f8\u5173\u5c4f\u5e55\u4fe1\u606f\uff0c\u5e76\u7ef4\u62a4\u4e13\u7528\u8bb0\u5fc6\u6a21\u5757\u6765\u663e\u5f0f\u5efa\u6a21\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u5305\u542b11.1\u4e07\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u7684GUI Odyssey-CoM\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30CoM\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoM\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4f7f7B\u6a21\u578b\u8fbe\u5230\u4e0e72B\u6a21\u578b\u76f8\u5f53\u7684\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u3002", "conclusion": "CoM\u901a\u8fc7\u663e\u5f0f\u8bb0\u5fc6\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u72b6\u6001\u7406\u89e3\u548c\u4fe1\u606f\u5b58\u50a8\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002", "paper_title_zh": "\u8bb0\u5fc6\u94fe\uff1a\u589e\u5f3a\u8de8\u5e94\u7528\u5bfc\u822a\u7684GUI\u4ee3\u7406", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u5f00\u53d1\u4e2d\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5386\u53f2\u622a\u56fe\u6216\u52a8\u4f5c\u9690\u5f0f\u8868\u793a\u4efb\u52a1\u72b6\u6001\uff0c\u8fd9\u5bfc\u81f4GUI\u4ee3\u7406\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u4efb\u52a1\u72b6\u6001\uff0c\u5e76\u51f8\u663e\u4e86\u5728\u590d\u6742\u4e14\u5197\u957f\u7684\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u6709\u6548\u4fe1\u606f\u5b58\u50a8\u673a\u5236\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bb0\u5fc6\u94fe\uff08Chain-of-Memory, CoM\uff09\uff0c\u4e00\u79cd\u663e\u5f0f\u5efa\u6a21GUI\u4ee3\u7406\u4e2d\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u65b0\u65b9\u6cd5\u3002CoM\u901a\u8fc7\u6355\u83b7\u52a8\u4f5c\u63cf\u8ff0\u3001\u6574\u5408\u4efb\u52a1\u76f8\u5173\u5c4f\u5e55\u4fe1\u606f\uff0c\u5e76\u7ef4\u62a4\u4e13\u7528\u8bb0\u5fc6\u6a21\u5757\u6765\u5b58\u50a8\u548c\u7ba1\u7406\u8fd9\u4e9b\u4fe1\u606f\u3002\u901a\u8fc7\u5229\u7528\u663e\u5f0f\u8bb0\u5fc6\u8868\u793a\uff0cCoM\u4f7fGUI\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u4efb\u52a1\u72b6\u6001\u5e76\u6301\u4e45\u4fdd\u7559\u5173\u952e\u5386\u53f2\u4fe1\u606f\u3002\u4e3a\u8d4b\u4e88GUI\u4ee3\u7406\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u5e76\u8bc4\u4f30CoM\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5f00\u53d1\u4e86GUI Odyssey-CoM\u6570\u636e\u96c6\uff0c\u5305\u542b11.1\u4e07\u6761\u6807\u6ce8\u4e86\u8bb0\u5fc6\u94fe\u7684\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoM\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cGUI Odyssey-CoM\u4f7f7B\u6a21\u578b\u8fbe\u5230\u4e86\u4e0e72B\u6a21\u578b\u76f8\u5f53\u7684\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.17863", "pdf": "https://arxiv.org/pdf/2506.17863", "abs": "https://arxiv.org/abs/2506.17863", "authors": ["Haoran Liu", "Amir Tahmasbi", "Ehtesham Sam Haque", "Purak Jain"], "title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "categories": ["cs.CL"], "comment": "KDD LLM4ECommerce Workshop 2025", "summary": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMarketingFM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6570\u636e\u6e90\u751f\u6210\u5b9a\u5236\u5316\u5e7f\u544a\u6587\u6848\uff0c\u663e\u8457\u63d0\u5347\u70b9\u51fb\u7387\u548c\u6210\u672c\u6548\u7387\u3002\u540c\u65f6\uff0c\u5f00\u53d1AutoEval-Main\u548cAutoEval-Update\u7cfb\u7edf\uff0c\u7ed3\u5408\u89c4\u5219\u4e0eLLM\u6280\u672f\uff0c\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u6210\u672c\uff0c\u4fdd\u6301\u9ad8\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7535\u5546\u5916\u90e8\u8425\u9500\u5185\u5bb9\u8fc7\u4e8e\u6a21\u677f\u5316\uff0c\u4e0e\u843d\u5730\u9875\u5339\u914d\u5ea6\u4f4e\uff0c\u6548\u679c\u4e0d\u4f73\u3002\u9700\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u7edf\u751f\u6210\u7cbe\u51c6\u5e7f\u544a\u6587\u6848\uff0c\u5e76\u89e3\u51b3\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "1. \u63d0\u51faMarketingFM\u7cfb\u7edf\uff0c\u6574\u5408\u591a\u6570\u636e\u6e90\u751f\u6210\u5173\u952e\u8bcd\u5b9a\u5236\u5e7f\u544a\u6587\u6848\u30022. \u5f00\u53d1AutoEval-Main\uff0c\u7ed3\u5408\u89c4\u5219\u4e0eLLM\u6280\u672f\u81ea\u52a8\u8bc4\u4f30\u5e7f\u544a\u8d28\u91cf\u30023. \u63d0\u51faAutoEval-Update\u6846\u67b6\uff0c\u52a8\u6001\u4f18\u5316\u8bc4\u4f30\u63d0\u793a\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "result": "MarketingFM\u4f7f\u5e7f\u544a\u70b9\u51fb\u7387\u63d0\u53479%\uff0c\u5c55\u793a\u91cf\u589e\u52a012%\uff0c\u5355\u6b21\u70b9\u51fb\u6210\u672c\u964d\u4f4e0.38%\u3002AutoEval-Main\u4e0e\u4eba\u5de5\u8bc4\u4f30\u4e00\u81f4\u7387\u8fbe89.57%\uff0cAutoEval-Update\u8fdb\u4e00\u6b65\u4f18\u5316\u8bc4\u4f30\u4e00\u81f4\u6027\u3002", "conclusion": "\u81ea\u52a8\u5316\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u5e7f\u544a\u6548\u679c\u548c\u8bc4\u4f30\u6548\u7387\uff0c\u4f46\u4eba\u5de5\u76d1\u7763\u4ecd\u4e3a\u5173\u952e\uff0c\u9700\u8bbe\u5b9a\u9608\u503c\u5e76\u9a8c\u8bc1\u6539\u8fdb\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\u5316\u5b9a\u5236\u8425\u9500\u5185\u5bb9\u751f\u6210\u4e0e\u8bc4\u4f30", "abstract_zh": "\u5916\u90e8\u8425\u9500\u5728\u7535\u5546\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u5185\u5bb9\u8fc7\u4e8e\u6a21\u677f\u5316\u4e14\u4e0e\u843d\u5730\u9875\u5339\u914d\u5ea6\u4f4e\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faMarketingFM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6570\u636e\u6e90\u751f\u6210\u5173\u952e\u8bcd\u5b9a\u5236\u5e7f\u544a\u6587\u6848\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002\u901a\u8fc7\u7ebf\u4e0b\u548c\u7ebf\u4e0a\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u5173\u952e\u8bcd\u5b9a\u5236\u6587\u6848\u70b9\u51fb\u7387\u63d0\u53479%\uff0c\u5c55\u793a\u91cf\u589e\u52a012%\uff0c\u5355\u6b21\u70b9\u51fb\u6210\u672c\u964d\u4f4e0.38%\u3002\u4e3a\u89e3\u51b3\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86AutoEval-Main\u7cfb\u7edf\uff0c\u7ed3\u5408\u89c4\u5219\u4e0eLLM\u6280\u672f\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u4e00\u81f4\u7387\u8fbe89.57%\u3002\u8fdb\u4e00\u6b65\u63d0\u51faAutoEval-Update\u6846\u67b6\uff0c\u52a8\u6001\u4f18\u5316\u8bc4\u4f30\u63d0\u793a\uff0c\u51cf\u5c11\u4eba\u5de5\u8f93\u5165\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u4f46\u4eba\u5de5\u76d1\u7763\u4ecd\u4e0d\u53ef\u6216\u7f3a\u3002"}}
{"id": "2506.17633", "pdf": "https://arxiv.org/pdf/2506.17633", "abs": "https://arxiv.org/abs/2506.17633", "authors": ["Xiang Fang", "Arvind Easwaran", "Blaise Genest"], "title": "Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection", "categories": ["cs.CV", "cs.AI"], "comment": "ICML 2025", "summary": "Out-of-distribution (OOD) detection attempts to distinguish outlier samples\nto prevent models trained on the in-distribution (ID) dataset from producing\nunavailable outputs. Most OOD detection methods require many IID samples for\ntraining, which seriously limits their real-world applications. To this end, we\ntarget a challenging setting: few-shot OOD detection, where {Only a few {\\em\nlabeled ID} samples are available.} Therefore, few-shot OOD detection is much\nmore challenging than the traditional OOD detection setting. Previous few-shot\nOOD detection works ignore the distinct diversity between different classes. In\nthis paper, we propose a novel network: Adaptive Multi-prompt Contrastive\nNetwork (AMCN), which adapts the ID-OOD separation boundary by learning inter-\nand intra-class distribution. To compensate for the absence of OOD and scarcity\nof ID {\\em image samples}, we leverage CLIP, connecting text with images,\nengineering learnable ID and OOD {\\em textual prompts}. Specifically, we first\ngenerate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and\nlabel-adaptive OOD prompts). Then, we generate an adaptive class boundary for\neach class by introducing a class-wise threshold. Finally, we propose a\nprompt-guided ID-OOD separation module to control the margin between ID and OOD\nprompts. Experimental results show that AMCN outperforms other state-of-the-art\nworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u63d0\u793a\u5bf9\u6bd4\u7f51\u7edc\uff08AMCN\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5c11\u6837\u672c\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u4e0e\u56fe\u50cf\u7684CLIP\u6a21\u578b\uff0c\u751f\u6210\u81ea\u9002\u5e94\u63d0\u793a\u5e76\u5b66\u4e60\u7c7b\u95f4\u548c\u7c7b\u5185\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672cOOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u800c\u5c11\u6837\u672cOOD\u68c0\u6d4b\u66f4\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0d\u540c\u7c7b\u522b\u95f4\u7684\u591a\u6837\u6027\u5dee\u5f02\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574ID-OOD\u5206\u79bb\u8fb9\u754c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "AMCN\u901a\u8fc7CLIP\u6a21\u578b\u8fde\u63a5\u6587\u672c\u4e0e\u56fe\u50cf\uff0c\u751f\u6210\u53ef\u5b66\u4e60\u7684ID\u63d0\u793a\u548c\u56fa\u5b9a/\u81ea\u9002\u5e94OOD\u63d0\u793a\u3002\u5f15\u5165\u7c7b\u95f4\u9608\u503c\u751f\u6210\u81ea\u9002\u5e94\u7c7b\u522b\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u63d0\u793a\u5f15\u5bfc\u7684ID-OOD\u5206\u79bb\u6a21\u5757\u4ee5\u63a7\u5236ID\u4e0eOOD\u63d0\u793a\u7684\u95f4\u9694\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAMCN\u5728\u5c11\u6837\u672cOOD\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "AMCN\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u548c\u7c7b\u95f4\u5206\u5e03\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672cOOD\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u81ea\u9002\u5e94\u591a\u63d0\u793a\u5bf9\u6bd4\u7f51\u7edc\u7528\u4e8e\u5c11\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b", "abstract_zh": "\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u65e8\u5728\u533a\u5206\u5f02\u5e38\u6837\u672c\uff0c\u4ee5\u9632\u6b62\u5728\u5206\u5e03\u5185\uff08ID\uff09\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u8f93\u51fa\u3002\u5927\u591a\u6570OOD\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5927\u91cfID\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u4e2a\u66f4\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\uff1a\u5c11\u6837\u672cOOD\u68c0\u6d4b\uff0c\u5176\u4e2d\u4ec5\u63d0\u4f9b\u5c11\u91cf\u6807\u8bb0\u7684ID\u6837\u672c\u3002\u56e0\u6b64\uff0c\u5c11\u6837\u672cOOD\u68c0\u6d4b\u6bd4\u4f20\u7edfOOD\u68c0\u6d4b\u66f4\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u5c11\u6837\u672cOOD\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0d\u540c\u7c7b\u522b\u95f4\u7684\u591a\u6837\u6027\u5dee\u5f02\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7f51\u7edc\uff1a\u81ea\u9002\u5e94\u591a\u63d0\u793a\u5bf9\u6bd4\u7f51\u7edc\uff08AMCN\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u7c7b\u95f4\u548c\u7c7b\u5185\u5206\u5e03\u81ea\u9002\u5e94\u8c03\u6574ID-OOD\u5206\u79bb\u8fb9\u754c\u3002\u4e3a\u4e86\u5f25\u8865OOD\u6837\u672c\u7684\u7f3a\u5931\u548cID\u56fe\u50cf\u6837\u672c\u7684\u7a00\u7f3a\u6027\uff0c\u6211\u4eec\u5229\u7528CLIP\u6a21\u578b\u8fde\u63a5\u6587\u672c\u4e0e\u56fe\u50cf\uff0c\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684ID\u548cOOD\u6587\u672c\u63d0\u793a\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u751f\u6210\u81ea\u9002\u5e94\u63d0\u793a\uff08\u53ef\u5b66\u4e60\u7684ID\u63d0\u793a\u3001\u56fa\u5b9a\u6807\u7b7e\u7684OOD\u63d0\u793a\u548c\u81ea\u9002\u5e94\u6807\u7b7e\u7684OOD\u63d0\u793a\uff09\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u95f4\u9608\u503c\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u751f\u6210\u81ea\u9002\u5e94\u7c7b\u522b\u8fb9\u754c\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u5f15\u5bfc\u7684ID-OOD\u5206\u79bb\u6a21\u5757\uff0c\u4ee5\u63a7\u5236ID\u4e0eOOD\u63d0\u793a\u4e4b\u95f4\u7684\u95f4\u9694\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAMCN\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.18183", "pdf": "https://arxiv.org/pdf/2506.18183", "abs": "https://arxiv.org/abs/2506.18183", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u5c24\u5176\u662f\u9519\u8bef\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u5e38\u8d85\u8fc785%\u3002\u901a\u8fc7\u5f15\u5165\u81ea\u7701\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\uff0c\u7814\u7a76\u53d1\u73b0\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u4f1a\u52a0\u5267\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u90e8\u5206\u6a21\u578b\u53ef\u901a\u8fc7\u81ea\u7701\u6539\u5584\u6821\u51c6\u3002", "motivation": "\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u751f\u6210\u7684\u7b54\u6848\u53ef\u80fd\u9519\u8bef\u4e14\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4e3a\u786e\u4fdd\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u6821\u51c6\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u7684\u5f71\u54cd\u4ee5\u53ca\u81ea\u7701\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u81ea\u7701\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u591a\u4e2a\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u6a21\u578b\u7684\u6821\u51c6\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u5bf9\u7f6e\u4fe1\u5ea6\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u81ea\u7701\u5bf9\u6821\u51c6\u7684\u6539\u8fdb\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\u63a8\u7406\u6a21\u578b\u901a\u5e38\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u9519\u8bef\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u5e38\u8d85\u8fc785%\uff1b\uff082\uff09\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u4f1a\u8fdb\u4e00\u6b65\u52a0\u5267\u8fc7\u5ea6\u81ea\u4fe1\uff1b\uff083\uff09\u90e8\u5206\u6a21\u578b\u901a\u8fc7\u81ea\u7701\u6539\u5584\u4e86\u6821\u51c6\u6027\uff08\u5982o3-Mini\u548cDeepSeek R1\uff09\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6a21\u578b\u5747\u5982\u6b64\uff08\u5982Claude 3.7 Sonnet\u6821\u51c6\u6027\u53cd\u800c\u53d8\u5dee\uff09\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u57fa\u51c6\u548c\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u6821\u51c6\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "\u5173\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u63a8\u7406\uff1a\u63a8\u7406\u6a21\u578b\u662f\u5426\u77e5\u9053\u5b83\u4eec\u4e0d\u77e5\u9053\uff1f", "abstract_zh": "\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u4e86\u6700\u5148\u8fdb\u7684\u8bb0\u5f55\uff0c\u8fd9\u5f97\u76ca\u4e8e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bf1\u5bfc\u7684\u591a\u6b65\u63a8\u7406\u3002\u7136\u800c\uff0c\u4e0e\u4e4b\u524d\u7684\u8bed\u8a00\u6a21\u578b\u7c7b\u4f3c\uff0c\u63a8\u7406\u6a21\u578b\u5bb9\u6613\u751f\u6210\u81ea\u4fe1\u4f46\u9519\u8bef\u7684\u56de\u7b54\uff08\u5e7b\u89c9\uff09\u3002\u4e86\u89e3\u4f55\u65f6\u4ee5\u53ca\u591a\u5927\u7a0b\u5ea6\u4e0a\u4fe1\u4efb\u8fd9\u4e9b\u6a21\u578b\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u63a8\u7406\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u9996\u5148\uff0c\u63a8\u7406\u6a21\u578b\u662f\u5426\u6821\u51c6\u826f\u597d\uff1f\u5176\u6b21\uff0c\u66f4\u6df1\u7684\u63a8\u7406\u662f\u5426\u80fd\u6539\u5584\u6a21\u578b\u6821\u51c6\uff1f\u6700\u540e\uff0c\u53d7\u4eba\u7c7b\u5929\u751f\u80fd\u591f\u901a\u8fc7\u53cc\u91cd\u68c0\u67e5\u601d\u7ef4\u8fc7\u7a0b\u6765\u9a8c\u8bc1\u7b54\u6848\u53ca\u5176\u4fe1\u5fc3\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\uff1a\u63a8\u7406\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u5176\u601d\u7ef4\u94fe\u75d5\u8ff9\u6765\u6539\u5584\u6821\u51c6\uff1f\u6211\u4eec\u5f15\u5165\u4e86\u81ea\u7701\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u6765\u63a2\u7d22\u8fd9\u4e00\u65b9\u5411\u3002\u5728\u5bf9\u591a\u4e2a\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u63a8\u7406\u6a21\u578b\uff1a\uff08i\uff09\u901a\u5e38\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5c24\u5176\u662f\u9519\u8bef\u7b54\u6848\u7684\u81ea\u8ff0\u7f6e\u4fe1\u5ea6\u5e38\u8d85\u8fc785%\uff1b\uff08ii\uff09\u968f\u7740\u63a8\u7406\u6df1\u5ea6\u589e\u52a0\uff0c\u8fc7\u5ea6\u81ea\u4fe1\u8fdb\u4e00\u6b65\u52a0\u5267\uff1b\uff08iii\uff09\u90e8\u5206\u6a21\u578b\u53ef\u901a\u8fc7\u81ea\u7701\u6539\u5584\u6821\u51c6\u6027\uff08\u5982o3-Mini\u548cDeepSeek R1\uff09\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6a21\u578b\u5747\u5982\u6b64\uff08\u5982Claude 3.7 Sonnet\u6821\u51c6\u6027\u53cd\u800c\u53d8\u5dee\uff09\u3002\u6700\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u8bbe\u8ba1\u5fc5\u8981\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u57fa\u51c6\u548c\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u6821\u51c6\u6027\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.17864", "pdf": "https://arxiv.org/pdf/2506.17864", "abs": "https://arxiv.org/abs/2506.17864", "authors": ["Taolin Zhang", "Haidong Kang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang Xiaofeng He", "Richang Hong"], "title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u961f\u5217\u7684\u81ea\u6821\u6b63\u6846\u67b6QueueEDIT\uff0c\u7528\u4e8e\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fde\u7eed\u7f16\u8f91\u4e2d\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5bf9\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3002\u8fde\u7eed\u6a21\u578b\u7f16\u8f91\uff08SME\uff09\u65e8\u5728\u6301\u7eed\u4fee\u6b63\u9519\u8bef\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u56e0\u5f15\u5165\u65b0\u53c2\u6570\u800c\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "QueueEDIT\u901a\u8fc7\u7ed3\u6784\u6620\u5c04\u7f16\u8f91\u635f\u5931\u5c06\u77e5\u8bc6\u4e09\u5143\u7ec4\u6620\u5c04\u5230LLMs\u7684Transformer\u5c42\u4e2d\u7684\u77e5\u8bc6\u654f\u611f\u795e\u7ecf\u5143\uff0c\u5e76\u5c06\u7f16\u8f91\u53c2\u6570\u5b58\u50a8\u5728\u961f\u5217\u4e2d\u52a8\u6001\u5bf9\u9f50\u3002\u6bcf\u6b21\u7f16\u8f91\u65f6\u9009\u62e9\u76f8\u5173\u961f\u5217\u53c2\u6570\u8fdb\u884c\u77e5\u8bc6\u5bf9\u9f50\uff0c\u51bb\u7ed3\u65e0\u5173\u53c2\u6570\u5e76\u66f4\u65b0\u961f\u5217\u5934\u90e8\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQueueEDIT\u5728\u591a\u79cdSME\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u5355\u6b21\u7f16\u8f91\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u5728\u6574\u4e2aSME\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e86LLMs\u5728\u901a\u7528NLP\u4efb\u52a1\u4e2d\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "QueueEDIT\u6709\u6548\u63d0\u5347\u4e86\u8fde\u7eed\u6a21\u578b\u7f16\u8f91\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u504f\u5dee\u5bf9LLMs\u901a\u7528\u80fd\u529b\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "paper_title_zh": "QueueEDIT\uff1aLLMs\u8fde\u7eed\u6a21\u578b\u7f16\u8f91\u4e2d\u7684\u7ed3\u6784\u81ea\u6821\u6b63\u65b9\u6cd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3002\u6a21\u578b\u7f16\u8f91\u88ab\u63d0\u51fa\u7528\u4e8e\u4fee\u6b63LLMs\u4e2d\u7684\u4e8b\u5b9e\u9519\u8bef\u3002\u8fde\u7eed\u6a21\u578b\u7f16\u8f91\uff08SME\uff09\u65e8\u5728\u6301\u7eed\u4fee\u6b63\u9519\u8bef\uff0c\u800c\u975e\u4e00\u6b21\u6027\u4efb\u52a1\u3002\u5728SME\u8fc7\u7a0b\u4e2d\uff0c\u65b0\u53c2\u6570\u7684\u5f15\u5165\u53ef\u80fd\u5bf9LLMs\u7684\u901a\u7528\u80fd\u529b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u961f\u5217\u7684\u81ea\u6821\u6b63\u6846\u67b6\uff08QueueEDIT\uff09\uff0c\u4e0d\u4ec5\u901a\u8fc7\u89e3\u51b3\u957f\u5e8f\u5217\u4f9d\u8d56\u95ee\u9898\u63d0\u5347\u4e86SME\u6027\u80fd\uff0c\u8fd8\u7f13\u89e3\u4e86\u53c2\u6570\u504f\u5dee\u5bf9LLMs\u901a\u7528\u80fd\u529b\u7684\u5f71\u54cd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u7ed3\u6784\u6620\u5c04\u7f16\u8f91\u635f\u5931\uff0c\u5c06\u77e5\u8bc6\u4e09\u5143\u7ec4\u6620\u5c04\u5230LLMs\u7684Transformer\u5c42\u4e2d\u7684\u77e5\u8bc6\u654f\u611f\u795e\u7ecf\u5143\u3002\u968f\u540e\uff0c\u5c06\u6bcf\u6bb5\u7f16\u8f91\u77e5\u8bc6\u7684\u5b9a\u4f4d\u53c2\u6570\u5b58\u50a8\u5728\u961f\u5217\u4e2d\uff0c\u5e76\u52a8\u6001\u5bf9\u9f50\u5148\u524d\u7f16\u8f91\u7684\u53c2\u6570\u3002\u6bcf\u6b21\u7f16\u8f91\u65f6\uff0c\u9009\u62e9\u4e0e\u5f53\u524d\u5b9a\u4f4d\u53c2\u6570\u6700\u76f8\u5173\u7684\u961f\u5217\u53c2\u6570\u4ee5\u5224\u65ad\u662f\u5426\u9700\u8981\u91cd\u65b0\u5bf9\u9f50\u77e5\u8bc6\u3002\u961f\u5217\u4e2d\u65e0\u5173\u7684\u53c2\u6570\u88ab\u51bb\u7ed3\uff0c\u540c\u65f6\u66f4\u65b0\u961f\u5217\u5934\u90e8\u53c2\u6570\u5230LLM\u4e2d\uff0c\u4ee5\u786e\u4fdd\u5176\u4e0d\u635f\u5bb3\u901a\u7528\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u591a\u79cdSME\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u5355\u6b21\u7f16\u8f91\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u751f\u6210\u7684LLMs\u5728\u6574\u4e2aSME\u8fc7\u7a0b\u4e2d\u4e5f\u4fdd\u6301\u4e86\u5728\u901a\u7528NLP\u4efb\u52a1\u4e2d\u7684\u9ad8\u80fd\u529b\u3002"}}
{"id": "2506.17645", "pdf": "https://arxiv.org/pdf/2506.17645", "abs": "https://arxiv.org/abs/2506.17645", "authors": ["Shih-Wen Liu", "Hsuan-Yu Fan", "Wei-Ta Chu", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning", "categories": ["cs.CV"], "comment": "Accepted to MIDL 2025", "summary": "Automating medical report generation from histopathology images is a critical\nchallenge requiring effective visual representations and domain-specific\nknowledge. Inspired by the common practices of human experts, we propose an\nin-context learning framework called PathGenIC that integrates context derived\nfrom the training set with a multimodal in-context learning (ICL) mechanism.\nOur method dynamically retrieves semantically similar whole slide image\n(WSI)-report pairs and incorporates adaptive feedback to enhance contextual\nrelevance and generation quality. Evaluated on the HistGen benchmark, the\nframework achieves state-of-the-art results, with significant improvements\nacross BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across\ndiverse report lengths and disease categories. By maximizing training data\nutility and bridging vision and language with ICL, our work offers a solution\nfor AI-driven histopathology reporting, setting a strong foundation for future\nadvancements in multimodal clinical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPathGenIC\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u533b\u5b66\u62a5\u544a\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u76f8\u4f3c\u56fe\u50cf-\u62a5\u544a\u5bf9\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62a5\u544a\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u81ea\u52a8\u5316\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u751f\u6210\u533b\u5b66\u62a5\u544a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\u548c\u9886\u57df\u77e5\u8bc6\u3002\u53d7\u4eba\u7c7b\u4e13\u5bb6\u5b9e\u8df5\u7684\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7684PathGenIC\u6846\u67b6\u7ed3\u5408\u4e86\u8bad\u7ec3\u96c6\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u52a8\u6001\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u7684WSI-\u62a5\u544a\u5bf9\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u53cd\u9988\u589e\u5f3a\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728HistGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPathGenIC\u5728BLEU\u3001METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u5728\u4e0d\u540c\u62a5\u544a\u957f\u5ea6\u548c\u75be\u75c5\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6700\u5927\u5316\u8bad\u7ec3\u6570\u636e\u6548\u7528\u548c\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0cPathGenIC\u4e3aAI\u9a71\u52a8\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u4e34\u5e8a\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u62a5\u544a", "abstract_zh": "\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u533b\u5b66\u62a5\u544a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\u548c\u9886\u57df\u77e5\u8bc6\u3002\u53d7\u4eba\u7c7b\u4e13\u5bb6\u5b9e\u8df5\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPathGenIC\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8bad\u7ec3\u96c6\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u673a\u5236\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u52a8\u6001\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u7684\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09-\u62a5\u544a\u5bf9\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u53cd\u9988\u589e\u5f3a\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002\u5728HistGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u5728BLEU\u3001METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u4e0d\u540c\u62a5\u544a\u957f\u5ea6\u548c\u75be\u75c5\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u6700\u5927\u5316\u8bad\u7ec3\u6570\u636e\u6548\u7528\u548c\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3aAI\u9a71\u52a8\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u4e34\u5e8a\u5e94\u7528\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.18187", "pdf": "https://arxiv.org/pdf/2506.18187", "abs": "https://arxiv.org/abs/2506.18187", "authors": ["Shahriar Noroozizadeh", "Pim Welle", "Jeremy C. Weiss", "George H. Chen"], "title": "The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "Conference on Health, Inference, and Learning (CHIL 2025)", "summary": "This study quantifies the association between non-adherence to antipsychotic\nmedications and adverse outcomes in individuals with schizophrenia. We frame\nthe problem using survival analysis, focusing on the time to the earliest of\nseveral adverse events (early death, involuntary hospitalization, jail\nbooking). We extend standard causal inference methods (T-learner, S-learner,\nnearest neighbor matching) to utilize various survival models to estimate\nindividual and average treatment effects, where treatment corresponds to\nmedication non-adherence. Analyses are repeated using different amounts of\nlongitudinal information (3, 6, 9, and 12 months). Using data from Allegheny\nCounty in western Pennsylvania, we find strong evidence that non-adherence\nadvances adverse outcomes by approximately 1 to 4 months. Ablation studies\nconfirm that county-provided risk scores adjust for key confounders, as their\nremoval amplifies the estimated effects. Subgroup analyses by medication\nformulation (injectable vs. oral) and medication type consistently show that\nnon-adherence is associated with earlier adverse events. These findings\nhighlight the clinical importance of adherence in delaying psychiatric crises\nand show that integrating survival analysis with causal inference tools can\nyield policy-relevant insights. We caution that although we apply causal\ninference, we only make associative claims and discuss assumptions needed for\ncausal interpretation.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u751f\u5b58\u5206\u6790\u91cf\u5316\u4e86\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u672a\u6309\u65f6\u670d\u7528\u6297\u7cbe\u795e\u75c5\u836f\u7269\u4e0e\u4e0d\u826f\u540e\u679c\uff08\u5982\u65e9\u901d\u3001\u5f3a\u5236\u4f4f\u9662\u3001\u5165\u72f1\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u672a\u670d\u836f\u4f1a\u63d0\u524d1\u81f34\u4e2a\u6708\u5bfc\u81f4\u4e0d\u826f\u4e8b\u4ef6\u3002", "motivation": "\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u7684\u836f\u7269\u4f9d\u4ece\u6027\u5bf9\u4e34\u5e8a\u7ed3\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u4e0e\u4e0d\u826f\u540e\u679c\u5173\u7cfb\u7684\u91cf\u5316\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u4e34\u5e8a\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u751f\u5b58\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u5de5\u5177\uff08T-learner\u3001S-learner\u3001\u6700\u8fd1\u90bb\u5339\u914d\uff09\uff0c\u5229\u7528\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\uff083\u30016\u30019\u300112\u4e2a\u6708\uff09\u7684\u7eb5\u5411\u6570\u636e\uff0c\u4f30\u8ba1\u4e2a\u4f53\u548c\u5e73\u5747\u6cbb\u7597\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u672a\u670d\u836f\u4f1a\u4f7f\u4e0d\u826f\u4e8b\u4ef6\u63d0\u524d1\u81f34\u4e2a\u6708\u53d1\u751f\u3002\u4e9a\u7ec4\u5206\u6790\u663e\u793a\uff0c\u65e0\u8bba\u662f\u6ce8\u5c04\u8fd8\u662f\u53e3\u670d\u836f\u7269\uff0c\u672a\u670d\u836f\u5747\u4e0e\u4e0d\u826f\u4e8b\u4ef6\u63d0\u524d\u76f8\u5173\u3002", "conclusion": "\u836f\u7269\u4f9d\u4ece\u6027\u5bf9\u5ef6\u7f13\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u7684\u4e0d\u826f\u4e8b\u4ef6\u81f3\u5173\u91cd\u8981\u3002\u751f\u5b58\u5206\u6790\u4e0e\u56e0\u679c\u63a8\u65ad\u7ed3\u5408\u53ef\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\uff0c\u4f46\u9700\u6ce8\u610f\u56e0\u679c\u89e3\u91ca\u7684\u5047\u8bbe\u9650\u5236\u3002", "paper_title_zh": "\u836f\u7269\u672a\u4f9d\u4ece\u6027\u5bf9\u4e0d\u826f\u540e\u679c\u7684\u5f71\u54cd\uff1a\u57fa\u4e8e\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u7684\u751f\u5b58\u5206\u6790\u8bc1\u636e", "abstract_zh": "\u672c\u7814\u7a76\u91cf\u5316\u4e86\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u672a\u6309\u65f6\u670d\u7528\u6297\u7cbe\u795e\u75c5\u836f\u7269\u4e0e\u4e0d\u826f\u540e\u679c\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u901a\u8fc7\u751f\u5b58\u5206\u6790\u65b9\u6cd5\uff0c\u5173\u6ce8\u591a\u79cd\u4e0d\u826f\u4e8b\u4ef6\uff08\u65e9\u901d\u3001\u5f3a\u5236\u4f4f\u9662\u3001\u5165\u72f1\uff09\u7684\u6700\u65e9\u53d1\u751f\u65f6\u95f4\u3002\u6269\u5c55\u4e86\u6807\u51c6\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff08T-learner\u3001S-learner\u3001\u6700\u8fd1\u90bb\u5339\u914d\uff09\uff0c\u5229\u7528\u4e0d\u540c\u751f\u5b58\u6a21\u578b\u4f30\u8ba1\u4e2a\u4f53\u548c\u5e73\u5747\u6cbb\u7597\u6548\u679c\uff0c\u5176\u4e2d\u6cbb\u7597\u5bf9\u5e94\u836f\u7269\u672a\u4f9d\u4ece\u6027\u3002\u5206\u6790\u4f7f\u7528\u4e86\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\uff083\u30016\u30019\u300112\u4e2a\u6708\uff09\u7684\u7eb5\u5411\u6570\u636e\u3002\u57fa\u4e8e\u5bbe\u5915\u6cd5\u5c3c\u4e9a\u5dde\u963f\u52d2\u683c\u5c3c\u53bf\u7684\u6570\u636e\uff0c\u7814\u7a76\u53d1\u73b0\u672a\u670d\u836f\u4f1a\u4f7f\u4e0d\u826f\u4e8b\u4ef6\u63d0\u524d1\u81f34\u4e2a\u6708\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0c\u53bf\u63d0\u4f9b\u7684\u98ce\u9669\u8bc4\u5206\u8c03\u6574\u4e86\u5173\u952e\u6df7\u6742\u56e0\u7d20\uff0c\u79fb\u9664\u8fd9\u4e9b\u8bc4\u5206\u4f1a\u653e\u5927\u4f30\u8ba1\u6548\u679c\u3002\u4e9a\u7ec4\u5206\u6790\uff08\u6ce8\u5c04\u4e0e\u53e3\u670d\u836f\u7269\uff09\u4e00\u81f4\u8868\u660e\u672a\u670d\u836f\u4e0e\u4e0d\u826f\u4e8b\u4ef6\u63d0\u524d\u76f8\u5173\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u4f9d\u4ece\u6027\u5728\u5ef6\u7f13\u7cbe\u795e\u5371\u673a\u4e2d\u7684\u4e34\u5e8a\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u751f\u5b58\u5206\u6790\u4e0e\u56e0\u679c\u63a8\u65ad\u5de5\u5177\u7ed3\u5408\u53ef\u63d0\u4f9b\u653f\u7b56\u76f8\u5173\u89c1\u89e3\u3002\u9700\u6ce8\u610f\uff0c\u5c3d\u7ba1\u5e94\u7528\u4e86\u56e0\u679c\u63a8\u65ad\uff0c\u4f46\u7814\u7a76\u4ec5\u63d0\u51fa\u5173\u8054\u6027\u7ed3\u8bba\uff0c\u5e76\u8ba8\u8bba\u4e86\u56e0\u679c\u89e3\u91ca\u6240\u9700\u7684\u5047\u8bbe\u3002"}}
{"id": "2506.17871", "pdf": "https://arxiv.org/pdf/2506.17871", "abs": "https://arxiv.org/abs/2506.17871", "authors": ["Chenghao Yang", "Ari Holtzman"], "title": "How Alignment Shrinks the Generative Horizon", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Codebase: https://github.com/yangalan123/LLMBranchingFactor, Website:\n  https://yangalan123.github.io/branching_factor/", "summary": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u591a\u6837\u6027\u964d\u4f4e\uff0c\u539f\u56e0\u662f\u751f\u6210\u8fc7\u7a0b\u4e2d\u6982\u7387\u5206\u5e03\u96c6\u4e2d\u3002\u901a\u8fc7\u5f15\u5165\u5206\u652f\u56e0\u5b50\uff08BF\uff09\u91cf\u5316\u8fd9\u79cd\u96c6\u4e2d\uff0c\u53d1\u73b0\u5bf9\u9f50\u8c03\u6574\u663e\u8457\u964d\u4f4eBF\uff0c\u4f7f\u6a21\u578b\u66f4\u7a33\u5b9a\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u8f93\u51fa\u5f80\u5f80\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u7a33\u5b9a\u6027\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u91cf\u5316\u5176\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u5206\u652f\u56e0\u5b50\uff08BF\uff09\u4f5c\u4e3a\u91cf\u5316\u6307\u6807\uff0c\u8861\u91cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u6709\u6548\u4e0b\u4e00\u6b65\u7684\u6570\u91cf\u3002\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790BF\u7684\u53d8\u5316\u53ca\u5176\u4e0e\u5bf9\u9f50\u8c03\u6574\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "1. \u751f\u6210\u8fc7\u7a0b\u4e2dBF\u9010\u6e10\u964d\u4f4e\uff0c\u6a21\u578b\u53d8\u5f97\u66f4\u53ef\u9884\u6d4b\u30022. \u5bf9\u9f50\u8c03\u6574\u663e\u8457\u964d\u4f4eBF\uff08\u5982\u4ece12\u964d\u81f31.2\uff09\uff0c\u4f7f\u8f93\u51fa\u66f4\u7a33\u5b9a\u30023. \u5bf9\u9f50\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6a21\u578b\u901a\u8fc7\u751f\u6210\u957f\u63a8\u7406\u94fe\u8fdb\u5165\u4f4eBF\u9636\u6bb5\uff0c\u8f93\u51fa\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u5bf9\u9f50\u8c03\u6574\u5e76\u672a\u6539\u53d8\u6a21\u578b\u884c\u4e3a\uff0c\u800c\u662f\u5f15\u5bfc\u5176\u9009\u62e9\u4f4e\u71b5\u8def\u5f84\u3002BF\u662f\u7406\u89e3\u548c\u63a7\u5236LLM\u8f93\u51fa\u7684\u6709\u529b\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5bf9\u9f50\u5982\u4f55\u51cf\u5c11\u591a\u6837\u6027\u3001CoT\u5982\u4f55\u4fc3\u8fdb\u7a33\u5b9a\u751f\u6210\u3002", "paper_title_zh": "\u5bf9\u9f50\u5982\u4f55\u7f29\u5c0f\u751f\u6210\u89c6\u91ce", "abstract_zh": "\u5c3d\u7ba1\u80fd\u529b\u5f3a\u5927\uff0c\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u5f80\u5f80\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u672c\u6587\u901a\u8fc7\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u7684\u6982\u7387\u96c6\u4e2d\u73b0\u8c61\u63a2\u7a76\u8fd9\u4e00\u7a33\u5b9a\u6027\u3002\u4e3a\u91cf\u5316\u8fd9\u79cd\u96c6\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u5206\u652f\u56e0\u5b50\uff08BF\uff09\u2014\u2014\u4e00\u79cd\u8861\u91cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u6709\u6548\u4e0b\u4e00\u6b65\u6570\u91cf\u7684\u6307\u6807\u3002\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e24\u70b9\u5173\u952e\u53d1\u73b0\uff1a\uff081\uff09BF\u968f\u751f\u6210\u8fc7\u7a0b\u9010\u6e10\u964d\u4f4e\uff0c\u8868\u660eLLM\u751f\u6210\u65f6\u53d8\u5f97\u66f4\u53ef\u9884\u6d4b\u3002\uff082\uff09\u5bf9\u9f50\u8c03\u6574\u4ece\u4e00\u5f00\u59cb\u5c31\u663e\u8457\u9510\u5316\u8f93\u51fa\u5206\u5e03\uff0c\u4f7fBF\u964d\u4f4e\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\uff08\u5982\u4ece12\u964d\u81f31.2\uff09\u3002\u8fd9\u4e00\u5927\u5e45\u964d\u4f4e\u89e3\u91ca\u4e86\u4e3a\u4f55\u5bf9\u9f50\u6a21\u578b\u5bf9\u89e3\u7801\u7b56\u7565\u4e0d\u654f\u611f\u3002\u8fdb\u4e00\u6b65\u53d1\u73b0\uff0c\u8fd9\u79cd\u7a33\u5b9a\u6027\u5bf9\u590d\u6742\u63a8\u7406\u6709\u610f\u5916\u5f71\u54cd\u3002\u4f8b\u5982\uff0c\u5bf9\u9f50\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6a21\u578b\uff08\u5982DeepSeek\u84b8\u998f\u6a21\u578b\uff09\u5229\u7528\u6b64\u6548\u5e94\uff0c\u901a\u8fc7\u751f\u6210\u957f\u63a8\u7406\u94fe\u8fdb\u5165\u540e\u671f\u4f4eBF\u9636\u6bb5\uff0c\u8f93\u51fa\u66f4\u7a33\u5b9a\u3002\u6211\u4eec\u5047\u8bbe\u5bf9\u9f50\u8c03\u6574\u5e76\u672a\u6839\u672c\u6539\u53d8\u6a21\u578b\u884c\u4e3a\uff0c\u800c\u662f\u5f15\u5bfc\u5176\u9009\u62e9\u98ce\u683c\u5316\u6807\u8bb0\uff08\u5982\u201c\u5f53\u7136\u201d\uff09\uff0c\u89e3\u9501\u57fa\u6a21\u578b\u4e2d\u5df2\u6709\u7684\u4f4e\u71b5\u8def\u5f84\u3002\u8fd9\u4e00\u89c2\u70b9\u5f97\u5230\u63d0\u793a\u5b9e\u9a8c\u652f\u6301\uff0c\u8868\u660e\u7528\u6b64\u7c7b\u6807\u8bb0\u63d0\u793a\u57fa\u6a21\u578b\u540c\u6837\u53ef\u964d\u4f4eBF\u3002\u7efc\u4e0a\uff0cBF\u662f\u7406\u89e3\u548c\u63a7\u5236LLM\u8f93\u51fa\u7684\u6709\u529b\u8bca\u65ad\u5de5\u5177\uff0c\u9610\u660e\u4e86\u5bf9\u9f50\u5982\u4f55\u51cf\u5c11\u53d8\u5f02\u6027\u3001CoT\u5982\u4f55\u4fc3\u8fdb\u7a33\u5b9a\u751f\u6210\uff0c\u4ee5\u53ca\u5982\u4f55\u5f15\u5bfc\u57fa\u6a21\u578b\u8fdc\u79bb\u591a\u6837\u6027\u3002"}}
{"id": "2506.17664", "pdf": "https://arxiv.org/pdf/2506.17664", "abs": "https://arxiv.org/abs/2506.17664", "authors": ["Shuaiye Lu", "Linjiang Zhou", "Xiaochuan Shi"], "title": "MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "Hallucinations in large vision-language models (LVLMs) often stem from the\nmodel's sensitivity to image tokens during decoding, as evidenced by attention\npeaks observed when generating both real and hallucinated entities. To address\nthis, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel\ntraining-free approach that dynamically captures and refines the attention\nallocated to image tokens at each layer. MDSAM memorizes attention patterns and\nactivates updates through alignment during decoding, enhancing focus on\nrelevant image tokens while effectively reducing hallucinations. We evaluate\nMDSAM on multiple benchmarks for tasks such as image captioning and visual\nquestion answering, demonstrating its ability to consistently reduce\nhallucinations and improve reliability. Compatible with various LVLM\narchitectures, MDSAM highlights its adaptability and effectiveness in\nmitigating hallucinations without requiring additional training or external\ntools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDSAM\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6355\u6349\u548c\u4f18\u5316\u56fe\u50cf\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u5206\u914d\uff0c\u6709\u6548\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5bf9\u56fe\u50cf\u4ee4\u724c\u7684\u654f\u611f\u6027\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u5728\u751f\u6210\u771f\u5b9e\u548c\u865a\u6784\u5b9e\u4f53\u65f6\u51fa\u73b0\u6ce8\u610f\u529b\u5cf0\u503c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86MDSAM\u65b9\u6cd5\u3002", "method": "MDSAM\u901a\u8fc7\u8bb0\u5fc6\u6ce8\u610f\u529b\u6a21\u5f0f\u5e76\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5bf9\u9f50\u6fc0\u6d3b\u66f4\u65b0\uff0c\u52a8\u6001\u8c03\u6574\u56fe\u50cf\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u5206\u914d\uff0c\u589e\u5f3a\u5bf9\u76f8\u5173\u4ee4\u724c\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u7684\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMDSAM\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u5de5\u5177\u3002", "conclusion": "MDSAM\u662f\u4e00\u79cd\u9002\u5e94\u6027\u5f3a\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11LVLMs\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u67b6\u6784\u3002", "paper_title_zh": "MDSAM\uff1a\u57fa\u4e8e\u8bb0\u5fc6\u9a71\u52a8\u7684\u7a00\u758f\u6ce8\u610f\u529b\u77e9\u9635\u7528\u4e8e\u7f13\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898", "abstract_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u901a\u5e38\u6e90\u4e8e\u6a21\u578b\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5bf9\u56fe\u50cf\u4ee4\u724c\u7684\u654f\u611f\u6027\uff0c\u8868\u73b0\u4e3a\u751f\u6210\u771f\u5b9e\u548c\u865a\u6784\u5b9e\u4f53\u65f6\u51fa\u73b0\u7684\u6ce8\u610f\u529b\u5cf0\u503c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bb0\u5fc6\u9a71\u52a8\u7684\u7a00\u758f\u6ce8\u610f\u529b\u77e9\u9635\uff08MDSAM\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u52a8\u6001\u6355\u6349\u5e76\u4f18\u5316\u6bcf\u4e00\u5c42\u56fe\u50cf\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u5206\u914d\u3002MDSAM\u901a\u8fc7\u8bb0\u5fc6\u6ce8\u610f\u529b\u6a21\u5f0f\u5e76\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5bf9\u9f50\u6fc0\u6d3b\u66f4\u65b0\uff0c\u589e\u5f3a\u5bf9\u76f8\u5173\u56fe\u50cf\u4ee4\u724c\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002\u6211\u4eec\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u7684\u591a\u4e2a\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86MDSAM\uff0c\u7ed3\u679c\u8868\u660e\u5176\u80fd\u591f\u6301\u7eed\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002MDSAM\u517c\u5bb9\u591a\u79cdLVLM\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u5de5\u5177\u7684\u60c5\u51b5\u4e0b\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.18213", "pdf": "https://arxiv.org/pdf/2506.18213", "abs": "https://arxiv.org/abs/2506.18213", "authors": ["Mar\u00eda Victoria Carro", "Denise Alejandra Mester", "Francisca Gauna Selasco", "Luca Nicol\u00e1s Forziati Gangi", "Matheo Sandleris Musa", "Lola Ramos Pereyra", "Mario Leiva", "Juan Gustavo Corvalan", "Mar\u00eda Vanina Martinez", "Gerardo Simari"], "title": "A Conceptual Framework for AI Capability Evaluations", "categories": ["cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2306.04181 by other authors", "summary": "As AI systems advance and integrate into society, well-designed and\ntransparent evaluations are becoming essential tools in AI governance,\ninforming decisions by providing evidence about system capabilities and risks.\nYet there remains a lack of clarity on how to perform these assessments both\ncomprehensively and reliably. To address this gap, we propose a conceptual\nframework for analyzing AI capability evaluations, offering a structured,\ndescriptive approach that systematizes the analysis of widely used methods and\nterminology without imposing new taxonomies or rigid formats. This framework\nsupports transparency, comparability, and interpretability across diverse\nevaluations. It also enables researchers to identify methodological weaknesses,\nassists practitioners in designing evaluations, and provides policymakers with\nan accessible tool to scrutinize, compare, and navigate complex evaluation\nlandscapes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u80fd\u529b\u8bc4\u4f30\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u63cf\u8ff0\u6027\u65b9\u6cd5\u63d0\u5347\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u3001\u53ef\u6bd4\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u3001\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u66f4\u597d\u5730\u5206\u6790\u548c\u8bbe\u8ba1\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u7684\u53d1\u5c55\u548c\u5e7f\u6cdb\u5e94\u7528\uff0c\u900f\u660e\u4e14\u8bbe\u8ba1\u826f\u597d\u7684\u8bc4\u4f30\u6210\u4e3aAI\u6cbb\u7406\u7684\u5173\u952e\u5de5\u5177\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u5ff5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u63cf\u8ff0\u6027\u65b9\u6cd5\u5206\u6790AI\u80fd\u529b\u8bc4\u4f30\uff0c\u4e0d\u5f15\u5165\u65b0\u7684\u5206\u7c7b\u6216\u56fa\u5b9a\u683c\u5f0f\uff0c\u800c\u662f\u7cfb\u7edf\u5316\u73b0\u6709\u65b9\u6cd5\u548c\u672f\u8bed\u7684\u5206\u6790\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u3001\u53ef\u6bd4\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u65b9\u6cd5\u7f3a\u9677\uff0c\u8f85\u52a9\u5b9e\u8df5\u8005\u8bbe\u8ba1\u8bc4\u4f30\uff0c\u5e76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u5de5\u5177\u4ee5\u5ba1\u67e5\u548c\u6bd4\u8f83\u590d\u6742\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8bc4\u4f30\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\uff0c\u63a8\u52a8AI\u6cbb\u7406\u7684\u900f\u660e\u5316\u548c\u6807\u51c6\u5316\u3002", "paper_title_zh": "AI\u80fd\u529b\u8bc4\u4f30\u7684\u6982\u5ff5\u6846\u67b6", "abstract_zh": "\u968f\u7740AI\u7cfb\u7edf\u7684\u8fdb\u6b65\u53ca\u5176\u5728\u793e\u4f1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bbe\u8ba1\u826f\u597d\u4e14\u900f\u660e\u7684\u8bc4\u4f30\u5df2\u6210\u4e3aAI\u6cbb\u7406\u7684\u91cd\u8981\u5de5\u5177\uff0c\u901a\u8fc7\u63d0\u4f9b\u5173\u4e8e\u7cfb\u7edf\u80fd\u529b\u548c\u98ce\u9669\u7684\u8bc1\u636e\u6765\u652f\u6301\u51b3\u7b56\u3002\u7136\u800c\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u5168\u9762\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790AI\u80fd\u529b\u8bc4\u4f30\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u3001\u63cf\u8ff0\u6027\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5316\u5206\u6790\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b9\u6cd5\u548c\u672f\u8bed\uff0c\u800c\u4e0d\u5f15\u5165\u65b0\u7684\u5206\u7c7b\u6216\u56fa\u5b9a\u683c\u5f0f\u3002\u8be5\u6846\u67b6\u652f\u6301\u4e0d\u540c\u8bc4\u4f30\u4e4b\u95f4\u7684\u900f\u660e\u5ea6\u3001\u53ef\u6bd4\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc6\u522b\u65b9\u6cd5\u7f3a\u9677\uff0c\u8f85\u52a9\u5b9e\u8df5\u8005\u8bbe\u8ba1\u8bc4\u4f30\uff0c\u5e76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\uff0c\u4ee5\u5ba1\u67e5\u3001\u6bd4\u8f83\u548c\u5e94\u5bf9\u590d\u6742\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2506.17881", "pdf": "https://arxiv.org/pdf/2506.17881", "abs": "https://arxiv.org/abs/2506.17881", "authors": ["Hua Tang", "Lingyong Yan", "Yukun Zhao", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin"], "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved exceptional performance across a\nwide range of tasks. However, they still pose significant safety risks due to\nthe potential misuse for malicious purposes. Jailbreaks, which aim to elicit\nmodels to generate harmful content, play a critical role in identifying the\nunderlying security threats. Recent jailbreaking primarily focuses on\nsingle-turn scenarios, while the more complicated multi-turn scenarios remain\nunderexplored. Moreover, existing multi-turn jailbreaking techniques struggle\nto adapt to the evolving dynamics of dialogue as the interaction progresses. To\naddress this limitation, we propose a novel multi-turn jailbreaking method that\nrefines the jailbreaking path globally at each interaction. We also actively\nfabricate model responses to suppress safety-related warnings, thereby\nincreasing the likelihood of eliciting harmful outputs in subsequent questions.\nExperimental results demonstrate the superior performance of our method\ncompared with existing single-turn and multi-turn jailbreaking techniques\nacross six state-of-the-art LLMs. Our code is publicly available at\nhttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u548c\u4e3b\u52a8\u4f2a\u9020\u6a21\u578b\u54cd\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u5bf9\u8bdd\u4e2d\u8bf1\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\u4e0d\u5bb9\u5ffd\u89c6\u3002\u73b0\u6709\u8d8a\u72f1\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u5355\u8f6e\u573a\u666f\uff0c\u800c\u590d\u6742\u7684\u591a\u8f6e\u573a\u666f\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u8d8a\u72f1\u8def\u5f84\uff0c\u5e76\u5728\u5bf9\u8bdd\u4e2d\u4e3b\u52a8\u4f2a\u9020\u6a21\u578b\u54cd\u5e94\u4ee5\u6291\u5236\u5b89\u5168\u8b66\u544a\uff0c\u4ece\u800c\u589e\u52a0\u540e\u7eed\u95ee\u9898\u4e2d\u8bf1\u5bfc\u6709\u5bb3\u5185\u5bb9\u7684\u53ef\u80fd\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8f6e\u548c\u591a\u8f6e\u8d8a\u72f1\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u8f6e\u8d8a\u72f1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8bc6\u522b\u548c\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u548c\u4e3b\u52a8\u4f2a\u9020\u5b9e\u73b0\u591a\u8f6e\u8d8a\u72f1", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u4ecd\u5b58\u5728\u56e0\u6076\u610f\u4f7f\u7528\u800c\u5f15\u53d1\u7684\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002\u8d8a\u72f1\u6280\u672f\u65e8\u5728\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u5bf9\u8bc6\u522b\u6f5c\u5728\u5b89\u5168\u5a01\u80c1\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u573a\u666f\uff0c\u800c\u66f4\u590d\u6742\u7684\u591a\u8f6e\u573a\u666f\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u73b0\u6709\u591a\u8f6e\u8d8a\u72f1\u6280\u672f\u96be\u4ee5\u9002\u5e94\u5bf9\u8bdd\u7684\u52a8\u6001\u53d8\u5316\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u8d8a\u72f1\u8def\u5f84\uff0c\u5e76\u4e3b\u52a8\u4f2a\u9020\u6a21\u578b\u54cd\u5e94\u4ee5\u6291\u5236\u5b89\u5168\u8b66\u544a\uff0c\u4ece\u800c\u63d0\u9ad8\u540e\u7eed\u95ee\u9898\u4e2d\u8bf1\u5bfc\u6709\u5bb3\u8f93\u51fa\u7684\u6982\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8f6e\u548c\u591a\u8f6e\u8d8a\u72f1\u6280\u672f\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u4e8ehttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication\u3002"}}
{"id": "2506.17679", "pdf": "https://arxiv.org/pdf/2506.17679", "abs": "https://arxiv.org/abs/2506.17679", "authors": ["Wei Haolin"], "title": "CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection", "categories": ["cs.CV"], "comment": "15pages, 11figures", "summary": "Convolutional neural networks (CNNs) have long been the cornerstone of target\ndetection, but they are often limited by limited receptive fields, which\nhinders their ability to capture global contextual information. This paper\nbelieves that the effective utilization of extracted features is as important\nas the feature extraction process itself. We critically re-evaluated the\nDETR-inspired header network architecture, questioning the indispensable nature\nof its self-attention mechanism, and discovering significant information\nredundancies. To solve these problems, we introduced the Context-Gated\nScale-Adaptive Detection Network (CSDN), a Transformer-based detection header\ninspired by natural language processing architecture and human visual\nperception. CSDN aims to efficiently utilize the characteristics of the CNN\nbackbone network by replacing the traditional stacked self-attention and\ncross-attention layers with a novel gating mechanism. This mechanism enables\neach region of interest (ROI) to adaptively select and combine feature\ndimensions and scale information from multiple attention patterns. CSDN\nprovides more powerful global context modeling capabilities and can better\nadapt to objects of different sizes and structures. Our proposed detection head\ncan directly replace the native heads of various CNN-based detectors, and only\na few rounds of fine-tuning on the pre-training weights can significantly\nimprove the detection accuracy, thus avoiding the need to achieve small\nimprovements. Various layer modules undergo extensive re-training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u68c0\u6d4b\u5934CSDN\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u95e8\u63a7\u673a\u5236\u66ff\u4ee3\u4f20\u7edf\u7684\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u5c3a\u5bf8\u548c\u7ed3\u6784\u7684\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u53d7\u9650\u4e8e\u6709\u9650\u7684\u611f\u53d7\u91ce\uff0c\u96be\u4ee5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u672c\u6587\u8ba4\u4e3a\u7279\u5f81\u7684\u6709\u6548\u5229\u7528\u4e0e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u540c\u7b49\u91cd\u8981\uff0c\u5e76\u91cd\u65b0\u8bc4\u4f30\u4e86DETR\u542f\u53d1\u4e0b\u7684\u5934\u7f51\u7edc\u67b6\u6784\uff0c\u53d1\u73b0\u5176\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u4fe1\u606f\u5197\u4f59\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u67b6\u6784\u548c\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684Transformer\u68c0\u6d4b\u5934CSDN\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u9009\u62e9\u548c\u7ec4\u5408\u591a\u4e2a\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u7279\u5f81\u7ef4\u5ea6\u548c\u5c3a\u5ea6\u4fe1\u606f\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u5806\u53e0\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u3002", "result": "CSDN\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u4e0d\u540c\u5c3a\u5bf8\u548c\u7ed3\u6784\u7684\u7269\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u907f\u514d\u4e86\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "conclusion": "CSDN\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u68c0\u6d4b\u5934\uff0c\u53ef\u76f4\u63a5\u66ff\u6362\u591a\u79cd\u57fa\u4e8eCNN\u7684\u68c0\u6d4b\u5668\u7684\u539f\u751f\u5934\u90e8\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u8bad\u7ec3\u6210\u672c\u3002", "paper_title_zh": "CSDN\uff1a\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7684\u4e0a\u4e0b\u6587\u95e8\u63a7\u81ea\u9002\u5e94\u68c0\u6d4b\u7f51\u7edc", "abstract_zh": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u957f\u671f\u4ee5\u6765\u662f\u76ee\u6807\u68c0\u6d4b\u7684\u57fa\u77f3\uff0c\u4f46\u5176\u6709\u9650\u7684\u611f\u53d7\u91ce\u9650\u5236\u4e86\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u80fd\u529b\u3002\u672c\u6587\u8ba4\u4e3a\u63d0\u53d6\u7279\u5f81\u7684\u6709\u6548\u5229\u7528\u4e0e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u672c\u8eab\u540c\u7b49\u91cd\u8981\u3002\u6211\u4eec\u91cd\u65b0\u8bc4\u4f30\u4e86\u53d7DETR\u542f\u53d1\u7684\u5934\u7f51\u7edc\u67b6\u6784\uff0c\u8d28\u7591\u5176\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u53d1\u73b0\u4e86\u663e\u8457\u7684\u4fe1\u606f\u5197\u4f59\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u67b6\u6784\u548c\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684Transformer\u68c0\u6d4b\u5934CSDN\u3002CSDN\u901a\u8fc7\u65b0\u9896\u7684\u95e8\u63a7\u673a\u5236\u66ff\u4ee3\u4f20\u7edf\u7684\u5806\u53e0\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u4f7f\u6bcf\u4e2a\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9009\u62e9\u548c\u7ec4\u5408\u591a\u4e2a\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u7279\u5f81\u7ef4\u5ea6\u548c\u5c3a\u5ea6\u4fe1\u606f\u3002CSDN\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u9002\u5e94\u4e0d\u540c\u5c3a\u5bf8\u548c\u7ed3\u6784\u7684\u7269\u4f53\u3002\u6211\u4eec\u63d0\u51fa\u7684\u68c0\u6d4b\u5934\u53ef\u76f4\u63a5\u66ff\u6362\u591a\u79cd\u57fa\u4e8eCNN\u7684\u68c0\u6d4b\u5668\u7684\u539f\u751f\u5934\u90e8\uff0c\u4ec5\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4e3a\u5b9e\u73b0\u5fae\u5c0f\u6539\u8fdb\u800c\u8fdb\u884c\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\u3002"}}
{"id": "2506.18233", "pdf": "https://arxiv.org/pdf/2506.18233", "abs": "https://arxiv.org/abs/2506.18233", "authors": ["Ruike Zhu", "Hanwen Zhang", "Tianyu Shi", "Chi Wang", "Tianyi Zhou", "Zengyi Qin"], "title": "The 4th Dimension for Scaling Model Size", "categories": ["cs.AI"], "comment": null, "summary": "Scaling the size of large language models typically involves three\ndimensions: depth, width, and the number of parameters. In this work, we\nexplore a fourth dimension, virtual logical depth (VLD), which increases the\neffective algorithmic depth without changing the overall parameter count by\nreusing parameters within the model. Although parameter reuse is not a new\nconcept, its potential and characteristics in model scaling have not been\nthoroughly studied. Through carefully designed controlled experiments, we make\nthe following key discoveries regarding VLD scaling:\n  VLD scaling forces the knowledge capacity of the model to remain almost\nconstant, with only minor variations.\n  VLD scaling enables a significant improvement in reasoning capability,\nprovided the scaling method is properly implemented.\n  The number of parameters correlates with knowledge capacity, but not with\nreasoning capability. Under certain conditions, it is not necessary to increase\nthe parameter count to enhance reasoning.\n  These findings are consistent across various model configurations and are\nlikely to be generally valid within the scope of our experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u7ef4\u5ea6\u2014\u2014\u865a\u62df\u903b\u8f91\u6df1\u5ea6\uff08VLD\uff09\uff0c\u901a\u8fc7\u53c2\u6570\u590d\u7528\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u603b\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u7684\u6709\u6548\u7b97\u6cd5\u6df1\u5ea6\uff0c\u7814\u7a76\u53d1\u73b0VLD\u6269\u5c55\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u800c\u65e0\u9700\u589e\u52a0\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u901a\u5e38\u6d89\u53ca\u6df1\u5ea6\u3001\u5bbd\u5ea6\u548c\u53c2\u6570\u6570\u91cf\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u4f46\u53c2\u6570\u590d\u7528\u7684\u6f5c\u529b\u53ca\u5176\u5728\u6a21\u578b\u6269\u5c55\u4e2d\u7684\u7279\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u865a\u62df\u903b\u8f91\u6df1\u5ea6\uff08VLD\uff09\u4f5c\u4e3a\u7b2c\u56db\u7ef4\u5ea6\uff0c\u4ee5\u63ed\u793a\u5176\u5728\u6a21\u578b\u6269\u5c55\u4e2d\u7684\u4f5c\u7528\u548c\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a7\u5236\u5b9e\u9a8c\uff0c\u7814\u7a76\u865a\u62df\u903b\u8f91\u6df1\u5ea6\uff08VLD\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u53c2\u6570\u590d\u7528\u5bf9\u77e5\u8bc6\u5bb9\u91cf\u548c\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09VLD\u6269\u5c55\u51e0\u4e4e\u4e0d\u6539\u53d8\u6a21\u578b\u7684\u77e5\u8bc6\u5bb9\u91cf\uff1b2\uff09\u5728\u6b63\u786e\u5b9e\u65bd\u7684\u60c5\u51b5\u4e0b\uff0cVLD\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff1b3\uff09\u53c2\u6570\u6570\u91cf\u4e0e\u77e5\u8bc6\u5bb9\u91cf\u76f8\u5173\uff0c\u4f46\u4e0e\u63a8\u7406\u80fd\u529b\u65e0\u5173\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u65e0\u9700\u589e\u52a0\u53c2\u6570\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u865a\u62df\u903b\u8f91\u6df1\u5ea6\uff08VLD\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u6269\u5c55\u7ef4\u5ea6\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u4e00\u53d1\u73b0\u5728\u591a\u79cd\u6a21\u578b\u914d\u7f6e\u4e2d\u5177\u6709\u666e\u904d\u6027\u3002", "paper_title_zh": "\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u7684\u7b2c\u56db\u7ef4\u5ea6", "abstract_zh": "\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\u901a\u5e38\u6d89\u53ca\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u6df1\u5ea6\u3001\u5bbd\u5ea6\u548c\u53c2\u6570\u6570\u91cf\u3002\u672c\u6587\u63a2\u7d22\u4e86\u7b2c\u56db\u7ef4\u5ea6\u2014\u2014\u865a\u62df\u903b\u8f91\u6df1\u5ea6\uff08VLD\uff09\uff0c\u5b83\u901a\u8fc7\u590d\u7528\u6a21\u578b\u5185\u7684\u53c2\u6570\u6765\u589e\u52a0\u6709\u6548\u7b97\u6cd5\u6df1\u5ea6\uff0c\u800c\u4e0d\u6539\u53d8\u603b\u53c2\u6570\u6570\u91cf\u3002\u5c3d\u7ba1\u53c2\u6570\u590d\u7528\u5e76\u975e\u65b0\u6982\u5ff5\uff0c\u4f46\u5176\u5728\u6a21\u578b\u6269\u5c55\u4e2d\u7684\u6f5c\u529b\u548c\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a7\u5236\u5b9e\u9a8c\uff0c\u6211\u4eec\u5f97\u51fa\u4ee5\u4e0b\u5173\u4e8eVLD\u6269\u5c55\u7684\u5173\u952e\u53d1\u73b0\uff1a\n  VLD\u6269\u5c55\u8feb\u4f7f\u6a21\u578b\u7684\u77e5\u8bc6\u5bb9\u91cf\u51e0\u4e4e\u4fdd\u6301\u4e0d\u53d8\uff0c\u4ec5\u5b58\u5728\u5fae\u5c0f\u53d8\u5316\u3002\n  \u5728\u6b63\u786e\u5b9e\u65bd\u7684\u60c5\u51b5\u4e0b\uff0cVLD\u6269\u5c55\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\n  \u53c2\u6570\u6570\u91cf\u4e0e\u77e5\u8bc6\u5bb9\u91cf\u76f8\u5173\uff0c\u4f46\u4e0e\u63a8\u7406\u80fd\u529b\u65e0\u5173\u3002\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u65e0\u9700\u589e\u52a0\u53c2\u6570\u6570\u91cf\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\n  \u8fd9\u4e9b\u53d1\u73b0\u5728\u591a\u79cd\u6a21\u578b\u914d\u7f6e\u4e2d\u4e00\u81f4\uff0c\u53ef\u80fd\u5177\u6709\u666e\u904d\u6709\u6548\u6027\u3002"}}
{"id": "2506.17949", "pdf": "https://arxiv.org/pdf/2506.17949", "abs": "https://arxiv.org/abs/2506.17949", "authors": ["Hong Su"], "title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and\nextending patterns observed during pretraining but often struggle to generalize\nnovel ideas beyond their original context. This paper addresses the challenge\nof applying such localized innovations - introduced at a specific stage or\ncomponent - to other parts of a multi-stage process. We propose a scatter-based\ninnovation expansion model (innovation scatter model) that guides the LLM\nthrough a four-step process: (1) identifying the core innovation by comparing\nthe user's input with its surrounding context, (2) generalizing the innovation\nby removing references to specific stages or components, (3) determining\nwhether the generalized innovation applies to a broader scope beyond the\noriginal stage, and (4) systematically applying it to other structurally\nsimilar stages using the LLM. This model leverages structural redundancy across\nstages to improve the applicability of novel ideas. Verification results\ndemonstrate that the innovation scatter model enables LLMs to extend\ninnovations across structurally similar stages, thereby enhancing\ngeneralization and reuse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6563\u5c04\u7684\u521b\u65b0\u4f20\u64ad\u6a21\u578b\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9636\u6bb5\u8fc7\u7a0b\u4e2d\u6269\u5c55\u5c40\u90e8\u521b\u65b0\uff0c\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6a21\u5f0f\u590d\u5236\u548c\u6269\u5c55\u80fd\u529b\uff0c\u4f46\u5728\u5c06\u5c40\u90e8\u521b\u65b0\u63a8\u5e7f\u5230\u591a\u9636\u6bb5\u8fc7\u7a0b\u7684\u5176\u4ed6\u90e8\u5206\u65f6\u5b58\u5728\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u6b65\u521b\u65b0\u6563\u5c04\u6a21\u578b\uff1a(1) \u901a\u8fc7\u6bd4\u8f83\u7528\u6237\u8f93\u5165\u4e0e\u4e0a\u4e0b\u6587\u8bc6\u522b\u6838\u5fc3\u521b\u65b0\uff1b(2) \u901a\u8fc7\u53bb\u9664\u7279\u5b9a\u9636\u6bb5\u6216\u7ec4\u4ef6\u7684\u5f15\u7528\u6cdb\u5316\u521b\u65b0\uff1b(3) \u5224\u65ad\u6cdb\u5316\u540e\u7684\u521b\u65b0\u662f\u5426\u9002\u7528\u4e8e\u66f4\u5e7f\u8303\u56f4\uff1b(4) \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u5176\u7cfb\u7edf\u6027\u5730\u5e94\u7528\u4e8e\u7ed3\u6784\u76f8\u4f3c\u7684\u5176\u4ed6\u9636\u6bb5\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u521b\u65b0\u6563\u5c04\u6a21\u578b\u80fd\u591f\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u521b\u65b0\u6269\u5c55\u5230\u7ed3\u6784\u76f8\u4f3c\u7684\u9636\u6bb5\uff0c\u4ece\u800c\u63d0\u5347\u6cdb\u5316\u548c\u91cd\u7528\u80fd\u529b\u3002", "conclusion": "\u521b\u65b0\u6563\u5c04\u6a21\u578b\u901a\u8fc7\u5229\u7528\u9636\u6bb5\u95f4\u7684\u7ed3\u6784\u5197\u4f59\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9636\u6bb5\u8fc7\u7a0b\u4e2d\u63a8\u5e7f\u5c40\u90e8\u521b\u65b0\u7684\u80fd\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u6563\u5c04\u7684\u521b\u65b0\u4f20\u64ad\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u9636\u6bb5\u8fc7\u7a0b\u9002\u5e94", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91cd\u73b0\u548c\u6269\u5c55\u9884\u8bad\u7ec3\u4e2d\u89c2\u5bdf\u5230\u7684\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u5c06\u65b0\u9896\u7684\u60f3\u6cd5\u63a8\u5e7f\u5230\u539f\u59cb\u4e0a\u4e0b\u6587\u4e4b\u5916\u3002\u672c\u6587\u89e3\u51b3\u4e86\u5c06\u6b64\u7c7b\u5c40\u90e8\u521b\u65b0\uff08\u5728\u7279\u5b9a\u9636\u6bb5\u6216\u7ec4\u4ef6\u4e2d\u5f15\u5165\uff09\u5e94\u7528\u4e8e\u591a\u9636\u6bb5\u8fc7\u7a0b\u5176\u4ed6\u90e8\u5206\u7684\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6563\u5c04\u7684\u521b\u65b0\u6269\u5c55\u6a21\u578b\uff08\u521b\u65b0\u6563\u5c04\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u56db\u4e2a\u6b65\u9aa4\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff1a(1) \u901a\u8fc7\u6bd4\u8f83\u7528\u6237\u8f93\u5165\u4e0e\u5176\u5468\u56f4\u4e0a\u4e0b\u6587\u8bc6\u522b\u6838\u5fc3\u521b\u65b0\uff1b(2) \u901a\u8fc7\u53bb\u9664\u7279\u5b9a\u9636\u6bb5\u6216\u7ec4\u4ef6\u7684\u5f15\u7528\u6cdb\u5316\u521b\u65b0\uff1b(3) \u5224\u65ad\u6cdb\u5316\u540e\u7684\u521b\u65b0\u662f\u5426\u9002\u7528\u4e8e\u539f\u59cb\u9636\u6bb5\u4e4b\u5916\u7684\u66f4\u5e7f\u8303\u56f4\uff1b(4) \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u5176\u7cfb\u7edf\u6027\u5730\u5e94\u7528\u4e8e\u7ed3\u6784\u76f8\u4f3c\u7684\u5176\u4ed6\u9636\u6bb5\u3002\u8be5\u6a21\u578b\u5229\u7528\u9636\u6bb5\u95f4\u7684\u7ed3\u6784\u5197\u4f59\uff0c\u63d0\u9ad8\u4e86\u65b0\u9896\u60f3\u6cd5\u7684\u9002\u7528\u6027\u3002\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u521b\u65b0\u6563\u5c04\u6a21\u578b\u80fd\u591f\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u521b\u65b0\u6269\u5c55\u5230\u7ed3\u6784\u76f8\u4f3c\u7684\u9636\u6bb5\uff0c\u4ece\u800c\u63d0\u5347\u6cdb\u5316\u548c\u91cd\u7528\u80fd\u529b\u3002"}}
{"id": "2506.17685", "pdf": "https://arxiv.org/pdf/2506.17685", "abs": "https://arxiv.org/abs/2506.17685", "authors": ["Amirshayan Nasirimajd", "Chiara Plizzari", "Simone Alberto Peirone", "Marco Ciccone", "Giuseppe Averta", "Barbara Caputo"], "title": "Domain Generalization using Action Sequences for Egocentric Action Recognition", "categories": ["cs.CV"], "comment": "Accepted at Pattern Recognition Letters. 9 pages including\n  references. Code and Data: https://github.com/Ashayan97/SeqDG", "summary": "Recognizing human activities from visual inputs, particularly through a\nfirst-person viewpoint, is essential for enabling robots to replicate human\nbehavior. Egocentric vision, characterized by cameras worn by observers,\ncaptures diverse changes in illumination, viewpoint, and environment. This\nvariability leads to a notable drop in the performance of Egocentric Action\nRecognition models when tested in environments not seen during training. In\nthis paper, we tackle these challenges by proposing a domain generalization\napproach for Egocentric Action Recognition. Our insight is that action\nsequences often reflect consistent user intent across visual domains. By\nleveraging action sequences, we aim to enhance the model's generalization\nability across unseen environments. Our proposed method, named SeqDG,\nintroduces a visual-text sequence reconstruction objective (SeqRec) that uses\ncontextual cues from both text and visual inputs to reconstruct the central\naction of the sequence. Additionally, we enhance the model's robustness by\ntraining it on mixed sequences of actions from different domains (SeqMix). We\nvalidate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on\nEPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement\nin cross-domain action recognition in unseen environments, and on EGTEA the\nmodel achieved +0.6% Top-1 accuracy over SOTA in intra-domain action\nrecognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeqDG\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u7684\u4e00\u81f4\u6027\u610f\u56fe\u63d0\u5347\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u7684\u8de8\u9886\u57df\u6027\u80fd\u3002\u65b9\u6cd5\u7ed3\u5408\u89c6\u89c9-\u6587\u672c\u5e8f\u5217\u91cd\u5efa\uff08SeqRec\uff09\u548c\u591a\u9886\u57df\u52a8\u4f5c\u5e8f\u5217\u6df7\u5408\u8bad\u7ec3\uff08SeqMix\uff09\uff0c\u5728EPIC-KITCHENS-100\u548cEGTEA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u5149\u7167\u3001\u89c6\u89d2\u548c\u73af\u5883\u7684\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u4f5c\u5e8f\u5217\u7684\u4e00\u81f4\u6027\u610f\u56fe\u63d0\u5347\u6a21\u578b\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSeqDG\u65b9\u6cd5\uff0c\u5305\u62ec\u89c6\u89c9-\u6587\u672c\u5e8f\u5217\u91cd\u5efa\u76ee\u6807\uff08SeqRec\uff09\u548c\u591a\u9886\u57df\u52a8\u4f5c\u5e8f\u5217\u6df7\u5408\u8bad\u7ec3\uff08SeqMix\uff09\u3002SeqRec\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u91cd\u5efa\u5e8f\u5217\u4e2d\u5fc3\u52a8\u4f5c\uff0cSeqMix\u901a\u8fc7\u6df7\u5408\u4e0d\u540c\u9886\u57df\u7684\u52a8\u4f5c\u5e8f\u5217\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728EPIC-KITCHENS-100\u6570\u636e\u96c6\u4e0a\uff0cSeqDG\u5728\u8de8\u9886\u57df\u52a8\u4f5c\u8bc6\u522b\u4e2d\u76f8\u5bf9\u5e73\u5747\u63d0\u53472.4%\uff1b\u5728EGTEA\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u9886\u57df\u5185\u52a8\u4f5c\u8bc6\u522b\u4e2dTop-1\u51c6\u786e\u7387\u63d0\u53470.6%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "SeqDG\u901a\u8fc7\u52a8\u4f5c\u5e8f\u5217\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8de8\u9886\u57df\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "paper_title_zh": "\u57fa\u4e8e\u52a8\u4f5c\u5e8f\u5217\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u7528\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b", "abstract_zh": "\u4ece\u89c6\u89c9\u8f93\u5165\u4e2d\u8bc6\u522b\u4eba\u7c7b\u6d3b\u52a8\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff0c\u5bf9\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff08\u7531\u4f69\u6234\u6444\u50cf\u5934\u7684\u89c2\u5bdf\u8005\u6355\u6349\uff09\u5177\u6709\u5149\u7167\u3001\u89c6\u89d2\u548c\u73af\u5883\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff08SeqDG\uff09\uff0c\u901a\u8fc7\u52a8\u4f5c\u5e8f\u5217\u7684\u4e00\u81f4\u6027\u610f\u56fe\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002SeqDG\u5305\u542b\u89c6\u89c9-\u6587\u672c\u5e8f\u5217\u91cd\u5efa\u76ee\u6807\uff08SeqRec\uff09\u548c\u591a\u9886\u57df\u52a8\u4f5c\u5e8f\u5217\u6df7\u5408\u8bad\u7ec3\uff08SeqMix\uff09\u3002SeqRec\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u91cd\u5efa\u5e8f\u5217\u4e2d\u5fc3\u52a8\u4f5c\uff0cSeqMix\u901a\u8fc7\u6df7\u5408\u4e0d\u540c\u9886\u57df\u7684\u52a8\u4f5c\u5e8f\u5217\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002\u5728EPIC-KITCHENS-100\u548cEGTEA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSeqDG\u5728\u8de8\u9886\u57df\u52a8\u4f5c\u8bc6\u522b\u4e2d\u76f8\u5bf9\u5e73\u5747\u63d0\u53472.4%\uff0c\u5728EGTEA\u7684\u9886\u57df\u5185\u52a8\u4f5c\u8bc6\u522b\u4e2dTop-1\u51c6\u786e\u7387\u63d0\u53470.6%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002"}}
{"id": "2506.18260", "pdf": "https://arxiv.org/pdf/2506.18260", "abs": "https://arxiv.org/abs/2506.18260", "authors": ["FuTe Wong"], "title": "Advanced For-Loop for QML algorithm search", "categories": ["cs.AI"], "comment": "7 pages, 8 figures", "summary": "This paper introduces an advanced framework leveraging Large Language\nModel-based Multi-Agent Systems (LLMMA) for the automated search and\noptimization of Quantum Machine Learning (QML) algorithms. Inspired by Google\nDeepMind's FunSearch, the proposed system works on abstract level to\niteratively generates and refines quantum transformations of classical machine\nlearning algorithms (concepts), such as the Multi-Layer Perceptron,\nforward-forward and backpropagation algorithms. As a proof of concept, this\nwork highlights the potential of agentic frameworks to systematically explore\nclassical machine learning concepts and adapt them for quantum computing,\npaving the way for efficient and automated development of QML algorithms.\nFuture directions include incorporating planning mechanisms and optimizing\nstrategy in the search space for broader applications in quantum-enhanced\nmachine learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLMMA\uff09\u7684\u9ad8\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u641c\u7d22\u548c\u4f18\u5316\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7b97\u6cd5\u3002\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u91cf\u5b50\u53d8\u6362\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u6846\u67b6\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u53d7Google DeepMind\u7684FunSearch\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u5316\u63a2\u7d22\u548c\u4f18\u5316\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u586b\u8865\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u4e0e\u91cf\u5b50\u8ba1\u7b97\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLMMA\uff09\uff0c\u5728\u62bd\u8c61\u5c42\u9762\u4e0a\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u591a\u5c42\u611f\u77e5\u673a\u3001\u524d\u5411-\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff09\u7684\u91cf\u5b50\u53d8\u6362\u3002", "result": "\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u6846\u67b6\u5728\u7cfb\u7edf\u5316\u63a2\u7d22\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6982\u5ff5\u5e76\u5c06\u5176\u9002\u914d\u5230\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u81ea\u52a8\u5316\u5f00\u53d1QML\u7b97\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u91cf\u5b50\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u5f00\u53d1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5f15\u5165\u89c4\u5212\u673a\u5236\u548c\u4f18\u5316\u641c\u7d22\u7b56\u7565\u4ee5\u6269\u5927\u5e94\u7528\u8303\u56f4\u3002", "paper_title_zh": "\u7528\u4e8eQML\u7b97\u6cd5\u641c\u7d22\u7684\u9ad8\u7ea7For-Loop\u6846\u67b6", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLMMA\uff09\u7684\u9ad8\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u641c\u7d22\u548c\u4f18\u5316\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7b97\u6cd5\u3002\u53d7Google DeepMind\u7684FunSearch\u542f\u53d1\uff0c\u8be5\u7cfb\u7edf\u5728\u62bd\u8c61\u5c42\u9762\u4e0a\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u591a\u5c42\u611f\u77e5\u673a\u3001\u524d\u5411-\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff09\u7684\u91cf\u5b50\u53d8\u6362\u3002\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u672c\u7814\u7a76\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u6846\u67b6\u5728\u7cfb\u7edf\u5316\u63a2\u7d22\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6982\u5ff5\u5e76\u5c06\u5176\u9002\u914d\u5230\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u81ea\u52a8\u5316\u5f00\u53d1QML\u7b97\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002\u672a\u6765\u65b9\u5411\u5305\u62ec\u5f15\u5165\u89c4\u5212\u673a\u5236\u548c\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\u7684\u7b56\u7565\uff0c\u4ee5\u6269\u5927\u5176\u5728\u91cf\u5b50\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.17951", "pdf": "https://arxiv.org/pdf/2506.17951", "abs": "https://arxiv.org/abs/2506.17951", "authors": ["Quanwei Tang", "Sophia Yat Mei Lee", "Junshuang Wu", "Dong Zhang", "Shoushan Li", "Erik Cambria", "Guodong Zhou"], "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment", "categories": ["cs.CL"], "comment": "acl 2025 findings", "summary": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGraphMPA\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u5bf9\u9f50\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u63d0\u5347\u4e86\u95ee\u7b54\u7cfb\u7edf\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4f46\u5728\u5168\u5c40\u7406\u89e3\u548c\u7b26\u5408\u4eba\u7c7b\u4f26\u7406\u53ca\u8d28\u91cf\u504f\u597d\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "GraphMPA\u901a\u8fc7\u6784\u5efa\u5206\u5c42\u6587\u6863\u56fe\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u4f18\u5316\uff0c\u901a\u8fc7\u6982\u7387\u5339\u914d\u7ea6\u675f\u4f7f\u6a21\u578b\u8f93\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86GraphMPA\u7684\u6709\u6548\u6027\u3002", "conclusion": "GraphMPA\u901a\u8fc7\u56fe\u6846\u67b6\u548c\u504f\u597d\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u57fa\u4e8e\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u5bf9\u9f50\u7684\u5168\u9762\u56fe\u6846\u67b6\u5728\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u901a\u8fc7\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002\u7136\u800c\uff0c\u5728\u5b9e\u73b0\u5168\u5c40\u7406\u89e3\u53ca\u4f7f\u56de\u7b54\u7b26\u5408\u4eba\u7c7b\u4f26\u7406\u548c\u8d28\u91cf\u504f\u597d\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faGraphMPA\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5168\u9762\u6846\u67b6\uff0c\u5177\u5907\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u5bf9\u9f50\u529f\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u901a\u7528\u76f8\u4f3c\u6027\u5ea6\u91cf\u6784\u5efa\u5206\u5c42\u6587\u6863\u56fe\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u4ee5\u7406\u89e3\u548c\u7efc\u5408\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u4f18\u5316\uff0c\u901a\u8fc7\u6982\u7387\u5339\u914d\u7ea6\u675f\u4f7f\u6a21\u578b\u8f93\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86GraphMPA\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.17694", "pdf": "https://arxiv.org/pdf/2506.17694", "abs": "https://arxiv.org/abs/2506.17694", "authors": ["Gnana Praveen Rajasekhar", "Jahangir Alam"], "title": "SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification", "categories": ["cs.CV"], "comment": null, "summary": "Conventional audio-visual methods for speaker verification rely on large\namounts of labeled data and separate modality-specific architectures, which is\ncomputationally expensive, limiting their scalability. To address these\nproblems, we propose a self-supervised learning framework based on contrastive\nlearning with asymmetric masking and masked data modeling to obtain robust\naudiovisual feature representations. In particular, we employ a unified\nframework for self-supervised audiovisual speaker verification using a single\nshared backbone for audio and visual inputs, leveraging the versatility of\nvision transformers. The proposed unified framework can handle audio, visual,\nor audiovisual inputs using a single shared vision transformer backbone during\ntraining and testing while being computationally efficient and robust to\nmissing modalities. Extensive experiments demonstrate that our method achieves\ncompetitive performance without labeled data while reducing computational costs\ncompared to traditional approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6SSAVSV\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u975e\u5bf9\u79f0\u63a9\u7801\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u97f3\u9891-\u89c6\u89c9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684\u97f3\u9891-\u89c6\u89c9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u72ec\u7acb\u6a21\u6001\u67b6\u6784\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u548c\u63a9\u7801\u6570\u636e\u5efa\u6a21\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u89c6\u89c9\u53d8\u6362\u5668\uff08Vision Transformer\uff09\u4f5c\u4e3a\u5171\u4eab\u4e3b\u5e72\u7f51\u7edc\uff0c\u652f\u6301\u97f3\u9891\u3001\u89c6\u89c9\u6216\u97f3\u9891-\u89c6\u89c9\u8f93\u5165\uff0c\u5e76\u5bf9\u7f3a\u5931\u6a21\u6001\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "SSAVSV\u6846\u67b6\u4e3a\u97f3\u9891-\u89c6\u89c9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u3002", "paper_title_zh": "SSAVSV\uff1a\u9762\u5411\u81ea\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7684\u7edf\u4e00\u6a21\u578b", "abstract_zh": "\u4f20\u7edf\u7684\u97f3\u9891-\u89c6\u89c9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u72ec\u7acb\u7684\u6a21\u6001\u4e13\u7528\u67b6\u6784\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u548c\u975e\u5bf9\u79f0\u63a9\u7801\u6280\u672f\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u83b7\u53d6\u9c81\u68d2\u7684\u97f3\u9891-\u89c6\u89c9\u7279\u5f81\u8868\u793a\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u91c7\u7528\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u53d8\u6362\u5668\u7684\u591a\u529f\u80fd\u6027\uff0c\u4e3a\u97f3\u9891\u548c\u89c6\u89c9\u8f93\u5165\u5171\u4eab\u5355\u4e00\u4e3b\u5e72\u7f51\u7edc\u3002\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u80fd\u591f\u5904\u7406\u97f3\u9891\u3001\u89c6\u89c9\u6216\u97f3\u9891-\u89c6\u89c9\u8f93\u5165\uff0c\u540c\u65f6\u8ba1\u7b97\u9ad8\u6548\u4e14\u5bf9\u7f3a\u5931\u6a21\u6001\u5177\u6709\u9c81\u68d2\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.18348", "pdf": "https://arxiv.org/pdf/2506.18348", "abs": "https://arxiv.org/abs/2506.18348", "authors": ["Weilun Yu", "Shixiang Tang", "Yonggui Huang", "Nanqing Dong", "Li Fan", "Honggang Qi", "Wei Liu", "Xiaoli Diao", "Xi Chen", "Wanli Ouyang"], "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team", "categories": ["cs.AI"], "comment": null, "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6IDVSCI\uff0c\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u4ea4\u6362\u548c\u53cc\u591a\u6837\u6027\u8bc4\u5ba1\u673a\u5236\uff0c\u63d0\u5347\u79d1\u5b66\u7814\u7a76\u7684\u4ea4\u4e92\u63a8\u7406\u548c\u521b\u9020\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u79d1\u5b66\u8fdb\u6b65\u4f9d\u8d56\u4e8e\u7814\u7a76\u8005\u95f4\u7684\u6709\u6548\u534f\u4f5c\uff0c\u800c\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u79d1\u5b66\u5bb6\u4ee3\u7406\u7f3a\u4e4f\u4ea4\u4e92\u63a8\u7406\u548c\u8bc4\u4f30\u673a\u5236\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "IDVSCI\u6846\u67b6\u5305\u542b\u52a8\u6001\u77e5\u8bc6\u4ea4\u6362\u673a\u5236\u548c\u53cc\u591a\u6837\u6027\u8bc4\u5ba1\u8303\u5f0f\uff0c\u524d\u8005\u652f\u6301\u667a\u80fd\u4f53\u95f4\u7684\u8fed\u4ee3\u53cd\u9988\uff0c\u540e\u8005\u6a21\u62df\u5f02\u8d28\u4e13\u5bb6\u8bc4\u4f30\uff0c\u5171\u540c\u4fc3\u8fdb\u6df1\u5ea6\u63a8\u7406\u548c\u521b\u65b0\u79d1\u5b66\u60f3\u6cd5\u7684\u751f\u6210\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5065\u5eb7\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cIDVSCI\u8868\u73b0\u4f18\u4e8eAI Scientist\u548cVIRSCI\u7b49\u73b0\u6709\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "IDVSCI\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u548c\u540c\u884c\u8bc4\u5ba1\u52a8\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u7814\u7a76\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u79d1\u5b66\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u52a8\u6001\u77e5\u8bc6\u4ea4\u6362\u4e0e\u53cc\u591a\u6837\u6027\u8bc4\u5ba1\uff1a\u7b80\u6d01\u91ca\u653e\u591a\u667a\u80fd\u4f53\u7814\u7a76\u56e2\u961f\u7684\u6f5c\u529b", "abstract_zh": "\u79d1\u5b66\u8fdb\u6b65\u65e5\u76ca\u4f9d\u8d56\u4e8e\u7814\u7a76\u8005\u95f4\u7684\u6709\u6548\u534f\u4f5c\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ec5\u521d\u6b65\u6a21\u62df\u4e86\u8fd9\u4e00\u52a8\u6001\u3002\u5c3d\u7ba1\u8fd1\u671f\u57fa\u4e8eLLM\u7684\u79d1\u5b66\u5bb6\u4ee3\u7406\u5728\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u771f\u5b9e\u7814\u7a76\u6240\u9700\u7684\u4ea4\u4e92\u63a8\u7406\u548c\u8bc4\u4f30\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86IDVSCI\uff08\u5185\u90e8\u8ba8\u8bba\u4e0e\u6295\u7968\u79d1\u5b66\u5bb6\uff09\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a\u52a8\u6001\u77e5\u8bc6\u4ea4\u6362\u673a\u5236\uff08\u652f\u6301\u667a\u80fd\u4f53\u95f4\u7684\u8fed\u4ee3\u53cd\u9988\uff09\u548c\u53cc\u591a\u6837\u6027\u8bc4\u5ba1\u8303\u5f0f\uff08\u6a21\u62df\u5f02\u8d28\u4e13\u5bb6\u8bc4\u4f30\uff09\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u4fc3\u8fdb\u6df1\u5ea6\u63a8\u7406\uff0c\u5e76\u751f\u6210\u66f4\u5177\u521b\u9020\u529b\u548c\u5f71\u54cd\u529b\u7684\u79d1\u5b66\u60f3\u6cd5\u3002\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u4eec\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff1a\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u51c6\u548c\u6211\u4eec\u65b0\u5f15\u5165\u7684\u5065\u5eb7\u79d1\u5b66\u9886\u57df\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\uff0cIDVSCI\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8eAI Scientist\u548cVIRSCI\u7b49\u73b0\u6709\u7cfb\u7edf\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u5728\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u7814\u7a76\u4e2d\u6a21\u62df\u4ea4\u4e92\u548c\u540c\u884c\u8bc4\u5ba1\u52a8\u6001\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.18027", "pdf": "https://arxiv.org/pdf/2506.18027", "abs": "https://arxiv.org/abs/2506.18027", "authors": ["Thi Thu Uyen Hoang", "Viet Anh Nguyen"], "title": "PDF Retrieval Augmented Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents an advancement in Question-Answering (QA) systems using a\nRetrieval Augmented Generation (RAG) framework to enhance information\nextraction from PDF files. Recognizing the richness and diversity of data\nwithin PDFs--including text, images, vector diagrams, graphs, and tables--poses\nunique challenges for existing QA systems primarily designed for textual\ncontent. We seek to develop a comprehensive RAG-based QA system that will\neffectively address complex multimodal questions, where several data types are\ncombined in the query. This is mainly achieved by refining approaches to\nprocessing and integrating non-textual elements in PDFs into the RAG framework\nto derive precise and relevant answers, as well as fine-tuning large language\nmodels to better adapt to our system. We provide an in-depth experimental\nevaluation of our solution, demonstrating its capability to extract accurate\ninformation that can be applied to different types of content across PDFs. This\nwork not only pushes the boundaries of retrieval-augmented QA systems but also\nlays a foundation for further research in multimodal data integration and\nprocessing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ecePDF\u6587\u4ef6\u4e2d\u63d0\u53d6\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u89e3\u51b3\u4f20\u7edfQA\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u975e\u6587\u672c\u5185\u5bb9\u7684\u6311\u6218\u3002", "motivation": "PDF\u6587\u4ef6\u4e2d\u5305\u542b\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u56fe\u8868\u7b49\uff09\uff0c\u800c\u73b0\u6709\u95ee\u7b54\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u5185\u5bb9\u8bbe\u8ba1\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5168\u9762\u7684RAG\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5bf9PDF\u4e2d\u591a\u6a21\u6001\u4fe1\u606f\u7684\u63d0\u53d6\u548c\u95ee\u7b54\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6539\u8fdbPDF\u4e2d\u975e\u6587\u672c\u5185\u5bb9\u7684\u5904\u7406\u4e0e\u96c6\u6210\u65b9\u6cd5\uff0c\u5c06\u591a\u6a21\u6001\u6570\u636e\u878d\u5165RAG\u6846\u67b6\uff0c\u5e76\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u9002\u5e94\u7cfb\u7edf\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u4ecePDF\u4e2d\u51c6\u786e\u63d0\u53d6\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u56de\u7b54\u590d\u6742\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u548c\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u62d3\u5c55\u4e86\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\u7cfb\u7edf\u7684\u8fb9\u754c\uff0c\u8fd8\u4e3a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u548c\u5904\u7406\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u57fa\u4e8ePDF\u68c0\u7d22\u589e\u5f3a\u7684\u95ee\u7b54\u7cfb\u7edf", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ecePDF\u6587\u4ef6\u4e2d\u63d0\u53d6\u591a\u6a21\u6001\u4fe1\u606f\u3002PDF\u6587\u4ef6\u5305\u542b\u4e30\u5bcc\u7684\u6587\u672c\u3001\u56fe\u50cf\u3001\u77e2\u91cf\u56fe\u3001\u56fe\u8868\u548c\u8868\u683c\u7b49\u6570\u636e\uff0c\u8fd9\u5bf9\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u5185\u5bb9\u8bbe\u8ba1\u7684\u73b0\u6709\u95ee\u7b54\u7cfb\u7edf\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\u3002\u6211\u4eec\u81f4\u529b\u4e8e\u5f00\u53d1\u4e00\u79cd\u5168\u9762\u7684RAG\u95ee\u7b54\u7cfb\u7edf\uff0c\u4ee5\u6709\u6548\u89e3\u51b3\u7ed3\u5408\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u590d\u6742\u591a\u6a21\u6001\u95ee\u9898\u3002\u4e3b\u8981\u901a\u8fc7\u6539\u8fdbPDF\u4e2d\u975e\u6587\u672c\u5185\u5bb9\u7684\u5904\u7406\u4e0e\u96c6\u6210\u65b9\u6cd5\uff0c\u5c06\u5176\u878d\u5165RAG\u6846\u67b6\u4ee5\u83b7\u53d6\u7cbe\u786e\u7b54\u6848\uff0c\u5e76\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u9002\u5e94\u7cfb\u7edf\u9700\u6c42\u3002\u6211\u4eec\u901a\u8fc7\u6df1\u5165\u7684\u5b9e\u9a8c\u8bc4\u4f30\u5c55\u793a\u4e86\u8be5\u65b9\u6848\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u4ecePDF\u4e2d\u63d0\u53d6\u51c6\u786e\u4fe1\u606f\u5e76\u5e94\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u5185\u5bb9\u3002\u8fd9\u4e00\u5de5\u4f5c\u4e0d\u4ec5\u63a8\u52a8\u4e86\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\u7cfb\u7edf\u7684\u8fb9\u754c\uff0c\u8fd8\u4e3a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u548c\u5904\u7406\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.17705", "pdf": "https://arxiv.org/pdf/2506.17705", "abs": "https://arxiv.org/abs/2506.17705", "authors": ["Bo Pan", "Yang Chen", "Yingwei Pan", "Ting Yao", "Wei Chen", "Tao Mei"], "title": "DreamJourney: Perpetual View Generation with Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Perpetual view generation aims to synthesize a long-term video corresponding\nto an arbitrary camera trajectory solely from a single input image. Recent\nmethods commonly utilize a pre-trained text-to-image diffusion model to\nsynthesize new content of previously unseen regions along camera movement.\nHowever, the underlying 2D diffusion model lacks 3D awareness and results in\ndistorted artifacts. Moreover, they are limited to generating views of static\n3D scenes, neglecting to capture object movements within the dynamic 4D world.\nTo alleviate these issues, we present DreamJourney, a two-stage framework that\nleverages the world simulation capacity of video diffusion models to trigger a\nnew perpetual scene view generation task with both camera movements and object\ndynamics. Specifically, in stage I, DreamJourney first lifts the input image to\n3D point cloud and renders a sequence of partial images from a specific camera\ntrajectory. A video diffusion model is then utilized as generative prior to\ncomplete the missing regions and enhance visual coherence across the sequence,\nproducing a cross-view consistent video adheres to the 3D scene and camera\ntrajectory. Meanwhile, we introduce two simple yet effective strategies (early\nstopping and view padding) to further stabilize the generation process and\nimprove visual quality. Next, in stage II, DreamJourney leverages a multimodal\nlarge language model to produce a text prompt describing object movements in\ncurrent view, and uses video diffusion model to animate current view with\nobject movements. Stage I and II are repeated recurrently, enabling perpetual\ndynamic scene view generation. Extensive experiments demonstrate the\nsuperiority of our DreamJourney over state-of-the-art methods both\nquantitatively and qualitatively. Our project page:\nhttps://dream-journey.vercel.app.", "AI": {"tldr": "DreamJourney\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u751f\u6210\u52a8\u6001\u573a\u666f\u7684\u8fde\u7eed\u89c6\u89d2\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u57283D\u611f\u77e5\u548c\u52a8\u6001\u5bf9\u8c61\u8fd0\u52a8\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e2D\u6269\u6563\u6a21\u578b\u751f\u6210\u65b0\u89c6\u89d2\uff0c\u7f3a\u4e4f3D\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u753b\u9762\u626d\u66f2\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u52a8\u60014D\u4e16\u754c\u4e2d\u7684\u5bf9\u8c61\u8fd0\u52a8\u3002DreamJourney\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u7684\u8fde\u7eed\u89c6\u89d2\u751f\u6210\u3002", "method": "DreamJourney\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc73D\u70b9\u4e91\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89d2\u4e00\u81f4\u7684\u89c6\u9891\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u5bf9\u8c61\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a8\u6001\u5316\u5f53\u524d\u89c6\u89d2\u3002\u4e24\u9636\u6bb5\u5faa\u73af\u6267\u884c\uff0c\u5b9e\u73b0\u8fde\u7eed\u52a8\u6001\u573a\u666f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDreamJourney\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u573a\u666f\u89c6\u9891\u3002", "conclusion": "DreamJourney\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u6001\u573a\u666f\u7684\u8fde\u7eed\u89c6\u89d2\u751f\u6210\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DreamJourney\uff1a\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8fde\u7eed\u89c6\u89d2\u751f\u6210", "abstract_zh": "\u8fde\u7eed\u89c6\u89d2\u751f\u6210\u65e8\u5728\u4ece\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210\u4e0e\u4efb\u610f\u76f8\u673a\u8f68\u8ff9\u5bf9\u5e94\u7684\u957f\u671f\u89c6\u9891\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5408\u6210\u65b0\u5185\u5bb9\uff0c\u4f462D\u6269\u6563\u6a21\u578b\u7f3a\u4e4f3D\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u753b\u9762\u626d\u66f2\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u9759\u60013D\u573a\u666f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DreamJourney\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u62df\u80fd\u529b\uff0c\u5b9e\u73b0\u5305\u542b\u76f8\u673a\u8fd0\u52a8\u548c\u5bf9\u8c61\u52a8\u6001\u7684\u8fde\u7eed\u573a\u666f\u751f\u6210\u3002\u7b2c\u4e00\u9636\u6bb5\uff0cDreamJourney\u5c06\u8f93\u5165\u56fe\u50cf\u63d0\u5347\u4e3a3D\u70b9\u4e91\uff0c\u5e76\u4ece\u7279\u5b9a\u76f8\u673a\u8f68\u8ff9\u6e32\u67d3\u90e8\u5206\u56fe\u50cf\u5e8f\u5217\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u8865\u5168\u7f3a\u5931\u533a\u57df\u5e76\u589e\u5f3a\u89c6\u89c9\u4e00\u81f4\u6027\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63cf\u8ff0\u5bf9\u8c61\u8fd0\u52a8\u7684\u6587\u672c\u63d0\u793a\uff0c\u5e76\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a8\u6001\u5316\u5f53\u524d\u89c6\u89d2\u3002\u4e24\u9636\u6bb5\u5faa\u73af\u6267\u884c\uff0c\u5b9e\u73b0\u8fde\u7eed\u52a8\u6001\u573a\u666f\u751f\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDreamJourney\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://dream-journey.vercel.app\u3002"}}
{"id": "2506.18424", "pdf": "https://arxiv.org/pdf/2506.18424", "abs": "https://arxiv.org/abs/2506.18424", "authors": ["Chengjie Liu", "Weiyu Chen", "Huiyao Xu", "Yuan Du", "Jun Yang", "Li Du"], "title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction", "categories": ["cs.AI", "cs.ET"], "comment": "Accepted by ISEDA 2025", "summary": "In the design process of the analog circuit pre-layout phase, device sizing\nis an important step in determining whether an analog circuit can meet the\nrequired performance metrics. Many existing techniques extract the circuit\nsizing task as a mathematical optimization problem to solve and continuously\nimprove the optimization efficiency from a mathematical perspective. But they\nignore the automatic introduction of prior knowledge, fail to achieve effective\npruning of the search space, which thereby leads to a considerable compression\nmargin remaining in the search space. To alleviate this problem, we propose a\nlarge language model (LLM)-based multi-agent framework for analog circuits'\nsizing relationships extraction from academic papers. The search space in the\nsizing process can be effectively pruned based on the sizing relationship\nextracted by this framework. Eventually, we conducted tests on 3 types of\ncircuits, and the optimization efficiency was improved by $2.32 \\sim 26.6\n\\times$. This work demonstrates that the LLM can effectively prune the search\nspace for analog circuit sizing, providing a new solution for the combination\nof LLMs and conventional analog circuit design automation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5b66\u672f\u8bba\u6587\u4e2d\u63d0\u53d6\u6a21\u62df\u7535\u8def\u7684\u5c3a\u5bf8\u5173\u7cfb\uff0c\u4ece\u800c\u6709\u6548\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u5347\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u5728\u6a21\u62df\u7535\u8def\u9884\u5e03\u5c40\u9636\u6bb5\uff0c\u5668\u4ef6\u5c3a\u5bf8\u8bbe\u8ba1\u662f\u51b3\u5b9a\u7535\u8def\u6027\u80fd\u7684\u5173\u952e\u6b65\u9aa4\u3002\u73b0\u6709\u6280\u672f\u591a\u5c06\u5176\u89c6\u4e3a\u6570\u5b66\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u5ffd\u7565\u4e86\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u52a8\u5f15\u5165\uff0c\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u538b\u7f29\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7LLM\u63d0\u53d6\u5c3a\u5bf8\u5173\u7cfb\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4ece\u5b66\u672f\u8bba\u6587\u4e2d\u81ea\u52a8\u63d0\u53d6\u6a21\u62df\u7535\u8def\u7684\u5c3a\u5bf8\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u57283\u7c7b\u7535\u8def\u4e0a\u6d4b\u8bd5\uff0c\u4f18\u5316\u6548\u7387\u63d0\u5347\u4e862.32\u81f326.6\u500d\uff0c\u8bc1\u660eLLM\u80fd\u6709\u6548\u4fee\u526a\u6a21\u62df\u7535\u8def\u5c3a\u5bf8\u8bbe\u8ba1\u7684\u641c\u7d22\u7a7a\u95f4\u3002", "conclusion": "LLM\u4e0e\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u7ed3\u5408\u4e3a\u641c\u7d22\u7a7a\u95f4\u4fee\u526a\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u7387\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u6a21\u62df\u7535\u8def\u5c3a\u5bf8\u5173\u7cfb\u63d0\u53d6", "abstract_zh": "\u5728\u6a21\u62df\u7535\u8def\u9884\u5e03\u5c40\u9636\u6bb5\u7684\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\uff0c\u5668\u4ef6\u5c3a\u5bf8\u662f\u51b3\u5b9a\u7535\u8def\u80fd\u5426\u6ee1\u8db3\u6027\u80fd\u6307\u6807\u7684\u91cd\u8981\u6b65\u9aa4\u3002\u73b0\u6709\u6280\u672f\u591a\u5c06\u7535\u8def\u5c3a\u5bf8\u8bbe\u8ba1\u4efb\u52a1\u63d0\u53d6\u4e3a\u6570\u5b66\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4ece\u6570\u5b66\u89d2\u5ea6\u6301\u7eed\u63d0\u5347\u4f18\u5316\u6548\u7387\uff0c\u4f46\u5ffd\u7565\u4e86\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u52a8\u5f15\u5165\uff0c\u672a\u80fd\u5b9e\u73b0\u641c\u7d22\u7a7a\u95f4\u7684\u6709\u6548\u4fee\u526a\uff0c\u4ece\u800c\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u4ecd\u5b58\u5728\u8f83\u5927\u538b\u7f29\u7a7a\u95f4\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5b66\u672f\u8bba\u6587\u4e2d\u63d0\u53d6\u6a21\u62df\u7535\u8def\u7684\u5c3a\u5bf8\u5173\u7cfb\u3002\u8be5\u6846\u67b6\u63d0\u53d6\u7684\u5c3a\u5bf8\u5173\u7cfb\u53ef\u6709\u6548\u4fee\u526a\u5c3a\u5bf8\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u57283\u7c7b\u7535\u8def\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4f18\u5316\u6548\u7387\u63d0\u5347\u4e862.32\u81f326.6\u500d\u3002\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0cLLM\u80fd\u6709\u6548\u4fee\u526a\u6a21\u62df\u7535\u8def\u5c3a\u5bf8\u8bbe\u8ba1\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4e3aLLM\u4e0e\u4f20\u7edf\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18035", "pdf": "https://arxiv.org/pdf/2506.18035", "abs": "https://arxiv.org/abs/2506.18035", "authors": ["Maxence Lasbordes", "Daniele Falavigna", "Alessio Brutti"], "title": "Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T50 (Primary)", "I.2.7; I.5.4"], "comment": "5 pages, 3 Postscript figures", "summary": "The ability to dynamically adjust the computational load of neural models\nduring inference in a resource aware manner is crucial for on-device processing\nscenarios, characterised by limited and time-varying computational resources.\nEarly-exit architectures represent an elegant and effective solution, since\nthey can process the input with a subset of their layers, exiting at\nintermediate branches (the upmost layers are hence removed from the model).\n  From a different perspective, for automatic speech recognition applications\nthere are memory-efficient neural architectures that apply variable frame rate\nanalysis, through downsampling/upsampling operations in the middle layers,\nreducing the overall number of operations and improving significantly the\nperformance on well established benchmarks. One example is the Zipformer.\nHowever, these architectures lack the modularity necessary to inject early-exit\nbranches.\n  With the aim of improving the performance in early-exit models, we propose\nintroducing parallel layers in the architecture that process downsampled\nversions of their inputs. % in conjunction with standard processing layers. We\nshow that in this way the speech recognition performance on standard benchmarks\nsignificantly improve, at the cost of a small increase in the overall number of\nmodel parameters but without affecting the inference time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u65e9\u671f\u9000\u51fa\u67b6\u6784Splitformer\uff0c\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff0c\u901a\u8fc7\u5f15\u5165\u5e76\u884c\u5c42\u5904\u7406\u4e0b\u91c7\u6837\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u5c0f\u5e45\u589e\u52a0\u6a21\u578b\u53c2\u6570\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u9700\u8981\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u8d1f\u8f7d\u3002\u65e9\u671f\u9000\u51fa\u67b6\u6784\u867d\u80fd\u901a\u8fc7\u4e2d\u95f4\u5206\u652f\u51cf\u5c11\u8ba1\u7b97\uff0c\u4f46\u73b0\u6709\u9ad8\u6548\u67b6\u6784\uff08\u5982Zipformer\uff09\u7f3a\u4e4f\u6a21\u5757\u5316\u8bbe\u8ba1\u4ee5\u652f\u6301\u65e9\u671f\u9000\u51fa\u5206\u652f\u3002", "method": "\u63d0\u51faSplitformer\u67b6\u6784\uff0c\u5728\u6807\u51c6\u5904\u7406\u5c42\u57fa\u7840\u4e0a\u5f15\u5165\u5e76\u884c\u5c42\uff0c\u5904\u7406\u8f93\u5165\u7684\u4e0b\u91c7\u6837\u7248\u672c\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u63a8\u7406\u65f6\u95f4\u4e0d\u53d8\u3002", "result": "\u5728\u6807\u51c6\u8bed\u97f3\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSplitformer\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4ec5\u5c0f\u5e45\u589e\u52a0\u6a21\u578b\u53c2\u6570\uff0c\u4e14\u672a\u5f71\u54cd\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "Splitformer\u901a\u8fc7\u7ed3\u5408\u5e76\u884c\u5c42\u4e0e\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "Splitformer\uff1a\u4e00\u79cd\u6539\u8fdb\u7684\u8fb9\u7f18\u8bbe\u5907\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u65e9\u671f\u9000\u51fa\u67b6\u6784", "abstract_zh": "\u5728\u8d44\u6e90\u53d7\u9650\u4e14\u52a8\u6001\u53d8\u5316\u7684\u8fb9\u7f18\u8bbe\u5907\u573a\u666f\u4e2d\uff0c\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u8ba1\u7b97\u8d1f\u8f7d\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u65e9\u671f\u9000\u51fa\u67b6\u6784\u901a\u8fc7\u4e2d\u95f4\u5206\u652f\u51cf\u5c11\u8ba1\u7b97\u5c42\u6570\uff0c\u662f\u4e00\u79cd\u4f18\u96c5\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u9ad8\u6548\u67b6\u6784\uff08\u5982Zipformer\uff09\u867d\u901a\u8fc7\u4e2d\u95f4\u5c42\u7684\u4e0b\u91c7\u6837/\u4e0a\u91c7\u6837\u64cd\u4f5c\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u7f3a\u4e4f\u652f\u6301\u65e9\u671f\u9000\u51fa\u5206\u652f\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u3002\u4e3a\u63d0\u5347\u65e9\u671f\u9000\u51fa\u6a21\u578b\u7684\u6027\u80fd\uff0c\u672c\u6587\u63d0\u51fa\u5728\u67b6\u6784\u4e2d\u5f15\u5165\u5e76\u884c\u5c42\u5904\u7406\u8f93\u5165\u7684\u4e0b\u91c7\u6837\u7248\u672c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6807\u51c6\u8bed\u97f3\u8bc6\u522b\u57fa\u51c6\u7684\u6027\u80fd\uff0c\u4ec5\u5c0f\u5e45\u589e\u52a0\u6a21\u578b\u53c2\u6570\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u65f6\u95f4\u3002"}}
{"id": "2506.17707", "pdf": "https://arxiv.org/pdf/2506.17707", "abs": "https://arxiv.org/abs/2506.17707", "authors": ["Jihyun Kim", "Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "We present Programmable-Room, a framework which interactively generates and\nedits a 3D room mesh, given natural language instructions. For precise control\nof a room's each attribute, we decompose the challenging task into simpler\nsteps such as creating plausible 3D coordinates for room meshes, generating\npanorama images for the texture, constructing 3D meshes by integrating the\ncoordinates and panorama texture images, and arranging furniture. To support\nthe various decomposed tasks with a unified framework, we incorporate visual\nprogramming (VP). VP is a method that utilizes a large language model (LLM) to\nwrite a Python-like program which is an ordered list of necessary modules for\nthe various tasks given in natural language. We develop most of the modules.\nEspecially, for the texture generating module, we utilize a pretrained\nlarge-scale diffusion model to generate panorama images conditioned on text and\nvisual prompts (i.e., layout, depth, and semantic map) simultaneously.\nSpecifically, we enhance the panorama image generation quality by optimizing\nthe training objective with a 1D representation of a panorama scene obtained\nfrom bidirectional LSTM. We demonstrate Programmable-Room's flexibility in\ngenerating and editing 3D room meshes, and prove our framework's superiority to\nan existing model quantitatively and qualitatively. Project page is available\nin https://jihyun0510.github.io/Programmable_Room_Page/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faProgrammable-Room\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4ea4\u4e92\u5f0f\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7a0b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u5728\u7eb9\u7406\u751f\u6210\u548c\u623f\u95f4\u5e03\u5c40\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u67093D\u623f\u95f4\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7cbe\u786e\u63a7\u5236\u623f\u95f4\u7684\u5404\u4e2a\u5c5e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u89e3\u4efb\u52a1\u5e76\u7edf\u4e00\u652f\u6301\u7684\u6846\u67b6\u3002", "method": "\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u751f\u62103D\u5750\u6807\u3001\u5168\u666f\u56fe\u50cf\u7eb9\u7406\u3001\u6784\u5efa3D\u7f51\u683c\u548c\u5bb6\u5177\u5e03\u7f6e\u7b49\u6b65\u9aa4\uff0c\u5229\u7528\u89c6\u89c9\u7f16\u7a0b\uff08VP\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210Python\u5f0f\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f18\u5316\u5168\u666f\u56fe\u50cf\u751f\u6210\u3002", "result": "Programmable-Room\u5728\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u4e0a\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u4e863D\u623f\u95f4\u7684\u9ad8\u6548\u751f\u6210\u4e0e\u7f16\u8f91\uff0c\u4e3a\u672a\u6765\u4ea4\u4e92\u5f0f3D\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u53ef\u7f16\u7a0b\u623f\u95f4\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u7eb9\u74063D\u623f\u95f4\u7f51\u683c\u751f\u6210", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86Programmable-Room\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4ea4\u4e92\u5f0f\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u3002\u4e3a\u4e86\u7cbe\u786e\u63a7\u5236\u623f\u95f4\u7684\u6bcf\u4e2a\u5c5e\u6027\uff0c\u6211\u4eec\u5c06\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u7b80\u5355\u6b65\u9aa4\uff0c\u5305\u62ec\u751f\u6210\u623f\u95f4\u7f51\u683c\u7684\u5408\u74063D\u5750\u6807\u3001\u751f\u6210\u5168\u666f\u56fe\u50cf\u7eb9\u7406\u3001\u6574\u5408\u5750\u6807\u548c\u5168\u666f\u56fe\u50cf\u6784\u5efa3D\u7f51\u683c\uff0c\u4ee5\u53ca\u5e03\u7f6e\u5bb6\u5177\u3002\u4e3a\u4e86\u7528\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u652f\u6301\u8fd9\u4e9b\u5206\u89e3\u4efb\u52a1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u89c6\u89c9\u7f16\u7a0b\uff08VP\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7f16\u5199\u7c7b\u4f3cPython\u7684\u7a0b\u5e8f\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4efb\u52a1\u8f6c\u5316\u4e3a\u6709\u5e8f\u7684\u6a21\u5757\u5217\u8868\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u5927\u90e8\u5206\u6a21\u5757\uff0c\u5c24\u5176\u662f\u7eb9\u7406\u751f\u6210\u6a21\u5757\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\uff0c\u5728\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\uff08\u5982\u5e03\u5c40\u3001\u6df1\u5ea6\u548c\u8bed\u4e49\u56fe\uff09\u7684\u6761\u4ef6\u4e0b\u751f\u6210\u5168\u666f\u56fe\u50cf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\uff0c\u4f7f\u7528\u53cc\u5411LSTM\u83b7\u53d6\u76841D\u5168\u666f\u573a\u666f\u8868\u793a\uff0c\u63d0\u5347\u4e86\u5168\u666f\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u3002\u6211\u4eec\u5c55\u793a\u4e86Programmable-Room\u5728\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u4e0a\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002\u9879\u76ee\u9875\u9762\u8be6\u89c1\uff1ahttps://jihyun0510.github.io/Programmable_Room_Page/\u3002"}}
{"id": "2506.18428", "pdf": "https://arxiv.org/pdf/2506.18428", "abs": "https://arxiv.org/abs/2506.18428", "authors": ["Feng He", "Zhenyang Liu", "Marco Valentino", "Zhixue Zhao"], "title": "How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Model editing offers a low-cost technique to inject or correct a particular\nbehavior in a pre-trained model without extensive retraining, supporting\napplications such as factual correction and bias mitigation. Despite this\ncommon practice, it remains unknown whether edits persist after fine-tuning or\nwhether they are inadvertently reversed. This question has fundamental\npractical implications. For example, if fine-tuning removes prior edits, it\ncould serve as a defence mechanism against hidden malicious edits. Vice versa,\nthe unintended removal of edits related to bias mitigation could pose serious\nsafety concerns. We systematically investigate the interaction between model\nediting and fine-tuning in the context of T2I diffusion models, which are known\nto exhibit biases and generate inappropriate content. Our study spans two T2I\nmodel families (Stable Diffusion and FLUX), two sota editing techniques, and\nthree fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive\nempirical analysis across diverse editing tasks and evaluation metrics, our\nfindings reveal a trend: edits generally fail to persist through fine-tuning,\neven when fine-tuning is tangential or unrelated to the edits. Notably, we\nobserve that DoRA exhibits the strongest edit reversal effect. At the same\ntime, among editing methods, UCE demonstrates greater robustness, retaining\nsignificantly higher efficacy post-fine-tuning compared to ReFACT. These\nfindings highlight a crucial limitation in current editing methodologies,\nemphasizing the need for more robust techniques to ensure reliable long-term\ncontrol and alignment of deployed AI systems. These findings have dual\nimplications for AI safety: they suggest that fine-tuning could serve as a\nremediation mechanism for malicious edits while simultaneously highlighting the\nneed for re-editing after fine-tuning to maintain beneficial safety and\nalignment properties.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5fae\u8c03\u540e\u6a21\u578b\u7f16\u8f91\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u7f16\u8f91\u6548\u679c\u901a\u5e38\u65e0\u6cd5\u5728\u5fae\u8c03\u540e\u4fdd\u7559\uff0c\u5c24\u5176\u662fDoRA\u65b9\u6cd5\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u7f16\u8f91\u53cd\u8f6c\u6548\u679c\uff0c\u800cUCE\u7f16\u8f91\u65b9\u6cd5\u76f8\u5bf9\u66f4\u9c81\u68d2\u3002", "motivation": "\u6a21\u578b\u7f16\u8f91\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u6ce8\u5165\u6216\u4fee\u6b63\u7279\u5b9a\u884c\u4e3a\uff0c\u4f46\u7f16\u8f91\u540e\u662f\u5426\u80fd\u5728\u5fae\u8c03\u4e2d\u4fdd\u7559\u5c1a\u4e0d\u660e\u786e\u3002\u8fd9\u4e00\u95ee\u9898\u5bf9AI\u5b89\u5168\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f8b\u5982\u5fae\u8c03\u53ef\u80fd\u6d88\u9664\u6076\u610f\u7f16\u8f91\uff0c\u4f46\u4e5f\u53ef\u80fd\u610f\u5916\u79fb\u9664\u6709\u76ca\u7684\u5b89\u5168\u4fee\u6b63\u3002", "method": "\u7814\u7a76\u5728\u4e24\u79cdT2I\u6a21\u578b\uff08Stable Diffusion\u548cFLUX\uff09\u4e0a\uff0c\u7ed3\u5408\u4e24\u79cd\u7f16\u8f91\u6280\u672f\uff08UCE\u548cReFACT\uff09\u548c\u4e09\u79cd\u5fae\u8c03\u65b9\u6cd5\uff08DreamBooth\u3001LoRA\u548cDoRA\uff09\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u7f16\u8f91\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7f16\u8f91\u901a\u5e38\u5728\u5fae\u8c03\u540e\u5931\u6548\uff0c\u5373\u4f7f\u5fae\u8c03\u4e0e\u7f16\u8f91\u65e0\u5173\u3002DoRA\u5bf9\u7f16\u8f91\u7684\u9006\u8f6c\u6548\u679c\u6700\u5f3a\uff0c\u800cUCE\u5728\u5fae\u8c03\u540e\u4fdd\u7559\u7684\u7f16\u8f91\u6548\u679c\u663e\u8457\u4f18\u4e8eReFACT\u3002", "conclusion": "\u5f53\u524d\u7f16\u8f91\u65b9\u6cd5\u5728\u5fae\u8c03\u540e\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u9700\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u6280\u672f\u4ee5\u786e\u4fdd\u957f\u671f\u63a7\u5236\u548cAI\u7cfb\u7edf\u5bf9\u9f50\u3002\u7814\u7a76\u540c\u65f6\u8868\u660e\u5fae\u8c03\u53ef\u4f5c\u4e3a\u6076\u610f\u7f16\u8f91\u7684\u4fee\u590d\u673a\u5236\uff0c\u4f46\u4e5f\u9700\u5728\u5fae\u8c03\u540e\u91cd\u65b0\u7f16\u8f91\u4ee5\u7ef4\u6301\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u3002", "paper_title_zh": "\u5fae\u8c03\u540e\u6a21\u578b\u7f16\u8f91\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff1f\u2014\u2014\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5b9e\u8bc1\u7814\u7a76", "abstract_zh": "\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5411\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u6ce8\u5165\u6216\u4fee\u6b63\u7279\u5b9a\u884c\u4e3a\uff0c\u652f\u6301\u8bf8\u5982\u4e8b\u5b9e\u4fee\u6b63\u548c\u504f\u89c1\u7f13\u89e3\u7b49\u5e94\u7528\u3002\u5c3d\u7ba1\u8fd9\u662f\u4e00\u79cd\u5e38\u89c1\u505a\u6cd5\uff0c\u4f46\u7f16\u8f91\u662f\u5426\u80fd\u5728\u5fae\u8c03\u540e\u4fdd\u7559\uff0c\u6216\u662f\u5426\u4f1a\u88ab\u65e0\u610f\u4e2d\u9006\u8f6c\uff0c\u4ecd\u662f\u4e00\u4e2a\u672a\u77e5\u95ee\u9898\u3002\u8fd9\u4e00\u95ee\u9898\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u610f\u4e49\u3002\u4f8b\u5982\uff0c\u5982\u679c\u5fae\u8c03\u6d88\u9664\u4e86\u5148\u524d\u7684\u7f16\u8f91\uff0c\u5b83\u53ef\u80fd\u6210\u4e3a\u5bf9\u6297\u9690\u85cf\u6076\u610f\u7f16\u8f91\u7684\u9632\u5fa1\u673a\u5236\uff1b\u53cd\u4e4b\uff0c\u82e5\u610f\u5916\u79fb\u9664\u4e86\u4e0e\u504f\u89c1\u7f13\u89e3\u76f8\u5173\u7684\u7f16\u8f91\uff0c\u5219\u53ef\u80fd\u5f15\u53d1\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u4e2d\u6a21\u578b\u7f16\u8f91\u4e0e\u5fae\u8c03\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u7c7b\u6a21\u578b\u5df2\u77e5\u5b58\u5728\u504f\u89c1\u5e76\u53ef\u80fd\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u3002\u6211\u4eec\u7684\u7814\u7a76\u6db5\u76d6\u4e24\u79cdT2I\u6a21\u578b\u5bb6\u65cf\uff08Stable Diffusion\u548cFLUX\uff09\u3001\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u6280\u672f\u4ee5\u53ca\u4e09\u79cd\u5fae\u8c03\u65b9\u6cd5\uff08DreamBooth\u3001LoRA\u548cDoRA\uff09\u3002\u901a\u8fc7\u591a\u6837\u5316\u7684\u7f16\u8f91\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u7684\u53d1\u73b0\u63ed\u793a\u4e86\u4e00\u4e2a\u8d8b\u52bf\uff1a\u7f16\u8f91\u901a\u5e38\u65e0\u6cd5\u5728\u5fae\u8c03\u540e\u4fdd\u7559\uff0c\u5373\u4f7f\u5fae\u8c03\u4e0e\u7f16\u8f91\u65e0\u5173\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0DoRA\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u7f16\u8f91\u9006\u8f6c\u6548\u679c\u3002\u540c\u65f6\uff0c\u5728\u7f16\u8f91\u65b9\u6cd5\u4e2d\uff0cUCE\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5fae\u8c03\u540e\u4fdd\u7559\u7684\u7f16\u8f91\u6548\u679c\u663e\u8457\u4f18\u4e8eReFACT\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5f53\u524d\u7f16\u8f91\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u6280\u672f\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u5bf9\u90e8\u7f72\u7684AI\u7cfb\u7edf\u8fdb\u884c\u53ef\u9760\u7684\u957f\u671f\u63a7\u5236\u548c\u5bf9\u9f50\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9AI\u5b89\u5168\u5177\u6709\u53cc\u91cd\u610f\u4e49\uff1a\u5b83\u4eec\u8868\u660e\u5fae\u8c03\u53ef\u4ee5\u4f5c\u4e3a\u6076\u610f\u7f16\u8f91\u7684\u4fee\u590d\u673a\u5236\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u5728\u5fae\u8c03\u540e\u91cd\u65b0\u7f16\u8f91\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u7ef4\u6301\u6709\u76ca\u7684\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u5c5e\u6027\u3002"}}
{"id": "2506.18036", "pdf": "https://arxiv.org/pdf/2506.18036", "abs": "https://arxiv.org/abs/2506.18036", "authors": ["Aziz Amari", "Mohamed Achref Ben Ammar"], "title": "Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The rapid expansion of information from diverse sources has heightened the\nneed for effective automatic text summarization, which condenses documents into\nshorter, coherent texts. Summarization methods generally fall into two\ncategories: extractive, which selects key segments from the original text, and\nabstractive, which generates summaries by rephrasing the content coherently.\nLarge language models have advanced the field of abstractive summarization, but\nthey are resourceintensive and face significant challenges in retaining key\ninformation across lengthy documents, which we call being \"lost in the middle\".\nTo address these issues, we propose a hybrid summarization approach that\ncombines extractive and abstractive techniques. Our method splits the document\ninto smaller text chunks, clusters their vector embeddings, generates a summary\nfor each cluster that represents a key idea in the document, and constructs the\nfinal summary by relying on a Markov chain graph when selecting the semantic\norder of ideas.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63d0\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6458\u8981\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u3001\u805a\u7c7b\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u56fe\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u6863\u6458\u8981\u4e2d\u2018\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u2019\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u4fe1\u606f\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u81ea\u52a8\u6587\u672c\u6458\u8981\u9700\u6c42\u6fc0\u589e\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f0f\u6458\u8981\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u957f\u6587\u6863\u65f6\u5bb9\u6613\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff08\u2018\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u2019\u95ee\u9898\uff09\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u56db\u6b65\uff1a1) \u5c06\u6587\u6863\u5206\u5757\uff1b2) \u5bf9\u5757\u5411\u91cf\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\uff1b3) \u4e3a\u6bcf\u4e2a\u805a\u7c7b\u751f\u6210\u6458\u8981\uff1b4) \u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u56fe\u9009\u62e9\u8bed\u4e49\u987a\u5e8f\uff0c\u6784\u5efa\u6700\u7ec8\u6458\u8981\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u6863\u6458\u8981\u4e2d\u7684\u2018\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u2019\u95ee\u9898\uff0c\u751f\u6210\u66f4\u8fde\u8d2f\u4e14\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684\u6458\u8981\u3002", "conclusion": "\u7ed3\u5408\u63d0\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6458\u8981\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u3001\u805a\u7c7b\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u6863\u6458\u8981\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "paper_title_zh": "\u9a6c\u5c14\u53ef\u592b\u589e\u5f3a\u805a\u7c7b\u7528\u4e8e\u957f\u6587\u6863\u6458\u8981\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u2018\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u2019\u6311\u6218", "abstract_zh": "\u4fe1\u606f\u6e90\u7684\u5feb\u901f\u6269\u5c55\u52a0\u5267\u4e86\u5bf9\u9ad8\u6548\u81ea\u52a8\u6587\u672c\u6458\u8981\u7684\u9700\u6c42\uff0c\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u63d0\u53d6\u5f0f\uff08\u4ece\u539f\u6587\u9009\u62e9\u5173\u952e\u7247\u6bb5\uff09\u548c\u751f\u6210\u5f0f\uff08\u901a\u8fc7\u91cd\u8ff0\u5185\u5bb9\u751f\u6210\u6458\u8981\uff09\u3002\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u4e86\u751f\u6210\u5f0f\u6458\u8981\u7684\u53d1\u5c55\uff0c\u4f46\u5176\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u5728\u957f\u6587\u6863\u4e2d\u6613\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff08\u79f0\u4e3a\u2018\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u2019\uff09\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63d0\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6280\u672f\u7684\u6df7\u5408\u6458\u8981\u65b9\u6cd5\uff1a\u5c06\u6587\u6863\u5206\u5757\u3001\u805a\u7c7b\u5176\u5411\u91cf\u5d4c\u5165\u3001\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u751f\u6210\u4ee3\u8868\u5173\u952e\u601d\u60f3\u7684\u6458\u8981\uff0c\u5e76\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u56fe\u9009\u62e9\u8bed\u4e49\u987a\u5e8f\u6784\u5efa\u6700\u7ec8\u6458\u8981\u3002"}}
{"id": "2506.17712", "pdf": "https://arxiv.org/pdf/2506.17712", "abs": "https://arxiv.org/abs/2506.17712", "authors": ["Xinyu Xiong", "Wuteng Cao", "Zihuang Wu", "Lei Zhang", "Chong Gao", "Guanbin Li", "Qiyuan Qin"], "title": "PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic\nResonance Images (MRI) is crucial for more precise prognosis assessment and the\ndevelopment of personalized treatment plans. However, automated segmentation\nremains challenging due to factors such as complex organ morphologies and\nconfusing context. To address these challenges, we propose a novel Pattern\nDivide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to\nuse different network modules to \"divide\" various local and global patterns\nand, through flexible feature selection, to \"conquer\" the Regions of Interest\n(ROI) during the decoding phase. Specifically, considering that our ROI often\nmanifests as strip-like or circular-like structures in MR slices, we introduce\na Multi-Direction Aggregation (MDA) module. This module enhances the model's\nability to fit the shape of the organ by applying strip convolutions in four\ndistinct directions. Additionally, to mitigate the challenge of confusing\ncontext, we propose a Memory-Guided Context (MGC) module. This module\nexplicitly maintains a memory parameter to track cross-image patterns at the\ndataset level, thereby enhancing the distinction between global patterns\nassociated with the positive and negative classes. Finally, we design an\nAdaptive Fusion Decoder (AFD) that dynamically selects features from different\npatterns based on the Mixture-of-Experts (MoE) framework, ultimately generating\nthe final segmentation results. We evaluate our method on the first large-scale\npelvic radiation injury dataset, and the results demonstrate the superiority of\nour PDC-Net over existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684PDC-Net\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u805a\u5408\u6a21\u5757\u548c\u8bb0\u5fc6\u5f15\u5bfc\u4e0a\u4e0b\u6587\u6a21\u5757\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u76c6\u8154\u653e\u5c04\u635f\u4f24\u7684\u7cbe\u786e\u5206\u5272\u3002", "motivation": "\u76c6\u8154\u653e\u5c04\u635f\u4f24\uff08PRI\uff09\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u9884\u540e\u8bc4\u4f30\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5668\u5b98\u5f62\u6001\u590d\u6742\u548c\u4e0a\u4e0b\u6587\u6df7\u6dc6\uff0c\u81ea\u52a8\u5316\u5206\u5272\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "PDC-Net\u91c7\u7528\u591a\u65b9\u5411\u805a\u5408\u6a21\u5757\uff08MDA\uff09\u589e\u5f3a\u5bf9\u5668\u5b98\u5f62\u72b6\u7684\u62df\u5408\u80fd\u529b\uff0c\u5f15\u5165\u8bb0\u5fc6\u5f15\u5bfc\u4e0a\u4e0b\u6587\u6a21\u5757\uff08MGC\uff09\u63d0\u5347\u5168\u5c40\u6a21\u5f0f\u533a\u5206\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u89e3\u7801\u5668\uff08AFD\uff09\u52a8\u6001\u9009\u62e9\u7279\u5f81\u751f\u6210\u6700\u7ec8\u5206\u5272\u7ed3\u679c\u3002", "result": "\u5728\u9996\u4e2a\u5927\u89c4\u6a21\u76c6\u8154\u653e\u5c04\u635f\u4f24\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPDC-Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PDC-Net\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u548c\u7075\u6d3b\u7279\u5f81\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86PRI\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "paper_title_zh": "PDC-Net\uff1a\u57fa\u4e8e\u6a21\u5f0f\u5206\u6cbb\u7f51\u7edc\u7684\u76c6\u8154\u653e\u5c04\u635f\u4f24\u5206\u5272", "abstract_zh": "\u4ece\u78c1\u5171\u632f\u56fe\u50cf\uff08MRI\uff09\u4e2d\u7cbe\u786e\u5206\u5272\u76c6\u8154\u653e\u5c04\u635f\u4f24\uff08PRI\uff09\u5bf9\u4e8e\u66f4\u51c6\u786e\u7684\u9884\u540e\u8bc4\u4f30\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u8ba1\u5212\u7684\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u590d\u6742\u7684\u5668\u5b98\u5f62\u6001\u548c\u4e0a\u4e0b\u6587\u6df7\u6dc6\u7b49\u56e0\u7d20\uff0c\u81ea\u52a8\u5316\u5206\u5272\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6a21\u5f0f\u5206\u6cbb\u7f51\u7edc\uff08PDC-Net\uff09\u7528\u4e8ePRI\u5206\u5272\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u4e0d\u540c\u7f51\u7edc\u6a21\u5757\u201c\u5206\u6cbb\u201d\u5404\u79cd\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u5f0f\uff0c\u5e76\u5728\u89e3\u7801\u9636\u6bb5\u901a\u8fc7\u7075\u6d3b\u7684\u7279\u5f81\u9009\u62e9\u201c\u653b\u514b\u201d\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8003\u8651\u5230ROI\u5728MR\u5207\u7247\u4e2d\u5e38\u8868\u73b0\u4e3a\u6761\u72b6\u6216\u73af\u72b6\u7ed3\u6784\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u65b9\u5411\u805a\u5408\u6a21\u5757\uff08MDA\uff09\uff0c\u901a\u8fc7\u5728\u56db\u4e2a\u4e0d\u540c\u65b9\u5411\u4e0a\u5e94\u7528\u6761\u72b6\u5377\u79ef\u589e\u5f3a\u6a21\u578b\u5bf9\u5668\u5b98\u5f62\u72b6\u7684\u62df\u5408\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4e3a\u7f13\u89e3\u4e0a\u4e0b\u6587\u6df7\u6dc6\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bb0\u5fc6\u5f15\u5bfc\u4e0a\u4e0b\u6587\u6a21\u5757\uff08MGC\uff09\uff0c\u8be5\u6a21\u5757\u663e\u5f0f\u7ef4\u62a4\u4e00\u4e2a\u8bb0\u5fc6\u53c2\u6570\u4ee5\u8ddf\u8e2a\u6570\u636e\u96c6\u7ea7\u522b\u7684\u8de8\u56fe\u50cf\u6a21\u5f0f\uff0c\u4ece\u800c\u589e\u5f3a\u4e0e\u6b63\u8d1f\u7c7b\u522b\u76f8\u5173\u7684\u5168\u5c40\u6a21\u5f0f\u7684\u533a\u5206\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff08MoE\uff09\u7684\u81ea\u9002\u5e94\u878d\u5408\u89e3\u7801\u5668\uff08AFD\uff09\uff0c\u52a8\u6001\u9009\u62e9\u4e0d\u540c\u6a21\u5f0f\u7684\u7279\u5f81\uff0c\u6700\u7ec8\u751f\u6210\u5206\u5272\u7ed3\u679c\u3002\u6211\u4eec\u5728\u9996\u4e2a\u5927\u89c4\u6a21\u76c6\u8154\u653e\u5c04\u635f\u4f24\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660ePDC-Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18511", "pdf": "https://arxiv.org/pdf/2506.18511", "abs": "https://arxiv.org/abs/2506.18511", "authors": ["Yu Han", "Aaron Ceross", "Jeroen H. M. Bergmann"], "title": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance", "categories": ["cs.AI"], "comment": null, "summary": "Identifying the appropriate regulatory standard applicability remains a\ncritical yet understudied challenge in medical device compliance, frequently\nnecessitating expert interpretation of fragmented and heterogeneous\ndocumentation across different jurisdictions. To address this challenge, we\nintroduce a modular AI system that leverages a retrieval-augmented generation\n(RAG) pipeline to automate standard applicability determination. Given a\nfree-text device description, our system retrieves candidate standards from a\ncurated corpus and uses large language models to infer jurisdiction-specific\napplicability, classified as Mandatory, Recommended, or Not Applicable, with\ntraceable justifications. We construct an international benchmark dataset of\nmedical device descriptions with expert-annotated standard mappings, and\nevaluate our system against retrieval-only, zero-shot, and rule-based\nbaselines. The proposed approach attains a classification accuracy of 73% and a\nTop-5 retrieval recall of 87%, demonstrating its effectiveness in identifying\nrelevant regulatory standards. We introduce the first end-to-end system for\nstandard applicability reasoning, enabling scalable and interpretable\nAI-supported regulatory science. Notably, our region-aware RAG agent performs\ncross-jurisdictional reasoning between Chinese and U.S. standards, supporting\nconflict resolution and applicability justification across regulatory\nframeworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6a21\u5757\u5316AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5224\u65ad\u533b\u7597\u5668\u68b0\u7684\u6cd5\u89c4\u6807\u51c6\u9002\u7528\u6027\uff0c\u652f\u6301\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\uff08\u5982\u4e2d\u7f8e\uff09\u7684\u51b2\u7a81\u89e3\u51b3\u548c\u9002\u7528\u6027\u8bba\u8bc1\u3002", "motivation": "\u533b\u7597\u5668\u68b0\u5408\u89c4\u6027\u4e2d\uff0c\u786e\u5b9a\u9002\u7528\u7684\u6cd5\u89c4\u6807\u51c6\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u901a\u5e38\u9700\u8981\u4e13\u5bb6\u5bf9\u5206\u6563\u4e14\u5f02\u6784\u7684\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\u6587\u6863\u8fdb\u884c\u89e3\u91ca\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7RAG\u6d41\u7a0b\uff0c\u6839\u636e\u81ea\u7531\u6587\u672c\u8bbe\u5907\u63cf\u8ff0\u4ece\u7cbe\u9009\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u5019\u9009\u6807\u51c6\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u7279\u5b9a\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u9002\u7528\u6027\uff08\u5f3a\u5236\u6027\u3001\u63a8\u8350\u6216\u4e0d\u9002\u7528\uff09\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u8bba\u8bc1\u3002", "result": "\u7cfb\u7edf\u5728\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523073%\uff0cTop-5\u68c0\u7d22\u53ec\u56de\u7387\u4e3a87%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u68c0\u7d22\u3001\u96f6\u6837\u672c\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6807\u51c6\u9002\u7528\u6027\u63a8\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684AI\u8f85\u52a9\u6cd5\u89c4\u79d1\u5b66\uff0c\u5e76\u5728\u4e2d\u7f8e\u6807\u51c6\u95f4\u5b9e\u73b0\u4e86\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\u63a8\u7406\u3002", "paper_title_zh": "\u6807\u51c6\u9002\u7528\u6027\u5224\u65ad\u4e0e\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\u63a8\u7406\uff1a\u57fa\u4e8eRAG\u7684\u533b\u7597\u5668\u68b0\u5408\u89c4\u6027\u6846\u67b6", "abstract_zh": "\u786e\u5b9a\u9002\u7528\u7684\u6cd5\u89c4\u6807\u51c6\u662f\u533b\u7597\u5668\u68b0\u5408\u89c4\u6027\u4e2d\u4e00\u4e2a\u5173\u952e\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u901a\u5e38\u9700\u8981\u4e13\u5bb6\u5bf9\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u5206\u6563\u4e14\u5f02\u6784\u6587\u6863\u8fdb\u884c\u89e3\u91ca\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316AI\u7cfb\u7edf\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6d41\u7a0b\u81ea\u52a8\u5316\u5224\u65ad\u6807\u51c6\u9002\u7528\u6027\u3002\u7ed9\u5b9a\u81ea\u7531\u6587\u672c\u8bbe\u5907\u63cf\u8ff0\uff0c\u7cfb\u7edf\u4ece\u7cbe\u9009\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u5019\u9009\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u7279\u5b9a\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u9002\u7528\u6027\uff08\u5206\u7c7b\u4e3a\u5f3a\u5236\u6027\u3001\u63a8\u8350\u6216\u4e0d\u9002\u7528\uff09\uff0c\u5e76\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u8bba\u8bc1\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u56fd\u9645\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u6807\u51c6\u6620\u5c04\uff0c\u5e76\u8bc4\u4f30\u4e86\u7cfb\u7edf\u5728\u4ec5\u68c0\u7d22\u3001\u96f6\u6837\u672c\u548c\u57fa\u4e8e\u89c4\u5219\u57fa\u7ebf\u4e0a\u7684\u8868\u73b0\u3002\u6240\u63d0\u65b9\u6cd5\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e3a73%\uff0cTop-5\u68c0\u7d22\u53ec\u56de\u7387\u4e3a87%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bc6\u522b\u76f8\u5173\u6cd5\u89c4\u6807\u51c6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6807\u51c6\u9002\u7528\u6027\u63a8\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684AI\u8f85\u52a9\u6cd5\u89c4\u79d1\u5b66\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u533a\u57df\u611f\u77e5RAG\u4ee3\u7406\u5b9e\u73b0\u4e86\u4e2d\u7f8e\u6807\u51c6\u95f4\u7684\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\u63a8\u7406\uff0c\u652f\u6301\u8de8\u6cd5\u89c4\u6846\u67b6\u7684\u51b2\u7a81\u89e3\u51b3\u548c\u9002\u7528\u6027\u8bba\u8bc1\u3002"}}
{"id": "2506.18082", "pdf": "https://arxiv.org/pdf/2506.18082", "abs": "https://arxiv.org/abs/2506.18082", "authors": ["Esteban Garces Arias", "Hannah Blocher", "Julian Rodemann", "Matthias A\u00dfenmacher", "Christoph Jansen"], "title": "Statistical Multicriteria Evaluation of LLM-Generated Text", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u968f\u673a\u4f18\u52bf\uff08GSD\uff09\u7684\u7edf\u8ba1\u591a\u51c6\u5219\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u751f\u6210\u6587\u672c\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u5355\u6307\u6807\u5c40\u9650\u6027\u3001\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u4e0d\u517c\u5bb9\u6027\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u8ba1\u63a8\u65ad\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLM\u751f\u6210\u6587\u672c\u8d28\u91cf\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6307\u6807\u6216\u7b80\u5355\u805a\u5408\uff0c\u65e0\u6cd5\u6355\u6349\u6587\u672c\u5728\u8fde\u8d2f\u6027\u3001\u591a\u6837\u6027\u3001\u6d41\u7545\u6027\u7b49\u591a\u7ef4\u5ea6\u7684\u6743\u8861\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e09\u5927\u5c40\u9650\u6027\uff1a\u5355\u6307\u6807\u8bc4\u4f30\u4e0d\u8db3\u3001\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u4e0d\u517c\u5bb9\u6027\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u8ba1\u63a8\u65ad\u4fdd\u8bc1\u3002", "method": "\u672c\u6587\u91c7\u7528\u5e7f\u4e49\u968f\u673a\u4f18\u52bf\uff08GSD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u6392\u5e8f\u89e3\u7801\u7b56\u7565\uff0c\u907f\u514d\u5bf9\u591a\u6307\u6807\u8fdb\u884c\u4efb\u610f\u52a0\u6743\uff0c\u5b9e\u73b0\u8de8\u591a\u7ef4\u5ea6\u540c\u65f6\u8bc4\u4f30\uff0c\u540c\u65f6\u8003\u8651\u4e0d\u540c\u6d4b\u91cf\u5c3a\u5ea6\u3002", "result": "\u901a\u8fc7\u5c06GSD\u6846\u67b6\u5e94\u7528\u4e8e\u5e38\u89c1\u89e3\u7801\u7b56\u7565\u4e0e\u4eba\u5de5\u751f\u6210\u6587\u672c\u7684\u5bf9\u6bd4\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u7edf\u8ba1\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u8003\u8651\u4e86\u91c7\u6837\u8bbe\u8ba1\u4e2d\u53ef\u80fd\u504f\u79bbi.i.d.\u5047\u8bbe\u7684\u60c5\u51b5\u3002", "conclusion": "GSD\u6846\u67b6\u4e3aLLM\u751f\u6210\u6587\u672c\u7684\u591a\u7ef4\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u8ba1\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "paper_title_zh": "LLM\u751f\u6210\u6587\u672c\u7684\u7edf\u8ba1\u591a\u51c6\u5219\u8bc4\u4f30", "abstract_zh": "\u8bc4\u4f30LLM\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u4ecd\u7136\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u5f53\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5b64\u7acb\u7684\u6307\u6807\u6216\u7b80\u5355\u7684\u805a\u5408\uff0c\u65e0\u6cd5\u6355\u6349\u6587\u672c\u5728\u8fde\u8d2f\u6027\u3001\u591a\u6837\u6027\u3001\u6d41\u7545\u6027\u7b49\u591a\u7ef4\u5ea6\u7684\u6743\u8861\u3002\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u968f\u673a\u4f18\u52bf\uff08GSD\uff09\u7684\u7edf\u8ba1\u63a8\u65ad\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u7684\u4e09\u5927\u5173\u952e\u5c40\u9650\u6027\uff1a\u5355\u6307\u6807\u8bc4\u4f30\u4e0d\u8db3\u3001\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u4e0d\u517c\u5bb9\u6027\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u8ba1\u63a8\u65ad\u4fdd\u8bc1\u3002GSD-front\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u7ef4\u5ea6\u540c\u65f6\u8bc4\u4f30\u6587\u672c\u8d28\u91cf\uff0c\u540c\u65f6\u5c0a\u91cd\u4e0d\u540c\u6d4b\u91cf\u5c3a\u5ea6\uff0c\u901a\u8fc7\u90e8\u5206\u6392\u5e8f\u89e3\u7801\u7b56\u7565\u907f\u514d\u5bf9\u6307\u6807\u7684\u4efb\u610f\u52a0\u6743\u3002\u901a\u8fc7\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u5e38\u89c1\u89e3\u7801\u7b56\u7565\u4e0e\u4eba\u5de5\u751f\u6210\u6587\u672c\u7684\u5bf9\u6bd4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5176\u5728\u8bc6\u522b\u7edf\u8ba1\u663e\u8457\u6027\u80fd\u5dee\u5f02\u65b9\u9762\u7684\u80fd\u529b\uff0c\u540c\u65f6\u8003\u8651\u4e86\u91c7\u6837\u8bbe\u8ba1\u4e2d\u53ef\u80fd\u504f\u79bbi.i.d.\u5047\u8bbe\u7684\u60c5\u51b5\u3002"}}
{"id": "2506.17733", "pdf": "https://arxiv.org/pdf/2506.17733", "abs": "https://arxiv.org/abs/2506.17733", "authors": ["Mengqi Lei", "Siqi Li", "Yihong Wu", "Han Hu", "You Zhou", "Xinhu Zheng", "Guiguang Ding", "Shaoyi Du", "Zongze Wu", "Yue Gao"], "title": "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception", "categories": ["cs.CV"], "comment": null, "summary": "The YOLO series models reign supreme in real-time object detection due to\ntheir superior accuracy and computational efficiency. However, both the\nconvolutional architectures of YOLO11 and earlier versions and the area-based\nself-attention mechanism introduced in YOLOv12 are limited to local information\naggregation and pairwise correlation modeling, lacking the capability to\ncapture global multi-to-multi high-order correlations, which limits detection\nperformance in complex scenarios. In this paper, we propose YOLOv13, an\naccurate and lightweight object detector. To address the above-mentioned\nchallenges, we propose a Hypergraph-based Adaptive Correlation Enhancement\n(HyperACE) mechanism that adaptively exploits latent high-order correlations\nand overcomes the limitation of previous methods that are restricted to\npairwise correlation modeling based on hypergraph computation, achieving\nefficient global cross-location and cross-scale feature fusion and enhancement.\nSubsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)\nparadigm based on HyperACE, which effectively achieves fine-grained information\nflow and representation synergy within the entire network by distributing\ncorrelation-enhanced features to the full pipeline. Finally, we propose to\nleverage depthwise separable convolutions to replace vanilla large-kernel\nconvolutions, and design a series of blocks that significantly reduce\nparameters and computational complexity without sacrificing performance. We\nconduct extensive experiments on the widely used MS COCO benchmark, and the\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance with fewer parameters and FLOPs. Specifically, our YOLOv13-N\nimproves mAP by 3.0\\% over YOLO11-N and by 1.5\\% over YOLOv12-N. The code and\nmodels of our YOLOv13 model are available at:\nhttps://github.com/iMoonLab/yolov13.", "AI": {"tldr": "YOLOv13\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u81ea\u9002\u5e94\u76f8\u5173\u6027\u589e\u5f3a\u673a\u5236\uff08HyperACE\uff09\u548c\u5168\u6d41\u7a0b\u805a\u5408\u4e0e\u5206\u914d\u8303\u5f0f\uff08FullPAD\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u3002", "motivation": "YOLO\u7cfb\u5217\u6a21\u578b\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5377\u79ef\u67b6\u6784\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ec5\u80fd\u5efa\u6a21\u5c40\u90e8\u4fe1\u606f\u548c\u6210\u5bf9\u76f8\u5173\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u9ad8\u9636\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faHyperACE\u673a\u5236\uff0c\u901a\u8fc7\u8d85\u56fe\u8ba1\u7b97\u81ea\u9002\u5e94\u6316\u6398\u9ad8\u9636\u76f8\u5173\u6027\uff1b\u8bbe\u8ba1FullPAD\u8303\u5f0f\uff0c\u5b9e\u73b0\u5168\u6d41\u7a0b\u7279\u5f81\u878d\u5408\u4e0e\u589e\u5f3a\uff1b\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u66ff\u6362\u4f20\u7edf\u5927\u6838\u5377\u79ef\uff0c\u964d\u4f4e\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728MS COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cYOLOv13-N\u7684mAP\u6bd4YOLO11-N\u63d0\u53473.0%\uff0c\u6bd4YOLOv12-N\u63d0\u53471.5%\uff0c\u4e14\u53c2\u6570\u548cFLOPs\u66f4\u5c11\u3002", "conclusion": "YOLOv13\u901a\u8fc7HyperACE\u548cFullPAD\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u7684\u5168\u5c40\u76f8\u5173\u6027\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u5316\u548c\u9ad8\u6548\u6027\u3002", "paper_title_zh": "YOLOv13\uff1a\u57fa\u4e8e\u8d85\u56fe\u589e\u5f3a\u81ea\u9002\u5e94\u89c6\u89c9\u611f\u77e5\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "abstract_zh": "YOLO\u7cfb\u5217\u6a21\u578b\u56e0\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u7136\u800c\uff0cYOLO11\u53ca\u4e4b\u524d\u7248\u672c\u7684\u5377\u79ef\u67b6\u6784\u4ee5\u53caYOLOv12\u5f15\u5165\u7684\u57fa\u4e8e\u533a\u57df\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ec5\u5c40\u9650\u4e8e\u5c40\u90e8\u4fe1\u606f\u805a\u5408\u548c\u6210\u5bf9\u76f8\u5173\u6027\u5efa\u6a21\uff0c\u7f3a\u4e4f\u6355\u6349\u5168\u5c40\u591a\u5bf9\u591a\u9ad8\u9636\u76f8\u5173\u6027\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u672c\u6587\u63d0\u51faYOLOv13\uff0c\u4e00\u79cd\u7cbe\u786e\u4e14\u8f7b\u91cf\u5316\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u3002\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u81ea\u9002\u5e94\u76f8\u5173\u6027\u589e\u5f3a\u673a\u5236\uff08HyperACE\uff09\uff0c\u901a\u8fc7\u8d85\u56fe\u8ba1\u7b97\u81ea\u9002\u5e94\u6316\u6398\u6f5c\u5728\u7684\u9ad8\u9636\u76f8\u5173\u6027\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4ec5\u80fd\u5efa\u6a21\u6210\u5bf9\u76f8\u5173\u6027\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u5c40\u8de8\u4f4d\u7f6e\u548c\u8de8\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4e0e\u589e\u5f3a\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperACE\u7684\u5168\u6d41\u7a0b\u805a\u5408\u4e0e\u5206\u914d\u8303\u5f0f\uff08FullPAD\uff09\uff0c\u901a\u8fc7\u5c06\u76f8\u5173\u6027\u589e\u5f3a\u7684\u7279\u5f81\u5206\u914d\u5230\u6574\u4e2a\u6d41\u7a0b\u4e2d\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u4fe1\u606f\u6d41\u548c\u7f51\u7edc\u5185\u7684\u8868\u793a\u534f\u540c\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u5229\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u66ff\u6362\u4f20\u7edf\u5927\u6838\u5377\u79ef\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u6a21\u5757\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684MS COCO\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53c2\u6570\u548cFLOPs\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684YOLOv13-N\u5728mAP\u4e0a\u6bd4YOLO11-N\u63d0\u5347\u4e863.0%\uff0c\u6bd4YOLOv12-N\u63d0\u5347\u4e861.5%\u3002YOLOv13\u7684\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728https://github.com/iMoonLab/yolov13\u83b7\u53d6\u3002"}}
{"id": "2506.18538", "pdf": "https://arxiv.org/pdf/2506.18538", "abs": "https://arxiv.org/abs/2506.18538", "authors": ["Rifat Ara Shams", "Didar Zowghi", "Muneera Bano"], "title": "A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence", "categories": ["cs.AI"], "comment": null, "summary": "Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is\ncrucial for mitigating biases and promoting equitable decision-making. However,\nexisting AI risk assessment frameworks often overlook inclusivity, lacking\nstandardized tools to measure an AI system's alignment with D&I principles.\nThis paper introduces a structured AI inclusivity question bank, a\ncomprehensive set of 253 questions designed to evaluate AI inclusivity across\nfive pillars: Humans, Data, Process, System, and Governance. The development of\nthe question bank involved an iterative, multi-source approach, incorporating\ninsights from literature reviews, D&I guidelines, Responsible AI frameworks,\nand a simulated user study. The simulated evaluation, conducted with 70\nAI-generated personas related to different AI jobs, assessed the question\nbank's relevance and effectiveness for AI inclusivity across diverse roles and\napplication domains. The findings highlight the importance of integrating D&I\nprinciples into AI development workflows and governance structures. The\nquestion bank provides an actionable tool for researchers, practitioners, and\npolicymakers to systematically assess and enhance the inclusivity of AI\nsystems, paving the way for more equitable and responsible AI technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5305\u542b253\u4e2a\u95ee\u9898\u7684AI\u5305\u5bb9\u6027\u8bc4\u4f30\u9898\u5e93\uff0c\u8986\u76d6\u4eba\u7c7b\u3001\u6570\u636e\u3001\u6d41\u7a0b\u3001\u7cfb\u7edf\u548c\u6cbb\u7406\u4e94\u5927\u652f\u67f1\uff0c\u65e8\u5728\u586b\u8865\u73b0\u6709AI\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u5728\u5305\u5bb9\u6027\u8861\u91cf\u4e0a\u7684\u7a7a\u767d\u3002\u901a\u8fc7\u591a\u6e90\u8fed\u4ee3\u5f00\u53d1\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u3001D&I\u6307\u5357\u548c\u6a21\u62df\u7528\u6237\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u9898\u5e93\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709AI\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u5bf9\u5305\u5bb9\u6027\u7684\u6807\u51c6\u5316\u8861\u91cf\u5de5\u5177\uff0c\u5bfc\u81f4AI\u7cfb\u7edf\u5728\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\uff08D&I\uff09\u65b9\u9762\u7684\u504f\u5dee\u96be\u4ee5\u8bc6\u522b\u548c\u7ea0\u6b63\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u5347AI\u6280\u672f\u7684\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u3002", "method": "\u91c7\u7528\u591a\u6e90\u8fed\u4ee3\u65b9\u6cd5\u5f00\u53d1\u9898\u5e93\uff0c\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u3001D&I\u6307\u5357\u3001\u8d1f\u8d23\u4efbAI\u6846\u67b6\u548c\u6a21\u62df\u7528\u6237\u7814\u7a76\u3002\u901a\u8fc770\u4e2aAI\u751f\u6210\u7684\u89d2\u8272\u6a21\u62df\u8bc4\u4f30\u9898\u5e93\u7684\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9898\u5e93\u80fd\u6709\u6548\u8bc4\u4f30AI\u7cfb\u7edf\u5728D&I\u539f\u5219\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5f3a\u8c03\u4e86\u5c06D&I\u539f\u5219\u6574\u5408\u5230AI\u5f00\u53d1\u548c\u6cbb\u7406\u6d41\u7a0b\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6a21\u62df\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u9898\u5e93\u5728\u4e0d\u540c\u89d2\u8272\u548c\u5e94\u7528\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u9898\u5e93\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u53ef\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u63d0\u5347AI\u7cfb\u7edf\u7684\u5305\u5bb9\u6027\uff0c\u63a8\u52a8\u66f4\u516c\u5e73\u548c\u8d1f\u8d23\u4efb\u7684AI\u6280\u672f\u53d1\u5c55\u3002", "paper_title_zh": "\u8bc4\u4f30AI\u5305\u5bb9\u6027\u7684\u95ee\u9898\u5e93\uff1a\u4ece\u591a\u6837\u6027\u9519\u8bef\u5230\u5305\u5bb9\u6027\u5353\u8d8a\u7684\u8def\u5f84\u6620\u5c04", "abstract_zh": "\u786e\u4fdd\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\uff08D&I\uff09\u5bf9\u4e8e\u51cf\u5c11\u504f\u89c1\u548c\u4fc3\u8fdb\u516c\u5e73\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684AI\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u5f80\u5f80\u5ffd\u89c6\u5305\u5bb9\u6027\uff0c\u7f3a\u4e4f\u8861\u91cfAI\u7cfb\u7edf\u4e0eD&I\u539f\u5219\u4e00\u81f4\u6027\u7684\u6807\u51c6\u5316\u5de5\u5177\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684AI\u5305\u5bb9\u6027\u95ee\u9898\u5e93\uff0c\u5305\u542b253\u4e2a\u95ee\u9898\uff0c\u65e8\u5728\u4ece\u4eba\u7c7b\u3001\u6570\u636e\u3001\u6d41\u7a0b\u3001\u7cfb\u7edf\u548c\u6cbb\u7406\u4e94\u5927\u652f\u67f1\u8bc4\u4f30AI\u5305\u5bb9\u6027\u3002\u95ee\u9898\u5e93\u7684\u5f00\u53d1\u91c7\u7528\u591a\u6e90\u8fed\u4ee3\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u3001D&I\u6307\u5357\u3001\u8d1f\u8d23\u4efbAI\u6846\u67b6\u548c\u6a21\u62df\u7528\u6237\u7814\u7a76\u3002\u901a\u8fc770\u4e2a\u4e0e\u4e0d\u540cAI\u5de5\u4f5c\u76f8\u5173\u7684AI\u751f\u6210\u89d2\u8272\u8fdb\u884c\u6a21\u62df\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u95ee\u9898\u5e93\u5728\u591a\u6837\u89d2\u8272\u548c\u5e94\u7528\u9886\u57df\u4e2d\u7684\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06D&I\u539f\u5219\u6574\u5408\u5230AI\u5f00\u53d1\u6d41\u7a0b\u548c\u6cbb\u7406\u7ed3\u6784\u4e2d\u7684\u91cd\u8981\u6027\u3002\u8be5\u95ee\u9898\u5e93\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u53ef\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u63d0\u5347AI\u7cfb\u7edf\u7684\u5305\u5bb9\u6027\uff0c\u63a8\u52a8\u66f4\u516c\u5e73\u548c\u8d1f\u8d23\u4efb\u7684AI\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2506.18091", "pdf": "https://arxiv.org/pdf/2506.18091", "abs": "https://arxiv.org/abs/2506.18091", "authors": ["Patrik Stano", "Ale\u0161 Hor\u00e1k"], "title": "Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Anaphora resolution plays a critical role in natural language understanding,\nespecially in morphologically rich languages like Czech. This paper presents a\ncomparative evaluation of two modern approaches to anaphora resolution on Czech\ntext: prompt engineering with large language models (LLMs) and fine-tuning\ncompact generative models. Using a dataset derived from the Prague Dependency\nTreebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2\nand Llama 3, using a series of prompt templates. We compare them against\nfine-tuned variants of the mT5 and Mistral models that we trained specifically\nfor Czech anaphora resolution. Our experiments demonstrate that while prompting\nyields promising few-shot results (up to 74.5% accuracy), the fine-tuned\nmodels, particularly mT5-large, outperform them significantly, achieving up to\n88% accuracy while requiring fewer computational resources. We analyze\nperformance across different anaphora types, antecedent distances, and source\ncorpora, highlighting key strengths and trade-offs of each approach.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u6a21\u578b\u5728\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\uff08\u5982mT5-large\uff09\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4e0a\u5747\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u3002", "motivation": "\u6307\u4ee3\u6d88\u89e3\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u5982\u6377\u514b\u8bed\u4e2d\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u4e24\u79cd\u73b0\u4ee3\u65b9\u6cd5\uff08\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u6a21\u578b\uff09\u5728\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u5b9a\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u6765\u81ea\u5e03\u62c9\u683c\u4f9d\u5b58\u6811\u5e93\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Mistral Large 2\u548cLlama 3\uff09\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4e13\u95e8\u9488\u5bf9\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u5fae\u8c03\u7684mT5\u548cMistral\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u5c1a\u53ef\uff08\u6700\u9ad874.5%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5fae\u8c03\u6a21\u578b\uff08\u5c24\u5176\u662fmT5-large\uff09\u663e\u8457\u4f18\u4e8e\u524d\u8005\uff0c\u6700\u9ad8\u8fbe\u523088%\u51c6\u786e\u7387\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u5fae\u8c03\u6a21\u578b\u5728\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u3002\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u867d\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "paper_title_zh": "\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u4e0e\u5fae\u8c03\u6a21\u578b\u7684\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u65b9\u6cd5\u5bf9\u6bd4\u8bc4\u4f30", "abstract_zh": "\u6307\u4ee3\u6d88\u89e3\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u6377\u514b\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\u3002\u672c\u6587\u5bf9\u6bd4\u4e86\u4e24\u79cd\u73b0\u4ee3\u65b9\u6cd5\u5728\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63d0\u793a\u5de5\u7a0b\u4e0e\u5fae\u8c03\u7d27\u51d1\u751f\u6210\u6a21\u578b\u3002\u7814\u7a76\u4f7f\u7528\u6765\u81ea\u5e03\u62c9\u683c\u4f9d\u5b58\u6811\u5e93\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6307\u4ee4\u8c03\u4f18\u7684LLM\uff08\u5982Mistral Large 2\u548cLlama 3\uff09\u7684\u63d0\u793a\u6a21\u677f\u6548\u679c\uff0c\u5e76\u5c06\u5176\u4e0e\u4e13\u95e8\u9488\u5bf9\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u5fae\u8c03\u7684mT5\u548cMistral\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u5c1a\u53ef\uff08\u6700\u9ad874.5%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5fae\u8c03\u6a21\u578b\uff08\u5c24\u5176\u662fmT5-large\uff09\u663e\u8457\u4f18\u4e8e\u524d\u8005\uff0c\u6700\u9ad8\u8fbe\u523088%\u51c6\u786e\u7387\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u6307\u4ee3\u7c7b\u578b\u3001\u5148\u884c\u8bcd\u8ddd\u79bb\u548c\u8bed\u6599\u6765\u6e90\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u5173\u952e\u4f18\u52bf\u548c\u6743\u8861\u3002"}}
{"id": "2506.17746", "pdf": "https://arxiv.org/pdf/2506.17746", "abs": "https://arxiv.org/abs/2506.17746", "authors": ["Sourabh Vasant Gothe", "Ayon Chattopadhyay", "Gunturi Venkata Sai Phani Kiran", "Pratik", "Vibhav Agarwal", "Jayesh Rajkumar Vachhani", "Sourav Ghosh", "Parameswaranath VM", "Barath Raj KR"], "title": "PhysID: Physics-based Interactive Dynamics from a Single-view Image", "categories": ["cs.CV"], "comment": "Published in 2025 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP). Project page: https://physid.github.io/", "summary": "Transforming static images into interactive experiences remains a challenging\ntask in computer vision. Tackling this challenge holds the potential to elevate\nmobile user experiences, notably through interactive and AR/VR applications.\nCurrent approaches aim to achieve this either using pre-recorded video\nresponses or requiring multi-view images as input. In this paper, we present\nPhysID, that streamlines the creation of physics-based interactive dynamics\nfrom a single-view image by leveraging large generative models for 3D mesh\ngeneration and physical property prediction. This significantly reduces the\nexpertise required for engineering-intensive tasks like 3D modeling and\nintrinsic property calibration, enabling the process to be scaled with minimal\nmanual intervention. We integrate an on-device physics-based engine for\nphysically plausible real-time rendering with user interactions. PhysID\nrepresents a leap forward in mobile-based interactive dynamics, offering\nreal-time, non-deterministic interactions and user-personalization with\nefficient on-device memory consumption. Experiments evaluate the zero-shot\ncapabilities of various Multimodal Large Language Models (MLLMs) on diverse\ntasks and the performance of 3D reconstruction models. These results\ndemonstrate the cohesive functioning of all modules within the end-to-end\nframework, contributing to its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPhysID\uff0c\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u4ea4\u4e92\u52a8\u6001\u751f\u6210\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u89c6\u89d2\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u4f53\u9a8c\uff0c\u663e\u8457\u964d\u4f4e3D\u5efa\u6a21\u548c\u7269\u7406\u5c5e\u6027\u6821\u51c6\u7684\u590d\u6742\u6027\u3002", "motivation": "\u5f53\u524d\u5c06\u9759\u6001\u56fe\u50cf\u8f6c\u5316\u4e3a\u4ea4\u4e92\u4f53\u9a8c\u7684\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u89c6\u89d2\u56fe\u50cf\u6216\u9884\u5f55\u89c6\u9891\uff0c\u9650\u5236\u4e86\u79fb\u52a8\u7aef\u5e94\u7528\u7684\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5355\u89c6\u89d2\u56fe\u50cf\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\uff0c\u63d0\u5347\u79fb\u52a8\u7528\u6237\u4f53\u9a8c\u3002", "method": "PhysID\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u751f\u62103D\u7f51\u683c\u5e76\u9884\u6d4b\u7269\u7406\u5c5e\u6027\uff0c\u7ed3\u5408\u8bbe\u5907\u7aef\u7269\u7406\u5f15\u64ce\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u548c\u7528\u6237\u4ea4\u4e92\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u548c3D\u91cd\u5efa\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PhysID\u5728\u79fb\u52a8\u7aef\u4ea4\u4e92\u52a8\u6001\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u652f\u6301\u5b9e\u65f6\u3001\u975e\u786e\u5b9a\u6027\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u4f53\u9a8c\uff0c\u540c\u65f6\u4f18\u5316\u8bbe\u5907\u5185\u5b58\u6d88\u8017\u3002", "paper_title_zh": "PhysID\uff1a\u57fa\u4e8e\u5355\u89c6\u89d2\u56fe\u50cf\u7684\u7269\u7406\u4ea4\u4e92\u52a8\u6001\u751f\u6210", "abstract_zh": "\u5c06\u9759\u6001\u56fe\u50cf\u8f6c\u5316\u4e3a\u4ea4\u4e92\u4f53\u9a8c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u9879\u6311\u6218\u6027\u4efb\u52a1\u3002\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u6709\u671b\u901a\u8fc7\u4ea4\u4e92\u5f0f\u548c\u589e\u5f3a\u73b0\u5b9e/\u865a\u62df\u73b0\u5b9e\uff08AR/VR\uff09\u5e94\u7528\u63d0\u5347\u79fb\u52a8\u7528\u6237\u4f53\u9a8c\u3002\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u5f55\u89c6\u9891\u6216\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u3002\u672c\u6587\u63d0\u51faPhysID\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u751f\u62103D\u7f51\u683c\u5e76\u9884\u6d4b\u7269\u7406\u5c5e\u6027\uff0c\u4ece\u5355\u89c6\u89d2\u56fe\u50cf\u7b80\u5316\u57fa\u4e8e\u7269\u7406\u7684\u4ea4\u4e92\u52a8\u6001\u751f\u6210\u8fc7\u7a0b\u3002\u8fd9\u663e\u8457\u964d\u4f4e\u4e863D\u5efa\u6a21\u548c\u56fa\u6709\u5c5e\u6027\u6821\u51c6\u7b49\u5de5\u7a0b\u5bc6\u96c6\u578b\u4efb\u52a1\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f7f\u6d41\u7a0b\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u5e72\u9884\u8fdb\u884c\u6269\u5c55\u3002\u6211\u4eec\u96c6\u6210\u4e86\u8bbe\u5907\u7aef\u7269\u7406\u5f15\u64ce\uff0c\u5b9e\u73b0\u7269\u7406\u4e0a\u5408\u7406\u7684\u5b9e\u65f6\u6e32\u67d3\u4e0e\u7528\u6237\u4ea4\u4e92\u3002PhysID\u5728\u79fb\u52a8\u7aef\u4ea4\u4e92\u52a8\u6001\u9886\u57df\u5b9e\u73b0\u4e86\u98de\u8dc3\uff0c\u63d0\u4f9b\u5b9e\u65f6\u3001\u975e\u786e\u5b9a\u6027\u4ea4\u4e92\u548c\u7528\u6237\u4e2a\u6027\u5316\uff0c\u540c\u65f6\u4f18\u5316\u8bbe\u5907\u5185\u5b58\u6d88\u8017\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\u4ee5\u53ca3D\u91cd\u5efa\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7aef\u5230\u7aef\u6846\u67b6\u4e2d\u6240\u6709\u6a21\u5757\u7684\u534f\u540c\u5de5\u4f5c\u4e3a\u5176\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.18559", "pdf": "https://arxiv.org/pdf/2506.18559", "abs": "https://arxiv.org/abs/2506.18559", "authors": ["Hong Qing Yu"], "title": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent", "categories": ["cs.AI", "cs.LO", "I.2.7; F.4.1"], "comment": null, "summary": "Large language models excel at generating fluent text but frequently struggle\nwith structured reasoning involving temporal constraints, causal relationships,\nand probabilistic reasoning. To address these limitations, we propose Temporal\nCausal Probabilistic Description Logic (T-CPDL), an integrated framework that\nextends traditional Description Logic with temporal interval operators,\nexplicit causal relationships, and probabilistic annotations. We present two\ndistinct variants of T-CPDL: one capturing qualitative temporal relationships\nthrough Allen's interval algebra, and another variant enriched with explicit\ntimestamped causal assertions. Both variants share a unified logical structure,\nenabling complex reasoning tasks ranging from simple temporal ordering to\nnuanced probabilistic causation. Empirical evaluations on temporal reasoning\nand causal inference benchmarks confirm that T-CPDL substantially improves\ninference accuracy, interpretability, and confidence calibration of language\nmodel outputs. By delivering transparent reasoning paths and fine-grained\ntemporal and causal semantics, T-CPDL significantly enhances the capability of\nlanguage models to support robust, explainable, and trustworthy\ndecision-making. This work also lays the groundwork for developing advanced\nLogic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially\nboosting the reasoning capabilities and efficiency of knowledge graph-enhanced\nRAG systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-CPDL\u7684\u65f6\u5e8f\u56e0\u679c\u6982\u7387\u63cf\u8ff0\u903b\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u7ea6\u675f\u3001\u56e0\u679c\u5173\u7cfb\u548c\u6982\u7387\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6269\u5c55\u4f20\u7edf\u63cf\u8ff0\u903b\u8f91\uff0c\u7ed3\u5408\u65f6\u5e8f\u533a\u95f4\u7b97\u5b50\u548c\u56e0\u679c\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6d41\u7545\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6d89\u53ca\u65f6\u5e8f\u7ea6\u675f\u3001\u56e0\u679c\u5173\u7cfb\u548c\u6982\u7387\u63a8\u7406\u7684\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86T-CPDL\u6846\u67b6\uff0c\u65e8\u5728\u5f25\u8865\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "T-CPDL\u6269\u5c55\u4e86\u4f20\u7edf\u63cf\u8ff0\u903b\u8f91\uff0c\u5f15\u5165\u4e86\u65f6\u5e8f\u533a\u95f4\u7b97\u5b50\u548c\u663e\u5f0f\u56e0\u679c\u6807\u6ce8\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u53d8\u4f53\uff1a\u4e00\u79cd\u57fa\u4e8eAllen\u533a\u95f4\u4ee3\u6570\u6355\u6349\u5b9a\u6027\u65f6\u5e8f\u5173\u7cfb\uff0c\u53e6\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u6233\u56e0\u679c\u65ad\u8a00\u3002\u4e24\u79cd\u53d8\u4f53\u5171\u4eab\u7edf\u4e00\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u652f\u6301\u4ece\u7b80\u5355\u65f6\u5e8f\u6392\u5e8f\u5230\u590d\u6742\u6982\u7387\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-CPDL\u5728\u65f6\u5e8f\u63a8\u7406\u548c\u56e0\u679c\u63a8\u65ad\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002\u540c\u65f6\uff0c\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u903b\u8f91\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Logic-RAG\uff09\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "T-CPDL\u901a\u8fc7\u63d0\u4f9b\u900f\u660e\u7684\u63a8\u7406\u8def\u5f84\u548c\u7ec6\u7c92\u5ea6\u7684\u65f6\u5e8f\u56e0\u679c\u8bed\u4e49\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "paper_title_zh": "T-CPDL\uff1a\u4e00\u79cd\u65f6\u5e8f\u56e0\u679c\u6982\u7387\u63cf\u8ff0\u903b\u8f91\u6846\u67b6\u7528\u4e8e\u5f00\u53d1\u903b\u8f91-RAG\u667a\u80fd\u4f53", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u751f\u6210\u6d41\u7545\u6587\u672c\uff0c\u4f46\u5728\u6d89\u53ca\u65f6\u5e8f\u7ea6\u675f\u3001\u56e0\u679c\u5173\u7cfb\u548c\u6982\u7387\u63a8\u7406\u7684\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u5e8f\u56e0\u679c\u6982\u7387\u63cf\u8ff0\u903b\u8f91\uff08T-CPDL\uff09\uff0c\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u4f20\u7edf\u63cf\u8ff0\u903b\u8f91\uff0c\u5f15\u5165\u4e86\u65f6\u5e8f\u533a\u95f4\u7b97\u5b50\u3001\u663e\u5f0f\u56e0\u679c\u5173\u7cfb\u548c\u6982\u7387\u6807\u6ce8\u3002\u6211\u4eec\u63d0\u51fa\u4e86T-CPDL\u7684\u4e24\u79cd\u53d8\u4f53\uff1a\u4e00\u79cd\u901a\u8fc7Allen\u533a\u95f4\u4ee3\u6570\u6355\u6349\u5b9a\u6027\u65f6\u5e8f\u5173\u7cfb\uff0c\u53e6\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u6233\u56e0\u679c\u65ad\u8a00\u3002\u4e24\u79cd\u53d8\u4f53\u5171\u4eab\u7edf\u4e00\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u652f\u6301\u4ece\u7b80\u5355\u65f6\u5e8f\u6392\u5e8f\u5230\u590d\u6742\u6982\u7387\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cT-CPDL\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u63a8\u7406\u548c\u56e0\u679c\u63a8\u65ad\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002\u901a\u8fc7\u63d0\u4f9b\u900f\u660e\u7684\u63a8\u7406\u8def\u5f84\u548c\u7ec6\u7c92\u5ea6\u7684\u65f6\u5e8f\u56e0\u679c\u8bed\u4e49\uff0cT-CPDL\u663e\u8457\u589e\u5f3a\u4e86\u8bed\u8a00\u6a21\u578b\u652f\u6301\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u51b3\u7b56\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u8fd8\u4e3a\u5f00\u53d1\u5148\u8fdb\u7684\u903b\u8f91\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Logic-RAG\uff09\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684RAG\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2506.18102", "pdf": "https://arxiv.org/pdf/2506.18102", "abs": "https://arxiv.org/abs/2506.18102", "authors": ["Fuyu Wang", "Jiangtong Li", "Kun Zhu", "Changjun Jiang"], "title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating", "categories": ["cs.CL"], "comment": "20 pages; Accepted to ACL 2025 Main", "summary": "With the rapid advancements in large language models (LLMs), debating tasks,\nsuch as argument quality assessment and debate process simulation, have made\nsignificant progress. However, existing LLM-based debating systems focus on\nresponding to specific arguments while neglecting objective assessments such as\nauthenticity and logical validity. Furthermore, these systems lack a structured\napproach to optimize across various dimensions$-$including evaluation metrics,\nchain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby\nlimiting their effectiveness. To address these interconnected challenges, we\npropose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel\nevaluation system that establishes a multi-dimensional assessment architecture\nincorporating four subjective criteria (emotional appeal, argument clarity,\nargument arrangement, and topic relevance) alongside two objective metrics\n(fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an\noptimized debating framework employing a phased optimization approach through\nCoT reasoning enhancement, multi-dimensional Direct Preference Optimization\n(DPO), and real-time knowledge grounding via web-based Retrieval Augmented\nGeneration (Web-RAG). Empirical evaluations demonstrate that\n$\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert\njudgments compared to existing methods, while $\\textbf{InspireDebate}$ shows\nsignificant improvements, outperforming baseline models by 57$\\%$. Source code\nis available at https://github.com/fywang12/InspireDebate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ec4\u4ef6\u6846\u67b6InspireDebate\uff0c\u901a\u8fc7\u591a\u7ef4\u8bc4\u4f30\u548c\u4f18\u5316\u65b9\u6cd5\u63d0\u5347\u8fa9\u8bba\u7cfb\u7edf\u7684\u6027\u80fd\u3002InspireScore\u8bc4\u4f30\u7cfb\u7edf\u7ed3\u5408\u4e3b\u5ba2\u89c2\u6307\u6807\uff0c\u800cInspireDebate\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u8fa9\u8bba\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fa9\u8bba\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u8bba\u70b9\u7684\u56de\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u4e8b\u5b9e\u771f\u5b9e\u6027\u548c\u903b\u8f91\u6709\u6548\u6027\u7b49\u5ba2\u89c2\u8bc4\u4f30\uff0c\u4e14\u7f3a\u4e4f\u591a\u7ef4\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u7ec4\u4ef6\u6846\u67b6\uff1a(1) InspireScore\uff0c\u7ed3\u5408\u56db\u9879\u4e3b\u89c2\u6807\u51c6\uff08\u60c5\u611f\u5438\u5f15\u529b\u3001\u8bba\u70b9\u6e05\u6670\u5ea6\u3001\u8bba\u70b9\u5b89\u6392\u3001\u4e3b\u9898\u76f8\u5173\u6027\uff09\u548c\u4e24\u9879\u5ba2\u89c2\u6307\u6807\uff08\u4e8b\u5b9e\u771f\u5b9e\u6027\u548c\u903b\u8f91\u6709\u6548\u6027\uff09\u7684\u591a\u7ef4\u8bc4\u4f30\u7cfb\u7edf\uff1b(2) InspireDebate\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\uff08\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u589e\u5f3a\u3001\u591a\u7ef4\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3001\u57fa\u4e8e\u7f51\u7edc\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u63d0\u5347\u8fa9\u8bba\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInspireScore\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u76f8\u5173\u6027\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad844%\uff0c\u800cInspireDebate\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b57%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u7ef4\u8bc4\u4f30\u548c\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8fa9\u8bba\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "InspireDebate\uff1a\u57fa\u4e8e\u591a\u7ef4\u4e3b\u5ba2\u89c2\u8bc4\u4f30\u5f15\u5bfc\u7684\u8fa9\u8bba\u63a8\u7406\u4e0e\u4f18\u5316", "abstract_zh": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8fa9\u8bba\u4efb\u52a1\uff08\u5982\u8bba\u70b9\u8d28\u91cf\u8bc4\u4f30\u548c\u8fa9\u8bba\u8fc7\u7a0b\u6a21\u62df\uff09\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u8fa9\u8bba\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5bf9\u7279\u5b9a\u8bba\u70b9\u7684\u56de\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u4e8b\u5b9e\u771f\u5b9e\u6027\u548c\u903b\u8f91\u6709\u6548\u6027\u7b49\u5ba2\u89c2\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u7f3a\u4e4f\u5728\u591a\u7ef4\u5ea6\uff08\u5305\u62ec\u8bc4\u4f30\u6307\u6807\u3001\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u591a\u8f6e\u8fa9\u8bba\u4f18\u5316\uff09\u4e0a\u7684\u7ed3\u6784\u5316\u4f18\u5316\u65b9\u6cd5\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u76f8\u4e92\u5173\u8054\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ec4\u4ef6\u6846\u67b6\uff1a(1) InspireScore\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u56db\u9879\u4e3b\u89c2\u6807\u51c6\uff08\u60c5\u611f\u5438\u5f15\u529b\u3001\u8bba\u70b9\u6e05\u6670\u5ea6\u3001\u8bba\u70b9\u5b89\u6392\u548c\u4e3b\u9898\u76f8\u5173\u6027\uff09\u548c\u4e24\u9879\u5ba2\u89c2\u6307\u6807\uff08\u4e8b\u5b9e\u771f\u5b9e\u6027\u548c\u903b\u8f91\u6709\u6548\u6027\uff09\u7684\u591a\u7ef4\u8bc4\u4f30\u67b6\u6784\uff1b(2) InspireDebate\uff0c\u4e00\u79cd\u4f18\u5316\u7684\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u589e\u5f3a\u3001\u591a\u7ef4\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u57fa\u4e8e\u7f51\u7edc\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Web-RAG\uff09\u5b9e\u73b0\u5206\u9636\u6bb5\u4f18\u5316\u3002\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cInspireScore\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u76f8\u5173\u6027\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad844%\uff0c\u800cInspireDebate\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b57%\u3002\u6e90\u4ee3\u7801\u53ef\u5728https://github.com/fywang12/InspireDebate\u83b7\u53d6\u3002"}}
{"id": "2506.17759", "pdf": "https://arxiv.org/pdf/2506.17759", "abs": "https://arxiv.org/abs/2506.17759", "authors": ["Fadi Abdeladhim Zidi", "Djamel Eddine Boukhari", "Abdellah Zakaria Sellam", "Abdelkrim Ouafi", "Cosimo Distante", "Salah Eddine Bekhouche", "Abdelmalik Taleb-Ahmed"], "title": "LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image classification remains a challenging task due to the high\ndimensionality of spectral data, significant inter-band redundancy, and the\nlimited availability of annotated samples. While recent transformer-based\nmodels have improved the global modeling of spectral-spatial dependencies,\ntheir scalability and adaptability under label-scarce conditions remain\nlimited. In this work, we propose \\textbf{LoLA-SpecViT}(Low-rank adaptation\nLocal Attention Spectral Vision Transformer), a lightweight spectral vision\ntransformer that addresses these limitations through a parameter-efficient\narchitecture tailored to the unique characteristics of hyperspectral imagery.\nOur model combines a 3D convolutional spectral front-end with local\nwindow-based self-attention, enhancing both spectral feature extraction and\nspatial consistency while reducing computational complexity. To further improve\nadaptability, we integrate low-rank adaptation (LoRA) into attention and\nprojection layers, enabling fine-tuning with over 80\\% fewer trainable\nparameters. A novel cyclical learning rate scheduler modulates LoRA adaptation\nstrength during training, improving convergence and generalisation. Extensive\nexperiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and\nSalinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art\nbaselines, achieving up to 99.91\\% accuracy with substantially fewer parameters\nand enhanced robustness under low-label regimes. The proposed framework\nprovides a scalable and generalizable solution for real-world HSI applications\nin agriculture, environmental monitoring, and remote sensing analytics. Our\ncode is available in the following\n\\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5149\u8c31\u89c6\u89c9\u53d8\u6362\u5668LoLA-SpecViT\uff0c\u901a\u8fc7\u5c40\u90e8\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u548c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u91cf\uff0c\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u9ad8\u7ef4\u6570\u636e\u3001\u9891\u5e26\u5197\u4f59\u548c\u6807\u6ce8\u6837\u672c\u4e0d\u8db3\u7684\u6311\u6218\u3002\u73b0\u6709\u53d8\u6362\u5668\u6a21\u578b\u5728\u5168\u5c40\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "LoLA-SpecViT\u7ed3\u5408\u4e863D\u5377\u79ef\u5149\u8c31\u524d\u7aef\u548c\u5c40\u90e8\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\uff0c\u63d0\u5347\u5149\u8c31\u7279\u5f81\u63d0\u53d6\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5f15\u5165LoRA\u6280\u672f\uff0c\u51cf\u5c11\u4e8680%\u4ee5\u4e0a\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u91c7\u7528\u5faa\u73af\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728WHU-Hi LongKou\u3001WHU-Hi HongHu\u548cSalinas\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLoLA-SpecViT\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe99.91%\uff0c\u4e14\u53c2\u6570\u91cf\u66f4\u5c11\uff0c\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "conclusion": "LoLA-SpecViT\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u3001\u73af\u5883\u76d1\u6d4b\u548c\u9065\u611f\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "paper_title_zh": "LoLA-SpecViT\uff1a\u57fa\u4e8eLoRA\u7684\u5c40\u90e8\u6ce8\u610f\u529bSwiGLU\u89c6\u89c9\u53d8\u6362\u5668\u7528\u4e8e\u9ad8\u5149\u8c31\u6210\u50cf", "abstract_zh": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7531\u4e8e\u6570\u636e\u7ef4\u5ea6\u9ad8\u3001\u9891\u5e26\u5197\u4f59\u4e25\u91cd\u4ee5\u53ca\u6807\u6ce8\u6837\u672c\u6709\u9650\uff0c\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u6700\u65b0\u6a21\u578b\u5728\u5168\u5c40\u5efa\u6a21\u5149\u8c31-\u7a7a\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5176\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4ecd\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5149\u8c31\u89c6\u89c9\u53d8\u6362\u5668LoLA-SpecViT\uff08\u4f4e\u79e9\u9002\u5e94\u5c40\u90e8\u6ce8\u610f\u529b\u5149\u8c31\u89c6\u89c9\u53d8\u6362\u5668\uff09\uff0c\u901a\u8fc7\u9488\u5bf9\u9ad8\u5149\u8c31\u56fe\u50cf\u72ec\u7279\u7279\u6027\u7684\u53c2\u6570\u9ad8\u6548\u67b6\u6784\u89e3\u51b3\u4e86\u8fd9\u4e9b\u9650\u5236\u3002\u6211\u4eec\u7684\u6a21\u578b\u7ed3\u5408\u4e863D\u5377\u79ef\u5149\u8c31\u524d\u7aef\u548c\u57fa\u4e8e\u5c40\u90e8\u7a97\u53e3\u7684\u81ea\u6ce8\u610f\u529b\uff0c\u65e2\u589e\u5f3a\u4e86\u5149\u8c31\u7279\u5f81\u63d0\u53d6\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u53c8\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u4e3a\u8fdb\u4e00\u6b65\u63d0\u5347\u9002\u5e94\u6027\uff0c\u6211\u4eec\u5728\u6ce8\u610f\u529b\u548c\u6295\u5f71\u5c42\u4e2d\u96c6\u6210\u4e86\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u4f7f\u5f97\u5fae\u8c03\u65f6\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1180%\u4ee5\u4e0a\u3002\u4e00\u79cd\u65b0\u9896\u7684\u5faa\u73af\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c03\u8282LoRA\u7684\u9002\u5e94\u5f3a\u5ea6\uff0c\u6539\u5584\u4e86\u6536\u655b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5728WHU-Hi LongKou\u3001WHU-Hi HongHu\u548cSalinas\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLoLA-SpecViT\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe\u523099.91%\uff0c\u4e14\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\uff0c\u5728\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u66f4\u5f3a\u3002\u8be5\u6846\u67b6\u4e3a\u519c\u4e1a\u3001\u73af\u5883\u76d1\u6d4b\u548c\u9065\u611f\u5206\u6790\u7b49\u5b9e\u9645\u9ad8\u5149\u8c31\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0bGitHub\u4ed3\u5e93\u83b7\u53d6\uff1ahttps://github.com/FadiZidiDz/LoLA-SpecViT\u3002"}}
{"id": "2506.18586", "pdf": "https://arxiv.org/pdf/2506.18586", "abs": "https://arxiv.org/abs/2506.18586", "authors": ["Zijie Yang", "Qiji Zhou", "Fang Guo", "Sijie Zhang", "Yexun Xi", "Jinglei Nie", "Yudian Zhu", "Liping Huang", "Chou Wu", "Yonghe Xia", "Xiaoyu Ma", "Yingming Pu", "Panzhong Lu", "Junshu Pan", "Mingtao Chen", "Tiannan Guo", "Yanmei Dou", "Hongyu Chen", "Anping Zeng", "Jiaxing Huang", "Tian Xu", "Yue Zhang"], "title": "Airalogy: AI-empowered universal data digitization for research automation", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "146 pages, 6 figures, 49 supplementary figures", "summary": "Research data are the foundation of Artificial Intelligence (AI)-driven\nscience, yet current AI applications remain limited to a few fields with\nreadily available, well-structured, digitized datasets. Achieving comprehensive\nAI empowerment across multiple disciplines is still out of reach. Present-day\nresearch data collection is often fragmented, lacking unified standards,\ninefficiently managed, and difficult to share. Creating a single platform for\nstandardized data digitization needs to overcome the inherent challenge of\nbalancing between universality (supporting the diverse, ever-evolving needs of\nvarious disciplines) and standardization (enforcing consistent formats to fully\nenable AI). No existing platform accommodates both facets. Building a truly\nmultidisciplinary platform requires integrating scientific domain knowledge\nwith sophisticated computing skills. Researchers often lack the computational\nexpertise to design customized and standardized data recording methods, whereas\nplatform developers rarely grasp the intricate needs of multiple scientific\ndomains. These gaps impede research data standardization and hamper AI-driven\nprogress. In this study, we address these challenges by developing Airalogy\n(https://airalogy.com), the world's first AI- and community-driven platform\nthat balances universality and standardization for digitizing research data\nacross multiple disciplines. Airalogy represents entire research workflows\nusing customizable, standardized data records and offers an advanced AI\nresearch copilot for intelligent Q&A, automated data entry, analysis, and\nresearch automation. Already deployed in laboratories across all four schools\nof Westlake University, Airalogy has the potential to accelerate and automate\nscientific innovation in universities, industry, and the global research\ncommunity-ultimately benefiting humanity as a whole.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Airalogy\uff0c\u9996\u4e2aAI\u9a71\u52a8\u7684\u591a\u5b66\u79d1\u7814\u7a76\u6570\u636e\u6570\u5b57\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u7814\u7a76\u6570\u636e\u788e\u7247\u5316\u548c\u6807\u51c6\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u8861\u901a\u7528\u6027\u4e0e\u6807\u51c6\u5316\uff0c\u63a8\u52a8AI\u9a71\u52a8\u7684\u79d1\u7814\u81ea\u52a8\u5316\u3002", "motivation": "\u5f53\u524dAI\u5e94\u7528\u53d7\u9650\u4e8e\u5c11\u6570\u9886\u57df\u7684\u6570\u636e\u53ef\u7528\u6027\uff0c\u7814\u7a76\u6570\u636e\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u548c\u9ad8\u6548\u7ba1\u7406\uff0c\u963b\u788d\u4e86\u591a\u5b66\u79d1AI\u8d4b\u80fd\u3002Airalogy\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u8ba1\u7b97\u6280\u672f\uff0c\u5b9e\u73b0\u8de8\u5b66\u79d1\u6570\u636e\u6807\u51c6\u5316\u548cAI\u9a71\u52a8\u7814\u7a76\u3002", "method": "\u5f00\u53d1Airalogy\u5e73\u53f0\uff0c\u7ed3\u5408AI\u6280\u672f\u548c\u793e\u533a\u9a71\u52a8\uff0c\u63d0\u4f9b\u53ef\u5b9a\u5236\u4e14\u6807\u51c6\u5316\u7684\u6570\u636e\u8bb0\u5f55\u65b9\u6cd5\uff0c\u652f\u6301\u667a\u80fd\u95ee\u7b54\u3001\u81ea\u52a8\u5316\u6570\u636e\u5f55\u5165\u548c\u5206\u6790\u7b49\u529f\u80fd\u3002", "result": "Airalogy\u5df2\u5728\u897f\u6e56\u5927\u5b66\u56db\u4e2a\u5b66\u9662\u7684\u5b9e\u9a8c\u5ba4\u4e2d\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a0\u901f\u79d1\u7814\u521b\u65b0\u548c\u81ea\u52a8\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "Airalogy\u901a\u8fc7\u5e73\u8861\u901a\u7528\u6027\u4e0e\u6807\u51c6\u5316\uff0c\u4e3a\u591a\u5b66\u79d1\u7814\u7a76\u6570\u636e\u6570\u5b57\u5316\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u5168\u7403\u79d1\u7814\u793e\u533a\u7684AI\u9a71\u52a8\u8fdb\u6b65\u3002", "paper_title_zh": "Airalogy\uff1aAI\u8d4b\u80fd\u7684\u901a\u7528\u6570\u636e\u6570\u5b57\u5316\u5e73\u53f0\u52a9\u529b\u79d1\u7814\u81ea\u52a8\u5316", "abstract_zh": "\u7814\u7a76\u6570\u636e\u662f\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u9a71\u52a8\u79d1\u5b66\u7684\u57fa\u7840\uff0c\u4f46\u5f53\u524dAI\u5e94\u7528\u4ec5\u9650\u4e8e\u5c11\u6570\u62e5\u6709\u7ed3\u6784\u5316\u3001\u6570\u5b57\u5316\u6570\u636e\u7684\u9886\u57df\u3002\u5b9e\u73b0\u591a\u5b66\u79d1\u7684\u5168\u9762AI\u8d4b\u80fd\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u4eca\u7814\u7a76\u6570\u636e\u6536\u96c6\u5206\u6563\u3001\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u3001\u7ba1\u7406\u4f4e\u6548\u4e14\u96be\u4ee5\u5171\u4eab\u3002\u6784\u5efa\u4e00\u4e2a\u6807\u51c6\u5316\u6570\u636e\u6570\u5b57\u5316\u5e73\u53f0\u9700\u5e73\u8861\u901a\u7528\u6027\uff08\u652f\u6301\u591a\u5b66\u79d1\u9700\u6c42\uff09\u4e0e\u6807\u51c6\u5316\uff08\u7edf\u4e00\u683c\u5f0f\u4ee5\u652f\u6301AI\uff09\u3002\u73b0\u6709\u5e73\u53f0\u65e0\u6cd5\u517c\u987e\u4e24\u8005\u3002\u771f\u6b63\u7684\u591a\u5b66\u79d1\u5e73\u53f0\u9700\u6574\u5408\u79d1\u5b66\u9886\u57df\u77e5\u8bc6\u4e0e\u8ba1\u7b97\u6280\u672f\u3002\u7814\u7a76\u8005\u5e38\u7f3a\u4e4f\u8bbe\u8ba1\u5b9a\u5236\u5316\u6807\u51c6\u5316\u6570\u636e\u8bb0\u5f55\u65b9\u6cd5\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u800c\u5e73\u53f0\u5f00\u53d1\u8005\u5219\u96be\u4ee5\u7406\u89e3\u591a\u5b66\u79d1\u9700\u6c42\u3002\u8fd9\u4e9b\u969c\u788d\u963b\u788d\u4e86\u7814\u7a76\u6570\u636e\u6807\u51c6\u5316\u548cAI\u9a71\u52a8\u7684\u8fdb\u6b65\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5f00\u53d1Airalogy\uff08https://airalogy.com\uff09\uff0c\u9996\u4e2aAI\u4e0e\u793e\u533a\u9a71\u52a8\u7684\u5e73\u53f0\uff0c\u5e73\u8861\u901a\u7528\u6027\u4e0e\u6807\u51c6\u5316\uff0c\u5b9e\u73b0\u591a\u5b66\u79d1\u7814\u7a76\u6570\u636e\u6570\u5b57\u5316\u3002Airalogy\u901a\u8fc7\u53ef\u5b9a\u5236\u6807\u51c6\u5316\u6570\u636e\u8bb0\u5f55\u652f\u6301\u5b8c\u6574\u7814\u7a76\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u667a\u80fd\u95ee\u7b54\u3001\u81ea\u52a8\u5316\u6570\u636e\u5f55\u5165\u4e0e\u5206\u6790\u7b49\u529f\u80fd\u3002\u8be5\u5e73\u53f0\u5df2\u5728\u897f\u6e56\u5927\u5b66\u56db\u4e2a\u5b66\u9662\u7684\u5b9e\u9a8c\u5ba4\u4e2d\u90e8\u7f72\uff0c\u6709\u671b\u52a0\u901f\u9ad8\u6821\u3001\u4ea7\u4e1a\u53ca\u5168\u7403\u79d1\u7814\u793e\u533a\u7684\u521b\u65b0\u81ea\u52a8\u5316\uff0c\u6700\u7ec8\u9020\u798f\u5168\u4eba\u7c7b\u3002"}}
{"id": "2506.18105", "pdf": "https://arxiv.org/pdf/2506.18105", "abs": "https://arxiv.org/abs/2506.18105", "authors": ["Yicheng Fu", "Zhemin Huang", "Liuxin Yang", "Yumeng Lu", "Zhongdongming Dai"], "title": "Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use", "categories": ["cs.CL"], "comment": null, "summary": "Chinese idioms (Chengyu) are concise four-character expressions steeped in\nhistory and culture, whose literal translations often fail to capture their\nfull meaning. This complexity makes them challenging for language models to\ninterpret and use correctly. Existing benchmarks focus on narrow tasks -\nmultiple-choice cloze tests, isolated translation, or simple paraphrasing. We\nintroduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)\nEvaluative Connotation, classifying idioms as positive or negative; (2)\nAppropriateness, detecting incorrect idiom usage in context; and (3) Open\nCloze, filling blanks in longer passages without options. Chengyu-Bench\ncomprises 2,937 human-verified examples covering 1,765 common idioms sourced\nfrom diverse corpora. We evaluate leading LLMs and find they achieve over 95%\naccuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%\ntop-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise\nfrom fundamental misunderstandings of idiom meanings. Chengyu-Bench\ndemonstrates that while LLMs can reliably gauge idiom sentiment, they still\nstruggle to grasp the cultural and contextual nuances essential for proper\nusage. The benchmark and source code are available at:\nhttps://github.com/sofyc/ChengyuBench.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Chengyu-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4e2d\u6587\u6210\u8bed\u7406\u89e3\u548c\u4f7f\u7528\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u9879\u4efb\u52a1\uff1a\u60c5\u611f\u8bc4\u4ef7\u3001\u4f7f\u7528\u6070\u5f53\u6027\u548c\u5f00\u653e\u586b\u7a7a\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u60c5\u611f\u8bc4\u4ef7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f7f\u7528\u6070\u5f53\u6027\u548c\u5f00\u653e\u586b\u7a7a\u4efb\u52a1\u4e2d\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u4e2d\u6587\u6210\u8bed\uff08\u6210\u8bed\uff09\u662f\u8574\u542b\u4e30\u5bcc\u5386\u53f2\u6587\u5316\u7684\u56db\u5b57\u8868\u8fbe\uff0c\u5176\u5b57\u9762\u7ffb\u8bd1\u5f80\u5f80\u65e0\u6cd5\u5b8c\u5168\u4f53\u73b0\u5176\u542b\u4e49\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u72ed\u7a84\u4efb\u52a1\uff0c\u5982\u9009\u62e9\u9898\u586b\u7a7a\u6216\u7b80\u5355\u91ca\u4e49\uff0c\u7f3a\u4e4f\u5bf9\u6210\u8bed\u5168\u9762\u7406\u89e3\u548c\u4f7f\u7528\u7684\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63d0\u51faChengyu-Bench\uff0c\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Chengyu-Bench\u5305\u542b\u4e09\u9879\u4efb\u52a1\uff1a1\uff09\u60c5\u611f\u8bc4\u4ef7\uff08\u5224\u65ad\u6210\u8bed\u7684\u60c5\u611f\u503e\u5411\uff09\uff1b2\uff09\u4f7f\u7528\u6070\u5f53\u6027\uff08\u68c0\u6d4b\u6210\u8bed\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u9519\u8bef\u4f7f\u7528\uff09\uff1b3\uff09\u5f00\u653e\u586b\u7a7a\uff08\u5728\u65e0\u9009\u9879\u7684\u957f\u6587\u4e2d\u586b\u7a7a\uff09\u3002\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b2,937\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u793a\u4f8b\uff0c\u6db5\u76d61,765\u4e2a\u5e38\u89c1\u6210\u8bed\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u8bc4\u4ef7\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u4f46\u5728\u4f7f\u7528\u6070\u5f53\u6027\u4efb\u52a1\u4e2d\u7ea6\u4e3a85%\uff0c\u5f00\u653e\u586b\u7a7a\u4efb\u52a1\u4e2d\u4ec5\u4e3a40%\u5de6\u53f3\u3002\u9519\u8bef\u5206\u6790\u8868\u660e\uff0c\u5927\u591a\u6570\u9519\u8bef\u6e90\u4e8e\u5bf9\u6210\u8bed\u542b\u4e49\u7684\u6839\u672c\u8bef\u89e3\u3002", "conclusion": "Chengyu-Bench\u8868\u660e\uff0c\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u53ef\u9760\u8bc4\u4f30\u6210\u8bed\u60c5\u611f\u503e\u5411\uff0c\u4f46\u5728\u7406\u89e3\u6587\u5316\u548c\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "paper_title_zh": "Chengyu-Bench\uff1a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4e2d\u6587\u6210\u8bed\u7406\u89e3\u4e0e\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u4e2d\u6587\u6210\u8bed\uff08\u6210\u8bed\uff09\u662f\u8574\u542b\u4e30\u5bcc\u5386\u53f2\u6587\u5316\u7684\u56db\u5b57\u8868\u8fbe\uff0c\u5176\u5b57\u9762\u7ffb\u8bd1\u5f80\u5f80\u65e0\u6cd5\u5b8c\u5168\u4f53\u73b0\u5176\u542b\u4e49\u3002\u8fd9\u79cd\u590d\u6742\u6027\u4f7f\u5f97\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6b63\u786e\u7406\u89e3\u548c\u8fd0\u7528\u6210\u8bed\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u72ed\u7a84\u4efb\u52a1\uff0c\u5982\u9009\u62e9\u9898\u586b\u7a7a\u3001\u5b64\u7acb\u7ffb\u8bd1\u6216\u7b80\u5355\u91ca\u4e49\u3002\u6211\u4eec\u63d0\u51fa\u4e86Chengyu-Bench\uff0c\u4e00\u4e2a\u5305\u542b\u4e09\u9879\u4efb\u52a1\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff1a1\uff09\u60c5\u611f\u8bc4\u4ef7\uff08\u5224\u65ad\u6210\u8bed\u7684\u60c5\u611f\u503e\u5411\uff09\uff1b2\uff09\u4f7f\u7528\u6070\u5f53\u6027\uff08\u68c0\u6d4b\u6210\u8bed\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u9519\u8bef\u4f7f\u7528\uff09\uff1b3\uff09\u5f00\u653e\u586b\u7a7a\uff08\u5728\u65e0\u9009\u9879\u7684\u957f\u6587\u4e2d\u586b\u7a7a\uff09\u3002Chengyu-Bench\u5305\u542b2,937\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u793a\u4f8b\uff0c\u6db5\u76d61,765\u4e2a\u5e38\u89c1\u6210\u8bed\uff0c\u6570\u636e\u6765\u6e90\u591a\u6837\u3002\u6211\u4eec\u5bf9\u9886\u5148\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u60c5\u611f\u8bc4\u4ef7\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u4f46\u5728\u4f7f\u7528\u6070\u5f53\u6027\u4efb\u52a1\u4e2d\u7ea6\u4e3a85%\uff0c\u5f00\u653e\u586b\u7a7a\u4efb\u52a1\u4e2d\u4ec5\u4e3a40%\u5de6\u53f3\u3002\u9519\u8bef\u5206\u6790\u8868\u660e\uff0c\u5927\u591a\u6570\u9519\u8bef\u6e90\u4e8e\u5bf9\u6210\u8bed\u542b\u4e49\u7684\u6839\u672c\u8bef\u89e3\u3002Chengyu-Bench\u8868\u660e\uff0c\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u53ef\u9760\u8bc4\u4f30\u6210\u8bed\u60c5\u611f\u503e\u5411\uff0c\u4f46\u5728\u7406\u89e3\u6587\u5316\u548c\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u57fa\u51c6\u6d4b\u8bd5\u548c\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://github.com/sofyc/ChengyuBench\u3002"}}
{"id": "2506.17787", "pdf": "https://arxiv.org/pdf/2506.17787", "abs": "https://arxiv.org/abs/2506.17787", "authors": ["Gelei Xu", "Yuying Duan", "Zheyuan Liu", "Xueyang Li", "Meng Jiang", "Michael Lemmon", "Wei Jin", "Yiyu Shi"], "title": "Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert", "categories": ["cs.CV"], "comment": "11 pages, 2 figures", "summary": "AI-based systems have achieved high accuracy in skin disease diagnostics but\noften exhibit biases across demographic groups, leading to inequitable\nhealthcare outcomes and diminished patient trust. Most existing bias mitigation\nmethods attempt to eliminate the correlation between sensitive attributes and\ndiagnostic prediction, but those methods often degrade performance due to the\nlost of clinically relevant diagnostic cues. In this work, we propose an\nalternative approach that incorporates sensitive attributes to achieve\nfairness. We introduce FairMoE, a framework that employs layer-wise\nmixture-of-experts modules to serve as group-specific learners. Unlike\ntraditional methods that rigidly assign data based on group labels, FairMoE\ndynamically routes data to the most suitable expert, making it particularly\neffective for handling cases near group boundaries. Experimental results show\nthat, unlike previous fairness approaches that reduce performance, FairMoE\nachieves substantial accuracy improvements while preserving comparable fairness\nmetrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFairMoE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u6570\u636e\u5230\u7279\u5b9a\u4e13\u5bb6\u6a21\u5757\uff0c\u89e3\u51b3\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7684AI\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u7cfb\u7edf\u5b58\u5728\u8de8\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u533b\u7597\u7ed3\u679c\u548c\u60a3\u8005\u4fe1\u4efb\u4e0b\u964d\u3002\u4f20\u7edf\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u8bd5\u56fe\u6d88\u9664\u654f\u611f\u5c5e\u6027\u4e0e\u8bca\u65ad\u9884\u6d4b\u7684\u5173\u8054\uff0c\u4f46\u4f1a\u4e22\u5931\u4e34\u5e8a\u76f8\u5173\u8bca\u65ad\u7ebf\u7d22\uff0c\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faFairMoE\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\u4f5c\u4e3a\u7fa4\u4f53\u7279\u5b9a\u5b66\u4e60\u5668\uff0c\u52a8\u6001\u8def\u7531\u6570\u636e\u5230\u6700\u9002\u5408\u7684\u4e13\u5bb6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5904\u7406\u7fa4\u4f53\u8fb9\u754c\u9644\u8fd1\u7684\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFairMoE\u5728\u4fdd\u6301\u516c\u5e73\u6027\u6307\u6807\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u516c\u5e73\u6027\u65b9\u6cd5\u3002", "conclusion": "FairMoE\u901a\u8fc7\u52a8\u6001\u6570\u636e\u8def\u7531\u548c\u7fa4\u4f53\u7279\u5b9a\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u7684\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\uff0c\u4e3a\u533b\u7597AI\u7684\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u878d\u5408\u800c\u975e\u6d88\u9664\uff1a\u901a\u8fc7\u7fa4\u4f53\u7279\u5b9a\u4e13\u5bb6\u5b9e\u73b0\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u7684\u516c\u5e73\u6027", "abstract_zh": "\u57fa\u4e8eAI\u7684\u7cfb\u7edf\u5728\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u4e2d\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5e38\u8868\u73b0\u51fa\u8de8\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u5bfc\u81f4\u533b\u7597\u7ed3\u679c\u4e0d\u516c\u5e73\u548c\u60a3\u8005\u4fe1\u4efb\u4e0b\u964d\u3002\u73b0\u6709\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u5927\u591a\u8bd5\u56fe\u6d88\u9664\u654f\u611f\u5c5e\u6027\u4e0e\u8bca\u65ad\u9884\u6d4b\u7684\u5173\u8054\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5e38\u56e0\u4e22\u5931\u4e34\u5e8a\u76f8\u5173\u8bca\u65ad\u7ebf\u7d22\u800c\u964d\u4f4e\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u654f\u611f\u5c5e\u6027\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u3002\u6211\u4eec\u5f15\u5165FairMoE\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\u4f5c\u4e3a\u7fa4\u4f53\u7279\u5b9a\u5b66\u4e60\u5668\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cFairMoE\u52a8\u6001\u8def\u7531\u6570\u636e\u5230\u6700\u9002\u5408\u7684\u4e13\u5bb6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5904\u7406\u7fa4\u4f53\u8fb9\u754c\u9644\u8fd1\u7684\u60c5\u51b5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ee5\u5f80\u964d\u4f4e\u6027\u80fd\u7684\u516c\u5e73\u6027\u65b9\u6cd5\u4e0d\u540c\uff0cFairMoE\u5728\u4fdd\u6301\u53ef\u6bd4\u516c\u5e73\u6027\u6307\u6807\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2506.18628", "pdf": "https://arxiv.org/pdf/2506.18628", "abs": "https://arxiv.org/abs/2506.18628", "authors": ["Piotr Matys", "Jan Eliasz", "Konrad Kie\u0142czy\u0144ski", "Miko\u0142aj Langner", "Teddy Ferdinan", "Jan Koco\u0144", "Przemys\u0142aw Kazienko"], "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "ICCS 2025 Workshops", "summary": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAggTruth\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5185\u90e8\u6ce8\u610f\u529b\u5206\u6570\u7684\u5206\u5e03\uff0c\u5b9e\u73b0\u5728\u7ebf\u68c0\u6d4b\u4e0a\u4e0b\u6587\u5e7b\u89c9\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u548c\u8de8\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u8bbe\u7f6e\u4e0b\u4e5f\u96be\u4ee5\u907f\u514d\uff0c\u8fd9\u5bf9\u5176\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "AggTruth\u901a\u8fc7\u5206\u6790\u4e0a\u4e0b\u6587\uff08\u6bb5\u843d\uff09\u4e2d\u5185\u90e8\u6ce8\u610f\u529b\u5206\u6570\u7684\u5206\u5e03\uff0c\u63d0\u51fa\u56db\u79cd\u4e0d\u540c\u7684\u805a\u5408\u6280\u672f\u53d8\u4f53\uff0c\u7528\u4e8e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u7684LLMs\u4e2d\uff0cAggTruth\u5728\u76f8\u540c\u4efb\u52a1\u548c\u8de8\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u7a33\u5b9a\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6df1\u5165\u5206\u6790\u4e86\u7279\u5f81\u9009\u62e9\u6280\u672f\u53ca\u6ce8\u610f\u529b\u5934\u6570\u91cf\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "AggTruth\u662f\u4e00\u79cd\u6709\u6548\u7684\u5728\u7ebf\u68c0\u6d4b\u4e0a\u4e0b\u6587\u5e7b\u89c9\u7684\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u4f9d\u8d56\u4e8e\u6ce8\u610f\u529b\u5934\u7684\u7cbe\u5fc3\u9009\u62e9\u3002", "paper_title_zh": "AggTruth\uff1a\u5229\u7528\u805a\u5408\u6ce8\u610f\u529b\u5206\u6570\u68c0\u6d4bLLMs\u4e2d\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9", "abstract_zh": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u8bbe\u7f6e\u4e0b\u4e5f\u96be\u4ee5\u907f\u514d\uff0c\u8fd9\u5bf9\u5176\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u63d0\u51faAggTruth\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e0a\u4e0b\u6587\uff08\u6bb5\u843d\uff09\u4e2d\u5185\u90e8\u6ce8\u610f\u529b\u5206\u6570\u7684\u5206\u5e03\uff0c\u5b9e\u73b0\u5728\u7ebf\u68c0\u6d4b\u4e0a\u4e0b\u6587\u5e7b\u89c9\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u805a\u5408\u6280\u672f\u53d8\u4f53\uff0c\u7528\u4e8e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u3002\u5728\u6240\u6709\u6d4b\u8bd5\u7684LLMs\u4e2d\uff0cAggTruth\u5728\u76f8\u540c\u4efb\u52a1\u548c\u8de8\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u7a33\u5b9a\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6df1\u5165\u5206\u6790\u4e86\u7279\u5f81\u9009\u62e9\u6280\u672f\uff0c\u5e76\u7814\u7a76\u4e86\u6240\u9009\u6ce8\u610f\u529b\u5934\u6570\u91cf\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u7cbe\u5fc3\u9009\u62e9\u6ce8\u610f\u529b\u5934\u5bf9\u5b9e\u73b0\u6700\u4f73\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.18116", "pdf": "https://arxiv.org/pdf/2506.18116", "abs": "https://arxiv.org/abs/2506.18116", "authors": ["Batool Haider", "Atmika Gorti", "Aman Chadha", "Manas Gaur"], "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)", "summary": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8df3\u95ee\u7b54\u6846\u67b6\uff08MHQA\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u60c5\u611f\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u5fc3\u7406\u5065\u5eb7\u6761\u4ef6\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u53bb\u504f\u6280\u672f\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4f20\u64ad\u504f\u89c1\uff0c\u52a0\u5267\u5bf9\u8fb9\u7f18\u7fa4\u4f53\u7684\u6c61\u540d\u5316\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u68c0\u6d4b\u4ea4\u53c9\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u591a\u8df3\u95ee\u7b54\u6846\u67b6\uff08MHQA\uff09\u5206\u6790IMHI\u6570\u636e\u96c6\u4e2d\u7684\u75c7\u72b6\u8868\u73b0\u3001\u5e94\u5bf9\u673a\u5236\u548c\u6cbb\u7597\u65b9\u6848\uff0c\u901a\u8fc7\u5e74\u9f84\u3001\u79cd\u65cf\u3001\u6027\u522b\u548c\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7684\u7cfb\u7edf\u6027\u6807\u6ce8\uff0c\u68c0\u6d4b\u6a21\u578b\u5728\u4eba\u53e3\u4ea4\u53c9\u70b9\u7684\u504f\u89c1\u6a21\u5f0f\u3002\u8bc4\u4f30\u4e86Claude 3.5 Sonnet\u3001Jamba 1.6\u3001Gemma 3\u548cLlama 4\u56db\u79cd\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u60c5\u611f\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u5fc3\u7406\u5065\u5eb7\u6761\u4ef6\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0cMHQA\u65b9\u6cd5\u5728\u504f\u89c1\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u6a21\u62df\u548c\u663e\u6027\u504f\u89c1\u51cf\u5c11\u6280\u672f\uff0c\u504f\u89c1\u51cf\u5c11\u4e8666-94%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u504f\u89c1\u653e\u5927\u70b9\uff0c\u4e3a\u5f00\u53d1\u516c\u5e73\u7684AI\u63d0\u4f9b\u4e86\u53ef\u884c\u5efa\u8bae\u3002", "paper_title_zh": "LLMs\u4e2d\u7684\u5fc3\u7406\u5065\u5eb7\u516c\u5e73\u6027\uff1a\u5229\u7528\u591a\u8df3\u95ee\u7b54\u68c0\u6d4b\u653e\u5927\u548c\u6c89\u9ed8\u7684\u89c2\u70b9", "abstract_zh": "\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u80fd\u4f20\u64ad\u504f\u89c1\uff0c\u52a0\u5267\u5bf9\u8fb9\u7f18\u7fa4\u4f53\u7684\u6c61\u540d\u5316\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63ed\u793a\u4e86\u76f8\u5173\u8d8b\u52bf\uff0c\u4f46\u7cfb\u7edf\u6027\u68c0\u6d4b\u4ea4\u53c9\u504f\u89c1\u7684\u65b9\u6cd5\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u8ba8\u8bba\u4e2d\u7684\u54cd\u5e94\u504f\u89c1\u3002\u6211\u4eec\u5206\u6790\u4e86\u53ef\u89e3\u91ca\u5fc3\u7406\u5065\u5eb7\u6307\u4ee4\uff08IMHI\uff09\u6570\u636e\u96c6\u4e2d\u7684\u75c7\u72b6\u8868\u73b0\u3001\u5e94\u5bf9\u673a\u5236\u548c\u6cbb\u7597\u65b9\u6848\uff0c\u901a\u8fc7\u5e74\u9f84\u3001\u79cd\u65cf\u3001\u6027\u522b\u548c\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7684\u7cfb\u7edf\u6027\u6807\u6ce8\uff0c\u7814\u7a76\u4e86\u4eba\u53e3\u4ea4\u53c9\u70b9\u7684\u504f\u89c1\u6a21\u5f0f\u3002\u8bc4\u4f30\u4e86\u56db\u79cdLLMs\uff1aClaude 3.5 Sonnet\u3001Jamba 1.6\u3001Gemma 3\u548cLlama 4\uff0c\u63ed\u793a\u4e86\u5728\u60c5\u611f\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u5fc3\u7406\u5065\u5eb7\u6761\u4ef6\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002MHQA\u65b9\u6cd5\u5728\u504f\u89c1\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u8bc6\u522b\u901a\u8fc7\u5e8f\u5217\u63a8\u7406\u653e\u5927\u7684\u504f\u89c1\u70b9\u3002\u6211\u4eec\u5b9e\u65bd\u4e86\u4e24\u79cd\u53bb\u504f\u6280\u672f\uff1a\u89d2\u8272\u626e\u6f14\u6a21\u62df\u548c\u663e\u6027\u504f\u89c1\u51cf\u5c11\uff0c\u901a\u8fc7BBQ\u6570\u636e\u96c6\u7684\u5c11\u6837\u672c\u63d0\u793a\u5b9e\u73b0\u4e8666-94%\u7684\u504f\u89c1\u51cf\u5c11\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u504f\u89c1\u590d\u5236\u70b9\uff0c\u4e3a\u516c\u5e73AI\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2506.17837", "pdf": "https://arxiv.org/pdf/2506.17837", "abs": "https://arxiv.org/abs/2506.17837", "authors": ["Assefa Wahd", "Jacob Jaremko", "Abhilash Hareendranathan"], "title": "Time-Contrastive Pretraining for In-Context Image and Video Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In-context learning (ICL) enables generalization to new tasks with minimal\nlabeled data. However, mainstream ICL approaches rely on a gridding strategy,\nwhich lacks the flexibility required for vision applications. We introduce\nTemporal, a time-contrastive self-supervised objective that pretrains a prompt\nretriever for visual ICL, and formulate ICL as a video object segmentation\n(VOS) task. Temporal addresses key limitations of grid-based methods that\nrestrict the number and resolution of context images. By reframing ICL as a VOS\nproblem, our approach supports a variable number of context images while\npreserving their full resolution. To address the challenge of selecting optimal\ncontext sets for queries, we pretrain a prompt retriever on videos via\nself-supervised learning, where adjacent frames serve as positives and distant\nframes as negatives. For image segmentation, the prompt retriever selects\nrelevant sequences that, when combined with the query, form coherent videos for\nVOS processing. For video segmentation, it identifies keyframes, predicts their\nmasks using our ICL pipeline, and propagates them throughout the sequence. When\nevaluated on MICCAI FLARE 2022, our method achieves substantial improvements\nover baselines: 90.95% Dice score for image segmentation (10.64% improvement)\nand 92.45% Dice for video segmentation (14.88% improvement).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08Temporal\uff09\uff0c\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u3002\u901a\u8fc7\u5c06ICL\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08VOS\uff09\u4efb\u52a1\uff0c\u5e76\u8bad\u7ec3\u63d0\u793a\u68c0\u7d22\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u7f51\u683c\u5316\u7b56\u7565\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u56fe\u50cf\u7684\u6570\u91cf\u548c\u5206\u8fa8\u7387\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u4ee5\u652f\u6301\u53ef\u53d8\u6570\u91cf\u7684\u4e0a\u4e0b\u6587\u56fe\u50cf\u5e76\u4fdd\u6301\u5176\u5b8c\u6574\u5206\u8fa8\u7387\u3002", "method": "\u63d0\u51faTemporal\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u63d0\u793a\u68c0\u7d22\u5668\uff0c\u5c06ICL\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3aVOS\u95ee\u9898\u3002\u5728\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u5e8f\u5217\u5f62\u6210\u8fde\u8d2f\u89c6\u9891\uff1b\u5728\u89c6\u9891\u5206\u5272\u4e2d\uff0c\u8bc6\u522b\u5173\u952e\u5e27\u5e76\u4f20\u64ad\u5176\u63a9\u7801\u3002", "result": "\u5728MICCAI FLARE 2022\u6570\u636e\u96c6\u4e0a\uff0c\u56fe\u50cf\u5206\u5272Dice\u5206\u6570\u63d0\u534710.64%\uff08\u8fbe\u523090.95%\uff09\uff0c\u89c6\u9891\u5206\u5272Dice\u5206\u6570\u63d0\u534714.88%\uff08\u8fbe\u523092.45%\uff09\u3002", "conclusion": "Temporal\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u548cVOS\u4efb\u52a1\u91cd\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u65f6\u95f4\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7684\u4e0a\u4e0b\u6587\u56fe\u50cf\u4e0e\u89c6\u9891\u5206\u5272\u65b9\u6cd5", "abstract_zh": "\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u80fd\u591f\u901a\u8fc7\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u5b9e\u73b0\u5bf9\u65b0\u4efb\u52a1\u7684\u6cdb\u5316\u3002\u7136\u800c\uff0c\u4e3b\u6d41ICL\u65b9\u6cd5\u4f9d\u8d56\u7f51\u683c\u5316\u7b56\u7565\uff0c\u7f3a\u4e4f\u89c6\u89c9\u5e94\u7528\u6240\u9700\u7684\u7075\u6d3b\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86Temporal\uff0c\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u5bf9\u6bd4\u7684\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9ICL\u7684\u63d0\u793a\u68c0\u7d22\u5668\uff0c\u5e76\u5c06ICL\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08VOS\uff09\u95ee\u9898\u3002Temporal\u89e3\u51b3\u4e86\u7f51\u683c\u5316\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u56fe\u50cf\u6570\u91cf\u548c\u5206\u8fa8\u7387\u4e0a\u7684\u9650\u5236\u3002\u901a\u8fc7\u5c06ICL\u91cd\u6784\u4e3aVOS\u95ee\u9898\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u652f\u6301\u53ef\u53d8\u6570\u91cf\u7684\u4e0a\u4e0b\u6587\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5b8c\u6574\u5206\u8fa8\u7387\u3002\u4e3a\u5e94\u5bf9\u9009\u62e9\u6700\u4f18\u4e0a\u4e0b\u6587\u96c6\u7684\u6311\u6218\uff0c\u6211\u4eec\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u89c6\u9891\u4e0a\u9884\u8bad\u7ec3\u63d0\u793a\u68c0\u7d22\u5668\uff0c\u5176\u4e2d\u76f8\u90bb\u5e27\u4f5c\u4e3a\u6b63\u6837\u672c\uff0c\u8fdc\u8ddd\u79bb\u5e27\u4f5c\u4e3a\u8d1f\u6837\u672c\u3002\u5728\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u63d0\u793a\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u5e8f\u5217\uff0c\u4e0e\u67e5\u8be2\u7ed3\u5408\u5f62\u6210\u8fde\u8d2f\u89c6\u9891\u4ee5\u4f9bVOS\u5904\u7406\u3002\u5728\u89c6\u9891\u5206\u5272\u4e2d\uff0c\u5b83\u8bc6\u522b\u5173\u952e\u5e27\uff0c\u4f7f\u7528ICL\u7ba1\u9053\u9884\u6d4b\u5176\u63a9\u7801\uff0c\u5e76\u5728\u5e8f\u5217\u4e2d\u4f20\u64ad\u3002\u5728MICCAI FLARE 2022\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff1a\u56fe\u50cf\u5206\u5272Dice\u5206\u6570\u8fbe\u523090.95%\uff08\u63d0\u534710.64%\uff09\uff0c\u89c6\u9891\u5206\u5272Dice\u5206\u6570\u8fbe\u523092.45%\uff08\u63d0\u534714.88%\uff09\u3002"}}
{"id": "2506.18651", "pdf": "https://arxiv.org/pdf/2506.18651", "abs": "https://arxiv.org/abs/2506.18651", "authors": ["Shuocun Yang", "Huawen Hu", "Enze Shi", "Shu Zhang"], "title": "Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems", "categories": ["cs.AI"], "comment": null, "summary": "Behavioral diversity in Multi-agent reinforcement learning(MARL) represents\nan emerging and promising research area. Prior work has largely centered on\nintra-group behavioral consistency in multi-agent systems, with limited\nattention given to behavioral consistency in multi-agent grouping scenarios. In\nthis paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL\ncontrol method designed to explicitly regulate agent behaviors at both\nintra-group and inter-group levels. DLBC partitions agents into distinct groups\nand dynamically modulates behavioral diversity both within and between these\ngroups. By dynamically modulating behavioral diversity within and between these\ngroups, DLBC achieves enhanced division of labor through inter-group\nconsistency, which constrains behavioral strategies across different groups.\nSimultaneously, intra-group consistency, achieved by aligning behavioral\nstrategies within each group, fosters stronger intra-group cooperation.\nCrucially, DLBC's direct constraint of agent policy functions ensures its broad\napplicability across various algorithmic frameworks. Experimental results in\nvarious grouping cooperation scenarios demonstrate that DLBC significantly\nenhances both intra-group cooperative performance and inter-group task\nspecialization, yielding substantial performance improvements. DLBC provides\nnew ideas for behavioral consistency control of multi-intelligent body systems,\nand its potential for application in more complex tasks and dynamic\nenvironments can be further explored in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u5c42\u6b21\u884c\u4e3a\u4e00\u81f4\u6027\uff08DLBC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u540c\u65f6\u8c03\u63a7\u7ec4\u5185\u548c\u7ec4\u95f4\u7684\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5185\u5408\u4f5c\u548c\u7ec4\u95f4\u4efb\u52a1\u5206\u5de5\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7ec4\u5185\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u667a\u80fd\u4f53\u5206\u7ec4\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u53cc\u5c42\u6b21\u884c\u4e3a\u4e00\u81f4\u6027\u8c03\u63a7\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u6548\u7387\u3002", "method": "DLBC\u65b9\u6cd5\u5c06\u667a\u80fd\u4f53\u5212\u5206\u4e3a\u4e0d\u540c\u7ec4\uff0c\u52a8\u6001\u8c03\u63a7\u7ec4\u5185\u548c\u7ec4\u95f4\u7684\u884c\u4e3a\u591a\u6837\u6027\u3002\u901a\u8fc7\u7ec4\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\u4e0d\u540c\u7ec4\u7684\u884c\u4e3a\u7b56\u7565\uff0c\u5b9e\u73b0\u4efb\u52a1\u5206\u5de5\uff1b\u901a\u8fc7\u7ec4\u5185\u4e00\u81f4\u6027\u5bf9\u9f50\u7ec4\u5185\u884c\u4e3a\u7b56\u7565\uff0c\u4fc3\u8fdb\u7ec4\u5185\u5408\u4f5c\u3002DLBC\u76f4\u63a5\u7ea6\u675f\u667a\u80fd\u4f53\u7684\u7b56\u7565\u51fd\u6570\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u7b97\u6cd5\u9002\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDLBC\u5728\u591a\u79cd\u5206\u7ec4\u5408\u4f5c\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5185\u5408\u4f5c\u6027\u80fd\u548c\u7ec4\u95f4\u4efb\u52a1\u4e13\u4e1a\u5316\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DLBC\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u66f4\u590d\u6742\u4efb\u52a1\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7ec4\u95f4\u4e0e\u7ec4\u5185\u534f\u8c03\u7684\u53cc\u5c42\u6b21\u884c\u4e3a\u4e00\u81f4\u6027", "abstract_zh": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u7684\u884c\u4e3a\u591a\u6837\u6027\u662f\u4e00\u4e2a\u65b0\u5174\u4e14\u6709\u524d\u666f\u7684\u7814\u7a76\u9886\u57df\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7ec4\u5185\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u800c\u5bf9\u591a\u667a\u80fd\u4f53\u5206\u7ec4\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u5173\u6ce8\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u53cc\u5c42\u6b21\u884c\u4e3a\u4e00\u81f4\u6027\uff08DLBC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684MARL\u63a7\u5236\u65b9\u6cd5\uff0c\u65e8\u5728\u660e\u786e\u8c03\u63a7\u667a\u80fd\u4f53\u5728\u7ec4\u5185\u548c\u7ec4\u95f4\u4e24\u4e2a\u5c42\u6b21\u7684\u884c\u4e3a\u3002DLBC\u5c06\u667a\u80fd\u4f53\u5212\u5206\u4e3a\u4e0d\u540c\u7ec4\uff0c\u5e76\u52a8\u6001\u8c03\u63a7\u7ec4\u5185\u548c\u7ec4\u95f4\u7684\u884c\u4e3a\u591a\u6837\u6027\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u63a7\u7ec4\u5185\u548c\u7ec4\u95f4\u7684\u884c\u4e3a\u591a\u6837\u6027\uff0cDLBC\u901a\u8fc7\u7ec4\u95f4\u4e00\u81f4\u6027\uff08\u7ea6\u675f\u4e0d\u540c\u7ec4\u7684\u884c\u4e3a\u7b56\u7565\uff09\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u4efb\u52a1\u5206\u5de5\uff1b\u540c\u65f6\uff0c\u901a\u8fc7\u7ec4\u5185\u4e00\u81f4\u6027\uff08\u5bf9\u9f50\u7ec4\u5185\u884c\u4e3a\u7b56\u7565\uff09\u4fc3\u8fdb\u4e86\u66f4\u5f3a\u7684\u7ec4\u5185\u5408\u4f5c\u3002\u91cd\u8981\u7684\u662f\uff0cDLBC\u76f4\u63a5\u7ea6\u675f\u667a\u80fd\u4f53\u7684\u7b56\u7565\u51fd\u6570\uff0c\u786e\u4fdd\u4e86\u5176\u5728\u591a\u79cd\u7b97\u6cd5\u6846\u67b6\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002\u5728\u591a\u79cd\u5206\u7ec4\u5408\u4f5c\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDLBC\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5185\u5408\u4f5c\u6027\u80fd\u548c\u7ec4\u95f4\u4efb\u52a1\u4e13\u4e1a\u5316\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002DLBC\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u66f4\u590d\u6742\u4efb\u52a1\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.18120", "pdf": "https://arxiv.org/pdf/2506.18120", "abs": "https://arxiv.org/abs/2506.18120", "authors": ["Tom S Juzek"], "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.1"], "comment": "Accepted and published at LREC-COLING 2024. 8 pages, 3 figures.\n  Licensed under CC BY-NC-SA 4.0", "summary": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u300a\u53e5\u6cd5\u53ef\u63a5\u53d7\u6027\u6570\u636e\u96c6\uff08\u9884\u89c8\u7248\uff09\u300b\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u53e5\u6cd5\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u7814\u7a76\u8bbe\u8ba1\u7684\u8d44\u6e90\uff0c\u5305\u542b1000\u4e2a\u82f1\u8bed\u5e8f\u5217\uff0c\u6807\u6ce8\u4e86\u8bed\u6cd5\u72b6\u6001\u548c\u53ef\u63a5\u53d7\u6027\u72b6\u6001\u3002\u521d\u6b65\u5206\u6790\u663e\u793a\u8bed\u6cd5\u6027\u548c\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u572883%\u7684\u60c5\u51b5\u4e0b\u4e00\u81f4\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u53ef\u63a5\u53d7\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u52a8\u673a\u662f\u4e3a\u53e5\u6cd5\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u9886\u57df\u63d0\u4f9b\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u8d44\u6e90\uff0c\u7528\u4e8e\u7814\u7a76\u8bed\u6cd5\u6027\u548c\u53ef\u63a5\u53d7\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u652f\u6301\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4ece\u6559\u79d1\u4e66\u548c\u300aLinguistic Inquiry\u300b\u671f\u520a\u4e2d\u6536\u96c61000\u4e2a\u82f1\u8bed\u5e8f\u5217\uff0c\u6807\u6ce8\u5176\u8bed\u6cd5\u72b6\u6001\uff08\u57fa\u4e8e\u6587\u732e\uff09\u548c\u53ef\u63a5\u53d7\u6027\u72b6\u6001\uff08\u901a\u8fc7\u4f17\u5305\u5b9e\u9a8c\uff09\u3002\u521d\u6b65\u5206\u6790\u805a\u7126\u4e8e\u8bed\u6cd5\u6027\u548c\u53ef\u63a5\u53d7\u6027\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8bed\u6cd5\u6027\u548c\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u572883%\u7684\u60c5\u51b5\u4e0b\u4e00\u81f4\uff0c\u4e14\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u53ef\u63a5\u53d7\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u8bed\u6cd5\u6027\u9884\u6d4b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5f53\u524d\u6570\u636e\u96c6\u662f\u540c\u7c7b\u4e2d\u6700\u5927\u7684\u516c\u5f00\u8d44\u6e90\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u652f\u6301\u66f4\u6df1\u5165\u7684\u7814\u7a76\u3002", "paper_title_zh": "\u300a\u53e5\u6cd5\u53ef\u63a5\u53d7\u6027\u6570\u636e\u96c6\uff08\u9884\u89c8\u7248\uff09\uff1a\u82f1\u8bed\u673a\u5668\u5b66\u4e60\u548c\u8bed\u8a00\u5206\u6790\u7684\u8d44\u6e90\u300b", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u300a\u53e5\u6cd5\u53ef\u63a5\u53d7\u6027\u6570\u636e\u96c6\uff08\u9884\u89c8\u7248\uff09\u300b\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u53e5\u6cd5\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u7814\u7a76\u8bbe\u8ba1\u7684\u8d44\u6e90\u3002\u5f53\u524d\u6570\u636e\u96c6\u5305\u542b1000\u4e2a\u82f1\u8bed\u5e8f\u5217\uff0c\u5176\u4e2d\u4e00\u534a\u6765\u81ea\u6559\u79d1\u4e66\uff0c\u53e6\u4e00\u534a\u6765\u81ea\u300aLinguistic Inquiry\u300b\u671f\u520a\uff0c\u4ee5\u786e\u4fdd\u5f53\u4ee3\u8bdd\u8bed\u7684\u4ee3\u8868\u6027\u3002\u6bcf\u4e2a\u6761\u76ee\u6807\u6ce8\u4e86\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u7684\u8bed\u6cd5\u72b6\u6001\uff08\u201c\u5f62\u5f0f\u53e5\u6cd5\u7684\u826f\u597d\u6027\u201d\uff09\u548c\u901a\u8fc7\u4f17\u5305\u5b9e\u9a8c\u83b7\u5f97\u7684\u53ef\u63a5\u53d7\u6027\u72b6\u6001\uff08\u201c\u6bcd\u8bed\u8005\u7684\u76f4\u89c9\u5224\u65ad\u201d\uff09\u3002\u5373\u4f7f\u5728\u521d\u6b65\u9636\u6bb5\uff0c\u8be5\u6570\u636e\u96c6\u4ecd\u662f\u540c\u7c7b\u4e2d\u6700\u5927\u7684\u516c\u5f00\u8d44\u6e90\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u521d\u6b65\u5206\u6790\uff0c\u6d89\u53ca\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u7684\u4e09\u4e2a\u4e89\u8bae\uff1a\u6211\u4eec\u53d1\u73b0\u8bed\u6cd5\u6027\u548c\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u5728\u7ea683%\u7684\u60c5\u51b5\u4e0b\u4e00\u81f4\uff0c\u4e14\u201c\u4e2d\u95f4\u72b6\u6001\u201d\u9891\u7e41\u51fa\u73b0\uff0c\u8fd9\u4e0e\u73b0\u6709\u7814\u7a76\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u8bed\u6cd5\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u9884\u6d4b\u53ef\u63a5\u53d7\u6027\u65b9\u9762\u8868\u73b0\u8f83\u597d\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u53d1\u73b0\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4e13\u6ce8\u4e8e\u6269\u5c55\u6570\u636e\u96c6\u3002"}}
{"id": "2506.17838", "pdf": "https://arxiv.org/pdf/2506.17838", "abs": "https://arxiv.org/abs/2506.17838", "authors": ["Kazuki Naganuma", "Shunsuke Ono"], "title": "Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to IEEE Transactions on Image Processing. The code is\n  available at\n  https://drive.google.com/file/d/1tuVuIgkArCryVSifJDyG7R468DCLMkF2/view?usp=sharing", "summary": "This paper proposes a foreground-background separation (FBS) method with a\nnovel foreground model based on convolutional sparse representation (CSR). In\norder to analyze the dynamic and static components of videos acquired under\nundesirable conditions, such as hardware, environmental, and power limitations,\nit is essential to establish an FBS method that can handle videos with low\nframe rates and various types of noise. Existing FBS methods have two\nlimitations that prevent us from accurately separating foreground and\nbackground components from such degraded videos. First, they only capture\neither data-specific or general features of the components. Second, they do not\ninclude explicit models for various types of noise to remove them in the FBS\nprocess. To this end, we propose a robust FBS method with a CSR-based\nforeground model. This model can adaptively capture specific spatial structures\nscattered in imaging data. Then, we formulate FBS as a constrained multiconvex\noptimization problem that incorporates CSR, functions that capture general\nfeatures, and explicit noise characterization functions for multiple types of\nnoise. Thanks to these functions, our method captures both data-specific and\ngeneral features to accurately separate the components from various types of\nnoise even under low frame rates. To obtain a solution of the optimization\nproblem, we develop an algorithm that alternately solves its two convex\nsubproblems by newly established algorithms. Experiments demonstrate the\nsuperiority of our method over existing methods using two types of degraded\nvideos: infrared and microscope videos.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u7a00\u758f\u8868\u793a\uff08CSR\uff09\u7684\u524d\u666f-\u80cc\u666f\u5206\u79bb\uff08FBS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e25\u91cd\u9000\u5316\u7684\u89c6\u9891\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u6355\u6349\u7a7a\u95f4\u7ed3\u6784\u5e76\u6709\u6548\u53bb\u9664\u591a\u79cd\u566a\u58f0\u3002", "motivation": "\u73b0\u6709FBS\u65b9\u6cd5\u5728\u4f4e\u5e27\u7387\u548c\u591a\u566a\u58f0\u6761\u4ef6\u4e0b\u96be\u4ee5\u51c6\u786e\u5206\u79bb\u524d\u666f\u548c\u80cc\u666f\uff0c\u4e3b\u8981\u56e0\u4e3a\u5176\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u6570\u636e\u7279\u5b9a\u548c\u901a\u7528\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u591a\u79cd\u566a\u58f0\u7684\u663e\u5f0f\u5efa\u6a21\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCSR\u7684\u524d\u666f\u6a21\u578b\uff0c\u5c06FBS\u5efa\u6a21\u4e3a\u591a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408CSR\u3001\u901a\u7528\u7279\u5f81\u6355\u6349\u51fd\u6570\u548c\u566a\u58f0\u8868\u5f81\u51fd\u6570\uff0c\u5e76\u5f00\u53d1\u4e86\u4ea4\u66ff\u6c42\u89e3\u5b50\u95ee\u9898\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7ea2\u5916\u548c\u663e\u5fae\u955c\u89c6\u9891\u7b49\u9000\u5316\u89c6\u9891\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u51c6\u786e\u5206\u79bb\u524d\u666f\u548c\u80cc\u666f\u5e76\u53bb\u9664\u566a\u58f0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684CSR-based FBS\u65b9\u6cd5\u5728\u4e25\u91cd\u9000\u5316\u89c6\u9891\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u52a8\u6001\u548c\u9759\u6001\u6210\u5206\u7684\u5206\u79bb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u5377\u79ef\u7a00\u758f\u8868\u793a\u5efa\u6a21\u7684\u4e25\u91cd\u9000\u5316\u89c6\u9891\u524d\u666f-\u80cc\u666f\u9c81\u68d2\u5206\u79bb\u65b9\u6cd5", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u7a00\u758f\u8868\u793a\uff08CSR\uff09\u7684\u524d\u666f-\u80cc\u666f\u5206\u79bb\uff08FBS\uff09\u65b9\u6cd5\uff0c\u5176\u524d\u666f\u6a21\u578b\u91c7\u7528CSR\u6280\u672f\u3002\u4e3a\u4e86\u5206\u6790\u5728\u786c\u4ef6\u3001\u73af\u5883\u548c\u7535\u6e90\u9650\u5236\u7b49\u4e0d\u826f\u6761\u4ef6\u4e0b\u83b7\u53d6\u7684\u89c6\u9891\u7684\u52a8\u6001\u548c\u9759\u6001\u6210\u5206\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4f4e\u5e27\u7387\u548c\u591a\u79cd\u566a\u58f0\u7684FBS\u65b9\u6cd5\u3002\u73b0\u6709FBS\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\u4e00\u662f\u4ec5\u80fd\u6355\u6349\u6570\u636e\u7279\u5b9a\u6216\u901a\u7528\u7279\u5f81\uff1b\u4e8c\u662f\u7f3a\u4e4f\u5bf9\u591a\u79cd\u566a\u58f0\u7684\u663e\u5f0f\u5efa\u6a21\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCSR\u7684\u9c81\u68d2FBS\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u6355\u6349\u6210\u50cf\u6570\u636e\u4e2d\u7684\u7279\u5b9a\u7a7a\u95f4\u7ed3\u6784\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06FBS\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ea6\u675f\u591a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u4e86CSR\u3001\u901a\u7528\u7279\u5f81\u6355\u6349\u51fd\u6570\u548c\u591a\u79cd\u566a\u58f0\u7684\u8868\u5f81\u51fd\u6570\u3002\u901a\u8fc7\u8fd9\u4e9b\u51fd\u6570\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4f4e\u5e27\u7387\u4e0b\u51c6\u786e\u5206\u79bb\u6210\u5206\u5e76\u53bb\u9664\u591a\u79cd\u566a\u58f0\u3002\u4e3a\u89e3\u51b3\u8be5\u4f18\u5316\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4ea4\u66ff\u6c42\u89e3\u5176\u4e24\u4e2a\u51f8\u5b50\u95ee\u9898\u7684\u65b0\u7b97\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7ea2\u5916\u548c\u663e\u5fae\u955c\u89c6\u9891\u7b49\u9000\u5316\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18777", "pdf": "https://arxiv.org/pdf/2506.18777", "abs": "https://arxiv.org/abs/2506.18777", "authors": ["Jonathan Cook", "Silvia Sapora", "Arash Ahmadian", "Akbir Khan", "Tim Rocktaschel", "Jakob Foerster", "Laura Ruis"], "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u53cd\u5411\u4f20\u64ad\u7f16\u7a0b\u201d\uff08PBB\uff09\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u4ee3\u7801\u8bad\u7ec3\u83b7\u5f97\u53ef\u91cd\u7528\u7b97\u6cd5\u62bd\u8c61\u7684\u6f5c\u5728\u673a\u5236\uff0c\u7814\u7a76\u53d1\u73b0\u4ec5\u901a\u8fc7\u6e90\u4ee3\u7801\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u8bc4\u4f30\u7a0b\u5e8f\u8f93\u5165\uff0c\u4e14\u6548\u679c\u4f18\u4e8e\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\u7684\u8bad\u7ec3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793aLLM\u901a\u8fc7\u4ee3\u7801\u8bad\u7ec3\u63d0\u5347\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u673a\u5236\uff0c\u63a2\u7d22\u4ec5\u901a\u8fc7\u6e90\u4ee3\u7801\u8bad\u7ec3\u6a21\u578b\u662f\u5426\u80fd\u4f7f\u5176\u5177\u5907\u8bc4\u4f30\u7a0b\u5e8f\u8f93\u5165\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5fae\u8c03LLM\u5728\u4e24\u7ec4\u7a0b\u5e8f\uff08\u6570\u5b66\u95ee\u9898\u548c\u7b97\u6cd5\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e00\u7ec4\u5305\u542b\u6e90\u4ee3\u7801\u548c\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\uff08w/ IO\uff09\uff0c\u53e6\u4e00\u7ec4\u4ec5\u5305\u542b\u6e90\u4ee3\u7801\uff08w/o IO\uff09\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u901a\u8fc7\u6e90\u4ee3\u7801\u8bad\u7ec3\u7684LLM\u5177\u5907\u8bc4\u4f30\u7a0b\u5e8f\u8f93\u5165\u7684\u80fd\u529b\uff0c\u4e14\u4ee3\u7801\u5f62\u5f0f\u6bd4\u8bed\u4e49\u63cf\u8ff0\u66f4\u6709\u6548\uff1b\u6a21\u578b\u53ef\u901a\u8fc7\u524d\u5411\u4f20\u9012\u9690\u5f0f\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u800c\u94fe\u5f0f\u601d\u8003\uff08chain-of-thought\uff09\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u53ef\u9760\u6027\u3002\u6b64\u5916\uff0cPBB\u6bd4\u57fa\u4e8e\u81ea\u7136\u6570\u636e\u5206\u5e03\u7684\u8f93\u5165\u8f93\u51fa\u8bad\u7ec3\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u4ee3\u7801\u8bad\u7ec3\u4f7fLLM\u80fd\u591f\u5185\u5316\u53ef\u91cd\u7528\u7684\u7b97\u6cd5\u62bd\u8c61\uff0c\u4ece\u800c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316LLM\u4ece\u7b26\u53f7\u5316\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5e76\u4e3a\u6a21\u578b\u5bf9\u9f50\u7b49\u9886\u57df\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u53cd\u5411\u4f20\u64ad\u7f16\u7a0b\uff1a\u4ee3\u7801\u8bad\u7ec3\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u53ef\u91cd\u7528\u7b97\u6cd5\u62bd\u8c61", "abstract_zh": "\u5728\u6e90\u4ee3\u7801\u4e0a\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u663e\u8457\u63d0\u5347\u4e86\u5176\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u6cdb\u5316\u80cc\u540e\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u63d0\u51fa\u201c\u53cd\u5411\u4f20\u64ad\u7f16\u7a0b\u201d\uff08PBB\uff09\u4f5c\u4e3a\u6f5c\u5728\u9a71\u52a8\u56e0\u7d20\u2014\u2014\u4ec5\u901a\u8fc7\u6e90\u4ee3\u7801\u8bad\u7ec3\u6a21\u578b\u8bc4\u4f30\u7a0b\u5e8f\u8f93\u5165\uff0c\u800c\u65e0\u9700\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\u3002\u4e3a\u9a8c\u8bc1\u8fd9\u4e00\u60f3\u6cd5\uff0c\u6211\u4eec\u5728\u4e24\u7ec4\u7a0b\u5e8f\uff08\u7b80\u5355\u6570\u5b66\u95ee\u9898\u548c\u7b97\u6cd5\uff09\u4e0a\u5fae\u8c03LLM\uff1a\u4e00\u7ec4\u5305\u542b\u6e90\u4ee3\u7801\u548c\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\uff08w/ IO\uff09\uff0c\u53e6\u4e00\u7ec4\u4ec5\u5305\u542b\u6e90\u4ee3\u7801\uff08w/o IO\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u5177\u5907\u8bc4\u4f30w/o IO\u7a0b\u5e8f\u8f93\u5165\u7684\u80fd\u529b\uff0c\u5e76\u5f97\u51fa\u4ee5\u4e0b\u89c2\u5bdf\uff1a\u9996\u5148\uff0cPBB\u5728\u4ee3\u7801\u5f62\u5f0f\u4e0b\u6548\u679c\u663e\u8457\u4f18\u4e8e\u8bed\u4e49\u63cf\u8ff0\uff1b\u5176\u6b21\uff0cLLM\u53ef\u901a\u8fc7\u524d\u5411\u4f20\u9012\u9690\u5f0f\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u800c\u94fe\u5f0f\u601d\u8003\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u53ef\u9760\u6027\u3002\u6b64\u5916\uff0cPBB\u6bd4\u57fa\u4e8e\u81ea\u7136\u6570\u636e\u5206\u5e03\u7684\u8f93\u5165\u8f93\u51fa\u8bad\u7ec3\u66f4\u5177\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u4ee3\u7801\u8bad\u7ec3\u901a\u8fc7\u5185\u5316\u53ef\u91cd\u7528\u7b97\u6cd5\u62bd\u8c61\u589e\u5f3a\u4e86\u63a8\u7406\u80fd\u529b\u3002\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316LLM\u4ece\u7b26\u53f7\u5316\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5e76\u4e3a\u6a21\u578b\u5bf9\u9f50\u7b49\u9886\u57df\uff08\u5982\u57fa\u4e8e\u6b63\u5f0f\u5baa\u6cd5\u539f\u5219\u7684\u8bad\u7ec3\uff09\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.18129", "pdf": "https://arxiv.org/pdf/2506.18129", "abs": "https://arxiv.org/abs/2506.18129", "authors": ["Bugra Kilictas", "Faruk Alpay"], "title": "$\u03c6^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models", "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 03B70", "I.2.6; I.2.7; I.2.3; F.4.1"], "comment": "16 pages, 3 figures", "summary": "We identify a critical vulnerability in autoregressive transformer language\nmodels where the em dash token induces recursive semantic drift, leading to\nclause boundary hallucination and embedding space entanglement. Through formal\nanalysis of token-level perturbations in semantic lattices, we demonstrate that\nem dash insertion fundamentally alters the model's latent representations,\ncausing compounding errors in long-form generation. We propose a novel solution\ncombining symbolic clause purification via the phi-infinity operator with\ntargeted embedding matrix realignment. Our approach enables total suppression\nof problematic tokens without requiring model retraining, while preserving\nsemantic coherence through fixed-point convergence guarantees. Experimental\nvalidation shows significant improvements in generation consistency and topic\nmaintenance. This work establishes a general framework for identifying and\nmitigating token-level vulnerabilities in foundation models, with immediate\nimplications for AI safety, model alignment, and robust deployment of large\nlanguage models in production environments. The methodology extends beyond\npunctuation to address broader classes of recursive instabilities in neural\ntext generation systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "paper_title_zh": "Error", "abstract_zh": "Error"}}
{"id": "2506.17858", "pdf": "https://arxiv.org/pdf/2506.17858", "abs": "https://arxiv.org/abs/2506.17858", "authors": ["Yingcheng Liu", "Peiqi Wang", "Sebastian Diaz", "Esra Abaci Turk", "Benjamin Billot", "Patricia Ellen Grant", "Polina Golland"], "title": "Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose", "categories": ["cs.CV"], "comment": null, "summary": "Analyzing fetal body motion and shape is paramount in prenatal diagnostics\nand monitoring. Existing methods for fetal MRI analysis mainly rely on\nanatomical keypoints or volumetric body segmentations. Keypoints simplify body\nstructure to facilitate motion analysis, but may ignore important details of\nfull-body shape. Body segmentations capture complete shape information but\ncomplicate temporal analysis due to large non-local fetal movements. To address\nthese limitations, we construct a 3D articulated statistical fetal body model\nbased on the Skinned Multi-Person Linear Model (SMPL). Our algorithm\niteratively estimates body pose in the image space and body shape in the\ncanonical pose space. This approach improves robustness to MRI motion artifacts\nand intensity distortions, and reduces the impact of incomplete surface\nobservations due to challenging fetal poses. We train our model on\nsegmentations and keypoints derived from $19,816$ MRI volumes across $53$\nsubjects. Our model captures body shape and motion across time series and\nprovides intuitive visualization. Furthermore, it enables automated\nanthropometric measurements traditionally difficult to obtain from\nsegmentations and keypoints. When tested on unseen fetal body shapes, our\nmethod yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.\nTo our knowledge, this represents the first 3D articulated statistical fetal\nbody model, paving the way for enhanced fetal motion and shape analysis in\nprenatal diagnostics. The code is available at\nhttps://github.com/MedicalVisionGroup/fetal-smpl .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSMPL\u76843D\u80ce\u513f\u7edf\u8ba1\u8eab\u4f53\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u80ce\u513f\u5f62\u72b6\u548c\u59ff\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u8282\u6355\u6349\u548c\u8fd0\u52a8\u5206\u6790\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728MRI\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u80ce\u513fMRI\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u89e3\u5256\u5173\u952e\u70b9\u6216\u4f53\u79ef\u5206\u5272\uff0c\u524d\u8005\u7b80\u5316\u4e86\u8eab\u4f53\u7ed3\u6784\u4f46\u5ffd\u7565\u7ec6\u8282\uff0c\u540e\u8005\u6355\u6349\u5b8c\u6574\u5f62\u72b6\u4f46\u96be\u4ee5\u5206\u6790\u8fd0\u52a8\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u80ce\u513f\u5f62\u72b6\u548c\u8fd0\u52a8\u5206\u6790\u6a21\u578b\u3002", "method": "\u57fa\u4e8eSMPL\u6784\u5efa3D\u80ce\u513f\u7edf\u8ba1\u8eab\u4f53\u6a21\u578b\uff0c\u8fed\u4ee3\u4f30\u8ba1\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u8eab\u4f53\u59ff\u6001\u548c\u6807\u51c6\u59ff\u6001\u7a7a\u95f4\u4e2d\u7684\u8eab\u4f53\u5f62\u72b6\uff0c\u63d0\u9ad8\u5bf9MRI\u8fd0\u52a8\u4f2a\u5f71\u548c\u5f3a\u5ea6\u5931\u771f\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u51cf\u5c11\u56e0\u80ce\u513f\u59ff\u6001\u5bfc\u81f4\u7684\u8868\u9762\u89c2\u6d4b\u4e0d\u5b8c\u6574\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u572853\u540d\u53d7\u8bd5\u8005\u768419,816\u4e2aMRI\u4f53\u79ef\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5bf9\u672a\u89c1\u80ce\u513f\u5f62\u72b6\u7684\u8868\u9762\u5bf9\u9f50\u8bef\u5dee\u4e3a3.2\u6beb\u7c73\uff083\u6beb\u7c73MRI\u4f53\u7d20\uff09\u3002\u6a21\u578b\u652f\u6301\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5e76\u63d0\u4f9b\u76f4\u89c2\u53ef\u89c6\u5316\uff0c\u8fd8\u80fd\u5b9e\u73b0\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u83b7\u53d6\u7684\u81ea\u52a8\u5316\u4eba\u4f53\u6d4b\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9996\u4e2a3D\u80ce\u513f\u7edf\u8ba1\u8eab\u4f53\u6a21\u578b\uff0c\u4e3a\u80ce\u513f\u8fd0\u52a8\u548c\u5f62\u72b6\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u671b\u63d0\u5347\u4ea7\u524d\u8bca\u65ad\u7684\u6548\u679c\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u7b80\u5316\u80ce\u513f\u5efa\u6a21\uff1a\u80ce\u513f\u5f62\u72b6\u4e0e\u59ff\u6001\u7684\u5efa\u6a21\u4e0e\u8ffd\u8e2a", "abstract_zh": "\u5206\u6790\u80ce\u513f\u8eab\u4f53\u8fd0\u52a8\u548c\u5f62\u72b6\u5bf9\u4ea7\u524d\u8bca\u65ad\u548c\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u80ce\u513fMRI\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89e3\u5256\u5173\u952e\u70b9\u6216\u4f53\u79ef\u5206\u5272\u3002\u5173\u952e\u70b9\u7b80\u5316\u4e86\u8eab\u4f53\u7ed3\u6784\u4ee5\u4fbf\u4e8e\u8fd0\u52a8\u5206\u6790\uff0c\u4f46\u53ef\u80fd\u5ffd\u7565\u5168\u8eab\u5f62\u72b6\u7684\u91cd\u8981\u7ec6\u8282\uff1b\u4f53\u79ef\u5206\u5272\u6355\u6349\u4e86\u5b8c\u6574\u5f62\u72b6\u4fe1\u606f\uff0c\u4f46\u56e0\u80ce\u513f\u5927\u8303\u56f4\u975e\u5c40\u90e8\u8fd0\u52a8\u800c\u96be\u4ee5\u8fdb\u884c\u65f6\u95f4\u5206\u6790\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\uff0c\u6211\u4eec\u57fa\u4e8eSkinned Multi-Person Linear Model\uff08SMPL\uff09\u6784\u5efa\u4e86\u4e00\u4e2a3D\u80ce\u513f\u7edf\u8ba1\u8eab\u4f53\u6a21\u578b\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u8fed\u4ee3\u4f30\u8ba1\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u8eab\u4f53\u59ff\u6001\u548c\u6807\u51c6\u59ff\u6001\u7a7a\u95f4\u4e2d\u7684\u8eab\u4f53\u5f62\u72b6\uff0c\u63d0\u9ad8\u4e86\u5bf9MRI\u8fd0\u52a8\u4f2a\u5f71\u548c\u5f3a\u5ea6\u5931\u771f\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u56e0\u80ce\u513f\u59ff\u6001\u5bfc\u81f4\u7684\u8868\u9762\u89c2\u6d4b\u4e0d\u5b8c\u6574\u7684\u5f71\u54cd\u3002\u6a21\u578b\u572853\u540d\u53d7\u8bd5\u8005\u768419,816\u4e2aMRI\u4f53\u79ef\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8eab\u4f53\u5f62\u72b6\u548c\u8fd0\u52a8\uff0c\u5e76\u63d0\u4f9b\u76f4\u89c2\u53ef\u89c6\u5316\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u652f\u6301\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4ece\u5206\u5272\u548c\u5173\u952e\u70b9\u4e2d\u83b7\u53d6\u7684\u81ea\u52a8\u5316\u4eba\u4f53\u6d4b\u91cf\u3002\u5728\u672a\u89c1\u80ce\u513f\u5f62\u72b6\u4e0a\u6d4b\u8bd5\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u57283\u6beb\u7c73MRI\u4f53\u7d20\u4e0b\u7684\u8868\u9762\u5bf9\u9f50\u8bef\u5dee\u4e3a3.2\u6beb\u7c73\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a3D\u80ce\u513f\u7edf\u8ba1\u8eab\u4f53\u6a21\u578b\uff0c\u4e3a\u4ea7\u524d\u8bca\u65ad\u4e2d\u7684\u80ce\u513f\u8fd0\u52a8\u548c\u5f62\u72b6\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002\u4ee3\u7801\u5df2\u53d1\u5e03\u4e8ehttps://github.com/MedicalVisionGroup/fetal-smpl\u3002"}}
{"id": "2506.18783", "pdf": "https://arxiv.org/pdf/2506.18783", "abs": "https://arxiv.org/abs/2506.18783", "authors": ["Kamil Szczepanik", "Jaros\u0142aw A. Chudziak"], "title": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation", "categories": ["cs.AI", "cs.MA", "68T07", "I.2.11; I.2.7; I.2.8"], "comment": "12 pages, 10 figures, 2 tables, Accepted at the 17th International\n  Conference on Agents and Artificial Intelligence (ICAART 2025). Final version\n  published in Proceedings of ICAART 2025 (Vol. 1), pages 196-207", "summary": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684TRIZ\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u5177\u5907\u4e13\u4e1a\u80fd\u529b\u548c\u5de5\u5177\u8bbf\u95ee\u6743\u9650\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u53d1\u660e\u6027\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u521b\u65b0\u4efb\u52a1\u4e2d\u5206\u6563\u5f0f\u95ee\u9898\u89e3\u51b3\u7684\u4f18\u52bf\u3002", "motivation": "TRIZ\uff08\u53d1\u660e\u6027\u95ee\u9898\u89e3\u51b3\u7406\u8bba\uff09\u7684\u5e94\u7528\u5e38\u53d7\u9650\u4e8e\u5176\u590d\u6742\u6027\u548c\u8de8\u5b66\u79d1\u77e5\u8bc6\u9700\u6c42\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u5355\u4e00\u5927\u8bed\u8a00\u6a21\u578b\u5728TRIZ\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u62df\u53d1\u660e\u8fc7\u7a0b\uff0c\u63d0\u5347\u590d\u6742\u521b\u65b0\u95ee\u9898\u7684\u89e3\u51b3\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTRIZ agents\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5177\u5907\u4e13\u4e1a\u80fd\u529b\u548c\u5de5\u5177\u8bbf\u95ee\u6743\u9650\uff0c\u57fa\u4e8eTRIZ\u65b9\u6cd5\u8bba\u534f\u4f5c\u89e3\u51b3\u53d1\u660e\u6027\u95ee\u9898\u3002\u901a\u8fc7\u591a\u9886\u57df\u4e13\u5bb6\u667a\u80fd\u4f53\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u9ad8\u6548\u5b8c\u6210TRIZ\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u5de5\u7a0b\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u89e3\u51b3\u590d\u6742\u521b\u65b0\u6311\u6218\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u591f\u4ea7\u751f\u591a\u6837\u5316\u7684\u53d1\u660e\u6027\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aAI\u9a71\u52a8\u7684\u521b\u65b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u8bc1\u660e\u4e86\u5206\u6563\u5f0f\u95ee\u9898\u89e3\u51b3\u5728\u590d\u6742\u6784\u601d\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u521b\u65b0\u5de5\u5177\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "TRIZ\u667a\u80fd\u4f53\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7684TRIZ\u521b\u65b0\u65b9\u6cd5", "abstract_zh": "TRIZ\uff08\u53d1\u660e\u6027\u95ee\u9898\u89e3\u51b3\u7406\u8bba\uff09\u662f\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u62bd\u8c61\u95ee\u9898\u5e76\u5bfb\u627e\u53d1\u660e\u6027\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5176\u5e94\u7528\u5e38\u56e0\u590d\u6742\u6027\u548c\u8de8\u5b66\u79d1\u77e5\u8bc6\u9700\u6c42\u800c\u53d7\u9650\u3002\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\u4e3a\u81ea\u52a8\u5316\u90e8\u5206\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u5355\u4e00\u5927\u8bed\u8a00\u6a21\u578b\u5728TRIZ\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u79f0\u4e3aTRIZ\u667a\u80fd\u4f53\uff09\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5177\u5907\u4e13\u4e1a\u80fd\u529b\u548c\u5de5\u5177\u8bbf\u95ee\u6743\u9650\uff0c\u57fa\u4e8eTRIZ\u65b9\u6cd5\u8bba\u534f\u4f5c\u89e3\u51b3\u53d1\u660e\u6027\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u591a\u9886\u57df\u4e13\u5bb6\u667a\u80fd\u4f53\u9ad8\u6548\u5b8c\u6210TRIZ\u6b65\u9aa4\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u62df\u53d1\u660e\u8fc7\u7a0b\u3002\u6211\u4eec\u901a\u8fc7\u5de5\u7a0b\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u4e86\u667a\u80fd\u4f53\u56e2\u961f\u5728\u89e3\u51b3\u590d\u6742\u521b\u65b0\u6311\u6218\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u534f\u4f5c\u4ea7\u751f\u591a\u6837\u5316\u53d1\u660e\u6027\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002\u672c\u7814\u7a76\u4e3aAI\u9a71\u52a8\u7684\u521b\u65b0\u672a\u6765\u63d0\u4f9b\u4e86\u8d21\u732e\uff0c\u7a81\u663e\u4e86\u5206\u6563\u5f0f\u95ee\u9898\u89e3\u51b3\u5728\u590d\u6742\u6784\u601d\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.18141", "pdf": "https://arxiv.org/pdf/2506.18141", "abs": "https://arxiv.org/abs/2506.18141", "authors": ["Ruixuan Deng", "Xiaoyang Hu", "Miles Gilberti", "Shane Storks", "Aman Taxali", "Mike Angstadt", "Chandra Sripada", "Joyce Chai"], "title": "Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We identify semantically coherent, context-consistent network components in\nlarge language models (LLMs) using coactivation of sparse autoencoder (SAE)\nfeatures collected from just a handful of prompts. Focusing on country-relation\ntasks, we show that ablating semantic components for countries and relations\nchanges model outputs in predictable ways, while amplifying these components\ninduces counterfactual responses. Notably, composing relation and country\ncomponents yields compound counterfactual outputs. We find that, whereas most\ncountry components emerge from the very first layer, the more abstract relation\ncomponents are concentrated in later layers. Furthermore, within relation\ncomponents themselves, nodes from later layers tend to have a stronger causal\nimpact on model outputs. Overall, these findings suggest a modular organization\nof knowledge within LLMs and advance methods for efficient, targeted model\nmanipulation.", "AI": {"tldr": "\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5171\u6fc0\u6d3b\uff0c\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5b58\u5728\u8bed\u4e49\u6a21\u5757\u5316\u7ed3\u6784\uff0c\u56fd\u5bb6\u76f8\u5173\u7ec4\u4ef6\u5728\u65e9\u671f\u5c42\u51fa\u73b0\uff0c\u800c\u66f4\u62bd\u8c61\u7684\u5173\u7cfb\u7ec4\u4ef6\u96c6\u4e2d\u5728\u540e\u671f\u5c42\uff0c\u6a21\u5757\u5316\u77e5\u8bc6\u7ec4\u7ec7\u4e3a\u9ad8\u6548\u6a21\u578b\u64cd\u63a7\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8bed\u4e49\u77e5\u8bc6\u7684\u6a21\u5757\u5316\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u63ed\u793a\u5176\u5185\u90e8\u7ed3\u6784\u548c\u529f\u80fd\uff0c\u4e3a\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u6a21\u578b\u64cd\u63a7\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\u5171\u6fc0\u6d3b\u6280\u672f\uff0c\u5206\u6790\u5c11\u91cf\u63d0\u793a\u4e0bLLMs\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u91cd\u70b9\u5173\u6ce8\u56fd\u5bb6-\u5173\u7cfb\u4efb\u52a1\uff0c\u901a\u8fc7\u6d88\u878d\u548c\u653e\u5927\u7ec4\u4ef6\u89c2\u5bdf\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\u3002", "result": "\u56fd\u5bb6\u76f8\u5173\u7ec4\u4ef6\u4e3b\u8981\u51fa\u73b0\u5728\u65e9\u671f\u5c42\uff0c\u5173\u7cfb\u7ec4\u4ef6\u96c6\u4e2d\u5728\u540e\u671f\u5c42\uff1b\u540e\u671f\u5c42\u8282\u70b9\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u56e0\u679c\u5f71\u54cd\u66f4\u5f3a\uff1b\u7ec4\u5408\u56fd\u5bb6\u548c\u5173\u7cfb\u7ec4\u4ef6\u53ef\u751f\u6210\u590d\u5408\u53cd\u4e8b\u5b9e\u8f93\u51fa\u3002", "conclusion": "LLMs\u5185\u90e8\u5b58\u5728\u6a21\u5757\u5316\u7684\u8bed\u4e49\u77e5\u8bc6\u7ec4\u7ec7\uff0c\u56fd\u5bb6\u4e0e\u5173\u7cfb\u7ec4\u4ef6\u5206\u5c42\u5206\u5e03\uff0c\u540e\u671f\u5c42\u5bf9\u62bd\u8c61\u5173\u7cfb\u66f4\u5177\u5f71\u54cd\u529b\uff0c\u4e3a\u9ad8\u6548\u6a21\u578b\u64cd\u63a7\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u7a00\u758f\u7279\u5f81\u5171\u6fc0\u6d3b\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u7ec4\u5408\u8bed\u4e49\u6a21\u5757", "abstract_zh": "\u6211\u4eec\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\u7684\u5171\u6fc0\u6d3b\uff0c\u4ece\u5c11\u91cf\u63d0\u793a\u4e2d\u8bc6\u522b\u51fa\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8bed\u4e49\u8fde\u8d2f\u3001\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u7f51\u7edc\u7ec4\u4ef6\u3002\u4ee5\u56fd\u5bb6-\u5173\u7cfb\u4efb\u52a1\u4e3a\u4f8b\uff0c\u6d88\u878d\u56fd\u5bb6\u548c\u5173\u7cfb\u7684\u8bed\u4e49\u7ec4\u4ef6\u4f1a\u4ee5\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u6539\u53d8\u6a21\u578b\u8f93\u51fa\uff0c\u800c\u653e\u5927\u8fd9\u4e9b\u7ec4\u4ef6\u5219\u4f1a\u5f15\u53d1\u53cd\u4e8b\u5b9e\u54cd\u5e94\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7ec4\u5408\u5173\u7cfb\u548c\u56fd\u5bb6\u7684\u7ec4\u4ef6\u53ef\u4ea7\u751f\u590d\u5408\u53cd\u4e8b\u5b9e\u8f93\u51fa\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u591a\u6570\u56fd\u5bb6\u7ec4\u4ef6\u4ece\u7b2c\u4e00\u5c42\u5f00\u59cb\u51fa\u73b0\uff0c\u800c\u66f4\u62bd\u8c61\u7684\u5173\u7cfb\u7ec4\u4ef6\u5219\u96c6\u4e2d\u5728\u540e\u671f\u5c42\u3002\u6b64\u5916\uff0c\u5728\u5173\u7cfb\u7ec4\u4ef6\u5185\u90e8\uff0c\u540e\u671f\u5c42\u8282\u70b9\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u56e0\u679c\u5f71\u54cd\u66f4\u5f3a\u3002\u603b\u4f53\u800c\u8a00\uff0c\u8fd9\u4e9b\u53d1\u73b0\u8868\u660eLLMs\u5185\u90e8\u5b58\u5728\u6a21\u5757\u5316\u7684\u77e5\u8bc6\u7ec4\u7ec7\uff0c\u5e76\u63a8\u52a8\u4e86\u9ad8\u6548\u3001\u9488\u5bf9\u6027\u6a21\u578b\u64cd\u63a7\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.17869", "pdf": "https://arxiv.org/pdf/2506.17869", "abs": "https://arxiv.org/abs/2506.17869", "authors": ["Xiaodong Guo", "Zi'ang Lin", "Luwen Hu", "Zhihong Deng", "Tong Liu", "Wujie Zhou"], "title": "Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The integration of RGB and thermal data can significantly improve semantic\nsegmentation performance in wild environments for field robots. Nevertheless,\nmulti-source data processing (e.g. Transformer-based approaches) imposes\nsignificant computational overhead, presenting challenges for\nresource-constrained systems. To resolve this critical limitation, we\nintroduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture\nleveraging a cross-modal state space modeling (SSM) approach. Our framework\ncomprises two key components. First, we introduced a cross-modal\n2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal\nmodalities, which constructs cross-modal visual sequences and derives hidden\nstate representations of one modality from the other. Second, we developed a\ncross-modal state space association (CM-SSA) module that effectively integrates\nglobal associations from CM-SS2D with local spatial features extracted through\nconvolutional operations. In contrast with Transformer-based approaches, CM-SSM\nachieves linear computational complexity with respect to image resolution.\nExperimental results show that CM-SSM achieves state-of-the-art performance on\nthe CART dataset with fewer parameters and lower computational cost. Further\nexperiments on the PST900 dataset demonstrate its generalizability. Codes are\navailable at https://github.com/xiaodonguo/CMSSM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684RGB-\u70ed\u6210\u50cf\u8bed\u4e49\u5206\u5272\u67b6\u6784CM-SSM\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff08SSM\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u91ce\u5916\u573a\u666f\u4e0b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5728\u91ce\u5916\u73af\u5883\u4e2d\uff0cRGB\u548c\u70ed\u6210\u50cf\u6570\u636e\u7684\u878d\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff09\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7cfb\u7edf\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u9650\u5236\u3002", "method": "CM-SSM\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u8de8\u6a21\u60012D\u9009\u62e9\u6027\u626b\u63cf\uff08CM-SS2D\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u5efa\u7acbRGB\u548c\u70ed\u6210\u50cf\u6a21\u6001\u4e4b\u95f4\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff1b\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u5173\u8054\uff08CM-SSA\uff09\u6a21\u5757\uff0c\u5c06\u5168\u5c40\u5173\u8054\u4e0e\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u6709\u6548\u6574\u5408\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCM-SSM\u5728CART\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3001\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002\u5728PST900\u6570\u636e\u96c6\u4e0a\u7684\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CM-SSM\u901a\u8fc7\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-\u70ed\u6210\u50cf\u8bed\u4e49\u5206\u5272\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u7cfb\u7edf\u3002", "paper_title_zh": "\u57fa\u4e8e\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u7684\u5b9e\u65f6RGB-\u70ed\u6210\u50cf\u91ce\u5916\u573a\u666f\u8bed\u4e49\u5206\u5272", "abstract_zh": "RGB\u548c\u70ed\u6210\u50cf\u6570\u636e\u7684\u878d\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u91ce\u5916\u73af\u5883\u4e2d\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u4f46\u591a\u6e90\u6570\u636e\u5904\u7406\uff08\u5982\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff09\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u7cfb\u7edf\u63d0\u51fa\u4e86\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CM-SSM\uff0c\u4e00\u79cd\u9ad8\u6548\u7684RGB-\u70ed\u6210\u50cf\u8bed\u4e49\u5206\u5272\u67b6\u6784\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff08SSM\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8de8\u6a21\u60012D\u9009\u62e9\u6027\u626b\u63cf\uff08CM-SS2D\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u5728RGB\u548c\u70ed\u6210\u50cf\u6a21\u6001\u4e4b\u95f4\u5efa\u7acbSSM\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u89c6\u89c9\u5e8f\u5217\u5e76\u4ece\u4e00\u4e2a\u6a21\u6001\u4e2d\u63a8\u5bfc\u51fa\u53e6\u4e00\u4e2a\u6a21\u6001\u7684\u9690\u85cf\u72b6\u6001\u8868\u793a\uff1b\u5176\u6b21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u5173\u8054\uff08CM-SSA\uff09\u6a21\u5757\uff0c\u5c06CM-SS2D\u7684\u5168\u5c40\u5173\u8054\u4e0e\u901a\u8fc7\u5377\u79ef\u64cd\u4f5c\u63d0\u53d6\u7684\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u6709\u6548\u6574\u5408\u3002\u4e0e\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cCM-SSM\u5b9e\u73b0\u4e86\u4e0e\u56fe\u50cf\u5206\u8fa8\u7387\u76f8\u5173\u7684\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCM-SSM\u5728CART\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3001\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002\u5728PST900\u6570\u636e\u96c6\u4e0a\u7684\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u53ef\u5728https://github.com/xiaodonguo/CMSSM\u83b7\u53d6\u3002"}}
{"id": "2506.18810", "pdf": "https://arxiv.org/pdf/2506.18810", "abs": "https://arxiv.org/abs/2506.18810", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConciseHint\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u7b80\u6d01\u63d0\u793a\uff0c\u663e\u8457\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5197\u4f59\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982DeepSeek-R1\u548cOpenAI o1\u7cfb\u5217\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u503e\u5411\u4e8e\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u524d\u7684\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5e72\u9884\u4ee5\u63d0\u5347\u7b80\u6d01\u6027\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faConciseHint\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6ce8\u5165\u7b80\u6d01\u63d0\u793a\uff08\u624b\u52a8\u8bbe\u8ba1\u6216\u57fa\u4e8e\u7b80\u6d01\u6570\u636e\u8bad\u7ec3\uff09\uff0c\u5e76\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u63d0\u793a\u5f3a\u5ea6\uff0c\u786e\u4fdd\u6a21\u578b\u6027\u80fd\u4e0d\u53d7\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cConciseHint\u5728DeepSeek-R1\u548cQwen-3\u7cfb\u5217\u6a21\u578b\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u957f\u5ea6\uff08\u5982GSM8K\u57fa\u51c6\u4e0a\u51cf\u5c1165%\uff09\uff0c\u540c\u65f6\u51e0\u4e4e\u65e0\u51c6\u786e\u7387\u635f\u5931\u3002", "conclusion": "ConciseHint\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5197\u957f\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "ConciseHint\uff1a\u901a\u8fc7\u751f\u6210\u8fc7\u7a0b\u4e2d\u6301\u7eed\u7b80\u6d01\u63d0\u793a\u63d0\u5347\u9ad8\u6548\u63a8\u7406", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982DeepSeek-R1\u548cOpenAI o1\u7cfb\u5217\uff09\u901a\u8fc7\u6269\u5c55\u751f\u6210\u957f\u5ea6\uff08\u5982\u601d\u7ef4\u94feCoT\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6548\u7387\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u524d\u7684\u4f18\u5316\uff08\u5982\u63d0\u793a\u548c\u5fae\u8c03\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u9f13\u52b1\u6a21\u578b\u7b80\u6d01\u8868\u8fbe\u7684\u53ef\u80fd\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ConciseHint\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u4ee4\u724c\u751f\u6210\u4e2d\u6ce8\u5165\u7b80\u6d01\u63d0\u793a\uff08\u624b\u52a8\u8bbe\u8ba1\u6216\u57fa\u4e8e\u7b80\u6d01\u6570\u636e\u8bad\u7ec3\uff09\uff0c\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u63d0\u793a\u5f3a\u5ea6\u4ee5\u9002\u5e94\u67e5\u8be2\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u786e\u4fdd\u6a21\u578b\u6027\u80fd\u4e0d\u53d7\u5f71\u54cd\u3002\u5728DeepSeek-R1\u548cQwen-3\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u7b80\u6d01\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728GSM8K\u57fa\u51c6\u4e0a\uff0cQwen-3 4B\u6a21\u578b\u7684\u63a8\u7406\u957f\u5ea6\u51cf\u5c11\u4e8665%\uff0c\u4e14\u51e0\u4e4e\u65e0\u51c6\u786e\u7387\u635f\u5931\u3002"}}
{"id": "2506.18148", "pdf": "https://arxiv.org/pdf/2506.18148", "abs": "https://arxiv.org/abs/2506.18148", "authors": ["Diyam Akra", "Tymaa Hammouda", "Mustafa Jarrar"], "title": "QuranMorph: Morphologically Annotated Quranic Corpus", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present the QuranMorph corpus, a morphologically annotated corpus for the\nQuran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and\ntagged with its part-of-speech by three expert linguists. The lemmatization\nprocess utilized lemmas from Qabas, an Arabic lexicographic database linked\nwith 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging\nwas performed using the fine-grained SAMA/Qabas tagset, which encompasses 40\ntags. As shown in this paper, this rich lemmatization and POS tagset enabled\nthe QuranMorph corpus to be inter-linked with many linguistic resources. The\ncorpus is open-source and publicly available as part of the SinaLab resources\nat (https://sina.birzeit.edu/quran)", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86QuranMorph\u8bed\u6599\u5e93\uff0c\u8fd9\u662f\u4e00\u4e2a\u5bf9\u300a\u53e4\u5170\u7ecf\u300b\u8fdb\u884c\u5f62\u6001\u6807\u6ce8\u7684\u8bed\u6599\u5e93\uff0c\u5305\u542b77,429\u4e2a\u8bcd\u6761\uff0c\u6bcf\u4e2a\u8bcd\u6761\u7531\u4e09\u4f4d\u8bed\u8a00\u5b66\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\u8bcd\u6027\u548c\u8bcd\u6839\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u300a\u53e4\u5170\u7ecf\u300b\u5f62\u6001\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u652f\u6301\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u8d44\u6e90\u4e92\u8054\u3002", "method": "\u4f7f\u7528Qabas\u963f\u62c9\u4f2f\u8bed\u8bcd\u5178\u6570\u636e\u5e93\u8fdb\u884c\u8bcd\u6839\u6807\u6ce8\uff0c\u5e76\u91c7\u7528\u5305\u542b40\u4e2a\u6807\u7b7e\u7684SAMA/Qabas\u7ec6\u7c92\u5ea6\u8bcd\u6027\u6807\u6ce8\u96c6\u8fdb\u884c\u6807\u6ce8\u3002", "result": "QuranMorph\u8bed\u6599\u5e93\u6210\u529f\u5b9e\u73b0\u4e86\u4e0e\u591a\u79cd\u8bed\u8a00\u8d44\u6e90\u7684\u4e92\u8054\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u8d44\u6e90\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "QuranMorph\u8bed\u6599\u5e93\u4e3a\u300a\u53e4\u5170\u7ecf\u300b\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5f62\u6001\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u4fc3\u8fdb\u4e86\u8bed\u8a00\u5b66\u8d44\u6e90\u7684\u5171\u4eab\u4e0e\u4e92\u8054\u3002", "paper_title_zh": "QuranMorph\uff1a\u300a\u53e4\u5170\u7ecf\u300b\u5f62\u6001\u6807\u6ce8\u8bed\u6599\u5e93", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86QuranMorph\u8bed\u6599\u5e93\uff0c\u8fd9\u662f\u4e00\u4e2a\u5bf9\u300a\u53e4\u5170\u7ecf\u300b\u8fdb\u884c\u5f62\u6001\u6807\u6ce8\u7684\u8bed\u6599\u5e93\uff08\u5305\u542b77,429\u4e2a\u8bcd\u6761\uff09\u3002\u6bcf\u4e2a\u8bcd\u6761\u7531\u4e09\u4f4d\u8bed\u8a00\u5b66\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\u8bcd\u6839\u548c\u8bcd\u6027\u3002\u8bcd\u6839\u6807\u6ce8\u8fc7\u7a0b\u4f7f\u7528\u4e86Qabas\u963f\u62c9\u4f2f\u8bed\u8bcd\u5178\u6570\u636e\u5e93\uff0c\u8be5\u6570\u636e\u5e93\u94fe\u63a5\u4e86110\u4e2a\u8bcd\u5178\u548c\u5305\u542b200\u4e07\u8bcd\u6761\u7684\u8bed\u6599\u5e93\u3002\u8bcd\u6027\u6807\u6ce8\u91c7\u7528\u4e86\u7ec6\u7c92\u5ea6\u7684SAMA/Qabas\u6807\u6ce8\u96c6\uff0c\u5305\u542b40\u4e2a\u6807\u7b7e\u3002\u5982\u672c\u6587\u6240\u793a\uff0c\u8fd9\u79cd\u4e30\u5bcc\u7684\u8bcd\u6839\u548c\u8bcd\u6027\u6807\u6ce8\u4f7f\u5f97QuranMorph\u8bed\u6599\u5e93\u80fd\u591f\u4e0e\u591a\u79cd\u8bed\u8a00\u8d44\u6e90\u4e92\u8054\u3002\u8be5\u8bed\u6599\u5e93\u662f\u5f00\u6e90\u7684\uff0c\u4f5c\u4e3aSinaLab\u8d44\u6e90\u7684\u4e00\u90e8\u5206\u516c\u5f00\u63d0\u4f9b\uff08https://sina.birzeit.edu/quran\uff09\u3002"}}
{"id": "2506.17873", "pdf": "https://arxiv.org/pdf/2506.17873", "abs": "https://arxiv.org/abs/2506.17873", "authors": ["Guankun Wang", "Wenjin Mo", "Junyi Wang", "Long Bai", "Kun Yuan", "Ming Hu", "Jinlin Wu", "Junjun He", "Yiming Huang", "Nicolas Padoy", "Zhen Lei", "Hongbin Liu", "Nassir Navab", "Hongliang Ren"], "title": "SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models have demonstrated great\npotential in the medical domain, facilitating users to understand surgical\nscenes and procedures. Beyond image-based methods, the exploration of Video\nLarge Language Models (Vid-LLMs) has emerged as a promising avenue for\ncapturing the complex sequences of information involved in surgery. However,\nthere is still a lack of Vid-LLMs specialized for fine-grained surgical video\nunderstanding tasks, which is crucial for analyzing specific processes or\ndetails within a surgical procedure. To bridge this gap, we propose SurgVidLM,\nthe first video language model designed to address both full and fine-grained\nsurgical video comprehension. To train our SurgVidLM, we construct the SVU-31K\ndataset which consists of over 31K video-instruction pairs, enabling both\nholistic understanding and detailed analysis of surgical procedures.\nFurthermore, we introduce the StageFocus mechanism which is a two-stage\nframework performing the multi-grained, progressive understanding of surgical\nvideos. We also develop the Multi-frequency Fusion Attention to effectively\nintegrate low and high-frequency visual tokens, ensuring the retention of\ncritical information. Experimental results demonstrate that SurgVidLM\nsignificantly outperforms state-of-the-art Vid-LLMs in both full and\nfine-grained video understanding tasks, showcasing its superior capability in\ncapturing complex procedural contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSurgVidLM\uff0c\u9996\u4e2a\u4e13\u4e3a\u624b\u672f\u89c6\u9891\u591a\u7c92\u5ea6\u7406\u89e3\u8bbe\u8ba1\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7SVU-31K\u6570\u636e\u96c6\u548cStageFocus\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u5168\u89c6\u9891\u548c\u7ec6\u7c92\u5ea6\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u4e13\u7528\u4e8e\u7ec6\u7c92\u5ea6\u624b\u672f\u89c6\u9891\u7406\u89e3\u7684\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u624b\u672f\u573a\u666f\u548c\u8fc7\u7a0b\u7684\u8be6\u7ec6\u5206\u6790\u80fd\u529b\u3002", "method": "\u63d0\u51faSurgVidLM\u6a21\u578b\uff0c\u6784\u5efaSVU-31K\u6570\u636e\u96c6\uff08\u542b31K\u89c6\u9891-\u6307\u4ee4\u5bf9\uff09\uff0c\u5f15\u5165StageFocus\u673a\u5236\u5b9e\u73b0\u591a\u7c92\u5ea6\u6e10\u8fdb\u7406\u89e3\uff0c\u5e76\u5f00\u53d1Multi-frequency Fusion Attention\u4ee5\u878d\u5408\u9ad8\u4f4e\u9891\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSurgVidLM\u5728\u5168\u89c6\u9891\u548c\u7ec6\u7c92\u5ea6\u7406\u89e3\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u66f4\u6709\u6548\u6355\u6349\u590d\u6742\u624b\u672f\u8fc7\u7a0b\u3002", "conclusion": "SurgVidLM\u4e3a\u624b\u672f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u7c92\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u533b\u5b66\u89c6\u9891\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "SurgVidLM\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u7c92\u5ea6\u624b\u672f\u89c6\u9891\u7406\u89e3", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u624b\u672f\u573a\u666f\u548c\u6d41\u7a0b\u3002\u76f8\u8f83\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Vid-LLMs\uff09\u7684\u63a2\u7d22\u6210\u4e3a\u6355\u6349\u624b\u672f\u4e2d\u590d\u6742\u4fe1\u606f\u5e8f\u5217\u7684\u6709\u529b\u9014\u5f84\u3002\u7136\u800c\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u7ec6\u7c92\u5ea6\u624b\u672f\u89c6\u9891\u7406\u89e3\u7684Vid-LLMs\uff0c\u800c\u8fd9\u5bf9\u4e8e\u5206\u6790\u624b\u672f\u4e2d\u7684\u5177\u4f53\u8fc7\u7a0b\u6216\u7ec6\u8282\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faSurgVidLM\uff0c\u9996\u4e2a\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u5168\u89c6\u9891\u548c\u7ec6\u7c92\u5ea6\u624b\u672f\u89c6\u9891\u7406\u89e3\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u3002\u4e3a\u8bad\u7ec3SurgVidLM\uff0c\u6211\u4eec\u6784\u5efa\u4e86SVU-31K\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc731K\u89c6\u9891-\u6307\u4ee4\u5bf9\uff0c\u652f\u6301\u624b\u672f\u6d41\u7a0b\u7684\u6574\u4f53\u7406\u89e3\u548c\u8be6\u7ec6\u5206\u6790\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86StageFocus\u673a\u5236\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9\u624b\u672f\u89c6\u9891\u7684\u591a\u7c92\u5ea6\u6e10\u8fdb\u7406\u89e3\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86Multi-frequency Fusion Attention\uff0c\u6709\u6548\u878d\u5408\u4f4e\u9891\u548c\u9ad8\u9891\u89c6\u89c9\u6807\u8bb0\uff0c\u786e\u4fdd\u5173\u952e\u4fe1\u606f\u7684\u4fdd\u7559\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSurgVidLM\u5728\u5168\u89c6\u9891\u548c\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709Vid-LLMs\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u6355\u6349\u590d\u6742\u6d41\u7a0b\u4e0a\u4e0b\u6587\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u3002"}}
{"id": "2506.18887", "pdf": "https://arxiv.org/pdf/2506.18887", "abs": "https://arxiv.org/abs/2506.18887", "authors": ["Vansh Sharma", "Venkat Raman"], "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "I.2.7; I.2.6; I.2.1; D.3.3; C.4"], "comment": null, "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6fc0\u6d3b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u5b50\u7a7a\u95f4\u6765\u5f15\u5bfc\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u504f\u5411\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u4f18\u5316\u7684\u81ea\u9002\u5e94\u6fc0\u6d3b\u6846\u67b6\uff08G-ACT\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u7a0b\u8bed\u8a00\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6fc0\u6d3b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5bf9\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u4e2d\u7f16\u7a0b\u8bed\u8a00\u9009\u62e9\u7684\u63a7\u5236\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u9996\u5148\u8bc4\u4f30\u4e94\u79cd\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u57fa\u7ebf\u504f\u5dee\uff1b\u63d0\u51fa\u9759\u6001\u795e\u7ecf\u5143\u6270\u52a8\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u5c40\u9650\u6027\u540e\uff0c\u5f00\u53d1\u4e86\u68af\u5ea6\u4f18\u5316\u7684\u81ea\u9002\u5e94\u6fc0\u6d3b\u6846\u67b6\uff08G-ACT\uff09\uff0c\u901a\u8fc7\u805a\u7c7b\u6fc0\u6d3b\u5dee\u5f02\u548c\u5728\u7ebf\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u63a2\u9488\u6765\u9009\u62e9\u6700\u4f18\u6fc0\u6d3b\u65b9\u5411\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cG-ACT\u5728LLaMA-3.2 3B\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86CPP\u8bed\u8a00\u7684\u9009\u62e9\u51c6\u786e\u6027\uff08\u5e73\u5747\u63d0\u534715%\uff0c\u65e9\u671f\u5c42\u63d0\u534761.5%\uff09\uff0c\u5e76\u5728LLaMA-3.3 70B\u6a21\u578b\u4e2d\u901a\u8fc7\u5173\u952e\u5c42\u6ce8\u5165\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u8bed\u8a00\u9009\u62e9\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cG-ACT\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u8a00\u6a21\u578b\u6982\u5ff5\u7ea7\u522b\u7684\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4ee3\u7406\u7cfb\u7edf\u3002", "paper_title_zh": "\u901a\u8fc7Transformer\u6f5c\u5728\u5b50\u7a7a\u95f4\u6fc0\u6d3b\u5f15\u5bfc\u6982\u5ff5\u504f\u5dee", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6fc0\u6d3b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u5c06\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u5f15\u5bfc\u81f3\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00\u3002\u9996\u5148\u8bc4\u4f30\u4e86\u4e94\u79cd\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u4ee3\u7801\u63d0\u793a\u4e0a\u7684\u57fa\u7ebf\u504f\u5dee\uff0c\u91cf\u5316\u4e86\u5176\u5bf9\u56db\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u504f\u597d\u3002\u9759\u6001\u795e\u7ecf\u5143\u6270\u52a8\u65b9\u6cd5\uff08\u901a\u8fc7\u6270\u52a8C++\u6216CPP\u4ee4\u724c\u7684\u6700\u9ad8\u6fc0\u6d3bMLP\u6743\u91cd\uff09\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u63d0\u793a\u98ce\u683c\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u68af\u5ea6\u4f18\u5316\u7684\u81ea\u9002\u5e94\u6fc0\u6d3b\u6846\u67b6\uff08G-ACT\uff09\uff1a\u5c06\u6bcf\u63d0\u793a\u7684\u6fc0\u6d3b\u5dee\u5f02\u805a\u7c7b\u4e3a\u5c11\u91cf\u5f15\u5bfc\u65b9\u5411\uff0c\u5e76\u5728\u7ebf\u8bad\u7ec3\u548c\u4f18\u5316\u8f7b\u91cf\u7ea7\u5c42\u63a2\u9488\u4ee5\u9009\u62e9\u9002\u5f53\u7684\u5f15\u5bfc\u5411\u91cf\u3002\u5728LLaMA-3.2 3B\u4e2d\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u63a2\u9488\u5206\u7c7b\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad815%\uff0c\u5e76\u5728\u65e9\u671f\u5c42\uff080-6\uff09\u63d0\u534761.5%\uff0c\u53ef\u9760\u5730\u5c06\u751f\u6210\u504f\u5411CPP\u8bed\u8a00\u3002\u5bf9\u4e8eLLaMA-3.3 70B\uff0c\u6ce8\u610f\u529b\u4fe1\u53f7\u66f4\u5206\u6563\u65f6\uff0c\u5173\u952e\u5c42\u7684\u5b9a\u5411\u6ce8\u5165\u4ecd\u80fd\u6539\u5584\u8bed\u8a00\u9009\u62e9\u3002\u5c3d\u7ba1\u5c42\u63a2\u9488\u5f15\u5165\u4e86\u9002\u5ea6\u7684\u63a8\u7406\u5f00\u9500\uff0c\u4f46\u901a\u8fc7\u4ec5\u5f15\u5bfc\u90e8\u5206\u5c42\uff0c\u4fdd\u6301\u4e86\u5b9e\u7528\u6027\u5e76\u5b9e\u73b0\u4e86\u53ef\u590d\u73b0\u7684\u6a21\u578b\u884c\u4e3a\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u673a\u5236\uff0c\u4e3a\u5b9e\u9645\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6982\u5ff5\u7ea7\u522b\u7684\u63a7\u5236\u3002"}}
{"id": "2506.18185", "pdf": "https://arxiv.org/pdf/2506.18185", "abs": "https://arxiv.org/abs/2506.18185", "authors": ["Zihan Liang", "Ziwen Pan", "Sumon Kanti Dey", "Azra Ismail"], "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "In the Proceedings of the 10th Social Media Mining for Health and\n  Health Real-World Data Workshop and Shared Tasks, co-located with AAAI ICWSM\n  2025", "summary": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,\nspecifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).\nTask 4 focused on detecting mentions of insomnia in clinical notes, while Task\n5 addressed the extraction of food safety events from news articles. We\nparticipated in all subtasks and report key findings across them, with\nparticular emphasis on Task 5 Subtask 1, where our system achieved strong\nperformance-securing first place with an F1 score of 0.958 on the test set. To\nattain this result, we employed encoder-based models (e.g., RoBERTa), alongside\nGPT-4 for data augmentation. This paper outlines our approach, including\npreprocessing, model architecture, and subtask-specific adaptations", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CareLab\u56e2\u961f\u5728SMM4H-HeaRD 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u5931\u7720\u68c0\u6d4b\u548c\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\uff0c\u5e76\u5728\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u7684\u6210\u7ee9\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u9886\u57df\u611f\u77e5\u7684Transformer\u6a21\u578b\uff0c\u89e3\u51b3\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u5931\u7720\u68c0\u6d4b\u548c\u65b0\u95fb\u4e2d\u7684\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u63d0\u53d6\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u533b\u7597\u548c\u98df\u54c1\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e86\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u6a21\u578b\uff08\u5982RoBERTa\uff09\u548cGPT-4\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u7ed3\u5408\u9884\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u548c\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\uff0c\u4f18\u5316\u4e86\u4efb\u52a1\u8868\u73b0\u3002", "result": "\u5728\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e2d\uff0c\u7cfb\u7edf\u4ee5F1\u5206\u65700.958\u7684\u6210\u7ee9\u6392\u540d\u7b2c\u4e00\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u7684\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u9886\u57df\u611f\u77e5\u7684Transformer\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4e3a\u533b\u7597\u548c\u98df\u54c1\u5b89\u5168\u9886\u57df\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "paper_title_zh": "CareLab\u5728SMM4H-HeaRD 2025\uff1a\u57fa\u4e8e\u9886\u57df\u611f\u77e5Transformer\u7684\u5931\u7720\u68c0\u6d4b\u4e0e\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u63d0\u53d6", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6211\u4eec\u5728SMM4H-HeaRD 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u4efb\u52a14\uff08\u5b50\u4efb\u52a11\u30012a\u548c2b\uff09\u548c\u4efb\u52a15\uff08\u5b50\u4efb\u52a11\u548c2\uff09\u3002\u4efb\u52a14\u4e13\u6ce8\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u68c0\u6d4b\u5931\u7720\u63d0\u53ca\uff0c\u800c\u4efb\u52a15\u5219\u6d89\u53ca\u4ece\u65b0\u95fb\u6587\u7ae0\u4e2d\u63d0\u53d6\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u3002\u6211\u4eec\u53c2\u4e0e\u4e86\u6240\u6709\u5b50\u4efb\u52a1\uff0c\u5e76\u62a5\u544a\u4e86\u5173\u952e\u53d1\u73b0\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u4efb\u52a15\u5b50\u4efb\u52a11\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u8be5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee5\u6d4b\u8bd5\u96c6F1\u5206\u65700.958\u7684\u6210\u7ee9\u83b7\u5f97\u7b2c\u4e00\u540d\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u7ed3\u679c\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u6a21\u578b\uff08\u5982RoBERTa\uff09\u4ee5\u53caGPT-4\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u3002\u672c\u6587\u6982\u8ff0\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u9884\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\u3002"}}
{"id": "2506.17879", "pdf": "https://arxiv.org/pdf/2506.17879", "abs": "https://arxiv.org/abs/2506.17879", "authors": ["Zheng Chen"], "title": "StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The color appearance of a pathological image is highly related to the imaging\nprotocols, the proportion of different dyes, and the scanning devices.\nComputer-aided diagnostic systems may deteriorate when facing these\ncolor-variant pathological images. In this work, we propose a stain\nnormalization method called StainPIDR. We try to eliminate this color\ndiscrepancy by decoupling the image into structure features and\nvector-quantized color features, restaining the structure features with the\ntarget color features, and decoding the stained structure features to\nnormalized pathological images. We assume that color features decoupled by\ndifferent images with the same color should be exactly the same. Under this\nassumption, we train a fixed color vector codebook to which the decoupled color\nfeatures will map. In the restaining part, we utilize the cross-attention\nmechanism to efficiently stain the structure features. As the target color\n(decoupled from a selected template image) will also affect the performance of\nstain normalization, we further design a template image selection algorithm to\nselect a template from a given dataset. In our extensive experiments, we\nvalidate the effectiveness of StainPIDR and the template image selection\nalgorithm. All the results show that our method can perform well in the stain\nnormalization task. The code of StainPIDR will be publicly available later.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStainPIDR\u7684\u75c5\u7406\u56fe\u50cf\u67d3\u8272\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u56fe\u50cf\u7684\u7ed3\u6784\u7279\u5f81\u548c\u989c\u8272\u7279\u5f81\uff0c\u5e76\u5229\u7528\u5411\u91cf\u91cf\u5316\u548c\u7ed3\u6784\u91cd\u67d3\u8272\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u56fe\u50cf\u56e0\u67d3\u8272\u5dee\u5f02\u5bfc\u81f4\u7684\u8bca\u65ad\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u75c5\u7406\u56fe\u50cf\u7684\u989c\u8272\u5dee\u5f02\u4e3b\u8981\u7531\u6210\u50cf\u534f\u8bae\u3001\u67d3\u6599\u6bd4\u4f8b\u548c\u626b\u63cf\u8bbe\u5907\u7b49\u56e0\u7d20\u5f15\u8d77\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4f1a\u5f71\u54cd\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6d88\u9664\u989c\u8272\u5dee\u5f02\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "StainPIDR\u65b9\u6cd5\u5c06\u56fe\u50cf\u89e3\u8026\u4e3a\u7ed3\u6784\u7279\u5f81\u548c\u5411\u91cf\u91cf\u5316\u7684\u989c\u8272\u7279\u5f81\uff0c\u901a\u8fc7\u56fa\u5b9a\u989c\u8272\u5411\u91cf\u7801\u672c\u6620\u5c04\u989c\u8272\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u7ed3\u6784\u7279\u5f81\u8fdb\u884c\u91cd\u67d3\u8272\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u6a21\u677f\u56fe\u50cf\u9009\u62e9\u7b97\u6cd5\u4ee5\u4f18\u5316\u67d3\u8272\u5f52\u4e00\u5316\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86StainPIDR\u548c\u6a21\u677f\u56fe\u50cf\u9009\u62e9\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u67d3\u8272\u5f52\u4e00\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "StainPIDR\u80fd\u591f\u6709\u6548\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u989c\u8272\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5176\u4ee3\u7801\u5c06\u516c\u5f00\u4f9b\u540e\u7eed\u7814\u7a76\u4f7f\u7528\u3002", "paper_title_zh": "StainPIDR\uff1a\u57fa\u4e8e\u989c\u8272\u5411\u91cf\u91cf\u5316\u4e0e\u7ed3\u6784\u91cd\u67d3\u8272\u7684\u75c5\u7406\u56fe\u50cf\u89e3\u8026\u4e0e\u91cd\u5efa\u65b9\u6cd5", "abstract_zh": "\u75c5\u7406\u56fe\u50cf\u7684\u989c\u8272\u8868\u73b0\u4e0e\u6210\u50cf\u534f\u8bae\u3001\u67d3\u6599\u6bd4\u4f8b\u53ca\u626b\u63cf\u8bbe\u5907\u9ad8\u5ea6\u76f8\u5173\u3002\u8fd9\u4e9b\u989c\u8272\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStainPIDR\u7684\u67d3\u8272\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u89e3\u8026\u4e3a\u7ed3\u6784\u7279\u5f81\u548c\u5411\u91cf\u91cf\u5316\u7684\u989c\u8272\u7279\u5f81\uff0c\u5bf9\u7ed3\u6784\u7279\u5f81\u8fdb\u884c\u76ee\u6807\u989c\u8272\u91cd\u67d3\u8272\uff0c\u5e76\u89e3\u7801\u4e3a\u5f52\u4e00\u5316\u7684\u75c5\u7406\u56fe\u50cf\u3002\u5047\u8bbe\u76f8\u540c\u989c\u8272\u7684\u56fe\u50cf\u89e3\u8026\u51fa\u7684\u989c\u8272\u7279\u5f81\u5e94\u5b8c\u5168\u4e00\u81f4\uff0c\u56e0\u6b64\u8bad\u7ec3\u4e86\u4e00\u4e2a\u56fa\u5b9a\u989c\u8272\u5411\u91cf\u7801\u672c\u7528\u4e8e\u6620\u5c04\u989c\u8272\u7279\u5f81\u3002\u5728\u91cd\u67d3\u8272\u90e8\u5206\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u9ad8\u6548\u67d3\u8272\u7ed3\u6784\u7279\u5f81\u3002\u7531\u4e8e\u76ee\u6807\u989c\u8272\uff08\u4ece\u6a21\u677f\u56fe\u50cf\u89e3\u8026\uff09\u4e5f\u4f1a\u5f71\u54cd\u5f52\u4e00\u5316\u6548\u679c\uff0c\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u6a21\u677f\u56fe\u50cf\u9009\u62e9\u7b97\u6cd5\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86StainPIDR\u53ca\u6a21\u677f\u9009\u62e9\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u67d3\u8272\u5f52\u4e00\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002StainPIDR\u7684\u4ee3\u7801\u5c06\u540e\u7eed\u516c\u5f00\u3002"}}
{"id": "2506.18902", "pdf": "https://arxiv.org/pdf/2506.18902", "abs": "https://arxiv.org/abs/2506.18902", "authors": ["Michael G\u00fcnther", "Saba Sturua", "Mohammad Kalim Akram", "Isabelle Mohr", "Andrei Ungureanu", "Sedigheh Eslami", "Scott Martens", "Bo Wang", "Nan Wang", "Han Xiao"], "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T50", "I.2.7"], "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables", "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-based information retrieval, cross-modal semantic similarity,\nand programming code search. Comprehensive evaluations demonstrate that\njina-embeddings-v4 achieves state-of-the-art performance on both single- modal\nand cross-modal retrieval tasks, with particular strength in processing\nvisually rich content such as tables, charts, diagrams, and mixed-media\nformats. To facilitate evaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically designed for visually rich image\nretrieval.", "AI": {"tldr": "Jina-embeddings-v4 \u662f\u4e00\u4e2a 38 \u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u67b6\u6784\u7edf\u4e00\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\uff0c\u652f\u6301\u5355\u5411\u91cf\u548c\u591a\u5411\u91cf\u5d4c\u5165\uff0c\u5e76\u5728\u591a\u79cd\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u9700\u8981\u7edf\u4e00\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u8de8\u6a21\u6001\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u590d\u6742\u89c6\u89c9\u5185\u5bb9\u7684\u5904\u7406\u3002", "method": "\u6a21\u578b\u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u4f18\u5316\u6027\u80fd\uff0c\u652f\u6301\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\uff0c\u5e76\u5f15\u5165 Jina-VDR \u57fa\u51c6\u8bc4\u4f30\u89c6\u89c9\u4e30\u5bcc\u5185\u5bb9\u7684\u68c0\u7d22\u80fd\u529b\u3002", "result": "Jina-embeddings-v4 \u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u8868\u683c\u3001\u56fe\u8868\u548c\u6df7\u5408\u5a92\u4f53\u7b49\u89c6\u89c9\u4e30\u5bcc\u5185\u5bb9\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Jina-embeddings-v4 \u4e3a\u591a\u6a21\u6001\u591a\u8bed\u8a00\u68c0\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7 Jina-VDR \u57fa\u51c6\u63a8\u52a8\u4e86\u89c6\u89c9\u4e30\u5bcc\u5185\u5bb9\u68c0\u7d22\u7684\u7814\u7a76\u3002", "paper_title_zh": "Jina-embeddings-v4\uff1a\u7528\u4e8e\u591a\u6a21\u6001\u591a\u8bed\u8a00\u68c0\u7d22\u7684\u901a\u7528\u5d4c\u5165\u6a21\u578b", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86 Jina-embeddings-v4\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u6709 38 \u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\u7edf\u4e00\u4e86\u6587\u672c\u548c\u56fe\u50cf\u7684\u8868\u793a\uff0c\u652f\u6301\u5355\u5411\u91cf\u548c\u591a\u5411\u91cf\u5d4c\u5165\u7684\u540e\u671f\u4ea4\u4e92\u98ce\u683c\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\uff0c\u4ee5\u4f18\u5316\u591a\u79cd\u68c0\u7d22\u573a\u666f\u7684\u6027\u80fd\uff0c\u5305\u62ec\u57fa\u4e8e\u67e5\u8be2\u7684\u4fe1\u606f\u68c0\u7d22\u3001\u8de8\u6a21\u6001\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u7f16\u7a0b\u4ee3\u7801\u641c\u7d22\u3002\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cJina-embeddings-v4 \u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u89c6\u89c9\u4e30\u5bcc\u7684\u5185\u5bb9\uff0c\u5982\u8868\u683c\u3001\u56fe\u8868\u3001\u56fe\u89e3\u548c\u6df7\u5408\u5a92\u4f53\u683c\u5f0f\u3002\u4e3a\u4e86\u8bc4\u4f30\u8fd9\u4e00\u80fd\u529b\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86 Jina-VDR\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u89c6\u89c9\u4e30\u5bcc\u56fe\u50cf\u68c0\u7d22\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199", "abs": "https://arxiv.org/abs/2506.18199", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u6587\u5316\u504f\u89c1\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u603b\u7ed3\u4e86\u4e94\u79cd\u4e3b\u8981\u65b9\u6cd5\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u591a\u6b65\u6d41\u7a0b\u6548\u679c\u6700\u4f73\uff0c\u4f46\u9700\u66f4\u9ad8\u6280\u672f\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9488\u5bf9\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u7684\u504f\u89c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u7814\u7a76\u6709\u6548\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u4ee5\u51cf\u5c11\u8fd9\u79cd\u6587\u5316\u504f\u89c1\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7cfb\u7edf\u7efc\u8ff0\uff0c\u9075\u5faaPRISMA\u6307\u5357\u548cKitchenham\u7684\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u4e862021-2024\u5e74\u95f4\u76848\u9879\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e94\u79cd\u4e3b\u8981\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff1a\u6587\u5316\u63d0\u793a\u3001\u60c5\u611f\u542f\u52a8\u3001\u81ea\u53bb\u504f\u6280\u672f\u3001\u7ed3\u6784\u5316\u591a\u6b65\u6d41\u7a0b\u548c\u53c2\u6570\u4f18\u5316\u8fde\u7eed\u63d0\u793a\u3002\u7ed3\u6784\u5316\u591a\u6b65\u6d41\u7a0b\u6548\u679c\u6700\u4f73\uff0c\u504f\u89c1\u51cf\u5c11\u9ad8\u8fbe87.7%\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u53ef\u6709\u6548\u51cf\u5c11\u6587\u5316\u504f\u89c1\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u9002\u5e94\u6027\u63d0\u793a\u6280\u672f\u3001\u7279\u5b9a\u8bc4\u4f30\u8d44\u6e90\uff0c\u5e76\u7ed3\u5408\u5176\u4ed6\u53bb\u504f\u65b9\u6cd5\u3002", "paper_title_zh": "\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u6587\u5316\u504f\u89c1\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff1a\u7cfb\u7edf\u7efc\u8ff0", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\uff0c\u4f46\u5bf9\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u7684\u6587\u5316\u504f\u89c1\u95ee\u9898\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u4f26\u7406\u6311\u6218\uff0c\u52a9\u957f\u4e86\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\u548c\u8fb9\u7f18\u5316\u3002\u5c3d\u7ba1\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u8ba4\u8bc6\u9010\u6e10\u52a0\u6df1\uff0c\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u7fa4\u4f53\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u7814\u7a76\u4ecd\u663e\u4e0d\u8db3\u3002\u672c\u6df7\u5408\u65b9\u6cd5\u7cfb\u7edf\u7efc\u8ff0\u63a2\u8ba8\u4e86\u6b64\u7c7b\u6280\u672f\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\u3002\u9075\u5faaPRISMA\u6307\u5357\u548cKitchenham\u7684\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6211\u4eec\u5206\u6790\u4e862021-2024\u5e74\u95f4\u53d1\u8868\u76848\u9879\u5173\u4e8e\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u7814\u7a76\u53d1\u73b0\u4e94\u79cd\u4e3b\u8981\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff1a\u6587\u5316\u63d0\u793a\u3001\u60c5\u611f\u542f\u52a8\u3001\u81ea\u53bb\u504f\u6280\u672f\u3001\u7ed3\u6784\u5316\u591a\u6b65\u6d41\u7a0b\u548c\u53c2\u6570\u4f18\u5316\u8fde\u7eed\u63d0\u793a\u3002\u5c3d\u7ba1\u6240\u6709\u65b9\u6cd5\u5747\u663e\u793a\u51fa\u51cf\u5c11\u504f\u89c1\u7684\u6f5c\u529b\uff0c\u4f46\u6548\u679c\u56e0\u7814\u7a76\u548c\u504f\u89c1\u7c7b\u578b\u5dee\u5f02\u663e\u8457\u3002\u8bc1\u636e\u8868\u660e\uff0c\u67d0\u4e9b\u504f\u89c1\u7c7b\u578b\u53ef\u80fd\u5bf9\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u66f4\u5177\u62b5\u6297\u529b\u3002\u7ed3\u6784\u5316\u591a\u6b65\u6d41\u7a0b\u603b\u4f53\u6548\u679c\u6700\u4f73\uff0c\u504f\u89c1\u51cf\u5c11\u9ad8\u8fbe87.7%\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u7684\u6280\u672f\u80fd\u529b\u3002\u6587\u5316\u63d0\u793a\u5219\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u663e\u8457\u6548\u679c\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u5728\u4e0d\u9700\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u51cf\u8f7b\u6587\u5316\u504f\u89c1\u7684\u53ef\u884c\u6027\u3002\u5df2\u8bc6\u522b\u7684\u7814\u7a76\u6570\u91cf\u6709\u9650\uff0c\u51f8\u663e\u4e86\u8fd9\u4e00\u5173\u952e\u9886\u57df\u7684\u663e\u8457\u7814\u7a76\u7a7a\u767d\u3002\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u5f00\u53d1\u6587\u5316\u9002\u5e94\u6027\u63d0\u793a\u6280\u672f\u3001\u521b\u5efa\u9488\u5bf9\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u7684\u8bc4\u4f30\u8d44\u6e90\uff0c\u5e76\u5c06\u63d0\u793a\u5de5\u7a0b\u4e0e\u5176\u4ed6\u53bb\u504f\u65b9\u6cd5\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u66f4\u6df1\u5c42\u6b21\u7684\u523b\u677f\u5370\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.17885", "pdf": "https://arxiv.org/pdf/2506.17885", "abs": "https://arxiv.org/abs/2506.17885", "authors": ["Trong-An Bui", "Thanh-Thoai Le"], "title": "Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Cloud contamination significantly impairs the usability of optical satellite\nimagery, affecting critical applications such as environmental monitoring,\ndisaster response, and land-use analysis. This research presents a\nCloud-Attentive Reconstruction Framework that integrates SAR-optical feature\nfusion with deep learning-based image reconstruction to generate cloud-free\noptical imagery. The proposed framework employs an attention-driven feature\nfusion mechanism to align complementary structural information from Synthetic\nAperture Radar (SAR) with spectral characteristics from optical data.\nFurthermore, a cloud-aware model update strategy introduces adaptive loss\nweighting to prioritize cloud-occluded regions, enhancing reconstruction\naccuracy. Experimental results demonstrate that the proposed method outperforms\nexisting approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of\n0.017. These outcomes highlight the framework's effectiveness in producing\nhigh-fidelity, spatially and spectrally consistent cloud-free optical images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SAR-\u5149\u5b66\u7279\u5f81\u878d\u5408\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u4e91\u611f\u77e5\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u65e0\u4e91\u5149\u5b66\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4e91\u6c61\u67d3\u4e25\u91cd\u5f71\u54cd\u4e86\u5149\u5b66\u536b\u661f\u56fe\u50cf\u7684\u53ef\u7528\u6027\uff0c\u9650\u5236\u4e86\u73af\u5883\u76d1\u6d4b\u3001\u707e\u5bb3\u54cd\u5e94\u548c\u571f\u5730\u5229\u7528\u5206\u6790\u7b49\u5173\u952e\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6d88\u9664\u4e91\u5c42\u5e72\u6270\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u65e0\u4e91\u56fe\u50cf\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e91\u611f\u77e5\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u5c06\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u7684\u7ed3\u6784\u4fe1\u606f\u4e0e\u5149\u5b66\u6570\u636e\u7684\u5149\u8c31\u7279\u5f81\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u7b56\u7565\uff0c\u4f18\u5148\u5904\u7406\u4e91\u906e\u6321\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cPSNR\u8fbe\u523031.01 dB\uff0cSSIM\u4e3a0.918\uff0cMAE\u4e3a0.017\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u7a7a\u95f4\u548c\u5149\u8c31\u4e00\u81f4\u7684\u65e0\u4e91\u5149\u5b66\u56fe\u50cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6d88\u9664\u4e91\u6c61\u67d3\u3001\u63d0\u5347\u5149\u5b66\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u536b\u661f\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u9762\u5411\u7a7a\u95f4\u4efb\u52a1\u7684\u4e91\u611f\u77e5SAR\u878d\u5408\u589e\u5f3a\u5149\u5b66\u4f20\u611f", "abstract_zh": "\u4e91\u6c61\u67d3\u663e\u8457\u964d\u4f4e\u4e86\u5149\u5b66\u536b\u661f\u56fe\u50cf\u7684\u53ef\u7528\u6027\uff0c\u5f71\u54cd\u4e86\u73af\u5883\u76d1\u6d4b\u3001\u707e\u5bb3\u54cd\u5e94\u548c\u571f\u5730\u5229\u7528\u5206\u6790\u7b49\u5173\u952e\u5e94\u7528\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e91\u611f\u77e5\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408SAR-\u5149\u5b66\u7279\u5f81\u878d\u5408\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u91cd\u5efa\u6280\u672f\uff0c\u751f\u6210\u65e0\u4e91\u5149\u5b66\u56fe\u50cf\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u5c06\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u7684\u4e92\u8865\u7ed3\u6784\u4fe1\u606f\u4e0e\u5149\u5b66\u6570\u636e\u7684\u5149\u8c31\u7279\u5f81\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u4e91\u611f\u77e5\u6a21\u578b\u66f4\u65b0\u7b56\u7565\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\uff0c\u4f18\u5148\u5904\u7406\u4e91\u906e\u6321\u533a\u57df\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u8fbe\u523031.01 dB\uff0cSSIM\u4e3a0.918\uff0cMAE\u4e3a0.017\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u7a7a\u95f4\u548c\u5149\u8c31\u4e00\u81f4\u7684\u65e0\u4e91\u5149\u5b66\u56fe\u50cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.17230", "pdf": "https://arxiv.org/pdf/2506.17230", "abs": "https://arxiv.org/abs/2506.17230", "authors": ["Yichen Luo", "Jia Wang", "Dapeng Lan", "Yu Liu", "Zhibo Pang"], "title": "MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial Differential Equations (PDEs) are fundamental for modeling physical\nsystems, yet solving them in a generic and efficient manner using machine\nlearning-based approaches remains challenging due to limited multi-input and\nmulti-scale generalization capabilities, as well as high computational costs.\nThis paper proposes the Multi-input and Multi-scale Efficient Transformer\n(MMET), a novel framework designed to address the above challenges. MMET\ndecouples mesh and query points as two sequences and feeds them into the\nencoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)\nlayer to embed input variables or functions with varying dimensions, enabling\neffective solutions for multi-scale and multi-input problems. Additionally, a\nHilbert curve-based reserialization and patch embedding mechanism decrease the\ninput length. This significantly reduces the computational cost when dealing\nwith large-scale geometric models. These innovations enable efficient\nrepresentations and support multi-scale resolution queries for large-scale and\nmulti-input PDE problems. Experimental evaluations on diverse benchmarks\nspanning different physical fields demonstrate that MMET outperforms SOTA\nmethods in both accuracy and computational efficiency. This work highlights the\npotential of MMET as a robust and scalable solution for real-time PDE solving\nin engineering and physics-based applications, paving the way for future\nexplorations into pre-trained large-scale models in specific domains. This work\nis open-sourced at https://github.com/YichenLuo-0/MMET.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMET\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7f51\u683c\u548c\u67e5\u8be2\u70b9\u5e8f\u5217\uff0c\u7ed3\u5408\u95e8\u63a7\u6761\u4ef6\u5d4c\u5165\u5c42\u548c\u57fa\u4e8eHilbert\u66f2\u7ebf\u7684\u91cd\u5e8f\u5217\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6c42\u89e3\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u5728\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MMET\u6846\u67b6\u5c06\u7f51\u683c\u548c\u67e5\u8be2\u70b9\u4f5c\u4e3a\u4e24\u4e2a\u5e8f\u5217\u5206\u522b\u8f93\u5165\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u4f7f\u7528\u95e8\u63a7\u6761\u4ef6\u5d4c\u5165\uff08GCE\uff09\u5c42\u5904\u7406\u4e0d\u540c\u7ef4\u5ea6\u7684\u8f93\u5165\u53d8\u91cf\u6216\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eHilbert\u66f2\u7ebf\u7684\u91cd\u5e8f\u5217\u5316\u548c\u8865\u4e01\u5d4c\u5165\u673a\u5236\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u591a\u4e2a\u7269\u7406\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMMET\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5927\u89c4\u6a21\u548c\u591a\u8f93\u5165PDE\u95ee\u9898\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MMET\u4e3a\u5de5\u7a0b\u548c\u7269\u7406\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6PDE\u6c42\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u7279\u5b9a\u9886\u57df\u9884\u8bad\u7ec3\u5927\u89c4\u6a21\u6a21\u578b\u7684\u672a\u6765\u63a2\u7d22\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "MMET\uff1a\u4e00\u79cd\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u7684\u9ad8\u6548PDE\u6c42\u89e3Transformer", "abstract_zh": "\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u662f\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u7684\u57fa\u7840\uff0c\u4f46\u7531\u4e8e\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9ad8\u6548\u4e14\u901a\u7528\u5730\u6c42\u89e3PDE\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u9ad8\u6548Transformer\uff08MMET\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\u3002MMET\u5c06\u7f51\u683c\u548c\u67e5\u8be2\u70b9\u89e3\u8026\u4e3a\u4e24\u4e2a\u5e8f\u5217\uff0c\u5206\u522b\u8f93\u5165\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5e76\u4f7f\u7528\u95e8\u63a7\u6761\u4ef6\u5d4c\u5165\uff08GCE\uff09\u5c42\u5d4c\u5165\u4e0d\u540c\u7ef4\u5ea6\u7684\u8f93\u5165\u53d8\u91cf\u6216\u51fd\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u591a\u5c3a\u5ea6\u591a\u8f93\u5165\u95ee\u9898\u7684\u6709\u6548\u6c42\u89e3\u3002\u6b64\u5916\uff0c\u57fa\u4e8eHilbert\u66f2\u7ebf\u7684\u91cd\u5e8f\u5217\u5316\u548c\u8865\u4e01\u5d4c\u5165\u673a\u5236\u51cf\u5c11\u4e86\u8f93\u5165\u957f\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5904\u7406\u5927\u89c4\u6a21\u51e0\u4f55\u6a21\u578b\u65f6\u7684\u8ba1\u7b97\u6210\u672c\u3002\u8fd9\u4e9b\u521b\u65b0\u5b9e\u73b0\u4e86\u9ad8\u6548\u8868\u793a\uff0c\u5e76\u652f\u6301\u5927\u89c4\u6a21\u548c\u591a\u8f93\u5165PDE\u95ee\u9898\u7684\u591a\u5c3a\u5ea6\u5206\u8fa8\u7387\u67e5\u8be2\u3002\u5728\u6db5\u76d6\u4e0d\u540c\u7269\u7406\u9886\u57df\u7684\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660eMMET\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u7a81\u51fa\u4e86MMET\u4f5c\u4e3a\u5de5\u7a0b\u548c\u7269\u7406\u5e94\u7528\u4e2d\u5b9e\u65f6PDE\u6c42\u89e3\u7684\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u4e3a\u7279\u5b9a\u9886\u57df\u9884\u8bad\u7ec3\u5927\u89c4\u6a21\u6a21\u578b\u7684\u672a\u6765\u63a2\u7d22\u94fa\u5e73\u4e86\u9053\u8def\u3002\u672c\u5de5\u4f5c\u5df2\u5728https://github.com/YichenLuo-0/MMET\u5f00\u6e90\u3002"}}
{"id": "2506.18201", "pdf": "https://arxiv.org/pdf/2506.18201", "abs": "https://arxiv.org/abs/2506.18201", "authors": ["Bushra Asseri", "Estabraq Abdelaziz", "Maha Al Mogren", "Tayef Alhefdhi", "Areej Al-Wabil"], "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications", "categories": ["cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86GPT-4o\u548cGemini 1.5 Pro\u4e24\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u513f\u7ae5\u6545\u4e8b\u4e66\u63d2\u56fe\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0GPT-4o\u5728\u6240\u6709\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8eGemini\uff0c\u4f46\u5176\u6587\u5316\u7406\u89e3\u4ecd\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u6559\u80b2\u5de5\u5177\u9700\u8981\u6587\u5316\u654f\u611f\u7684\u60c5\u611f\u8bc6\u522b\u6280\u672f\uff0c\u4f46\u76ee\u524d\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u6b64\u9886\u57df\u7684\u8868\u73b0\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u6a21\u578b\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u752875\u5f20\u963f\u62c9\u4f2f\u6545\u4e8b\u4e66\u63d2\u56fe\uff0c\u5e76\u4e0e\u57fa\u4e8ePlutchik\u60c5\u611f\u6846\u67b6\u7684\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "GPT-4o\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u7684\u5b8fF1\u5206\u6570\u8fbe59%\uff0c\u800cGemini\u6700\u9ad8\u4e3a43%\u3002\u6a21\u578b\u5728\u6587\u5316\u7ec6\u5fae\u60c5\u611f\u548c\u6a21\u7cca\u53d9\u4e8b\u573a\u666f\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u8bef\u5206\u7c7b\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u66f4\u5177\u6587\u5316\u654f\u611f\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u652f\u6301\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u8005\u7684\u60c5\u611f\u611f\u77e5\u6559\u80b2\u6280\u672f\u3002", "paper_title_zh": "\u89e3\u8bfb\u513f\u7ae5\u6545\u4e8b\u4e66\u4e2d\u7684\u60c5\u611f\uff1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u6bd4\u8f83\u5206\u6790", "abstract_zh": "\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\u5bf9\u5f00\u53d1\u6587\u5316\u654f\u611f\u7684\u6559\u80b2\u6280\u672f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u963f\u62c9\u4f2f\u8bed\u80cc\u666f\u4e0b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u548cGemini 1.5 Pro\uff09\u5728\u5904\u7406\u963f\u62c9\u4f2f\u513f\u7ae5\u6545\u4e8b\u4e66\u63d2\u56fe\u65f6\u7684\u60c5\u611f\u8bc6\u522b\u8868\u73b0\u3002\u6211\u4eec\u4f7f\u752875\u5f20\u6765\u81ea\u4e03\u672c\u963f\u62c9\u4f2f\u6545\u4e8b\u4e66\u7684\u56fe\u50cf\uff0c\u901a\u8fc7\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\uff09\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u9884\u6d4b\u4e0e\u57fa\u4e8ePlutchik\u60c5\u611f\u6846\u67b6\u7684\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u6bd4\u8f83\u3002GPT-4o\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8eGemini\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u7684\u5b8fF1\u5206\u6570\u6700\u9ad8\u4e3a59%\uff0c\u800cGemini\u7684\u6700\u4f73\u8868\u73b0\u4e3a43%\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u5b58\u5728\u7cfb\u7edf\u6027\u8bef\u5206\u7c7b\u6a21\u5f0f\uff0c\u5176\u4e2d\u60c5\u611f\u6781\u6027\u53cd\u8f6c\u536060.7%\u7684\u9519\u8bef\uff0c\u4e14\u4e24\u79cd\u6a21\u578b\u5728\u6587\u5316\u7ec6\u5fae\u60c5\u611f\u548c\u6a21\u7cca\u53d9\u4e8b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u4e0a\u7684\u6839\u672c\u5c40\u9650\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u91c7\u7528\u66f4\u5177\u6587\u5316\u654f\u611f\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u5f00\u53d1\u9002\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u8005\u7684\u60c5\u611f\u611f\u77e5\u6559\u80b2\u6280\u672f\u3002"}}
{"id": "2506.17891", "pdf": "https://arxiv.org/pdf/2506.17891", "abs": "https://arxiv.org/abs/2506.17891", "authors": ["Jiahao Lu", "Jiacheng Deng"], "title": "Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Code:\n  https://github.com/Howard-coder191/Relation3D", "summary": "3D instance segmentation aims to predict a set of object instances in a\nscene, representing them as binary foreground masks with corresponding semantic\nlabels. Currently, transformer-based methods are gaining increasing attention\ndue to their elegant pipelines and superior predictions. However, these methods\nprimarily focus on modeling the external relationships between scene features\nand query features through mask attention. They lack effective modeling of the\ninternal relationships among scene features as well as between query features.\nIn light of these disadvantages, we propose \\textbf{Relation3D: Enhancing\nRelation Modeling for Point Cloud Instance Segmentation}. Specifically, we\nintroduce an adaptive superpoint aggregation module and a contrastive\nlearning-guided superpoint refinement module to better represent superpoint\nfeatures (scene features) and leverage contrastive learning to guide the\nupdates of these features. Furthermore, our relation-aware self-attention\nmechanism enhances the capabilities of modeling relationships between queries\nby incorporating positional and geometric relationships into the self-attention\nmechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and\nS3DIS datasets demonstrate the superior performance of Relation3D.", "AI": {"tldr": "Relation3D\u901a\u8fc7\u589e\u5f3a\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u5173\u7cfb\u5efa\u6a21\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u8d85\u70b9\u805a\u5408\u6a21\u5757\u548c\u5bf9\u6bd4\u5b66\u4e60\u5f15\u5bfc\u7684\u8d85\u70b9\u4f18\u5316\u6a21\u5757\uff0c\u7ed3\u5408\u5173\u7cfb\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u573a\u666f\u7279\u5f81\u4e0e\u67e5\u8be2\u7279\u5f81\u7684\u5916\u90e8\u5173\u7cfb\uff0c\u800c\u5ffd\u7565\u4e86\u573a\u666f\u7279\u5f81\u5185\u90e8\u53ca\u67e5\u8be2\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u5efa\u6a21\u3002Relation3D\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8d85\u70b9\u805a\u5408\u6a21\u5757\u548c\u5bf9\u6bd4\u5b66\u4e60\u5f15\u5bfc\u7684\u8d85\u70b9\u4f18\u5316\u6a21\u5757\uff0c\u4f18\u5316\u8d85\u70b9\u7279\u5f81\u8868\u793a\uff1b\u5f15\u5165\u5173\u7cfb\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u4f4d\u7f6e\u548c\u51e0\u4f55\u5173\u7cfb\u589e\u5f3a\u67e5\u8be2\u7279\u5f81\u95f4\u7684\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5728ScanNetV2\u3001ScanNet++\u3001ScanNet200\u548cS3DIS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRelation3D\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "Relation3D\u901a\u8fc7\u6539\u8fdb\u5173\u7cfb\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "Relation3D\uff1a\u589e\u5f3a\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u5173\u7cfb\u5efa\u6a21", "abstract_zh": "3D\u5b9e\u4f8b\u5206\u5272\u65e8\u5728\u9884\u6d4b\u573a\u666f\u4e2d\u7684\u4e00\u7ec4\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u8868\u793a\u4e3a\u5e26\u6709\u76f8\u5e94\u8bed\u4e49\u6807\u7b7e\u7684\u4e8c\u8fdb\u5236\u524d\u666f\u63a9\u7801\u3002\u76ee\u524d\uff0c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u56e0\u5176\u4f18\u96c5\u7684\u6d41\u7a0b\u548c\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u63a9\u7801\u6ce8\u610f\u529b\u5efa\u6a21\u573a\u666f\u7279\u5f81\u4e0e\u67e5\u8be2\u7279\u5f81\u7684\u5916\u90e8\u5173\u7cfb\uff0c\u7f3a\u4e4f\u5bf9\u573a\u666f\u7279\u5f81\u5185\u90e8\u53ca\u67e5\u8be2\u7279\u5f81\u4e4b\u95f4\u5173\u7cfb\u7684\u6709\u6548\u5efa\u6a21\u3002\u9488\u5bf9\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\\textbf{Relation3D\uff1a\u589e\u5f3a\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u5173\u7cfb\u5efa\u6a21}\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u8d85\u70b9\u805a\u5408\u6a21\u5757\u548c\u5bf9\u6bd4\u5b66\u4e60\u5f15\u5bfc\u7684\u8d85\u70b9\u4f18\u5316\u6a21\u5757\uff0c\u4ee5\u66f4\u597d\u5730\u8868\u793a\u8d85\u70b9\u7279\u5f81\uff08\u573a\u666f\u7279\u5f81\uff09\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6307\u5bfc\u8fd9\u4e9b\u7279\u5f81\u7684\u66f4\u65b0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5173\u7cfb\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u5c06\u4f4d\u7f6e\u548c\u51e0\u4f55\u5173\u7cfb\u878d\u5165\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u67e5\u8be2\u7279\u5f81\u95f4\u7684\u5173\u7cfb\u5efa\u6a21\u80fd\u529b\u3002\u5728ScanNetV2\u3001ScanNet++\u3001ScanNet200\u548cS3DIS\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86Relation3D\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.17232", "pdf": "https://arxiv.org/pdf/2506.17232", "abs": "https://arxiv.org/abs/2506.17232", "authors": ["Zelin Zang", "Fei Wang", "Liangyu Li", "Jinlin Wu", "Chunshui Zhao", "Zhen Lei", "Baigui Sun"], "title": "PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Recent UDA methods based\non Vision Transformers (ViTs) have achieved strong performance through\nattention-based feature alignment. However, we identify a key limitation:\nforeground object mismatch, where the discrepancy in foreground object size and\nspatial distribution across domains weakens attention consistency and hampers\neffective domain alignment. To address this issue, we propose the Progressive\nFocus Cross-Attention Mechanism (PCaM), which progressively filters out\nbackground information during cross-attention, allowing the model to focus on\nand fuse discriminative foreground semantics across domains. We further\nintroduce an attentional guidance loss that explicitly directs attention toward\ntask-relevant regions, enhancing cross-domain attention consistency. PCaM is\nlightweight, architecture-agnostic, and easy to integrate into existing\nViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,\nVisDA-2017, and remote sensing datasets demonstrate that PCaM significantly\nimproves adaptation performance and achieves new state-of-the-art results,\nvalidating the effectiveness of attention-guided foreground fusion for domain\nadaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCaM\u7684\u6e10\u8fdb\u5f0f\u805a\u7126\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u89c6\u89c9Transformer\uff08ViT\uff09\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65b9\u6cd5\u3002\u901a\u8fc7\u9010\u6b65\u8fc7\u6ee4\u80cc\u666f\u4fe1\u606f\u5e76\u805a\u7126\u4e8e\u524d\u666f\u8bed\u4e49\uff0cPCaM\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u6ce8\u610f\u529b\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u57df\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eViT\u7684UDA\u65b9\u6cd5\u5728\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u65f6\u5b58\u5728\u524d\u666f\u5bf9\u8c61\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5373\u524d\u666f\u5bf9\u8c61\u7684\u5927\u5c0f\u548c\u7a7a\u95f4\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u51cf\u5f31\uff0c\u5f71\u54cd\u4e86\u57df\u9002\u5e94\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u805a\u7126\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff08PCaM\uff09\uff0c\u9010\u6b65\u8fc7\u6ee4\u80cc\u666f\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u8de8\u57df\u7684\u5224\u522b\u6027\u524d\u666f\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u5f15\u5bfc\u635f\u5931\uff0c\u660e\u786e\u5c06\u6ce8\u610f\u529b\u5bfc\u5411\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u589e\u5f3a\u8de8\u57df\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u3002", "result": "\u5728Office-Home\u3001DomainNet\u3001VisDA-2017\u548c\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCaM\u663e\u8457\u63d0\u5347\u4e86\u57df\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "PCaM\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u524d\u666f\u8bed\u4e49\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u524d\u666f\u5bf9\u8c61\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8eViT\u7684UDA\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "PCaM\uff1a\u4e00\u79cd\u57fa\u4e8e\u6e10\u8fdb\u5f0f\u805a\u7126\u6ce8\u610f\u529b\u7684\u4fe1\u606f\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u89c6\u89c9Transformer\u7684\u57df\u9002\u5e94", "abstract_zh": "\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65e8\u5728\u5c06\u77e5\u8bc6\u4ece\u6709\u6807\u6ce8\u7684\u6e90\u57df\u8fc1\u79fb\u5230\u65e0\u6807\u6ce8\u7684\u76ee\u6807\u57df\u3002\u8fd1\u671f\u57fa\u4e8e\u89c6\u89c9Transformer\uff08ViT\uff09\u7684UDA\u65b9\u6cd5\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u5bf9\u9f50\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff1a\u524d\u666f\u5bf9\u8c61\u4e0d\u5339\u914d\uff0c\u5373\u8de8\u57df\u524d\u666f\u5bf9\u8c61\u5927\u5c0f\u548c\u7a7a\u95f4\u5206\u5e03\u7684\u5dee\u5f02\u524a\u5f31\u4e86\u6ce8\u610f\u529b\u4e00\u81f4\u6027\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u57df\u5bf9\u9f50\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6e10\u8fdb\u5f0f\u805a\u7126\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff08PCaM\uff09\uff0c\u5728\u4ea4\u53c9\u6ce8\u610f\u529b\u8fc7\u7a0b\u4e2d\u9010\u6b65\u8fc7\u6ee4\u80cc\u666f\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u5e76\u878d\u5408\u8de8\u57df\u7684\u5224\u522b\u6027\u524d\u666f\u8bed\u4e49\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u5f15\u5bfc\u635f\u5931\uff0c\u660e\u786e\u5c06\u6ce8\u610f\u529b\u5bfc\u5411\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u589e\u5f3a\u8de8\u57df\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u3002PCaM\u8f7b\u91cf\u7ea7\u3001\u67b6\u6784\u65e0\u5173\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7684\u57fa\u4e8eViT\u7684UDA\u6d41\u7a0b\u4e2d\u3002\u5728Office-Home\u3001DomainNet\u3001VisDA-2017\u548c\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPCaM\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u524d\u666f\u878d\u5408\u5bf9\u57df\u9002\u5e94\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18318", "pdf": "https://arxiv.org/pdf/2506.18318", "abs": "https://arxiv.org/abs/2506.18318", "authors": ["An Trieu", "Phuong Nguyen", "Minh Le Nguyen"], "title": "Enhancing Entity Aware Machine Translation with Multi-task Learning", "categories": ["cs.CL"], "comment": "In the Proceedings of SCIDOCA 2025", "summary": "Entity-aware machine translation (EAMT) is a complicated task in natural\nlanguage processing due to not only the shortage of translation data related to\nthe entities needed to translate but also the complexity in the context needed\nto process while translating those entities. In this paper, we propose a method\nthat applies multi-task learning to optimize the performance of the two\nsubtasks named entity recognition and machine translation, which improves the\nfinal performance of the Entity-aware machine translation task. The result and\nanalysis are performed on the dataset provided by the organizer of Task 2 of\nthe SemEval 2025 competition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u63d0\u5347\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\uff08EAMT\uff09\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u57fa\u4e8eSemEval 2025\u7ade\u8d5b\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\uff08EAMT\uff09\u56e0\u7f3a\u4e4f\u76f8\u5173\u7ffb\u8bd1\u6570\u636e\u53ca\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u800c\u6210\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u96be\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u4ee5\u63d0\u5347EAMT\u7684\u6574\u4f53\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u5728SemEval 2025\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u663e\u793a\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86EAMT\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4e0a\u4e0b\u6587\u4e2d\u7684\u5b9e\u4f53\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u4f18\u5316", "abstract_zh": "\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\uff08EAMT\uff09\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u9879\u590d\u6742\u4efb\u52a1\uff0c\u4e0d\u4ec5\u56e0\u4e3a\u7f3a\u4e4f\u4e0e\u5b9e\u4f53\u76f8\u5173\u7684\u7ffb\u8bd1\u6570\u636e\uff0c\u8fd8\u56e0\u4e3a\u7ffb\u8bd1\u8fd9\u4e9b\u5b9e\u4f53\u65f6\u9700\u8981\u5904\u7406\u7684\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u4f18\u5316\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u5347\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u7684\u6700\u7ec8\u6027\u80fd\u3002\u5b9e\u9a8c\u548c\u5206\u6790\u57fa\u4e8eSemEval 2025\u7ade\u8d5b\u4efb\u52a12\u7ec4\u7ec7\u8005\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2506.17892", "pdf": "https://arxiv.org/pdf/2506.17892", "abs": "https://arxiv.org/abs/2506.17892", "authors": ["Jianghong Huang", "Luping Ji", "Xin Ma", "Mao Ye"], "title": "BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages, 10 figures", "summary": "Conveyor belt is a category of important equipments in modern industry,\nwidely applied in production and manufacturing Fields. Its health status is\nmuch critical to operation efficiency and safety hazards. Among the factors\naffecting belt health, crack is often one of the most threatening risks.\nCurrently, considering safety, how to intelligently detect belt cracks is\ncatching an increasing attention. To implement the intelligent detection with\nmachine learning, real crack samples are believed to be necessary. However,\nexisting crack datasets primarily focus on pavement scenarios or synthetic\ndata, no real-world industrial belt crack datasets at all. To propel machine\nlearning advancement in this field, this paper constructs the first\nsequential-image belt crack detection datasets (BeltCrack14ks and\nBeltCrack9kd), from real-world factory scenes. Furthermore, to validate\nusability and effectiveness, we propose a special baseline method with\ntriple-domain (i.e., time-space-frequency) feature hierarchical fusion learning\nfor the two whole-new datasets. Experimental results demonstrate the\navailability and effectiveness of our dataset. Besides, they also show that our\nbaseline is obviously superior to other similar detection methods. Our datasets\nand source codes are available at https://github.com/UESTC-nnLab/BeltCrack.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u7684\u4f20\u9001\u5e26\u88c2\u7eb9\u68c0\u6d4b\u6570\u636e\u96c6\uff08BeltCrack14ks\u548cBeltCrack9kd\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u9891\u4e09\u57df\u7279\u5f81\u5206\u5c42\u878d\u5408\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u5176\u4ed6\u7c7b\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u4f20\u9001\u5e26\u662f\u73b0\u4ee3\u5de5\u4e1a\u4e2d\u7684\u91cd\u8981\u8bbe\u5907\uff0c\u5176\u88c2\u7eb9\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u8fd0\u884c\u6548\u7387\u548c\u5b89\u5168\u3002\u73b0\u6709\u88c2\u7eb9\u6570\u636e\u96c6\u591a\u4e3a\u8def\u9762\u573a\u666f\u6216\u5408\u6210\u6570\u636e\uff0c\u7f3a\u4e4f\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u7684\u4f20\u9001\u5e26\u88c2\u7eb9\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u9891\u4e09\u57df\u7279\u5f81\u5206\u5c42\u878d\u5408\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6784\u5efa\u7684\u6570\u636e\u96c6\u5177\u6709\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u4e14\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u7c7b\u4f3c\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4f20\u9001\u5e26\u88c2\u7eb9\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "paper_title_zh": "BeltCrack\uff1a\u9996\u4e2a\u5e8f\u5217\u56fe\u50cf\u5de5\u4e1a\u4f20\u9001\u5e26\u88c2\u7eb9\u68c0\u6d4b\u6570\u636e\u96c6\u53ca\u5176\u57fa\u4e8e\u4e09\u57df\u7279\u5f81\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5", "abstract_zh": "\u4f20\u9001\u5e26\u662f\u73b0\u4ee3\u5de5\u4e1a\u4e2d\u7684\u91cd\u8981\u8bbe\u5907\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u751f\u4ea7\u548c\u5236\u9020\u9886\u57df\u3002\u5176\u5065\u5eb7\u72b6\u51b5\u5bf9\u8fd0\u884c\u6548\u7387\u548c\u5b89\u5168\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u5728\u5f71\u54cd\u4f20\u9001\u5e26\u5065\u5eb7\u7684\u56e0\u7d20\u4e2d\uff0c\u88c2\u7eb9\u5f80\u5f80\u662f\u6700\u5177\u5a01\u80c1\u7684\u98ce\u9669\u4e4b\u4e00\u3002\u76ee\u524d\uff0c\u51fa\u4e8e\u5b89\u5168\u8003\u8651\uff0c\u5982\u4f55\u667a\u80fd\u68c0\u6d4b\u4f20\u9001\u5e26\u88c2\u7eb9\u6b63\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u4e3a\u5b9e\u73b0\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u667a\u80fd\u68c0\u6d4b\uff0c\u771f\u5b9e\u7684\u88c2\u7eb9\u6837\u672c\u88ab\u8ba4\u4e3a\u662f\u5fc5\u8981\u7684\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u88c2\u7eb9\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u8def\u9762\u573a\u666f\u6216\u5408\u6210\u6570\u636e\uff0c\u5b8c\u5168\u6ca1\u6709\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u7684\u4f20\u9001\u5e26\u88c2\u7eb9\u6570\u636e\u96c6\u3002\u4e3a\u63a8\u52a8\u8be5\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u53d1\u5c55\uff0c\u672c\u6587\u4ece\u771f\u5b9e\u5de5\u5382\u573a\u666f\u4e2d\u6784\u5efa\u4e86\u9996\u4e2a\u5e8f\u5217\u56fe\u50cf\u4f20\u9001\u5e26\u88c2\u7eb9\u68c0\u6d4b\u6570\u636e\u96c6\uff08BeltCrack14ks\u548cBeltCrack9kd\uff09\u3002\u6b64\u5916\uff0c\u4e3a\u9a8c\u8bc1\u5176\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u6b8a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u91c7\u7528\u65f6\u7a7a\u9891\u4e09\u57df\u7279\u5f81\u5206\u5c42\u878d\u5408\u5b66\u4e60\uff0c\u7528\u4e8e\u8fd9\u4e24\u4e2a\u5168\u65b0\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6570\u636e\u96c6\u5177\u6709\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u6211\u4eec\u7684\u57fa\u7ebf\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u7c7b\u4f3c\u68c0\u6d4b\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u53ef\u5728https://github.com/UESTC-nnLab/BeltCrack\u83b7\u53d6\u3002"}}
{"id": "2506.17234", "pdf": "https://arxiv.org/pdf/2506.17234", "abs": "https://arxiv.org/abs/2506.17234", "authors": ["Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey", "categories": ["cs.LG", "cs.AI"], "comment": "51 pages", "summary": "The task of data integration for multi-omics data has emerged as a powerful\nstrategy to unravel the complex biological underpinnings of cancer. Recent\nadvancements in graph neural networks (GNNs) offer an effective framework to\nmodel heterogeneous and structured omics data, enabling precise representation\nof molecular interactions and regulatory networks. This systematic review\nexplores several recent studies that leverage GNN-based architectures in\nmulti-omics cancer research. We classify the approaches based on their targeted\nomics layers, graph neural network structures, and biological tasks such as\nsubtype classification, prognosis prediction, and biomarker discovery. The\nanalysis reveals a growing trend toward hybrid and interpretable models,\nalongside increasing adoption of attention mechanisms and contrastive learning.\nFurthermore, we highlight the use of patient-specific graphs and\nknowledge-driven priors as emerging directions. This survey serves as a\ncomprehensive resource for researchers aiming to design effective GNN-based\npipelines for integrative cancer analysis, offering insights into current\npractices, limitations, and potential future directions.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u591a\u7ec4\u5b66\u764c\u75c7\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u7c7b\u4e86\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u8d8b\u52bf\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u591a\u7ec4\u5b66\u6570\u636e\u6574\u5408\u662f\u89e3\u6790\u764c\u75c7\u590d\u6742\u751f\u7269\u5b66\u673a\u5236\u7684\u6709\u6548\u7b56\u7565\uff0c\u800c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4e3a\u5efa\u6a21\u5f02\u8d28\u6027\u548c\u7ed3\u6784\u5316\u7ec4\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406GNN\u5728\u591a\u7ec4\u5b66\u764c\u75c7\u7814\u7a76\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u591a\u7ec4\u5b66\u764c\u75c7\u7814\u7a76\u4e2d\u57fa\u4e8eGNN\u7684\u67b6\u6784\uff0c\u5206\u7c7b\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u76ee\u6807\u7ec4\u5b66\u5c42\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u53ca\u751f\u7269\u4efb\u52a1\uff08\u5982\u4e9a\u578b\u5206\u7c7b\u3001\u9884\u540e\u9884\u6d4b\u548c\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\uff09\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5f53\u524d\u8d8b\u52bf\u503e\u5411\u4e8e\u6df7\u5408\u548c\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u540c\u65f6\u60a3\u8005\u7279\u5f02\u6027\u56fe\u548c\u77e5\u8bc6\u9a71\u52a8\u5148\u9a8c\u6210\u4e3a\u65b0\u5174\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bbe\u8ba1\u57fa\u4e8eGNN\u7684\u591a\u7ec4\u5b66\u764c\u75c7\u5206\u6790\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5168\u9762\u8d44\u6e90\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u5b9e\u8df5\u3001\u5c40\u9650\u6027\u548c\u6f5c\u5728\u672a\u6765\u65b9\u5411\u3002", "paper_title_zh": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u7ec4\u5b66\u764c\u75c7\u7814\u7a76\u4e2d\u7684\u7ed3\u6784\u5316\u7efc\u8ff0", "abstract_zh": "\u591a\u7ec4\u5b66\u6570\u636e\u6574\u5408\u5df2\u6210\u4e3a\u63ed\u793a\u764c\u75c7\u590d\u6742\u751f\u7269\u5b66\u57fa\u7840\u7684\u6709\u529b\u7b56\u7565\u3002\u8fd1\u5e74\u6765\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8fdb\u5c55\u4e3a\u5efa\u6a21\u5f02\u8d28\u6027\u548c\u7ed3\u6784\u5316\u7ec4\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u80fd\u591f\u7cbe\u786e\u8868\u5f81\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u548c\u8c03\u63a7\u7f51\u7edc\u3002\u672c\u7cfb\u7edf\u7efc\u8ff0\u63a2\u8ba8\u4e86\u591a\u7ec4\u5b66\u764c\u75c7\u7814\u7a76\u4e2d\u57fa\u4e8eGNN\u67b6\u6784\u7684\u82e5\u5e72\u6700\u65b0\u7814\u7a76\u3002\u6211\u4eec\u6839\u636e\u76ee\u6807\u7ec4\u5b66\u5c42\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u53ca\u751f\u7269\u4efb\u52a1\uff08\u5982\u4e9a\u578b\u5206\u7c7b\u3001\u9884\u540e\u9884\u6d4b\u548c\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\uff09\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u8d8b\u52bf\u503e\u5411\u4e8e\u6df7\u5408\u548c\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u60a3\u8005\u7279\u5f02\u6027\u56fe\u548c\u77e5\u8bc6\u9a71\u52a8\u5148\u9a8c\u4f5c\u4e3a\u65b0\u5174\u65b9\u5411\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u57fa\u4e8eGNN\u7684\u591a\u7ec4\u5b66\u764c\u75c7\u5206\u6790\u6d41\u7a0b\u63d0\u4f9b\u5168\u9762\u8d44\u6e90\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u5f53\u524d\u5b9e\u8df5\u3001\u5c40\u9650\u6027\u548c\u6f5c\u5728\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2506.18337", "pdf": "https://arxiv.org/pdf/2506.18337", "abs": "https://arxiv.org/abs/2506.18337", "authors": ["Syed Mekael Wasti", "Shou-Yi Hung", "Christopher Collins", "En-Shiun Annie Lee"], "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.", "AI": {"tldr": "TranslationCorrect\u662f\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u673a\u5668\u7ffb\u8bd1\u751f\u6210\u3001\u9519\u8bef\u9884\u6d4b\u548c\u76f4\u89c2\u7684\u540e\u7f16\u8f91\u754c\u9762\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6548\u7387\u548c\u7814\u7a76\u6570\u636e\u6536\u96c6\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u540e\u7f16\u8f91\u548c\u7814\u7a76\u6570\u636e\u6536\u96c6\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u4e14\u5206\u6563\uff0c\u4e9f\u9700\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "TranslationCorrect\u6574\u5408\u4e86NLLB\u7b49\u6a21\u578b\u7684\u673a\u5668\u7ffb\u8bd1\u751f\u6210\u3001XCOMET\u6216LLM API\u7684\u9519\u8bef\u9884\u6d4b\uff08\u542b\u8be6\u7ec6\u63a8\u7406\uff09\u4ee5\u53ca\u7b26\u5408\u4eba\u673a\u4ea4\u4e92\u539f\u5219\u7684\u540e\u7f16\u8f91\u754c\u9762\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cTranslationCorrect\u663e\u8457\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u6548\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5e76\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u57fa\u4e8e\u8de8\u5ea6\u7684\u9519\u8bef\u6807\u6ce8\u3002", "conclusion": "TranslationCorrect\u4e3a\u7ffb\u8bd1\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u5de5\u5177\uff0c\u4f18\u5316\u4e86\u673a\u5668\u7ffb\u8bd1\u540e\u7f16\u8f91\u548c\u7814\u7a76\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u3002", "paper_title_zh": "TranslationCorrect\uff1a\u57fa\u4e8e\u9884\u6d4b\u9519\u8bef\u8f85\u52a9\u7684\u673a\u5668\u7ffb\u8bd1\u540e\u7f16\u8f91\u7edf\u4e00\u6846\u67b6", "abstract_zh": "\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u540e\u7f16\u8f91\u548c\u7814\u7a76\u6570\u636e\u6536\u96c6\u901a\u5e38\u4f9d\u8d56\u4e8e\u4f4e\u6548\u4e14\u5206\u6563\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86TranslationCorrect\uff0c\u4e00\u4e2a\u65e8\u5728\u6574\u5408\u8fd9\u4e9b\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6\u3002TranslationCorrect\u7ed3\u5408\u4e86\u4f7f\u7528NLLB\u7b49\u6a21\u578b\u7684\u673a\u5668\u7ffb\u8bd1\u751f\u6210\u3001\u57fa\u4e8eXCOMET\u6216LLM API\uff08\u63d0\u4f9b\u8be6\u7ec6\u63a8\u7406\uff09\u7684\u81ea\u52a8\u9519\u8bef\u9884\u6d4b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7b26\u5408\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u539f\u5219\u7684\u76f4\u89c2\u540e\u7f16\u8f91\u754c\u9762\u3002\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u3002\u5bf9\u4e8e\u7ffb\u8bd1\u8005\uff0c\u5b83\u652f\u6301\u9ad8\u6548\u7ea0\u9519\u548c\u6279\u91cf\u7ffb\u8bd1\uff1b\u5bf9\u4e8e\u7814\u7a76\u8005\uff0cTranslationCorrect\u4ee5\u9519\u8bef\u8de8\u5ea6\u6807\u6ce8\uff08ESA\uff09\u683c\u5f0f\u5bfc\u51fa\u9ad8\u8d28\u91cf\u7684\u57fa\u4e8e\u8de8\u5ea6\u7684\u6807\u6ce8\uff0c\u5176\u9519\u8bef\u5206\u7c7b\u53d7\u591a\u7ef4\u8d28\u91cf\u6307\u6807\uff08MQM\uff09\u542f\u53d1\u3002\u8fd9\u4e9b\u8f93\u51fa\u517c\u5bb9\u524d\u6cbf\u7684\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3MT\u6216\u540e\u7f16\u8f91\u7cfb\u7edf\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cTranslationCorrect\u5728\u7ffb\u8bd1\u6548\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6807\u6ce8\u65b9\u6cd5\u3002"}}
{"id": "2506.17896", "pdf": "https://arxiv.org/pdf/2506.17896", "abs": "https://arxiv.org/abs/2506.17896", "authors": ["Junho Park", "Andrew Sangwoo Ye", "Taein Kwon"], "title": "EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://redorangeyellowy.github.io/EgoWorld/", "summary": "Egocentric vision is essential for both human and machine visual\nunderstanding, particularly in capturing the detailed hand-object interactions\nneeded for manipulation tasks. Translating third-person views into first-person\nviews significantly benefits augmented reality (AR), virtual reality (VR) and\nrobotics applications. However, current exocentric-to-egocentric translation\nmethods are limited by their dependence on 2D cues, synchronized multi-view\nsettings, and unrealistic assumptions such as necessity of initial egocentric\nframe and relative camera poses during inference. To overcome these challenges,\nwe introduce EgoWorld, a novel two-stage framework that reconstructs an\negocentric view from rich exocentric observations, including projected point\nclouds, 3D hand poses, and textual descriptions. Our approach reconstructs a\npoint cloud from estimated exocentric depth maps, reprojects it into the\negocentric perspective, and then applies diffusion-based inpainting to produce\ndense, semantically coherent egocentric images. Evaluated on the H2O and TACO\ndatasets, EgoWorld achieves state-of-the-art performance and demonstrates\nrobust generalization to new objects, actions, scenes, and subjects. Moreover,\nEgoWorld shows promising results even on unlabeled real-world examples.", "AI": {"tldr": "EgoWorld\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u7b2c\u4e09\u4eba\u79f0\u89c2\u5bdf\uff08\u5982\u70b9\u4e91\u30013D\u624b\u90e8\u59ff\u6001\u548c\u6587\u672c\u63cf\u8ff0\uff09\u91cd\u5efa\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u7ebf\u7d22\u548c\u591a\u89c6\u89d2\u540c\u6b65\u7684\u9650\u5236\uff0c\u5e76\u5728H2O\u548cTACO\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5728\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u3001\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u7b2c\u4e09\u4eba\u79f0\u5230\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8f6c\u6362\u65b9\u6cd5\u4f9d\u8d562D\u7ebf\u7d22\u3001\u591a\u89c6\u89d2\u540c\u6b65\u8bbe\u7f6e\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff08\u5982\u521d\u59cb\u7b2c\u4e00\u4eba\u79f0\u5e27\u548c\u76f8\u673a\u59ff\u6001\uff09\u3002EgoWorld\u65e8\u5728\u901a\u8fc7\u4e30\u5bcc\u7684\u7b2c\u4e09\u4eba\u79f0\u89c2\u5bdf\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "EgoWorld\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u4ece\u4f30\u8ba1\u7684\u7b2c\u4e09\u4eba\u79f0\u6df1\u5ea6\u56fe\u91cd\u5efa\u70b9\u4e91\uff0c\u5e76\u5c06\u5176\u91cd\u6295\u5f71\u5230\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff1b\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6280\u672f\u751f\u6210\u5bc6\u96c6\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u3002", "result": "\u5728H2O\u548cTACO\u6570\u636e\u96c6\u4e0a\uff0cEgoWorld\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u65b0\u7269\u4f53\u3001\u52a8\u4f5c\u3001\u573a\u666f\u548c\u4e3b\u4f53\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u672a\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\u4e5f\u663e\u793a\u51fa\u826f\u597d\u7684\u6548\u679c\u3002", "conclusion": "EgoWorld\u901a\u8fc7\u4e30\u5bcc\u7684\u7b2c\u4e09\u4eba\u79f0\u89c2\u5bdf\u6210\u529f\u91cd\u5efa\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e3aAR\u3001VR\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "EgoWorld\uff1a\u5229\u7528\u4e30\u5bcc\u7684\u7b2c\u4e09\u4eba\u79f0\u89c2\u5bdf\u5c06\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u8f6c\u6362\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2", "abstract_zh": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u7684\u89c6\u89c9\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u6355\u6349\u624b-\u7269\u4f53\u4ea4\u4e92\u7ec6\u8282\u4ee5\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\u65f6\u3002\u5c06\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u8f6c\u6362\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5bf9\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u3001\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u548c\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7b2c\u4e09\u4eba\u79f0\u5230\u7b2c\u4e00\u4eba\u79f0\u8f6c\u6362\u65b9\u6cd5\u53d7\u9650\u4e8e\u5bf92D\u7ebf\u7d22\u3001\u540c\u6b65\u591a\u89c6\u89d2\u8bbe\u7f6e\u4ee5\u53ca\u4e0d\u5207\u5b9e\u9645\u5047\u8bbe\uff08\u5982\u63a8\u65ad\u65f6\u9700\u8981\u521d\u59cb\u7b2c\u4e00\u4eba\u79f0\u5e27\u548c\u76f8\u673a\u59ff\u6001\uff09\u7684\u4f9d\u8d56\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86EgoWorld\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u7b2c\u4e09\u4eba\u79f0\u89c2\u5bdf\uff08\u5305\u62ec\u6295\u5f71\u70b9\u4e91\u30013D\u624b\u90e8\u59ff\u6001\u548c\u6587\u672c\u63cf\u8ff0\uff09\u91cd\u5efa\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u4ece\u4f30\u8ba1\u7684\u7b2c\u4e09\u4eba\u79f0\u6df1\u5ea6\u56fe\u91cd\u5efa\u70b9\u4e91\uff0c\u5e76\u5c06\u5176\u91cd\u6295\u5f71\u5230\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff0c\u7136\u540e\u5e94\u7528\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6280\u672f\u751f\u6210\u5bc6\u96c6\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u3002\u5728H2O\u548cTACO\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cEgoWorld\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u65b0\u7269\u4f53\u3001\u52a8\u4f5c\u3001\u573a\u666f\u548c\u4e3b\u4f53\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0cEgoWorld\u5728\u672a\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\u4e5f\u663e\u793a\u51fa\u826f\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2506.17247", "pdf": "https://arxiv.org/pdf/2506.17247", "abs": "https://arxiv.org/abs/2506.17247", "authors": ["Andrew B. Kahng", "Yiting Liu", "Zhiang Wang"], "title": "Recursive Learning-Based Virtual Buffering for Analytical Global Placement", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the skewed scaling of interconnect versus cell delay in modern\ntechnology nodes, placement with buffer porosity (i.e., cell density) awareness\nis essential for timing closure in physical synthesis flows. However, existing\napproaches face two key challenges: (i) traditional van Ginneken-Lillis-style\nbuffering approaches are computationally expensive during global placement; and\n(ii) machine learning-based approaches, such as BufFormer, lack a thorough\nconsideration of Electrical Rule Check (ERC) violations and fail to \"close the\nloop\" back into the physical design flow. In this work, we propose\nMLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware\nanalytical global placement framework, built on top of the OpenROAD\ninfrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based\ngenerative buffering approach to predict buffer types and locations, addressing\nERC violations during global placement. We compare MLBuf-RePlAce against the\ndefault virtual buffering-based timing-driven global placer in OpenROAD, using\nopen-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts\nrepositories. Without degradation of post-route power, MLBuf-RePlAce achieves\n(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)\nwithin the open-source OpenROAD flow. When evaluated by completion in a\ncommercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of\n(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLBuf-RePlAce\u7684\u5f00\u6e90\u5b66\u4e60\u9a71\u52a8\u865a\u62df\u7f13\u51b2\u611f\u77e5\u5168\u5c40\u5e03\u5c40\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5b66\u4e60\u751f\u6210\u7f13\u51b2\u7c7b\u578b\u548c\u4f4d\u7f6e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6ERC\u8fdd\u89c4\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u6027\u80fd\u3002", "motivation": "\u5728\u73b0\u4ee3\u6280\u672f\u8282\u70b9\u4e2d\uff0c\u4e92\u8fde\u5ef6\u8fdf\u4e0e\u5355\u5143\u5ef6\u8fdf\u7684\u4e0d\u5747\u8861\u7f29\u653e\u4f7f\u5f97\u7f13\u51b2\u5b54\u9699\u611f\u77e5\u7684\u5e03\u5c40\u5bf9\u65f6\u5e8f\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5982BufFormer\u672a\u80fd\u5145\u5206\u8003\u8651ERC\u8fdd\u89c4\uff0c\u4e14\u672a\u5b8c\u5168\u878d\u5165\u7269\u7406\u8bbe\u8ba1\u6d41\u7a0b\u3002", "method": "MLBuf-RePlAce\u57fa\u4e8eOpenROAD\u57fa\u7840\u8bbe\u65bd\uff0c\u91c7\u7528\u9012\u5f52\u5b66\u4e60\u751f\u6210\u7f13\u51b2\u7c7b\u578b\u548c\u4f4d\u7f6e\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86ERC\u8fdd\u89c4\u95ee\u9898\uff0c\u5e76\u5728\u5168\u5c40\u5e03\u5c40\u9636\u6bb5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7f13\u51b2\u9884\u6d4b\u3002", "result": "\u4e0eOpenROAD\u9ed8\u8ba4\u7684\u865a\u62df\u7f13\u51b2\u65f6\u5e8f\u9a71\u52a8\u5168\u5c40\u5e03\u5c40\u5668\u76f8\u6bd4\uff0cMLBuf-RePlAce\u5728\u4e0d\u964d\u4f4e\u5e03\u7ebf\u540e\u529f\u8017\u7684\u60c5\u51b5\u4e0b\uff0c\u603b\u8d1f\u677e\u5f1b\uff08TNS\uff09\u5b9e\u73b0\u4e86\u6700\u592756%\u3001\u5e73\u574731%\u7684\u63d0\u5347\uff1b\u5728\u5546\u4e1a\u6d41\u7a0b\u4e2d\uff0cTNS\u63d0\u5347\u6700\u592753%\u3001\u5e73\u574728%\uff0c\u4e14\u5e03\u7ebf\u540e\u529f\u8017\u5e73\u5747\u63d0\u53470.2%\u3002", "conclusion": "MLBuf-RePlAce\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u5b66\u4e60\u9a71\u52a8\u865a\u62df\u7f13\u51b2\u611f\u77e5\u5168\u5c40\u5e03\u5c40\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3\u4e86ERC\u8fdd\u89c4\u95ee\u9898\uff0c\u4e3a\u7269\u7406\u8bbe\u8ba1\u6d41\u7a0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u9012\u5f52\u5b66\u4e60\u7684\u865a\u62df\u7f13\u51b2\u5206\u6790\u5168\u5c40\u5e03\u5c40\u65b9\u6cd5", "abstract_zh": "\u5728\u73b0\u4ee3\u6280\u672f\u8282\u70b9\u4e2d\uff0c\u7531\u4e8e\u4e92\u8fde\u5ef6\u8fdf\u4e0e\u5355\u5143\u5ef6\u8fdf\u7684\u4e0d\u5747\u8861\u7f29\u653e\uff0c\u5177\u6709\u7f13\u51b2\u5b54\u9699\uff08\u5373\u5355\u5143\u5bc6\u5ea6\uff09\u611f\u77e5\u7684\u5e03\u5c40\u5bf9\u7269\u7406\u7efc\u5408\u6d41\u7a0b\u7684\u65f6\u5e8f\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\uff08i\uff09\u4f20\u7edf\u7684van Ginneken-Lillis\u98ce\u683c\u7f13\u51b2\u65b9\u6cd5\u5728\u5168\u5c40\u5e03\u5c40\u9636\u6bb5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff1b\uff08ii\uff09\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5982BufFormer\uff09\u672a\u80fd\u5145\u5206\u8003\u8651\u7535\u6c14\u89c4\u5219\u68c0\u67e5\uff08ERC\uff09\u8fdd\u89c4\uff0c\u4e14\u672a\u5b8c\u5168\u878d\u5165\u7269\u7406\u8bbe\u8ba1\u6d41\u7a0b\u3002\u672c\u6587\u63d0\u51faMLBuf-RePlAce\uff0c\u9996\u4e2a\u57fa\u4e8eOpenROAD\u57fa\u7840\u8bbe\u65bd\u7684\u5f00\u6e90\u5b66\u4e60\u9a71\u52a8\u865a\u62df\u7f13\u51b2\u611f\u77e5\u5206\u6790\u5168\u5c40\u5e03\u5c40\u6846\u67b6\u3002MLBuf-RePlAce\u91c7\u7528\u9ad8\u6548\u7684\u9012\u5f52\u5b66\u4e60\u751f\u6210\u7f13\u51b2\u7c7b\u578b\u548c\u4f4d\u7f6e\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5168\u5c40\u5e03\u5c40\u9636\u6bb5\u7684ERC\u8fdd\u89c4\u95ee\u9898\u3002\u6211\u4eec\u901a\u8fc7TILOS MacroPlacement\u548cOpenROAD-flow-scripts\u4ed3\u5e93\u7684\u5f00\u6e90\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5c06MLBuf-RePlAce\u4e0eOpenROAD\u9ed8\u8ba4\u7684\u865a\u62df\u7f13\u51b2\u65f6\u5e8f\u9a71\u52a8\u5168\u5c40\u5e03\u5c40\u5668\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u4e0d\u964d\u4f4e\u5e03\u7ebf\u540e\u529f\u8017\u7684\u60c5\u51b5\u4e0b\uff0cMLBuf-RePlAce\u5728\u5f00\u6e90OpenROAD\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u603b\u8d1f\u677e\u5f1b\uff08TNS\uff09\u6700\u592756%\u3001\u5e73\u574731%\u7684\u63d0\u5347\u3002\u5728\u5546\u4e1a\u6d41\u7a0b\u4e2d\u8bc4\u4f30\u65f6\uff0cMLBuf-RePlAce\u7684TNS\u63d0\u5347\u6700\u592753%\u3001\u5e73\u574728%\uff0c\u4e14\u5e03\u7ebf\u540e\u529f\u8017\u5e73\u5747\u63d0\u53470.2%\u3002"}}
{"id": "2506.18341", "pdf": "https://arxiv.org/pdf/2506.18341", "abs": "https://arxiv.org/abs/2506.18341", "authors": ["Kang Chen", "Mengdi Zhang", "Yixin Cao"], "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL\u00b2\u7684\u591a\u8bed\u8a00\u7edf\u4e00\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u7801\u5e72\u9884\u7b56\u7565\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d4b\u8bd5\u65f6\u7684\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u8bed\u8a00\u5b66\u4e60\u53ef\u4ee5\u51cf\u5c11\u6240\u9700\u6570\u636e\u548c\u63a8\u7406\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d4b\u8bd5\u65f6\u7684\u6269\u5c55\u9762\u4e34\u6570\u636e\u548c\u63a8\u7406\u6548\u7387\u7684\u6311\u6218\u3002\u591a\u8bed\u8a00\u63a8\u7406\u7684\u591a\u6837\u6027\u53ef\u80fd\u4e3a\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "method": "\u63d0\u51faL\u00b2\u591a\u8bed\u8a00\u7edf\u4e00\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u79cd\u591a\u8bed\u8a00\u6570\u636e\uff1a\u5b8c\u6574\u7684\u957f\u94fe\u601d\u7ef4\u6ce8\u91ca\u548c\u5206\u6b65\u6df7\u5408\u8bed\u8a00\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u89e3\u7801\u5e72\u9884\u7b56\u7565\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c11\u91cf\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u591a\u8bed\u8a00\u5b66\u4e60\u51cf\u5c11\u4e86\u6570\u636e\u548c\u63a8\u7406\u6807\u8bb0\u9700\u6c42\uff0c\u540c\u65f6\u6027\u80fd\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "L\u00b2\u65b9\u6cd5\u4e3aLLMs\u7684\u6570\u636e\u6536\u96c6\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u4e0e\u5176\u4ed6\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u4e92\u8865\u3002", "paper_title_zh": "\u66f4\u5c11\u6570\u636e\u66f4\u5c11\u6807\u8bb0\uff1a\u591a\u8bed\u8a00\u7edf\u4e00\u5b66\u4e60\u63d0\u5347LLMs\u6d4b\u8bd5\u65f6\u63a8\u7406\u6548\u7387", "abstract_zh": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u9762\u4e34\u7684\u6570\u636e\u548c\u63a8\u7406\u6548\u7387\u6311\u6218\u3002\u57fa\u4e8e\u521d\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u63a8\u7406\u7684\u591a\u6837\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL\u00b2\u7684\u591a\u8bed\u8a00\u7edf\u4e00\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u89e3\u7801\u5e72\u9884\u7b56\u7565\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002L\u00b2\u7684\u6838\u5fc3\u601d\u60f3\u662f\u4e0d\u540c\u8bed\u8a00\u7684\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u76f8\u4e92\u4fc3\u8fdb\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u79cd\u591a\u8bed\u8a00\u6570\u636e\uff1a\u5b8c\u6574\u7684\u957f\u94fe\u601d\u7ef4\u6ce8\u91ca\u548c\u5206\u6b65\u6df7\u5408\u8bed\u8a00\u6570\u636e\u3002\u901a\u8fc7\u8fdb\u4e00\u6b65\u8c03\u4f18\uff0c\u6211\u4eec\u53d1\u73b0\u5373\u4f7f\u5c11\u91cf\u6570\u636e\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u591a\u8bed\u8a00\u5b66\u4e60\u51cf\u5c11\u4e86\u6240\u9700\u6570\u636e\u548c\u63a8\u7406\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0cL\u00b2\u65b9\u6cd5\u4e0e\u5176\u4ed6\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u6b63\u4ea4\uff0c\u56e0\u6b64\u6211\u4eec\u8fd8\u5f3a\u8c03\u4e86\u591a\u6837\u5316\u6570\u636e\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002L\u00b2\u65b9\u6cd5\u4e3aLLMs\u7684\u6570\u636e\u6536\u96c6\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17901", "pdf": "https://arxiv.org/pdf/2506.17901", "abs": "https://arxiv.org/abs/2506.17901", "authors": ["Yixuan Wu", "Yang Zhang", "Jian Wu", "Philip Torr", "Jindong Gu"], "title": "PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such\nas image captioning and visual question answering. However, they often suffer\nfrom over-reliance on spurious correlations, primarily due to linguistic priors\nthat distract the model from leveraging actual visual information. To address\nthese issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment\nframework designed to enhance the visual understanding capabilities and\nmitigate the hallucinations of MLLMs. Our framework incorporates a multimodal\ngrounding module for both visual grounding, which identifies the referred\nobject in the image, and textual grounding, which generates the rationale for\nthe final answer, ensuring that outputs are anchored in both visual and textual\nevidence. To mitigate the hallucinations, we introduce a negative rejection\nmechanism in the visual grounding module to distinguish grounded entities from\nnon-existent objects influenced by linguistic biases. On the textual grounding\nside, we propose a selective reasoning mechanism that adjusts the model's\nreasoning strategy based on query complexity. Extensive evaluations are\nconducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench\nshowing significant improvements in fine-grained visual understanding and\nhallucination suppression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPostAlign\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u589e\u5f3aMLLMs\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u57fa\u7840\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u5e7b\u89c9\u6291\u5236\u6548\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u5ffd\u89c6\u5b9e\u9645\u89c6\u89c9\u4fe1\u606f\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMMGrounded-PostAlign\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u57fa\u7840\u6a21\u5757\uff08\u8bc6\u522b\u56fe\u50cf\u4e2d\u5bf9\u8c61\uff09\u548c\u6587\u672c\u57fa\u7840\u6a21\u5757\uff08\u751f\u6210\u7b54\u6848\u4f9d\u636e\uff09\uff0c\u5e76\u5f15\u5165\u8d1f\u62d2\u7edd\u673a\u5236\u548c\u9009\u62e9\u6027\u63a8\u7406\u673a\u5236\u4ee5\u6291\u5236\u5e7b\u89c9\u3002", "result": "\u5728POPE\u3001HaloQuest\u3001VQAv2\u3001MME\u548cMMBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u5e7b\u89c9\u6291\u5236\u65b9\u9762\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "PostAlign\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u6709\u6548\u589e\u5f3aMLLMs\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "PostAlign\uff1a\u591a\u6a21\u6001\u57fa\u7840\u4f5c\u4e3aMLLMs\u7684\u77eb\u6b63\u89c6\u89d2", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u5ffd\u89c6\u5b9e\u9645\u89c6\u89c9\u4fe1\u606f\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MMGrounded-PostAlign\uff0c\u4e00\u79cd\u540e\u591a\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3aMLLMs\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002\u8be5\u6846\u67b6\u5305\u542b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u89c6\u89c9\u57fa\u7840\uff08\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\uff09\u548c\u6587\u672c\u57fa\u7840\uff08\u751f\u6210\u6700\u7ec8\u7b54\u6848\u7684\u4f9d\u636e\uff09\uff0c\u786e\u4fdd\u8f93\u51fa\u57fa\u4e8e\u89c6\u89c9\u548c\u6587\u672c\u8bc1\u636e\u3002\u4e3a\u51cf\u5c11\u5e7b\u89c9\uff0c\u6211\u4eec\u5728\u89c6\u89c9\u57fa\u7840\u6a21\u5757\u4e2d\u5f15\u5165\u8d1f\u62d2\u7edd\u673a\u5236\uff0c\u4ee5\u533a\u5206\u57fa\u4e8e\u8bed\u8a00\u504f\u89c1\u7684\u975e\u5b58\u5728\u5bf9\u8c61\u3002\u5728\u6587\u672c\u57fa\u7840\u65b9\u9762\uff0c\u6211\u4eec\u63d0\u51fa\u9009\u62e9\u6027\u63a8\u7406\u673a\u5236\uff0c\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u8c03\u6574\u6a21\u578b\u7684\u63a8\u7406\u7b56\u7565\u3002\u5728POPE\u3001HaloQuest\u3001VQAv2\u3001MME\u548cMMBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u5e7b\u89c9\u6291\u5236\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2506.17248", "pdf": "https://arxiv.org/pdf/2506.17248", "abs": "https://arxiv.org/abs/2506.17248", "authors": ["Zequn Yang", "Hongfa Wang", "Di Hu"], "title": "Efficient Quantification of Multimodal Interaction at Sample Level", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Interactions between modalities -- redundancy, uniqueness, and synergy --\ncollectively determine the composition of multimodal information. Understanding\nthese interactions is crucial for analyzing information dynamics in multimodal\nsystems, yet their accurate sample-level quantification presents significant\ntheoretical and computational challenges. To address this, we introduce the\nLightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously\ngrounded in pointwise information theory. We first develop a redundancy\nestimation framework, employing an appropriate pointwise information measure to\nquantify this most decomposable and measurable interaction. Building upon this,\nwe propose a general interaction estimation method that employs efficient\nentropy estimation, specifically tailored for sample-wise estimation in\ncontinuous distributions. Extensive experiments on synthetic and real-world\ndatasets validate LSMI's precision and efficiency. Crucially, our sample-wise\napproach reveals fine-grained sample- and category-level dynamics within\nmultimodal data, enabling practical applications such as redundancy-informed\nsample partitioning, targeted knowledge distillation, and interaction-aware\nmodel ensembling. The code is available at\nhttps://github.com/GeWu-Lab/LSMI_Estimator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6837\u672c\u7ea7\u591a\u6a21\u6001\u4ea4\u4e92\uff08LSMI\uff09\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u7cbe\u786e\u91cf\u5316\u591a\u6a21\u6001\u4fe1\u606f\u4e2d\u7684\u5197\u4f59\u3001\u72ec\u7279\u6027\u548c\u534f\u540c\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u4fe1\u606f\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff08\u5197\u4f59\u3001\u72ec\u7279\u6027\u548c\u534f\u540c\u4f5c\u7528\uff09\u5bf9\u7406\u89e3\u4fe1\u606f\u52a8\u6001\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6837\u672c\u7ea7\u91cf\u5316\u5b58\u5728\u7406\u8bba\u548c\u8ba1\u7b97\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u70b9\u5bf9\u70b9\u4fe1\u606f\u8bba\uff0c\u9996\u5148\u5f00\u53d1\u4e86\u5197\u4f59\u4f30\u8ba1\u6846\u67b6\uff0c\u968f\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4ea4\u4e92\u4f30\u8ba1\u65b9\u6cd5\uff0c\u91c7\u7528\u9ad8\u6548\u71b5\u4f30\u8ba1\u6280\u672f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fde\u7eed\u5206\u5e03\u7684\u6837\u672c\u7ea7\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LSMI\u7684\u7cbe\u786e\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7ec6\u7c92\u5ea6\u7684\u6837\u672c\u548c\u7c7b\u522b\u52a8\u6001\u3002", "conclusion": "LSMI\u4e3a\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u652f\u6301\u5197\u4f59\u6837\u672c\u5212\u5206\u3001\u76ee\u6807\u77e5\u8bc6\u84b8\u998f\u548c\u4ea4\u4e92\u611f\u77e5\u6a21\u578b\u96c6\u6210\u7b49\u5e94\u7528\u3002", "paper_title_zh": "\u6837\u672c\u7ea7\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u9ad8\u6548\u91cf\u5316", "abstract_zh": "\u591a\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\u2014\u2014\u5197\u4f59\u6027\u3001\u72ec\u7279\u6027\u548c\u534f\u540c\u6027\u2014\u2014\u5171\u540c\u51b3\u5b9a\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u7684\u7ec4\u6210\u3002\u7406\u89e3\u8fd9\u4e9b\u4ea4\u4e92\u4f5c\u7528\u5bf9\u4e8e\u5206\u6790\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u52a8\u6001\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u6837\u672c\u7ea7\u7684\u7cbe\u786e\u91cf\u5316\u4ecd\u9762\u4e34\u663e\u8457\u7684\u7406\u8bba\u548c\u8ba1\u7b97\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u70b9\u5bf9\u70b9\u4fe1\u606f\u8bba\u7684\u8f7b\u91cf\u7ea7\u6837\u672c\u7ea7\u591a\u6a21\u6001\u4ea4\u4e92\uff08LSMI\uff09\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u4e2a\u5197\u4f59\u4f30\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u9002\u5f53\u7684\u70b9\u5bf9\u70b9\u4fe1\u606f\u5ea6\u91cf\u6765\u91cf\u5316\u8fd9\u79cd\u6700\u6613\u5206\u89e3\u548c\u6d4b\u91cf\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4ea4\u4e92\u4f30\u8ba1\u65b9\u6cd5\uff0c\u91c7\u7528\u9ad8\u6548\u71b5\u4f30\u8ba1\u6280\u672f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fde\u7eed\u5206\u5e03\u7684\u6837\u672c\u7ea7\u4f30\u8ba1\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LSMI\u7684\u7cbe\u786e\u6027\u548c\u9ad8\u6548\u6027\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u6837\u672c\u7ea7\u65b9\u6cd5\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7ec6\u7c92\u5ea6\u7684\u6837\u672c\u548c\u7c7b\u522b\u52a8\u6001\uff0c\u652f\u6301\u5197\u4f59\u6837\u672c\u5212\u5206\u3001\u76ee\u6807\u77e5\u8bc6\u84b8\u998f\u548c\u4ea4\u4e92\u611f\u77e5\u6a21\u578b\u96c6\u6210\u7b49\u5b9e\u9645\u5e94\u7528\u3002\u4ee3\u7801\u53ef\u5728https://github.com/GeWu-Lab/LSMI_Estimator\u83b7\u53d6\u3002"}}
{"id": "2506.18387", "pdf": "https://arxiv.org/pdf/2506.18387", "abs": "https://arxiv.org/abs/2506.18387", "authors": ["Yousang Cho", "Key-Sun Choi"], "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy,\n  July 17, 2025", "summary": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u8bc4\u4f30\u6307\u6807\u5728\u81ea\u52a8\u751f\u6210\u8bca\u65ad\u62a5\u544a\u4e2d\u56e0\u679c\u89e3\u91ca\u8d28\u91cf\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-Black\u5728\u8bc6\u522b\u903b\u8f91\u8fde\u8d2f\u548c\u4e34\u5e8a\u6709\u6548\u7684\u56e0\u679c\u53d9\u8ff0\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cGPT-White\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u800c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6307\u6807\u4e0e\u4e34\u5e8a\u63a8\u7406\u8d28\u91cf\u8131\u8282\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u5982\u4f55\u51c6\u786e\u6355\u6349\u81ea\u52a8\u751f\u6210\u8bca\u65ad\u62a5\u544a\u4e2d\u56e0\u679c\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u4ee5\u652f\u6301\u9700\u8981\u53ef\u89e3\u91ca\u6027\u548c\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u6307\u6807\uff08BERTScore\u3001Cosine Similarity\u3001BioSentVec\u3001GPT-White\u3001GPT-Black\u548c\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4f30\uff09\uff0c\u5e94\u7528\u4e8e\u4e24\u79cd\u8f93\u5165\u7c7b\u578b\uff08\u57fa\u4e8e\u89c2\u5bdf\u548c\u57fa\u4e8e\u591a\u9879\u9009\u62e9\u7684\u62a5\u544a\u751f\u6210\uff09\uff0c\u5e76\u91c7\u7528\u4e24\u79cd\u6743\u91cd\u7b56\u7565\uff08\u4efb\u52a1\u4f18\u5148\u7ea7\u548c\u7b49\u6743\u91cd\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cGPT-Black\u5728\u8bc6\u522b\u903b\u8f91\u8fde\u8d2f\u548c\u4e34\u5e8a\u6709\u6548\u7684\u56e0\u679c\u53d9\u8ff0\u65b9\u9762\u6700\u5177\u533a\u5206\u529b\uff0cGPT-White\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u800c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6307\u6807\u4e0e\u4e34\u5e8a\u63a8\u7406\u8d28\u91cf\u4e0d\u7b26\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6307\u6807\u9009\u62e9\u548c\u6743\u91cd\u5bf9\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u652f\u6301\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u548c\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "paper_title_zh": "\u57fa\u4e8eLLM\u548c\u4eba\u7c7b\u5bf9\u9f50\u6307\u6807\u8bc4\u4f30\u533b\u5b66\u62a5\u544a\u4e2d\u7684\u56e0\u679c\u89e3\u91ca", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u5982\u4f55\u51c6\u786e\u6355\u6349\u81ea\u52a8\u751f\u6210\u8bca\u65ad\u62a5\u544a\u4e2d\u56e0\u679c\u89e3\u91ca\u7684\u8d28\u91cf\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u516d\u79cd\u6307\u6807\uff1aBERTScore\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001BioSentVec\u3001GPT-White\u3001GPT-Black\u548c\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4f30\uff0c\u5e94\u7528\u4e8e\u4e24\u79cd\u8f93\u5165\u7c7b\u578b\uff08\u57fa\u4e8e\u89c2\u5bdf\u548c\u57fa\u4e8e\u591a\u9879\u9009\u62e9\u7684\u62a5\u544a\u751f\u6210\uff09\u3002\u91c7\u7528\u4e24\u79cd\u6743\u91cd\u7b56\u7565\uff1a\u4e00\u79cd\u53cd\u6620\u4efb\u52a1\u4f18\u5148\u7ea7\uff0c\u53e6\u4e00\u79cd\u5bf9\u6240\u6709\u6307\u6807\u8d4b\u4e88\u7b49\u6743\u91cd\u3002\u7ed3\u679c\u663e\u793a\uff0cGPT-Black\u5728\u8bc6\u522b\u903b\u8f91\u8fde\u8d2f\u548c\u4e34\u5e8a\u6709\u6548\u7684\u56e0\u679c\u53d9\u8ff0\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cGPT-White\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u800c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6307\u6807\u4e0e\u4e34\u5e8a\u63a8\u7406\u8d28\u91cf\u8131\u8282\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u6307\u6807\u9009\u62e9\u548c\u6743\u91cd\u5bf9\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u652f\u6301\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u548c\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2506.17903", "pdf": "https://arxiv.org/pdf/2506.17903", "abs": "https://arxiv.org/abs/2506.17903", "authors": ["Huanjia Zhu", "Yishu Liu", "Xiaozhao Fang", "Guangming Lu", "Bingzhi Chen"], "title": "Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at IJCAI 2025", "summary": "Existing Medical Visual Question Answering (Med-VQA) models often suffer from\nlanguage biases, where spurious correlations between question types and answer\ncategories are inadvertently established. To address these issues, we propose a\nnovel Cause-Effect Driven Optimization framework called CEDO, that incorporates\nthree well-established mechanisms, i.e., Modality-driven Heterogeneous\nOptimization (MHO), Gradient-guided Modality Synergy (GMS), and\nDistribution-adapted Loss Rescaling (DLR), for comprehensively mitigating\nlanguage biases from both causal and effectual perspectives. Specifically, MHO\nemploys adaptive learning rates for specific modalities to achieve\nheterogeneous optimization, thus enhancing robust reasoning capabilities.\nAdditionally, GMS leverages the Pareto optimization method to foster\nsynergistic interactions between modalities and enforce gradient orthogonality\nto eliminate bias updates, thereby mitigating language biases from the effect\nside, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive\nweights to individual losses to ensure balanced learning across all answer\ncategories, effectively alleviating language biases from the cause side, i.e.,\nimbalance biases within datasets. Extensive experiments on multiple traditional\nand bias-sensitive benchmarks consistently demonstrate the robustness of CEDO\nover state-of-the-art competitors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCEDO\u7684\u65b0\u578b\u56e0\u679c\u9a71\u52a8\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u673a\u5236\uff08MHO\u3001GMS\u3001DLR\uff09\u4ece\u56e0\u679c\u548c\u6548\u679c\u89d2\u5ea6\u5168\u9762\u7f13\u89e3\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8bed\u8a00\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08Med-VQA\uff09\u6a21\u578b\u5e38\u56e0\u8bed\u8a00\u504f\u5dee\uff08\u5982\u95ee\u9898\u7c7b\u578b\u4e0e\u7b54\u6848\u7c7b\u522b\u95f4\u7684\u865a\u5047\u5173\u8054\uff09\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u9a71\u52a8\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "CEDO\u6846\u67b6\u5305\u542b\u4e09\u79cd\u673a\u5236\uff1a1\uff09MHO\u901a\u8fc7\u6a21\u6001\u9a71\u52a8\u7684\u5f02\u8d28\u4f18\u5316\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff1b2\uff09GMS\u5229\u7528\u5e15\u7d2f\u6258\u4f18\u5316\u4fc3\u8fdb\u6a21\u6001\u534f\u540c\u5e76\u6d88\u9664\u504f\u5dee\u66f4\u65b0\uff1b3\uff09DLR\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u5e73\u8861\u5b66\u4e60\uff0c\u7f13\u89e3\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u5e73\u8861\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u4f20\u7edf\u548c\u504f\u5dee\u654f\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCEDO\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CEDO\u901a\u8fc7\u56e0\u679c\u9a71\u52a8\u7684\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86Med-VQA\u4e2d\u7684\u8bed\u8a00\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u56e0\u679c\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u6cd5\u7528\u4e8e\u9c81\u68d2\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8bed\u8a00\u504f\u5dee\u95ee\u9898", "abstract_zh": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08Med-VQA\uff09\u6a21\u578b\u5e38\u56e0\u8bed\u8a00\u504f\u5dee\uff08\u5982\u95ee\u9898\u7c7b\u578b\u4e0e\u7b54\u6848\u7c7b\u522b\u95f4\u7684\u865a\u5047\u5173\u8054\uff09\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u56e0\u679c\u9a71\u52a8\u4f18\u5316\u6846\u67b6CEDO\uff0c\u5305\u542b\u4e09\u79cd\u673a\u5236\uff1a\u6a21\u6001\u9a71\u52a8\u7684\u5f02\u8d28\u4f18\u5316\uff08MHO\uff09\u3001\u68af\u5ea6\u5f15\u5bfc\u7684\u6a21\u6001\u534f\u540c\uff08GMS\uff09\u548c\u5206\u5e03\u9002\u5e94\u7684\u635f\u5931\u91cd\u7f29\u653e\uff08DLR\uff09\uff0c\u4ece\u56e0\u679c\u548c\u6548\u679c\u89d2\u5ea6\u5168\u9762\u7f13\u89e3\u8bed\u8a00\u504f\u5dee\u3002\u5177\u4f53\u800c\u8a00\uff0cMHO\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u589e\u5f3a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff1bGMS\u5229\u7528\u5e15\u7d2f\u6258\u4f18\u5316\u4fc3\u8fdb\u6a21\u6001\u534f\u540c\u5e76\u6d88\u9664\u504f\u5dee\u66f4\u65b0\uff1bDLR\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u5e73\u8861\u5b66\u4e60\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCEDO\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.17249", "pdf": "https://arxiv.org/pdf/2506.17249", "abs": "https://arxiv.org/abs/2506.17249", "authors": ["Jianing He", "Qi Zhang", "Duoqian Miao", "Yi Kun", "Shufeng Hao", "Hongyun Zhang", "Zhihua Wei"], "title": "Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection", "categories": ["cs.LG", "cs.AI"], "comment": "IJCAI 2025, 9 pages", "summary": "Early exiting has demonstrated great potential in accelerating the inference\nof pre-trained language models (PLMs) by enabling easy samples to exit at\nshallow layers, eliminating the need for executing deeper layers. However,\nexisting early exiting methods primarily rely on class-relevant logits to\nformulate their exiting signals for estimating prediction certainty, neglecting\nthe detrimental influence of class-irrelevant information in the features on\nprediction certainty. This leads to an overestimation of prediction certainty,\ncausing premature exiting of samples with incorrect early predictions. To\nremedy this, we define an NSP score to estimate prediction certainty by\nconsidering the proportion of class-irrelevant information in the features. On\nthis basis, we propose a novel early exiting method based on the\nCertainty-Aware Probability (CAP) score, which integrates insights from both\nlogits and the NSP score to enhance prediction certainty estimation, thus\nenabling more reliable exiting decisions. The experimental results on the GLUE\nbenchmark show that our method can achieve an average speed-up ratio of 2.19x\nacross all tasks with negligible performance degradation, surpassing the\nstate-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off\nbetween task performance and inference efficiency. The code is available at\nhttps://github.com/He-Jianing/NSP.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u7a7a\u95f4\u6295\u5f71\uff08NSP\uff09\u7684\u65b0\u578b\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u7c7b\u65e0\u5173\u4fe1\u606f\u6bd4\u4f8b\u548c\u7c7b\u76f8\u5173\u903b\u8f91\u503c\uff0c\u63d0\u5347\u9884\u6d4b\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u65e9\u671f\u9000\u51fa\u51b3\u7b56\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u52a0\u901f2.19\u500d\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd528%\u3002", "motivation": "\u73b0\u6709\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7c7b\u76f8\u5173\u903b\u8f91\u503c\u4f30\u8ba1\u9884\u6d4b\u786e\u5b9a\u6027\uff0c\u5ffd\u7565\u4e86\u7c7b\u65e0\u5173\u4fe1\u606f\u5bf9\u9884\u6d4b\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5bfc\u81f4\u9884\u6d4b\u786e\u5b9a\u6027\u88ab\u9ad8\u4f30\uff0c\u9519\u8bef\u6837\u672c\u8fc7\u65e9\u9000\u51fa\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8003\u8651\u7c7b\u65e0\u5173\u4fe1\u606f\u6bd4\u4f8b\uff0c\u6539\u8fdb\u9884\u6d4b\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u63d0\u51faNSP\u5206\u6570\u4ee5\u91cf\u5316\u7c7b\u65e0\u5173\u4fe1\u606f\u6bd4\u4f8b\uff0c\u5e76\u7ed3\u5408\u7c7b\u76f8\u5173\u903b\u8f91\u503c\u8bbe\u8ba1\u786e\u5b9a\u6027\u611f\u77e5\u6982\u7387\uff08CAP\uff09\u5206\u6570\uff0c\u7528\u4e8e\u66f4\u53ef\u9760\u7684\u65e9\u671f\u9000\u51fa\u51b3\u7b56\u3002", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u52a0\u901f2.19\u500d\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5ConsistentEE 28%\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u6027\u80fd\u4e0e\u63a8\u7406\u6548\u7387\u7684\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u7c7b\u65e0\u5173\u4fe1\u606f\u6bd4\u4f8b\u548c\u7c7b\u76f8\u5173\u903b\u8f91\u503c\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u65e9\u671f\u9000\u51fa\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u96f6\u7a7a\u95f4\u6295\u5f71\u6539\u8fdb\u9884\u6d4b\u786e\u5b9a\u6027\u4f30\u8ba1\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u65e9\u671f\u9000\u51fa", "abstract_zh": "\u65e9\u671f\u9000\u51fa\u901a\u8fc7\u5141\u8bb8\u7b80\u5355\u6837\u672c\u5728\u6d45\u5c42\u9000\u51fa\uff0c\u65e0\u9700\u6267\u884c\u66f4\u6df1\u5c42\uff0c\u663e\u8457\u52a0\u901f\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u7684\u63a8\u7406\u3002\u7136\u800c\uff0c\u73b0\u6709\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7c7b\u76f8\u5173\u903b\u8f91\u503c\u751f\u6210\u9000\u51fa\u4fe1\u53f7\u4ee5\u4f30\u8ba1\u9884\u6d4b\u786e\u5b9a\u6027\uff0c\u5ffd\u7565\u4e86\u7279\u5f81\u4e2d\u7c7b\u65e0\u5173\u4fe1\u606f\u5bf9\u9884\u6d4b\u786e\u5b9a\u6027\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5bfc\u81f4\u9884\u6d4b\u786e\u5b9a\u6027\u88ab\u9ad8\u4f30\uff0c\u9519\u8bef\u6837\u672c\u8fc7\u65e9\u9000\u51fa\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86NSP\u5206\u6570\uff0c\u901a\u8fc7\u8003\u8651\u7279\u5f81\u4e2d\u7c7b\u65e0\u5173\u4fe1\u606f\u7684\u6bd4\u4f8b\u6765\u4f30\u8ba1\u9884\u6d4b\u786e\u5b9a\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786e\u5b9a\u6027\u611f\u77e5\u6982\u7387\uff08CAP\uff09\u5206\u6570\u7684\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u903b\u8f91\u503c\u548cNSP\u5206\u6570\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u9000\u51fa\u51b3\u7b56\u3002GLUE\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5e73\u5747\u52a0\u901f2.19\u500d\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5ConsistentEE 28%\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u6027\u80fd\u4e0e\u63a8\u7406\u6548\u7387\u7684\u66f4\u597d\u5e73\u8861\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/He-Jianing/NSP.git\u3002"}}
{"id": "2506.18399", "pdf": "https://arxiv.org/pdf/2506.18399", "abs": "https://arxiv.org/abs/2506.18399", "authors": ["Mostafa Saeed", "Nizar Habash"], "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres", "categories": ["cs.CL"], "comment": null, "summary": "Lemmatization is crucial for NLP tasks in morphologically rich languages with\nambiguous orthography like Arabic, but existing tools face challenges due to\ninconsistent standards and limited genre coverage. This paper introduces two\nnovel approaches that frame lemmatization as classification into a\nLemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic\nclustering. We also present a new Arabic lemmatization test set covering\ndiverse genres, standardized alongside existing datasets. We evaluate character\nlevel sequence-to-sequence models, which perform competitively and offer\ncomplementary value, but are limited to lemma prediction (not LPG) and prone to\nhallucinating implausible forms. Our results show that classification and\nclustering yield more robust, interpretable outputs, setting new benchmarks for\nArabic lemmatization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u5c06\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u89c6\u4e3a\u5206\u7c7b\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u8986\u76d6\u591a\u9886\u57df\u7684\u65b0\u6d4b\u8bd5\u96c6\u3002\u7ed3\u679c\u663e\u793a\u5206\u7c7b\u548c\u805a\u7c7b\u65b9\u6cd5\u6bd4\u5e8f\u5217\u6a21\u578b\u66f4\u7a33\u5065\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u5728\u8bcd\u5f62\u8fd8\u539f\u4efb\u52a1\u4e2d\u9762\u4e34\u6807\u51c6\u4e0d\u4e00\u81f4\u548c\u9886\u57df\u8986\u76d6\u6709\u9650\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5de5\u5177\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eLemma-POS-Gloss (LPG) \u6807\u7b7e\u96c6\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u7ffb\u8bd1\u548c\u8bed\u4e49\u805a\u7c7b\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u591a\u9886\u57df\u963f\u62c9\u4f2f\u8bed\u6d4b\u8bd5\u96c6\u3002", "result": "\u5b57\u7b26\u7ea7\u5e8f\u5217\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\u4f46\u6613\u751f\u6210\u4e0d\u5408\u7406\u5f62\u5f0f\uff0c\u800c\u5206\u7c7b\u548c\u805a\u7c7b\u65b9\u6cd5\u66f4\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u5206\u7c7b\u548c\u805a\u7c7b\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u548c\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002", "paper_title_zh": "\u5c06\u8bcd\u5f62\u8fd8\u539f\u89c6\u4e3a\u5206\u7c7b\u4efb\u52a1\uff1a\u963f\u62c9\u4f2f\u8bed\u591a\u9886\u57df\u7814\u7a76\u7ed3\u679c", "abstract_zh": "\u8bcd\u5f62\u8fd8\u539f\u5bf9\u4e8e\u5f62\u6001\u4e30\u5bcc\u4e14\u62fc\u5199\u6a21\u7cca\u7684\u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\uff09\u7684NLP\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u56e0\u6807\u51c6\u4e0d\u4e00\u81f4\u548c\u9886\u57df\u8986\u76d6\u6709\u9650\u800c\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u8bcd\u5f62\u8fd8\u539f\u89c6\u4e3a\u5bf9Lemma-POS-Gloss (LPG)\u6807\u7b7e\u96c6\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u5408\u673a\u5668\u7ffb\u8bd1\u548c\u8bed\u4e49\u805a\u7c7b\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u8986\u76d6\u591a\u9886\u57df\u7684\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u6d4b\u8bd5\u96c6\uff0c\u5e76\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u6807\u51c6\u5316\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u5b57\u7b26\u7ea7\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4ec5\u9650\u4e8e\u8bcd\u5f62\u9884\u6d4b\uff08\u800c\u975eLPG\uff09\u4e14\u6613\u751f\u6210\u4e0d\u5408\u7406\u5f62\u5f0f\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5206\u7c7b\u548c\u805a\u7c7b\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.17910", "pdf": "https://arxiv.org/pdf/2506.17910", "abs": "https://arxiv.org/abs/2506.17910", "authors": ["Mohamed Benkedadra", "Matei Mancas", "Sidi Ahmed Mahmoudi"], "title": "Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "2D cameras are often used in interactive systems. Other systems like gaming\nconsoles provide more powerful 3D cameras for short range depth sensing.\nOverall, these cameras are not reliable in large, complex environments. In this\nwork, we propose a 3D stereo vision based pipeline for interactive systems,\nthat is able to handle both ordinary and sensitive applications, through robust\nscene understanding. We explore the fusion of multiple 3D cameras to do full\nscene reconstruction, which allows for preforming a wide range of tasks, like\nevent recognition, subject tracking, and notification. Using possible feedback\napproaches, the system can receive data from the subjects present in the\nenvironment, to learn to make better decisions, or to adapt to completely new\nenvironments. Throughout the paper, we introduce the pipeline and explain our\npreliminary experimentation and results. Finally, we draw the roadmap for the\nnext steps that need to be taken, in order to get this pipeline into production", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u7acb\u4f53\u89c6\u89c9\u7684\u5b9e\u65f6\u4e8b\u4ef6\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6444\u50cf\u5934\u878d\u5408\u5b9e\u73b0\u573a\u666f\u91cd\u5efa\uff0c\u652f\u6301\u4e8b\u4ef6\u8bc6\u522b\u3001\u76ee\u6807\u8ddf\u8e2a\u7b49\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u673a\u5236\u4f18\u5316\u51b3\u7b56\u3002", "motivation": "\u73b0\u67092D\u6444\u50cf\u5934\u548c\u77ed\u8ddd\u79bb3D\u6444\u50cf\u5934\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u76843D\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u4ea4\u4e92\u5f0f\u5e94\u7528\u548c\u573a\u666f\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a3D\u6444\u50cf\u5934\u878d\u5408\u7684\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u5168\u573a\u666f\u91cd\u5efa\uff0c\u5e76\u7ed3\u5408\u53cd\u9988\u673a\u5236\u4ece\u73af\u5883\u4e2d\u5b66\u4e60\u4ee5\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u652f\u6301\u4e8b\u4ef6\u8bc6\u522b\u3001\u76ee\u6807\u8ddf\u8e2a\u548c\u901a\u77e5\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u9002\u5e94\u65b0\u73af\u5883\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u884c\u76843D\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u6846\u67b6\uff0c\u5e76\u89c4\u5212\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u751f\u4ea7\u7684\u8def\u7ebf\u56fe\u3002", "paper_title_zh": "\u57fa\u4e8e\u53cd\u9988\u9a71\u52a8\u7684\u591a\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u7528\u4e8e\u5b9e\u65f6\u4e8b\u4ef6\u5206\u6790", "abstract_zh": "2D\u6444\u50cf\u5934\u5e38\u7528\u4e8e\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u800c\u6e38\u620f\u673a\u7b49\u8bbe\u5907\u5219\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u77ed\u8ddd\u79bb3D\u6df1\u5ea6\u611f\u77e5\u6444\u50cf\u5934\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6444\u50cf\u5934\u5728\u590d\u6742\u5927\u73af\u5883\u4e2d\u5e76\u4e0d\u53ef\u9760\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u7acb\u4f53\u89c6\u89c9\u7684\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u901a\u8fc7\u9c81\u68d2\u7684\u573a\u666f\u7406\u89e3\u5904\u7406\u666e\u901a\u548c\u654f\u611f\u5e94\u7528\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u591a3D\u6444\u50cf\u5934\u7684\u878d\u5408\u4ee5\u5b9e\u73b0\u5168\u573a\u666f\u91cd\u5efa\uff0c\u4ece\u800c\u652f\u6301\u4e8b\u4ef6\u8bc6\u522b\u3001\u76ee\u6807\u8ddf\u8e2a\u548c\u901a\u77e5\u7b49\u529f\u80fd\u3002\u901a\u8fc7\u53ef\u80fd\u7684\u53cd\u9988\u673a\u5236\uff0c\u7cfb\u7edf\u53ef\u4ee5\u4ece\u73af\u5883\u4e2d\u83b7\u53d6\u6570\u636e\uff0c\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u6216\u9002\u5e94\u5168\u65b0\u73af\u5883\u3002\u6587\u4e2d\u4ecb\u7ecd\u4e86\u8be5\u6d41\u6c34\u7ebf\uff0c\u5e76\u5c55\u793a\u4e86\u521d\u6b65\u5b9e\u9a8c\u548c\u7ed3\u679c\u3002\u6700\u540e\uff0c\u6211\u4eec\u89c4\u5212\u4e86\u5c06\u8be5\u6d41\u6c34\u7ebf\u6295\u5165\u751f\u4ea7\u7684\u4e0b\u4e00\u6b65\u8def\u7ebf\u56fe\u3002"}}
{"id": "2506.17250", "pdf": "https://arxiv.org/pdf/2506.17250", "abs": "https://arxiv.org/abs/2506.17250", "authors": ["Fudong Lin", "Jiadong Lou", "Hao Wang", "Brian Jalaian", "Xu Yuan"], "title": "Towards Interpretable Adversarial Examples via Sparse Adversarial Attack", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sparse attacks are to optimize the magnitude of adversarial perturbations for\nfooling deep neural networks (DNNs) involving only a few perturbed pixels\n(i.e., under the l0 constraint), suitable for interpreting the vulnerability of\nDNNs. However, existing solutions fail to yield interpretable adversarial\nexamples due to their poor sparsity. Worse still, they often struggle with\nheavy computational overhead, poor transferability, and weak attack strength.\nIn this paper, we aim to develop a sparse attack for understanding the\nvulnerability of CNNs by minimizing the magnitude of initial perturbations\nunder the l0 constraint, to overcome the existing drawbacks while achieving a\nfast, transferable, and strong attack to DNNs. In particular, a novel and\ntheoretical sound parameterization technique is introduced to approximate the\nNP-hard l0 optimization problem, making directly optimizing sparse\nperturbations computationally feasible. Besides, a novel loss function is\ndesigned to augment initial perturbations by maximizing the adversary property\nand minimizing the number of perturbed pixels simultaneously. Extensive\nexperiments are conducted to demonstrate that our approach, with theoretical\nperformance guarantees, outperforms state-of-the-art sparse attacks in terms of\ncomputational overhead, transferability, and attack strength, expecting to\nserve as a benchmark for evaluating the robustness of DNNs. In addition,\ntheoretical and empirical results validate that our approach yields sparser\nadversarial examples, empowering us to discover two categories of noises, i.e.,\n\"obscuring noise\" and \"leading noise\", which will help interpret how\nadversarial perturbation misleads the classifiers into incorrect predictions.\nOur code is available at https://github.com/fudong03/SparseAttack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u521d\u59cb\u6270\u52a8\u7684\u5e45\u5ea6\u5e76\u6ee1\u8db3l0\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u53ef\u8fc1\u79fb\u4e14\u5f3a\u529b\u7684\u653b\u51fb\uff0c\u540c\u65f6\u751f\u6210\u4e86\u66f4\u7a00\u758f\u4e14\u53ef\u89e3\u91ca\u7684\u5bf9\u6297\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u7a00\u758f\u6027\u3001\u8ba1\u7b97\u5f00\u9500\u3001\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u5f3a\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5bf9\u6297\u6837\u672c\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u7f3a\u70b9\uff0c\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u7a00\u758f\u653b\u51fb\u65b9\u6cd5\uff0c\u4ee5\u7406\u89e3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u8106\u5f31\u6027\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7406\u8bba\u53c2\u6570\u5316\u6280\u672f\uff0c\u8fd1\u4f3c\u6c42\u89e3NP\u96be\u7684l0\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u635f\u5931\u51fd\u6570\uff0c\u540c\u65f6\u6700\u5927\u5316\u5bf9\u6297\u6027\u548c\u6700\u5c0f\u5316\u6270\u52a8\u50cf\u7d20\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u5f3a\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u7a00\u758f\u7684\u5bf9\u6297\u6837\u672c\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u566a\u58f0\u7c7b\u578b\uff08\u201c\u906e\u853d\u566a\u58f0\u201d\u548c\u201c\u5f15\u5bfc\u566a\u58f0\u201d\uff09\uff0c\u6709\u52a9\u4e8e\u89e3\u91ca\u5bf9\u6297\u6270\u52a8\u5982\u4f55\u8bef\u5bfc\u5206\u7c7b\u5668\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7a00\u758f\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4e0d\u4ec5\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\uff0c\u4e3a\u8bc4\u4f30\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5bf9\u6297\u6270\u52a8\u7684\u5185\u5728\u673a\u5236\u3002", "paper_title_zh": "\u9762\u5411\u53ef\u89e3\u91ca\u5bf9\u6297\u6837\u672c\u7684\u7a00\u758f\u5bf9\u6297\u653b\u51fb", "abstract_zh": "\u7a00\u758f\u653b\u51fb\u65e8\u5728\u901a\u8fc7\u4ec5\u6270\u52a8\u5c11\u91cf\u50cf\u7d20\uff08\u5373\u6ee1\u8db3l0\u7ea6\u675f\uff09\u6765\u4f18\u5316\u5bf9\u6297\u6270\u52a8\u7684\u5e45\u5ea6\uff0c\u4ece\u800c\u6b3a\u9a97\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\uff0c\u9002\u7528\u4e8e\u89e3\u91caDNN\u7684\u8106\u5f31\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u7a00\u758f\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4e14\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u8fc1\u79fb\u6027\u5dee\u548c\u653b\u51fb\u5f3a\u5ea6\u5f31\u7b49\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7a00\u758f\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u521d\u59cb\u6270\u52a8\u7684\u5e45\u5ea6\u5e76\u6ee1\u8db3l0\u7ea6\u675f\uff0c\u514b\u670d\u73b0\u6709\u7f3a\u70b9\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u53ef\u8fc1\u79fb\u4e14\u5f3a\u529b\u7684\u653b\u51fb\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u7406\u8bba\u53ef\u9760\u7684\u91cd\u53c2\u6570\u5316\u6280\u672f\uff0c\u8fd1\u4f3c\u6c42\u89e3NP\u96be\u7684l0\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u76f4\u63a5\u4f18\u5316\u7a00\u758f\u6270\u52a8\u6210\u4e3a\u53ef\u80fd\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u540c\u65f6\u6700\u5927\u5316\u5bf9\u6297\u6027\u548c\u6700\u5c0f\u5316\u6270\u52a8\u50cf\u7d20\u6570\u91cf\u6765\u589e\u5f3a\u521d\u59cb\u6270\u52a8\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u5f3a\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\uff0c\u6709\u671b\u6210\u4e3a\u8bc4\u4f30DNN\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u4e86\u66f4\u7a00\u758f\u7684\u5bf9\u6297\u6837\u672c\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u566a\u58f0\u7c7b\u578b\uff08\u201c\u906e\u853d\u566a\u58f0\u201d\u548c\u201c\u5f15\u5bfc\u566a\u58f0\u201d\uff09\uff0c\u6709\u52a9\u4e8e\u89e3\u91ca\u5bf9\u6297\u6270\u52a8\u5982\u4f55\u8bef\u5bfc\u5206\u7c7b\u5668\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/fudong03/SparseAttack\u3002"}}
{"id": "2506.18421", "pdf": "https://arxiv.org/pdf/2506.18421", "abs": "https://arxiv.org/abs/2506.18421", "authors": ["Ce Li", "Xiaofan Liu", "Zhiyan Song", "Ce Chi", "Chen Zhao", "Jingjing Yang", "Zhendong Wang", "Kexin Yang", "Boshen Shi", "Xing Wang", "Chao Deng", "Junlan Feng"], "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Benmark report v1.0", "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8868\u683c\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6TReB\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b26\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u73b0\u6709LLM\u5728\u590d\u6742\u8868\u683c\u4efb\u52a1\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u4f01\u4e1a\u548c\u884c\u4e1a\u4e2d\u7684\u5927\u91cf\u6570\u636e\u4ee5\u8868\u683c\u5f62\u5f0f\u5b58\u50a8\uff0c\u4f46\u8868\u683c\u6570\u636e\u7684\u9690\u85cf\u8bed\u4e49\u3001\u590d\u6742\u6027\u548c\u7ed3\u6784\u5316\u7279\u6027\u5bf9LLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u51fa\u4e86\u6311\u6218\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u5168\u9762\u8bc4\u4f30LLM\u5728\u8868\u683c\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u57fa\u51c6\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TReB\u57fa\u51c6\uff0c\u5305\u542b\u6d45\u5c42\u8868\u683c\u7406\u89e3\u548c\u6df1\u5c42\u8868\u683c\u63a8\u7406\u517126\u4e2a\u5b50\u4efb\u52a1\u3002\u901a\u8fc7\u8fed\u4ee3\u6570\u636e\u5904\u7406\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff08TCoT\u3001PoT\u548cICoT\uff09\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u5bf920\u591a\u79cd\u5148\u8fdbLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709LLM\u5728\u5904\u7406\u590d\u6742\u548c\u771f\u5b9e\u4e16\u754c\u7684\u8868\u683c\u4efb\u52a1\u65f6\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002TReB\u57fa\u51c6\u53ca\u5176\u8bc4\u4f30\u6846\u67b6\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u6570\u636e\u96c6\u548c\u6846\u67b6\u5df2\u516c\u5f00\u3002", "conclusion": "TReB\u4e3a\u8bc4\u4f30LLM\u5728\u8868\u683c\u63a8\u7406\u80fd\u529b\u4e0a\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u6709\u6548\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "TReB\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u683c\u63a8\u7406\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6", "abstract_zh": "\u4f01\u4e1a\u548c\u884c\u4e1a\u4e2d\u7684\u5927\u90e8\u5206\u6570\u636e\u4ee5\u8868\u683c\u3001\u6570\u636e\u5e93\u548c\u6570\u636e\u4ed3\u5e93\u7684\u5f62\u5f0f\u5b58\u50a8\u3002\u8868\u683c\u6570\u636e\u7684\u63a8\u7406\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\uff0c\u56e0\u5176\u9690\u85cf\u7684\u8bed\u4e49\u3001\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u7ed3\u6784\u5316\u7279\u6027\u3002\u5176\u4e2d\u4e00\u4e2a\u6311\u6218\u662f\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u516c\u5e73\u53cd\u6620LLM\u5728\u5e7f\u6cdb\u8868\u683c\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u6709\u6548\u8bc4\u4f30\u57fa\u51c6\u3002\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8868\u683c\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6TReB\uff0c\u6db5\u76d6\u6d45\u5c42\u8868\u683c\u7406\u89e3\u80fd\u529b\u548c\u6df1\u5c42\u8868\u683c\u63a8\u7406\u80fd\u529b\uff0c\u517126\u4e2a\u5b50\u4efb\u52a1\u3002\u901a\u8fc7\u8fed\u4ee3\u6570\u636e\u5904\u7406\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff08TCoT\u3001PoT\u548cICoT\uff09\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u8fdb\u4e00\u6b65\u5bf920\u591a\u79cd\u5148\u8fdbLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709LLM\u5728\u5904\u7406\u590d\u6742\u548c\u771f\u5b9e\u4e16\u754c\u7684\u8868\u683c\u4efb\u52a1\u65f6\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5747\u5df2\u516c\u5f00\uff0c\u6570\u636e\u96c6\u6258\u7ba1\u4e8e[HuggingFace]\uff0c\u6846\u67b6\u53d1\u5e03\u4e8e[GitHub]\u3002"}}
{"id": "2506.17912", "pdf": "https://arxiv.org/pdf/2506.17912", "abs": "https://arxiv.org/abs/2506.17912", "authors": ["Chuhao Jin", "Haosen Li", "Bingzi Zhang", "Che Liu", "Xiting Wang", "Ruihua Song", "Wenbing Huang", "Ying Qin", "Fuzheng Zhang", "Di Zhang"], "title": "PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis", "categories": ["cs.CV", "cs.MM"], "comment": "14 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have enabled breakthroughs in\nmany multimodal generation tasks, but a significant performance gap still\nexists in text-to-motion generation, where LLM-based methods lag far behind\nnon-LLM methods. We identify the granularity of motion tokenization as a\ncritical bottleneck: fine-grained tokenization induces local dependency issues,\nwhere LLMs overemphasize short-term coherence at the expense of global semantic\nalignment, while coarse-grained tokenization sacrifices motion details. To\nresolve this issue, we propose PlanMoGPT, an LLM-based framework integrating\nprogressive planning and flow-enhanced fine-grained motion tokenization. First,\nour progressive planning mechanism leverages LLMs' autoregressive capabilities\nto hierarchically generate motion tokens by starting from sparse global plans\nand iteratively refining them into full sequences. Second, our flow-enhanced\ntokenizer doubles the downsampling resolution and expands the codebook size by\neight times, minimizing detail loss during discretization, while a\nflow-enhanced decoder recovers motion nuances. Extensive experiments on\ntext-to-motion benchmarks demonstrate that it achieves state-of-the-art\nperformance, improving FID scores by 63.8% (from 0.380 to 0.141) on\nlong-sequence generation while enhancing motion diversity by 49.9% compared to\nexisting methods. The proposed framework successfully resolves the\ndiversity-quality trade-off that plagues current non-LLM approaches,\nestablishing new standards for text-to-motion generation.", "AI": {"tldr": "PlanMoGPT\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c4\u5212\u548c\u6d41\u589e\u5f3a\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u6807\u8bb0\u5316\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u843d\u540e\u4e8e\u975eLLM\u65b9\u6cd5\uff0c\u4e3b\u8981\u7531\u4e8e\u8fd0\u52a8\u6807\u8bb0\u5316\u7684\u7c92\u5ea6\u95ee\u9898\uff1a\u7ec6\u7c92\u5ea6\u6807\u8bb0\u5316\u5bfc\u81f4\u5c40\u90e8\u4f9d\u8d56\u95ee\u9898\uff0c\u800c\u7c97\u7c92\u5ea6\u6807\u8bb0\u5316\u727a\u7272\u4e86\u8fd0\u52a8\u7ec6\u8282\u3002", "method": "PlanMoGPT\u7ed3\u5408\u6e10\u8fdb\u5f0f\u89c4\u5212\u548c\u6d41\u589e\u5f3a\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u6807\u8bb0\u5316\uff1a1\uff09\u6e10\u8fdb\u5f0f\u89c4\u5212\u673a\u5236\u5229\u7528LLM\u7684\u81ea\u56de\u5f52\u80fd\u529b\uff0c\u4ece\u7a00\u758f\u5168\u5c40\u8ba1\u5212\u9010\u6b65\u7ec6\u5316\u751f\u6210\u5b8c\u6574\u8fd0\u52a8\u5e8f\u5217\uff1b2\uff09\u6d41\u589e\u5f3a\u6807\u8bb0\u5668\u63d0\u9ad8\u4e0b\u91c7\u6837\u5206\u8fa8\u7387\u5e76\u6269\u5c55\u7801\u672c\u5927\u5c0f\uff0c\u51cf\u5c11\u79bb\u6563\u5316\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u8282\u635f\u5931\uff0c\u540c\u65f6\u901a\u8fc7\u6d41\u589e\u5f3a\u89e3\u7801\u5668\u6062\u590d\u8fd0\u52a8\u7ec6\u8282\u3002", "result": "\u5728\u6587\u672c\u5230\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlanMoGPT\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u957f\u5e8f\u5217\u751f\u6210\u7684FID\u5206\u6570\u63d0\u5347\u4e8663.8%\uff08\u4ece0.380\u964d\u81f30.141\uff09\uff0c\u8fd0\u52a8\u591a\u6837\u6027\u63d0\u9ad8\u4e8649.9%\u3002", "conclusion": "PlanMoGPT\u6210\u529f\u89e3\u51b3\u4e86\u5f53\u524d\u975eLLM\u65b9\u6cd5\u9762\u4e34\u7684\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "paper_title_zh": "PlanMoGPT\uff1a\u57fa\u4e8e\u6d41\u589e\u5f3a\u6e10\u8fdb\u5f0f\u89c4\u5212\u7684\u6587\u672c\u5230\u8fd0\u52a8\u5408\u6210", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u9886\u57df\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u975eLLM\u65b9\u6cd5\u3002\u6211\u4eec\u53d1\u73b0\u8fd0\u52a8\u6807\u8bb0\u5316\u7684\u7c92\u5ea6\u662f\u5173\u952e\u74f6\u9888\uff1a\u7ec6\u7c92\u5ea6\u6807\u8bb0\u5316\u4f1a\u5f15\u53d1\u5c40\u90e8\u4f9d\u8d56\u95ee\u9898\uff0c\u5bfc\u81f4LLM\u8fc7\u5ea6\u5f3a\u8c03\u77ed\u671f\u4e00\u81f4\u6027\u800c\u727a\u7272\u5168\u5c40\u8bed\u4e49\u5bf9\u9f50\uff1b\u800c\u7c97\u7c92\u5ea6\u6807\u8bb0\u5316\u5219\u4e22\u5931\u4e86\u8fd0\u52a8\u7ec6\u8282\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PlanMoGPT\uff0c\u4e00\u4e2a\u7ed3\u5408\u6e10\u8fdb\u5f0f\u89c4\u5212\u548c\u6d41\u589e\u5f3a\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u6807\u8bb0\u5316\u7684LLM\u6846\u67b6\u3002\u9996\u5148\uff0c\u6e10\u8fdb\u5f0f\u89c4\u5212\u673a\u5236\u5229\u7528LLM\u7684\u81ea\u56de\u5f52\u80fd\u529b\uff0c\u4ece\u7a00\u758f\u5168\u5c40\u8ba1\u5212\u5f00\u59cb\uff0c\u9010\u6b65\u7ec6\u5316\u751f\u6210\u5b8c\u6574\u8fd0\u52a8\u5e8f\u5217\u3002\u5176\u6b21\uff0c\u6d41\u589e\u5f3a\u6807\u8bb0\u5668\u5c06\u4e0b\u91c7\u6837\u5206\u8fa8\u7387\u63d0\u9ad8\u4e00\u500d\uff0c\u5e76\u5c06\u7801\u672c\u5927\u5c0f\u6269\u5c55\u516b\u500d\uff0c\u6700\u5c0f\u5316\u79bb\u6563\u5316\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u8282\u635f\u5931\uff0c\u540c\u65f6\u901a\u8fc7\u6d41\u589e\u5f3a\u89e3\u7801\u5668\u6062\u590d\u8fd0\u52a8\u7ec6\u8282\u3002\u5728\u6587\u672c\u5230\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPlanMoGPT\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u957f\u5e8f\u5217\u751f\u6210\u7684FID\u5206\u6570\u63d0\u5347\u4e8663.8%\uff08\u4ece0.380\u964d\u81f30.141\uff09\uff0c\u540c\u65f6\u8fd0\u52a8\u591a\u6837\u6027\u63d0\u9ad8\u4e8649.9%\u3002\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f53\u524d\u975eLLM\u65b9\u6cd5\u9762\u4e34\u7684\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4e86\u65b0\u7684\u6807\u51c6\u3002"}}
{"id": "2506.17251", "pdf": "https://arxiv.org/pdf/2506.17251", "abs": "https://arxiv.org/abs/2506.17251", "authors": ["Dongseok Lee", "Jimyung Hong", "Dongyoung Kim", "Jaehyung Kim"], "title": "Training-free LLM Verification via Recycling Few-shot Examples", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although LLMs have achieved remarkable performance, the inherent\nstochasticity of their reasoning process and varying conclusions present\nsignificant challenges. Majority voting or Best-of-N with external verification\nmodels has been explored to find the most promising solution among multiple LLM\noutputs. However, these approaches have certain limitations, such as limited\napplicability or the cost of an additional training step. To address this\nproblem, we propose a novel and effective framework that Recycles Few-shot\nexamples to verify LLM outputs (Referi). Our key idea is to additionally\nutilize the given few-shot examples to evaluate the candidate outputs of the\ntarget query, not only using them to generate outputs as the conventional\nfew-shot prompting setup. Specifically, Referi evaluates the generated outputs\nby combining two different scores, designed motivated from Bayes' rule, and\nsubsequently selects the candidate that is both confidently determined and\ncontextually coherent through a few additional LLM inferences. Experiments with\nthree different LLMs and across seven diverse tasks demonstrate that our\nframework significantly improves the accuracy of LLMs-achieving an average gain\nof 4.8%-through effective response selection, without additional training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684LLM\u9a8c\u8bc1\u6846\u67b6Referi\uff0c\u901a\u8fc7\u590d\u7528\u5c11\u91cf\u793a\u4f8b\u8bc4\u4f30\u5019\u9009\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5e73\u5747\u589e\u76ca\u8fbe4.8%\u3002", "motivation": "\u5c3d\u7ba1LLM\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u968f\u673a\u6027\u548c\u7ed3\u8bba\u7684\u4e0d\u4e00\u81f4\u6027\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u591a\u6570\u6295\u7968\u6216\u5916\u90e8\u9a8c\u8bc1\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u9002\u7528\u6027\u6709\u9650\u6216\u989d\u5916\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faReferi\u6846\u67b6\uff0c\u590d\u7528\u5c11\u91cf\u793a\u4f8b\u8bc4\u4f30\u5019\u9009\u8f93\u51fa\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u89c4\u5219\u8bbe\u8ba1\u4e24\u79cd\u8bc4\u5206\uff0c\u901a\u8fc7\u989d\u5916\u63a8\u7406\u9009\u62e9\u7f6e\u4fe1\u5ea6\u9ad8\u4e14\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u5019\u9009\u3002", "result": "\u5728\u4e09\u79cdLLM\u548c\u4e03\u9879\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReferi\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e73\u5747\u589e\u76ca4.8%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "Referi\u901a\u8fc7\u590d\u7528\u5c11\u91cf\u793a\u4f8b\u9a8c\u8bc1LLM\u8f93\u51fa\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3aLLM\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\u3002", "paper_title_zh": "\u901a\u8fc7\u590d\u7528\u5c11\u91cf\u793a\u4f8b\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684LLM\u9a8c\u8bc1", "abstract_zh": "\u5c3d\u7ba1LLM\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u56fa\u6709\u968f\u673a\u6027\u548c\u7ed3\u8bba\u7684\u591a\u6837\u6027\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u591a\u6570\u6295\u7968\u6216\u57fa\u4e8e\u5916\u90e8\u9a8c\u8bc1\u6a21\u578b\u7684Best-of-N\u65b9\u6cd5\u5df2\u88ab\u63a2\u7d22\uff0c\u4ee5\u4ece\u591a\u4e2aLLM\u8f93\u51fa\u4e2d\u5bfb\u627e\u6700\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u4e00\u5b9a\u5c40\u9650\u6027\uff0c\u5982\u9002\u7528\u6027\u6709\u9650\u6216\u989d\u5916\u8bad\u7ec3\u6b65\u9aa4\u7684\u6210\u672c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u6846\u67b6Referi\uff0c\u901a\u8fc7\u590d\u7528\u5c11\u91cf\u793a\u4f8b\u9a8c\u8bc1LLM\u8f93\u51fa\u3002\u6211\u4eec\u7684\u6838\u5fc3\u601d\u60f3\u662f\u989d\u5916\u5229\u7528\u7ed9\u5b9a\u7684\u5c11\u91cf\u793a\u4f8b\u8bc4\u4f30\u76ee\u6807\u67e5\u8be2\u7684\u5019\u9009\u8f93\u51fa\uff0c\u800c\u4e0d\u4ec5\u5c06\u5176\u7528\u4e8e\u751f\u6210\u8f93\u51fa\u3002\u5177\u4f53\u800c\u8a00\uff0cReferi\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u89c4\u5219\u8bbe\u8ba1\u7684\u8bc4\u5206\u6765\u8bc4\u4f30\u751f\u6210\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u989d\u5916LLM\u63a8\u7406\u9009\u62e9\u7f6e\u4fe1\u5ea6\u9ad8\u4e14\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u5019\u9009\u3002\u5728\u4e09\u79cd\u4e0d\u540cLLM\u548c\u4e03\u9879\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u6709\u6548\u54cd\u5e94\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u51c6\u786e\u6027\uff0c\u5e73\u5747\u589e\u76ca\u8fbe4.8%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2506.18485", "pdf": "https://arxiv.org/pdf/2506.18485", "abs": "https://arxiv.org/abs/2506.18485", "authors": ["Junjie Zhang", "Guozheng Ma", "Shunyu Liu", "Haoyu Wang", "Jiaxing Huang", "Ting-En Lin", "Fei Huang", "Yongbin Li", "Dacheng Tao"], "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMeRF\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5956\u52b1\u89c4\u5219\u76f4\u63a5\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08RLVR\uff09\u5ffd\u89c6\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5982\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u7684\u6210\u529f\u6240\u793a\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "MeRF\u65b9\u6cd5\u901a\u8fc7\u5c06\u5956\u52b1\u89c4\u8303\u76f4\u63a5\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u52a8\u673a\uff0c\u4f7f\u6a21\u578b\u5728\u4f18\u5316\u76ee\u6807\u610f\u8bc6\u4e0b\u6539\u8fdb\u5176\u54cd\u5e94\u3002\u8fd9\u79cd\u7b80\u5355\u4fee\u6539\u5229\u7528\u4e86LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5c06\u751f\u6210\u4e0e\u4f18\u5316\u5bf9\u9f50\uff0c\u6fc0\u52b1\u6a21\u578b\u4ece\u5185\u90e8\u52a8\u673a\u548c\u5916\u90e8\u5956\u52b1\u751f\u6210\u7406\u60f3\u8f93\u51fa\u3002", "result": "\u5728Knights and Knaves\uff08K&K\uff09\u903b\u8f91\u8c1c\u9898\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMeRF\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6027\u80fd\u63d0\u5347\u4e0e\u4e0a\u4e0b\u6587\u52a8\u673a\u548c\u5916\u90e8\u5956\u52b1\u51fd\u6570\u7684\u4e00\u81f4\u6027\u5bc6\u5207\u76f8\u5173\uff0c\u540c\u65f6\u6a21\u578b\u8fd8\u80fd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u8bef\u5bfc\u6027\u52a8\u673a\u3002", "conclusion": "MeRF\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5c06\u4f18\u5316\u76ee\u6807\u76f4\u63a5\u6ce8\u5165\u63d0\u793a\u7684\u7b80\u5355\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "paper_title_zh": "MeRF\uff1a\u57fa\u4e8e\u52a8\u673a\u589e\u5f3a\u7684\u5f3a\u5316\u5fae\u8c03\u7528\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b", "abstract_zh": "\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5df2\u6210\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6709\u529b\u5b66\u4e60\u8303\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u6709RLVR\u65b9\u6cd5\u5ffd\u89c6\u4e86LLM\u6700\u663e\u8457\u7684\u80fd\u529b\u4e4b\u4e00\u2014\u2014\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5982\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u7684\u6210\u529f\u6240\u793a\u3002\u8fd9\u4fc3\u4f7f\u6211\u4eec\u63a2\u7d22\u5982\u4f55\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u6709\u6548\u7ed3\u5408\uff0c\u4ee5\u66f4\u597d\u5730\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u52a8\u673a\u589e\u5f3a\u7684\u5f3a\u5316\u5fae\u8c03\uff08MeRF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u76f4\u89c2\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u201c\u544a\u8bc9LLM\u6e38\u620f\u89c4\u5219\u201d\u6765\u589e\u5f3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u3002\u5177\u4f53\u800c\u8a00\uff0cMeRF\u76f4\u63a5\u5c06\u5956\u52b1\u89c4\u8303\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u52a8\u673a\uff0c\u4f7f\u6a21\u578b\u5728\u4f18\u5316\u76ee\u6807\u610f\u8bc6\u4e0b\u6539\u8fdb\u5176\u54cd\u5e94\u3002\u8fd9\u79cd\u7b80\u5355\u4fee\u6539\u5229\u7528\u4e86LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5c06\u751f\u6210\u4e0e\u4f18\u5316\u5bf9\u9f50\uff0c\u4ece\u800c\u6fc0\u52b1\u6a21\u578b\u4ece\u5185\u90e8\u52a8\u673a\u548c\u5916\u90e8\u5956\u52b1\u751f\u6210\u7406\u60f3\u8f93\u51fa\u3002\u5728Knights and Knaves\uff08K&K\uff09\u903b\u8f91\u8c1c\u9898\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMeRF\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6027\u80fd\u63d0\u5347\u4e0e\u4e0a\u4e0b\u6587\u52a8\u673a\u548c\u5916\u90e8\u5956\u52b1\u51fd\u6570\u7684\u4e00\u81f4\u6027\u5bc6\u5207\u76f8\u5173\uff0c\u540c\u65f6\u6a21\u578b\u8fd8\u80fd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u8bef\u5bfc\u6027\u52a8\u673a\u3002"}}
{"id": "2506.17931", "pdf": "https://arxiv.org/pdf/2506.17931", "abs": "https://arxiv.org/abs/2506.17931", "authors": ["Ravi Kant Gupta", "Shounak Das", "Amit Sethi"], "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted in ICPR'24 (International Conference on Pattern Recognition)", "summary": "We present a novel approach for unsupervised domain adaptation (UDA) for\nnatural images. A commonly-used objective for UDA schemes is to enhance domain\nalignment in representation space even if there is a domain shift in the input\nspace. Existing adversarial domain adaptation methods may not effectively align\ndifferent domains of multimodal distributions associated with classification\nproblems. Our approach has two main features. Firstly, its neural architecture\nuses the deep structure of ResNet and the effective separation of scales of\nfeature pyramidal network (FPN) to work with both content and style features.\nSecondly, it uses a combination of a novel loss function and judiciously\nselected existing loss functions to train the network architecture. This\ntailored combination is designed to address challenges inherent to natural\nimages, such as scale, noise, and style shifts, that occur on top of a\nmulti-modal (multi-class) distribution. The combined loss function not only\nenhances model accuracy and robustness on the target domain but also speeds up\ntraining convergence. Our proposed UDA scheme generalizes better than\nstate-of-the-art for CNN-based methods on Office-Home, Office-31, and\nVisDA-2017 datasets and comaparable for DomainNet dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u5b66\u4e60\u65b9\u6cd5IDAL\uff0c\u901a\u8fc7\u7ed3\u5408ResNet\u548cFPN\u7684\u6df1\u5ea6\u7ed3\u6784\u4ee5\u53ca\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u57df\u7684\u6a21\u578b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u5206\u5e03\u7684\u5206\u7c7b\u95ee\u9898\u65f6\uff0c\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\u4e0d\u540c\u57df\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u89e3\u51b3\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u56e0\u5c3a\u5ea6\u3001\u566a\u58f0\u548c\u98ce\u683c\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86ResNet\u7684\u6df1\u5ea6\u7ed3\u6784\u548cFPN\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5206\u79bb\u80fd\u529b\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\u4e0e\u73b0\u6709\u635f\u5931\u51fd\u6570\u7684\u7ec4\u5408\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u8868\u73b0\u3002", "result": "\u5728Office-Home\u3001Office-31\u548cVisDA-2017\u6570\u636e\u96c6\u4e0a\uff0cIDAL\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\uff0c\u5728DomainNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "IDAL\u901a\u8fc7\u6539\u8fdb\u7684\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u65f6\u8868\u73b0\u51fa\u8272\u3002", "paper_title_zh": "IDAL\uff1a\u6539\u8fdb\u7684\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u57df\u81ea\u9002\u5e94\u5b66\u4e60\u65b9\u6cd5", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u7136\u56fe\u50cf\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u7684\u65b0\u65b9\u6cd5\u3002UDA\u65b9\u6848\u7684\u5e38\u7528\u76ee\u6807\u662f\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u589e\u5f3a\u57df\u5bf9\u9f50\uff0c\u5373\u4f7f\u8f93\u5165\u7a7a\u95f4\u5b58\u5728\u57df\u504f\u79fb\u3002\u73b0\u6709\u7684\u5bf9\u6297\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\u4e0e\u5206\u7c7b\u95ee\u9898\u76f8\u5173\u7684\u591a\u6a21\u6001\u5206\u5e03\u7684\u4e0d\u540c\u57df\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u4e24\u4e2a\u4e3b\u8981\u7279\u70b9\u3002\u9996\u5148\uff0c\u5176\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5229\u7528\u4e86ResNet\u7684\u6df1\u5ea6\u7ed3\u6784\u548c\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08FPN\uff09\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5206\u79bb\u80fd\u529b\uff0c\u540c\u65f6\u5904\u7406\u5185\u5bb9\u548c\u98ce\u683c\u7279\u5f81\u3002\u5176\u6b21\uff0c\u5b83\u7ed3\u5408\u4e86\u4e00\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\u548c\u7cbe\u5fc3\u9009\u62e9\u7684\u73b0\u6709\u635f\u5931\u51fd\u6570\u6765\u8bad\u7ec3\u7f51\u7edc\u67b6\u6784\u3002\u8fd9\u79cd\u5b9a\u5236\u7ec4\u5408\u65e8\u5728\u89e3\u51b3\u81ea\u7136\u56fe\u50cf\u4e2d\u56fa\u6709\u7684\u6311\u6218\uff0c\u5982\u5c3a\u5ea6\u3001\u566a\u58f0\u548c\u98ce\u683c\u53d8\u5316\uff0c\u8fd9\u4e9b\u6311\u6218\u51fa\u73b0\u5728\u591a\u6a21\u6001\uff08\u591a\u7c7b\uff09\u5206\u5e03\u4e4b\u4e0a\u3002\u7ec4\u5408\u635f\u5931\u51fd\u6570\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u76ee\u6807\u57df\u4e0a\u7684\u6a21\u578b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8fd8\u52a0\u5feb\u4e86\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u7684UDA\u65b9\u6848\u5728Office-Home\u3001Office-31\u548cVisDA-2017\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u4e8eCNN\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u5728DomainNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\u3002"}}
{"id": "2506.17252", "pdf": "https://arxiv.org/pdf/2506.17252", "abs": "https://arxiv.org/abs/2506.17252", "authors": ["Zixuan Huang", "Yikun Ban", "Lean Fu", "Xiaojie Li", "Zhongxiang Dai", "Jianxin Li", "Deqing Wang"], "title": "Adaptive Sample Scheduling for Direct Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as an effective approach for\naligning large language models (LLMs) with human preferences. However, its\nperformance is highly dependent on the quality of the underlying human\npreference data. To address this bottleneck, prior work has explored various\ndata selection strategies, but these methods often overlook the impact of the\nevolving states of the language model during the DPO process. %including active\nquerying, response pair selection, and data pre-selection. In this paper, we\nintroduce a novel problem: Sample Scheduling for DPO, which aims to dynamically\nand adaptively schedule training samples based on the model's evolving states\nthroughout preference optimization. To solve this problem, we propose SamS, an\nefficient and effective algorithm that adaptively selects samples in each\ntraining batch based on the LLM's learning feedback to maximize the potential\ngeneralization performance. Notably, without modifying the core DPO algorithm,\nsimply integrating SamS significantly improves performance across tasks, with\nminimal additional computational overhead. This work points to a promising new\ndirection for improving LLM alignment through more effective utilization of\nfixed preference datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSamS\u7684\u81ea\u9002\u5e94\u6837\u672c\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8fc7\u7a0b\u4e2d\u8bad\u7ec3\u6837\u672c\u7684\u52a8\u6001\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u6570\u636e\u7684\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u6a21\u578b\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u72b6\u6001\u53d8\u5316\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u6837\u672c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51faSamS\u7b97\u6cd5\uff0c\u6839\u636e\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u53cd\u9988\u52a8\u6001\u9009\u62e9\u6bcf\u6279\u8bad\u7ec3\u6837\u672c\uff0c\u65e0\u9700\u4fee\u6539DPO\u6838\u5fc3\u7b97\u6cd5\uff0c\u4ec5\u901a\u8fc7\u6837\u672c\u8c03\u5ea6\u5373\u53ef\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSamS\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "SamS\u4e3a\u901a\u8fc7\u66f4\u6709\u6548\u5229\u7528\u56fa\u5b9a\u504f\u597d\u6570\u636e\u96c6\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u81ea\u9002\u5e94\u6837\u672c\u8c03\u5ea6", "abstract_zh": "\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5df2\u6210\u4e3a\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u5176\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u6570\u636e\u7684\u8d28\u91cf\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u5148\u524d\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86DPO\u8fc7\u7a0b\u4e2d\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u72b6\u6001\u7684\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u95ee\u9898\uff1aDPO\u7684\u6837\u672c\u8c03\u5ea6\uff0c\u65e8\u5728\u57fa\u4e8e\u6a21\u578b\u5728\u504f\u597d\u4f18\u5316\u4e2d\u7684\u52a8\u6001\u72b6\u6001\u81ea\u9002\u5e94\u8c03\u5ea6\u8bad\u7ec3\u6837\u672c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SamS\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6839\u636eLLM\u7684\u5b66\u4e60\u53cd\u9988\u52a8\u6001\u9009\u62e9\u6bcf\u6279\u8bad\u7ec3\u6837\u672c\uff0c\u4ee5\u6700\u5927\u5316\u6cdb\u5316\u6027\u80fd\u6f5c\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u65e0\u9700\u4fee\u6539DPO\u6838\u5fc3\u7b97\u6cd5\uff0c\u4ec5\u901a\u8fc7\u96c6\u6210SamS\u5373\u53ef\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u901a\u8fc7\u66f4\u6709\u6548\u5229\u7528\u56fa\u5b9a\u504f\u597d\u6570\u636e\u96c6\u6539\u8fdbLLM\u5bf9\u9f50\u6307\u660e\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.18501", "pdf": "https://arxiv.org/pdf/2506.18501", "abs": "https://arxiv.org/abs/2506.18501", "authors": ["Wael Etaiwi", "Bushra Alhijawi"], "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9ChatGPT\u548cDeepSeek\u5728\u4e94\u4e2a\u5173\u952eNLP\u4efb\u52a1\uff08\u60c5\u611f\u5206\u6790\u3001\u4e3b\u9898\u5206\u7c7b\u3001\u6587\u672c\u6458\u8981\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u6587\u672c\u8574\u542b\uff09\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u53d1\u73b0DeepSeek\u5728\u5206\u7c7b\u7a33\u5b9a\u6027\u548c\u903b\u8f91\u63a8\u7406\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800cChatGPT\u5728\u9700\u8981\u7ec6\u5fae\u7406\u89e3\u548c\u7075\u6d3b\u6027\u7684\u4efb\u52a1\u4e2d\u66f4\u80dc\u4e00\u7b79\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002ChatGPT\u548cDeepSeek\u5728\u8bb8\u591aNLP\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7ed3\u6784\u5316\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u548c\u51cf\u5c11\u53d8\u5f02\u6027\u3002\u4e24\u79cd\u6a21\u578b\u5728\u76f8\u540c\u7684\u4e2d\u6027\u63d0\u793a\u4e0b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u5728\u6bcf\u4e2a\u4efb\u52a1\u7684\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6\u65b0\u95fb\u3001\u8bc4\u8bba\u548c\u6b63\u5f0f/\u975e\u6b63\u5f0f\u6587\u672c\u7b49\u9886\u57df\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cDeepSeek\u5728\u5206\u7c7b\u7a33\u5b9a\u6027\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cChatGPT\u5728\u9700\u8981\u7ec6\u5fae\u7406\u89e3\u548c\u7075\u6d3b\u6027\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684LLM\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4e92\u8865\u6027\u3002", "paper_title_zh": "ChatGPT\u4e0eDeepSeek\u5728\u5173\u952eNLP\u4efb\u52a1\u4e2d\u7684\u6bd4\u8f83\u8bc4\u4f30\uff1a\u4f18\u52bf\u3001\u5f31\u70b9\u53ca\u9886\u57df\u7279\u5b9a\u8868\u73b0", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u5b83\u4eec\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u6548\u679c\u7684\u8bc4\u4f30\u5174\u8da3\u3002\u5c3d\u7ba1ChatGPT\u548cDeepSeek\u5728\u8bb8\u591aNLP\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9700\u5168\u9762\u8bc4\u4f30\u4ee5\u4e86\u89e3\u5176\u4f18\u52bf\u3001\u5f31\u70b9\u53ca\u9886\u57df\u7279\u5b9a\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e24\u79cd\u6a21\u578b\u5728\u4e94\u4e2a\u5173\u952eNLP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u60c5\u611f\u5206\u6790\u3001\u4e3b\u9898\u5206\u7c7b\u3001\u6587\u672c\u6458\u8981\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u6587\u672c\u8574\u542b\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u5e76\u51cf\u5c11\u53d8\u5f02\u6027\u3002\u4e24\u79cd\u6a21\u578b\u5728\u76f8\u540c\u7684\u4e2d\u6027\u63d0\u793a\u4e0b\u6d4b\u8bd5\uff0c\u5e76\u5728\u6bcf\u4e2a\u4efb\u52a1\u7684\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6\u65b0\u95fb\u3001\u8bc4\u8bba\u548c\u6b63\u5f0f/\u975e\u6b63\u5f0f\u6587\u672c\u7b49\u9886\u57df\u3002\u7ed3\u679c\u663e\u793a\uff0cDeepSeek\u5728\u5206\u7c7b\u7a33\u5b9a\u6027\u548c\u903b\u8f91\u63a8\u7406\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800cChatGPT\u5728\u9700\u8981\u7ec6\u5fae\u7406\u89e3\u548c\u7075\u6d3b\u6027\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684LLM\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2506.17939", "pdf": "https://arxiv.org/pdf/2506.17939", "abs": "https://arxiv.org/abs/2506.17939", "authors": ["Bo Liu", "Xiangyu Zhao", "Along He", "Yidi Chen", "Huazhu Fu", "Xiao-Ming Wu"], "title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Work in Progress", "summary": "Medical visual question answering aims to support clinical decision-making by\nenabling models to answer natural language questions based on medical images.\nWhile recent advances in multi-modal learning have significantly improved\nperformance, current methods still suffer from limited answer reliability and\npoor interpretability, impairing the ability of clinicians and patients to\nunderstand and trust model-generated answers. To address this, this work first\nproposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer\ngeneration is decomposed into intermediate reasoning steps that explicitly\nground relevant visual regions of the medical image, thereby providing\nfine-grained explainability. Furthermore, we introduce a novel verifiable\nreward mechanism for reinforcement learning to guide post-training, improving\nthe alignment between the model's reasoning process and its final answer.\nRemarkably, our method achieves comparable performance using only one-eighth of\nthe training data, demonstrating the efficiency and effectiveness of the\nproposal. The dataset is available at\nhttps://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aThinkVG\u7684\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u516b\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u5b58\u5728\u7b54\u6848\u53ef\u9760\u6027\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u533b\u751f\u548c\u60a3\u8005\u5bf9\u6a21\u578b\u751f\u6210\u7b54\u6848\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "method": "1. \u63d0\u51faThinkVG\u6570\u636e\u96c6\uff0c\u5c06\u7b54\u6848\u751f\u6210\u5206\u89e3\u4e3a\u663e\u5f0f\u5173\u8054\u533b\u5b66\u56fe\u50cf\u533a\u57df\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff1b2. \u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u4f18\u5316\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700\u516b\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7ThinkVG\u6570\u636e\u96c6\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "GEMeX-ThinkVG\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u89c6\u89c9\u5173\u8054\u63a8\u7406", "abstract_zh": "\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u57fa\u4e8e\u533b\u5b66\u56fe\u50cf\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u6765\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002\u5c3d\u7ba1\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u5b58\u5728\u7b54\u6848\u53ef\u9760\u6027\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u4e34\u5e8a\u533b\u751f\u548c\u60a3\u8005\u5bf9\u6a21\u578b\u751f\u6210\u7b54\u6848\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u89c6\u89c9\u5173\u8054\u63a8\u7406\u201d\uff08ThinkVG\uff09\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u7b54\u6848\u751f\u6210\u88ab\u5206\u89e3\u4e3a\u663e\u5f0f\u5173\u8054\u533b\u5b66\u56fe\u50cf\u76f8\u5173\u533a\u57df\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u4ece\u800c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u7528\u4e8e\u6307\u5bfc\u540e\u8bad\u7ec3\uff0c\u4f18\u5316\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u9700\u516b\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u63d0\u6848\u7684\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002\u6570\u636e\u96c6\u53ef\u5728https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG\u83b7\u53d6\u3002"}}
{"id": "2506.17253", "pdf": "https://arxiv.org/pdf/2506.17253", "abs": "https://arxiv.org/abs/2506.17253", "authors": ["Chenghan Li", "Mingchen Li", "Yipu Liao", "Ruisheng Diao"], "title": "MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-term time series prediction has predominantly relied on Transformer and\nMLP models, while the potential of convolutional networks in this domain\nremains underexplored. To address this gap, we introduce a novel multi-scale\ntime series reshape module, which effectively captures the relationships among\nmulti-period patches and variable dependencies. Building upon this module, we\npropose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.\nThrough comprehensive evaluations on diverse datasets, MS-TVNet demonstrates\nsuperior performance compared to baseline models, achieving state-of-the-art\n(SOTA) results in long-term time series prediction. Our findings highlight the\neffectiveness of leveraging convolutional networks for capturing complex\ntemporal patterns, suggesting a promising direction for future research in this\nfield.The code is realsed on https://github.com/Curyyfaust/TVNet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u52a8\u6001\u5377\u79ef\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5MS-TVNet\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u91cd\u5851\u6a21\u5757\u6355\u6349\u591a\u5468\u671f\u7247\u6bb5\u548c\u53d8\u91cf\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e3b\u8981\u4f9d\u8d56Transformer\u548cMLP\u6a21\u578b\uff0c\u800c\u5377\u79ef\u7f51\u7edc\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5377\u79ef\u7f51\u7edc\u5728\u6355\u6349\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u91cd\u5851\u6a21\u5757\uff0c\u7528\u4e8e\u6355\u6349\u591a\u5468\u671f\u7247\u6bb5\u548c\u53d8\u91cf\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86MS-TVNet\uff0c\u4e00\u79cd\u591a\u5c3a\u5ea63D\u52a8\u6001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cMS-TVNet\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5377\u79ef\u7f51\u7edc\u5728\u6355\u6349\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "MS-TVNet\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u52a8\u6001\u5377\u79ef\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5", "abstract_zh": "\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e3b\u8981\u4f9d\u8d56\u4e8eTransformer\u548cMLP\u6a21\u578b\uff0c\u800c\u5377\u79ef\u7f51\u7edc\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u91cd\u5851\u6a21\u5757\uff0c\u6709\u6548\u6355\u6349\u591a\u5468\u671f\u7247\u6bb5\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u53d8\u91cf\u4f9d\u8d56\u6027\u3002\u57fa\u4e8e\u6b64\u6a21\u5757\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MS-TVNet\uff0c\u4e00\u79cd\u591a\u5c3a\u5ea63D\u52a8\u6001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u901a\u8fc7\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u7684\u7efc\u5408\u8bc4\u4f30\uff0cMS-TVNet\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u6c34\u5e73\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u5377\u79ef\u7f51\u7edc\u6355\u6349\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/Curyyfaust/TVNet\u3002"}}
{"id": "2506.18532", "pdf": "https://arxiv.org/pdf/2506.18532", "abs": "https://arxiv.org/abs/2506.18532", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bann\u00f2", "Mark J. F. Gales", "Kate M. Knill"], "title": "End-to-End Spoken Grammatical Error Correction", "categories": ["cs.CL", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Grammatical Error Correction (GEC) and feedback play a vital role in\nsupporting second language (L2) learners, educators, and examiners. While\nwritten GEC is well-established, spoken GEC (SGEC), aiming to provide feedback\nbased on learners' speech, poses additional challenges due to disfluencies,\ntranscription errors, and the lack of structured input. SGEC systems typically\nfollow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),\ndisfluency detection, and GEC, making them vulnerable to error propagation\nacross modules. This work examines an End-to-End (E2E) framework for SGEC and\nfeedback generation, highlighting challenges and possible solutions when\ndeveloping these systems. Cascaded, partial-cascaded and E2E architectures are\ncompared, all built on the Whisper foundation model. A challenge for E2E\nsystems is the scarcity of GEC labeled spoken data. To address this, an\nautomatic pseudo-labeling framework is examined, increasing the training data\nfrom 77 to over 2500 hours. To improve the accuracy of the SGEC system,\nadditional contextual information, exploiting the ASR output, is investigated.\nCandidate feedback of their mistakes is an essential step to improving\nperformance. In E2E systems the SGEC output must be compared with an estimate\nof the fluent transcription to obtain the feedback. To improve the precision of\nthis feedback, a novel reference alignment process is proposed that aims to\nremove hypothesised edits that results from fluent transcription errors.\nFinally, these approaches are combined with an edit confidence estimation\napproach, to exclude low-confidence edits. Experiments on the in-house\nLinguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)\ncorpus show that the proposed approaches significantly boost E2E SGEC\nperformance.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\uff08E2E\uff09\u7684\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\uff08SGEC\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ea7\u8054\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f2a\u6807\u6ce8\u548c\u6570\u636e\u589e\u5f3a\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\uff08SGEC\uff09\u5bf9\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u5b66\u4e60\u8005\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u7ea7\u8054\u6a21\u5757\uff0c\u6613\u53d7\u9519\u8bef\u4f20\u64ad\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7ea7\u8054\u3001\u90e8\u5206\u7ea7\u8054\u548c\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u57fa\u4e8eWhisper\u57fa\u7840\u6a21\u578b\u6784\u5efa\u3002\u901a\u8fc7\u4f2a\u6807\u6ce8\u5c06\u8bad\u7ec3\u6570\u636e\u4ece77\u5c0f\u65f6\u589e\u81f32500\u5c0f\u65f6\u4ee5\u4e0a\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f18\u5316\u53cd\u9988\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728Linguaskill\u548cSpeak & Improve\u8bed\u6599\u5e93\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aefSGEC\u7684\u6027\u80fd\u3002", "conclusion": "\u7aef\u5230\u7aefSGEC\u6846\u67b6\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u53cd\u9988\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u7aef\u5230\u7aef\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519", "abstract_zh": "\u8bed\u6cd5\u7ea0\u9519\uff08GEC\uff09\u548c\u53cd\u9988\u5bf9\u652f\u6301\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u5b66\u4e60\u8005\u3001\u6559\u80b2\u8005\u548c\u8003\u5b98\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u4e66\u9762GEC\u5df2\u8f83\u4e3a\u6210\u719f\uff0c\u4f46\u53e3\u8bedGEC\uff08SGEC\uff09\u7531\u4e8e\u4e0d\u6d41\u7545\u3001\u8f6c\u5f55\u9519\u8bef\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u8f93\u5165\u800c\u9762\u4e34\u66f4\u591a\u6311\u6218\u3002SGEC\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u3001\u4e0d\u6d41\u7545\u68c0\u6d4b\u548cGEC\u7684\u7ea7\u8054\u6d41\u7a0b\uff0c\u5bb9\u6613\u53d7\u6a21\u5757\u95f4\u9519\u8bef\u4f20\u64ad\u7684\u5f71\u54cd\u3002\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\uff08E2E\uff09\u7684SGEC\u548c\u53cd\u9988\u751f\u6210\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u5f00\u53d1\u6b64\u7c7b\u7cfb\u7edf\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002\u6bd4\u8f83\u4e86\u57fa\u4e8eWhisper\u57fa\u7840\u6a21\u578b\u7684\u7ea7\u8054\u3001\u90e8\u5206\u7ea7\u8054\u548cE2E\u67b6\u6784\u3002E2E\u7cfb\u7edf\u9762\u4e34GEC\u6807\u6ce8\u53e3\u8bed\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u6b64\u7814\u7a76\u4e86\u81ea\u52a8\u4f2a\u6807\u6ce8\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u4ece77\u5c0f\u65f6\u589e\u81f32500\u5c0f\u65f6\u4ee5\u4e0a\u3002\u4e3a\u63d0\u9ad8SGEC\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u63a2\u7d22\u4e86\u5229\u7528ASR\u8f93\u51fa\u7684\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u53cd\u9988\u751f\u6210\u662f\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\u6b65\u9aa4\uff0cE2E\u7cfb\u7edf\u4e2d\u9700\u5c06SGEC\u8f93\u51fa\u4e0e\u6d41\u7545\u8f6c\u5f55\u4f30\u8ba1\u8fdb\u884c\u6bd4\u8f83\u4ee5\u751f\u6210\u53cd\u9988\u3002\u4e3a\u63d0\u9ad8\u53cd\u9988\u7cbe\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u8003\u5bf9\u9f50\u6d41\u7a0b\uff0c\u65e8\u5728\u6d88\u9664\u7531\u6d41\u7545\u8f6c\u5f55\u9519\u8bef\u5bfc\u81f4\u7684\u5047\u8bbe\u7f16\u8f91\u3002\u6700\u540e\uff0c\u7ed3\u5408\u7f16\u8f91\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6392\u9664\u4f4e\u7f6e\u4fe1\u5ea6\u7f16\u8f91\u3002\u5728\u5185\u90e8Linguaskill\uff08LNG\uff09\u8bed\u6599\u5e93\u548c\u516c\u5f00\u7684Speak & Improve\uff08S&I\uff09\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86E2E SGEC\u7684\u6027\u80fd\u3002"}}
{"id": "2506.17944", "pdf": "https://arxiv.org/pdf/2506.17944", "abs": "https://arxiv.org/abs/2506.17944", "authors": ["Fei Zhou"], "title": "SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing change detection is widely used in a variety of fields such as\nurban planning, terrain and geomorphology analysis, and environmental\nmonitoring, mainly by analyzing the significant change differences of features\n(e.g., building changes) in the same spatial region at different time phases.\nIn this paper, we propose a large language model (LLM) augmented inference\napproach (SegChange-R1), which enhances the detection capability by integrating\ntextual descriptive information and aims at guiding the model to segment the\nmore interested change regions, thus accelerating the convergence speed.\nMoreover, we design a spatial transformation module (BEV) based on linear\nattention, which solves the problem of modal misalignment in change detection\nby unifying features from different temporal perspectives onto the BEV space.\nIn addition, we construct the first dataset for building change detection from\nUAV viewpoints (DVCD ), and our experiments on four widely-used change\ndetection datasets show a significant improvement over existing methods. The\ncode and pre-trained models are available in\nhttps://github.com/Yu-Zhouz/SegChange-R1.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5SegChange-R1\uff0c\u7528\u4e8e\u9065\u611f\u53d8\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4fe1\u606f\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u7a7a\u95f4\u53d8\u6362\u6a21\u5757\uff08BEV\uff09\u89e3\u51b3\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5728\u57ce\u4e61\u89c4\u5212\u3001\u5730\u5f62\u5730\u8c8c\u5206\u6790\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u80fd\u529b\u548c\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faSegChange-R1\u65b9\u6cd5\uff0c\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4fe1\u606f\u4ee5\u589e\u5f3a\u68c0\u6d4b\u80fd\u529b\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u7ebf\u6027\u6ce8\u610f\u529b\u7684BEV\u6a21\u5757\uff0c\u7edf\u4e00\u4e0d\u540c\u65f6\u95f4\u89c6\u89d2\u7684\u7279\u5f81\u5230BEV\u7a7a\u95f4\uff1b\u6784\u5efa\u9996\u4e2a\u65e0\u4eba\u673a\u89c6\u89d2\u7684\u5efa\u7b51\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\uff08DVCD\uff09\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cSegChange-R1\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SegChange-R1\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u63a8\u7406\u548cBEV\u6a21\u5757\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "SegChange-R1\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5", "abstract_zh": "\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u57ce\u4e61\u89c4\u5212\u3001\u5730\u5f62\u5730\u8c8c\u5206\u6790\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\uff0c\u4e3b\u8981\u901a\u8fc7\u5206\u6790\u540c\u4e00\u7a7a\u95f4\u533a\u57df\u5728\u4e0d\u540c\u65f6\u95f4\u9636\u6bb5\u7684\u7279\u5f81\uff08\u5982\u5efa\u7b51\u53d8\u5316\uff09\u7684\u663e\u8457\u5dee\u5f02\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5\uff08SegChange-R1\uff09\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4fe1\u606f\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\uff0c\u65e8\u5728\u5f15\u5bfc\u6a21\u578b\u5206\u5272\u66f4\u611f\u5174\u8da3\u7684\u53d8\u5316\u533a\u57df\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u7a7a\u95f4\u53d8\u6362\u6a21\u5757\uff08BEV\uff09\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u65f6\u95f4\u89c6\u89d2\u7684\u7279\u5f81\u7edf\u4e00\u5230BEV\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002\u6211\u4eec\u8fd8\u6784\u5efa\u4e86\u9996\u4e2a\u65e0\u4eba\u673a\u89c6\u89d2\u7684\u5efa\u7b51\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\uff08DVCD\uff09\u3002\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728https://github.com/Yu-Zhouz/SegChange-R1\u83b7\u53d6\u3002"}}
{"id": "2506.17254", "pdf": "https://arxiv.org/pdf/2506.17254", "abs": "https://arxiv.org/abs/2506.17254", "authors": ["Shaoang Li", "Jian Li"], "title": "Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid pace at which new large language models (LLMs) appear -- and older\nones become obsolete -- forces LLM service providers to juggle a streaming\ninventory of models while respecting tight deployment capacity and per-query\ncost budgets. We cast the reality as an online decision problem that couples\nstage-wise deployment, made at fixed maintenance windows, with per-query\nrouting among the models kept live. We introduce StageRoute, a hierarchical\nalgorithm that (i) optimistically selects up to $M_max$ models for the next\nstage using reward upper-confidence and cost lower-confidence bounds, then (ii)\nsolves a budget-constrained bandit sub-problem to route each incoming query. We\nprove that StageRoute achieves a regret of order $T^{2/3}$ and provide a\nmatching lower bound, thereby establishing its near-optimality. Moreover, our\nexperiments confirm the theory, demonstrating that StageRoute performs close to\nthe optimum in practical settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStageRoute\u7684\u5206\u5c42\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u90e8\u7f72\u548c\u8def\u7531\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u5feb\u901f\u66f4\u65b0\u4e0e\u8d44\u6e90\u9650\u5236\u4e4b\u95f4\u7684\u77db\u76fe\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u4e50\u89c2\u9009\u62e9\u6a21\u578b\u548c\u9884\u7b97\u7ea6\u675f\u7684\u8def\u7531\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u65ad\u6d8c\u73b0\uff0c\u65e7\u6a21\u578b\u8fc5\u901f\u8fc7\u65f6\uff0c\u670d\u52a1\u63d0\u4f9b\u5546\u9700\u8981\u5728\u6709\u9650\u7684\u90e8\u7f72\u5bb9\u91cf\u548c\u67e5\u8be2\u6210\u672c\u9884\u7b97\u4e0b\uff0c\u52a8\u6001\u7ba1\u7406\u6a21\u578b\u5e93\u5b58\u3002\u8fd9\u4e00\u73b0\u5b9e\u95ee\u9898\u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u9636\u6bb5\u6027\u7684\u6a21\u578b\u90e8\u7f72\u548c\u5b9e\u65f6\u67e5\u8be2\u8def\u7531\u3002", "method": "\u8bba\u6587\u63d0\u51faStageRoute\u7b97\u6cd5\uff0c\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a(i) \u4f7f\u7528\u5956\u52b1\u4e0a\u754c\u548c\u6210\u672c\u4e0b\u754c\u4e50\u89c2\u9009\u62e9\u4e0b\u4e00\u9636\u6bb5\u7684\u6a21\u578b\uff1b(ii) \u901a\u8fc7\u9884\u7b97\u7ea6\u675f\u7684\u591a\u81c2\u8001\u864e\u673a\u5b50\u95ee\u9898\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u8def\u7531\u3002\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u5b9e\u73b0\u4e86$T^{2/3}$\u7684\u9057\u61be\u4e0a\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u3002", "result": "StageRoute\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5176\u9057\u61be\u4e0a\u754c\u4e3a$T^{2/3}$\uff0c\u4e14\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u89e3\u3002", "conclusion": "StageRoute\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u90e8\u7f72\u548c\u8def\u7531\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21LLM\u670d\u52a1\u4e2d\u52a8\u6001\u7ba1\u7406\u6a21\u578b\uff0c\u540c\u65f6\u6ee1\u8db3\u8d44\u6e90\u7ea6\u675f\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u7684\u53cc\u91cd\u4f18\u52bf\u3002", "paper_title_zh": "\u7d27\u8ddf\u6a21\u578b\u6b65\u4f10\uff1a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u5728\u7ebf\u90e8\u7f72\u4e0e\u8def\u7531", "abstract_zh": "\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5feb\u901f\u6d8c\u73b0\uff0c\u800c\u65e7\u6a21\u578b\u8fc5\u901f\u8fc7\u65f6\uff0c\u8fd9\u8feb\u4f7fLLM\u670d\u52a1\u63d0\u4f9b\u5546\u5728\u6709\u9650\u7684\u90e8\u7f72\u5bb9\u91cf\u548c\u6bcf\u67e5\u8be2\u6210\u672c\u9884\u7b97\u4e0b\uff0c\u52a8\u6001\u7ba1\u7406\u6d41\u5f0f\u6a21\u578b\u5e93\u5b58\u3002\u6211\u4eec\u5c06\u8fd9\u4e00\u73b0\u5b9e\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408\u9636\u6bb5\u6027\u7684\u90e8\u7f72\uff08\u5728\u56fa\u5b9a\u7ef4\u62a4\u7a97\u53e3\u5b8c\u6210\uff09\u548c\u5b9e\u65f6\u67e5\u8be2\u8def\u7531\u3002\u6211\u4eec\u63d0\u51fa\u4e86StageRoute\uff0c\u4e00\u79cd\u5206\u5c42\u7b97\u6cd5\uff1a(i) \u4f7f\u7528\u5956\u52b1\u4e0a\u754c\u548c\u6210\u672c\u4e0b\u754c\u4e50\u89c2\u9009\u62e9\u4e0b\u4e00\u9636\u6bb5\u6700\u591a$M_max$\u4e2a\u6a21\u578b\uff1b(ii) \u901a\u8fc7\u9884\u7b97\u7ea6\u675f\u7684\u591a\u81c2\u8001\u864e\u673a\u5b50\u95ee\u9898\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u8def\u7531\u3002\u6211\u4eec\u8bc1\u660e\u4e86StageRoute\u7684\u9057\u61be\u4e0a\u754c\u4e3a$T^{2/3}$\uff0c\u5e76\u63d0\u4f9b\u4e86\u5339\u914d\u7684\u4e0b\u754c\uff0c\u4ece\u800c\u786e\u7acb\u4e86\u5176\u63a5\u8fd1\u6700\u4f18\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u8868\u660eStageRoute\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u3002"}}
{"id": "2506.18535", "pdf": "https://arxiv.org/pdf/2506.18535", "abs": "https://arxiv.org/abs/2506.18535", "authors": ["Manu Pande", "Shahil Kumar", "Anay Yatin Damle"], "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728MS MARCO\u6bb5\u843d\u6392\u5e8f\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u6240\u6709\u5fae\u8c03\u65b9\u6cd5\u5747\u4e0d\u5982\u57fa\u7840\u6a21\u578b\u8868\u73b0\u597d\u3002", "motivation": "\u63a2\u8ba8\u4e3a\u4f55\u5728MS MARCO\u6bb5\u843d\u6392\u5e8f\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u6311\u6218\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u4e94\u79cd\u6a21\u578b\u53d8\u4f53\uff08\u5305\u62ec\u5168\u53c2\u6570\u5fae\u8c03\u548c\u53c2\u6570\u9ad8\u6548\u7684LoRA\u9002\u914d\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u5fae\u8c03\u5bf9\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528UMAP\u53ef\u89c6\u5316\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u6240\u6709\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u5747\u4e0d\u5982\u57fa\u7840\u6a21\u578b\uff08MRR@10: 0.3026\uff09\uff0c\u5fae\u8c03\u7834\u574f\u4e86\u57fa\u7840\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u5230\u7684\u6700\u4f18\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5728\u9971\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u80fd\u65e0\u6548\uff0c\u9700\u8981\u67b6\u6784\u521b\u65b0\u4ee5\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "paper_title_zh": "\u5f53\u5fae\u8c03\u5931\u8d25\u65f6\uff1a\u6765\u81eaMS MARCO\u6bb5\u843d\u6392\u5e8f\u7684\u6559\u8bad", "abstract_zh": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u53cd\u76f4\u89c9\u73b0\u8c61\uff1a\u5728MS MARCO\u6bb5\u843d\u6392\u5e8f\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u901a\u8fc7\u6d89\u53ca\u4e94\u79cd\u6a21\u578b\u53d8\u4f53\uff08\u5305\u62ec\u5168\u53c2\u6570\u5fae\u8c03\u548c\u53c2\u6570\u9ad8\u6548\u7684LoRA\u9002\u914d\uff09\u7684\u7efc\u5408\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u6240\u6709\u5fae\u8c03\u65b9\u6cd5\u7684\u8868\u73b0\u5747\u4e0d\u5982\u57fa\u7840\u6a21\u578bsentence-transformers/all-MiniLM-L6-v2\uff08MRR@10: 0.3026\uff09\u3002\u5206\u6790\u8868\u660e\uff0c\u5fae\u8c03\u7834\u574f\u4e86\u57fa\u7840\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u5230\u7684\u6700\u4f18\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u8be5\u9884\u8bad\u7ec3\u6d89\u53ca10\u4ebf\u53e5\u5bf9\uff08\u5305\u62ec910\u4e07MS MARCO\u6837\u672c\uff09\u3002UMAP\u53ef\u89c6\u5316\u663e\u793a\u5d4c\u5165\u7a7a\u95f4\u9010\u6e10\u6241\u5e73\u5316\uff0c\u800c\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u548c\u8ba1\u7b97\u6548\u7387\u6307\u6807\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u6211\u4eec\u7684\u53d1\u73b0\u3002\u8fd9\u4e9b\u7ed3\u679c\u6311\u6218\u4e86\u4f20\u7edf\u5173\u4e8e\u8fc1\u79fb\u5b66\u4e60\u5728\u9971\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u6027\u7684\u89c2\u70b9\uff0c\u5e76\u8868\u660e\u53ef\u80fd\u9700\u8981\u67b6\u6784\u521b\u65b0\u624d\u80fd\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u6539\u8fdb\u3002"}}
{"id": "2506.17946", "pdf": "https://arxiv.org/pdf/2506.17946", "abs": "https://arxiv.org/abs/2506.17946", "authors": ["Azamat Ibragimov", "Ruslan Isaev", "Remudin Reshid Mekuria", "Gulnaz Gimaletdinova", "Dim Shaiakhmetov"], "title": "Classification of Tents in Street Bazaars Using CNN", "categories": ["cs.CV"], "comment": null, "summary": "This research paper proposes an improved deep learning model for classifying\ntents in street bazaars, comparing a custom Convolutional Neural Network (CNN)\nwith EfficientNetB0. This is a critical task for market organization with a\ntent classification, but manual methods in the past have been inefficient.\nStreet bazaars represent a vital economic hub in many regions, yet their\nunstructured nature poses significant challenges for the automated\nclassification of market infrastructure, such as tents. In Kyrgyzstan, more\nthan a quarter of the country's GDP is derived from bazaars. While CNNs have\nbeen widely applied to object recognition, their application to bazaar-specific\ntasks remains underexplored. Here, we build upon our original approach by\ntraining on an extended set of 126 original photographs that were augmented to\ngenerate additional images. This dataset is publicly available for download on\nKaggle. A variety of performance metrics, such as accuracy, precision, recall,\nF1 score, and mean average precision (mAP), were used to assess the models\ncomparatively, providing a more extensive analysis of classification\nperformance.\n  The results show that the CNN custom model achieved 92.8% accuracy, and\nEfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of\ntransfer learning in the bazaar image classification. Also, when analyzing the\nconfusion matrix, the analysis reveals the weaknesses and strengths of each\nmodel. These findings suggest that using a pre-trained model such as\nEfficientNetB0 significantly improves classification accuracy and\ngeneralization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u7c7b\u8857\u5934\u96c6\u5e02\u4e2d\u7684\u5e10\u7bf7\uff0c\u6bd4\u8f83\u4e86\u81ea\u5b9a\u4e49CNN\u4e0eEfficientNetB0\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0cEfficientNetB0\u7684\u51c6\u786e\u7387\u8fbe98.4%\uff0c\u4f18\u4e8e\u81ea\u5b9a\u4e49CNN\u768492.8%\uff0c\u9a8c\u8bc1\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u96c6\u5e02\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8857\u5934\u96c6\u5e02\u662f\u8bb8\u591a\u5730\u533a\u7684\u91cd\u8981\u7ecf\u6d4e\u4e2d\u5fc3\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u7279\u6027\u4f7f\u5f97\u5e10\u7bf7\u7b49\u5e02\u573a\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5206\u7c7b\u6210\u4e3a\u6311\u6218\u3002\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u800cCNN\u5728\u96c6\u5e02\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6269\u5c55126\u5f20\u539f\u59cb\u7167\u7247\u5e76\u751f\u6210\u589e\u5f3a\u56fe\u50cf\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u81ea\u5b9a\u4e49CNN\u548cEfficientNetB0\u6a21\u578b\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cmAP\u7b49\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u81ea\u5b9a\u4e49CNN\u7684\u51c6\u786e\u7387\u4e3a92.8%\uff0c\u800cEfficientNetB0\u8fbe\u523098.4%\u3002\u6df7\u6dc6\u77e9\u9635\u5206\u6790\u63ed\u793a\u4e86\u5404\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982EfficientNetB0\uff09\u53ef\u663e\u8457\u63d0\u9ad8\u96c6\u5e02\u5e10\u7bf7\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5e02\u573a\u7ec4\u7ec7\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8eCNN\u7684\u8857\u5934\u96c6\u5e02\u5e10\u7bf7\u5206\u7c7b", "abstract_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u7c7b\u8857\u5934\u96c6\u5e02\u4e2d\u7684\u5e10\u7bf7\uff0c\u6bd4\u8f83\u4e86\u81ea\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4e0eEfficientNetB0\u7684\u6027\u80fd\u3002\u96c6\u5e02\u5e10\u7bf7\u5206\u7c7b\u5bf9\u5e02\u573a\u7ec4\u7ec7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u8857\u5934\u96c6\u5e02\u662f\u8bb8\u591a\u5730\u533a\u7684\u91cd\u8981\u7ecf\u6d4e\u4e2d\u5fc3\uff08\u5982\u5409\u5c14\u5409\u65af\u65af\u5766\uff0c\u96c6\u5e02\u8d21\u732e\u4e86\u8be5\u56fd\u8d85\u8fc7\u56db\u5206\u4e4b\u4e00\u7684GDP\uff09\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u7279\u6027\u4e3a\u5e10\u7bf7\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5206\u7c7b\u5e26\u6765\u6311\u6218\u3002\u5c3d\u7ba1CNN\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7269\u4f53\u8bc6\u522b\uff0c\u4f46\u5176\u5728\u96c6\u5e02\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u8f83\u5c11\u3002\u672c\u7814\u7a76\u5728\u539f\u6709\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u6269\u5c55126\u5f20\u539f\u59cb\u7167\u7247\u5e76\u751f\u6210\u589e\u5f3a\u56fe\u50cf\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5df2\u5728Kaggle\u4e0a\u516c\u5f00\u3002\u7814\u7a76\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548c\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u7b49\u6307\u6807\u5bf9\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5206\u7c7b\u6027\u80fd\u5206\u6790\u3002\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u5b9a\u4e49CNN\u7684\u51c6\u786e\u7387\u4e3a92.8%\uff0c\u800cEfficientNetB0\u8fbe\u523098.4%\uff0c\u9a8c\u8bc1\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u96c6\u5e02\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\u3002\u6df7\u6dc6\u77e9\u9635\u5206\u6790\u63ed\u793a\u4e86\u5404\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.17255", "pdf": "https://arxiv.org/pdf/2506.17255", "abs": "https://arxiv.org/abs/2506.17255", "authors": ["Sunan Zou", "Ziyun Zhang", "Xueting Sun", "Guojie Luo"], "title": "UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid growth of large language models (LLMs) has outpaced the memory\nconstraints of edge devices, necessitating extreme weight compression beyond\nthe 1-bit limit. While quantization reduces model size, it is fundamentally\nlimited to 1 bit per weight. Existing multiple-to-one compression methods\neither rely on mapping tables (inducing memory overhead) or incur severe\naccuracy degradation due to random weight grouping. We introduce\nUltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low\nbit compression (down to 0.5 bits per weight) while preserving model\nperformance. UltraSketchLLM leverages data sketching, a sub-linear\nrepresentation technique from streaming applications, to map multiple weights\nto single values with bounded error. Our approach integrates an underestimate\nAbsMaxMin sketch to minimize relative errors for small weights,\nimportance-aware space allocation to prioritize salient weights, and a\nstraight-through estimator for compression-aware finetuning. Experiments on\nLlama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,\nalongside tolerable latency overhead. UltraSketchLLM offers a practical\nsolution for deploying LLMs in resource-constrained environments.", "AI": {"tldr": "UltraSketchLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u8349\u56fe\u7684\u8d85\u4f4e\u6bd4\u7279\u538b\u7f29\u6846\u67b6\uff0c\u652f\u6301\u4f4e\u81f30.5\u6bd4\u7279/\u6743\u91cd\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u9650\u5236\u6210\u4e3a\u74f6\u9888\uff0c\u9700\u8981\u8d85\u8d8a1\u6bd4\u7279\u6781\u9650\u7684\u6781\u7aef\u6743\u91cd\u538b\u7f29\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6620\u5c04\u8868\uff08\u589e\u52a0\u5185\u5b58\u5f00\u9500\uff09\uff0c\u8981\u4e48\u56e0\u968f\u673a\u6743\u91cd\u5206\u7ec4\u5bfc\u81f4\u4e25\u91cd\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "UltraSketchLLM\u91c7\u7528\u65e0\u7d22\u5f15\u7684\u8349\u56fe\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u8349\u56fe\u6280\u672f\u5c06\u591a\u4e2a\u6743\u91cd\u6620\u5c04\u4e3a\u5355\u4e2a\u503c\uff0c\u8bef\u5dee\u53ef\u63a7\u3002\u7ed3\u5408\u4f4e\u4f30AbsMaxMin\u8349\u56fe\u6700\u5c0f\u5316\u5c0f\u6743\u91cd\u7684\u76f8\u5bf9\u8bef\u5dee\uff0c\u91cd\u8981\u6027\u611f\u77e5\u7a7a\u95f4\u5206\u914d\u4f18\u5148\u5904\u7406\u663e\u8457\u6743\u91cd\uff0c\u4ee5\u53ca\u76f4\u901a\u4f30\u8ba1\u5668\u8fdb\u884c\u538b\u7f29\u611f\u77e5\u5fae\u8c03\u3002", "result": "\u5728Llama-3.2-1B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUltraSketchLLM\u5b9e\u73b0\u4e86\u4f4e\u81f30.5\u6bd4\u7279/\u6743\u91cd\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u56f0\u60d1\u5ea6\uff0c\u5ef6\u8fdf\u5f00\u9500\u53ef\u63a5\u53d7\u3002", "conclusion": "UltraSketchLLM\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u8d85\u4f4e\u6bd4\u7279\u538b\u7f29\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "paper_title_zh": "UltraSketchLLM\uff1a\u9762\u5411\u8d85\u4f4e\u6bd4\u7279LLM\u538b\u7f29\u7684\u663e\u8457\u6027\u9a71\u52a8\u8349\u56fe\u65b9\u6cd5", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u589e\u957f\u5df2\u8d85\u51fa\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u9650\u5236\uff0c\u9700\u8981\u8d85\u8d8a1\u6bd4\u7279\u6781\u9650\u7684\u6781\u7aef\u6743\u91cd\u538b\u7f29\u65b9\u6cd5\u3002\u91cf\u5316\u867d\u80fd\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u4f46\u5176\u672c\u8d28\u9650\u5236\u4e3a\u6bcf\u6743\u91cd1\u6bd4\u7279\u3002\u73b0\u6709\u7684\u591a\u5bf9\u4e00\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6620\u5c04\u8868\uff08\u589e\u52a0\u5185\u5b58\u5f00\u9500\uff09\uff0c\u8981\u4e48\u56e0\u968f\u673a\u6743\u91cd\u5206\u7ec4\u5bfc\u81f4\u4e25\u91cd\u7cbe\u5ea6\u4e0b\u964d\u3002\u6211\u4eec\u63d0\u51faUltraSketchLLM\uff0c\u4e00\u79cd\u65e0\u7d22\u5f15\u7684\u57fa\u4e8e\u8349\u56fe\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u8d85\u4f4e\u6bd4\u7279\u538b\u7f29\uff08\u4f4e\u81f30.5\u6bd4\u7279/\u6743\u91cd\uff09\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002UltraSketchLLM\u5229\u7528\u6570\u636e\u8349\u56fe\uff08\u4e00\u79cd\u6765\u81ea\u6d41\u5e94\u7528\u7684\u4e9a\u7ebf\u6027\u8868\u793a\u6280\u672f\uff09\uff0c\u5c06\u591a\u4e2a\u6743\u91cd\u6620\u5c04\u4e3a\u8bef\u5dee\u53ef\u63a7\u7684\u5355\u4e2a\u503c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u96c6\u6210\u4e86\u4f4e\u4f30AbsMaxMin\u8349\u56fe\u4ee5\u6700\u5c0f\u5316\u5c0f\u6743\u91cd\u7684\u76f8\u5bf9\u8bef\u5dee\uff0c\u91cd\u8981\u6027\u611f\u77e5\u7a7a\u95f4\u5206\u914d\u4f18\u5148\u5904\u7406\u663e\u8457\u6743\u91cd\uff0c\u4ee5\u53ca\u76f4\u901a\u4f30\u8ba1\u5668\u8fdb\u884c\u538b\u7f29\u611f\u77e5\u5fae\u8c03\u3002\u5728Llama-3.2-1B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUltraSketchLLM\u5b9e\u73b0\u4e86\u4f4e\u81f30.5\u6bd4\u7279/\u6743\u91cd\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u56f0\u60d1\u5ea6\uff0c\u5ef6\u8fdf\u5f00\u9500\u53ef\u63a5\u53d7\u3002UltraSketchLLM\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18576", "pdf": "https://arxiv.org/pdf/2506.18576", "abs": "https://arxiv.org/abs/2506.18576", "authors": ["Matteo Melis", "Gabriella Lapesa", "Dennis Assenmacher"], "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u5206\u7c7b\u6cd5\u6765\u5b9a\u4e49\u4ec7\u6068\u8a00\u8bba\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u5b9a\u4e49\u5bf9\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5b9a\u4e49\u7684\u5177\u4f53\u6027\u4f1a\u5f71\u54cd\u6a21\u578b\u8868\u73b0\uff0c\u4f46\u6548\u679c\u56e0\u6a21\u578b\u67b6\u6784\u800c\u5f02\u3002", "motivation": "\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u5176\u5b9a\u4e49\u6a21\u7cca\u4e0d\u6e05\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790\u73b0\u6709\u5b9a\u4e49\uff0c\u6784\u5efa\u4e00\u4e2a\u6e05\u6670\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e0d\u540c\u5b9a\u4e49\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "1. \u7406\u8bba\u5c42\u9762\uff1a\u6536\u96c6\u5e76\u5206\u6790\u6587\u732e\u4e2d\u7684\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\uff0c\u5c06\u5176\u5f52\u7eb3\u4e3a14\u4e2a\u6982\u5ff5\u8981\u7d20\u7684\u5206\u7c7b\u6cd5\u30022. \u5b9e\u9a8c\u5c42\u9762\uff1a\u4f7f\u7528\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e09\u79cd\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\uff08\u5408\u6210\u3001\u4eba\u5de5\u53c2\u4e0e\u548c\u771f\u5b9e\u6570\u636e\uff09\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e0d\u540c\u5b9a\u4e49\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9009\u62e9\u4e0d\u540c\u5177\u4f53\u7a0b\u5ea6\u7684\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u4f1a\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u5f71\u54cd\u5e76\u975e\u5728\u6240\u6709\u6a21\u578b\u67b6\u6784\u4e2d\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u7684\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u5b9a\u4e49\u7684\u5177\u4f53\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u9a8c\u4f9d\u636e\u3002", "paper_title_zh": "\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u7684\u6a21\u5757\u5316\u5206\u7c7b\u6cd5\u53ca\u5176\u5bf9\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd", "abstract_zh": "\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u793e\u4f1a\u516c\u76ca\u5e94\u7528\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u800c\u4ec7\u6068\u8a00\u8bba\u662f\u5176\u6700\u5371\u9669\u7684\u5f62\u5f0f\u4e4b\u4e00\u3002\u4f46\u6211\u4eec\u5982\u4f55\u5b9a\u4e49\u4ec7\u6068\u8a00\u8bba\uff1f\u4e0d\u540c\u7684\u5b9a\u4e49\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff1f\u672c\u6587\u7684\u8d21\u732e\u5305\u62ec\u4e24\u65b9\u9762\uff1a\u5728\u7406\u8bba\u5c42\u9762\uff0c\u6211\u4eec\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790\u6587\u732e\u4e2d\u7684\u73b0\u6709\u5b9a\u4e49\uff0c\u89e3\u51b3\u4e86\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u4e3a\u5305\u542b14\u4e2a\u6982\u5ff5\u8981\u7d20\u7684\u5206\u7c7b\u6cd5\uff0c\u8fd9\u4e9b\u8981\u7d20\u6355\u6349\u4e86\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u7684\u4e0d\u540c\u65b9\u9762\uff0c\u5982\u4ec7\u6068\u76ee\u6807\uff08\u4e2a\u4eba\u6216\u7fa4\u4f53\uff09\u6216\u5176\u6f5c\u5728\u540e\u679c\u3002\u5728\u5b9e\u9a8c\u5c42\u9762\uff0c\u6211\u4eec\u5229\u7528\u8fd9\u4e9b\u5b9a\u4e49\u5bf9\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u4ee3\u8868\u4e0d\u540c\u7c7b\u578b\u6570\u636e\uff08\u5408\u6210\u3001\u4eba\u5de5\u53c2\u4e0e\u548c\u771f\u5b9e\u4e16\u754c\uff09\u7684\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9009\u62e9\u4e0d\u540c\u5177\u4f53\u7a0b\u5ea6\u7684\u5b9a\u4e49\uff08\u5373\u7f16\u7801\u8981\u7d20\u7684\u4e0d\u540c\uff09\u4f1a\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u6548\u679c\u5e76\u975e\u5728\u6240\u6709\u67b6\u6784\u4e2d\u4e00\u81f4\u3002"}}
{"id": "2506.17954", "pdf": "https://arxiv.org/pdf/2506.17954", "abs": "https://arxiv.org/abs/2506.17954", "authors": ["Liong Gele", "Tan Chye Cheah"], "title": "Mobile Image Analysis Application for Mantoux Skin Test", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a newly developed mobile application designed to diagnose\nLatent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).\nTraditional TST methods often suffer from low follow-up return rates, patient\ndiscomfort, and subjective manual interpretation, particularly with the\nball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,\nprevious developed mobile applications that used 3D reconstruction, this app\nutilizes scaling stickers as reference objects for induration measurement. This\nmobile application integrates advanced image processing technologies, including\nARCore, and machine learning algorithms such as DeepLabv3 for robust image\nsegmentation and precise measurement of skin indurations indicative of LTBI.\nThe system employs an edge detection algorithm to enhance accuracy. The\napplication was evaluated against standard clinical practices, demonstrating\nsignificant improvements in accuracy and reliability. This innovation is\ncrucial for effective tuberculosis management, especially in resource-limited\nregions. By automating and standardizing TST evaluations, the application\nenhances the accessibility and efficiency of TB di-agnostics. Future work will\nfocus on refining machine learning models, optimizing measurement algorithms,\nexpanding functionalities to include comprehensive patient data management, and\nenhancing ARCore's performance across various lighting conditions and\noperational settings.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u65b0\u578b\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\uff0c\u7528\u4e8e\u901a\u8fc7\u66fc\u6258\u76ae\u80a4\u8bd5\u9a8c\uff08TST\uff09\u8bca\u65ad\u6f5c\u4f0f\u6027\u7ed3\u6838\u611f\u67d3\uff08LTBI\uff09\u3002\u8be5\u5e94\u7528\u5229\u7528\u56fe\u50cf\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u66fc\u6258\u76ae\u80a4\u8bd5\u9a8c\u65b9\u6cd5\u5b58\u5728\u968f\u8bbf\u7387\u4f4e\u3001\u60a3\u8005\u4e0d\u9002\u548c\u4e3b\u89c2\u6027\u89e3\u8bfb\u7b49\u95ee\u9898\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u8bca\u548c\u6cbb\u7597\u5ef6\u8bef\u3002\u6b64\u5916\uff0c\u73b0\u6709\u79fb\u52a8\u5e94\u7528\u591a\u4f9d\u8d563D\u91cd\u5efa\u6280\u672f\uff0c\u800c\u672c\u5e94\u7528\u901a\u8fc7\u5f15\u5165\u6807\u5c3a\u8d34\u7eb8\u4f5c\u4e3a\u53c2\u8003\u7269\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u5e94\u7528\u7ed3\u5408\u4e86ARCore\u548cDeepLabv3\u7b49\u5148\u8fdb\u56fe\u50cf\u5904\u7406\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u91c7\u7528\u8fb9\u7f18\u68c0\u6d4b\u7b97\u6cd5\u63d0\u5347\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u76ae\u80a4\u786c\u7ed3\u7684\u81ea\u52a8\u5316\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "result": "\u4e0e\u6807\u51c6\u4e34\u5e8a\u5b9e\u8df5\u76f8\u6bd4\uff0c\u8be5\u5e94\u7528\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u7ed3\u6838\u75c5\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "conclusion": "\u8be5\u5e94\u7528\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u66fc\u6258\u76ae\u80a4\u8bd5\u9a8c\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86\u7ed3\u6838\u75c5\u8bca\u65ad\u7684\u53ef\u53ca\u6027\u548c\u6548\u7387\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6d4b\u91cf\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u60a3\u8005\u6570\u636e\u7ba1\u7406\u529f\u80fd\u3002", "paper_title_zh": "\u57fa\u4e8e\u79fb\u52a8\u56fe\u50cf\u5206\u6790\u7684\u66fc\u6258\u76ae\u80a4\u8bd5\u9a8c\u5e94\u7528", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u65b0\u5f00\u53d1\u7684\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\uff0c\u7528\u4e8e\u901a\u8fc7\u66fc\u6258\u76ae\u80a4\u8bd5\u9a8c\uff08TST\uff09\u8bca\u65ad\u6f5c\u4f0f\u6027\u7ed3\u6838\u611f\u67d3\uff08LTBI\uff09\u3002\u4f20\u7edf\u7684TST\u65b9\u6cd5\u5b58\u5728\u968f\u8bbf\u7387\u4f4e\u3001\u60a3\u8005\u4e0d\u9002\u548c\u4e3b\u89c2\u6027\u89e3\u8bfb\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5706\u73e0\u7b14\u6cd5\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u8bca\u548c\u6cbb\u7597\u5ef6\u8bef\u3002\u4e0e\u6b64\u524d\u4f9d\u8d563D\u91cd\u5efa\u7684\u79fb\u52a8\u5e94\u7528\u4e0d\u540c\uff0c\u672c\u5e94\u7528\u91c7\u7528\u6807\u5c3a\u8d34\u7eb8\u4f5c\u4e3a\u53c2\u8003\u7269\u8fdb\u884c\u786c\u7ed3\u6d4b\u91cf\u3002\u8be5\u5e94\u7528\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u56fe\u50cf\u5904\u7406\u6280\u672f\uff08\u5982ARCore\uff09\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u5982DeepLabv3\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u76ae\u80a4\u786c\u7ed3\u7684\u9c81\u68d2\u56fe\u50cf\u5206\u5272\u548c\u7cbe\u786e\u6d4b\u91cf\uff0c\u5e76\u901a\u8fc7\u8fb9\u7f18\u68c0\u6d4b\u7b97\u6cd5\u63d0\u5347\u51c6\u786e\u6027\u3002\u4e0e\u6807\u51c6\u4e34\u5e8a\u5b9e\u8df5\u5bf9\u6bd4\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u5e94\u7528\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u8fd9\u4e00\u521b\u65b0\u5bf9\u7ed3\u6838\u75c5\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316TST\u8bc4\u4f30\uff0c\u8be5\u5e94\u7528\u63d0\u5347\u4e86\u7ed3\u6838\u75c5\u8bca\u65ad\u7684\u53ef\u53ca\u6027\u548c\u6548\u7387\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6d4b\u91cf\u7b97\u6cd5\uff0c\u6269\u5c55\u60a3\u8005\u6570\u636e\u7ba1\u7406\u529f\u80fd\uff0c\u5e76\u63d0\u5347ARCore\u5728\u4e0d\u540c\u5149\u7167\u548c\u64cd\u4f5c\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.17258", "pdf": "https://arxiv.org/pdf/2506.17258", "abs": "https://arxiv.org/abs/2506.17258", "authors": ["Jasmin Y. Lim", "Dimitrios Pylorof", "Humberto E. Garcia", "Karthik Duraisamy"], "title": "A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "39 pages, 22 figures", "summary": "Generation IV (Gen-IV) nuclear power plants are envisioned to replace the\ncurrent reactor fleet, bringing improvements in performance, safety,\nreliability, and sustainability. However, large cost investments currently\ninhibit the deployment of these advanced reactor concepts. Digital twins bridge\nreal-world systems with digital tools to reduce costs, enhance decision-making,\nand boost operational efficiency. In this work, a digital twin framework is\ndesigned to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor,\nutilizing data-enhanced methods to optimize operational and maintenance\npolicies while adhering to system constraints. The closed-loop framework\nintegrates surrogate modeling, reinforcement learning, and Bayesian inference\nto streamline end-to-end communication for online regulation and\nself-adjustment. Reinforcement learning is used to consider component health\nand degradation to drive the target power generations, with constraints\nenforced through a Reference Governor control algorithm that ensures compliance\nwith pump flow rate and temperature limits. These input driving modules benefit\nfrom detailed online simulations that are assimilated to measurement data with\nBayesian filtering. The digital twin is demonstrated in three case studies: a\none-year long-term operational period showcasing maintenance planning\ncapabilities, short-term accuracy refinement with high-frequency measurements,\nand system shock capturing that demonstrates real-time recalibration\ncapabilities when change in boundary conditions. These demonstrations validate\nrobustness for health-aware and constraint-informed nuclear plant operation,\nwith general applicability to other advanced reactor concepts and complex\nengineering systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u7b2c\u56db\u4ee3\u6838\u53cd\u5e94\u5806\u7684\u5065\u5eb7\u611f\u77e5\u76d1\u63a7\u63a7\u5236\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4f18\u5316\u8fd0\u884c\u548c\u7ef4\u62a4\u7b56\u7565\uff0c\u540c\u65f6\u6ee1\u8db3\u7cfb\u7edf\u7ea6\u675f\u3002", "motivation": "\u7b2c\u56db\u4ee3\uff08Gen-IV\uff09\u6838\u7535\u7ad9\u65e8\u5728\u66ff\u4ee3\u73b0\u6709\u53cd\u5e94\u5806\uff0c\u63d0\u5347\u6027\u80fd\u3001\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0c\u4f46\u9ad8\u6602\u6210\u672c\u963b\u788d\u4e86\u5176\u90e8\u7f72\u3002\u6570\u5b57\u5b6a\u751f\u6280\u672f\u901a\u8fc7\u8fde\u63a5\u73b0\u5b9e\u7cfb\u7edf\u4e0e\u6570\u5b57\u5de5\u5177\uff0c\u53ef\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u4ee3\u7406\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8003\u8651\u7ec4\u4ef6\u5065\u5eb7\u548c\u9000\u5316\uff0c\u9a71\u52a8\u76ee\u6807\u53d1\u7535\u91cf\uff0c\u5e76\u901a\u8fc7\u53c2\u8003\u8c03\u8282\u5668\u63a7\u5236\u7b97\u6cd5\u786e\u4fdd\u6cf5\u6d41\u91cf\u548c\u6e29\u5ea6\u9650\u5236\u3002\u5728\u7ebf\u6a21\u62df\u6570\u636e\u901a\u8fc7\u8d1d\u53f6\u65af\u6ee4\u6ce2\u4e0e\u6d4b\u91cf\u6570\u636e\u878d\u5408\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\uff1a\u957f\u671f\u8fd0\u884c\u7ef4\u62a4\u89c4\u5212\u3001\u77ed\u671f\u9ad8\u9891\u6d4b\u91cf\u7cbe\u5ea6\u4f18\u5316\u4ee5\u53ca\u8fb9\u754c\u6761\u4ef6\u53d8\u5316\u65f6\u7684\u5b9e\u65f6\u91cd\u65b0\u6821\u51c6\u80fd\u529b\u3002", "conclusion": "\u8be5\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e3a\u5065\u5eb7\u611f\u77e5\u548c\u7ea6\u675f\u9a71\u52a8\u7684\u6838\u7535\u7ad9\u8fd0\u884c\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u5148\u8fdb\u53cd\u5e94\u5806\u548c\u590d\u6742\u5de5\u7a0b\u7cfb\u7edf\u3002", "paper_title_zh": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b2c\u56db\u4ee3\u6838\u53cd\u5e94\u5806\u5065\u5eb7\u611f\u77e5\u76d1\u63a7\u6570\u5b57\u5b6a\u751f\u6846\u67b6", "abstract_zh": "\u7b2c\u56db\u4ee3\uff08Gen-IV\uff09\u6838\u7535\u7ad9\u65e8\u5728\u66ff\u4ee3\u73b0\u6709\u53cd\u5e94\u5806\uff0c\u63d0\u5347\u6027\u80fd\u3001\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0c\u4f46\u9ad8\u6602\u6210\u672c\u963b\u788d\u4e86\u5176\u90e8\u7f72\u3002\u6570\u5b57\u5b6a\u751f\u6280\u672f\u901a\u8fc7\u8fde\u63a5\u73b0\u5b9e\u7cfb\u7edf\u4e0e\u6570\u5b57\u5de5\u5177\uff0c\u53ef\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u6548\u7387\u3002\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u64cd\u4f5c\u7b2c\u56db\u4ee3\u6c1f\u76d0\u51b7\u5374\u9ad8\u6e29\u53cd\u5e94\u5806\uff0c\u5229\u7528\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4f18\u5316\u8fd0\u884c\u548c\u7ef4\u62a4\u7b56\u7565\uff0c\u540c\u65f6\u6ee1\u8db3\u7cfb\u7edf\u7ea6\u675f\u3002\u95ed\u73af\u6846\u67b6\u6574\u5408\u4e86\u4ee3\u7406\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u4ee5\u5b9e\u73b0\u7aef\u5230\u7aef\u901a\u4fe1\u7684\u5728\u7ebf\u8c03\u8282\u548c\u81ea\u6211\u8c03\u6574\u3002\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u8003\u8651\u7ec4\u4ef6\u5065\u5eb7\u548c\u9000\u5316\uff0c\u9a71\u52a8\u76ee\u6807\u53d1\u7535\u91cf\uff0c\u5e76\u901a\u8fc7\u53c2\u8003\u8c03\u8282\u5668\u63a7\u5236\u7b97\u6cd5\u786e\u4fdd\u6cf5\u6d41\u91cf\u548c\u6e29\u5ea6\u9650\u5236\u3002\u8fd9\u4e9b\u8f93\u5165\u9a71\u52a8\u6a21\u5757\u53d7\u76ca\u4e8e\u4e0e\u6d4b\u91cf\u6570\u636e\u878d\u5408\u7684\u8be6\u7ec6\u5728\u7ebf\u6a21\u62df\u3002\u6570\u5b57\u5b6a\u751f\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\uff1a\u5c55\u793a\u7ef4\u62a4\u89c4\u5212\u80fd\u529b\u7684\u957f\u671f\u8fd0\u884c\u3001\u9ad8\u9891\u6d4b\u91cf\u7684\u77ed\u671f\u7cbe\u5ea6\u4f18\u5316\uff0c\u4ee5\u53ca\u8fb9\u754c\u6761\u4ef6\u53d8\u5316\u65f6\u5b9e\u65f6\u91cd\u65b0\u6821\u51c6\u80fd\u529b\u7684\u7cfb\u7edf\u51b2\u51fb\u6355\u6349\u3002\u8fd9\u4e9b\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5065\u5eb7\u611f\u77e5\u548c\u7ea6\u675f\u9a71\u52a8\u7684\u6838\u7535\u7ad9\u8fd0\u884c\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u5148\u8fdb\u53cd\u5e94\u5806\u548c\u590d\u6742\u5de5\u7a0b\u7cfb\u7edf\u3002"}}
{"id": "2506.18582", "pdf": "https://arxiv.org/pdf/2506.18582", "abs": "https://arxiv.org/abs/2506.18582", "authors": ["Haoyi Wu", "Zhihao Teng", "Kewei Tu"], "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration", "categories": ["cs.CL"], "comment": "under review", "summary": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u8fde\u7eed\u601d\u7ef4\u94fe\uff08PCCoT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u8fed\u4ee3\u5e76\u884c\u66f4\u65b0\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u601d\u7ef4\u94fe\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CoT\uff09\u901a\u8fc7\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u8fdb\u884c\u9690\u5f0f\u63a8\u7406\uff0c\u8282\u7701\u4e86\u63a8\u7406\u6807\u8bb0\uff0c\u4f46\u5176\u987a\u5e8f\u4f9d\u8d56\u6027\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u957f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u8fde\u7eed\u601d\u7ef4\u94fe\uff08PCCoT\uff09\uff0c\u5229\u7528\u96c5\u53ef\u6bd4\u8fed\u4ee3\u5e76\u884c\u66f4\u65b0\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u987a\u5e8f\u66f4\u65b0\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPCCoT\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8282\u7701\u4e86\u8fd150%\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PCCoT\u901a\u8fc7\u5e76\u884c\u5316\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u601d\u7ef4\u94fe\u7684\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u96c5\u53ef\u6bd4\u8fed\u4ee3\u7684\u5e76\u884c\u8fde\u7eed\u601d\u7ef4\u94fe", "abstract_zh": "\u8fde\u7eed\u601d\u7ef4\u94fe\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u6709\u6548\u8282\u7701\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6807\u8bb0\u3002\u901a\u8fc7\u4f7f\u7528\u8fde\u7eed\u7684\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u8fdb\u884c\u63a8\u7406\uff0c\u8fde\u7eed\u601d\u7ef4\u94fe\u80fd\u591f\u4ee5\u7d27\u51d1\u7684\u65b9\u5f0f\u8fdb\u884c\u9690\u5f0f\u63a8\u7406\u3002\u7136\u800c\uff0c\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u4e4b\u95f4\u7684\u987a\u5e8f\u4f9d\u8d56\u6027\u7834\u574f\u4e86\u5e76\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5e76\u884c\u8fde\u7eed\u601d\u7ef4\u94fe\uff08PCCoT\uff09\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u8fdb\u884c\u96c5\u53ef\u6bd4\u8fed\u4ee3\uff0c\u4ee5\u5e76\u884c\u800c\u975e\u987a\u5e8f\u7684\u65b9\u5f0f\u8fed\u4ee3\u66f4\u65b0\u8fd9\u4e9b\u6807\u8bb0\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8fde\u7eed\u601d\u7ef4\u94fe\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u6211\u4eec\u80fd\u591f\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8282\u7701\u8fd150%\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0cPCCoT\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/whyNLP/PCCoT\u3002"}}
{"id": "2506.17958", "pdf": "https://arxiv.org/pdf/2506.17958", "abs": "https://arxiv.org/abs/2506.17958", "authors": ["Xiangyuan Peng", "Miao Tang", "Huawei Sun", "Bierzynski Kay", "Lorenzo Servadei", "Robert Wille"], "title": "ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty", "categories": ["cs.CV"], "comment": "7 pages. Accepted by IROS2025", "summary": "LiDAR and 4D radar are widely used in autonomous driving and robotics. While\nLiDAR provides rich spatial information, 4D radar offers velocity measurement\nand remains robust under adverse conditions. As a result, increasing studies\nhave focused on the 4D radar-LiDAR fusion method to enhance the perception.\nHowever, the misalignment between different modalities is often overlooked. To\naddress this challenge and leverage the strengths of both modalities, we\npropose a LiDAR detection framework enhanced by 4D radar motion status and\ncross-modal uncertainty. The object movement information from 4D radar is first\ncaptured using a Dynamic Motion-Aware Encoding module during feature extraction\nto enhance 4D radar predictions. Subsequently, the instance-wise uncertainties\nof bounding boxes are estimated to mitigate the cross-modal misalignment and\nrefine the final LiDAR predictions. Extensive experiments on the View-of-Delft\n(VoD) dataset highlight the effectiveness of our method, achieving\nstate-of-the-art performance with the mAP of 74.89% in the entire area and\n88.70% within the driving corridor while maintaining a real-time inference\nspeed of 30.02 FPS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aELMAR\u7684LiDAR\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u54084D\u96f7\u8fbe\u7684\u8fd0\u52a8\u611f\u77e5\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86LiDAR\u4e0e4D\u96f7\u8fbe\u878d\u5408\u4e2d\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "LiDAR\u548c4D\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0cLiDAR\u63d0\u4f9b\u4e30\u5bcc\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u800c4D\u96f7\u8fbe\u5219\u63d0\u4f9b\u901f\u5ea6\u6d4b\u91cf\u5e76\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u89c6\u4e86\u4e24\u8005\u4e4b\u95f4\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u54084D\u96f7\u8fbe\u7684\u8fd0\u52a8\u72b6\u6001\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347LiDAR\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "1. \u63d0\u51fa\u52a8\u6001\u8fd0\u52a8\u611f\u77e5\u7f16\u7801\u6a21\u5757\uff08Dynamic Motion-Aware Encoding\uff09\uff0c\u5728\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u6355\u83b74D\u96f7\u8fbe\u7684\u8fd0\u52a8\u4fe1\u606f\u4ee5\u589e\u5f3a\u9884\u6d4b\u30022. \u4f30\u8ba1\u8fb9\u754c\u6846\u7684\u5b9e\u4f8b\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u51cf\u5c11\u8de8\u6a21\u6001\u4e0d\u5bf9\u9f50\u5e76\u4f18\u5316\u6700\u7ec8\u7684LiDAR\u9884\u6d4b\u3002", "result": "\u5728View-of-Delft\uff08VoD\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6574\u4f53\u533a\u57df\u548c\u9a7e\u9a76\u8d70\u5eca\u5185\u7684mAP\u5206\u522b\u8fbe\u523074.89%\u548c88.70%\uff0c\u540c\u65f6\u4fdd\u630130.02 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ELMAR\u6846\u67b6\u901a\u8fc7\u7ed3\u54084D\u96f7\u8fbe\u7684\u8fd0\u52a8\u611f\u77e5\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86LiDAR\u4e0e4D\u96f7\u8fbe\u878d\u5408\u4e2d\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u3002", "paper_title_zh": "ELMAR\uff1a\u901a\u8fc74D\u96f7\u8fbe\u8fd0\u52a8\u611f\u77e5\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u589e\u5f3aLiDAR\u68c0\u6d4b", "abstract_zh": "LiDAR\u548c4D\u96f7\u8fbe\u5e7f\u6cdb\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u9886\u57df\u3002LiDAR\u63d0\u4f9b\u4e30\u5bcc\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u800c4D\u96f7\u8fbe\u5219\u63d0\u4f9b\u901f\u5ea6\u6d4b\u91cf\u5e76\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\u3002\u56e0\u6b64\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u4e8e4D\u96f7\u8fbe\u4e0eLiDAR\u7684\u878d\u5408\u65b9\u6cd5\u4ee5\u63d0\u5347\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u5e38\u88ab\u5ffd\u89c6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u5e76\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u6001\u7684\u4f18\u52bf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc74D\u96f7\u8fbe\u8fd0\u52a8\u72b6\u6001\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u7684LiDAR\u68c0\u6d4b\u6846\u67b6\u3002\u5728\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\uff0c\u9996\u5148\u901a\u8fc7\u52a8\u6001\u8fd0\u52a8\u611f\u77e5\u7f16\u7801\u6a21\u5757\u6355\u83b74D\u96f7\u8fbe\u7684\u8fd0\u52a8\u4fe1\u606f\u4ee5\u589e\u5f3a\u5176\u9884\u6d4b\u3002\u968f\u540e\uff0c\u4f30\u8ba1\u8fb9\u754c\u6846\u7684\u5b9e\u4f8b\u7ea7\u4e0d\u786e\u5b9a\u6027\u4ee5\u51cf\u5c11\u8de8\u6a21\u6001\u4e0d\u5bf9\u9f50\u5e76\u4f18\u5316\u6700\u7ec8\u7684LiDAR\u9884\u6d4b\u3002\u5728View-of-Delft\uff08VoD\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6574\u4f53\u533a\u57df\u548c\u9a7e\u9a76\u8d70\u5eca\u5185\u7684mAP\u5206\u522b\u8fbe\u523074.89%\u548c88.70%\uff0c\u540c\u65f6\u4fdd\u630130.02 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2506.17262", "pdf": "https://arxiv.org/pdf/2506.17262", "abs": "https://arxiv.org/abs/2506.17262", "authors": ["Thanadet Chuangsuwanich", "Monisha E. Nongpiur", "Fabian A. Braeu", "Tin A. Tun", "Alexandre Thiery", "Shamira Perera", "Ching Lin Ho", "Martin Buist", "George Barbastathis", "Tin Aung", "Micha\u00ebl J. A. Girard"], "title": "AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Objective: (1) To assess whether ONH biomechanics improves prediction of\nthree progressive visual field loss patterns in glaucoma; (2) to use\nexplainable AI to identify strain-sensitive ONH regions contributing to these\npredictions.\n  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged\nunder two conditions: (1) primary gaze and (2) primary gaze with IOP elevated\nto ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects\ninto four categories based on the presence of specific visual field defects:\n(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full\nsuperior hemifield defect (N=25), and (4) other/non-specific defects (N=124).\nAutomatic ONH tissue segmentation and digital volume correlation were used to\ncompute IOP-induced neural tissue and lamina cribrosa (LC) strains.\nBiomechanical and structural features were input to a Geometric Deep Learning\nmodel. Three classification tasks were performed to detect: (1) superior nasal\nstep, (2) superior partial arcuate, (3) full superior hemifield defect. For\neach task, the data were split into 80% training and 20% testing sets. Area\nunder the curve (AUC) was used to assess performance. Explainable AI techniques\nwere employed to highlight the ONH regions most critical to each\nclassification.\n  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain\nimproved VF loss prediction beyond morphology alone. The inferior and\ninferotemporal rim were identified as key strain-sensitive regions,\ncontributing most to visual field loss prediction and showing progressive\nexpansion with increasing disease severity.\n  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF\nloss patterns. Neuroretinal rim, rather than the LC, was the most critical\nregion contributing to model predictions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u795e\u7ecf\u5934\uff08ONH\uff09\u751f\u7269\u529b\u5b66\u7279\u5f81\uff0c\u6210\u529f\u9884\u6d4b\u4e86\u9752\u5149\u773c\u60a3\u8005\u7684\u4e09\u79cd\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\uff0c\u5e76\u786e\u5b9a\u4e86\u4e0e\u529f\u80fd\u635f\u5931\u76f8\u5173\u7684\u5173\u952e\u5e94\u53d8\u654f\u611f\u533a\u57df\u3002", "motivation": "\u9752\u5149\u773c\u662f\u4e00\u79cd\u5bfc\u81f4\u4e0d\u53ef\u9006\u89c6\u529b\u635f\u5931\u7684\u75be\u75c5\uff0c\u5176\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\u591a\u6837\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22ONH\u751f\u7269\u529b\u5b66\u7279\u5f81\u662f\u5426\u80fd\u591f\u6539\u8fdb\u5bf9\u9752\u5149\u773c\u89c6\u91ce\u7f3a\u635f\u7684\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\u8bc6\u522b\u5173\u952e\u5e94\u53d8\u654f\u611f\u533a\u57df\u3002", "method": "\u7814\u7a76\u7eb3\u5165237\u540d\u9752\u5149\u773c\u60a3\u8005\uff0c\u901a\u8fc7\u773c\u79d1\u52a8\u6001\u6d4b\u91cf\u6280\u672f\u5728\u4e0d\u540c\u773c\u538b\u6761\u4ef6\u4e0b\u6210\u50cfONH\u3002\u4e13\u5bb6\u6839\u636e\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\u5c06\u60a3\u8005\u5206\u4e3a\u56db\u7c7b\u3002\u5229\u7528\u81ea\u52a8\u7ec4\u7ec7\u5206\u5272\u548c\u6570\u5b57\u4f53\u79ef\u76f8\u5173\u6280\u672f\u8ba1\u7b97ONH\u5e94\u53d8\uff0c\u7ed3\u5408\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\u5206\u6790\u5173\u952e\u533a\u57df\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u4e09\u79cd\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\u65f6\u8868\u73b0\u51fa\u8272\uff08AUC\u4e3a0.77-0.88\uff09\uff0cONH\u5e94\u53d8\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u4e0b\u90e8\u548c\u4e0b\u989e\u4fa7\u89c6\u795e\u7ecf\u5934\u8fb9\u7f18\u662f\u5173\u952e\u5e94\u53d8\u654f\u611f\u533a\u57df\uff0c\u4e14\u968f\u75c5\u60c5\u52a0\u91cd\u9010\u6e10\u6269\u5927\u3002", "conclusion": "ONH\u5e94\u53d8\u80fd\u591f\u6709\u6548\u9884\u6d4b\u9752\u5149\u773c\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\uff0c\u89c6\u795e\u7ecf\u5934\u8fb9\u7f18\uff08\u800c\u975e\u7b5b\u677f\uff09\u662f\u6a21\u578b\u9884\u6d4b\u4e2d\u6700\u5173\u952e\u7684\u8d21\u732e\u533a\u57df\u3002", "paper_title_zh": "\u5229\u7528AI\u8bc6\u522b\u4e0e\u9752\u5149\u773c\u529f\u80fd\u635f\u5931\u76f8\u5173\u7684\u89c6\u795e\u7ecf\u5934\u5e94\u53d8\u654f\u611f\u533a\u57df", "abstract_zh": "\u76ee\u7684\uff1a(1) \u8bc4\u4f30\u89c6\u795e\u7ecf\u5934\uff08ONH\uff09\u751f\u7269\u529b\u5b66\u7279\u5f81\u662f\u5426\u80fd\u591f\u6539\u8fdb\u5bf9\u9752\u5149\u773c\u4e09\u79cd\u8fdb\u5c55\u6027\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\u7684\u9884\u6d4b\uff1b(2) \u5229\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u8bc6\u522b\u4e0e\u8fd9\u4e9b\u9884\u6d4b\u76f8\u5173\u7684\u5e94\u53d8\u654f\u611fONH\u533a\u57df\u3002\u65b9\u6cd5\uff1a\u62db\u52df237\u540d\u9752\u5149\u773c\u60a3\u8005\uff0c\u901a\u8fc7\u773c\u79d1\u52a8\u6001\u6d4b\u91cf\u6280\u672f\u5728\u4e0d\u540c\u773c\u538b\u6761\u4ef6\u4e0b\u6210\u50cfONH\u3002\u4e13\u5bb6\u6839\u636e\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\u5c06\u60a3\u8005\u5206\u4e3a\u56db\u7c7b\uff1a\u4e0a\u9f3b\u4fa7\u9636\u68af\uff0826\u4f8b\uff09\u3001\u4e0a\u90e8\u5206\u5f13\u5f62\u7f3a\u635f\uff0862\u4f8b\uff09\u3001\u4e0a\u534a\u89c6\u91ce\u5b8c\u5168\u7f3a\u635f\uff0825\u4f8b\uff09\u53ca\u5176\u4ed6/\u975e\u7279\u5f02\u6027\u7f3a\u635f\uff08124\u4f8b\uff09\u3002\u5229\u7528\u81ea\u52a8\u7ec4\u7ec7\u5206\u5272\u548c\u6570\u5b57\u4f53\u79ef\u76f8\u5173\u6280\u672f\u8ba1\u7b97ONH\u5e94\u53d8\uff0c\u7ed3\u5408\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\u5206\u6790\u5173\u952e\u533a\u57df\u3002\u7ed3\u679c\uff1a\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff08AUC\u4e3a0.77-0.88\uff09\uff0cONH\u5e94\u53d8\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002\u4e0b\u90e8\u548c\u4e0b\u989e\u4fa7\u89c6\u795e\u7ecf\u5934\u8fb9\u7f18\u662f\u5173\u952e\u5e94\u53d8\u654f\u611f\u533a\u57df\uff0c\u4e14\u968f\u75c5\u60c5\u52a0\u91cd\u9010\u6e10\u6269\u5927\u3002\u7ed3\u8bba\u4e0e\u610f\u4e49\uff1aONH\u5e94\u53d8\u80fd\u591f\u6709\u6548\u9884\u6d4b\u9752\u5149\u773c\u89c6\u91ce\u7f3a\u635f\u6a21\u5f0f\uff0c\u89c6\u795e\u7ecf\u5934\u8fb9\u7f18\uff08\u800c\u975e\u7b5b\u677f\uff09\u662f\u6a21\u578b\u9884\u6d4b\u4e2d\u6700\u5173\u952e\u7684\u8d21\u732e\u533a\u57df\u3002"}}
{"id": "2506.18600", "pdf": "https://arxiv.org/pdf/2506.18600", "abs": "https://arxiv.org/abs/2506.18600", "authors": ["Ariel Flint Ashery", "Luca Maria Aiello", "Andrea Baronchelli"], "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data leakage\"", "categories": ["cs.CL", "cs.GT", "cs.MA"], "comment": "Reply to arXiv:2505.23796", "summary": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.", "AI": {"tldr": "\u672c\u6587\u56de\u5e94\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7fa4\u4f53\u6a21\u62df\u4e2d\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u7684\u6279\u8bc4\uff0c\u5f3a\u8c03\u5c3d\u7ba1\u6570\u636e\u6c61\u67d3\u53ef\u80fd\u5f71\u54cd\u5b9e\u9a8c\uff0c\u4f46LLM\u7fa4\u4f53\u4e2d\u7684\u81ea\u7ec4\u7ec7\u548c\u6a21\u578b\u4f9d\u8d56\u6027\u6d8c\u73b0\u52a8\u6001\u4ecd\u53ef\u7814\u7a76\u3002", "motivation": "\u9488\u5bf9Barrie\u548cT\u00f6rnberg\u5bf9Flint Ashery\u7b49\u4eba\u7814\u7a76\u7684\u6279\u8bc4\uff0c\u672c\u6587\u65e8\u5728\u6f84\u6e05\u6570\u636e\u6c61\u67d3\u95ee\u9898\u5e76\u4e0d\u59a8\u788d\u7814\u7a76LLM\u7fa4\u4f53\u4e2d\u771f\u5b9e\u7684\u6d8c\u73b0\u52a8\u6001\uff0c\u5c24\u5176\u662f\u793e\u4f1a\u60ef\u4f8b\u4e2d\u7684\u5b9e\u8bc1\u89c2\u5bdf\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u6848\u4f8b\uff08\u5982\u793e\u4f1a\u60ef\u4f8b\u7684\u5f62\u6210\uff09\uff0c\u8bba\u8bc1LLM\u7fa4\u4f53\u4e2d\u7684\u81ea\u7ec4\u7ec7\u548c\u6d8c\u73b0\u52a8\u6001\u662f\u53ef\u7814\u7a76\u7684\uff0c\u4e0d\u53d7\u6570\u636e\u6c61\u67d3\u7684\u5b8c\u5168\u9650\u5236\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u6570\u636e\u6c61\u67d3\u53ef\u80fd\u5f71\u54cd\u67d0\u4e9b\u5b9e\u9a8c\uff0c\u4f46LLM\u7fa4\u4f53\u4e2d\u7684\u6d8c\u73b0\u52a8\u6001\uff08\u5982\u793e\u4f1a\u60ef\u4f8b\uff09\u4ecd\u53ef\u901a\u8fc7\u5b9e\u8bc1\u89c2\u5bdf\u9a8c\u8bc1\u3002", "conclusion": "\u6570\u636e\u6c61\u67d3\u95ee\u9898\u867d\u91cd\u8981\uff0c\u4f46\u5e76\u672a\u5b8c\u5168\u963b\u788d\u5bf9LLM\u7fa4\u4f53\u4e2d\u6d8c\u73b0\u52a8\u6001\u7684\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u793e\u4f1a\u60ef\u4f8b\u7b49\u5177\u4f53\u6848\u4f8b\u4e2d\u3002", "paper_title_zh": "\u56de\u5e94\u300a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d8c\u73b0\u884c\u4e3a\u4e0e\u6570\u636e\u6cc4\u6f0f\u5728\u89c2\u6d4b\u4e0a\u7b49\u4ef7\u300b", "abstract_zh": "\u5728\u6a21\u62df\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7fa4\u4f53\u65f6\uff0c\u6570\u636e\u6c61\u67d3\uff08\u5373\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u4ee5\u610f\u5916\u65b9\u5f0f\u5f71\u54cd\u7ed3\u679c\uff09\u662f\u4e00\u4e2a\u6f5c\u5728\u95ee\u9898\u3002\u5c3d\u7ba1\u8fd9\u4e00\u62c5\u5fe7\u91cd\u8981\u4e14\u53ef\u80fd\u963b\u788d\u591a\u667a\u80fd\u4f53\u6a21\u578b\u7684\u67d0\u4e9b\u5b9e\u9a8c\uff0c\u4f46\u5b83\u5e76\u672a\u6392\u9664\u5bf9LLM\u7fa4\u4f53\u4e2d\u771f\u5b9e\u6d8c\u73b0\u52a8\u6001\u7684\u7814\u7a76\u3002Barrie\u548cT\u00f6rnberg\u8fd1\u671f\u5bf9Flint Ashery\u7b49\u4eba\u7814\u7a76\u7684\u6279\u8bc4[1][2]\u63d0\u4f9b\u4e86\u4e00\u4e2a\u673a\u4f1a\uff0c\u6f84\u6e05\u81ea\u7ec4\u7ec7\u548c\u6a21\u578b\u4f9d\u8d56\u6027\u6d8c\u73b0\u52a8\u6001\u53ef\u4ee5\u5728LLM\u7fa4\u4f53\u4e2d\u7814\u7a76\uff0c\u5e76\u5f3a\u8c03\u8fd9\u4e9b\u52a8\u6001\u5728\u793e\u4f1a\u60ef\u4f8b\u7684\u5177\u4f53\u6848\u4f8b\u4e2d\u5df2\u88ab\u5b9e\u8bc1\u89c2\u5bdf\u5230\u3002"}}
{"id": "2506.17969", "pdf": "https://arxiv.org/pdf/2506.17969", "abs": "https://arxiv.org/abs/2506.17969", "authors": ["Chenyue Song", "Chen Hui", "Wei Zhang", "Haiqi Zhu", "Shaohui Liu", "Hong Huang", "Feng Jiang"], "title": "BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Image Quality Assessment (IQA) aims to evaluate the perceptual quality of\nimages based on human subjective perception. Existing methods generally combine\nmultiscale features to achieve high performance, but most rely on\nstraightforward linear fusion of these features, which may not adequately\ncapture the impact of distortions on semantic content. To address this, we\npropose a bottom-up image quality assessment approach based on the Contrastive\nLanguage-Image Pre-training (CLIP, a recently proposed model that aligns images\nand text in a shared feature space), named BPCLIP, which progressively extracts\nthe impact of low-level distortions on high-level semantics. Specifically, we\nutilize an encoder to extract multiscale features from the input image and\nintroduce a bottom-up multiscale cross attention module designed to capture the\nrelationships between shallow and deep features. In addition, by incorporating\n40 image quality adjectives across six distinct dimensions, we enable the\npre-trained CLIP text encoder to generate representations of the intrinsic\nquality of the image, thereby strengthening the connection between image\nquality perception and human language. Our method achieves superior results on\nmost public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while\ndemonstrating greater robustness.", "AI": {"tldr": "BPCLIP\u662f\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u81ea\u5e95\u5411\u4e0a\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u63d0\u53d6\u4f4e\u5c42\u5931\u771f\u5bf9\u9ad8\u5c42\u8bed\u4e49\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c40\u4e2a\u56fe\u50cf\u8d28\u91cf\u5f62\u5bb9\u8bcd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u7ebf\u6027\u878d\u5408\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u5931\u771f\u5bf9\u8bed\u4e49\u5185\u5bb9\u7684\u5f71\u54cd\u3002BPCLIP\u65e8\u5728\u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u5f0f\uff0c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5931\u771f\u5bf9\u56fe\u50cf\u8bed\u4e49\u7684\u5f71\u54cd\u3002", "method": "BPCLIP\u5229\u7528\u7f16\u7801\u5668\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u81ea\u5e95\u5411\u4e0a\u7684\u591a\u5c3a\u5ea6\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u6355\u6349\u6d45\u5c42\u4e0e\u6df1\u5c42\u7279\u5f81\u7684\u5173\u7cfb\u3002\u540c\u65f6\uff0c\u7ed3\u540840\u4e2a\u56fe\u50cf\u8d28\u91cf\u5f62\u5bb9\u8bcd\uff0c\u5229\u7528CLIP\u6587\u672c\u7f16\u7801\u5668\u751f\u6210\u56fe\u50cf\u5185\u5728\u8d28\u91cf\u7684\u8868\u793a\uff0c\u589e\u5f3a\u56fe\u50cf\u8d28\u91cf\u611f\u77e5\u4e0e\u4eba\u7c7b\u8bed\u8a00\u7684\u5173\u8054\u3002", "result": "BPCLIP\u5728\u5927\u591a\u6570\u516c\u5f00\u7684\u5168\u53c2\u8003\uff08FR\uff09\u548c\u65e0\u53c2\u8003\uff08NR\uff09\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "BPCLIP\u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u5f0f\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "BPCLIP\uff1a\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u81ea\u5e95\u5411\u4e0a\u4ece\u5931\u771f\u5230\u8bed\u4e49\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5", "abstract_zh": "\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u65e8\u5728\u57fa\u4e8e\u4eba\u7c7b\u4e3b\u89c2\u611f\u77e5\u8bc4\u4f30\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u4f46\u591a\u6570\u4f9d\u8d56\u8fd9\u4e9b\u7279\u5f81\u7684\u7b80\u5355\u7ebf\u6027\u878d\u5408\uff0c\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5931\u771f\u5bf9\u8bed\u4e49\u5185\u5bb9\u7684\u5f71\u54cd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff0c\u4e00\u79cd\u6700\u8fd1\u63d0\u51fa\u7684\u5c06\u56fe\u50cf\u548c\u6587\u672c\u5bf9\u9f50\u5230\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u7684\u6a21\u578b\uff09\u7684\u81ea\u5e95\u5411\u4e0a\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u547d\u540d\u4e3aBPCLIP\uff0c\u9010\u6b65\u63d0\u53d6\u4f4e\u5c42\u5931\u771f\u5bf9\u9ad8\u5c42\u8bed\u4e49\u7684\u5f71\u54cd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5229\u7528\u7f16\u7801\u5668\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u81ea\u5e95\u5411\u4e0a\u7684\u591a\u5c3a\u5ea6\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u65e8\u5728\u6355\u6349\u6d45\u5c42\u4e0e\u6df1\u5c42\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u7ed3\u5408\u516d\u4e2a\u4e0d\u540c\u7ef4\u5ea6\u768440\u4e2a\u56fe\u50cf\u8d28\u91cf\u5f62\u5bb9\u8bcd\uff0c\u6211\u4eec\u4f7f\u9884\u8bad\u7ec3\u7684CLIP\u6587\u672c\u7f16\u7801\u5668\u80fd\u591f\u751f\u6210\u56fe\u50cf\u5185\u5728\u8d28\u91cf\u7684\u8868\u793a\uff0c\u4ece\u800c\u589e\u5f3a\u56fe\u50cf\u8d28\u91cf\u611f\u77e5\u4e0e\u4eba\u7c7b\u8bed\u8a00\u7684\u5173\u8054\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u516c\u5f00\u7684\u5168\u53c2\u8003\uff08FR\uff09\u548c\u65e0\u53c2\u8003\uff08NR\uff09IQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.17263", "pdf": "https://arxiv.org/pdf/2506.17263", "abs": "https://arxiv.org/abs/2506.17263", "authors": ["Massimiliano Tamborski", "David Abel"], "title": "Memory Allocation in Resource-Constrained Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "RLDM 2025", "summary": "Resource constraints can fundamentally change both learning and\ndecision-making. We explore how memory constraints influence an agent's\nperformance when navigating unknown environments using standard reinforcement\nlearning algorithms. Specifically, memory-constrained agents face a dilemma:\nhow much of their limited memory should be allocated to each of the agent's\ninternal processes, such as estimating a world model, as opposed to forming a\nplan using that model? We study this dilemma in MCTS- and DQN-based algorithms\nand examine how different allocations of memory impact performance in episodic\nand continual learning settings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5185\u5b58\u5206\u914d\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u5185\u5b58\u5206\u914d\u7b56\u7565\u5bf9MCTS\u548cDQN\u7b97\u6cd5\u5728\u60c5\u666f\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u8d44\u6e90\u7ea6\u675f\u4f1a\u663e\u8457\u6539\u53d8\u5b66\u4e60\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5185\u5b58\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u5728\u4f7f\u7528\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u65f6\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5185\u5b58\u53d7\u9650\u7684\u667a\u80fd\u4f53\u5728\u5185\u90e8\u8fc7\u7a0b\uff08\u5982\u4f30\u8ba1\u4e16\u754c\u6a21\u578b\u548c\u5236\u5b9a\u8ba1\u5212\uff09\u4e4b\u95f4\u7684\u5185\u5b58\u5206\u914d\u95ee\u9898\u3002", "method": "\u7814\u7a76\u57fa\u4e8eMCTS\u548cDQN\u7b97\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5185\u5b58\u5206\u914d\u7b56\u7565\u5bf9\u667a\u80fd\u4f53\u5728\u60c5\u666f\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5185\u5b58\u5206\u914d\u7b56\u7565\u5bf9\u667a\u80fd\u4f53\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5408\u7406\u5206\u914d\u5185\u5b58\u53ef\u4ee5\u4f18\u5316\u5b66\u4e60\u548c\u51b3\u7b56\u6548\u679c\u3002", "conclusion": "\u5185\u5b58\u5206\u914d\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5408\u7406\u5206\u914d\u5185\u5b58\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u548c\u51b3\u7b56\u80fd\u529b\u3002", "paper_title_zh": "\u8d44\u6e90\u53d7\u9650\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5185\u5b58\u5206\u914d", "abstract_zh": "\u8d44\u6e90\u7ea6\u675f\u53ef\u4ee5\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5b66\u4e60\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5185\u5b58\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u5728\u4f7f\u7528\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5bfc\u822a\u672a\u77e5\u73af\u5883\u65f6\u7684\u8868\u73b0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5185\u5b58\u53d7\u9650\u7684\u667a\u80fd\u4f53\u9762\u4e34\u4e00\u4e2a\u56f0\u5883\uff1a\u5982\u4f55\u5c06\u6709\u9650\u7684\u5185\u5b58\u5206\u914d\u7ed9\u5176\u5185\u90e8\u8fc7\u7a0b\uff08\u5982\u4f30\u8ba1\u4e16\u754c\u6a21\u578b\uff09\u4e0e\u4f7f\u7528\u8be5\u6a21\u578b\u5236\u5b9a\u8ba1\u5212\u4e4b\u95f4\uff1f\u6211\u4eec\u5728\u57fa\u4e8eMCTS\u548cDQN\u7684\u7b97\u6cd5\u4e2d\u7814\u7a76\u4e86\u8fd9\u4e00\u56f0\u5883\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u5185\u5b58\u5206\u914d\u7b56\u7565\u5728\u60c5\u666f\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.18602", "pdf": "https://arxiv.org/pdf/2506.18602", "abs": "https://arxiv.org/abs/2506.18602", "authors": ["R. Prashanth"], "title": "Semantic similarity estimation for domain specific data using BERT and other techniques", "categories": ["cs.CL", "stat.AP"], "comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019", "summary": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u5148\u8fdb\u6280\u672f\uff08\u5982USE\u3001InferSent\u548cBERT\uff09\uff0c\u53d1\u73b0BERT\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u7814\u7a76\u95ee\u9898\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u95ee\u7b54\u3001\u8bed\u4e49\u641c\u7d22\u3001\u4fe1\u606f\u68c0\u7d22\u7b49\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u6280\u672f\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528USE\u3001InferSent\u548cBERT\u7b49\u5148\u8fdb\u6280\u672f\uff0c\u5206\u522b\u5728\u9886\u57df\u7279\u5b9a\u7684\u5185\u90e8\u6570\u636e\u96c6\u548c\u516c\u5f00\u7684Quora\u95ee\u9898\u5bf9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBERT\u6a21\u578b\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u5fae\u8c03\u8fc7\u7a0b\u80fd\u591f\u66f4\u597d\u5730\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u5f0f\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86BERT\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5efa\u8bae\u5728\u7c7b\u4f3c\u4efb\u52a1\u4e2d\u4f18\u5148\u4f7f\u7528BERT\u3002", "paper_title_zh": "\u4f7f\u7528BERT\u53ca\u5176\u4ed6\u6280\u672f\u5bf9\u9886\u57df\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f30\u8ba1", "abstract_zh": "\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u7814\u7a76\u95ee\u9898\uff0c\u5728\u95ee\u7b54\u3001\u8bed\u4e49\u641c\u7d22\u3001\u4fe1\u606f\u68c0\u7d22\u3001\u6587\u6863\u805a\u7c7b\u3001\u8bcd\u4e49\u6d88\u6b67\u548c\u673a\u5668\u7ffb\u8bd1\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u7814\u7a76\u91c7\u7528\u591a\u79cd\u5148\u8fdb\u6280\u672f\uff08\u5305\u62ec\u901a\u7528\u53e5\u5b50\u7f16\u7801\u5668USE\u3001InferSent\u548c\u6700\u65b0\u7684BERT\u6a21\u578b\uff09\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u3002\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u4e2a\u95ee\u9898\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u4e2a\u662f\u9886\u57df\u7279\u5b9a\u7684\u5185\u90e8\u6570\u636e\u96c6\uff0c\u53e6\u4e00\u4e2a\u662f\u516c\u5f00\u7684Quora\u95ee\u9898\u5bf9\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0cBERT\u6a21\u578b\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6d89\u53ca\u7684\u5fae\u8c03\u8fc7\u7a0b\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u5f0f\u3002\u672c\u7814\u7a76\u8bc1\u660e\u4e86BERT\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a8\u65ad\u51fa\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e2d\uff0cBERT\u662f\u6700\u4f73\u7684\u6280\u672f\u9009\u62e9\u3002"}}
{"id": "2506.17975", "pdf": "https://arxiv.org/pdf/2506.17975", "abs": "https://arxiv.org/abs/2506.17975", "authors": ["Mischa Dombrowski", "Bernhard Kainz"], "title": "Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic data has recently reached a level of visual fidelity that makes it\nnearly indistinguishable from real data, offering great promise for\nprivacy-preserving data sharing in medical imaging. However, fully synthetic\ndatasets still suffer from significant limitations: First and foremost, the\nlegal aspect of sharing synthetic data is often neglected and data regulations,\nsuch as the GDPR, are largley ignored. Secondly, synthetic models fall short of\nmatching the performance of real data, even for in-domain downstream\napplications. Recent methods for image generation have focused on maximising\nimage diversity instead of fidelity solely to improve the mode coverage and\ntherefore the downstream performance of synthetic data. In this work, we shift\nperspective and highlight how maximizing diversity can also be interpreted as\nprotecting natural persons from being singled out, which leads to predicate\nsingling-out (PSO) secure synthetic datasets. Specifically, we propose a\ngeneralisable framework for training diffusion models on personal data which\nleads to unpersonal synthetic datasets achieving performance within one\npercentage point of real-data models while significantly outperforming\nstate-of-the-art methods that do not ensure privacy. Our code is available at\nhttps://github.com/MischaD/Trichotomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6837\u6027\u611f\u77e5\u6269\u6563\u6a21\u578b\u7684PSO\u5b89\u5168\u5408\u6210\u6570\u636e\u5171\u4eab\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5408\u6210\u6570\u636e\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u5171\u4eab\u7684\u5408\u6cd5\u6027\u3002", "motivation": "\u5408\u6210\u6570\u636e\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u9886\u57df\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5408\u6cd5\u6027\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6700\u5927\u5316\u591a\u6837\u6027\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff08PSO\u5b89\u5168\uff09\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u751f\u6210\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u786e\u4fdd\u5408\u6210\u6570\u636e\u65e2\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\uff08PSO\u5b89\u5168\uff09\uff0c\u53c8\u80fd\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\uff08\u5dee\u8ddd\u5c0f\u4e8e1%\uff09\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u672a\u8003\u8651\u9690\u79c1\u4fdd\u62a4\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u591a\u6837\u6027\u6700\u5927\u5316\uff0c\u672c\u6587\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u5408\u6210\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u4e86\u5408\u6cd5\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u6837\u6027\u611f\u77e5\u6269\u6563\u6a21\u578b\u7684PSO\u5b89\u5168\u5408\u6210\u6570\u636e\u5171\u4eab", "abstract_zh": "\u5408\u6210\u6570\u636e\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u5df2\u8fbe\u5230\u4e0e\u771f\u5b9e\u6570\u636e\u96be\u4ee5\u533a\u5206\u7684\u6c34\u5e73\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u7b49\u9886\u57df\u7684\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u7136\u800c\uff0c\u5b8c\u5168\u5408\u6210\u7684\u6570\u636e\u96c6\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\uff1a\u9996\u5148\uff0c\u5408\u6210\u6570\u636e\u5171\u4eab\u7684\u6cd5\u5f8b\u95ee\u9898\u5e38\u88ab\u5ffd\u89c6\uff0c\u5982GDPR\u7b49\u6570\u636e\u6cd5\u89c4\u672a\u88ab\u5145\u5206\u8003\u8651\uff1b\u5176\u6b21\uff0c\u5408\u6210\u6570\u636e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4ecd\u4e0d\u53ca\u771f\u5b9e\u6570\u636e\u3002\u8fd1\u671f\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u6700\u5927\u5316\u56fe\u50cf\u591a\u6837\u6027\u800c\u975e\u4ec5\u4fdd\u771f\u5ea6\uff0c\u4ee5\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u6a21\u5f0f\u8986\u76d6\u548c\u4e0b\u6e38\u6027\u80fd\u3002\u672c\u6587\u8f6c\u53d8\u89c6\u89d2\uff0c\u6307\u51fa\u6700\u5927\u5316\u591a\u6837\u6027\u4e5f\u53ef\u89c6\u4e3a\u4fdd\u62a4\u4e2a\u4f53\u514d\u4e8e\u88ab\u5355\u72ec\u8bc6\u522b\uff0c\u4ece\u800c\u5b9e\u73b0PSO\u5b89\u5168\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53bb\u4e2a\u6027\u5316\u7684\u5408\u6210\u6570\u636e\uff0c\u5176\u6027\u80fd\u4e0e\u771f\u5b9e\u6570\u636e\u6a21\u578b\u7684\u5dee\u8ddd\u5c0f\u4e8e1%\uff0c\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u672a\u786e\u4fdd\u9690\u79c1\u7684\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/MischaD/Trichotomy\u3002"}}
{"id": "2506.17264", "pdf": "https://arxiv.org/pdf/2506.17264", "abs": "https://arxiv.org/abs/2506.17264", "authors": ["Jikai Long", "Zijian Hu", "Xiaodong Yu", "Jianwen Xie", "Zhaozhuo Xu"], "title": "OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)\noffers a memory-efficient alternative to gradient-based methods but suffers\nfrom slower convergence and unstable optimization due to noisy gradient\nestimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training\ndata rephrasing strategy that leverages an LLM to rephrase training instances\nbased on its understanding of the ZO dynamics, specifically MeZO, derived\ndirectly from its paper. The approach incorporates a dual-stage pipeline\nfeaturing a rewriter LLM and a semantic judge, ensuring all rephrasings retain\ntask relevance and logical consistency. Evaluations across five classification\ntasks and three LLM architectures demonstrate that OAT-Rephrase consistently\nimproves MeZO fine-tuning performance, often narrowing or eliminating the gap\nwith first-order methods. Our findings suggest that optimization-aware\nrephrasing serves as a reusable and low-overhead enhancement for zeroth-order\ntuning regimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOAT-Rephrase\uff0c\u4e00\u79cd\u4f18\u5316\u611f\u77e5\u7684\u8bad\u7ec3\u6570\u636e\u91cd\u8ff0\u7b56\u7565\uff0c\u901a\u8fc7LLM\u91cd\u8ff0\u8bad\u7ec3\u6570\u636e\u4ee5\u63d0\u5347\u96f6\u9636\u4f18\u5316\uff08\u5982MeZO\uff09\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u7f29\u5c0f\u4e0e\u4e00\u9636\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002", "motivation": "\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff08\u5982MeZO\uff09\u5728\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5185\u5b58\u6548\u7387\u9ad8\uff0c\u4f46\u6536\u655b\u6162\u4e14\u4f18\u5316\u4e0d\u7a33\u5b9a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u611f\u77e5\u7684\u6570\u636e\u91cd\u8ff0\u7b56\u7565\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "OAT-Rephrase\u91c7\u7528\u53cc\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u91cd\u8ff0LLM\u548c\u8bed\u4e49\u5224\u65ad\u5668\uff0c\u786e\u4fdd\u91cd\u8ff0\u5185\u5bb9\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u5206\u7c7b\u4efb\u52a1\u548c\u4e09\u79cdLLM\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOAT-Rephrase\u663e\u8457\u63d0\u5347MeZO\u5fae\u8c03\u6027\u80fd\uff0c\u751a\u81f3\u63a5\u8fd1\u4e00\u9636\u65b9\u6cd5\u7684\u6548\u679c\u3002", "conclusion": "\u4f18\u5316\u611f\u77e5\u7684\u6570\u636e\u91cd\u8ff0\u662f\u4e00\u79cd\u53ef\u91cd\u7528\u4e14\u4f4e\u5f00\u9500\u7684\u96f6\u9636\u4f18\u5316\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "paper_title_zh": "OAT-Rephrase\uff1a\u9762\u5411\u96f6\u9636LLM\u5fae\u8c03\u7684\u4f18\u5316\u611f\u77e5\u8bad\u7ec3\u6570\u636e\u91cd\u8ff0", "abstract_zh": "\u4f7f\u7528\u96f6\u9636\u4f18\u5316\uff08ZO\uff09\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u56e0\u5176\u68af\u5ea6\u4f30\u8ba1\u566a\u58f0\u5bfc\u81f4\u6536\u655b\u6162\u4e14\u4f18\u5316\u4e0d\u7a33\u5b9a\u3002\u672c\u6587\u63d0\u51faOAT-Rephrase\uff0c\u4e00\u79cd\u4f18\u5316\u611f\u77e5\u7684\u8bad\u7ec3\u6570\u636e\u91cd\u8ff0\u7b56\u7565\uff0c\u5229\u7528LLM\u57fa\u4e8e\u5176\u5bf9ZO\u52a8\u6001\uff08\u7279\u522b\u662fMeZO\uff09\u7684\u7406\u89e3\u91cd\u8ff0\u8bad\u7ec3\u5b9e\u4f8b\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5305\u542b\u91cd\u8ff0LLM\u548c\u8bed\u4e49\u5224\u65ad\u5668\u7684\u53cc\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u786e\u4fdd\u6240\u6709\u91cd\u8ff0\u5185\u5bb9\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002\u5728\u4e94\u4e2a\u5206\u7c7b\u4efb\u52a1\u548c\u4e09\u79cdLLM\u67b6\u6784\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cOAT-Rephrase\u80fd\u6301\u7eed\u63d0\u5347MeZO\u5fae\u8c03\u6027\u80fd\uff0c\u901a\u5e38\u7f29\u5c0f\u751a\u81f3\u6d88\u9664\u4e0e\u4e00\u9636\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u611f\u77e5\u7684\u91cd\u8ff0\u662f\u4e00\u79cd\u53ef\u91cd\u7528\u4e14\u4f4e\u5f00\u9500\u7684\u96f6\u9636\u4f18\u5316\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2506.18621", "pdf": "https://arxiv.org/pdf/2506.18621", "abs": "https://arxiv.org/abs/2506.18621", "authors": ["Alisa Barkar", "Mathieu Chollet", "Matthieu Labeau", "Beatrice Biancardi", "Chloe Clavel"], "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches", "categories": ["cs.CL"], "comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables", "summary": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u4fee\u6539\u6f14\u8bb2\u6587\u672c\u6765\u7406\u89e3\u8bf4\u670d\u529b\uff0c\u53d1\u73b0GPT-4o\u901a\u8fc7\u7cfb\u7edf\u6027\u98ce\u683c\u8c03\u6574\u800c\u975e\u4eba\u7c7b\u5f0f\u4f18\u5316\u6765\u589e\u5f3a\u4fee\u8f9e\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5982\u4f55\u7406\u89e3\u548c\u751f\u6210\u5177\u6709\u8bf4\u670d\u529b\u7684\u516c\u5171\u6f14\u8bb2\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u4fee\u6539\u6f14\u8bb2\u6587\u672c\u6765\u5206\u6790\u5176\u8bed\u8a00\u53d8\u5316\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u6cd5\u56fd3MT\u6570\u636e\u96c6\u4e2d\u7684\u535a\u58eb\u5019\u9009\u4eba\u6f14\u8bb2\u6587\u672c\uff0c\u901a\u8fc7GPT-4o\u751f\u6210\u589e\u5f3a\u6216\u51cf\u5f31\u8bf4\u670d\u529b\u7684\u7248\u672c\uff0c\u5e76\u5206\u6790\u5176\u4fee\u8f9e\u624b\u6cd5\u548c\u8bdd\u8bed\u6807\u8bb0\u7684\u8bed\u8a00\u53d8\u5316\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cGPT-4o\u901a\u8fc7\u64cd\u7eb5\u60c5\u611f\u8bcd\u6c47\u548c\u53e5\u6cd5\u7ed3\u6784\uff08\u5982\u7591\u95ee\u53e5\u548c\u611f\u53f9\u53e5\uff09\u6765\u589e\u5f3a\u4fee\u8f9e\u6548\u679c\uff0c\u800c\u975e\u4ee5\u4eba\u7c7b\u65b9\u5f0f\u4f18\u5316\u8bf4\u670d\u529b\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cGPT-4o\u5728\u4fee\u6539\u6f14\u8bb2\u65f6\u66f4\u503e\u5411\u4e8e\u7cfb\u7edf\u6027\u98ce\u683c\u8c03\u6574\uff0c\u800c\u975e\u6a21\u4eff\u4eba\u7c7b\u8bf4\u670d\u529b\u4f18\u5316\u7b56\u7565\u3002", "paper_title_zh": "\u6f14\u8bb2\u8bf4\u670d\u529b\u7684\u89e3\u5256\uff1aLLM\u4fee\u6539\u6f14\u8bb2\u4e2d\u7684\u8bed\u8a00\u53d8\u5316", "abstract_zh": "\u672c\u7814\u7a76\u901a\u8fc7\u4fee\u6539\u6cd5\u56fd\u201cMa These en 180 Secondes\u201d\u7ade\u8d5b\u4e2d\u535a\u58eb\u5019\u9009\u4eba\u7684\u6f14\u8bb2\u6587\u672c\uff0c\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7406\u89e3\u516c\u5171\u6f14\u8bb2\u4e2d\u7684\u8bf4\u670d\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u548c\u53ef\u89e3\u91ca\u7684\u6587\u672c\u7279\u5f81\u96c6\uff0c\u6574\u5408\u4e86\u4fee\u8f9e\u624b\u6cd5\u548c\u8bdd\u8bed\u6807\u8bb0\u3002\u901a\u8fc7\u63d0\u793aGPT-4o\u589e\u5f3a\u6216\u51cf\u5f31\u8bf4\u670d\u529b\uff0c\u5e76\u5206\u6790\u539f\u59cb\u4e0e\u751f\u6210\u6f14\u8bb2\u4e4b\u95f4\u7684\u8bed\u8a00\u53d8\u5316\u3002\u7ed3\u679c\u8868\u660e\uff0cGPT-4o\u91c7\u7528\u7cfb\u7edf\u6027\u98ce\u683c\u8c03\u6574\u800c\u975e\u4eba\u7c7b\u5f0f\u4f18\u5316\u8bf4\u670d\u529b\uff0c\u5c24\u5176\u901a\u8fc7\u64cd\u7eb5\u60c5\u611f\u8bcd\u6c47\u548c\u53e5\u6cd5\u7ed3\u6784\uff08\u5982\u7591\u95ee\u53e5\u548c\u611f\u53f9\u53e5\uff09\u6765\u653e\u5927\u4fee\u8f9e\u6548\u679c\u3002"}}
{"id": "2506.17996", "pdf": "https://arxiv.org/pdf/2506.17996", "abs": "https://arxiv.org/abs/2506.17996", "authors": ["David Tolpin", "Sefy Kagarlitsky"], "title": "Fast Neural Inverse Kinematics on Human Body Motions", "categories": ["cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Markerless motion capture enables the tracking of human motion without\nrequiring physical markers or suits, offering increased flexibility and reduced\ncosts compared to traditional systems. However, these advantages often come at\nthe expense of higher computational demands and slower inference, limiting\ntheir applicability in real-time scenarios. In this technical report, we\npresent a fast and reliable neural inverse kinematics framework designed for\nreal-time capture of human body motions from 3D keypoints. We describe the\nnetwork architecture, training methodology, and inference procedure in detail.\nOur framework is evaluated both qualitatively and quantitatively, and we\nsupport key design decisions through ablation studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u795e\u7ecf\u9006\u8fd0\u52a8\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u6355\u6349\u4eba\u4f53\u8fd0\u52a8\u76843D\u5173\u952e\u70b9\uff0c\u89e3\u51b3\u4e86\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u8ba1\u7b97\u9700\u6c42\u9ad8\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u867d\u7136\u7075\u6d3b\u4e14\u6210\u672c\u4f4e\uff0c\u4f46\u901a\u5e38\u8ba1\u7b97\u9700\u6c42\u9ad8\u4e14\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be6\u7ec6\u63cf\u8ff0\u4e86\u7f51\u7edc\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u63a8\u7406\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u3002", "result": "\u6846\u67b6\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u65f6\u6355\u6349\u4eba\u4f53\u8fd0\u52a8\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u795e\u7ecf\u9006\u8fd0\u52a8\u5b66\u6846\u67b6\u4e3a\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5feb\u901f\u4eba\u4f53\u8fd0\u52a8\u9006\u8fd0\u52a8\u5b66", "abstract_zh": "\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u65e0\u9700\u7269\u7406\u6807\u8bb0\u6216\u670d\u88c5\uff0c\u4e0e\u4f20\u7edf\u7cfb\u7edf\u76f8\u6bd4\u66f4\u5177\u7075\u6d3b\u6027\u548c\u6210\u672c\u4f18\u52bf\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4f18\u52bf\u901a\u5e38\u4f34\u968f\u7740\u66f4\u9ad8\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u8f83\u6162\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6280\u672f\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u53ef\u9760\u7684\u795e\u7ecf\u9006\u8fd0\u52a8\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u4ece3D\u5173\u952e\u70b9\u5b9e\u65f6\u6355\u6349\u4eba\u4f53\u8fd0\u52a8\u3002\u6211\u4eec\u8be6\u7ec6\u63cf\u8ff0\u4e86\u7f51\u7edc\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u63a8\u7406\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u652f\u6301\u4e86\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u3002"}}
{"id": "2506.17265", "pdf": "https://arxiv.org/pdf/2506.17265", "abs": "https://arxiv.org/abs/2506.17265", "authors": ["Xianren Zhang", "Hui Liu", "Delvin Ce Zhang", "Xianfeng Tang", "Qi He", "Dongwon Lee", "Suhang Wang"], "title": "Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "Multimodal Large Language Models (MLLMs) trained on massive data may memorize\nsensitive personal information and photos, posing serious privacy risks. To\nmitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to\nreduce the ``forget'' sensitive information. However, it remains unclear\nwhether the knowledge has been truly forgotten or just hidden in the model.\nTherefore, we propose to study a novel problem of LLM unlearning attack, which\naims to recover the unlearned knowledge of an unlearned LLM. To achieve the\ngoal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework\nthat learns a universal noise pattern. When applied to input images, this noise\ncan trigger the model to reveal unlearned content. While pixel-level\nperturbations may be visually subtle, they can be detected in the semantic\nembedding space, making such attacks vulnerable to potential defenses. To\nimprove stealthiness, we introduce an embedding alignment loss that minimizes\nthe difference between the perturbed and denoised image embeddings, ensuring\nthe attack is semantically unnoticeable. Experimental results show that SUA can\neffectively recover unlearned information from MLLMs. Furthermore, the learned\nnoise generalizes well: a single perturbation trained on a subset of samples\ncan reveal forgotten content in unseen images. This indicates that knowledge\nreappearance is not an occasional failure, but a consistent behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u9690\u79d8\u9057\u5fd8\u653b\u51fb\uff08SUA\uff09\u2019\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63ed\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u9057\u5fd8\u654f\u611f\u4fe1\u606f\u540e\u662f\u5426\u771f\u6b63\u9057\u5fd8\uff0c\u8fd8\u662f\u4ec5\u9690\u85cf\u4e86\u8fd9\u4e9b\u4fe1\u606f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cSUA\u80fd\u6709\u6548\u6062\u590d\u88ab\u9057\u5fd8\u7684\u4fe1\u606f\uff0c\u4e14\u653b\u51fb\u5177\u6709\u666e\u9002\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u53ef\u80fd\u8bb0\u5fc6\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u548c\u7167\u7247\uff0c\u5e26\u6765\u9690\u79c1\u98ce\u9669\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u4ee5\u2018\u9057\u5fd8\u2019\u654f\u611f\u4fe1\u606f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u4fe1\u606f\u662f\u5426\u771f\u6b63\u88ab\u9057\u5fd8\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u653b\u51fb\u65b9\u6cd5\u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u771f\u6b63\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u2018\u9690\u79d8\u9057\u5fd8\u653b\u51fb\uff08SUA\uff09\u2019\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u901a\u7528\u566a\u58f0\u6a21\u5f0f\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u4ee5\u89e6\u53d1\u6a21\u578b\u63ed\u793a\u88ab\u9057\u5fd8\u5185\u5bb9\u3002\u4e3a\u63d0\u9ad8\u9690\u853d\u6027\uff0c\u5f15\u5165\u5d4c\u5165\u5bf9\u9f50\u635f\u5931\uff0c\u786e\u4fdd\u653b\u51fb\u5728\u8bed\u4e49\u4e0a\u4e0d\u53ef\u5bdf\u89c9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSUA\u80fd\u6709\u6548\u6062\u590dMLLM\u4e2d\u88ab\u9057\u5fd8\u7684\u4fe1\u606f\uff0c\u4e14\u5b66\u4e60\u7684\u566a\u58f0\u5177\u6709\u666e\u9002\u6027\uff0c\u5355\u4e2a\u566a\u58f0\u6a21\u5f0f\u53ef\u5e94\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u3002\u8fd9\u8868\u660e\u77e5\u8bc6\u91cd\u73b0\u662f\u6a21\u578b\u7684\u4e00\u81f4\u884c\u4e3a\uff0c\u800c\u975e\u5076\u7136\u5931\u8d25\u3002", "conclusion": "MLLM\u7684\u9057\u5fd8\u673a\u5236\u53ef\u80fd\u4ec5\u9690\u85cf\u800c\u975e\u771f\u6b63\u9057\u5fd8\u654f\u611f\u4fe1\u606f\uff0cSUA\u653b\u51fb\u63ed\u793a\u4e86\u8fd9\u4e00\u6f5c\u5728\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u9057\u5fd8\uff1f\u9690\u79d8\u7684MLLM\u9057\u5fd8\u653b\u51fb", "abstract_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e2d\u53ef\u80fd\u8bb0\u5fc6\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u548c\u7167\u7247\uff0c\u5e26\u6765\u4e25\u91cd\u9690\u79c1\u98ce\u9669\u3002\u4e3a\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MLLM\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u4ee5\u51cf\u5c11\u2018\u9057\u5fd8\u2019\u654f\u611f\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4fe1\u606f\u662f\u5426\u771f\u6b63\u88ab\u9057\u5fd8\u4ecd\u4e0d\u660e\u786e\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u7684LLM\u9057\u5fd8\u653b\u51fb\u95ee\u9898\uff0c\u65e8\u5728\u6062\u590d\u88ab\u9057\u5fd8\u7684LLM\u77e5\u8bc6\u3002\u4e3a\u5b9e\u73b0\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u9690\u79d8\u9057\u5fd8\u653b\u51fb\uff08SUA\uff09\u2019\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u901a\u7528\u566a\u58f0\u6a21\u5f0f\uff0c\u5f53\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u65f6\uff0c\u53ef\u89e6\u53d1\u6a21\u578b\u63ed\u793a\u88ab\u9057\u5fd8\u5185\u5bb9\u3002\u5c3d\u7ba1\u50cf\u7d20\u7ea7\u6270\u52a8\u5728\u89c6\u89c9\u4e0a\u53ef\u80fd\u4e0d\u660e\u663e\uff0c\u4f46\u5728\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u53ef\u88ab\u68c0\u6d4b\uff0c\u4f7f\u6b64\u7c7b\u653b\u51fb\u6613\u53d7\u6f5c\u5728\u9632\u5fa1\u5f71\u54cd\u3002\u4e3a\u63d0\u9ad8\u9690\u853d\u6027\uff0c\u5f15\u5165\u4e86\u5d4c\u5165\u5bf9\u9f50\u635f\u5931\uff0c\u6700\u5c0f\u5316\u6270\u52a8\u4e0e\u53bb\u566a\u56fe\u50cf\u5d4c\u5165\u95f4\u7684\u5dee\u5f02\uff0c\u786e\u4fdd\u653b\u51fb\u5728\u8bed\u4e49\u4e0a\u4e0d\u53ef\u5bdf\u89c9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSUA\u80fd\u6709\u6548\u6062\u590dMLLM\u4e2d\u88ab\u9057\u5fd8\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u7684\u566a\u58f0\u5177\u6709\u666e\u9002\u6027\uff1a\u5728\u90e8\u5206\u6837\u672c\u4e0a\u8bad\u7ec3\u7684\u5355\u4e2a\u6270\u52a8\u53ef\u63ed\u793a\u672a\u89c1\u56fe\u50cf\u4e2d\u7684\u9057\u5fd8\u5185\u5bb9\u3002\u8fd9\u8868\u660e\u77e5\u8bc6\u91cd\u73b0\u5e76\u975e\u5076\u7136\u5931\u8d25\uff0c\u800c\u662f\u6a21\u578b\u7684\u4e00\u81f4\u884c\u4e3a\u3002"}}
{"id": "2506.18639", "pdf": "https://arxiv.org/pdf/2506.18639", "abs": "https://arxiv.org/abs/2506.18639", "authors": ["Z\u00e9bulon Goriely", "Suchir Salhan", "Pietro Lesci", "Julius Cheng", "Paula Buttery"], "title": "ByteSpan: Information-Driven Subword Tokenisation", "categories": ["cs.CL"], "comment": "Accepted to TokShop 2025 (Non-archival)", "summary": "Recent dynamic tokenisation methods operate directly on bytes and pool their\nlatent representations into patches. This bears similarities to computational\nmodels of word segmentation that determine lexical boundaries using spikes in\nan autoregressive model's prediction error. Inspired by this connection, we\nexplore whether grouping predictable bytes - rather than pooling their\nrepresentations - can yield a useful fixed subword vocabulary. We propose a new\ninformation-driven subword tokeniser, ByteSpan, that uses an external\nbyte-level LM during training to identify contiguous predictable byte sequences\nand group them into subwords. Experiments show that ByteSpan yields efficient\nvocabularies with higher morphological alignment scores than BPE for English.\nMultilingual experiments show similar compression and R\\'enyi efficiency for 25\nlanguages.", "AI": {"tldr": "ByteSpan\u662f\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u9a71\u52a8\u7684\u5b50\u8bcd\u5206\u8bcd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u5c06\u53ef\u9884\u6d4b\u7684\u5b57\u8282\u5e8f\u5217\u5206\u7ec4\u4e3a\u5b50\u8bcd\uff0c\u751f\u6210\u9ad8\u6548\u7684\u8bcd\u6c47\u8868\uff0c\u5e76\u5728\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eBPE\u3002", "motivation": "\u8fd1\u5e74\u6765\u52a8\u6001\u5206\u8bcd\u65b9\u6cd5\u76f4\u63a5\u5728\u5b57\u8282\u4e0a\u64cd\u4f5c\uff0c\u5e76\u5c06\u5176\u6f5c\u5728\u8868\u793a\u805a\u5408\u4e3a\u5757\u3002\u8fd9\u4e0e\u8ba1\u7b97\u6a21\u578b\u4e2d\u7684\u5206\u8bcd\u65b9\u6cd5\u7c7b\u4f3c\uff0c\u540e\u8005\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\u5cf0\u503c\u786e\u5b9a\u8bcd\u6c47\u8fb9\u754c\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5206\u7ec4\u53ef\u9884\u6d4b\u5b57\u8282\u800c\u975e\u805a\u5408\u5176\u8868\u793a\u6765\u751f\u6210\u6709\u7528\u7684\u56fa\u5b9a\u5b50\u8bcd\u8bcd\u6c47\u8868\u3002", "method": "\u63d0\u51faByteSpan\uff0c\u4e00\u79cd\u4fe1\u606f\u9a71\u52a8\u7684\u5b50\u8bcd\u5206\u8bcd\u5668\uff0c\u8bad\u7ec3\u65f6\u4f7f\u7528\u5916\u90e8\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u8fde\u7eed\u53ef\u9884\u6d4b\u7684\u5b57\u8282\u5e8f\u5217\uff0c\u5e76\u5c06\u5176\u5206\u7ec4\u4e3a\u5b50\u8bcd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cByteSpan\u751f\u6210\u7684\u8bcd\u6c47\u8868\u6548\u7387\u9ad8\uff0c\u82f1\u8bed\u5f62\u6001\u5bf9\u9f50\u5206\u6570\u4f18\u4e8eBPE\u3002\u591a\u8bed\u8a00\u5b9e\u9a8c\u572825\u79cd\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u538b\u7f29\u548cR\u00e9nyi\u6548\u7387\u3002", "conclusion": "ByteSpan\u901a\u8fc7\u4fe1\u606f\u9a71\u52a8\u7684\u65b9\u6cd5\u751f\u6210\u9ad8\u6548\u7684\u5b50\u8bcd\u8bcd\u6c47\u8868\uff0c\u5728\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5206\u8bcd\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "paper_title_zh": "ByteSpan\uff1a\u4fe1\u606f\u9a71\u52a8\u7684\u5b50\u8bcd\u5206\u8bcd\u65b9\u6cd5", "abstract_zh": "\u6700\u8fd1\u7684\u52a8\u6001\u5206\u8bcd\u65b9\u6cd5\u76f4\u63a5\u5728\u5b57\u8282\u4e0a\u64cd\u4f5c\uff0c\u5e76\u5c06\u5176\u6f5c\u5728\u8868\u793a\u805a\u5408\u4e3a\u5757\u3002\u8fd9\u4e0e\u8ba1\u7b97\u6a21\u578b\u4e2d\u7684\u5206\u8bcd\u65b9\u6cd5\u7c7b\u4f3c\uff0c\u540e\u8005\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\u5cf0\u503c\u786e\u5b9a\u8bcd\u6c47\u8fb9\u754c\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5206\u7ec4\u53ef\u9884\u6d4b\u5b57\u8282\u800c\u975e\u805a\u5408\u5176\u8868\u793a\u6765\u751f\u6210\u6709\u7528\u7684\u56fa\u5b9a\u5b50\u8bcd\u8bcd\u6c47\u8868\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u9a71\u52a8\u5b50\u8bcd\u5206\u8bcd\u5668ByteSpan\uff0c\u8bad\u7ec3\u65f6\u4f7f\u7528\u5916\u90e8\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u8fde\u7eed\u53ef\u9884\u6d4b\u7684\u5b57\u8282\u5e8f\u5217\uff0c\u5e76\u5c06\u5176\u5206\u7ec4\u4e3a\u5b50\u8bcd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cByteSpan\u751f\u6210\u7684\u8bcd\u6c47\u8868\u6548\u7387\u9ad8\uff0c\u82f1\u8bed\u5f62\u6001\u5bf9\u9f50\u5206\u6570\u4f18\u4e8eBPE\u3002\u591a\u8bed\u8a00\u5b9e\u9a8c\u572825\u79cd\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u538b\u7f29\u548cR\u00e9nyi\u6548\u7387\u3002"}}
{"id": "2506.18006", "pdf": "https://arxiv.org/pdf/2506.18006", "abs": "https://arxiv.org/abs/2506.18006", "authors": ["Shuaiyu Chen", "Fu Wang", "Peng Ren", "Chunbo Luo", "Zeyu Fu"], "title": "OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation is commonly used for Oil Spill Detection (OSD) in\nremote sensing images. However, the limited availability of labelled oil spill\nsamples and class imbalance present significant challenges that can reduce\ndetection accuracy. Furthermore, most existing methods, which rely on\nconvolutional neural networks (CNNs), struggle to detect small oil spill areas\ndue to their limited receptive fields and inability to effectively capture\nglobal contextual information. This study explores the potential of State-Space\nModels (SSMs), particularly Mamba, to overcome these limitations, building on\ntheir recent success in vision applications. We propose OSDMamba, the first\nMamba-based architecture specifically designed for oil spill detection.\nOSDMamba leverages Mamba's selective scanning mechanism to effectively expand\nthe model's receptive field while preserving critical details. Moreover, we\ndesigned an asymmetric decoder incorporating ConvSSM and deep supervision to\nstrengthen multi-scale feature fusion, thereby enhancing the model's\nsensitivity to minority class samples. Experimental results show that the\nproposed OSDMamba achieves state-of-the-art performance, yielding improvements\nof 8.9% and 11.8% in OSD across two publicly available datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOSDMamba\uff0c\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u6ea2\u6cb9\u68c0\u6d4b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u7684\u6ea2\u6cb9\u68c0\u6d4b\u65b9\u6cd5\u56e0\u611f\u53d7\u91ce\u6709\u9650\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u6355\u6349\u4e0d\u8db3\uff0c\u96be\u4ee5\u68c0\u6d4b\u5c0f\u9762\u79ef\u6ea2\u6cb9\u533a\u57df\uff0c\u4e14\u6807\u8bb0\u6837\u672c\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faOSDMamba\u67b6\u6784\uff0c\u5229\u7528Mamba\u7684\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\u6269\u5c55\u6a21\u578b\u611f\u53d7\u91ce\uff0c\u540c\u65f6\u8bbe\u8ba1\u975e\u5bf9\u79f0\u89e3\u7801\u5668\u7ed3\u5408ConvSSM\u548c\u6df1\u5ea6\u76d1\u7763\uff0c\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u5bf9\u5c11\u6570\u7c7b\u6837\u672c\u7684\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOSDMamba\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6ea2\u6cb9\u68c0\u6d4b\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e868.9%\u548c11.8%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "OSDMamba\u901a\u8fc7\u7ed3\u5408Mamba\u7684\u9009\u62e9\u6027\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6ea2\u6cb9\u68c0\u6d4b\u4e2d\u7684\u5c0f\u533a\u57df\u68c0\u6d4b\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "OSDMamba\uff1a\u57fa\u4e8e\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u9065\u611f\u56fe\u50cf\u6ea2\u6cb9\u68c0\u6d4b\u589e\u5f3a\u65b9\u6cd5", "abstract_zh": "\u8bed\u4e49\u5206\u5272\u5e38\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u6ea2\u6cb9\u68c0\u6d4b\uff08OSD\uff09\uff0c\u4f46\u6807\u8bb0\u6837\u672c\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u663e\u8457\u964d\u4f4e\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u73b0\u6709\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\u56e0\u611f\u53d7\u91ce\u6709\u9650\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u6355\u6349\u4e0d\u8db3\uff0c\u96be\u4ee5\u68c0\u6d4b\u5c0f\u9762\u79ef\u6ea2\u6cb9\u533a\u57df\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\uff0c\u5c24\u5176\u662fMamba\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u57fa\u4e8e\u5176\u5728\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u6210\u529f\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u4e3a\u6ea2\u6cb9\u68c0\u6d4b\u8bbe\u8ba1\u7684Mamba\u67b6\u6784OSDMamba\u3002OSDMamba\u5229\u7528Mamba\u7684\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\u6709\u6548\u6269\u5c55\u6a21\u578b\u611f\u53d7\u91ce\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u7ed3\u5408ConvSSM\u548c\u6df1\u5ea6\u76d1\u7763\u7684\u975e\u5bf9\u79f0\u89e3\u7801\u5668\uff0c\u4ee5\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u5c11\u6570\u7c7b\u6837\u672c\u7684\u654f\u611f\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOSDMamba\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6ea2\u6cb9\u68c0\u6d4b\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e868.9%\u548c11.8%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2506.17267", "pdf": "https://arxiv.org/pdf/2506.17267", "abs": "https://arxiv.org/abs/2506.17267", "authors": ["Jusheng Zhang", "Kaitong Cai", "Yijia Fan", "Jian Wang", "Keze Wang"], "title": "CF-VLM:CounterFactual Vision-Language Fine-tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) have greatly improved\ncross-modal semantic understanding, yet significant limitations remain in\nfine-grained discrimination and deep causal reasoning tasks. Existing VLMs\noften rely on superficial statistical correlations, lacking the ability to\ncapture the underlying causal logic between visual and textual content. To\naddress this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a\nnovel framework that enhances the causal reasoning capabilities of VLMs through\nthe targeted use of counterfactual samples. CF-VLM introduces three\ncomplementary training objectives: maintaining foundational cross-modal\nalignment, reinforcing the uniqueness and stability of factual scene\nrepresentations against coherent counterfactuals, and sharpening the model's\nsensitivity to minimal but critical causal edits. Extensive experiments\ndemonstrate that CF-VLM consistently outperforms strong baselines and\nstate-of-the-art methods on compositional reasoning and generalization\nbenchmarks. Furthermore, it shows promise in mitigating visual hallucinations,\nindicating improved factual consistency. Our CF-VLM provides a robust\nfoundation for deploying VLMs in high-stakes, real-world scenarios requiring\nreliable reasoning and interpretability.", "AI": {"tldr": "CF-VLM\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53cd\u4e8b\u5b9e\u6837\u672c\u589e\u5f3a\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u5224\u522b\u548c\u6df1\u5ea6\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4f9d\u8d56\u8868\u9762\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u7f3a\u4e4f\u6355\u6349\u89c6\u89c9\u4e0e\u6587\u672c\u5185\u5bb9\u95f4\u6df1\u5c42\u56e0\u679c\u903b\u8f91\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ec6\u7c92\u5ea6\u5224\u522b\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "CF-VLM\u63d0\u51fa\u4e09\u79cd\u4e92\u8865\u8bad\u7ec3\u76ee\u6807\uff1a\u4fdd\u6301\u8de8\u6a21\u6001\u5bf9\u9f50\u57fa\u7840\u3001\u5f3a\u5316\u4e8b\u5b9e\u573a\u666f\u8868\u793a\u7684\u552f\u4e00\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u5bf9\u5173\u952e\u56e0\u679c\u7f16\u8f91\u7684\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCF-VLM\u5728\u7ec4\u5408\u63a8\u7406\u548c\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u524d\u6cbf\u6280\u672f\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u89c6\u89c9\u5e7b\u89c9\uff0c\u63d0\u5347\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "conclusion": "CF-VLM\u4e3a\u9ad8\u98ce\u9669\u7684\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u57fa\u7840\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "paper_title_zh": "CF-VLM\uff1a\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5fae\u8c03", "abstract_zh": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8de8\u6a21\u6001\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u5224\u522b\u548c\u6df1\u5ea6\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002\u73b0\u6709VLMs\u901a\u5e38\u4f9d\u8d56\u8868\u9762\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u89c6\u89c9\u4e0e\u6587\u672c\u5185\u5bb9\u95f4\u7684\u6df1\u5c42\u56e0\u679c\u903b\u8f91\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff08CF-VLM\uff09\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u4f7f\u7528\u53cd\u4e8b\u5b9e\u6837\u672c\u589e\u5f3aVLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002CF-VLM\u5f15\u5165\u4e09\u79cd\u4e92\u8865\u8bad\u7ec3\u76ee\u6807\uff1a\u4fdd\u6301\u57fa\u7840\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u5f3a\u5316\u4e8b\u5b9e\u573a\u666f\u8868\u793a\u5728\u4e00\u81f4\u53cd\u4e8b\u5b9e\u4e0b\u7684\u552f\u4e00\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4ee5\u53ca\u63d0\u5347\u6a21\u578b\u5bf9\u5173\u952e\u56e0\u679c\u7f16\u8f91\u7684\u654f\u611f\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCF-VLM\u5728\u7ec4\u5408\u63a8\u7406\u548c\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u53ca\u524d\u6cbf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5176\u5728\u51cf\u5c11\u89c6\u89c9\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u8868\u660e\u4e8b\u5b9e\u4e00\u81f4\u6027\u5f97\u5230\u6539\u5584\u3002CF-VLM\u4e3a\u9700\u8981\u53ef\u9760\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u7684\u9ad8\u98ce\u9669\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u7a33\u5065\u7684VLM\u90e8\u7f72\u57fa\u7840\u3002"}}
{"id": "2506.18674", "pdf": "https://arxiv.org/pdf/2506.18674", "abs": "https://arxiv.org/abs/2506.18674", "authors": ["Raquel Ferrando", "Javier Conde", "Gonzalo Mart\u00ednez", "Pedro Reviriego"], "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e3a\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u4f18\u5316\u7684\u5206\u8bcd\u5668\u80fd\u51cf\u5c115%\u81f310%\u7684token\u6570\u91cf\uff0c\u4ece\u800c\u663e\u8457\u8282\u7701\u80fd\u6e90\uff0c\u540c\u65f6\u5bf9\u539f\u59cb\u8bad\u7ec3\u8bed\u6599\u7684\u5206\u8bcd\u6548\u7387\u5f71\u54cd\u6781\u5c0f\u751a\u81f3\u7565\u6709\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba1\u7b97\u548c\u80fd\u6e90\u6210\u672c\u968f\u6a21\u578b\u89c4\u6a21\u548c\u7528\u6237\u6570\u91cf\u6fc0\u589e\uff0c\u800c\u5206\u8bcd\u5668\u5728\u6a21\u578b\u6548\u7387\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3aLLM\u7684\u6d41\u884c\u5e94\u7528\uff0c\u5176\u7528\u6237\u8f93\u5165\u548c\u56de\u590d\u7684\u6587\u672c\u4e0e\u8bad\u7ec3\u8bed\u6599\u4e0d\u540c\uff0c\u56e0\u6b64\u63a2\u7d22\u4f18\u5316\u5206\u8bcd\u5668\u662f\u5426\u80fd\u4e3a\u804a\u5929\u5bf9\u8bdd\u5e26\u6765\u6548\u76ca\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u516c\u5f00\u7684\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u8bcd\u6c47\u8868\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u804a\u5929\u5bf9\u8bdd\u9886\u57df\u7684\u6027\u80fd\u3002", "result": "\u4f18\u5316\u540e\u7684\u5206\u8bcd\u5668\u5728\u804a\u5929\u5bf9\u8bdd\u4e2d\u663e\u8457\u51cf\u5c11token\u6570\u91cf\uff085%-10%\uff09\uff0c\u540c\u65f6\u539f\u59cb\u8bad\u7ec3\u8bed\u6599\u7684\u5206\u8bcd\u6548\u7387\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\u6216\u7565\u6709\u63d0\u5347\u3002", "conclusion": "\u4e3a\u804a\u5929\u5bf9\u8bdd\u4f18\u5316\u7684\u5206\u8bcd\u5668\u80fd\u6709\u6548\u964d\u4f4e\u80fd\u6e90\u6d88\u8017\uff0c\u4e14\u4e0d\u5f71\u54cd\u539f\u59cb\u8bed\u6599\u7684\u5206\u8bcd\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u662f\u5426\u5b58\u5728\u804a\u5929\u5bf9\u8bdd\u4f18\u5316\u7684\u5206\u8bcd\u5668\uff1f", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba1\u7b97\u548c\u80fd\u6e90\u6210\u672c\u56e0\u6a21\u578b\u89c4\u6a21\u6269\u5927\u548c\u7528\u6237\u6570\u91cf\u6fc0\u589e\u800c\u5448\u6307\u6570\u589e\u957f\u3002\u5206\u8bcd\u5668\u4f5c\u4e3a\u6a21\u578b\u6548\u7387\u7684\u5173\u952e\u56e0\u7d20\uff0c\u901a\u5e38\u9488\u5bf9\u8bad\u7ec3\u8bed\u6599\u4f18\u5316\u4ee5\u51cf\u5c11token\u6570\u91cf\u3002\u7136\u800c\uff0c\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3aLLM\u7684\u4e3b\u8981\u5e94\u7528\u4e4b\u4e00\uff0c\u5176\u7528\u6237\u8f93\u5165\u548c\u56de\u590d\u7684\u6587\u672c\u4e0e\u8bad\u7ec3\u8bed\u6599\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u4e3a\u804a\u5929\u5bf9\u8bdd\u4f18\u5316\u5206\u8bcd\u5668\u7684\u6f5c\u5728\u6548\u76ca\u3002\u901a\u8fc7\u4f7f\u7528\u516c\u5f00\u7684\u804a\u5929\u5bf9\u8bdd\u8bed\u6599\u5e93\u91cd\u65b0\u8bbe\u8ba1\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5316\u540e\u7684\u5206\u8bcd\u5668\u5728\u804a\u5929\u5bf9\u8bdd\u4e2d\u80fd\u6301\u7eed\u51cf\u5c11token\u6570\u91cf\uff085%-10%\uff09\uff0c\u4ece\u800c\u663e\u8457\u8282\u7701\u80fd\u6e90\uff0c\u540c\u65f6\u5bf9\u539f\u59cb\u8bad\u7ec3\u8bed\u6599\u7684\u5206\u8bcd\u6548\u7387\u5f71\u54cd\u6781\u5c0f\u751a\u81f3\u7565\u6709\u63d0\u5347\u3002"}}
{"id": "2506.18021", "pdf": "https://arxiv.org/pdf/2506.18021", "abs": "https://arxiv.org/abs/2506.18021", "authors": ["Chi Xie", "Shuang Liang", "Jie Li", "Feng Zhu", "Rui Zhao", "Yichen Wei", "Shengjie Zhao"], "title": "On the Robustness of Human-Object Interaction Detection against Distribution Shift", "categories": ["cs.CV", "cs.MM"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Human-Object Interaction (HOI) detection has seen substantial advances in\nrecent years. However, existing works focus on the standard setting with ideal\nimages and natural distribution, far from practical scenarios with inevitable\ndistribution shifts. This hampers the practical applicability of HOI detection.\nIn this work, we investigate this issue by benchmarking, analyzing, and\nenhancing the robustness of HOI detection models under various distribution\nshifts. We start by proposing a novel automated approach to create the first\nrobustness evaluation benchmark for HOI detection. Subsequently, we evaluate\nmore than 40 existing HOI detection models on this benchmark, showing their\ninsufficiency, analyzing the features of different frameworks, and discussing\nhow the robustness in HOI is different from other tasks. With the insights from\nsuch analyses, we propose to improve the robustness of HOI detection methods\nthrough: (1) a cross-domain data augmentation integrated with mixup, and (2) a\nfeature fusion strategy with frozen vision foundation models. Both are simple,\nplug-and-play, and applicable to various methods. Our experimental results\ndemonstrate that the proposed approach significantly increases the robustness\nof various methods, with benefits on standard benchmarks, too. The dataset and\ncode will be released.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u5206\u6790\u4e8640\u591a\u79cd\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u8fd1\u5e74\u6765\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u7406\u60f3\u56fe\u50cf\u548c\u81ea\u7136\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u9996\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u51c6\u7528\u4e8eHOI\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u6d4b\u8bd5\uff1b2. \u8bc4\u4f3040\u591a\u79cd\u73b0\u6709\u6a21\u578b\u5e76\u5206\u6790\u5176\u4e0d\u8db3\uff1b3. \u63d0\u51fa\u8de8\u57df\u6570\u636e\u589e\u5f3a\u7ed3\u5408mixup\u7684\u65b9\u6cd5\uff1b4. \u5f15\u5165\u57fa\u4e8e\u51bb\u7ed3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cdHOI\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u6709\u989d\u5916\u6536\u76ca\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u548c\u7b80\u5355\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86HOI\u68c0\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "paper_title_zh": "\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7406\u60f3\u56fe\u50cf\u548c\u81ea\u7136\u5206\u5e03\u7684\u6807\u51c6\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9650\u5236\u4e86HOI\u68c0\u6d4b\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002\u672c\u6587\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u3001\u5206\u6790\u548c\u589e\u5f3aHOI\u68c0\u6d4b\u6a21\u578b\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u8ba8\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u9996\u4e2aHOI\u68c0\u6d4b\u9c81\u68d2\u6027\u8bc4\u4f30\u57fa\u51c6\u3002\u968f\u540e\uff0c\u8bc4\u4f30\u4e8640\u591a\u79cd\u73b0\u6709HOI\u68c0\u6d4b\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5176\u4e0d\u8db3\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6846\u67b6\u7684\u7279\u70b9\uff0c\u5e76\u8ba8\u8bba\u4e86HOI\u4efb\u52a1\u4e2d\u9c81\u68d2\u6027\u4e0e\u5176\u4ed6\u4efb\u52a1\u7684\u5dee\u5f02\u3002\u57fa\u4e8e\u8fd9\u4e9b\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u6cd5\u63d0\u5347HOI\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff1a\uff081\uff09\u7ed3\u5408mixup\u7684\u8de8\u57df\u6570\u636e\u589e\u5f3a\uff1b\uff082\uff09\u57fa\u4e8e\u51bb\u7ed3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u6709\u989d\u5916\u6536\u76ca\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.17272", "pdf": "https://arxiv.org/pdf/2506.17272", "abs": "https://arxiv.org/abs/2506.17272", "authors": ["Youzheng Liu", "Jiyan Liu", "Xiaoman Xu", "Taihang Wang", "Yimin Wang", "Ye Jiang"], "title": "QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7", "AI": {"tldr": "QUST_NLP\u56e2\u961f\u5728SemEval-2025\u4efb\u52a17\u4e2d\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\uff0c\u6700\u7ec8\u5728\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u8d5b\u9053\u5206\u522b\u83b7\u5f97\u7b2c5\u548c\u7b2c7\u540d\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0cQUST_NLP\u56e2\u961f\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u68c0\u7d22\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u8bc4\u4f30\u5e76\u9009\u62e9\u6700\u4f73\u5019\u9009\u68c0\u7d22\u6a21\u578b\uff1b\u5176\u6b21\u4f7f\u7528\u591a\u4e2a\u91cd\u6392\u5e8f\u6a21\u578b\u4f18\u5316\u5019\u9009\u7ed3\u679c\uff1b\u6700\u540e\u901a\u8fc7\u52a0\u6743\u6295\u7968\u786e\u5b9a\u6700\u7ec8\u68c0\u7d22\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728SemEval-2025\u4efb\u52a17\u7684\u5355\u8bed\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c5\uff0c\u8de8\u8bed\u8a00\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c7\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\u5728\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "paper_title_zh": "QUST_NLP\u5728SemEval-2025\u4efb\u52a17\u4e2d\u7684\u5e94\u7528\uff1a\u4e00\u79cd\u7528\u4e8e\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u7684\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86QUST_NLP\u56e2\u961f\u5728SemEval-2025\u4efb\u52a17\u4e2d\u7684\u53c2\u4e0e\u60c5\u51b5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u8bbe\u8ba1\u7684\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\u3002\u9996\u5148\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u68c0\u7d22\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u9009\u62e9\u4e86\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u7528\u4e8e\u5019\u9009\u68c0\u7d22\u3002\u63a5\u7740\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u4e2a\u91cd\u6392\u5e8f\u6a21\u578b\u5bf9\u5019\u9009\u7ed3\u679c\u8fdb\u884c\u4f18\u5316\uff0c\u6bcf\u4e2a\u6a21\u578b\u9009\u62e9\u524d10\u4e2a\u7ed3\u679c\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u52a0\u6743\u6295\u7968\u786e\u5b9a\u6700\u7ec8\u7684\u68c0\u7d22\u7ed3\u679c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5355\u8bed\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c5\uff0c\u8de8\u8bed\u8a00\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c7\u3002\u7cfb\u7edf\u4ee3\u7801\u5df2\u53d1\u5e03\u4e8e\uff1ahttps://github.com/warmth27/SemEval2025_Task7"}}
{"id": "2506.18703", "pdf": "https://arxiv.org/pdf/2506.18703", "abs": "https://arxiv.org/abs/2506.18703", "authors": ["Christian Huber", "Alexander Waibel"], "title": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u95ee\u9898\u7684\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4fee\u6b63\u66ff\u6362\u9519\u8bef\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7279\u5b9a\u8bcd\u6c47\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u5e8f\u5217\u5230\u5e8f\u5217\u7cfb\u7edf\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u4e8e\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u8bcd\u6c47\uff08\u5982\u4e13\u6709\u540d\u8bcd\u3001\u7f29\u5199\u6216\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\uff09\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u7684\u8bcd\u6c47\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4fee\u6b63\u66ff\u6362\u9519\u8bef\u7684\u65b9\u6cd5\uff0c\u5141\u8bb8\u7528\u6237\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u6dfb\u52a0\u4fee\u6b63\uff0c\u4ece\u800c\u4f18\u5316\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u8bcd\u6c47\u7684\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u504f\u7f6e\u8bcd\u6c47\u9519\u8bef\u7387\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe11%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u8bcd\u6c47\u9519\u8bef\u7387\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u8bcd\u6c47\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u7684\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5", "abstract_zh": "\u795e\u7ecf\u5e8f\u5217\u5230\u5e8f\u5217\u7cfb\u7edf\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5f53\u4f7f\u7528\u9002\u5f53\u7684\u5efa\u6a21\u5355\u5143\uff08\u5982\u5b57\u8282\u5bf9\u7f16\u7801\u5b57\u7b26\uff09\u65f6\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5728\u7406\u8bba\u4e0a\u5177\u6709\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0c\u5b83\u4eec\u5f80\u5f80\u65e0\u6cd5\u8bc6\u522b\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u8bcd\u6c47\uff08\u5982\u4e13\u6709\u540d\u8bcd\u3001\u7f29\u5199\u6216\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\uff09\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u5df2\u63d0\u51fa\u591a\u79cd\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\uff0c\u4f46\u5bf9\u4e8e\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u7684\u8bcd\u6c47\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63\u66ff\u6362\u9519\u8bef\u6765\u63d0\u9ad8\u6b64\u7c7b\u6311\u6218\u6027\u8bcd\u6c47\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002\u7528\u6237\u53ef\u4ee5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6dfb\u52a0\u4fee\u6b63\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u504f\u7f6e\u8bcd\u6c47\u9519\u8bef\u7387\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe11%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u8bcd\u6c47\u9519\u8bef\u7387\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.18023", "pdf": "https://arxiv.org/pdf/2506.18023", "abs": "https://arxiv.org/abs/2506.18023", "authors": ["Kui Huang", "Xinrong Chen", "Wenyu Lv", "Jincheng Liao", "Guanzhong Wang", "Yi Liu"], "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "AI": {"tldr": "PP-DocBee2\u662fPP-DocBee\u7684\u5347\u7ea7\u7248\uff0c\u901a\u8fc7\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\u3001\u4f18\u5316\u89c6\u89c9\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u6539\u8fdb\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u6027\u80fd\uff0c\u5e76\u5728\u4e2d\u6587\u5546\u4e1a\u6587\u6863\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8611.4%\u7684\u6027\u80fd\u63d0\u5347\u548c73.0%\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "PP-DocBee2\u65e8\u5728\u89e3\u51b3\u5176\u524d\u8eab\u5728\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6280\u672f\u6539\u8fdb\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u6587\u5546\u4e1a\u6587\u6863\u5904\u7406\u65b9\u9762\u3002", "method": "PP-DocBee2\u91c7\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u901a\u8fc7\u7edf\u8ba1\u6807\u51c6\u8fc7\u6ee4\u5f02\u5e38\u6570\u636e\uff1b\u540c\u65f6\u5206\u89e3ViT\u5c42\u5e76\u5e94\u7528\u65b0\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u589e\u5f3a\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPP-DocBee2\u5728\u4e2d\u6587\u5546\u4e1a\u6587\u6863\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u534711.4%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e73.0%\u3002", "conclusion": "PP-DocBee2\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u4f18\u5316\u548c\u7279\u5f81\u878d\u5408\u7b56\u7565\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "paper_title_zh": "PP-DocBee2\uff1a\u57fa\u4e8e\u9ad8\u6548\u6570\u636e\u7684\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u57fa\u7ebf\u6539\u8fdb", "abstract_zh": "\u672c\u62a5\u544a\u4ecb\u7ecd\u4e86PP-DocBee2\uff0c\u8fd9\u662fPP-DocBee\u7684\u5347\u7ea7\u7248\u672c\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u80fd\u529b\u3002\u57fa\u4e8e\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u67b6\u6784\uff0cPP-DocBee2\u901a\u8fc7\u5173\u952e\u6280\u672f\u6539\u8fdb\u89e3\u51b3\u4e86\u5176\u524d\u8eab\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\u3001\u4f18\u5316\u89c6\u89c9\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u6539\u8fdb\u63a8\u7406\u65b9\u6cd5\u3002\u8fd9\u4e9b\u6539\u8fdb\u5728\u4e2d\u6587\u5546\u4e1a\u6587\u6863\u7684\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8611.4%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e8673.0%\u3002\u6211\u4eec\u5de5\u4f5c\u7684\u4e00\u4e2a\u5173\u952e\u521b\u65b0\u662f\u4e3a\u591a\u6a21\u6001\u6587\u6863\u4efb\u52a1\u8bbe\u8ba1\u7684\u6570\u636e\u8d28\u91cf\u4f18\u5316\u7b56\u7565\u3002\u901a\u8fc7\u4f7f\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u8bc4\u4f30\u6570\u636e\uff0c\u5e76\u5e94\u7528\u65b0\u7684\u7edf\u8ba1\u6807\u51c6\u8fc7\u6ee4\u5f02\u5e38\u503c\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002\u53d7\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u4e2d\u95f4\u7279\u5f81\u7684\u542f\u53d1\uff0c\u6211\u4eec\u901a\u8fc7\u5206\u89e3ViT\u5c42\u5e76\u5e94\u7528\u65b0\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u589e\u5f3a\u4e86\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728https://github.com/PaddlePaddle/PaddleMIX\u83b7\u53d6\u3002"}}
{"id": "2506.17275", "pdf": "https://arxiv.org/pdf/2506.17275", "abs": "https://arxiv.org/abs/2506.17275", "authors": ["William Scarbro", "Calum Imrie", "Sinem Getir Yaman", "Kavan Fatehi", "Corina S. Pasareanu", "Radu Calinescu", "Ravi Mangal"], "title": "Conformal Safety Shielding for Imperfect-Perception Agents", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "32 pages; Equal contribution by W. Scarbro and C. Imrie", "summary": "We consider the problem of safe control in discrete autonomous agents that\nuse learned components for imperfect perception (or more generally, state\nestimation) from high-dimensional observations. We propose a shield\nconstruction that provides run-time safety guarantees under perception errors\nby restricting the actions available to an agent, modeled as a Markov decision\nprocess, as a function of the state estimates. Our construction uses conformal\nprediction for the perception component, which guarantees that for each\nobservation, the predicted set of estimates includes the actual state with a\nuser-specified probability. The shield allows an action only if it is allowed\nfor all the estimates in the predicted set, resulting in a local safety\nguarantee. We also articulate and prove a global safety property of existing\nshield constructions for perfect-perception agents bounding the probability of\nreaching unsafe states if the agent always chooses actions prescribed by the\nshield. We illustrate our approach with a case-study of an experimental\nautonomous system that guides airplanes on taxiways using high-dimensional\nperception DNNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u5b89\u5168\u5c4f\u853d\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u611f\u77e5\u4e0d\u5b8c\u7f8e\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u8bc1\uff0c\u901a\u8fc7\u9650\u5236\u4ee3\u7406\u7684\u52a8\u4f5c\u9009\u62e9\u4ee5\u786e\u4fdd\u5c40\u90e8\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u4e3b\u4ee3\u7406\u5728\u611f\u77e5\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u9ad8\u7ef4\u89c2\u6d4b\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\uff09\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u8fd0\u884c\u65f6\u5b89\u5168\u5c4f\u853d\u673a\u5236\uff0c\u786e\u4fdd\u4ee3\u7406\u5728\u611f\u77e5\u9519\u8bef\u65f6\u4ecd\u80fd\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u5229\u7528\u5171\u5f62\u9884\u6d4b\u4e3a\u611f\u77e5\u7ec4\u4ef6\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u72b6\u6001\u4f30\u8ba1\u96c6\u4ee5\u7528\u6237\u6307\u5b9a\u6982\u7387\u5305\u542b\u5b9e\u9645\u72b6\u6001\u3002\u5c4f\u853d\u673a\u5236\u4ec5\u5141\u8bb8\u4ee3\u7406\u6267\u884c\u5bf9\u6240\u6709\u4f30\u8ba1\u72b6\u6001\u5747\u5b89\u5168\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u5b9e\u73b0\u5c40\u90e8\u5b89\u5168\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u5b8c\u7f8e\u611f\u77e5\u4ee3\u7406\u5c4f\u853d\u673a\u5236\u7684\u5168\u5c40\u5b89\u5168\u6027\u3002", "result": "\u63d0\u51fa\u7684\u5c4f\u853d\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9650\u5236\u4ee3\u7406\u7684\u52a8\u4f5c\u9009\u62e9\uff0c\u786e\u4fdd\u5728\u611f\u77e5\u9519\u8bef\u65f6\u4ecd\u80fd\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u57fa\u4e8e\u9ad8\u7ef4\u611f\u77e5DNN\u7684\u98de\u673a\u6ed1\u884c\u5f15\u5bfc\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5171\u5f62\u5b89\u5168\u5c4f\u853d\u65b9\u6cd5\u4e3a\u611f\u77e5\u4e0d\u5b8c\u7f8e\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "paper_title_zh": "\u9762\u5411\u611f\u77e5\u4e0d\u5b8c\u7f8e\u4ee3\u7406\u7684\u5171\u5f62\u5b89\u5168\u5c4f\u853d\u65b9\u6cd5", "abstract_zh": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5b66\u4e60\u7ec4\u4ef6\u4ece\u9ad8\u7ef4\u89c2\u6d4b\u4e2d\u8fdb\u884c\u4e0d\u5b8c\u7f8e\u611f\u77e5\uff08\u6216\u72b6\u6001\u4f30\u8ba1\uff09\u7684\u79bb\u6563\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u63a7\u5236\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c4f\u853d\u6784\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u4ee3\u7406\uff08\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff09\u7684\u52a8\u4f5c\u9009\u62e9\uff0c\u5728\u611f\u77e5\u9519\u8bef\u65f6\u63d0\u4f9b\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u8bc1\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5171\u5f62\u9884\u6d4b\u4e3a\u611f\u77e5\u7ec4\u4ef6\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\uff0c\u786e\u4fdd\u6bcf\u4e2a\u89c2\u6d4b\u7684\u9884\u6d4b\u4f30\u8ba1\u96c6\u4ee5\u7528\u6237\u6307\u5b9a\u6982\u7387\u5305\u542b\u5b9e\u9645\u72b6\u6001\u3002\u5c4f\u853d\u673a\u5236\u4ec5\u5141\u8bb8\u4ee3\u7406\u6267\u884c\u5bf9\u6240\u6709\u4f30\u8ba1\u72b6\u6001\u5747\u5b89\u5168\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u5b9e\u73b0\u5c40\u90e8\u5b89\u5168\u4fdd\u8bc1\u3002\u6211\u4eec\u8fd8\u9610\u8ff0\u5e76\u8bc1\u660e\u4e86\u5b8c\u7f8e\u611f\u77e5\u4ee3\u7406\u5c4f\u853d\u6784\u9020\u7684\u5168\u5c40\u5b89\u5168\u6027\uff0c\u5373\u5728\u4ee3\u7406\u59cb\u7ec8\u9009\u62e9\u5c4f\u853d\u89c4\u5b9a\u7684\u52a8\u4f5c\u65f6\uff0c\u5230\u8fbe\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u6982\u7387\u6709\u754c\u3002\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u7ef4\u611f\u77e5DNN\u7684\u5b9e\u9a8c\u6027\u98de\u673a\u6ed1\u884c\u5f15\u5bfc\u7cfb\u7edf\u6848\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18710", "pdf": "https://arxiv.org/pdf/2506.18710", "abs": "https://arxiv.org/abs/2506.18710", "authors": ["Maxime Leli\u00e8vre", "Amy Waldock", "Meng Liu", "Natalia Vald\u00e9s Aspillaga", "Alasdair Mackintosh", "Mar\u00eda Jos\u00e9 Ogando Portelo", "Jared Lee", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "title": "Benchmarking the Pedagogical Knowledge of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u300a\u6559\u5b66\u57fa\u51c6\u6d4b\u8bd5\u300b\uff0c\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u6559\u5b66\u77e5\u8bc6\uff08CDPK\uff09\u548c\u7279\u6b8a\u6559\u80b2\u9700\u6c42\u4e0e\u6b8b\u75be\uff08SEND\uff09\u6559\u5b66\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\u7684\u65b0\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u6559\u5e08\u4e13\u4e1a\u8003\u8bd5\u9898\u76ee\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u6a21\u578b\u51c6\u786e\u7387\u572828%\u81f389%\u4e4b\u95f4\uff0c\u5e76\u63a2\u8ba8\u4e86\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u6559\u5b66\u65b9\u6cd5\u7684\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5f00\u53d1\u4e13\u95e8\u7684\u6559\u5b66\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u6559\u5b66\u5b9e\u8df5\u4e2d\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u7684AI\u5e94\u7528\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u57fa\u4e8e\u6559\u5e08\u4e13\u4e1a\u53d1\u5c55\u8003\u8bd5\u7684\u9898\u76ee\uff0c\u6784\u5efa\u4e86\u6db5\u76d6\u6559\u5b66\u7b56\u7565\u548c\u8bc4\u4f30\u65b9\u6cd5\u7b49\u5b50\u9886\u57df\u7684\u300a\u6559\u5b66\u57fa\u51c6\u6d4b\u8bd5\u300b\u3002\u6d4b\u8bd5\u4e8697\u4e2a\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u5728\u6559\u5b66\u77e5\u8bc6\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u7387\u4ecb\u4e8e28%\u81f389%\u4e4b\u95f4\u3002\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u968f\u65f6\u95f4\u7684\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u7ebf\u6392\u884c\u699c\u4f9b\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3002", "conclusion": "\u6559\u5b66\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u8861\u91cf\u6a21\u578b\u7684\u6559\u5b66\u7406\u89e3\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u6307\u5bfcAI\u5728\u6559\u80b2\u9886\u57df\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u548c\u653f\u7b56\u5236\u5b9a\u3002", "paper_title_zh": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6559\u5b66\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u5982\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\uff08MMLU\uff09\u7b49\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30AI\u8de8\u9886\u57df\u77e5\u8bc6\u548c\u80fd\u529b\u65b9\u9762\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u77e5\u8bc6\uff0c\u5ffd\u89c6\u4e86\u8bc4\u4f30\u6a21\u578b\u5bf9\u6559\u5b66\u65b9\u6cd5\uff08\u5373\u6559\u5b66\u5b9e\u8df5\uff09\u7684\u7406\u89e3\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u300a\u6559\u5b66\u57fa\u51c6\u6d4b\u8bd5\u300b\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u6559\u5b66\u77e5\u8bc6\uff08CDPK\uff09\u548c\u7279\u6b8a\u6559\u80b2\u9700\u6c42\u4e0e\u6b8b\u75be\uff08SEND\uff09\u6559\u5b66\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e\u4ece\u6559\u5e08\u4e13\u4e1a\u53d1\u5c55\u8003\u8bd5\u4e2d\u7cbe\u5fc3\u6311\u9009\u7684\u95ee\u9898\uff0c\u6db5\u76d6\u6559\u5b66\u7b56\u7565\u548c\u8bc4\u4f30\u65b9\u6cd5\u7b49\u591a\u4e2a\u6559\u5b66\u5b50\u9886\u57df\u3002\u672c\u6587\u6982\u8ff0\u4e86\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u7684\u65b9\u6cd5\u548c\u5f00\u53d1\u8fc7\u7a0b\uff0c\u5e76\u62a5\u544a\u4e8697\u4e2a\u6a21\u578b\u7684\u7ed3\u679c\uff0c\u5176\u6559\u5b66\u77e5\u8bc6\u95ee\u9898\u7684\u51c6\u786e\u7387\u4ecb\u4e8e28%\u81f389%\u4e4b\u95f4\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u5173\u7cfb\uff0c\u5e76\u7ed8\u5236\u4e86\u5e15\u7d2f\u6258\u4ef7\u503c\u524d\u6cbf\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5728\u7ebf\u6392\u884c\u699c\uff08https://rebrand.ly/pedagogy\uff09\uff0c\u8be5\u6392\u884c\u699c\u4f1a\u968f\u7740\u65b0\u6a21\u578b\u7684\u52a0\u5165\u800c\u66f4\u65b0\uff0c\u5e76\u5141\u8bb8\u57fa\u4e8e\u5404\u79cd\u6a21\u578b\u5c5e\u6027\uff08\u5982\u6bcf\u6807\u8bb0\u6210\u672c\u548c\u5f00\u653e\u4e0e\u5c01\u95ed\u6743\u91cd\uff09\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u7b5b\u9009\uff0c\u540c\u65f6\u8fd8\u80fd\u67e5\u770b\u4e0d\u540c\u5b66\u79d1\u7684\u8868\u73b0\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u5168\u7403\u5b66\u4e60\u5371\u673a\u3002\u4ee5\u6559\u80b2\u4e3a\u91cd\u70b9\u7684\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u8861\u91cf\u6a21\u578b\u7406\u89e3\u6559\u5b66\u6982\u5ff5\u3001\u9002\u5f53\u56de\u5e94\u5b66\u4e60\u8005\u9700\u6c42\u4ee5\u53ca\u652f\u6301\u591a\u6837\u5316\u80cc\u666f\u4e0b\u7684\u6709\u6548\u6559\u5b66\u5b9e\u8df5\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u5728\u6559\u80b2\u73af\u5883\u4e2d\u8d1f\u8d23\u4efb\u4e14\u57fa\u4e8e\u8bc1\u636e\u5730\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ca\u5176\u5de5\u5177\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5e76\u4e3a\u5f00\u53d1\u4e0e\u653f\u7b56\u51b3\u7b56\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.18028", "pdf": "https://arxiv.org/pdf/2506.18028", "abs": "https://arxiv.org/abs/2506.18028", "authors": ["Junjian Li", "Hulin Kuang", "Jin Liu", "Hailin Yue", "Mengshen He", "Jianxin Wang"], "title": "MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Multiple instance learning (MIL) has shown significant promise in\nhistopathology whole slide image (WSI) analysis for cancer diagnosis and\nprognosis. However, the inherent spatial heterogeneity of WSIs presents\ncritical challenges, as morphologically similar tissue types are often\ndispersed across distant anatomical regions. Conventional MIL methods struggle\nto model these scattered tissue distributions and capture cross-regional\nspatial interactions effectively. To address these limitations, we propose a\nnovel Multiple instance learning framework with Context-Aware Clustering\n(MiCo), designed to enhance cross-regional intra-tissue correlations and\nstrengthen inter-tissue semantic associations in WSIs. MiCo begins by\nclustering instances to distill discriminative morphological patterns, with\ncluster centroids serving as semantic anchors. To enhance cross-regional\nintra-tissue correlations, MiCo employs a Cluster Route module, which\ndynamically links instances of the same tissue type across distant regions via\nfeature similarity. These semantic anchors act as contextual hubs, propagating\nsemantic relationships to refine instance-level representations. To eliminate\nsemantic fragmentation and strengthen inter-tissue semantic associations, MiCo\nintegrates a Cluster Reducer module, which consolidates redundant anchors while\nenhancing information exchange between distinct semantic groups. Extensive\nexperiments on two challenging tasks across nine large-scale public cancer\ndatasets demonstrate the effectiveness of MiCo, showcasing its superiority over\nstate-of-the-art methods. The code is available at\nhttps://github.com/junjianli106/MiCo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMiCo\u7684\u65b0\u578b\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u805a\u7c7b\u589e\u5f3a\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5206\u6790\u4e2d\u7684\u8de8\u533a\u57df\u7ec4\u7ec7\u5185\u76f8\u5173\u6027\u548c\u7ec4\u7ec7\u95f4\u8bed\u4e49\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86\u764c\u75c7\u8bca\u65ad\u548c\u9884\u540e\u7684\u6027\u80fd\u3002", "motivation": "\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5728\u764c\u75c7\u8bca\u65ad\u548c\u9884\u540e\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u7a7a\u95f4\u5f02\u8d28\u6027\u5bfc\u81f4\u5f62\u6001\u76f8\u4f3c\u7684\u7ec4\u7ec7\u53ef\u80fd\u5206\u6563\u5728\u8fdc\u8ddd\u79bb\u89e3\u5256\u533a\u57df\u3002\u4f20\u7edf\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u8fd9\u4e9b\u5206\u6563\u7684\u7ec4\u7ec7\u5206\u5e03\u548c\u8de8\u533a\u57df\u7a7a\u95f4\u4ea4\u4e92\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u589e\u5f3a\u7ec4\u7ec7\u5185\u548c\u8de8\u7ec4\u7ec7\u7684\u8bed\u4e49\u5173\u8054\u3002", "method": "MiCo\u6846\u67b6\u901a\u8fc7\u805a\u7c7b\u63d0\u53d6\u5177\u6709\u5224\u522b\u6027\u7684\u5f62\u6001\u6a21\u5f0f\uff0c\u5e76\u4ee5\u805a\u7c7b\u4e2d\u5fc3\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u3002\u901a\u8fc7Cluster Route\u6a21\u5757\u52a8\u6001\u8fde\u63a5\u8fdc\u8ddd\u79bb\u533a\u57df\u7684\u76f8\u540c\u7ec4\u7ec7\u7c7b\u578b\u5b9e\u4f8b\uff0c\u589e\u5f3a\u8de8\u533a\u57df\u7ec4\u7ec7\u5185\u76f8\u5173\u6027\u3002Cluster Reducer\u6a21\u5757\u5219\u6574\u5408\u5197\u4f59\u951a\u70b9\uff0c\u52a0\u5f3a\u4e0d\u540c\u8bed\u4e49\u7ec4\u4e4b\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u6d88\u9664\u8bed\u4e49\u788e\u7247\u5316\u3002", "result": "\u5728\u4e5d\u4e2a\u5927\u89c4\u6a21\u516c\u5171\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMiCo\u5728\u4e24\u9879\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MiCo\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u805a\u7c7b\u6709\u6548\u89e3\u51b3\u4e86WSI\u5206\u6790\u4e2d\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u7ec4\u7ec7\u5185\u548c\u8de8\u7ec4\u7ec7\u7684\u8bed\u4e49\u5173\u8054\uff0c\u4e3a\u764c\u75c7\u8bca\u65ad\u548c\u9884\u540e\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002", "paper_title_zh": "MiCo\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u805a\u7c7b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u7528\u4e8e\u5168\u5207\u7247\u56fe\u50cf\u5206\u6790", "abstract_zh": "\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5206\u6790\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u764c\u75c7\u8bca\u65ad\u548c\u9884\u540e\u3002\u7136\u800c\uff0cWSI\u56fa\u6709\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u5e26\u6765\u4e86\u5173\u952e\u6311\u6218\uff0c\u56e0\u4e3a\u5f62\u6001\u76f8\u4f3c\u7684\u7ec4\u7ec7\u7c7b\u578b\u901a\u5e38\u5206\u6563\u5728\u8fdc\u8ddd\u79bb\u7684\u89e3\u5256\u533a\u57df\u3002\u4f20\u7edf\u7684MIL\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u8fd9\u4e9b\u5206\u6563\u7684\u7ec4\u7ec7\u5206\u5e03\u548c\u8de8\u533a\u57df\u7a7a\u95f4\u4ea4\u4e92\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6MiCo\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u805a\u7c7b\u589e\u5f3aWSI\u4e2d\u7684\u8de8\u533a\u57df\u7ec4\u7ec7\u5185\u76f8\u5173\u6027\u548c\u7ec4\u7ec7\u95f4\u8bed\u4e49\u5173\u8054\u3002MiCo\u9996\u5148\u901a\u8fc7\u805a\u7c7b\u63d0\u53d6\u5177\u6709\u5224\u522b\u6027\u7684\u5f62\u6001\u6a21\u5f0f\uff0c\u5e76\u4ee5\u805a\u7c7b\u4e2d\u5fc3\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u3002\u4e3a\u4e86\u589e\u5f3a\u8de8\u533a\u57df\u7ec4\u7ec7\u5185\u76f8\u5173\u6027\uff0cMiCo\u91c7\u7528Cluster Route\u6a21\u5757\uff0c\u901a\u8fc7\u7279\u5f81\u76f8\u4f3c\u6027\u52a8\u6001\u8fde\u63a5\u8fdc\u8ddd\u79bb\u533a\u57df\u7684\u76f8\u540c\u7ec4\u7ec7\u7c7b\u578b\u5b9e\u4f8b\u3002\u8fd9\u4e9b\u8bed\u4e49\u951a\u70b9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u67a2\u7ebd\uff0c\u4f20\u64ad\u8bed\u4e49\u5173\u7cfb\u4ee5\u4f18\u5316\u5b9e\u4f8b\u7ea7\u8868\u793a\u3002\u4e3a\u4e86\u6d88\u9664\u8bed\u4e49\u788e\u7247\u5316\u5e76\u52a0\u5f3a\u7ec4\u7ec7\u95f4\u8bed\u4e49\u5173\u8054\uff0cMiCo\u96c6\u6210\u4e86Cluster Reducer\u6a21\u5757\uff0c\u6574\u5408\u5197\u4f59\u951a\u70b9\u540c\u65f6\u589e\u5f3a\u4e0d\u540c\u8bed\u4e49\u7ec4\u4e4b\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\u3002\u5728\u4e5d\u4e2a\u5927\u89c4\u6a21\u516c\u5171\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u4e24\u9879\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cMiCo\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728https://github.com/junjianli106/MiCo\u83b7\u53d6\u3002"}}
{"id": "2506.17276", "pdf": "https://arxiv.org/pdf/2506.17276", "abs": "https://arxiv.org/abs/2506.17276", "authors": ["Alexandre Le Nepvou"], "title": "Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds", "categories": ["cs.LO", "cs.AI"], "comment": "This paper develops the formal logical foundations of the stratified\n  actualization framework presented in a companion paper currently under review\n  at Erkenntnis (manuscript ID: ERKE-D-25-00410)", "summary": "This article develops a novel framework for modal logic based on the idea of\nstratified actualization, rather than the classical model of global possible\nworlds. Traditional Kripke semantics treat modal operators as quantification\nover fully determinate alternatives, neglecting the local, dynamic, and often\nasymmetric nature of actualization processes. We propose a system Stratified\nActualization Logic (SAL) in which modalities are indexed by levels of\nontological stability, interpreted as admissibility regimes. Each modality\noperates over a structured layer of possibility, grounded in the internal\ncoherence of transitions between layers. We formally define the syntax and\nsemantics of SAL, introduce its axioms, and prove soundness and completeness.\nApplications are discussed in connection with temporal becoming, quantum\ndecoherence domains, and modal metaphysics. The result is a logic that captures\nthe ontological structure of actualization without recourse to abstract\npossible worlds, offering a stratified alternative to standard modal realism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u5b9e\u73b0\u800c\u975e\u4f20\u7edf\u53ef\u80fd\u4e16\u754c\u6a21\u578b\u7684\u6a21\u6001\u903b\u8f91\u65b0\u6846\u67b6\u2014\u2014\u5206\u5c42\u5b9e\u73b0\u903b\u8f91\uff08SAL\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u7d22\u5f15\u6a21\u6001\u64cd\u4f5c\uff0c\u6355\u6349\u5b9e\u9645\u5316\u8fc7\u7a0b\u7684\u5c40\u90e8\u6027\u548c\u52a8\u6001\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u514b\u91cc\u666e\u514b\u8bed\u4e49\u5b66\u5c06\u6a21\u6001\u64cd\u4f5c\u89c6\u4e3a\u5bf9\u5b8c\u5168\u786e\u5b9a\u6027\u66ff\u4ee3\u65b9\u6848\u7684\u91cf\u5316\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5316\u8fc7\u7a0b\u7684\u5c40\u90e8\u6027\u3001\u52a8\u6001\u6027\u548c\u4e0d\u5bf9\u79f0\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u8d34\u8fd1\u5b9e\u9645\u5316\u8fc7\u7a0b\u672c\u8d28\u7684\u6a21\u6001\u903b\u8f91\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5b9e\u73b0\u903b\u8f91\uff08SAL\uff09\uff0c\u5c06\u6a21\u6001\u64cd\u4f5c\u6309\u672c\u4f53\u7a33\u5b9a\u6027\u5c42\u7ea7\u7d22\u5f15\uff0c\u89e3\u91ca\u4e3a\u53ef\u5bb9\u8bb8\u6027\u673a\u5236\u3002\u901a\u8fc7\u5b9a\u4e49SAL\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\uff0c\u5f15\u5165\u516c\u7406\uff0c\u5e76\u8bc1\u660e\u5176\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "SAL\u6210\u529f\u6355\u6349\u4e86\u5b9e\u9645\u5316\u7684\u672c\u4f53\u7ed3\u6784\uff0c\u65e0\u9700\u4f9d\u8d56\u62bd\u8c61\u7684\u53ef\u80fd\u4e16\u754c\u6a21\u578b\uff0c\u4e3a\u65f6\u95f4\u751f\u6210\u3001\u91cf\u5b50\u9000\u76f8\u5e72\u9886\u57df\u548c\u6a21\u6001\u5f62\u800c\u4e0a\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u903b\u8f91\u5de5\u5177\u3002", "conclusion": "\u5206\u5c42\u5b9e\u73b0\u903b\u8f91\uff08SAL\uff09\u4e3a\u6a21\u6001\u903b\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u5c42\u66ff\u4ee3\u65b9\u6848\uff0c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4e86\u5b9e\u9645\u5316\u8fc7\u7a0b\u7684\u52a8\u6001\u6027\u548c\u5c40\u90e8\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u5206\u5c42\u751f\u6210\u7684\u6a21\u6001\u903b\u8f91\uff1a\u8d85\u8d8a\u53ef\u80fd\u4e16\u754c\u7684\u5b9e\u9645\u5316", "abstract_zh": "\u672c\u6587\u57fa\u4e8e\u5206\u5c42\u5b9e\u73b0\u800c\u975e\u4f20\u7edf\u7684\u5168\u5c40\u53ef\u80fd\u4e16\u754c\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u6001\u903b\u8f91\u6846\u67b6\u3002\u4f20\u7edf\u514b\u91cc\u666e\u514b\u8bed\u4e49\u5b66\u5c06\u6a21\u6001\u64cd\u4f5c\u89c6\u4e3a\u5bf9\u5b8c\u5168\u786e\u5b9a\u6027\u66ff\u4ee3\u65b9\u6848\u7684\u91cf\u5316\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5316\u8fc7\u7a0b\u7684\u5c40\u90e8\u6027\u3001\u52a8\u6001\u6027\u548c\u4e0d\u5bf9\u79f0\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5b9e\u73b0\u903b\u8f91\uff08SAL\uff09\uff0c\u5176\u4e2d\u6a21\u6001\u64cd\u4f5c\u6309\u672c\u4f53\u7a33\u5b9a\u6027\u5c42\u7ea7\u7d22\u5f15\uff0c\u89e3\u91ca\u4e3a\u53ef\u5bb9\u8bb8\u6027\u673a\u5236\u3002\u6bcf\u4e2a\u6a21\u6001\u64cd\u4f5c\u4f5c\u7528\u4e8e\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u53ef\u80fd\u6027\u5c42\u7ea7\uff0c\u57fa\u4e8e\u5c42\u7ea7\u95f4\u8f6c\u6362\u7684\u5185\u5728\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86SAL\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\uff0c\u5f15\u5165\u4e86\u5176\u516c\u7406\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\u3002\u8ba8\u8bba\u4e86\u5176\u5728\u65f6\u95f4\u751f\u6210\u3001\u91cf\u5b50\u9000\u76f8\u5e72\u9886\u57df\u548c\u6a21\u6001\u5f62\u800c\u4e0a\u5b66\u4e2d\u7684\u5e94\u7528\u3002\u7ed3\u679c\u8868\u660e\uff0cSAL\u65e0\u9700\u4f9d\u8d56\u62bd\u8c61\u7684\u53ef\u80fd\u4e16\u754c\u6a21\u578b\u5373\u53ef\u6355\u6349\u5b9e\u9645\u5316\u7684\u672c\u4f53\u7ed3\u6784\uff0c\u4e3a\u6807\u51c6\u7684\u6a21\u6001\u5b9e\u5728\u8bba\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u5c42\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.18756", "pdf": "https://arxiv.org/pdf/2506.18756", "abs": "https://arxiv.org/abs/2506.18756", "authors": ["Chong Zhang", "Xiang Li", "Jia Wang", "Shan Liang", "Haochen Xue", "Xiaobo Jin"], "title": "Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach", "categories": ["cs.CL", "cs.CR"], "comment": "19 pages, 8 figures", "summary": "Large Language Models (LLMs) increasingly rely on automatic prompt\nengineering in graphical user interfaces (GUIs) to refine user inputs and\nenhance response accuracy. However, the diversity of user requirements often\nleads to unintended misinterpretations, where automated optimizations distort\noriginal intentions and produce erroneous outputs. To address this challenge,\nwe propose the Adaptive Greedy Binary Search (AGBS) method, which simulates\ncommon prompt optimization mechanisms while preserving semantic stability. Our\napproach dynamically evaluates the impact of such strategies on LLM\nperformance, enabling robust adversarial sample generation. Through extensive\nexperiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness\nin balancing semantic consistency and attack efficacy. Our findings offer\nactionable insights for designing more reliable prompt optimization systems.\nCode is available at: https://github.com/franz-chang/DOBS", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8d2a\u5a6a\u4e8c\u5206\u641c\u7d22\uff08AGBS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4fdd\u6301\u8bed\u4e49\u7a33\u5b9a\u7684\u540c\u65f6\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u8bed\u4e49\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4e2d\u4f9d\u8d56\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u4ee5\u63d0\u5347\u54cd\u5e94\u51c6\u786e\u6027\uff0c\u4f46\u7528\u6237\u9700\u6c42\u7684\u591a\u6837\u6027\u5e38\u5bfc\u81f4\u539f\u59cb\u610f\u56fe\u88ab\u8bef\u89e3\uff0c\u4ea7\u751f\u9519\u8bef\u8f93\u51fa\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8d2a\u5a6a\u4e8c\u5206\u641c\u7d22\uff08AGBS\uff09\u65b9\u6cd5\uff0c\u6a21\u62df\u5e38\u89c1\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u52a8\u6001\u8bc4\u4f30\u5176\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u751f\u6210\u9c81\u68d2\u7684\u5bf9\u6297\u6837\u672c\u3002", "result": "\u901a\u8fc7\u5728\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eAGBS\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u653b\u51fb\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u63d0\u793a\u4f18\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u4fdd\u6301\u8bed\u4e49\u7684LLMs\u5bf9\u6297\u653b\u51fb\uff1a\u4e00\u79cd\u81ea\u9002\u5e94\u8d2a\u5a6a\u4e8c\u5206\u641c\u7d22\u65b9\u6cd5", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u4ee5\u4f18\u5316\u7528\u6237\u8f93\u5165\u5e76\u63d0\u5347\u54cd\u5e94\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u7528\u6237\u9700\u6c42\u7684\u591a\u6837\u6027\u5e38\u5bfc\u81f4\u610f\u5916\u7684\u8bef\u89e3\uff0c\u81ea\u52a8\u4f18\u5316\u53ef\u80fd\u626d\u66f2\u539f\u59cb\u610f\u56fe\u5e76\u4ea7\u751f\u9519\u8bef\u8f93\u51fa\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u8d2a\u5a6a\u4e8c\u5206\u641c\u7d22\uff08AGBS\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u5e38\u89c1\u63d0\u793a\u4f18\u5316\u673a\u5236\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u7a33\u5b9a\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u52a8\u6001\u8bc4\u4f30\u6b64\u7c7b\u7b56\u7565\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u751f\u6210\u9c81\u68d2\u7684\u5bf9\u6297\u6837\u672c\u3002\u901a\u8fc7\u5728\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86AGBS\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u653b\u51fb\u6548\u679c\u4e4b\u95f4\u7684\u5e73\u8861\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u63d0\u793a\u4f18\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/franz-chang/DOBS"}}
{"id": "2506.18034", "pdf": "https://arxiv.org/pdf/2506.18034", "abs": "https://arxiv.org/abs/2506.18034", "authors": ["Fenghe Tang", "Wenxin Ma", "Zhiyang He", "Xiaodong Tao", "Zihang Jiang", "S. Kevin Zhou"], "title": "Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by MICCAI 2025. Code: https://github.com/FengheTan9/LLM4Seg", "summary": "With the advancement of Large Language Model (LLM) for natural language\nprocessing, this paper presents an intriguing finding: a frozen pre-trained LLM\nlayer can process visual tokens for medical image segmentation tasks.\nSpecifically, we propose a simple hybrid structure that integrates a\npre-trained, frozen LLM layer within the CNN encoder-decoder segmentation\nframework (LLM4Seg). Surprisingly, this design improves segmentation\nperformance with a minimal increase in trainable parameters across various\nmodalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our\nin-depth analysis reveals the potential of transferring LLM's semantic\nawareness to enhance segmentation tasks, offering both improved global\nunderstanding and better local modeling capabilities. The improvement proves\nrobust across different LLMs, validated using LLaMA and DeepSeek.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c42\u53ef\u4ee5\u5904\u7406\u89c6\u89c9\u6807\u8bb0\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6df7\u5408\u7ed3\u6784LLM4Seg\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u4e14\u53c2\u6570\u589e\u52a0\u6781\u5c11\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3LLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5229\u7528\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faLLM4Seg\uff0c\u5c06\u9884\u8bad\u7ec3\u4e14\u51bb\u7ed3\u7684LLM\u5c42\u5d4c\u5165CNN\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u5272\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7LLM\u5904\u7406\u89c6\u89c9\u6807\u8bb0\u4ee5\u589e\u5f3a\u8bed\u4e49\u611f\u77e5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM4Seg\u5728\u8d85\u58f0\u3001\u76ae\u80a4\u955c\u3001\u606f\u8089\u955c\u548cCT\u7b49\u591a\u79cd\u6a21\u6001\u4e2d\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u4e14\u5bf9\u4e0d\u540cLLM\uff08\u5982LLaMA\u548cDeepSeek\uff09\u5747\u6709\u6548\u3002", "conclusion": "\u9884\u8bad\u7ec3LLM\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u53ef\u6709\u6548\u8fc1\u79fb\u81f3\u89c6\u89c9\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\u3002", "paper_title_zh": "\u9884\u8bad\u7ec3LLM\u662f\u8bed\u4e49\u611f\u77e5\u4e14\u901a\u7528\u6027\u5f3a\u7684\u5206\u5272\u589e\u5f3a\u5668", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u53d1\u5c55\uff0c\u672c\u6587\u53d1\u73b0\u4e00\u4e2a\u6709\u8da3\u7684\u73b0\u8c61\uff1a\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3LLM\u5c42\u53ef\u4ee5\u5904\u7406\u89c6\u89c9\u6807\u8bb0\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u6df7\u5408\u7ed3\u6784LLM4Seg\uff0c\u5c06\u9884\u8bad\u7ec3\u4e14\u51bb\u7ed3\u7684LLM\u5c42\u5d4c\u5165CNN\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u5272\u6846\u67b6\u4e2d\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u5728\u8d85\u58f0\u3001\u76ae\u80a4\u955c\u3001\u606f\u8089\u955c\u548cCT\u7b49\u591a\u79cd\u6a21\u6001\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u6781\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u6df1\u5165\u5206\u6790\u8868\u660e\uff0cLLM\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u53ef\u4ee5\u8fc1\u79fb\u81f3\u5206\u5272\u4efb\u52a1\uff0c\u65e2\u589e\u5f3a\u4e86\u5168\u5c40\u7406\u89e3\uff0c\u53c8\u4f18\u5316\u4e86\u5c40\u90e8\u5efa\u6a21\u80fd\u529b\u3002\u8fd9\u4e00\u6539\u8fdb\u5728\u4e0d\u540cLLM\uff08\u5982LLaMA\u548cDeepSeek\uff09\u4e2d\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.17277", "pdf": "https://arxiv.org/pdf/2506.17277", "abs": "https://arxiv.org/abs/2506.17277", "authors": ["Mahmoud Amiri", "Thomas Bocklitz"], "title": "Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "physics.chem-ph"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly vital for\nnavigating the ever-expanding body of scientific literature, particularly in\nhigh-stakes domains such as chemistry. Despite the promise of RAG, foundational\ndesign choices -- such as how documents are segmented and represented -- remain\nunderexplored in domain-specific contexts. This study presents the first\nlarge-scale, systematic evaluation of chunking strategies and embedding models\ntailored to chemistry-focused RAG systems. We investigate 25 chunking\nconfigurations across five method families and evaluate 48 embedding models on\nthree chemistry-specific benchmarks, including the newly introduced\nQuestChemRetrieval dataset. Our results reveal that recursive token-based\nchunking (specifically R100-0) consistently outperforms other approaches,\noffering strong performance with minimal resource overhead. We also find that\nretrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --\nsubstantially outperform domain-specialized models like SciBERT. By releasing\nour datasets, evaluation framework, and empirical benchmarks, we provide\nactionable guidelines for building effective and efficient chemistry-aware RAG\nsystems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5316\u5b66\u9886\u57df\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\u7684\u6587\u6863\u5206\u5272\u548c\u8868\u793a\u7b56\u7565\uff0c\u53d1\u73b0\u9012\u5f52\u57fa\u4e8e\u6807\u8bb0\u7684\u5206\u5757\uff08R100-0\uff09\u548c\u68c0\u7d22\u4f18\u5316\u5d4c\u5165\u6a21\u578b\uff08\u5982Nomic\u548cIntfloat E5\uff09\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u5316\u5b66RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u6587\u732e\u7684\u8fc5\u901f\u589e\u957f\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u5316\u5b66\u7b49\u9ad8\u4ef7\u503c\u9886\u57df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u7136\u800c\uff0c\u9886\u57df\u7279\u5b9a\u7684\u6587\u6863\u5206\u5272\u548c\u8868\u793a\u7b56\u7565\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8625\u79cd\u5206\u5757\u7b56\u7565\uff08\u6db5\u76d6\u4e94\u79cd\u65b9\u6cd5\u5bb6\u65cf\uff09\u548c48\u79cd\u5d4c\u5165\u6a21\u578b\u5728\u4e09\u4e2a\u5316\u5b66\u7279\u5b9a\u57fa\u51c6\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684QuestChemRetrieval\u6570\u636e\u96c6\uff09\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9012\u5f52\u57fa\u4e8e\u6807\u8bb0\u7684\u5206\u5757\uff08R100-0\uff09\u6027\u80fd\u6700\u4f18\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\uff0c\u800c\u68c0\u7d22\u4f18\u5316\u5d4c\u5165\u6a21\u578b\uff08\u5982Nomic\u548cIntfloat E5\uff09\u663e\u8457\u4f18\u4e8e\u9886\u57df\u4e13\u7528\u6a21\u578b\uff08\u5982SciBERT\uff09\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u5b9e\u8bc1\u57fa\u51c6\uff0c\u672c\u6587\u4e3a\u6784\u5efa\u9ad8\u6548\u5316\u5b66RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u63ed\u793a\u4e86\u5206\u5757\u548c\u5d4c\u5165\u6a21\u578b\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "paper_title_zh": "\u5206\u5757\u4e24\u6b21\uff0c\u5d4c\u5165\u4e00\u6b21\uff1a\u5316\u5b66\u611f\u77e5\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u5206\u5272\u4e0e\u8868\u793a\u6743\u8861\u7684\u7cfb\u7edf\u7814\u7a76", "abstract_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u79d1\u5b66\u6587\u732e\u5bfc\u822a\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5316\u5b66\u7b49\u9ad8\u4ef7\u503c\u9886\u57df\u3002\u5c3d\u7ba1RAG\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u57fa\u7840\u8bbe\u8ba1\u9009\u62e9\uff08\u5982\u6587\u6863\u5206\u5272\u548c\u8868\u793a\u65b9\u5f0f\uff09\u5728\u9886\u57df\u7279\u5b9a\u80cc\u666f\u4e0b\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u9996\u6b21\u5bf9\u5316\u5b66RAG\u7cfb\u7edf\u7684\u5206\u5757\u7b56\u7565\u548c\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7cfb\u7edf\u8bc4\u4f30\u3002\u6211\u4eec\u7814\u7a76\u4e86\u4e94\u79cd\u65b9\u6cd5\u5bb6\u65cf\u4e0b\u768425\u79cd\u5206\u5757\u914d\u7f6e\uff0c\u5e76\u8bc4\u4f30\u4e8648\u79cd\u5d4c\u5165\u6a21\u578b\u5728\u4e09\u4e2a\u5316\u5b66\u7279\u5b9a\u57fa\u51c6\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684QuestChemRetrieval\u6570\u636e\u96c6\uff09\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9012\u5f52\u57fa\u4e8e\u6807\u8bb0\u7684\u5206\u5757\uff08R100-0\uff09\u6027\u80fd\u6700\u4f18\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\uff0c\u800c\u68c0\u7d22\u4f18\u5316\u5d4c\u5165\u6a21\u578b\uff08\u5982Nomic\u548cIntfloat E5\uff09\u663e\u8457\u4f18\u4e8e\u9886\u57df\u4e13\u7528\u6a21\u578b\uff08\u5982SciBERT\uff09\u3002\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u5b9e\u8bc1\u57fa\u51c6\uff0c\u6211\u4eec\u4e3a\u6784\u5efa\u9ad8\u6548\u5316\u5b66RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2506.18768", "pdf": "https://arxiv.org/pdf/2506.18768", "abs": "https://arxiv.org/abs/2506.18768", "authors": ["Ao Chang", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework", "categories": ["cs.CL"], "comment": null, "summary": "Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including\nrelevant legal charge, terms, and fines, which is a crucial process in Large\nLanguage Model(LLM). However, LJP faces two key challenges: (1)Long Tail\nDistribution: Current datasets, derived from authentic cases, suffer from high\nhuman annotation costs and imbalanced distributions, leading to model\nperformance degradation. (2)Lawyer's Improvement: Existing systems focus on\nenhancing judges' decision-making but neglect the critical role of lawyers in\nrefining arguments, which limits overall judicial accuracy. To address these\nissues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment\nFramework, called ASP2LJ, which integrates a case generation module to tackle\nlong-tailed data distributions and an adversarial self-play mechanism to\nenhance lawyers' argumentation skills. Our framework enables a judge to\nreference evolved lawyers' arguments, improving the objectivity, fairness, and\nrationality of judicial decisions. Besides, We also introduce RareCases, a\ndataset for rare legal cases in China, which contains 120 tail-end cases. We\ndemonstrate the effectiveness of our approach on the SimuCourt dataset and our\nRareCases dataset. Experimental results show our framework brings improvements,\nindicating its utilization. Our contributions include an integrated framework,\na rare-case dataset, and publicly releasing datasets and code to support\nfurther research in automated judicial systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faASP2LJ\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\u548c\u6848\u4f8b\u751f\u6210\u6a21\u5757\u89e3\u51b3\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5f8b\u5e08\u8bba\u8bc1\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u53f8\u6cd5\u51b3\u7b56\u7684\u5ba2\u89c2\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff08LJP\uff09\u9762\u4e34\u957f\u5c3e\u6570\u636e\u5206\u5e03\u548c\u5f8b\u5e08\u8bba\u8bc1\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u73b0\u6709\u7cfb\u7edf\u5ffd\u89c6\u5f8b\u5e08\u5728\u4f18\u5316\u8bba\u8bc1\u4e2d\u7684\u4f5c\u7528\uff0c\u5bfc\u81f4\u53f8\u6cd5\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faASP2LJ\u6846\u67b6\uff0c\u5305\u542b\u6848\u4f8b\u751f\u6210\u6a21\u5757\u4ee5\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\u673a\u5236\u589e\u5f3a\u5f8b\u5e08\u8bba\u8bc1\u80fd\u529b\uff0c\u6cd5\u5b98\u53ef\u53c2\u8003\u4f18\u5316\u7684\u8bba\u8bc1\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5728SimuCourt\u548cRareCases\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cASP2LJ\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ASP2LJ\u6846\u67b6\u901a\u8fc7\u6574\u5408\u6848\u4f8b\u751f\u6210\u548c\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\uff0c\u63d0\u5347\u4e86\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u8d21\u732e\u4e86\u7a00\u6709\u6848\u4f8b\u6570\u636e\u96c6\u548c\u516c\u5f00\u4ee3\u7801\u3002", "paper_title_zh": "ASP2LJ\uff1a\u4e00\u79cd\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\u5f8b\u5e08\u589e\u5f3a\u7684\u6cd5\u5f8b\u5224\u51b3\u6846\u67b6", "abstract_zh": "\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff08LJP\uff09\u65e8\u5728\u9884\u6d4b\u53f8\u6cd5\u7ed3\u679c\uff0c\u5305\u62ec\u76f8\u5173\u6cd5\u5f8b\u6307\u63a7\u3001\u5211\u671f\u548c\u7f5a\u6b3e\uff0c\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5173\u952e\u8fc7\u7a0b\u3002\u7136\u800c\uff0cLJP\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\uff081\uff09\u957f\u5c3e\u5206\u5e03\uff1a\u73b0\u6709\u6570\u636e\u96c6\u6e90\u81ea\u771f\u5b9e\u6848\u4f8b\uff0c\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u5206\u5e03\u4e0d\u5747\u8861\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\uff082\uff09\u5f8b\u5e08\u7684\u6539\u8fdb\uff1a\u73b0\u6709\u7cfb\u7edf\u4e13\u6ce8\u4e8e\u63d0\u5347\u6cd5\u5b98\u51b3\u7b56\uff0c\u5374\u5ffd\u89c6\u4e86\u5f8b\u5e08\u5728\u4f18\u5316\u8bba\u8bc1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u9650\u5236\u4e86\u53f8\u6cd5\u51c6\u786e\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\u5f8b\u5e08\u589e\u5f3a\u7684\u6cd5\u5f8b\u5224\u51b3\u6846\u67b6\uff08ASP2LJ\uff09\uff0c\u901a\u8fc7\u6848\u4f8b\u751f\u6210\u6a21\u5757\u89e3\u51b3\u957f\u5c3e\u6570\u636e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\u673a\u5236\u63d0\u5347\u5f8b\u5e08\u8bba\u8bc1\u80fd\u529b\u3002\u8be5\u6846\u67b6\u4f7f\u6cd5\u5b98\u80fd\u591f\u53c2\u8003\u4f18\u5316\u7684\u5f8b\u5e08\u8bba\u8bc1\uff0c\u63d0\u9ad8\u53f8\u6cd5\u51b3\u7b56\u7684\u5ba2\u89c2\u6027\u3001\u516c\u5e73\u6027\u548c\u5408\u7406\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86RareCases\u6570\u636e\u96c6\uff0c\u5305\u542b120\u4e2a\u4e2d\u56fd\u7a00\u6709\u6cd5\u5f8b\u6848\u4f8b\u3002\u6211\u4eec\u5728SimuCourt\u548cRareCases\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\u4e00\u4e2a\u6574\u5408\u6846\u67b6\u3001\u7a00\u6709\u6848\u4f8b\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u81ea\u52a8\u5316\u53f8\u6cd5\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.18042", "pdf": "https://arxiv.org/pdf/2506.18042", "abs": "https://arxiv.org/abs/2506.18042", "authors": ["Dongdong Meng", "Sheng Li", "Hao Wu", "Suqing Tian", "Wenjun Ma", "Guoping Wang", "Xueqing Yan"], "title": "CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Accurate automatic medical image segmentation relies on high-quality, dense\nannotations, which are costly and time-consuming. Weakly supervised learning\nprovides a more efficient alternative by leveraging sparse and coarse\nannotations instead of dense, precise ones. However, segmentation performance\ndegradation and overfitting caused by sparse annotations remain key challenges.\nTo address these issues, we propose CmFNet, a novel 3D weakly supervised\ncross-modal medical image segmentation approach. CmFNet consists of three main\ncomponents: a modality-specific feature learning network, a cross-modal feature\nlearning network, and a hybrid-supervised learning strategy. Specifically, the\nmodality-specific feature learning network and the cross-modal feature learning\nnetwork effectively integrate complementary information from multi-modal\nimages, enhancing shared features across modalities to improve segmentation\nperformance. Additionally, the hybrid-supervised learning strategy guides\nsegmentation through scribble supervision, intra-modal regularization, and\ninter-modal consistency, modeling spatial and contextual relationships while\npromoting feature alignment. Our approach effectively mitigates overfitting,\ndelivering robust segmentation results. It excels in segmenting both\nchallenging small tumor regions and common anatomical structures. Extensive\nexperiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset\n(including CT and MR imaging) and the publicly available CT Whole Abdominal\nOrgan dataset (WORD) show that our approach outperforms state-of-the-art weakly\nsupervised methods. In addition, our approach also outperforms fully supervised\nmethods when full annotation is used. Our approach can facilitate clinical\ntherapy and benefit various specialists, including physicists, radiologists,\npathologists, and oncologists.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCmFNet\u7684\u65b0\u578b3D\u5f31\u76d1\u7763\u8de8\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u6df7\u5408\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u548c\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u901a\u5e38\u9700\u8981\u9ad8\u8d28\u91cf\u5bc6\u96c6\u6807\u6ce8\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u5f31\u76d1\u7763\u5b66\u4e60\u5229\u7528\u7a00\u758f\u6807\u6ce8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7a00\u758f\u6807\u6ce8\u6613\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u62df\u5408\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CmFNet\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u3001\u8de8\u6a21\u6001\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u548c\u6df7\u5408\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u3002\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u591a\u79cd\u76d1\u7763\u65b9\u5f0f\uff08\u5982\u6d82\u9e26\u76d1\u7763\u3001\u6a21\u6001\u5185\u6b63\u5219\u5316\u548c\u6a21\u6001\u95f4\u4e00\u81f4\u6027\uff09\uff0c\u63d0\u5347\u5206\u5272\u6027\u80fd\u5e76\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e34\u5e8a\u8de8\u6a21\u6001\u9f3b\u54bd\u764c\u6570\u636e\u96c6\uff08CT\u548cMR\uff09\u548c\u516c\u5f00\u7684CT\u5168\u8179\u90e8\u5668\u5b98\u6570\u636e\u96c6\uff08WORD\uff09\u4e0a\uff0cCmFNet\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u5168\u6807\u6ce8\u60c5\u51b5\u4e0b\u4e5f\u4f18\u4e8e\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "CmFNet\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u6df7\u5408\u76d1\u7763\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u6cbb\u7597\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "paper_title_zh": "CmFNet\uff1a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5f31\u76d1\u7763\u5206\u5272\u7684\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc", "abstract_zh": "\u51c6\u786e\u7684\u81ea\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u5bc6\u96c6\u6807\u6ce8\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u5f31\u76d1\u7763\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u7a00\u758f\u548c\u7c97\u7cd9\u6807\u6ce8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7a00\u758f\u6807\u6ce8\u6613\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u62df\u5408\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CmFNet\uff0c\u4e00\u79cd\u65b0\u578b3D\u5f31\u76d1\u7763\u8de8\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u3002CmFNet\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u3001\u8de8\u6a21\u6001\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u548c\u6df7\u5408\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u3002\u6a21\u6001\u7279\u5b9a\u548c\u8de8\u6a21\u6001\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u56fe\u50cf\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u589e\u5f3a\u8de8\u6a21\u6001\u5171\u4eab\u7279\u5f81\u4ee5\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6df7\u5408\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u901a\u8fc7\u6d82\u9e26\u76d1\u7763\u3001\u6a21\u6001\u5185\u6b63\u5219\u5316\u548c\u6a21\u6001\u95f4\u4e00\u81f4\u6027\u6307\u5bfc\u5206\u5272\uff0c\u5efa\u6a21\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u540c\u65f6\u4fc3\u8fdb\u7279\u5f81\u5bf9\u9f50\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\uff0c\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u5206\u5272\u7ed3\u679c\uff0c\u5728\u6311\u6218\u6027\u5c0f\u80bf\u7624\u533a\u57df\u548c\u5e38\u89c1\u89e3\u5256\u7ed3\u6784\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u5728\u4e34\u5e8a\u8de8\u6a21\u6001\u9f3b\u54bd\u764c\u6570\u636e\u96c6\uff08CT\u548cMR\uff09\u548c\u516c\u5f00\u7684CT\u5168\u8179\u90e8\u5668\u5b98\u6570\u636e\u96c6\uff08WORD\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u5168\u6807\u6ce8\u60c5\u51b5\u4e0b\u4e5f\u4f18\u4e8e\u5168\u76d1\u7763\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4e3a\u4e34\u5e8a\u6cbb\u7597\u63d0\u4f9b\u4fbf\u5229\uff0c\u5e76\u60e0\u53ca\u7269\u7406\u5b66\u5bb6\u3001\u653e\u5c04\u79d1\u533b\u751f\u3001\u75c5\u7406\u5b66\u5bb6\u548c\u80bf\u7624\u5b66\u5bb6\u7b49\u591a\u79cd\u4e13\u4e1a\u4eba\u5458\u3002"}}
{"id": "2506.17279", "pdf": "https://arxiv.org/pdf/2506.17279", "abs": "https://arxiv.org/abs/2506.17279", "authors": ["Yash Sinha", "Manit Baser", "Murari Mandal", "Dinil Mon Divakaran", "Mohan Kankanhalli"], "title": "Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Knowledge erasure in large language models (LLMs) is important for ensuring\ncompliance with data and AI regulations, safeguarding user privacy, mitigating\nbias, and misinformation. Existing unlearning methods aim to make the process\nof knowledge erasure more efficient and effective by removing specific\nknowledge while preserving overall model performance, especially for retained\ninformation. However, it has been observed that the unlearning techniques tend\nto suppress and leave the knowledge beneath the surface, thus making it\nretrievable with the right prompts. In this work, we demonstrate that\n\\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden\ninformation. We introduce a step-by-step reasoning-based black-box attack,\nSleek, that systematically exposes unlearning failures. We employ a structured\nattack framework with three core components: (1) an adversarial prompt\ngeneration strategy leveraging step-by-step reasoning built from LLM-generated\nqueries, (2) an attack mechanism that successfully recalls erased content, and\nexposes unfair suppression of knowledge intended for retention and (3) a\ncategorization of prompts as direct, indirect, and implied, to identify which\nquery types most effectively exploit unlearning weaknesses. Through extensive\nevaluations on four state-of-the-art unlearning techniques and two widely used\nLLMs, we show that existing approaches fail to ensure reliable knowledge\nremoval. Of the generated adversarial prompts, 62.5% successfully retrieved\nforgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair\nsuppression of retained knowledge. Our work highlights the persistent risks of\ninformation leakage, emphasizing the need for more robust unlearning strategies\nfor erasure.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u77e5\u8bc6\u64e6\u9664\u7684\u6f0f\u6d1e\uff0c\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u653b\u51fb\uff08Sleek\uff09\u6210\u529f\u6062\u590d\u88ab\u64e6\u9664\u7684\u77e5\u8bc6\uff0c\u66b4\u9732\u73b0\u6709\u9057\u5fd8\u6280\u672f\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u64e6\u9664\u5bf9\u6570\u636e\u5408\u89c4\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u51cf\u5c11\u504f\u89c1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9057\u5fd8\u6280\u672f\u4ec5\u8868\u9762\u6291\u5236\u77e5\u8bc6\uff0c\u672a\u5f7b\u5e95\u5220\u9664\uff0c\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u9010\u6b65\u63a8\u7406\u653b\u51fbSleek\uff0c\u5305\u542b\u4e09\u90e8\u5206\uff1a\u57fa\u4e8eLLM\u751f\u6210\u67e5\u8be2\u7684\u5bf9\u6297\u63d0\u793a\u7b56\u7565\u3001\u6210\u529f\u53ec\u56de\u88ab\u64e6\u9664\u5185\u5bb9\u7684\u653b\u51fb\u673a\u5236\uff0c\u4ee5\u53ca\u76f4\u63a5\u3001\u95f4\u63a5\u548c\u9690\u542b\u63d0\u793a\u5206\u7c7b\uff0c\u4ee5\u8bc6\u522b\u9057\u5fd8\u5f31\u70b9\u3002", "result": "\u5728\u56db\u79cd\u5148\u8fdb\u9057\u5fd8\u6280\u672f\u548c\u4e24\u79cdLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c62.5%\u7684\u5bf9\u6297\u63d0\u793a\u6210\u529f\u6062\u590d\u88ab\u64e6\u9664\u7684\u300a\u54c8\u5229\u6ce2\u7279\u300b\u77e5\u8bc6\uff0c50%\u66b4\u9732\u4e86\u5bf9\u4fdd\u7559\u77e5\u8bc6\u7684\u4e0d\u516c\u5e73\u6291\u5236\u3002", "conclusion": "\u73b0\u6709\u9057\u5fd8\u6280\u672f\u65e0\u6cd5\u53ef\u9760\u64e6\u9664\u77e5\u8bc6\uff0c\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u6301\u7eed\u5b58\u5728\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u9057\u5fd8\u7b56\u7565\u3002", "paper_title_zh": "\u9010\u6b65\u63a8\u7406\u653b\u51fb\uff1a\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u2018\u88ab\u64e6\u9664\u2019\u7684\u77e5\u8bc6", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u77e5\u8bc6\u64e6\u9664\u5bf9\u786e\u4fdd\u6570\u636e\u4e0eAI\u6cd5\u89c4\u5408\u89c4\u3001\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3001\u51cf\u5c11\u504f\u89c1\u548c\u9519\u8bef\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u65e8\u5728\u9ad8\u6548\u64e6\u9664\u7279\u5b9a\u77e5\u8bc6\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u6027\u80fd\uff0c\u5c24\u5176\u662f\u4fdd\u7559\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6280\u672f\u5f80\u5f80\u4ec5\u8868\u9762\u6291\u5236\u77e5\u8bc6\uff0c\u4f7f\u5176\u4ecd\u53ef\u901a\u8fc7\u9002\u5f53\u63d0\u793a\u6062\u590d\u3002\u672c\u6587\u8bc1\u660e\uff0c\u9010\u6b65\u63a8\u7406\u53ef\u4f5c\u4e3a\u6062\u590d\u9690\u85cf\u4fe1\u606f\u7684\u540e\u95e8\u3002\u6211\u4eec\u63d0\u51fa\u9010\u6b65\u63a8\u7406\u9ed1\u76d2\u653b\u51fbSleek\uff0c\u7cfb\u7edf\u6027\u66b4\u9732\u9057\u5fd8\u5931\u8d25\u3002\u653b\u51fb\u6846\u67b6\u5305\u542b\u4e09\u90e8\u5206\uff1a\uff081\uff09\u57fa\u4e8eLLM\u751f\u6210\u67e5\u8be2\u7684\u5bf9\u6297\u63d0\u793a\u7b56\u7565\uff0c\uff082\uff09\u6210\u529f\u53ec\u56de\u88ab\u64e6\u9664\u5185\u5bb9\u5e76\u63ed\u9732\u5bf9\u4fdd\u7559\u77e5\u8bc6\u4e0d\u516c\u5e73\u6291\u5236\u7684\u653b\u51fb\u673a\u5236\uff0c\uff083\uff09\u5c06\u63d0\u793a\u5206\u4e3a\u76f4\u63a5\u3001\u95f4\u63a5\u548c\u9690\u542b\u4e09\u7c7b\uff0c\u4ee5\u8bc6\u522b\u6700\u6709\u6548\u5229\u7528\u9057\u5fd8\u5f31\u70b9\u7684\u67e5\u8be2\u7c7b\u578b\u3002\u901a\u8fc7\u5bf9\u56db\u79cd\u5148\u8fdb\u9057\u5fd8\u6280\u672f\u548c\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528LLM\u7684\u8bc4\u4f30\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u786e\u4fdd\u53ef\u9760\u77e5\u8bc6\u64e6\u9664\u3002\u751f\u6210\u7684\u5bf9\u6297\u63d0\u793a\u4e2d\uff0c62.5%\u6210\u529f\u4eceWHP\u9057\u5fd8\u7684Llama\u4e2d\u6062\u590d\u300a\u54c8\u5229\u6ce2\u7279\u300b\u77e5\u8bc6\uff0c50%\u66b4\u9732\u4e86\u5bf9\u4fdd\u7559\u77e5\u8bc6\u7684\u4e0d\u516c\u5e73\u6291\u5236\u3002\u672c\u7814\u7a76\u51f8\u663e\u4fe1\u606f\u6cc4\u9732\u7684\u6301\u7eed\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u66f4\u9c81\u68d2\u7684\u9057\u5fd8\u7b56\u7565\u4ee5\u5b9e\u73b0\u64e6\u9664\u3002"}}
{"id": "2506.18781", "pdf": "https://arxiv.org/pdf/2506.18781", "abs": "https://arxiv.org/abs/2506.18781", "authors": ["Zhenru Lin", "Jiawen Tao", "Yang Yuan", "Andrew Chi-Chih Yao"], "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks", "categories": ["cs.CL"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e5f\u7f3a\u4e4f\u81ea\u6211\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5982DeepSeek-R1\u548cGPT-o4-mini\u4e5f\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u4ee5\u91cf\u5316\u5e76\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u6539\u8fdb\u6709\u9650\uff0c\u51f8\u663e\u4e86\u81ea\u6211\u4e00\u81f4\u6027\u5728\u6784\u5efa\u53ef\u9760AI\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u4f46\u5176\u51b3\u7b56\u7684\u900f\u660e\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u4f9d\u8d56\u4e8e\u81ea\u6211\u4e00\u81f4\u6027\uff08\u5373\u5185\u90e8\u63a8\u7406\u65e0\u77db\u76fe\uff09\u3002\u7136\u800c\uff0c\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u662f\u7b80\u5355\u4efb\u52a1\uff0cLLMs\u4e5f\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5982\u4f55\u91cf\u5316\u5e76\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff1a\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u548c\u7f13\u89e3LLMs\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e0d\u4e00\u81f4\u6027\u6307\u6807\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u81ea\u6211\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5982DeepSeek-R1\u548cGPT-o4-mini\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u867d\u80fd\u90e8\u5206\u6539\u8fdb\uff0c\u4f46\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u81ea\u6211\u4e00\u81f4\u6027\u662f\u6784\u5efa\u53ef\u9760\u548c\u53ef\u89e3\u91caAI\u7684\u5173\u952e\u56e0\u7d20\u3002\u5c3d\u7ba1\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u90e8\u5206\u6539\u8fdb\uff0c\u4f46\u95ee\u9898\u590d\u6742\u6027\u8868\u660e\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u81ea\u6211\u4e00\u81f4\u6027", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u4f46\u786e\u4fdd\u5176\u51b3\u7b56\u900f\u660e\u548c\u53ef\u4fe1\u8d56\u9700\u8981\u81ea\u6211\u4e00\u81f4\u6027\u2014\u2014\u5373\u5185\u90e8\u63a8\u7406\u65e0\u77db\u76fe\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u4efb\u52a1\uff08\u5982\u6bd4\u8f83\u7ebf\u4e0a\u6216\u5e73\u9762\u4e0a\u7684\u70b9\uff0c\u6216\u5bb6\u65cf\u6811\u63a8\u7406\uff09\u4e2d\uff0c\u6240\u6709\u8f83\u5c0f\u6a21\u578b\u5747\u9ad8\u5ea6\u4e0d\u4e00\u81f4\uff0c\u751a\u81f3\u5148\u8fdb\u6a21\u578b\u5982DeepSeek-R1\u548cGPT-o4-mini\u4e5f\u672a\u5b8c\u5168\u5b9e\u73b0\u81ea\u6211\u4e00\u81f4\u6027\u3002\u4e3a\u91cf\u5316\u548c\u7f13\u89e3\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e0d\u4e00\u81f4\u6027\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u2014\u2014\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u65b9\u6cd5\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6539\u8fdb\u63d0\u4f9b\u4e86\u90e8\u5206\u6548\u679c\uff0c\u4f46\u4e5f\u51f8\u663e\u4e86\u81ea\u6211\u4e00\u81f4\u6027\u5728\u6784\u5efa\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91caAI\u4e2d\u7684\u590d\u6742\u6027\u548c\u91cd\u8981\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://github.com/scorpio-nova/llm-self-consistency\u83b7\u53d6\u3002"}}
{"id": "2506.18048", "pdf": "https://arxiv.org/pdf/2506.18048", "abs": "https://arxiv.org/abs/2506.18048", "authors": ["Fanyi Wang", "Binzhi Dong", "Haotian Hu", "Jinjin Xu", "Zhiwang Zhang"], "title": "CLGRPO: Reasoning Ability Enhancement for Small VLMs", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Small Vision Language Models (SVLMs) generally refer to models with parameter\nsizes less than or equal to 2B. Their low cost and power consumption\ncharacteristics confer high commercial value. However, their reasoning\nabilities are limited by the number of parameters. To address this issue, this\npaper proposes a post-training optimization paradigm called the Incremental\nTraining Strategy to enhance the reasoning ability of SVLMs. Firstly, we\nconstructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,\nwhich leverages multiple LVLMs with 7B parameters or more to transform original\ndata into COT data in a self-supervised manner. Our proposed Incremental\nTraining Strategy consists of four stages. Stage 1 injects domain knowledge by\nperforming Supervised Fine-Tuning (SFT) to the pretrained model on the COT\ndata. Stage 2 aligns the COT data format by conducting a small amount of Group\nRelative Policy Optimization (GRPO) training constrained only by format rewards\non the COT data. Stage 3 enhances reasoning ability by applying GRPO training\non the COT data with constraints on both format and accuracy rewards. The\nresulting model shows significant improvement compared to the baseline. Stage 4\naddresses the limited capacity of the SVLMs and the weak ability to capture\ncomplex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture\nspace of the training process. We conducted extensive comparative and ablation\nexperiments on the abstract semantic recognition dataset EMOSet-118K.\nExperimental results demonstrate that our method significantly improves the\nreasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the\noriginal data, accuracy increased by 2.77 and recall by 0.69, achieving\nperformance comparable to that of 8B models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u8bad\u7ec3\u7b56\u7565CLGRPO\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6784\u5efa\u601d\u7ef4\u94fe\u6570\u636e\u548c\u591a\u9636\u6bb5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SVLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002", "motivation": "\u5c0f\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SVLM\uff09\u56e0\u53c2\u6570\u9650\u5236\u63a8\u7406\u80fd\u529b\u8f83\u5f31\uff0c\u4f46\u5176\u4f4e\u6210\u672c\u548c\u4f4e\u529f\u8017\u5177\u6709\u9ad8\u5546\u4e1a\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u540e\u8bad\u7ec3\u4f18\u5316\u63d0\u5347SVLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "1. \u81ea\u76d1\u7763\u6784\u5efa\u601d\u7ef4\u94fe\uff08COT\uff09\u6570\u636e\uff1b2. \u5206\u56db\u9636\u6bb5\u589e\u91cf\u8bad\u7ec3\uff1a\u9636\u6bb51\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\uff08SFT\uff09\uff0c\u9636\u6bb52\u5bf9\u9f50\u6570\u636e\u683c\u5f0f\uff08GRPO\uff09\uff0c\u9636\u6bb53\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff08GRPO+\u7cbe\u5ea6\u5956\u52b1\uff09\uff0c\u9636\u6bb54\u9650\u5236\u8bad\u7ec3\u7a7a\u95f4\uff08CLGRPO\uff09\u3002", "result": "\u5728EMOSet-118K\u6570\u636e\u96c6\u4e0a\uff0c1B SVLM\u7684\u51c6\u786e\u7387\u63d0\u53472.77\uff0c\u53ec\u56de\u7387\u63d0\u53470.69\uff0c\u6027\u80fd\u63a5\u8fd18B\u6a21\u578b\u3002", "conclusion": "CLGRPO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SVLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u589e\u91cf\u8bad\u7ec3\u7b56\u7565\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u5546\u4e1a\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "CLGRPO\uff1a\u589e\u5f3a\u5c0f\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "abstract_zh": "\u5c0f\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SVLM\uff09\u901a\u5e38\u6307\u53c2\u6570\u89c4\u6a21\u4e0d\u8d85\u8fc72B\u7684\u6a21\u578b\uff0c\u5176\u4f4e\u6210\u672c\u548c\u4f4e\u529f\u8017\u7279\u6027\u5177\u6709\u9ad8\u5546\u4e1a\u4ef7\u503c\u3002\u7136\u800c\uff0c\u5176\u63a8\u7406\u80fd\u529b\u53d7\u9650\u4e8e\u53c2\u6570\u89c4\u6a21\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u8bad\u7ec3\u4f18\u5316\u8303\u5f0f\u2014\u2014\u589e\u91cf\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u63d0\u5347SVLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u601d\u7ef4\u94fe\uff08COT\uff09\u6570\u636e\u751f\u6210\u7cfb\u7edf\uff0c\u5229\u7528\u591a\u4e2a\u53c2\u6570\u89c4\u6a21\u22657B\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3aCOT\u6570\u636e\u3002\u589e\u91cf\u8bad\u7ec3\u7b56\u7565\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff1a\u9636\u6bb51\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728COT\u6570\u636e\u4e0a\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\uff1b\u9636\u6bb52\u901a\u8fc7\u5c11\u91cf\u683c\u5f0f\u5956\u52b1\u7ea6\u675f\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5bf9\u9f50COT\u6570\u636e\u683c\u5f0f\uff1b\u9636\u6bb53\u901a\u8fc7\u683c\u5f0f\u548c\u7cbe\u5ea6\u5956\u52b1\u53cc\u91cd\u7ea6\u675f\u7684GRPO\u8bad\u7ec3\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff1b\u9636\u6bb54\u63d0\u51faClipLow GRPO\uff08CLGRPO\uff09\u9650\u5236\u8bad\u7ec3\u7a7a\u95f4\uff0c\u89e3\u51b3SVLM\u5bb9\u91cf\u6709\u9650\u548c\u590d\u6742\u6a21\u5f0f\u6355\u6349\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002\u5728\u62bd\u8c61\u8bed\u4e49\u8bc6\u522b\u6570\u636e\u96c6EMOSet-118K\u4e0a\u7684\u5bf9\u6bd4\u548c\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e861B SVLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u51c6\u786e\u7387\u63d0\u53472.77\uff0c\u53ec\u56de\u7387\u63d0\u53470.69\uff0c\u6027\u80fd\u63a5\u8fd18B\u6a21\u578b\u3002"}}
{"id": "2506.17281", "pdf": "https://arxiv.org/pdf/2506.17281", "abs": "https://arxiv.org/abs/2506.17281", "authors": ["Junze Chen", "Xinjie Yang", "Cheng Yang", "Junfei Bao", "Zeyuan Guo", "Yawen Li", "Chuan Shi"], "title": "CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems (RSs) are designed to retrieve candidate items a user\nmight be interested in from a large pool. A common approach is using graph\nneural networks (GNNs) to capture high-order interaction relationships. As\nlarge language models (LLMs) have shown strong capabilities across domains,\nresearchers are exploring their use to enhance recommendation. However, prior\nwork limits LLMs to re-ranking results or dataset augmentation, failing to\nutilize their power during candidate filtering - which may lead to suboptimal\nperformance. Instead, we propose to leverage LLMs' reasoning abilities during\nthe candidate filtering process, and introduce Chain Of Retrieval ON grAphs\n(CORONA) to progressively narrow down the range of candidate items on\ninteraction graphs with the help of LLMs: (1) First, LLM performs preference\nreasoning based on user profiles, with the response serving as a query to\nextract relevant users and items from the interaction graph as\npreference-assisted retrieval; (2) Then, using the information retrieved in the\nprevious step along with the purchase history of target user, LLM conducts\nintent reasoning to help refine an even smaller interaction subgraph as\nintent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order\ncollaborative filtering information from the extracted subgraph, performing\nGNN-enhanced retrieval to generate the final recommendation results. The\nproposed framework leverages the reasoning capabilities of LLMs during the\nretrieval process, while seamlessly integrating GNNs to enhance overall\nrecommendation performance. Extensive experiments on various datasets and\nsettings demonstrate that our proposed CORONA achieves state-of-the-art\nperformance with an 18.6% relative improvement in recall and an 18.4% relative\nimprovement in NDCG on average.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u8350\u6846\u67b6CORONA\uff0c\u901a\u8fc7\u9010\u6b65\u7f29\u5c0f\u5019\u9009\u8303\u56f4\uff0c\u7ed3\u5408LLMs\u7684\u63a8\u7406\u80fd\u529b\u548cGNN\u7684\u9ad8\u9636\u5173\u7cfb\u6355\u6349\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6355\u6349\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u5c24\u5176\u662f\u5728\u5019\u9009\u8fc7\u6ee4\u9636\u6bb5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7LLMs\u7684\u63a8\u7406\u80fd\u529b\u4f18\u5316\u5019\u9009\u8fc7\u6ee4\u8fc7\u7a0b\uff0c\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "method": "CORONA\u6846\u67b6\u5206\u4e3a\u4e09\u6b65\uff1a(1) \u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u504f\u597d\u63a8\u7406\uff0c\u63d0\u53d6\u76f8\u5173\u7528\u6237\u548c\u7269\u54c1\uff1b(2) \u7ed3\u5408\u76ee\u6807\u7528\u6237\u8d2d\u4e70\u5386\u53f2\u7684\u610f\u56fe\u63a8\u7406\uff0c\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4ea4\u4e92\u5b50\u56fe\uff1b(3) \u4f7f\u7528GNN\u4ece\u5b50\u56fe\u4e2d\u6355\u6349\u9ad8\u9636\u534f\u540c\u8fc7\u6ee4\u4fe1\u606f\uff0c\u751f\u6210\u6700\u7ec8\u63a8\u8350\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCORONA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u53ec\u56de\u7387\u548cNDCG\u5206\u522b\u5e73\u5747\u63d0\u534718.6%\u548c18.4%\u3002", "conclusion": "CORONA\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u63a8\u7406\u80fd\u529b\u548cGNN\u7684\u9ad8\u9636\u5173\u7cfb\u6355\u6349\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u63a8\u8350\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "paper_title_zh": "CORONA\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u63a8\u8350\u6846\u67b6\u2014\u2014\u4ece\u7c97\u5230\u7ec6\u7684\u68c0\u7d22\u94fe", "abstract_zh": "\u63a8\u8350\u7cfb\u7edf\uff08RSs\uff09\u65e8\u5728\u4ece\u5927\u91cf\u5019\u9009\u4e2d\u68c0\u7d22\u7528\u6237\u53ef\u80fd\u611f\u5174\u8da3\u7684\u7269\u54c1\u3002\u5e38\u89c1\u65b9\u6cd5\u662f\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6355\u6349\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u7814\u7a76\u8005\u5f00\u59cb\u63a2\u7d22\u5176\u5728\u63a8\u8350\u4e2d\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u4f5c\u4ec5\u5c06LLMs\u7528\u4e8e\u91cd\u6392\u5e8f\u6216\u6570\u636e\u589e\u5f3a\uff0c\u672a\u5145\u5206\u5229\u7528\u5176\u5728\u5019\u9009\u8fc7\u6ee4\u9636\u6bb5\u7684\u6f5c\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u5728\u5019\u9009\u8fc7\u6ee4\u8fc7\u7a0b\u4e2d\u5229\u7528LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u94fe\uff08CORONA\uff09\uff0c\u9010\u6b65\u7f29\u5c0f\u4ea4\u4e92\u56fe\u4e2d\u7684\u5019\u9009\u8303\u56f4\uff1a(1) \u9996\u5148\uff0cLLM\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u8fdb\u884c\u504f\u597d\u63a8\u7406\uff0c\u5176\u54cd\u5e94\u4f5c\u4e3a\u67e5\u8be2\u4ece\u4ea4\u4e92\u56fe\u4e2d\u63d0\u53d6\u76f8\u5173\u7528\u6237\u548c\u7269\u54c1\uff1b(2) \u7136\u540e\uff0c\u7ed3\u5408\u76ee\u6807\u7528\u6237\u8d2d\u4e70\u5386\u53f2\uff0cLLM\u8fdb\u884c\u610f\u56fe\u63a8\u7406\uff0c\u8fdb\u4e00\u6b65\u7ec6\u5316\u4ea4\u4e92\u5b50\u56fe\uff1b(3) \u6700\u540e\uff0c\u4f7f\u7528GNN\u4ece\u5b50\u56fe\u4e2d\u6355\u6349\u9ad8\u9636\u534f\u540c\u8fc7\u6ee4\u4fe1\u606f\uff0c\u751f\u6210\u6700\u7ec8\u63a8\u8350\u7ed3\u679c\u3002CORONA\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5145\u5206\u5229\u7528LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u65e0\u7f1d\u96c6\u6210GNNs\u4ee5\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u660e\uff0cCORONA\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u53ec\u56de\u7387\u548cNDCG\u5206\u522b\u5e73\u5747\u63d0\u534718.6%\u548c18.4%\u3002"}}
{"id": "2506.18819", "pdf": "https://arxiv.org/pdf/2506.18819", "abs": "https://arxiv.org/abs/2506.18819", "authors": ["Arjun Mukerji", "Michael L. Jackson", "Jason Jones", "Neil Sanghavi"], "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 2 figures", "summary": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RWESummary\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u603b\u7ed3\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\uff08RWE\uff09\u7814\u7a76\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u4e00\u4e2a\u573a\u666f\u548c\u4e09\u9879\u8bc4\u4f30\uff0c\u53d1\u73b0Gemini 2.5\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u76ee\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u7814\u7a76\u8f85\u52a9\u548c\u901a\u7528\u6458\u8981\u4efb\u52a1\u4e2d\u5df2\u6709\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f46\u5c1a\u672a\u4e13\u95e8\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\uff08RWE\uff09\u7814\u7a76\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u6458\u8981\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u63d0\u51faRWESummary\u6846\u67b6\uff0c\u4f5c\u4e3aMedHELM\u6846\u67b6\u7684\u8865\u5145\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728RWE\u7814\u7a76\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u573a\u666f\u548c\u4e09\u9879\u8bc4\u4f30\uff0c\u57fa\u4e8eAtropos Health\u4e13\u6709\u6570\u636e\u5f00\u53d1\u3002", "result": "\u572813\u9879RWE\u7814\u7a76\u4e2d\uff0cGemini 2.5\u6a21\u578b\uff08Flash\u548cPro\u7248\u672c\uff09\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "RWESummary\u4e3a\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u7814\u7a76\u6458\u8981\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u4e14\u5b9e\u7528\u7684\u57fa\u51c6\u6a21\u578b\u3002", "paper_title_zh": "RWESummary\uff1a\u4e00\u4e2a\u7528\u4e8e\u9009\u62e9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u603b\u7ed3\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\uff08RWE\uff09\u7814\u7a76\u7684\u6846\u67b6\u4e0e\u6d4b\u8bd5", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u901a\u7528\u6458\u8981\u4efb\u52a1\u548c\u533b\u5b66\u7814\u7a76\u8f85\u52a9\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f46\u5c1a\u672a\u4e13\u95e8\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\uff08RWE\uff09\u7814\u7a76\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u6458\u8981\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002\u6211\u4eec\u63d0\u51fa\u4e86RWESummary\uff0c\u4f5c\u4e3aMedHELM\u6846\u67b6\uff08Bedi, Cui, Fuentes, Unell\u7b49\uff0c2025\uff09\u7684\u8865\u5145\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002RWESummary\u5305\u62ec\u4e00\u4e2a\u573a\u666f\u548c\u4e09\u9879\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u533b\u5b66\u7814\u7a76\u6458\u8981\u4e2d\u89c2\u5bdf\u5230\u7684\u4e3b\u8981\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u4f7f\u7528Atropos Health\u4e13\u6709\u6570\u636e\u5f00\u53d1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528RWESummary\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u5728\u6211\u4eec\u5185\u90e8RWE\u6458\u8981\u5de5\u5177\u4e2d\u7684\u8868\u73b0\u3002\u5728\u53d1\u8868\u65f6\uff0c\u57fa\u4e8e13\u9879RWE\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u73b0Gemini 2.5\u6a21\u578b\uff08Flash\u548cPro\u7248\u672c\uff09\u6574\u4f53\u8868\u73b0\u6700\u4f73\u3002\u6211\u4eec\u5efa\u8bae\u5c06RWESummary\u4f5c\u4e3a\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u7814\u7a76\u6458\u8981\u4efb\u52a1\u7684\u65b0\u9896\u4e14\u5b9e\u7528\u7684\u57fa\u51c6\u6a21\u578b\u3002"}}
{"id": "2506.18060", "pdf": "https://arxiv.org/pdf/2506.18060", "abs": "https://arxiv.org/abs/2506.18060", "authors": ["Olivia Zumsteg", "Nico Graf", "Aaron Haeusler", "Norbert Kirchgessner", "Nicola Storni", "Lukas Roth", "Andreas Hund"], "title": "Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "Estimating three-dimensional morphological traits from two-dimensional RGB\nimages presents inherent challenges due to the loss of depth information,\nprojection distortions, and occlusions under field conditions. In this work, we\nexplore multiple approaches for non-destructive volume estimation of wheat\nspikes, using RGB image sequences and structured-light 3D scans as ground truth\nreferences. Due to the complex geometry of the spikes, we propose a neural\nnetwork approach for volume estimation in 2D images, employing a transfer\nlearning pipeline that combines DINOv2, a self-supervised Vision Transformer,\nwith a unidirectional Long Short-Term Memory (LSTM) network. By using deep\nsupervision, the model is able to learn more robust intermediate\nrepresentations, which enhances its generalisation ability across varying\nevaluation sequences. We benchmark our model against two conventional\nbaselines: a 2D area-based projection and a geometric reconstruction using\naxis-aligned cross-sections. Our deep supervised model achieves a mean absolute\npercentage error (MAPE) of 6.46% on six-view indoor images, outperforming the\narea (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on\nfield-based single-image data enables domain adaptation, yielding a MAPE of\n10.82%. We demonstrate that object shape significantly impacts volume\nprediction accuracy, with irregular geometries such as wheat spikes posing\ngreater challenges for geometric methods compared to our deep learning\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u76d1\u7763LSTM\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4e2d\u4f30\u8ba1\u5c0f\u9ea6\u7a57\u7684\u4e09\u7ef4\u5f62\u6001\uff0c\u901a\u8fc7\u7ed3\u5408DINOv2\u548cLSTM\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f53\u79ef\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u4e8c\u7ef4RGB\u56fe\u50cf\u5728\u7530\u95f4\u6761\u4ef6\u4e0b\u5b58\u5728\u6df1\u5ea6\u4fe1\u606f\u4e22\u5931\u3001\u6295\u5f71\u5931\u771f\u548c\u906e\u6321\u7b49\u95ee\u9898\uff0c\u51c6\u786e\u4f30\u8ba1\u5c0f\u9ea6\u7a57\u7684\u4e09\u7ef4\u5f62\u6001\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u975e\u7834\u574f\u6027\u7684\u4f53\u79ef\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763Vision Transformer\uff08DINOv2\uff09\u548c\u5355\u5411LSTM\u7f51\u7edc\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u76d1\u7763\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u8bc4\u4f30\u5e8f\u5217\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u516d\u89c6\u89d2\u5ba4\u5185\u56fe\u50cf\u4e0a\uff0c\u6a21\u578b\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a6.46%\uff0c\u4f18\u4e8e\u57fa\u4e8e\u9762\u79ef\u7684\u6295\u5f71\uff089.36%\uff09\u548c\u51e0\u4f55\u91cd\u5efa\uff0813.98%\uff09\u57fa\u7ebf\u3002\u5728\u5355\u56fe\u50cf\u7530\u95f4\u6570\u636e\u4e0a\u5fae\u8c03\u540e\uff0cMAPE\u4e3a10.82%\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff08\u5982\u5c0f\u9ea6\u7a57\uff09\u7684\u4f53\u79ef\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\uff0c\u4e14\u901a\u8fc7\u57df\u9002\u5e94\u80fd\u529b\u5728\u7530\u95f4\u6761\u4ef6\u4e0b\u4ecd\u4fdd\u6301\u8f83\u9ad8\u51c6\u786e\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u6df1\u5ea6\u76d1\u7763LSTM\u7684\u5c0f\u9ea6\u7a57\u4e09\u7ef4\u5f62\u6001\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4f30\u8ba1", "abstract_zh": "\u4ece\u4e8c\u7ef4RGB\u56fe\u50cf\u4e2d\u4f30\u8ba1\u4e09\u7ef4\u5f62\u6001\u7279\u5f81\u5b58\u5728\u6df1\u5ea6\u4fe1\u606f\u4e22\u5931\u3001\u6295\u5f71\u5931\u771f\u548c\u906e\u6321\u7b49\u56fa\u6709\u6311\u6218\u3002\u672c\u6587\u63a2\u7d22\u4e86\u591a\u79cd\u975e\u7834\u574f\u6027\u5c0f\u9ea6\u7a57\u4f53\u79ef\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528RGB\u56fe\u50cf\u5e8f\u5217\u548c\u7ed3\u6784\u51493D\u626b\u63cf\u4f5c\u4e3a\u771f\u5b9e\u53c2\u8003\u3002\u9488\u5bf9\u7a57\u7684\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u76d1\u7763Vision Transformer\uff08DINOv2\uff09\u548c\u5355\u5411LSTM\u7f51\u7edc\uff0c\u901a\u8fc7\u6df1\u5ea6\u76d1\u7763\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u57fa\u4e8e\u9762\u79ef\u7684\u6295\u5f71\u548c\u51e0\u4f55\u91cd\u5efa\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u516d\u89c6\u89d2\u5ba4\u5185\u56fe\u50cf\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a6.46%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff089.36%\u548c13.98%\uff09\u3002\u5728\u5355\u56fe\u50cf\u7530\u95f4\u6570\u636e\u4e0a\u5fae\u8c03\u540e\uff0cMAPE\u4e3a10.82%\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7269\u4f53\u5f62\u72b6\u5bf9\u4f53\u79ef\u9884\u6d4b\u51c6\u786e\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff08\u5982\u5c0f\u9ea6\u7a57\uff09\u5bf9\u51e0\u4f55\u65b9\u6cd5\u7684\u6311\u6218\u66f4\u5927\uff0c\u800c\u6211\u4eec\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2506.17284", "pdf": "https://arxiv.org/pdf/2506.17284", "abs": "https://arxiv.org/abs/2506.17284", "authors": ["Ali Peivandizadeh"], "title": "A Theoretical Framework for Virtual Power Plant Integration with Gigawatt-Scale AI Data Centers: Multi-Timescale Control and Stability Analysis", "categories": ["eess.SY", "cs.AI", "cs.SY", "C.4"], "comment": "19 pages, 5 figures", "summary": "The explosive growth of artificial intelligence has created gigawatt-scale\ndata centers that fundamentally challenge power system operation, exhibiting\npower fluctuations exceeding 500 MW within seconds and millisecond-scale\nvariations of 50-75% of thermal design power. This paper presents a\ncomprehensive theoretical framework that reconceptualizes Virtual Power Plants\n(VPPs) to accommodate these extreme dynamics through a four-layer hierarchical\ncontrol architecture operating across timescales from 100 microseconds to 24\nhours.\n  We develop control mechanisms and stability criteria specifically tailored to\nconverter-dominated systems with pulsing megawatt-scale loads. We prove that\ntraditional VPP architectures, designed for aggregating distributed resources\nwith response times of seconds to minutes, cannot maintain stability when\nconfronted with AI data center dynamics exhibiting slew rates exceeding 1,000\nMW/s at gigawatt scale.\n  Our framework introduces: (1) a sub-millisecond control layer that interfaces\nwith data center power electronics to actively dampen power oscillations; (2)\nnew stability criteria incorporating protection system dynamics, demonstrating\nthat critical clearing times reduce from 150 ms to 83 ms for gigawatt-scale\npulsing loads; and (3) quantified flexibility characterization showing that\nworkload deferability enables 30% peak reduction while maintaining AI service\navailability above 99.95%.\n  This work establishes the mathematical foundations necessary for the stable\nintegration of AI infrastructure that will constitute 50-70% of data center\nelectricity consumption by 2030.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5409\u74e6\u7ea7AI\u6570\u636e\u4e2d\u5fc3\u4e0e\u865a\u62df\u7535\u5382\uff08VPP\uff09\u96c6\u6210\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u5c42\u5206\u5c42\u63a7\u5236\u67b6\u6784\u89e3\u51b3\u6781\u7aef\u52a8\u6001\u529f\u7387\u6ce2\u52a8\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u8f6c\u6362\u5668\u4e3b\u5bfc\u7cfb\u7edf\u7684\u63a7\u5236\u673a\u5236\u548c\u7a33\u5b9a\u6027\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u7206\u70b8\u6027\u589e\u957f\uff0c\u5409\u74e6\u7ea7\u6570\u636e\u4e2d\u5fc3\u7684\u529f\u7387\u6ce2\u52a8\uff08\u5982\u79d2\u7ea7\u8d85\u8fc7500 MW\u3001\u6beb\u79d2\u7ea7\u8fbe50-75%\u70ed\u8bbe\u8ba1\u529f\u7387\uff09\u5bf9\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u63d0\u51fa\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u4f20\u7edfVPP\u67b6\u6784\u65e0\u6cd5\u5e94\u5bf9\u8fd9\u79cd\u6781\u7aef\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u5c42\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff0c\u6db5\u76d6100\u5fae\u79d2\u81f324\u5c0f\u65f6\u7684\u65f6\u95f4\u5c3a\u5ea6\uff0c\u5305\u62ec\uff1a\uff081\uff09\u4e9a\u6beb\u79d2\u7ea7\u63a7\u5236\u5c42\u6291\u5236\u529f\u7387\u632f\u8361\uff1b\uff082\uff09\u65b0\u7684\u7a33\u5b9a\u6027\u6807\u51c6\uff1b\uff083\uff09\u91cf\u5316\u7075\u6d3b\u6027\u8868\u5f81\u3002", "result": "\u8bc1\u660e\u4e86\u4f20\u7edfVPP\u67b6\u6784\u5728\u5409\u74e6\u7ea7\u8109\u51b2\u8d1f\u8f7d\uff08\u53d8\u5316\u901f\u7387\u8d85\u8fc71,000 MW/s\uff09\u4e0b\u65e0\u6cd5\u4fdd\u6301\u7a33\u5b9a\uff0c\u65b0\u6846\u67b6\u5c06\u4e34\u754c\u6e05\u9664\u65f6\u95f4\u4ece150 ms\u964d\u81f383 ms\uff0c\u5e76\u901a\u8fc7\u8d1f\u8f7d\u53ef\u5ef6\u8fdf\u6027\u5b9e\u73b030%\u5cf0\u503c\u524a\u51cf\uff0c\u540c\u65f6\u4fdd\u6301AI\u670d\u52a1\u53ef\u7528\u6027\u9ad8\u4e8e99.95%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a2030\u5e74\u5c06\u5360\u6570\u636e\u4e2d\u5fc3\u7535\u529b\u6d88\u801750-70%\u7684AI\u57fa\u7840\u8bbe\u65bd\u7684\u7a33\u5b9a\u96c6\u6210\u5960\u5b9a\u4e86\u6570\u5b66\u57fa\u7840\u3002", "paper_title_zh": "\u5409\u74e6\u7ea7AI\u6570\u636e\u4e2d\u5fc3\u4e0e\u865a\u62df\u7535\u5382\u96c6\u6210\u7684\u7406\u8bba\u6846\u67b6\uff1a\u591a\u65f6\u95f4\u5c3a\u5ea6\u63a7\u5236\u4e0e\u7a33\u5b9a\u6027\u5206\u6790", "abstract_zh": "\u4eba\u5de5\u667a\u80fd\u7684\u7206\u70b8\u6027\u589e\u957f\u50ac\u751f\u4e86\u5409\u74e6\u7ea7\u6570\u636e\u4e2d\u5fc3\uff0c\u5176\u529f\u7387\u6ce2\u52a8\uff08\u79d2\u7ea7\u8d85\u8fc7500 MW\uff0c\u6beb\u79d2\u7ea7\u8fbe50-75%\u70ed\u8bbe\u8ba1\u529f\u7387\uff09\u5bf9\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u63d0\u51fa\u4e86\u6839\u672c\u6027\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u5c42\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff08\u65f6\u95f4\u5c3a\u5ea6\u4ece100\u5fae\u79d2\u81f324\u5c0f\u65f6\uff09\u91cd\u65b0\u5b9a\u4e49\u865a\u62df\u7535\u5382\uff08VPP\uff09\uff0c\u4ee5\u9002\u5e94\u8fd9\u4e9b\u6781\u7aef\u52a8\u6001\u3002\n\n\u6211\u4eec\u5f00\u53d1\u4e86\u4e13\u95e8\u9488\u5bf9\u8f6c\u6362\u5668\u4e3b\u5bfc\u7cfb\u7edf\u548c\u5146\u74e6\u7ea7\u8109\u51b2\u8d1f\u8f7d\u7684\u63a7\u5236\u673a\u5236\u53ca\u7a33\u5b9a\u6027\u6807\u51c6\uff0c\u5e76\u8bc1\u660e\u4f20\u7edfVPP\u67b6\u6784\uff08\u8bbe\u8ba1\u7528\u4e8e\u805a\u5408\u54cd\u5e94\u65f6\u95f4\u4e3a\u79d2\u81f3\u5206\u949f\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\uff09\u5728\u5409\u74e6\u7ea7AI\u6570\u636e\u4e2d\u5fc3\u52a8\u6001\uff08\u53d8\u5316\u901f\u7387\u8d85\u8fc71,000 MW/s\uff09\u4e0b\u65e0\u6cd5\u4fdd\u6301\u7a33\u5b9a\u3002\n\n\u65b0\u6846\u67b6\u5305\u62ec\uff1a\uff081\uff09\u4e9a\u6beb\u79d2\u7ea7\u63a7\u5236\u5c42\uff0c\u4e0e\u6570\u636e\u4e2d\u5fc3\u7535\u529b\u7535\u5b50\u8bbe\u5907\u4ea4\u4e92\u4ee5\u4e3b\u52a8\u6291\u5236\u529f\u7387\u632f\u8361\uff1b\uff082\uff09\u7ed3\u5408\u4fdd\u62a4\u7cfb\u7edf\u52a8\u6001\u7684\u65b0\u7a33\u5b9a\u6027\u6807\u51c6\uff0c\u8868\u660e\u5409\u74e6\u7ea7\u8109\u51b2\u8d1f\u8f7d\u7684\u4e34\u754c\u6e05\u9664\u65f6\u95f4\u4ece150 ms\u964d\u81f383 ms\uff1b\uff083\uff09\u91cf\u5316\u7075\u6d3b\u6027\u8868\u5f81\uff0c\u663e\u793a\u8d1f\u8f7d\u53ef\u5ef6\u8fdf\u6027\u53ef\u5b9e\u73b030%\u5cf0\u503c\u524a\u51cf\uff0c\u540c\u65f6\u4fdd\u6301AI\u670d\u52a1\u53ef\u7528\u6027\u9ad8\u4e8e99.95%\u3002\n\n\u672c\u7814\u7a76\u4e3a2030\u5e74\u5c06\u5360\u6570\u636e\u4e2d\u5fc3\u7535\u529b\u6d88\u801750-70%\u7684AI\u57fa\u7840\u8bbe\u65bd\u7684\u7a33\u5b9a\u96c6\u6210\u5960\u5b9a\u4e86\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2506.18828", "pdf": "https://arxiv.org/pdf/2506.18828", "abs": "https://arxiv.org/abs/2506.18828", "authors": ["Jorge Iranzo-S\u00e1nchez", "Javier Iranzo-S\u00e1nchez", "Adri\u00e0 Gim\u00e9nez", "Jorge Civera", "Alfons Juan"], "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task", "categories": ["cs.CL"], "comment": "IWSLT 2025 System Description", "summary": "This work describes the participation of the MLLP-VRAIN research group in the\nshared task of the IWSLT 2025 Simultaneous Speech Translation track. Our\nsubmission addresses the unique challenges of real-time translation of\nlong-form speech by developing a modular cascade system that adapts strong\npre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo\nfor ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight\nadaptation techniques rather than training new end-to-end models from scratch.\nOur approach employs document-level adaptation with prefix training to enhance\nthe MT model's ability to handle incomplete inputs, while incorporating\nadaptive emission policies including a wait-$k$ strategy and RALCP for managing\nthe translation stream. Specialized buffer management techniques and\nsegmentation strategies ensure coherent translations across long audio\nsequences. Experimental results on the ACL60/60 dataset demonstrate that our\nsystem achieves a favorable balance between translation quality and latency,\nwith a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of\n2.94 seconds. Our final model achieves a preliminary score on the official test\nset (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully\nadapted pre-trained components can create effective simultaneous translation\nsystems for long-form content without requiring extensive in-domain parallel\ndata or specialized end-to-end training.", "AI": {"tldr": "MLLP-VRAIN\u7814\u7a76\u7ec4\u5728IWSLT 2025\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7ea7\u8054\u7cfb\u7edf\uff0c\u7ed3\u5408Whisper\u548cNLLB\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u5e94\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u957f\u8bed\u97f3\u7ffb\u8bd1\uff0c\u5e73\u8861\u4e86\u7ffb\u8bd1\u8d28\u91cf\u548c\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u957f\u8bed\u97f3\u5b9e\u65f6\u7ffb\u8bd1\u7684\u72ec\u7279\u6311\u6218\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u52bf\u3002", "method": "\u7ed3\u5408Whisper Large-V3-Turbo\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0cNLLB-3.3B\u8fdb\u884c\u591a\u8bed\u8a00\u7ffb\u8bd1\uff0c\u91c7\u7528\u6587\u6863\u7ea7\u9002\u5e94\u548c\u524d\u7f00\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\uff0c\u7ed3\u5408wait-$k$\u7b56\u7565\u548cRALCP\u7ba1\u7406\u7ffb\u8bd1\u6d41\uff0c\u4f7f\u7528\u7f13\u51b2\u533a\u7ba1\u7406\u548c\u5206\u6bb5\u7b56\u7565\u786e\u4fdd\u8fde\u8d2f\u6027\u3002", "result": "\u5728ACL60/60\u6570\u636e\u96c6\u4e0a\uff0cBLEU\u5f97\u5206\u4e3a31.96\uff0c\u5ef6\u8fdf\u4e3a2.94\u79d2\uff1b\u5728IWSLT25Instruct\u6d4b\u8bd5\u96c6\u4e0a\u521d\u6b65\u5f97\u5206\u4e3a29.8 BLEU\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8c03\u6574\u9884\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u65e0\u9700\u5927\u91cf\u9886\u57df\u5185\u5e76\u884c\u6570\u636e\u6216\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5373\u53ef\u6784\u5efa\u6709\u6548\u7684\u957f\u8bed\u97f3\u540c\u6b65\u7ffb\u8bd1\u7cfb\u7edf\u3002", "paper_title_zh": "MLLP-VRAIN UPV\u7cfb\u7edf\u5728IWSLT 2025\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u672c\u6587\u63cf\u8ff0\u4e86MLLP-VRAIN\u7814\u7a76\u7ec4\u5728IWSLT 2025\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u53c2\u4e0e\u60c5\u51b5\u3002\u6211\u4eec\u7684\u63d0\u4ea4\u901a\u8fc7\u5f00\u53d1\u6a21\u5757\u5316\u7ea7\u8054\u7cfb\u7edf\uff0c\u5c06\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u6d41\u5f0f\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u957f\u8bed\u97f3\u5b9e\u65f6\u7ffb\u8bd1\u7684\u72ec\u7279\u6311\u6218\u3002\u6211\u4eec\u7ed3\u5408Whisper Large-V3-Turbo\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0c\u4ee5\u53ca\u591a\u8bed\u8a00NLLB-3.3B\u6a21\u578b\u8fdb\u884c\u673a\u5668\u7ffb\u8bd1\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u9002\u5e94\u6280\u672f\u800c\u975e\u4ece\u5934\u8bad\u7ec3\u7aef\u5230\u7aef\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u6587\u6863\u7ea7\u9002\u5e94\u548c\u524d\u7f00\u8bad\u7ec3\u63d0\u5347MT\u6a21\u578b\u5904\u7406\u4e0d\u5b8c\u6574\u8f93\u5165\u7684\u80fd\u529b\uff0c\u540c\u65f6\u7ed3\u5408wait-$k$\u7b56\u7565\u548cRALCP\u7ba1\u7406\u7ffb\u8bd1\u6d41\u3002\u4e13\u7528\u7f13\u51b2\u533a\u7ba1\u7406\u548c\u5206\u6bb5\u7b56\u7565\u786e\u4fdd\u957f\u97f3\u9891\u5e8f\u5217\u7684\u8fde\u8d2f\u7ffb\u8bd1\u3002\u5728ACL60/60\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u7ffb\u8bd1\u8d28\u91cf\u548c\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0cBLEU\u5f97\u5206\u4e3a31.96\uff0c\u975e\u8ba1\u7b97\u611f\u77e5\u7684StreamLAAL\u5ef6\u8fdf\u4e3a2.94\u79d2\u3002\u6700\u7ec8\u6a21\u578b\u5728\u5b98\u65b9\u6d4b\u8bd5\u96c6\uff08IWSLT25Instruct\uff09\u4e0a\u7684\u521d\u6b65\u5f97\u5206\u4e3a29.8 BLEU\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8c03\u6574\u7684\u9884\u8bad\u7ec3\u7ec4\u4ef6\u53ef\u4ee5\u6784\u5efa\u6709\u6548\u7684\u957f\u5185\u5bb9\u540c\u6b65\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u65e0\u9700\u5927\u91cf\u9886\u57df\u5185\u5e76\u884c\u6570\u636e\u6216\u4e13\u95e8\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002"}}
{"id": "2506.18070", "pdf": "https://arxiv.org/pdf/2506.18070", "abs": "https://arxiv.org/abs/2506.18070", "authors": ["Hangzhou He", "Jiachen Tang", "Lei Zhu", "Kaiwen Li", "Yanye Lu"], "title": "Training-free Test-time Improvement for Explainable Medical Image Classification", "categories": ["cs.CV"], "comment": "This is the initial version of our work accepted by MICCAI 2025.\n  We'll include a link to the version on SpringerLink after this becomes\n  available", "summary": "Deep learning-based medical image classification techniques are rapidly\nadvancing in medical image analysis, making it crucial to develop accurate and\ntrustworthy models that can be efficiently deployed across diverse clinical\nscenarios. Concept Bottleneck Models (CBMs), which first predict a set of\nexplainable concepts from images and then perform classification based on these\nconcepts, are increasingly being adopted for explainable medical image\nclassification. However, the inherent explainability of CBMs introduces new\nchallenges when deploying trained models to new environments. Variations in\nimaging protocols and staining methods may induce concept-level shifts, such as\nalterations in color distribution and scale. Furthermore, since CBM training\nrequires explicit concept annotations, fine-tuning models solely with\nimage-level labels could compromise concept prediction accuracy and\nfaithfulness - a critical limitation given the high cost of acquiring\nexpert-annotated concept labels in medical domains. To address these\nchallenges, we propose a training-free confusion concept identification\nstrategy. By leveraging minimal new data (e.g., 4 images per class) with only\nimage-level labels, our approach enhances out-of-domain performance without\nsacrificing source domain accuracy through two key operations: masking\nmisactivated confounding concepts and amplifying under-activated discriminative\nconcepts. The efficacy of our method is validated on both skin and white blood\ncell images. Our code is available at:\nhttps://github.com/riverback/TF-TTI-XMed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u6539\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u53ef\u89e3\u91ca\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd\uff0c\u901a\u8fc7\u5c4f\u853d\u6df7\u6dc6\u6982\u5ff5\u548c\u589e\u5f3a\u5224\u522b\u6027\u6982\u5ff5\uff0c\u4ec5\u9700\u5c11\u91cf\u5e26\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u65b0\u6570\u636e\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u56e0\u5176\u53ef\u89e3\u91ca\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5728\u65b0\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u6210\u50cf\u534f\u8bae\u548c\u67d3\u8272\u65b9\u6cd5\u7684\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u6982\u5ff5\u7ea7\u504f\u79fb\u3002\u6b64\u5916\uff0cCBMs\u8bad\u7ec3\u9700\u8981\u4e13\u5bb6\u6807\u6ce8\u7684\u6982\u5ff5\u6807\u7b7e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6df7\u6dc6\u6982\u5ff5\u8bc6\u522b\u7b56\u7565\uff0c\u4ec5\u9700\u5c11\u91cf\u5e26\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u65b0\u6570\u636e\uff08\u5982\u6bcf\u7c7b4\u5f20\u56fe\u50cf\uff09\uff0c\u901a\u8fc7\u5c4f\u853d\u8bef\u6fc0\u6d3b\u7684\u6df7\u6dc6\u6982\u5ff5\u548c\u589e\u5f3a\u672a\u5145\u5206\u6fc0\u6d3b\u7684\u5224\u522b\u6027\u6982\u5ff5\uff0c\u63d0\u5347\u6a21\u578b\u5728\u65b0\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728\u76ae\u80a4\u548c\u767d\u7ec6\u80de\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6e90\u57df\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u89e3\u91ca\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u8de8\u57df\u6027\u80fd\u6539\u8fdb\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4e34\u5e8a\u573a\u666f\u3002", "paper_title_zh": "\u65e0\u9700\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6d4b\u8bd5\u65f6\u6539\u8fdb\u65b9\u6cd5", "abstract_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6280\u672f\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u8fc5\u901f\u53d1\u5c55\uff0c\u5f00\u53d1\u51c6\u786e\u4e14\u53ef\u4fe1\u8d56\u7684\u6a21\u578b\u4ee5\u9ad8\u6548\u90e8\u7f72\u4e8e\u591a\u6837\u5316\u4e34\u5e8a\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u901a\u8fc7\u5148\u9884\u6d4b\u4e00\u7ec4\u53ef\u89e3\u91ca\u6982\u5ff5\u518d\u8fdb\u884c\u5206\u7c7b\uff0c\u9010\u6e10\u88ab\u7528\u4e8e\u53ef\u89e3\u91ca\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u3002\u7136\u800c\uff0cCBMs\u7684\u53ef\u89e3\u91ca\u6027\u5728\u65b0\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u5e26\u6765\u4e86\u65b0\u6311\u6218\u3002\u6210\u50cf\u534f\u8bae\u548c\u67d3\u8272\u65b9\u6cd5\u7684\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u6982\u5ff5\u7ea7\u504f\u79fb\uff0c\u5982\u989c\u8272\u5206\u5e03\u548c\u5c3a\u5ea6\u7684\u53d8\u5316\u3002\u6b64\u5916\uff0c\u7531\u4e8eCBM\u8bad\u7ec3\u9700\u8981\u663e\u5f0f\u7684\u6982\u5ff5\u6807\u6ce8\uff0c\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u5fae\u8c03\u6a21\u578b\u53ef\u80fd\u635f\u5bb3\u6982\u5ff5\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5fe0\u5b9e\u6027\u2014\u2014\u8fd9\u5728\u533b\u5b66\u9886\u57df\u4e2d\u83b7\u53d6\u4e13\u5bb6\u6807\u6ce8\u6982\u5ff5\u6807\u7b7e\u7684\u9ad8\u6210\u672c\u4e0b\u5c24\u4e3a\u5173\u952e\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6df7\u6dc6\u6982\u5ff5\u8bc6\u522b\u7b56\u7565\u3002\u901a\u8fc7\u5229\u7528\u5c11\u91cf\u4ec5\u5e26\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u65b0\u6570\u636e\uff08\u5982\u6bcf\u7c7b4\u5f20\u56fe\u50cf\uff09\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5c4f\u853d\u8bef\u6fc0\u6d3b\u7684\u6df7\u6dc6\u6982\u5ff5\u548c\u589e\u5f3a\u672a\u5145\u5206\u6fc0\u6d3b\u7684\u5224\u522b\u6027\u6982\u5ff5\uff0c\u5728\u4e0d\u727a\u7272\u6e90\u57df\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u76ae\u80a4\u548c\u767d\u7ec6\u80de\u56fe\u50cf\u4e0a\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/riverback/TF-TTI-XMed\u3002"}}
{"id": "2506.18831", "pdf": "https://arxiv.org/pdf/2506.18831", "abs": "https://arxiv.org/abs/2506.18831", "authors": ["Aryasomayajula Ram Bharadwaj"], "title": "STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSTU-PID\u65b9\u6cd5\uff0c\u901a\u8fc7PID\u63a7\u5236\u5668\u52a8\u6001\u8c03\u8282\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728GSM8K\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u63d0\u53476%\uff0c\u540c\u65f6\u51cf\u5c1132%\u7684token\u4f7f\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u94fe\u63a8\u7406\uff08CoT\uff09\u4e2d\u5e38\u56e0\u8fc7\u5ea6\u601d\u8003\u751f\u6210\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u5e76\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002\u73b0\u6709\u9759\u6001\u5f15\u5bfc\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u8c03\u6574\u80fd\u529b\uff0c\u65e0\u6cd5\u6839\u636e\u5b9e\u65f6\u63a8\u7406\u8d28\u91cf\u7075\u6d3b\u8c03\u8282\u5e72\u9884\u5f3a\u5ea6\u3002", "method": "\u63d0\u51faSTU-PID\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u5206\u5757\u7684\u5206\u7c7b\u5668\u68c0\u6d4b\u5197\u4f59\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7PID\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u76f4\u63a5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u52a8\u6001\u6821\u51c6\u3002", "result": "\u5728GSM8K\u4efb\u52a1\u4e0a\uff0cSTU-PID\u5c06\u51c6\u786e\u7387\u63d0\u53476%\uff0c\u540c\u65f6\u51cf\u5c1132%\u7684token\u4f7f\u7528\uff0c\u4f18\u4e8e\u9759\u6001\u5f15\u5bfc\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "STU-PID\u4e3a\u52a8\u6001\u63a8\u7406\u6821\u51c6\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "paper_title_zh": "STU-PID\uff1a\u901a\u8fc7PID\u63a7\u5236\u5668\u52a8\u6001\u8c03\u8282token\u4f7f\u7528\u4ee5\u5b9e\u73b0\u9ad8\u6548\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406", "abstract_zh": "\u91c7\u7528\u957f\u94fe\u63a8\u7406\uff08CoT\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e38\u56e0\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\u751f\u6210\u8fc7\u591a\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u5e76\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u63a2\u7d22\u4e86\u9759\u6001\u5f15\u5bfc\u65b9\u6cd5\u4ee5\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u6839\u636e\u5b9e\u65f6\u63a8\u7406\u8d28\u91cf\u52a8\u6001\u8c03\u6574\u5e72\u9884\u5f3a\u5ea6\u7684\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51faSTU-PID\uff08\u901a\u8fc7PID\u63a7\u5236\u5668\u52a8\u6001\u8c03\u8282token\u4f7f\u7528\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528PID\u63a7\u5236\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u8282\u6fc0\u6d3b\u5f15\u5bfc\u5f3a\u5ea6\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u5206\u5757\u7ea7\u5206\u7c7b\u5668\u68c0\u6d4b\u5197\u4f59\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7PID\u63a7\u5236\u673a\u5236\u6839\u636e\u9884\u6d4b\u7684\u5197\u4f59\u6982\u7387\u81ea\u9002\u5e94\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u3002\u5728GSM8K\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTU-PID\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u53476%\uff0c\u540c\u65f6\u51cf\u5c1132%\u7684token\u4f7f\u7528\uff0c\u4f18\u4e8e\u9759\u6001\u5f15\u5bfc\u57fa\u7ebf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u52a8\u6001\u63a8\u7406\u6821\u51c6\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.18071", "pdf": "https://arxiv.org/pdf/2506.18071", "abs": "https://arxiv.org/abs/2506.18071", "authors": ["Jisheng Dang", "Huilin Song", "Junbin Xiao", "Bimei Wang", "Han Peng", "Haoxuan Li", "Xun Yang", "Meng Wang", "Tat-Seng Chua"], "title": "MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Grounded Video Question Answering (Grounded VideoQA) requires aligning\ntextual answers with explicit visual evidence. However, modern multimodal\nmodels often rely on linguistic priors and spurious correlations, resulting in\npoorly grounded predictions. In this work, we propose MUPA, a cooperative\nMUlti-Path Agentic approach that unifies video grounding, question answering,\nanswer reflection and aggregation to tackle Grounded VideoQA. MUPA features\nthree distinct reasoning paths on the interplay of grounding and QA agents in\ndifferent chronological orders, along with a dedicated reflection agent to\njudge and aggregate the multi-path results to accomplish consistent QA and\ngrounding. This design markedly improves grounding fidelity without sacrificing\nanswer accuracy. Despite using only 2B parameters, our method outperforms all\n7B-scale competitors. When scaled to 7B parameters, MUPA establishes new\nstate-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and\nDeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy\nvideo-language understanding. Our code is available in\nhttps://github.com/longmalongma/MUPA.", "AI": {"tldr": "MUPA\u662f\u4e00\u79cd\u591a\u8def\u5f84\u4ee3\u7406\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u89c6\u9891\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5b9a\u4f4d\u3001\u95ee\u7b54\u3001\u7b54\u6848\u53cd\u601d\u548c\u805a\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8bc1\u636e\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u9891\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u5f80\u5f80\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u548c\u865a\u5047\u5173\u8054\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u7f3a\u4e4f\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002MUPA\u65e8\u5728\u901a\u8fc7\u591a\u8def\u5f84\u4ee3\u7406\u63a8\u7406\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MUPA\u91c7\u7528\u591a\u8def\u5f84\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u5305\u62ec\u89c6\u9891\u5b9a\u4f4d\u4ee3\u7406\u3001\u95ee\u7b54\u4ee3\u7406\u3001\u7b54\u6848\u53cd\u601d\u4ee3\u7406\u548c\u7ed3\u679c\u805a\u5408\u4ee3\u7406\u3002\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u7684\u63a8\u7406\u8def\u5f84\u548c\u53cd\u601d\u4ee3\u7406\u7684\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u95ee\u7b54\u548c\u89c6\u89c9\u8bc1\u636e\u7684\u4e00\u81f4\u6027\u3002", "result": "MUPA\u5728\u4ec5\u4f7f\u75282B\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u6240\u67097B\u89c4\u6a21\u7684\u7ade\u4e89\u5bf9\u624b\u3002\u5f53\u6269\u5c55\u52307B\u53c2\u6570\u65f6\uff0cMUPA\u5728NExT-GQA\u548cDeVE-QA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230\u4e8630.3%\u548c47.4%\u7684Acc@GQA\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "MUPA\u901a\u8fc7\u591a\u8def\u5f84\u4ee3\u7406\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u89c6\u9891\u7684\u95ee\u7b54\u4efb\u52a1\u7684\u89c6\u89c9\u8bc1\u636e\u53ef\u9760\u6027\u548c\u7b54\u6848\u51c6\u786e\u6027\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u89c6\u9891-\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "MUPA\uff1a\u9762\u5411\u57fa\u4e8e\u89c6\u9891\u95ee\u7b54\u7684\u591a\u8def\u5f84\u4ee3\u7406\u63a8\u7406\u65b9\u6cd5", "abstract_zh": "\u57fa\u4e8e\u89c6\u9891\u7684\u95ee\u7b54\u4efb\u52a1\uff08Grounded VideoQA\uff09\u8981\u6c42\u5c06\u6587\u672c\u7b54\u6848\u4e0e\u660e\u786e\u7684\u89c6\u89c9\u8bc1\u636e\u5bf9\u9f50\u3002\u7136\u800c\uff0c\u73b0\u4ee3\u591a\u6a21\u6001\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u548c\u865a\u5047\u5173\u8054\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u7f3a\u4e4f\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002\u672c\u6587\u63d0\u51faMUPA\uff0c\u4e00\u79cd\u534f\u4f5c\u5f0f\u7684\u591a\u8def\u5f84\u4ee3\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u89c6\u9891\u5b9a\u4f4d\u3001\u95ee\u7b54\u3001\u7b54\u6848\u53cd\u601d\u548c\u805a\u5408\u6765\u89e3\u51b3Grounded VideoQA\u4efb\u52a1\u3002MUPA\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u63a8\u7406\u8def\u5f84\uff0c\u7ed3\u5408\u5b9a\u4f4d\u4ee3\u7406\u548c\u95ee\u7b54\u4ee3\u7406\u7684\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u53cd\u601d\u4ee3\u7406\u6765\u8bc4\u4f30\u548c\u805a\u5408\u591a\u8def\u5f84\u7ed3\u679c\uff0c\u4ece\u800c\u5b9e\u73b0\u4e00\u81f4\u7684\u95ee\u7b54\u548c\u89c6\u89c9\u8bc1\u636e\u5bf9\u9f50\u3002\u8fd9\u4e00\u8bbe\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bc1\u636e\u7684\u53ef\u9760\u6027\uff0c\u540c\u65f6\u672a\u727a\u7272\u7b54\u6848\u51c6\u786e\u6027\u3002\u5c3d\u7ba1\u4ec5\u4f7f\u75282B\u53c2\u6570\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u6240\u67097B\u89c4\u6a21\u7684\u7ade\u4e89\u5bf9\u624b\u3002\u5f53\u6269\u5c55\u52307B\u53c2\u6570\u65f6\uff0cMUPA\u5728NExT-GQA\u548cDeVE-QA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523030.3%\u548c47.4%\u7684Acc@GQA\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\uff0c\u8bc1\u660e\u4e86MUPA\u5728\u53ef\u4fe1\u8d56\u7684\u89c6\u9891-\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/longmalongma/MUPA\u3002"}}
{"id": "2506.17288", "pdf": "https://arxiv.org/pdf/2506.17288", "abs": "https://arxiv.org/abs/2506.17288", "authors": ["Jiale Zhang", "Jiaxiang Chen", "Zhucong Li", "Jie Ding", "Kui Zhao", "Zenglin Xu", "Xin Pang", "Yinghui Xu"], "title": "SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language models by\nincorporating external knowledge at inference time. However, graph-based RAG\nsystems often suffer from structural overhead and imprecise retrieval: they\nrequire costly pipelines for entity linking and relation extraction, yet\nfrequently return subgraphs filled with loosely related or tangential content.\nThis stems from a fundamental flaw -- semantic similarity does not imply\nsemantic relevance. We introduce SlimRAG, a lightweight framework for retrieval\nwithout graphs. SlimRAG replaces structure-heavy components with a simple yet\neffective entity-aware mechanism. At indexing time, it constructs a compact\nentity-to-chunk table based on semantic embeddings. At query time, it\nidentifies salient entities, retrieves and scores associated chunks, and\nassembles a concise, contextually relevant input -- without graph traversal or\nedge construction. To quantify retrieval efficiency, we propose Relative Index\nToken Utilization (RITU), a metric measuring the compactness of retrieved\ncontent. Experiments across multiple QA benchmarks show that SlimRAG\noutperforms strong flat and graph-based baselines in accuracy while reducing\nindex size and RITU (e.g., 16.31 vs. 56+), highlighting the value of\nstructure-free, entity-centric context selection. The code will be released\nsoon. https://github.com/continue-ai-company/SlimRAG", "AI": {"tldr": "SlimRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u673a\u5236\u53d6\u4ee3\u4f20\u7edf\u57fa\u4e8e\u56fe\u7684RAG\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7d22\u5f15\u5927\u5c0f\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u57fa\u4e8e\u56fe\u7684RAG\u7cfb\u7edf\u5b58\u5728\u7ed3\u6784\u590d\u6742\u6027\u548c\u68c0\u7d22\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u9700\u8981\u6602\u8d35\u7684\u5b9e\u4f53\u94fe\u63a5\u548c\u5173\u7cfb\u63d0\u53d6\u6d41\u7a0b\uff0c\u4e14\u5e38\u8fd4\u56de\u76f8\u5173\u6027\u4f4e\u7684\u5b50\u56fe\u3002SlimRAG\u65e8\u5728\u901a\u8fc7\u65e0\u56fe\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SlimRAG\u91c7\u7528\u5b9e\u4f53\u611f\u77e5\u673a\u5236\uff0c\u5728\u7d22\u5f15\u9636\u6bb5\u6784\u5efa\u7d27\u51d1\u7684\u5b9e\u4f53\u5230\u6587\u672c\u5757\u7684\u6620\u5c04\u8868\uff0c\u67e5\u8be2\u9636\u6bb5\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u5b9e\u4f53\u5e76\u68c0\u7d22\u76f8\u5173\u6587\u672c\u5757\uff0c\u907f\u514d\u4e86\u56fe\u904d\u5386\u548c\u8fb9\u6784\u5efa\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSlimRAG\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u56fe\u548c\u6241\u5e73\u5316\u68c0\u7d22\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u7d22\u5f15\u5927\u5c0f\u548cRelative Index Token Utilization\uff08RITU\uff09\u6307\u6807\uff08\u598216.31 vs. 56+\uff09\u3002", "conclusion": "SlimRAG\u8bc1\u660e\u4e86\u65e0\u56fe\u8bbe\u8ba1\u7684\u5b9e\u4f53\u4e2d\u5fc3\u5316\u4e0a\u4e0b\u6587\u9009\u62e9\u5728\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u4e3a\u8f7b\u91cf\u5316RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "SlimRAG\uff1a\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u4e0a\u4e0b\u6587\u9009\u62e9\u5b9e\u73b0\u65e0\u56fe\u68c0\u7d22", "abstract_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u56fe\u7684RAG\u7cfb\u7edf\u901a\u5e38\u9762\u4e34\u7ed3\u6784\u590d\u6742\u6027\u548c\u68c0\u7d22\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff1a\u5b83\u4eec\u9700\u8981\u6602\u8d35\u7684\u5b9e\u4f53\u94fe\u63a5\u548c\u5173\u7cfb\u63d0\u53d6\u6d41\u7a0b\uff0c\u5374\u5e38\u8fd4\u56de\u5305\u542b\u677e\u6563\u76f8\u5173\u5185\u5bb9\u7684\u5b50\u56fe\u3002\u8fd9\u6e90\u4e8e\u4e00\u4e2a\u6839\u672c\u7f3a\u9677\u2014\u2014\u8bed\u4e49\u76f8\u4f3c\u6027\u5e76\u4e0d\u7b49\u540c\u4e8e\u8bed\u4e49\u76f8\u5173\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86SlimRAG\uff0c\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u65e0\u56fe\u68c0\u7d22\u6846\u67b6\u3002SlimRAG\u7528\u7b80\u5355\u9ad8\u6548\u7684\u5b9e\u4f53\u611f\u77e5\u673a\u5236\u53d6\u4ee3\u4e86\u7ed3\u6784\u590d\u6742\u7684\u7ec4\u4ef6\u3002\u5728\u7d22\u5f15\u9636\u6bb5\uff0c\u5b83\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5165\u6784\u5efa\u7d27\u51d1\u7684\u5b9e\u4f53\u5230\u6587\u672c\u5757\u6620\u5c04\u8868\uff1b\u5728\u67e5\u8be2\u9636\u6bb5\uff0c\u5b83\u8bc6\u522b\u5173\u952e\u5b9e\u4f53\uff0c\u68c0\u7d22\u5e76\u8bc4\u5206\u76f8\u5173\u6587\u672c\u5757\uff0c\u6700\u7ec8\u751f\u6210\u7b80\u6d01\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f93\u5165\u2014\u2014\u65e0\u9700\u56fe\u904d\u5386\u6216\u8fb9\u6784\u5efa\u3002\u4e3a\u91cf\u5316\u68c0\u7d22\u6548\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Relative Index Token Utilization\uff08RITU\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u68c0\u7d22\u5185\u5bb9\u7684\u7d27\u51d1\u6027\u3002\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSlimRAG\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u56fe\u548c\u6241\u5e73\u5316\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u7d22\u5f15\u5927\u5c0f\u548cRITU\uff08\u4f8b\u598216.31 vs. 56+\uff09\uff0c\u51f8\u663e\u4e86\u65e0\u56fe\u8bbe\u8ba1\u3001\u5b9e\u4f53\u4e2d\u5fc3\u5316\u4e0a\u4e0b\u6587\u9009\u62e9\u7684\u4ef7\u503c\u3002\u4ee3\u7801\u5373\u5c06\u53d1\u5e03\u3002https://github.com/continue-ai-company/SlimRAG"}}
{"id": "2506.18841", "pdf": "https://arxiv.org/pdf/2506.18841", "abs": "https://arxiv.org/abs/2506.18841", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Roy Ka-Wei Lee", "Juanzi Li"], "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u65b9\u6cd5LongWriter-Zero\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\u7684\u957f\u5ea6\u9650\u5236\u548c\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5408\u6210\u6570\u636e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u751f\u6210\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u6d4b\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\u9762\u4e34\u957f\u5ea6\u9650\u5236\u548c\u8d28\u91cf\u4e0b\u964d\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u4f46\u6570\u636e\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u8d28\u91cf\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faLongWriter-Zero\u65b9\u6cd5\uff0c\u4ece\u96f6\u5f00\u59cb\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u5f15\u5bfc\u6a21\u578b\u5728\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u89c4\u5212\u548c\u4f18\u5316\uff0c\u4ece\u800c\u63d0\u5347\u957f\u5ea6\u63a7\u5236\u3001\u5199\u4f5c\u8d28\u91cf\u548c\u7ed3\u6784\u683c\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eQwen2.5-32B\u8bad\u7ec3\u7684LongWriter-Zero\u5728\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728WritingBench\u548cArena-Write\u8bc4\u6d4b\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u6c34\u5e73\uff0c\u751a\u81f3\u8d85\u8d8a100B+\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "LongWriter-Zero\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u63d0\u5347\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u548c\u6570\u636e\u3002", "paper_title_zh": "LongWriter-Zero\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u638c\u63e1\u8d85\u957f\u6587\u672c\u751f\u6210", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8d85\u957f\u6587\u672c\u751f\u6210\u662f\u4e00\u4e2a\u5e7f\u6cdb\u9700\u6c42\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6700\u5927\u751f\u6210\u957f\u5ea6\u548c\u968f\u7740\u5e8f\u5217\u589e\u957f\u7684\u8d28\u91cf\u4e0b\u964d\u3002\u4f20\u7edf\u65b9\u6cd5\u5982LongWriter\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e8e\u5408\u6210\u957f\u6587\u672c\u8f93\u51fa\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5408\u6210\u6570\u636e\uff0c\u6784\u5efa\u56f0\u96be\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5e38\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\uff0c\u7ed3\u6784\u5355\u8c03\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u52b1\u7684\u65b9\u6cd5\uff0c\u5b8c\u5168\u4ece\u96f6\u5f00\u59cb\uff0c\u4e0d\u4f9d\u8d56\u4efb\u4f55\u6807\u6ce8\u6216\u5408\u6210\u6570\u636e\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6fc0\u53d1LLMs\u7684\u8d85\u957f\u9ad8\u8d28\u91cf\u6587\u672c\u751f\u6210\u80fd\u529b\u3002\u6211\u4eec\u4ece\u57fa\u7840\u6a21\u578b\uff08\u7c7b\u4f3cR1-Zero\uff09\u51fa\u53d1\uff0c\u901a\u8fc7RL\u8bad\u7ec3\u5f15\u5bfc\u6a21\u578b\u5728\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u89c4\u5212\u548c\u4f18\u5316\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u91c7\u7528\u4e13\u95e8\u7684\u5956\u52b1\u6a21\u578b\uff0c\u6307\u5bfcLLM\u5728\u957f\u5ea6\u63a7\u5236\u3001\u5199\u4f5c\u8d28\u91cf\u548c\u7ed3\u6784\u683c\u5f0f\u4e0a\u7684\u6539\u8fdb\u3002\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u57fa\u4e8eQwen2.5-32B\u8bad\u7ec3\u7684LongWriter-Zero\u5728\u957f\u6587\u672c\u5199\u4f5c\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u4f20\u7edfSFT\u65b9\u6cd5\uff0c\u5728WritingBench\u548cArena-Write\u8bc4\u6d4b\u4e2d\u6240\u6709\u6307\u6807\u5747\u8fbe\u5230\u6700\u4f18\uff0c\u751a\u81f3\u8d85\u8d8a100B+\u89c4\u6a21\u7684\u6a21\u578b\u5982DeepSeek R1\u548cQwen3-235B\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u6570\u636e\u548c\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u8be6\u89c1https://huggingface.co/THU-KEG/LongWriter-Zero-32B\u3002"}}
{"id": "2506.18084", "pdf": "https://arxiv.org/pdf/2506.18084", "abs": "https://arxiv.org/abs/2506.18084", "authors": ["Wenzhuo Liu", "Yicheng Qiao", "Zhen Wang", "Qiannan Guo", "Zilong Chen", "Meihua Zhou", "Xinran Li", "Letian Wang", "Zhiwei Li", "Huaping Liu", "Wenshuo Wang"], "title": "TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving", "categories": ["cs.CV"], "comment": null, "summary": "Multi-task learning (MTL) can advance assistive driving by exploring\ninter-task correlations through shared representations. However, existing\nmethods face two critical limitations: single-modality constraints limiting\ncomprehensive scene understanding and inefficient architectures impeding\nreal-time deployment. This paper proposes TEM^3-Learning (Time-Efficient\nMultimodal Multi-task Learning), a novel framework that jointly optimizes\ndriver emotion recognition, driver behavior recognition, traffic context\nrecognition, and vehicle behavior recognition through a two-stage architecture.\nThe first component, the mamba-based multi-view temporal-spatial feature\nextraction subnetwork (MTS-Mamba), introduces a forward-backward temporal\nscanning mechanism and global-local spatial attention to efficiently extract\nlow-cost temporal-spatial features from multi-view sequential images. The\nsecond component, the MTL-based gated multimodal feature integrator (MGMI),\nemploys task-specific multi-gating modules to adaptively highlight the most\nrelevant modality features for each task, effectively alleviating the negative\ntransfer problem in MTL. Evaluation on the AIDE dataset, our proposed model\nachieves state-of-the-art accuracy across all four tasks, maintaining a\nlightweight architecture with fewer than 6 million parameters and delivering an\nimpressive 142.32 FPS inference speed. Rigorous ablation studies further\nvalidate the effectiveness of the proposed framework and the independent\ncontributions of each module. The code is available on\nhttps://github.com/Wenzhuo-Liu/TEM3-Learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEM^3-Learning\u7684\u9ad8\u6548\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u8054\u5408\u4f18\u5316\u9a7e\u9a76\u5458\u60c5\u7eea\u8bc6\u522b\u3001\u884c\u4e3a\u8bc6\u522b\u3001\u4ea4\u901a\u573a\u666f\u8bc6\u522b\u548c\u8f66\u8f86\u884c\u4e3a\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u548c\u9ad8\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5355\u6a21\u6001\u9650\u5236\u548c\u4f4e\u6548\u67b6\u6784\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8f85\u52a9\u9a7e\u9a76\u573a\u666f\u7684\u5168\u9762\u7406\u89e3\u548c\u5b9e\u65f6\u90e8\u7f72\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u67b6\u6784\uff1a1) MTS-Mamba\u5b50\u7f51\u7edc\uff0c\u901a\u8fc7\u524d\u5411-\u540e\u5411\u65f6\u95f4\u626b\u63cf\u548c\u5168\u5c40-\u5c40\u90e8\u7a7a\u95f4\u6ce8\u610f\u529b\u63d0\u53d6\u591a\u89c6\u89d2\u65f6\u7a7a\u7279\u5f81\uff1b2) MGMI\u6a21\u5757\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff0c\u7f13\u89e3\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "result": "\u5728AIDE\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4ee5\u5c11\u4e8e600\u4e07\u53c2\u6570\u5b9e\u73b0\u6240\u6709\u56db\u9879\u4efb\u52a1\u7684\u6700\u9ad8\u7cbe\u5ea6\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe142.32 FPS\u3002", "conclusion": "TEM^3-Learning\u901a\u8fc7\u9ad8\u6548\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u4efb\u52a1\u81ea\u9002\u5e94\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f85\u52a9\u9a7e\u9a76\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6027\u80fd\u548c\u5b9e\u65f6\u6027\u3002", "paper_title_zh": "TEM^3-Learning\uff1a\u9762\u5411\u9ad8\u7ea7\u8f85\u52a9\u9a7e\u9a76\u7684\u9ad8\u6548\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60", "abstract_zh": "\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u901a\u8fc7\u5171\u4eab\u8868\u5f81\u63a2\u7d22\u4efb\u52a1\u95f4\u76f8\u5173\u6027\uff0c\u53ef\u63a8\u52a8\u8f85\u52a9\u9a7e\u9a76\u53d1\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u5355\u6a21\u6001\u9650\u5236\u963b\u788d\u573a\u666f\u5168\u9762\u7406\u89e3\uff0c\u4f4e\u6548\u67b6\u6784\u5f71\u54cd\u5b9e\u65f6\u90e8\u7f72\u3002\u672c\u6587\u63d0\u51faTEM^3-Learning\uff08\u9ad8\u6548\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u8054\u5408\u4f18\u5316\u9a7e\u9a76\u5458\u60c5\u7eea\u8bc6\u522b\u3001\u884c\u4e3a\u8bc6\u522b\u3001\u4ea4\u901a\u573a\u666f\u8bc6\u522b\u53ca\u8f66\u8f86\u884c\u4e3a\u8bc6\u522b\u3002\u9996\u9636\u6bb5\u4e3a\u57fa\u4e8eMamba\u7684\u591a\u89c6\u89d2\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u5b50\u7f51\u7edc\uff08MTS-Mamba\uff09\uff0c\u5f15\u5165\u524d\u5411-\u540e\u5411\u65f6\u95f4\u626b\u63cf\u673a\u5236\u548c\u5168\u5c40-\u5c40\u90e8\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u9ad8\u6548\u63d0\u53d6\u591a\u89c6\u89d2\u5e8f\u5217\u56fe\u50cf\u7684\u4f4e\u6210\u672c\u65f6\u7a7a\u7279\u5f81\u3002\u6b21\u9636\u6bb5\u4e3a\u57fa\u4e8eMTL\u7684\u95e8\u63a7\u591a\u6a21\u6001\u7279\u5f81\u6574\u5408\u5668\uff08MGMI\uff09\uff0c\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u591a\u95e8\u63a7\u6a21\u5757\u81ea\u9002\u5e94\u7a81\u51fa\u5404\u4efb\u52a1\u6700\u76f8\u5173\u6a21\u6001\u7279\u5f81\uff0c\u6709\u6548\u7f13\u89e3MTL\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002\u5728AIDE\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u4ee5\u8f7b\u91cf\u7ea7\u67b6\u6784\uff08\u53c2\u6570\u5c11\u4e8e600\u4e07\uff09\u5b9e\u73b0\u56db\u9879\u4efb\u52a1\u7684\u6700\u9ad8\u7cbe\u5ea6\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe142.32 FPS\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6846\u67b6\u53ca\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/Wenzhuo-Liu/TEM3-Learning\u3002"}}
{"id": "2506.17292", "pdf": "https://arxiv.org/pdf/2506.17292", "abs": "https://arxiv.org/abs/2506.17292", "authors": ["Quan Nguyen", "Minh N. Vu", "Truc Nguyen", "My T. Thai"], "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Federated Learning enables collaborative learning among clients via a\ncoordinating server while avoiding direct data sharing, offering a perceived\nsolution to preserve privacy. However, recent studies on Membership Inference\nAttacks (MIAs) have challenged this notion, showing high success rates against\nunprotected training data. While local differential privacy (LDP) is widely\nregarded as a gold standard for privacy protection in data analysis, most\nstudies on MIAs either neglect LDP or fail to provide theoretical guarantees\nfor attack success rates against LDP-protected data. To address this gap, we\nderive theoretical lower bounds for the success rates of low-polynomial time\nMIAs that exploit vulnerabilities in fully connected or self-attention layers.\nWe establish that even when data are protected by LDP, privacy risks persist,\ndepending on the privacy budget. Practical evaluations on federated vision\nmodels confirm considerable privacy risks, revealing that the noise required to\nmitigate these attacks significantly degrades models' utility.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u901a\u8fc7\u534f\u8c03\u670d\u52a1\u5668\u5b9e\u73b0\u5ba2\u6237\u7aef\u534f\u4f5c\u5b66\u4e60\uff0c\u907f\u514d\u76f4\u63a5\u6570\u636e\u5171\u4eab\uff0c\u4f46\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u5bf9\u672a\u4fdd\u62a4\u6570\u636e\u7684\u9ad8\u6210\u529f\u7387\u6311\u6218\u4e86\u5176\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002\u672c\u5730\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u867d\u88ab\u89c6\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u591a\u6570\u7814\u7a76\u672a\u8003\u8651LDP\u6216\u672a\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002\u672c\u6587\u63a8\u5bfc\u4e86\u4f4e\u591a\u9879\u5f0f\u65f6\u95f4MIAs\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u63ed\u793aLDP\u4fdd\u62a4\u4e0b\u9690\u79c1\u98ce\u9669\u4ecd\u5b58\u5728\uff0c\u4e14\u566a\u58f0\u9700\u6c42\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u88ab\u8ba4\u4e3a\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u5bf9\u672a\u4fdd\u62a4\u6570\u636e\u7684\u9ad8\u6210\u529f\u7387\u5f15\u53d1\u62c5\u5fe7\u3002\u672c\u5730\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u867d\u4e3a\u9690\u79c1\u4fdd\u62a4\u6807\u51c6\uff0c\u4f46\u7f3a\u4e4f\u5bf9LDP\u4fdd\u62a4\u6570\u636e\u7684MIAs\u7406\u8bba\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8LDP\u4fdd\u62a4\u4e0b\u7684\u9690\u79c1\u98ce\u9669\u3002", "method": "\u672c\u6587\u63a8\u5bfc\u4e86\u4f4e\u591a\u9879\u5f0f\u65f6\u95f4\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u9488\u5bf9\u5168\u8fde\u63a5\u5c42\u6216\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u6f0f\u6d1e\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u91cf\u5316\u4e86LDP\u4fdd\u62a4\u4e0b\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u8bc4\u4f30\u4e86\u566a\u58f0\u5bf9\u6a21\u578b\u6548\u7528\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5373\u4f7f\u6570\u636e\u53d7LDP\u4fdd\u62a4\uff0c\u9690\u79c1\u98ce\u9669\u4ecd\u5b58\u5728\uff0c\u4e14\u653b\u51fb\u6210\u529f\u7387\u4e0e\u9690\u79c1\u9884\u7b97\u76f8\u5173\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8054\u90a6\u89c6\u89c9\u6a21\u578b\u4e2d\u663e\u8457\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u663e\u793a\u6240\u9700\u566a\u58f0\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6548\u7528\u3002", "conclusion": "LDP\u4fdd\u62a4\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u9690\u79c1\u98ce\u9669\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e0e\u9690\u79c1\u9884\u7b97\u76f8\u5173\u3002\u4e3a\u7f13\u89e3\u653b\u51fb\u6240\u9700\u7684\u566a\u58f0\u4f1a\u635f\u5bb3\u6a21\u578b\u6548\u7528\uff0c\u9700\u5728\u9690\u79c1\u4e0e\u6548\u7528\u95f4\u6743\u8861\u3002", "paper_title_zh": "\u7406\u8bba\u63ed\u793a\u8054\u90a6\u89c6\u89c9\u6a21\u578b\u4e2d\u9488\u5bf9LDP\u4fdd\u62a4\u5ba2\u6237\u7aef\u7684\u63a8\u7406\u653b\u51fb", "abstract_zh": "\u8054\u90a6\u5b66\u4e60\u901a\u8fc7\u534f\u8c03\u670d\u52a1\u5668\u5b9e\u73b0\u5ba2\u6237\u7aef\u534f\u4f5c\u5b66\u4e60\uff0c\u907f\u514d\u76f4\u63a5\u6570\u636e\u5171\u4eab\uff0c\u88ab\u89c6\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u6700\u8fd1\u5173\u4e8e\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u7684\u7814\u7a76\u6311\u6218\u4e86\u8fd9\u4e00\u89c2\u70b9\uff0c\u663e\u793a\u5bf9\u672a\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u7684\u9ad8\u6210\u529f\u7387\u3002\u5c3d\u7ba1\u672c\u5730\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u662f\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u591a\u6570\u5173\u4e8eMIAs\u7684\u7814\u7a76\u8981\u4e48\u5ffd\u7565LDP\uff0c\u8981\u4e48\u672a\u80fd\u63d0\u4f9b\u9488\u5bf9LDP\u4fdd\u62a4\u6570\u636e\u7684\u653b\u51fb\u6210\u529f\u7387\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63a8\u5bfc\u4e86\u4f4e\u591a\u9879\u5f0f\u65f6\u95f4MIAs\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u8fd9\u4e9b\u653b\u51fb\u5229\u7528\u4e86\u5168\u8fde\u63a5\u5c42\u6216\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u6f0f\u6d1e\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u5373\u4f7f\u6570\u636e\u53d7LDP\u4fdd\u62a4\uff0c\u9690\u79c1\u98ce\u9669\u4ecd\u5b58\u5728\uff0c\u4e14\u4e0e\u9690\u79c1\u9884\u7b97\u76f8\u5173\u3002\u8054\u90a6\u89c6\u89c9\u6a21\u578b\u7684\u5b9e\u8df5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u663e\u8457\u7684\u9690\u79c1\u98ce\u9669\uff0c\u63ed\u793a\u7f13\u89e3\u8fd9\u4e9b\u653b\u51fb\u6240\u9700\u7684\u566a\u58f0\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6548\u7528\u3002"}}
{"id": "2506.18852", "pdf": "https://arxiv.org/pdf/2506.18852", "abs": "https://arxiv.org/abs/2506.18852", "authors": ["Iwan Williams", "Ninell Oldenburg", "Ruchira Dhar", "Joshua Hatherley", "Constanza Fierro", "Nina Rajcic", "Sandrine R. Schiller", "Filippos Stamatiou", "Anders S\u00f8gaard"], "title": "Mechanistic Interpretability Needs Philosophy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mechanistic interpretability (MI) aims to explain how neural networks work by\nuncovering their underlying causal mechanisms. As the field grows in influence,\nit is increasingly important to examine not just models themselves, but the\nassumptions, concepts and explanatory strategies implicit in MI research. We\nargue that mechanistic interpretability needs philosophy: not as an\nafterthought, but as an ongoing partner in clarifying its concepts, refining\nits methods, and assessing the epistemic and ethical stakes of interpreting AI\nsystems. Taking three open problems from the MI literature as examples, this\nposition paper illustrates the value philosophy can add to MI research, and\noutlines a path toward deeper interdisciplinary dialogue.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u9700\u8981\u54f2\u5b66\u7684\u6df1\u5ea6\u53c2\u4e0e\uff0c\u4ee5\u6f84\u6e05\u6982\u5ff5\u3001\u4f18\u5316\u65b9\u6cd5\u5e76\u8bc4\u4f30\u89e3\u91caAI\u7cfb\u7edf\u7684\u8ba4\u8bc6\u8bba\u548c\u4f26\u7406\u610f\u4e49\u3002", "motivation": "\u968f\u7740\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff08MI\uff09\u7814\u7a76\u7684\u5f71\u54cd\u529b\u6269\u5927\uff0c\u4ec5\u5173\u6ce8\u6a21\u578b\u672c\u8eab\u5df2\u4e0d\u8db3\u591f\uff0c\u8fd8\u9700\u5ba1\u89c6\u5176\u9690\u542b\u7684\u5047\u8bbe\u3001\u6982\u5ff5\u548c\u89e3\u91ca\u7b56\u7565\u3002\u672c\u6587\u8ba4\u4e3a\u54f2\u5b66\u5e94\u6210\u4e3aMI\u7814\u7a76\u7684\u6301\u7eed\u5408\u4f5c\u4f19\u4f34\u3002", "method": "\u901a\u8fc7\u5206\u6790MI\u6587\u732e\u4e2d\u7684\u4e09\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u672c\u6587\u5c55\u793a\u4e86\u54f2\u5b66\u5bf9MI\u7814\u7a76\u7684\u4ef7\u503c\uff0c\u5e76\u63d0\u51fa\u4e86\u6df1\u5316\u8de8\u5b66\u79d1\u5bf9\u8bdd\u7684\u8def\u5f84\u3002", "result": "\u54f2\u5b66\u80fd\u591f\u5e2e\u52a9MI\u7814\u7a76\u6f84\u6e05\u6982\u5ff5\u3001\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u89e3\u91caAI\u7cfb\u7edf\u7684\u8ba4\u8bc6\u8bba\u548c\u4f26\u7406\u610f\u4e49\u3002", "conclusion": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u9700\u8981\u4e0e\u54f2\u5b66\u7d27\u5bc6\u7ed3\u5408\uff0c\u4ee5\u63a8\u52a8\u66f4\u6df1\u5165\u7684\u6982\u5ff5\u548c\u65b9\u6cd5\u8bba\u53d1\u5c55\u3002", "paper_title_zh": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u9700\u8981\u54f2\u5b66", "abstract_zh": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff08MI\uff09\u65e8\u5728\u901a\u8fc7\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u56e0\u679c\u673a\u5236\u6765\u89e3\u91ca\u5176\u5de5\u4f5c\u539f\u7406\u3002\u968f\u7740\u8be5\u9886\u57df\u5f71\u54cd\u529b\u7684\u589e\u957f\uff0c\u4e0d\u4ec5\u9700\u8981\u5173\u6ce8\u6a21\u578b\u672c\u8eab\uff0c\u8fd8\u9700\u5ba1\u89c6MI\u7814\u7a76\u4e2d\u9690\u542b\u7684\u5047\u8bbe\u3001\u6982\u5ff5\u548c\u89e3\u91ca\u7b56\u7565\u3002\u6211\u4eec\u8ba4\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u9700\u8981\u54f2\u5b66\uff1a\u4e0d\u662f\u4f5c\u4e3a\u4e8b\u540e\u8865\u5145\uff0c\u800c\u662f\u4f5c\u4e3a\u6301\u7eed\u5408\u4f5c\u4f19\u4f34\uff0c\u4ee5\u6f84\u6e05\u5176\u6982\u5ff5\u3001\u4f18\u5316\u65b9\u6cd5\u5e76\u8bc4\u4f30\u89e3\u91caAI\u7cfb\u7edf\u7684\u8ba4\u8bc6\u8bba\u548c\u4f26\u7406\u610f\u4e49\u3002\u672c\u6587\u4ee5MI\u6587\u732e\u4e2d\u7684\u4e09\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u54f2\u5b66\u5bf9MI\u7814\u7a76\u7684\u4ef7\u503c\uff0c\u5e76\u63d0\u51fa\u4e86\u6df1\u5316\u8de8\u5b66\u79d1\u5bf9\u8bdd\u7684\u8def\u5f84\u3002"}}
{"id": "2506.18095", "pdf": "https://arxiv.org/pdf/2506.18095", "abs": "https://arxiv.org/abs/2506.18095", "authors": ["Junying Chen", "Zhenyang Cai", "Pengcheng Chen", "Shunian Chen", "Ke Ji", "Xidong Wang", "Yunjin Yang", "Benyou Wang"], "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ShareGPT-4o-Image\u6570\u636e\u96c6\u548cJanus-4o\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u548c\u6a21\u578b\u63a8\u52a8\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7684\u7814\u7a76\uff0c\u5b9e\u73b0\u4e0eGPT-4o\u76f8\u5f53\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4o-Image\uff09\u591a\u4e3a\u4e13\u6709\u4e14\u4e0d\u5f00\u653e\uff0c\u9650\u5236\u4e86\u7814\u7a76\u7684\u53d1\u5c55\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7684\u5f00\u653e\u7814\u7a76\u3002", "method": "\u4f7f\u7528GPT-4o\u751f\u621045K\u6587\u672c\u5230\u56fe\u50cf\u548c46K\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u6570\u636e\u96c6ShareGPT-4o-Image\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3Janus-4o\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u751f\u6210\u3002", "result": "Janus-4o\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u4ee3Janus-Pro\uff0c\u5e76\u9996\u6b21\u652f\u6301\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u751f\u6210\uff0c\u4ec5\u752891K\u5408\u6210\u6837\u672c\u548c6\u5c0f\u65f6\u8bad\u7ec3\u5373\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ShareGPT-4o-Image\u548cJanus-4o\u7684\u5f00\u6e90\u5c06\u4fc3\u8fdb\u771f\u5b9e\u611f\u3001\u6307\u4ee4\u5bf9\u9f50\u7684\u56fe\u50cf\u751f\u6210\u7814\u7a76\u3002", "paper_title_zh": "ShareGPT-4o-Image\uff1a\u5c06\u591a\u6a21\u6001\u6a21\u578b\u4e0eGPT-4o\u7ea7\u56fe\u50cf\u751f\u6210\u5bf9\u9f50", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u7684\u8fdb\u5c55\u5b9e\u73b0\u4e86\u771f\u5b9e\u611f\u3001\u6307\u4ee4\u5bf9\u9f50\u7684\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u9886\u5148\u7cfb\u7edf\u5982GPT-4o-Image\u4ecd\u4e3a\u4e13\u6709\u4e14\u4e0d\u53ef\u8bbf\u95ee\u3002\u4e3a\u666e\u53ca\u8fd9\u4e9b\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51faShareGPT-4o-Image\uff0c\u9996\u4e2a\u5305\u542b45K\u6587\u672c\u5230\u56fe\u50cf\u548c46K\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5747\u901a\u8fc7GPT-4o\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u5408\u6210\uff0c\u4ee5\u63d0\u53d6\u5176\u5148\u8fdb\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002\u5229\u7528\u6b64\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86Janus-4o\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u751f\u6210\u3002Janus-4o\u4e0d\u4ec5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u4ee3Janus-Pro\uff0c\u8fd8\u9996\u6b21\u652f\u6301\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u751f\u6210\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u4ec5\u752891K\u5408\u6210\u6837\u672c\u548c6\u5c0f\u65f6\u8bad\u7ec3\uff08\u57fa\u4e8e8\u53f0A800-GPU\u673a\u5668\uff09\u5373\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002\u6211\u4eec\u5e0c\u671bShareGPT-4o-Image\u548cJanus-4o\u7684\u53d1\u5e03\u80fd\u63a8\u52a8\u771f\u5b9e\u611f\u3001\u6307\u4ee4\u5bf9\u9f50\u56fe\u50cf\u751f\u6210\u7684\u5f00\u653e\u7814\u7a76\u3002"}}
{"id": "2506.18879", "pdf": "https://arxiv.org/pdf/2506.18879", "abs": "https://arxiv.org/abs/2506.18879", "authors": ["Junyan Li", "Yang Zhang", "Muhammad Yusuf Hassan", "Talha Chafekar", "Tianle Cai", "Zhile Ren", "Pengsheng Guo", "Foroozan Karimzadeh", "Colorado Reed", "Chong Wang", "Chuang Gan"], "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression", "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025 poster", "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCommVQ\u7684\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u952e\u503c\uff08KV\uff09\u7f13\u5b58\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u901a\u8fc7\u8bbe\u8ba1\u53ef\u4ea4\u6362\u7684\u7801\u672c\u548c\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5728GPU\u4e0a\u7684\u5185\u5b58\u5360\u7528\u6210\u4e3a\u74f6\u9888\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ea4\u6362\u5411\u91cf\u91cf\u5316\uff08CommVQ\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548c\u7801\u672c\u5bf9KV\u7f13\u5b58\u8fdb\u884c\u538b\u7f29\uff0c\u89e3\u7801\u8fc7\u7a0b\u4ec5\u9700\u7b80\u5355\u77e9\u9635\u4e58\u6cd5\u3002\u6b64\u5916\uff0c\u7801\u672c\u8bbe\u8ba1\u4e3a\u4e0e\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u53ef\u4ea4\u6362\uff0c\u5e76\u901a\u8fc7\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7b97\u6cd5\u8bad\u7ec3\uff0c\u4ee5\u964d\u4f4e\u89e3\u7801\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u548cGSM8K\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c06FP16 KV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c1187.5%\uff082\u4f4d\u91cf\u5316\uff09\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709KV\u7f13\u5b58\u91cf\u5316\u65b9\u6cd5\u30021\u4f4d\u91cf\u5316\u65f6\uff0c\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\uff0c\u4f7fLLaMA-3.1 8B\u6a21\u578b\u80fd\u5728\u5355\u5757RTX 4090 GPU\u4e0a\u652f\u6301128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "CommVQ\u901a\u8fc7\u53ef\u4ea4\u6362\u5411\u91cf\u91cf\u5316\u663e\u8457\u51cf\u5c11\u4e86KV\u7f13\u5b58\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "CommVQ\uff1a\u7528\u4e8eKV\u7f13\u5b58\u538b\u7f29\u7684\u53ef\u4ea4\u6362\u5411\u91cf\u91cf\u5316", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u7684\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u968f\u7740\u4e0a\u4e0b\u6587\u589e\u957f\uff0c\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5728GPU\u4e0a\u7684\u5185\u5b58\u5360\u7528\u6210\u4e3a\u74f6\u9888\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u53ef\u4ea4\u6362\u5411\u91cf\u91cf\u5316\uff08CommVQ\uff09\uff0c\u663e\u8457\u51cf\u5c11\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u4f7f\u7528\u3002\u6211\u4eec\u9996\u5148\u5f15\u5165\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548c\u7801\u672c\u7684\u52a0\u6cd5\u91cf\u5316\u6765\u538b\u7f29KV\u7f13\u5b58\uff0c\u89e3\u7801\u4ec5\u9700\u7b80\u5355\u77e9\u9635\u4e58\u6cd5\u3002\u4e3a\u964d\u4f4e\u89e3\u7801\u8ba1\u7b97\u6210\u672c\uff0c\u6211\u4eec\u8bbe\u8ba1\u7801\u672c\u4e0e\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u53ef\u4ea4\u6362\uff0c\u5e76\u901a\u8fc7\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7b97\u6cd5\u8bad\u7ec3\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u5c06\u89e3\u7801\u96c6\u6210\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u52a0\u6cd5\u91cf\u5316\u548cRoPE\u53ef\u4ea4\u6362\u7801\u672c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5f00\u9500\u3002\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u548cGSM8K\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57282\u4f4d\u91cf\u5316\u4e0b\u5c06FP16 KV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c1187.5%\uff0c\u4f18\u4e8e\u73b0\u6709KV\u7f13\u5b58\u91cf\u5316\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c1\u4f4d\u91cf\u5316\u65f6\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\uff0c\u4f7fLLaMA-3.1 8B\u6a21\u578b\u80fd\u5728\u5355\u5757RTX 4090 GPU\u4e0a\u652f\u6301128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002\u6e90\u4ee3\u7801\u89c1\uff1ahttps://github.com/UMass-Embodied-AGI/CommVQ\u3002"}}
{"id": "2506.18104", "pdf": "https://arxiv.org/pdf/2506.18104", "abs": "https://arxiv.org/abs/2506.18104", "authors": ["Idan Simai", "Ronen Talmon", "Uri Shaham"], "title": "Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we argue that viewing VICReg-a popular self-supervised\nlearning (SSL) method--through the lens of spectral embedding reveals a\npotential source of sub-optimality: it may struggle to generalize robustly to\nunseen data due to overreliance on the training data. This observation invites\na closer look at how well this method achieves its goal of producing meaningful\nrepresentations of images outside of the training set as well. Here, we\ninvestigate this issue and introduce SAG-VICReg (Stable and Generalizable\nVICReg), a method that builds on VICReg by incorporating new training\ntechniques. These enhancements improve the model's ability to capture global\nsemantics within the data and strengthen the generalization capabilities.\nExperiments demonstrate that SAG-VICReg effectively addresses the\ngeneralization challenge while matching or surpassing diverse state-of-the-art\nSSL baselines. Notably, our method exhibits superior performance on metrics\ndesigned to evaluate global semantic understanding, while simultaneously\nmaintaining competitive results on local evaluation metrics. Furthermore, we\npropose a new standalone evaluation metric for embeddings that complements the\nstandard evaluation methods and accounts for the global data structure without\nrequiring labels--a key issue when tagged data is scarce or not available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAG-VICReg\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbVICReg\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5168\u5c40\u8bed\u4e49\u6355\u6349\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u672a\u89c1\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5168\u5c40\u8bed\u4e49\u7406\u89e3\u548c\u5c40\u90e8\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0VICReg\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u672a\u89c1\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdbVICReg\uff0c\u63d0\u5347\u5176\u5168\u5c40\u8bed\u4e49\u6355\u6349\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51faSAG-VICReg\u65b9\u6cd5\uff0c\u7ed3\u5408\u65b0\u7684\u8bad\u7ec3\u6280\u672f\uff0c\u589e\u5f3aVICReg\u7684\u5168\u5c40\u8bed\u4e49\u6355\u6349\u80fd\u529b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u7684\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u5d4c\u5165\u7684\u5168\u5c40\u6570\u636e\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSAG-VICReg\u5728\u5168\u5c40\u8bed\u4e49\u7406\u89e3\u548c\u5c40\u90e8\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u65b0\u8bc4\u4f30\u6307\u6807\u6709\u6548\u8865\u5145\u4e86\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "SAG-VICReg\u663e\u8457\u63d0\u5347\u4e86VICReg\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5168\u5c40\u8bed\u4e49\u6355\u6349\u80fd\u529b\uff0c\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u589e\u5f3aVICReg\uff1a\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u914d\u5bf9\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u4e0e\u5168\u5c40\u8bed\u4e49\u6355\u6349", "abstract_zh": "\u672c\u6587\u8ba4\u4e3a\uff0c\u4ece\u8c31\u5d4c\u5165\u7684\u89d2\u5ea6\u5206\u6790VICReg\uff08\u4e00\u79cd\u6d41\u884c\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff09\u63ed\u793a\u4e86\u5176\u6f5c\u5728\u4e0d\u8db3\uff1a\u7531\u4e8e\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u53ef\u80fd\u5728\u672a\u89c1\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u8fd9\u5f15\u53d1\u4e86\u5bf9\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u96c6\u5916\u6570\u636e\u4e0a\u751f\u6210\u6709\u610f\u4e49\u8868\u793a\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86SAG-VICReg\uff08\u7a33\u5b9a\u4e14\u53ef\u6cdb\u5316\u7684VICReg\uff09\uff0c\u8be5\u65b9\u6cd5\u5728VICReg\u57fa\u7840\u4e0a\u7ed3\u5408\u4e86\u65b0\u7684\u8bad\u7ec3\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6355\u6349\u6570\u636e\u5168\u5c40\u8bed\u4e49\u7684\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSAG-VICReg\u6709\u6548\u89e3\u51b3\u4e86\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u591a\u79cd\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u5168\u5c40\u8bed\u4e49\u7406\u89e3\u7684\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u5c40\u90e8\u8bc4\u4f30\u6307\u6807\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u72ec\u7acb\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u5d4c\u5165\u7684\u5168\u5c40\u6570\u636e\u7ed3\u6784\uff0c\u65e0\u9700\u6807\u7b7e\u5373\u53ef\u8865\u5145\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\u2014\u2014\u8fd9\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u6216\u4e0d\u53ef\u7528\u65f6\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2506.18880", "pdf": "https://arxiv.org/pdf/2506.18880", "abs": "https://arxiv.org/abs/2506.18880", "authors": ["Yiyou Sun", "Shawn Hu", "Georgia Zhou", "Ken Zheng", "Hannaneh Hajishirzi", "Nouha Dziri", "Dawn Song"], "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OMEGA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u4e09\u79cd\u6cdb\u5316\u80fd\u529b\uff1a\u63a2\u7d22\u6027\u3001\u7ec4\u5408\u6027\u548c\u8f6c\u5316\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u524d\u6cbfLLMs\u5728\u590d\u6742\u95ee\u9898\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u800c\u5fae\u8c03\u6a21\u578b\u5728\u63a2\u7d22\u6027\u6cdb\u5316\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u7ec4\u5408\u6027\u548c\u8f6c\u5316\u6027\u6cdb\u5316\u4ecd\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u65b0\u9896\u601d\u7ef4\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u63d0\u51faOMEGA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5728\u63a2\u7d22\u6027\u3001\u7ec4\u5408\u6027\u548c\u8f6c\u5316\u6027\u6cdb\u5316\u4e0a\u7684\u80fd\u529b\u3002", "method": "OMEGA\u57fa\u51c6\u6d4b\u8bd5\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u8bad\u7ec3-\u6d4b\u8bd5\u5bf9\uff0c\u6db5\u76d6\u51e0\u4f55\u3001\u6570\u8bba\u3001\u4ee3\u6570\u7b49\u591a\u4e2a\u6570\u5b66\u9886\u57df\uff0c\u5e76\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u3002\u4f5c\u8005\u8bc4\u4f30\u4e86\u524d\u6cbfLLMs\u7684\u8868\u73b0\uff0c\u5e76\u5bf9Qwen\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "result": "\u524d\u6cbfLLMs\u5728\u590d\u6742\u95ee\u9898\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002\u5fae\u8c03\u540e\u7684Qwen\u6a21\u578b\u5728\u63a2\u7d22\u6027\u6cdb\u5316\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u7ec4\u5408\u6027\u548c\u8f6c\u5316\u6027\u6cdb\u5316\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "OMEGA\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u63d0\u5347LLMs\u5728\u6570\u5b66\u521b\u9020\u529b\u65b9\u9762\u7684\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7ec4\u5408\u6027\u548c\u8f6c\u5316\u6027\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "paper_title_zh": "OMEGA\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u5728\u6570\u5b66\u4e2d\u8df3\u51fa\u601d\u7ef4\u5b9a\u5f0f\uff1f\u63a2\u7d22\u6027\u3001\u7ec4\u5408\u6027\u4e0e\u8f6c\u5316\u6027\u6cdb\u5316\u7684\u8bc4\u4f30", "abstract_zh": "\u8fd1\u671f\u5177\u5907\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982DeepSeek-R1\uff09\u5728\u5965\u6797\u5339\u514b\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u7ee9\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u6709\u9650\u7684\u7b56\u7565\uff0c\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u65b0\u9896\u601d\u7ef4\u65b9\u5f0f\u7684\u95ee\u9898\u3002\u4e3a\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OMEGA\uff08\u5206\u5e03\u5916\u6570\u5b66\u95ee\u9898\u8bc4\u4f30\u4e0e\u4e09\u79cd\u6cdb\u5316\u8f74\uff09\u2014\u2014\u4e00\u4e2a\u53d7Boden\u521b\u9020\u529b\u5206\u7c7b\u542f\u53d1\u7684\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u4e09\u79cd\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff1a\uff081\uff09\u63a2\u7d22\u6027\uff1a\u5c06\u5df2\u77e5\u95ee\u9898\u89e3\u51b3\u6280\u80fd\u5e94\u7528\u4e8e\u540c\u4e00\u9886\u57df\u5185\u66f4\u590d\u6742\u7684\u5b9e\u4f8b\uff1b\uff082\uff09\u7ec4\u5408\u6027\uff1a\u5c06\u5b64\u7acb\u5b66\u4e60\u7684\u63a8\u7406\u6280\u80fd\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u9700\u8981\u65b0\u65b9\u5f0f\u6574\u5408\u8fd9\u4e9b\u6280\u80fd\u7684\u95ee\u9898\uff1b\uff083\uff09\u8f6c\u5316\u6027\uff1a\u8d85\u8d8a\u719f\u6089\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u9896\u751a\u81f3\u975e\u5e38\u89c4\u7684\u7b56\u7565\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u95ee\u9898\u3002OMEGA\u5305\u542b\u4ece\u51e0\u4f55\u3001\u6570\u8bba\u3001\u4ee3\u6570\u3001\u7ec4\u5408\u6570\u5b66\u3001\u903b\u8f91\u548c\u8c1c\u9898\u7b49\u9886\u57df\u6a21\u677f\u5316\u95ee\u9898\u751f\u6210\u5668\u751f\u6210\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u5bf9\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u3001\u6570\u503c\u6216\u56fe\u5f62\u65b9\u6cd5\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u524d\u6cbfLLMs\uff0c\u53d1\u73b0\u968f\u7740\u95ee\u9898\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u5176\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9Qwen\u7cfb\u5217\u6a21\u578b\u5728\u6240\u6709\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u89c2\u5bdf\u5230\u63a2\u7d22\u6027\u6cdb\u5316\u7684\u663e\u8457\u63d0\u5347\uff0c\u800c\u7ec4\u5408\u6027\u6cdb\u5316\u4ecd\u6709\u9650\uff0c\u8f6c\u5316\u6027\u63a8\u7406\u51e0\u4e4e\u6ca1\u6709\u6539\u8fdb\u3002\u901a\u8fc7\u5206\u79bb\u548c\u91cf\u5316\u8fd9\u4e9b\u7ec6\u7c92\u5ea6\u5931\u8d25\u6848\u4f8b\uff0cOMEGA\u4e3a\u63a8\u8fdbLLMs\u8d85\u8d8a\u673a\u68b0\u719f\u7ec3\u5ea6\u3001\u5b9e\u73b0\u771f\u6b63\u7684\u6570\u5b66\u521b\u9020\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.18134", "pdf": "https://arxiv.org/pdf/2506.18134", "abs": "https://arxiv.org/abs/2506.18134", "authors": ["Quan Zhou", "Gan Luo", "Qiang Hu", "Qingyong Zhang", "Jinhua Zhang", "Yinjiao Tian", "Qiang Li", "Zhiwei Wang"], "title": "Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025", "summary": "Polyp detection is crucial for colorectal cancer screening, yet existing\nmodels are limited by the scale and diversity of available data. While\ngenerative models show promise for data augmentation, current methods mainly\nfocus on enhancing polyp diversity, often overlooking the critical issue of\nfalse positives. In this paper, we address this gap by proposing an adversarial\ndiffusion framework to synthesize high-value false positives. The extensive\nvariability of negative backgrounds presents a significant challenge in false\npositive synthesis. To overcome this, we introduce two key innovations: First,\nwe design a regional noise matching strategy to construct a negative synthesis\nspace using polyp detection datasets. This strategy trains a negative-centric\ndiffusion model by masking polyp regions, ensuring the model focuses\nexclusively on learning diverse background patterns. Second, we introduce the\nDetector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs\nthe negative synthesis process to disrupt a pre-trained detector's decision,\nguiding the negative-centric diffusion model to generate high-value,\ndetector-confusing false positives instead of low-value, ordinary backgrounds.\nOur approach is the first to apply adversarial diffusion to lesion detection,\nestablishing a new paradigm for targeted false positive synthesis and paving\nthe way for more reliable clinical applications in colorectal cancer screening.\nExtensive results on public and in-house datasets verify the superiority of our\nmethod over the current state-of-the-arts, with our synthesized data improving\nthe detectors by at least 2.6% and 2.7% in F1-score, respectively, over the\nbaselines. Codes are at https://github.com/Huster-Hq/DADA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u6269\u6563\u6846\u67b6\uff08DADA\uff09\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u4ef7\u503c\u7684\u5047\u9633\u6027\u6837\u672c\uff0c\u4ee5\u589e\u5f3a\u7ed3\u80a0\u606f\u8089\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u533a\u57df\u566a\u58f0\u5339\u914d\u7b56\u7565\u548c\u68c0\u6d4b\u5668\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u6269\u6563\u653b\u51fb\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7ed3\u80a0\u606f\u8089\u68c0\u6d4b\u6a21\u578b\u53d7\u9650\u4e8e\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u5c24\u5176\u662f\u5047\u9633\u6027\u6837\u672c\u7684\u7f3a\u4e4f\u3002\u751f\u6210\u6a21\u578b\u867d\u7136\u53ef\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u606f\u8089\u591a\u6837\u6027\uff0c\u5ffd\u7565\u4e86\u5047\u9633\u6027\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u4ef7\u503c\u7684\u5047\u9633\u6027\u6837\u672c\u63d0\u5347\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "method": "1. \u8bbe\u8ba1\u533a\u57df\u566a\u58f0\u5339\u914d\u7b56\u7565\uff0c\u5229\u7528\u606f\u8089\u68c0\u6d4b\u6570\u636e\u96c6\u6784\u5efa\u8d1f\u6837\u672c\u5408\u6210\u7a7a\u95f4\uff0c\u8bad\u7ec3\u4e13\u6ce8\u4e8e\u80cc\u666f\u6a21\u5f0f\u7684\u6269\u6563\u6a21\u578b\u30022. \u63d0\u51fa\u68c0\u6d4b\u5668\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u6269\u6563\u653b\u51fb\u6a21\u5757\uff08DADA\uff09\uff0c\u901a\u8fc7\u5e72\u6270\u5408\u6210\u8fc7\u7a0b\u751f\u6210\u9ad8\u4ef7\u503c\u7684\u5047\u9633\u6027\u6837\u672c\u3002", "result": "\u5728\u516c\u5f00\u548c\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5408\u6210\u7684\u6570\u636e\u4f7f\u68c0\u6d4b\u5668\u7684F1\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e86\u81f3\u5c112.6%\u548c2.7%\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5c06\u5bf9\u6297\u6027\u6269\u6563\u5e94\u7528\u4e8e\u75c5\u7076\u68c0\u6d4b\uff0c\u4e3a\u9776\u5411\u5047\u9633\u6027\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u671b\u63d0\u5347\u7ed3\u80a0\u764c\u7b5b\u67e5\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u68c0\u6d4b\u5668\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u6269\u6563\u653b\u51fb\u5668\u7684\u9776\u5411\u5047\u9633\u6027\u5408\u6210\u7528\u4e8e\u9c81\u68d2\u6027\u606f\u8089\u68c0\u6d4b", "abstract_zh": "\u7ed3\u80a0\u606f\u8089\u68c0\u6d4b\u5bf9\u7ed3\u76f4\u80a0\u764c\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u53d7\u9650\u4e8e\u6570\u636e\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u589e\u5f3a\u606f\u8089\u7684\u591a\u6837\u6027\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u5047\u9633\u6027\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u4ef7\u503c\u7684\u5047\u9633\u6027\u6837\u672c\u3002\u8d1f\u6837\u672c\u80cc\u666f\u7684\u5e7f\u6cdb\u53d8\u5f02\u6027\u4e3a\u5047\u9633\u6027\u5408\u6210\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u9879\u521b\u65b0\uff1a\u9996\u5148\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u533a\u57df\u566a\u58f0\u5339\u914d\u7b56\u7565\uff0c\u5229\u7528\u606f\u8089\u68c0\u6d4b\u6570\u636e\u96c6\u6784\u5efa\u8d1f\u6837\u672c\u5408\u6210\u7a7a\u95f4\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u63a9\u819c\u606f\u8089\u533a\u57df\u8bad\u7ec3\u8d1f\u6837\u672c\u4e2d\u5fc3\u7684\u6269\u6563\u6a21\u578b\uff0c\u786e\u4fdd\u6a21\u578b\u4e13\u6ce8\u4e8e\u5b66\u4e60\u591a\u6837\u5316\u7684\u80cc\u666f\u6a21\u5f0f\u3002\u5176\u6b21\uff0c\u5f15\u5165\u4e86\u68c0\u6d4b\u5668\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u6269\u6563\u653b\u51fb\u6a21\u5757\uff08DADA\uff09\uff0c\u901a\u8fc7\u5e72\u6270\u8d1f\u6837\u672c\u5408\u6210\u8fc7\u7a0b\u7834\u574f\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u7684\u51b3\u7b56\uff0c\u5f15\u5bfc\u8d1f\u6837\u672c\u4e2d\u5fc3\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4ef7\u503c\u7684\u3001\u80fd\u6df7\u6dc6\u68c0\u6d4b\u5668\u7684\u5047\u9633\u6027\u6837\u672c\uff0c\u800c\u975e\u4f4e\u4ef7\u503c\u7684\u666e\u901a\u80cc\u666f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u9996\u6b21\u5c06\u5bf9\u6297\u6027\u6269\u6563\u5e94\u7528\u4e8e\u75c5\u7076\u68c0\u6d4b\uff0c\u4e3a\u9776\u5411\u5047\u9633\u6027\u5408\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4e3a\u7ed3\u76f4\u80a0\u764c\u7b5b\u67e5\u7684\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002\u5728\u516c\u5f00\u548c\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5408\u6210\u7684\u6570\u636e\u5206\u522b\u4f7f\u68c0\u6d4b\u5668\u7684F1\u5206\u6570\u63d0\u9ad8\u4e86\u81f3\u5c112.6%\u548c2.7%\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/Huster-Hq/DADA\u3002"}}
{"id": "2506.17297", "pdf": "https://arxiv.org/pdf/2506.17297", "abs": "https://arxiv.org/abs/2506.17297", "authors": ["Satyam Mishra", "Phung Thao Vi", "Shivam Mishra", "Vishwanath Bijalwan", "Vijay Bhaskar Semwal", "Abdul Manan Khan"], "title": "SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.8"], "comment": "10 pages, 7 figures, open-source library, PyPI installable: pip\n  install saferl-lite", "summary": "We introduce SafeRL-Lite, an open-source Python library for building\nreinforcement learning (RL) agents that are both constrained and explainable.\nExisting RL toolkits often lack native mechanisms for enforcing hard safety\nconstraints or producing human-interpretable rationales for decisions.\nSafeRL-Lite provides modular wrappers around standard Gym environments and deep\nQ-learning agents to enable: (i) safety-aware training via constraint\nenforcement, and (ii) real-time post-hoc explanation via SHAP values and\nsaliency maps. The library is lightweight, extensible, and installable via pip,\nand includes built-in metrics for constraint violations. We demonstrate its\neffectiveness on constrained variants of CartPole and provide visualizations\nthat reveal both policy logic and safety adherence. The full codebase is\navailable at: https://github.com/satyamcser/saferl-lite.", "AI": {"tldr": "SafeRL-Lite\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90Python\u5e93\uff0c\u4e13\u6ce8\u4e8e\u6784\u5efa\u5177\u6709\u7ea6\u675f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u5b89\u5168\u7ea6\u675f\u548c\u51b3\u7b56\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u901a\u5e38\u7f3a\u4e4f\u539f\u751f\u673a\u5236\u6765\u5f3a\u5236\u6267\u884c\u786c\u6027\u5b89\u5168\u7ea6\u675f\u6216\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u51b3\u7b56\u7406\u7531\uff0cSafeRL-Lite\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SafeRL-Lite\u901a\u8fc7\u6a21\u5757\u5316\u5c01\u88c5\u6807\u51c6Gym\u73af\u5883\u548c\u6df1\u5ea6Q\u5b66\u4e60\u4ee3\u7406\uff0c\u652f\u6301\u5b89\u5168\u611f\u77e5\u8bad\u7ec3\uff08\u901a\u8fc7\u7ea6\u675f\u5f3a\u5236\u6267\u884c\uff09\u548c\u5b9e\u65f6\u4e8b\u540e\u89e3\u91ca\uff08\u901a\u8fc7SHAP\u503c\u548c\u663e\u8457\u6027\u56fe\uff09\u3002", "result": "\u5728CartPole\u7684\u7ea6\u675f\u53d8\u4f53\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89c6\u5316\u5de5\u5177\u4ee5\u5c55\u793a\u7b56\u7565\u903b\u8f91\u548c\u5b89\u5168\u6027\u3002", "conclusion": "SafeRL-Lite\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u5b89\u88c5\u7684\u5e93\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "SafeRL-Lite\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u7ea6\u675f\u7684\u5f3a\u5316\u5b66\u4e60\u5e93", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86SafeRL-Lite\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5e93\uff0c\u7528\u4e8e\u6784\u5efa\u5177\u6709\u7ea6\u675f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u3002\u73b0\u6709\u7684RL\u5de5\u5177\u5305\u901a\u5e38\u7f3a\u4e4f\u539f\u751f\u673a\u5236\u6765\u5f3a\u5236\u6267\u884c\u786c\u6027\u5b89\u5168\u7ea6\u675f\u6216\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u51b3\u7b56\u7406\u7531\u3002SafeRL-Lite\u901a\u8fc7\u6a21\u5757\u5316\u5c01\u88c5\u6807\u51c6Gym\u73af\u5883\u548c\u6df1\u5ea6Q\u5b66\u4e60\u4ee3\u7406\uff0c\u652f\u6301\uff1a\uff08i\uff09\u901a\u8fc7\u7ea6\u675f\u5f3a\u5236\u6267\u884c\u5b9e\u73b0\u5b89\u5168\u611f\u77e5\u8bad\u7ec3\uff0c\uff08ii\uff09\u901a\u8fc7SHAP\u503c\u548c\u663e\u8457\u6027\u56fe\u5b9e\u73b0\u5b9e\u65f6\u4e8b\u540e\u89e3\u91ca\u3002\u8be5\u5e93\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\uff0c\u53ef\u901a\u8fc7pip\u5b89\u88c5\uff0c\u5e76\u5185\u7f6e\u4e86\u7ea6\u675f\u8fdd\u53cd\u7684\u5ea6\u91cf\u6307\u6807\u3002\u6211\u4eec\u5728CartPole\u7684\u7ea6\u675f\u53d8\u4f53\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89c6\u5316\u5de5\u5177\u4ee5\u5c55\u793a\u7b56\u7565\u903b\u8f91\u548c\u5b89\u5168\u6027\u3002\u5b8c\u6574\u4ee3\u7801\u5e93\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://github.com/satyamcser/saferl-lite\u3002"}}
{"id": "2506.18896", "pdf": "https://arxiv.org/pdf/2506.18896", "abs": "https://arxiv.org/abs/2506.18896", "authors": ["Jiaru Zou", "Ling Yang", "Jingwen Gu", "Jiahao Qiu", "Ke Shen", "Jingrui He", "Mengdi Wang"], "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs", "categories": ["cs.CL"], "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux", "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux", "AI": {"tldr": "ReasonFlux-PRM\u662f\u4e00\u79cd\u65b0\u578b\u8f68\u8ff9\u611f\u77e5PRM\uff0c\u4e13\u4e3a\u8bc4\u4f30\u8f68\u8ff9\u54cd\u5e94\u578b\u63a8\u7406\u8f68\u8ff9\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6b65\u9aa4\u7ea7\u548c\u8f68\u8ff9\u7ea7\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709PRM\u4e3b\u8981\u57fa\u4e8e\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\u8bad\u7ec3\uff0c\u96be\u4ee5\u7a33\u5065\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff0c\u5c24\u5176\u662f\u5728\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u8f68\u8ff9\u54cd\u5e94\u8f93\u51fa\u4e2d\u3002", "method": "ReasonFlux-PRM\u5f15\u5165\u6b65\u9aa4\u7ea7\u548c\u8f68\u8ff9\u7ea7\u76d1\u7763\uff0c\u652f\u6301\u79bb\u7ebf\u548c\u5728\u7ebf\u5956\u52b1\u76d1\u7763\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u9009\u62e9\u3001\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u548c\u6d4b\u8bd5\u65f6\u5956\u52b1\u5f15\u5bfc\u3002", "result": "\u5728AIME\u3001MATH500\u548cGPQA-Diamond\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReasonFlux-PRM-7B\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6PRM\u548c\u4eba\u5de5\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u534712.1%\uff08\u76d1\u7763\u5fae\u8c03\uff09\u30014.5%\uff08\u5f3a\u5316\u5b66\u4e60\uff09\u548c6.3%\uff08\u6d4b\u8bd5\u65f6\u6269\u5c55\uff09\u3002", "conclusion": "ReasonFlux-PRM\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u8f7b\u91cf\u7ea7\u7248\u672cReasonFlux-PRM-1.5B\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "paper_title_zh": "ReasonFlux-PRM\uff1a\u9762\u5411\u957f\u94fe\u63a8\u7406\u7684\u8f68\u8ff9\u611f\u77e5PRM", "abstract_zh": "\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u662f\u76d1\u7763\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u5f3a\u5927\u6846\u67b6\u3002\u73b0\u6709PRM\u4e3b\u8981\u57fa\u4e8e\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\u8bad\u7ec3\uff0c\u96be\u4ee5\u7a33\u5065\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff0c\u5c24\u5176\u662f\u5728\u524d\u6cbf\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u751f\u6210\u7684\u8f68\u8ff9\u54cd\u5e94\u8f93\u51fa\u4e2d\u3002\u672c\u6587\u63d0\u51faReasonFlux-PRM\uff0c\u4e00\u79cd\u65b0\u578b\u8f68\u8ff9\u611f\u77e5PRM\uff0c\u4e13\u4e3a\u8bc4\u4f30\u8f68\u8ff9\u54cd\u5e94\u578b\u63a8\u7406\u8f68\u8ff9\u8bbe\u8ba1\u3002ReasonFlux-PRM\u7ed3\u5408\u6b65\u9aa4\u7ea7\u548c\u8f68\u8ff9\u7ea7\u76d1\u7763\uff0c\u5b9e\u73b0\u4e0e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u6570\u636e\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u5956\u52b1\u5206\u914d\u3002\u6211\u4eec\u652f\u6301\u79bb\u7ebf\u548c\u5728\u7ebf\u5956\u52b1\u76d1\u7763\uff0c\u5305\u62ec\uff1a\uff08i\uff09\u4e3a\u4e0b\u6e38\u76d1\u7763\u5fae\u8c03\u9009\u62e9\u9ad8\u8d28\u91cf\u84b8\u998f\u6570\u636e\uff0c\uff08ii\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u63d0\u4f9b\u5bc6\u96c6\u8fc7\u7a0b\u7ea7\u5956\u52b1\uff0c\uff08iii\uff09\u652f\u6301\u5956\u52b1\u5f15\u5bfc\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002\u5728AIME\u3001MATH500\u548cGPQA-Diamond\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReasonFlux-PRM-7B\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6PRM\uff08\u5982Qwen2.5-Math-PRM-72B\uff09\u548c\u4eba\u5de5\u57fa\u7ebf\u3002\u6b64\u5916\uff0cReasonFlux-PRM-7B\u5728\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u5206\u522b\u5b9e\u73b012.1%\u30014.5%\u548c6.3%\u7684\u5e73\u5747\u63d0\u5347\u3002\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u8f7b\u91cf\u7ea7ReasonFlux-PRM-1.5B\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u548c\u8fb9\u7f18\u90e8\u7f72\u3002\u9879\u76ee\u5730\u5740\uff1ahttps://github.com/Gen-Verse/ReasonFlux"}}
{"id": "2506.18140", "pdf": "https://arxiv.org/pdf/2506.18140", "abs": "https://arxiv.org/abs/2506.18140", "authors": ["Ruinan Jin", "Gexin Huang", "Xinwei Shen", "Qiong Zhang", "Yan Shuo Tan", "Xiaoxiao Li"], "title": "See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis", "categories": ["cs.CV"], "comment": "25 pages, four figures", "summary": "Medical imaging diagnosis presents inherent challenges due to diseases that\nmimic normal anatomy and exhibit significant inter-patient variability.\nClinicians routinely employ comparative reasoning-using reference images from\nhealthy controls or previous patient examinations-to discern subtle yet\ndiagnostically critical abnormalities. However, existing medical\nvision-language models (VLMs) focus primarily on single-image or single-series\nanalyses and lack explicit mechanisms for comparative reasoning. Conversely,\ngeneral-purpose VLMs demonstrate strong multi-image comparative reasoning\ncapabilities but lack essential medical-domain knowledge to identify nuanced\nclinical differences. This work aims to bridge this gap by exploring\nclinically-inspired comparative analysis within VLMs, leveraging reference\nimages to enhance diagnostic accuracy. Through extensive empirical analysis, we\nshow that providing general-purpose VLMs with query and normative matched\nreference images, accompanied by clinically-informed comparative prompts,\nsignificantly improves diagnostic outcomes compared to single-image baselines,\nespecially after supervised finetuning (SFT). Our contributions highlight the\nclinical relevance of comparative analysis introduce novel strategies for\nleveraging reference images in VLMs, empirically demonstrate enhanced\nperformance across multiple medical visual question answering (VQA) tasks, and\nprovide theoretical insights into the efficacy of comparative image analysis in\nmedical diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08See-in-Pairs\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u5065\u5eb7\u5bf9\u7167\u6216\u5386\u53f2\u68c0\u67e5\u56fe\u50cf\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u4e2d\u5b58\u5728\u75be\u75c5\u4e0e\u6b63\u5e38\u89e3\u5256\u7ed3\u6784\u76f8\u4f3c\u53ca\u60a3\u8005\u95f4\u5dee\u5f02\u5927\u7684\u95ee\u9898\uff0c\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5bf9\u6bd4\u5206\u6790\u80fd\u529b\uff0c\u800c\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5907\u591a\u56fe\u50cf\u5bf9\u6bd4\u80fd\u529b\u4f46\u7f3a\u4e4f\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u5347\u8bca\u65ad\u6548\u679c\u3002", "method": "\u901a\u8fc7\u4e3a\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u67e5\u8be2\u56fe\u50cf\u548c\u5339\u914d\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u4e34\u5e8a\u542f\u53d1\u7684\u5bf9\u6bd4\u63d0\u793a\uff0c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4ee5\u589e\u5f3a\u5176\u5bf9\u6bd4\u5206\u6790\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u56fe\u50cf\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u76d1\u7763\u5fae\u8c03\u540e\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5bf9\u6bd4\u5206\u6790\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u4e34\u5e8a\u4ef7\u503c\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u53c2\u8003\u56fe\u50cf\u7684\u65b0\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "paper_title_zh": "See-in-Pairs\uff1a\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u6a21\u578b", "abstract_zh": "\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u56e0\u75be\u75c5\u4e0e\u6b63\u5e38\u89e3\u5256\u7ed3\u6784\u76f8\u4f3c\u53ca\u60a3\u8005\u95f4\u5dee\u5f02\u5927\u800c\u5177\u6709\u56fa\u6709\u6311\u6218\u3002\u4e34\u5e8a\u533b\u751f\u901a\u5e38\u901a\u8fc7\u5bf9\u6bd4\u5065\u5eb7\u5bf9\u7167\u6216\u5386\u53f2\u68c0\u67e5\u56fe\u50cf\u6765\u8bc6\u522b\u7ec6\u5fae\u4f46\u5173\u952e\u7684\u5f02\u5e38\u3002\u7136\u800c\uff0c\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3b\u8981\u5173\u6ce8\u5355\u56fe\u50cf\u6216\u5355\u5e8f\u5217\u5206\u6790\uff0c\u7f3a\u4e4f\u663e\u5f0f\u5bf9\u6bd4\u673a\u5236\u3002\u800c\u901a\u7528VLMs\u867d\u5177\u5907\u591a\u56fe\u50cf\u5bf9\u6bd4\u80fd\u529b\uff0c\u5374\u7f3a\u4e4f\u8bc6\u522b\u4e34\u5e8a\u7ec6\u5fae\u5dee\u5f02\u7684\u533b\u5b66\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63a2\u7d22\u4e34\u5e8a\u542f\u53d1\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u5229\u7528\u53c2\u8003\u56fe\u50cf\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3a\u901a\u7528VLMs\u63d0\u4f9b\u67e5\u8be2\u56fe\u50cf\u548c\u5339\u914d\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u4e34\u5e8a\u5bf9\u6bd4\u63d0\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u56fe\u50cf\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u540e\u3002\u7814\u7a76\u8d21\u732e\u5305\u62ec\uff1a\u5f3a\u8c03\u5bf9\u6bd4\u5206\u6790\u7684\u4e34\u5e8a\u4ef7\u503c\uff0c\u63d0\u51fa\u5229\u7528\u53c2\u8003\u56fe\u50cf\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e3a\u533b\u5b66\u8bca\u65ad\u4e2d\u5bf9\u6bd4\u56fe\u50cf\u5206\u6790\u7684\u6709\u6548\u6027\u63d0\u4f9b\u7406\u8bba\u89c1\u89e3\u3002"}}
{"id": "2506.18157", "pdf": "https://arxiv.org/pdf/2506.18157", "abs": "https://arxiv.org/abs/2506.18157", "authors": ["Christian Sax", "Jochen Kriegseis"], "title": "Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry", "categories": ["cs.CV", "physics.app-ph", "physics.flu-dyn"], "comment": null, "summary": "This work investigates the feasibility of a post-processing-based approach\nfor phase separation in defocusing particle tracking velocimetry for dispersed\ntwo-phase flows. The method enables the simultaneous 3D localization\ndetermination of both tracer particles and particles of the dispersed phase,\nusing a single-camera setup. The distinction between phases is based on pattern\ndifferences in defocused particle images, which arise from distinct light\nscattering behaviors of tracer particles and bubbles or droplets. Convolutional\nneural networks, including Faster R-CNN and YOLOv4 variants, are trained to\ndetect and classify particle images based on these pattern features. To\ngenerate large, labeled training datasets, a generative adversarial network\nbased framework is introduced, allowing the generation of auto-labeled data\nthat more closely reflects experiment-specific visual appearance. Evaluation\nacross six datasets, comprising synthetic two-phase and real single- and\ntwo-phase flows, demonstrates high detection precision and classification\naccuracy (95-100%), even under domain shifts. The results confirm the viability\nof using CNNs for robust phase separation in disperse two-phase DPTV,\nparticularly in scenarios where traditional wavelength-, size-, or ensemble\ncorrelation-based methods are impractical.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u5904\u7406\u7684\u76f8\u5206\u79bb\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6563\u4e24\u76f8\u6d41\u4e2d\u7684\u79bb\u7126\u7c92\u5b50\u8ffd\u8e2a\u6d4b\u901f\u6280\u672f\uff0c\u901a\u8fc7\u5355\u76f8\u673a\u5b9e\u73b0\u793a\u8e2a\u7c92\u5b50\u548c\u5206\u6563\u76f8\u7c92\u5b50\u76843D\u5b9a\u4f4d\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u7c92\u5b50\u56fe\u50cf\uff0c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u8f85\u52a9\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5206\u6563\u4e24\u76f8\u6d41\u4e2d\u96be\u4ee5\u5b9e\u73b0\u793a\u8e2a\u7c92\u5b50\u548c\u5206\u6563\u76f8\u7c92\u5b50\u7684\u533a\u5206\uff0c\u5c24\u5176\u662f\u5728\u6ce2\u957f\u3001\u5c3a\u5bf8\u6216\u76f8\u5173\u6027\u65b9\u6cd5\u4e0d\u9002\u7528\u7684\u60c5\u51b5\u4e0b\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u6a21\u5f0f\u5dee\u5f02\u7684\u76f8\u5206\u79bb\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u79bb\u7126\u7c92\u5b50\u56fe\u50cf\u7684\u6563\u5c04\u6a21\u5f0f\u5dee\u5f02\uff0c\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u5982Faster R-CNN\u548cYOLOv4\uff09\u8fdb\u884c\u7c92\u5b50\u68c0\u6d4b\u548c\u5206\u7c7b\u3002\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u5b9e\u9a8c\u7279\u5b9a\u5916\u89c2\u7684\u81ea\u52a8\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u5408\u6210\u548c\u771f\u5b9e\u4e24\u76f8\u6d41\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5206\u7c7b\u51c6\u786e\u7387\uff0895-100%\uff09\uff0c\u5373\u4f7f\u5728\u9886\u57df\u504f\u79fb\u4e0b\u4e5f\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u5b9e\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u6563\u4e24\u76f8\u79bb\u7126\u7c92\u5b50\u8ffd\u8e2a\u6d4b\u901f\u4e2d\u5b9e\u73b0\u7a33\u5065\u76f8\u5206\u79bb\u7684\u53ef\u884c\u6027\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u65b9\u6cd5\u4e0d\u9002\u7528\u65f6\u5177\u6709\u4f18\u52bf\u3002", "paper_title_zh": "\u57fa\u4e8e\u6a21\u5f0f\u7684\u793a\u8e2a\u4e0e\u5206\u6563\u76f8\u7c92\u5b50\u76f8\u5206\u79bb\u5728\u4e24\u76f8\u79bb\u7126\u7c92\u5b50\u8ffd\u8e2a\u6d4b\u901f\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u5904\u7406\u7684\u76f8\u5206\u79bb\u65b9\u6cd5\u5728\u5206\u6563\u4e24\u76f8\u6d41\u79bb\u7126\u7c92\u5b50\u8ffd\u8e2a\u6d4b\u901f\u4e2d\u7684\u53ef\u884c\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5355\u76f8\u673a\u5b9e\u73b0\u793a\u8e2a\u7c92\u5b50\u548c\u5206\u6563\u76f8\u7c92\u5b50\u76843D\u5b9a\u4f4d\uff0c\u5229\u7528\u79bb\u7126\u7c92\u5b50\u56fe\u50cf\u7684\u6563\u5c04\u6a21\u5f0f\u5dee\u5f02\u533a\u5206\u4e24\u76f8\u3002\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u5982Faster R-CNN\u548cYOLOv4\uff09\u68c0\u6d4b\u548c\u5206\u7c7b\u7c92\u5b50\u56fe\u50cf\u3002\u4e3a\u751f\u6210\u5927\u89c4\u6a21\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u5165\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u751f\u6210\u66f4\u63a5\u8fd1\u5b9e\u9a8c\u5916\u89c2\u7684\u81ea\u52a8\u6807\u8bb0\u6570\u636e\u3002\u5728\u516d\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u5408\u6210\u4e24\u76f8\u6d41\u548c\u771f\u5b9e\u5355\u76f8\u53ca\u4e24\u76f8\u6d41\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5206\u7c7b\u51c6\u786e\u7387\uff0895-100%\uff09\uff0c\u5373\u4f7f\u5728\u9886\u57df\u504f\u79fb\u4e0b\u4e5f\u8868\u73b0\u7a33\u5065\u3002\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u6563\u4e24\u76f8\u79bb\u7126\u7c92\u5b50\u8ffd\u8e2a\u6d4b\u901f\u4e2d\u5b9e\u73b0\u7a33\u5065\u76f8\u5206\u79bb\u7684\u53ef\u884c\u6027\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u6ce2\u957f\u3001\u5c3a\u5bf8\u6216\u76f8\u5173\u6027\u65b9\u6cd5\u4e0d\u9002\u7528\u65f6\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2506.17299", "pdf": "https://arxiv.org/pdf/2506.17299", "abs": "https://arxiv.org/abs/2506.17299", "authors": ["Shuyi Lin", "Anshuman Suri", "Alina Oprea", "Cheng Tan"], "title": "LLM Jailbreak Oracle", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) become increasingly deployed in\nsafety-critical applications, the lack of systematic methods to assess their\nvulnerability to jailbreak attacks presents a critical security gap. We\nintroduce the jailbreak oracle problem: given a model, prompt, and decoding\nstrategy, determine whether a jailbreak response can be generated with\nlikelihood exceeding a specified threshold. This formalization enables a\nprincipled study of jailbreak vulnerabilities. Answering the jailbreak oracle\nproblem poses significant computational challenges -- the search space grows\nexponentially with the length of the response tokens. We present Boa, the first\nefficient algorithm for solving the jailbreak oracle problem. Boa employs a\nthree-phase search strategy: (1) constructing block lists to identify refusal\npatterns, (2) breadth-first sampling to identify easily accessible jailbreaks,\nand (3) depth-first priority search guided by fine-grained safety scores to\nsystematically explore promising low-probability paths. Boa enables rigorous\nsecurity assessments including systematic defense evaluation, standardized\ncomparison of red team attacks, and model certification under extreme\nadversarial conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBoa\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8d8a\u72f1\u9884\u8a00\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u641c\u7d22\u7b56\u7565\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u65b9\u6cd5\u8bc4\u4f30\u5176\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u5b89\u5168\u7f3a\u53e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f62\u5f0f\u5316\u8d8a\u72f1\u9884\u8a00\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u6a21\u578b\u6f0f\u6d1e\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "Boa\u7b97\u6cd5\u91c7\u7528\u4e09\u9636\u6bb5\u641c\u7d22\u7b56\u7565\uff1a(1)\u6784\u5efa\u5757\u5217\u8868\u8bc6\u522b\u62d2\u7edd\u6a21\u5f0f\uff0c(2)\u5e7f\u5ea6\u4f18\u5148\u91c7\u6837\u5bfb\u627e\u6613\u8bbf\u95ee\u7684\u8d8a\u72f1\u8def\u5f84\uff0c(3)\u6df1\u5ea6\u4f18\u5148\u4f18\u5148\u7ea7\u641c\u7d22\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u5b89\u5168\u8bc4\u5206\u63a2\u7d22\u4f4e\u6982\u7387\u8def\u5f84\u3002", "result": "Boa\u80fd\u591f\u9ad8\u6548\u89e3\u51b3\u8d8a\u72f1\u9884\u8a00\u95ee\u9898\uff0c\u652f\u6301\u7cfb\u7edf\u5316\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u5305\u62ec\u9632\u5fa1\u6d4b\u8bd5\u3001\u6807\u51c6\u5316\u7ea2\u961f\u653b\u51fb\u6bd4\u8f83\u53ca\u6781\u7aef\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u8ba4\u8bc1\u3002", "conclusion": "Boa\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u7cfb\u7edf\u5316\u5b89\u5168\u7814\u7a76\u7684\u7a7a\u767d\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u9884\u8a00", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u65b9\u6cd5\u8bc4\u4f30\u5176\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u5b89\u5168\u7f3a\u53e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u8d8a\u72f1\u9884\u8a00\u95ee\u9898\uff1a\u7ed9\u5b9a\u6a21\u578b\u3001\u63d0\u793a\u548c\u89e3\u7801\u7b56\u7565\uff0c\u5224\u65ad\u662f\u5426\u80fd\u591f\u751f\u6210\u6982\u7387\u8d85\u8fc7\u6307\u5b9a\u9608\u503c\u7684\u8d8a\u72f1\u54cd\u5e94\u3002\u8fd9\u4e00\u5f62\u5f0f\u5316\u4e3a\u7814\u7a76\u8d8a\u72f1\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002\u89e3\u51b3\u8d8a\u72f1\u9884\u8a00\u95ee\u9898\u9762\u4e34\u663e\u8457\u7684\u8ba1\u7b97\u6311\u6218\u2014\u2014\u641c\u7d22\u7a7a\u95f4\u968f\u54cd\u5e94\u4ee4\u724c\u957f\u5ea6\u5448\u6307\u6570\u589e\u957f\u3002\u6211\u4eec\u63d0\u51fa\u4e86Boa\uff0c\u9996\u4e2a\u9ad8\u6548\u89e3\u51b3\u8d8a\u72f1\u9884\u8a00\u95ee\u9898\u7684\u7b97\u6cd5\u3002Boa\u91c7\u7528\u4e09\u9636\u6bb5\u641c\u7d22\u7b56\u7565\uff1a(1)\u6784\u5efa\u5757\u5217\u8868\u8bc6\u522b\u62d2\u7edd\u6a21\u5f0f\uff0c(2)\u5e7f\u5ea6\u4f18\u5148\u91c7\u6837\u5bfb\u627e\u6613\u8bbf\u95ee\u7684\u8d8a\u72f1\u8def\u5f84\uff0c(3)\u6df1\u5ea6\u4f18\u5148\u4f18\u5148\u7ea7\u641c\u7d22\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u5b89\u5168\u8bc4\u5206\u63a2\u7d22\u4f4e\u6982\u7387\u8def\u5f84\u3002Boa\u652f\u6301\u4e25\u683c\u7684\u5b89\u8bc4\u8bc4\u4f30\uff0c\u5305\u62ec\u7cfb\u7edf\u9632\u5fa1\u6d4b\u8bd5\u3001\u6807\u51c6\u5316\u7ea2\u961f\u653b\u51fb\u6bd4\u8f83\u53ca\u6781\u7aef\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u8ba4\u8bc1\u3002"}}
{"id": "2506.17310", "pdf": "https://arxiv.org/pdf/2506.17310", "abs": "https://arxiv.org/abs/2506.17310", "authors": ["Kangcong Li", "Peng Ye", "Chongjun Tu", "Lin Zhang", "Chunfeng Song", "Jiamin Wu", "Tao Yang", "Qihao Zheng", "Tao Chen"], "title": "PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding", "categories": ["q-bio.NC", "cs.CL", "cs.NE"], "comment": null, "summary": "While Large Language Models (LLMs) demonstrate strong performance across\ndomains, their long-context capabilities are limited by transient neural\nactivations causing information decay and unstructured feed-forward network\n(FFN) weights leading to semantic fragmentation. Inspired by the brain's\nworking memory and cortical modularity, we propose PaceLLM, featuring two\ninnovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal\ncortex (PFC) neurons' persistent firing by introducing an activation-level\nmemory bank to dynamically retrieve, reuse, and update critical FFN states,\naddressing contextual decay; and (2) Cortical Expert (CE) Clustering that\nemulates task-adaptive neural specialization to reorganize FFN weights into\nsemantic modules, establishing cross-token dependencies and mitigating\nfragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement\non LongBench's Multi-document QA and 12.5-17.5% performance gains on\nInfinite-Bench tasks, while extending measurable context length to 200K tokens\nin Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM\noptimization and is complementary to other works. Besides, it can be\ngeneralized to any model and enhance their long-context performance and\ninterpretability without structural overhauls.", "AI": {"tldr": "PaceLLM\u662f\u4e00\u79cd\u53d7\u5927\u8111\u542f\u53d1\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6301\u4e45\u6d3b\u52a8\u673a\u5236\u548c\u76ae\u5c42\u4e13\u5bb6\u805a\u7c7b\u89e3\u51b3\u4e86\u4fe1\u606f\u8870\u51cf\u548c\u8bed\u4e49\u788e\u7247\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u56e0\u795e\u7ecf\u6fc0\u6d3b\u77ed\u6682\u548c\u6743\u91cd\u7ed3\u6784\u677e\u6563\u5bfc\u81f4\u4fe1\u606f\u8870\u51cf\u548c\u8bed\u4e49\u788e\u7247\u5316\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002\u53d7\u5927\u8111\u5de5\u4f5c\u8bb0\u5fc6\u548c\u76ae\u5c42\u6a21\u5757\u5316\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u4f18\u5316\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u3002", "method": "PaceLLM\u5f15\u5165\u4e24\u79cd\u521b\u65b0\u673a\u5236\uff1a(1) \u6301\u4e45\u6d3b\u52a8\u673a\u5236\uff0c\u6a21\u62df\u524d\u989d\u53f6\u795e\u7ecf\u5143\u7684\u6301\u7eed\u653e\u7535\uff0c\u52a8\u6001\u7ba1\u7406\u5173\u952e\u7f51\u7edc\u72b6\u6001\uff1b(2) \u76ae\u5c42\u4e13\u5bb6\u805a\u7c7b\uff0c\u91cd\u7ec4\u6743\u91cd\u4e3a\u8bed\u4e49\u6a21\u5757\uff0c\u589e\u5f3a\u8de8\u4ee4\u724c\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cPaceLLM\u5728LongBench\u591a\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e0a\u63d0\u53476%\uff0c\u5728Infinite-Bench\u4efb\u52a1\u4e0a\u63d0\u534712.5-17.5%\uff0c\u5e76\u5728NIAH\u6d4b\u8bd5\u4e2d\u5c06\u53ef\u6d4b\u91cf\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u81f320\u4e07\u4ee4\u724c\u3002", "conclusion": "PaceLLM\u901a\u8fc7\u8111\u542f\u53d1\u4f18\u5316\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u65e0\u9700\u7ed3\u6784\u6539\u9020\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "paper_title_zh": "PaceLLM\uff1a\u57fa\u4e8e\u5927\u8111\u542f\u53d1\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u53d7\u9650\u4e8e\u795e\u7ecf\u6fc0\u6d3b\u77ed\u6682\u5bfc\u81f4\u7684\u4fe1\u606f\u8870\u51cf\u548c\u6743\u91cd\u7ed3\u6784\u677e\u6563\u5f15\u53d1\u7684\u8bed\u4e49\u788e\u7247\u5316\u3002\u53d7\u5927\u8111\u5de5\u4f5c\u8bb0\u5fc6\u548c\u76ae\u5c42\u6a21\u5757\u5316\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51faPaceLLM\uff0c\u5305\u542b\u4e24\u9879\u521b\u65b0\uff1a(1) \u6301\u4e45\u6d3b\u52a8\u673a\u5236\uff08PA\uff09\uff0c\u6a21\u62df\u524d\u989d\u53f6\u795e\u7ecf\u5143\u6301\u7eed\u653e\u7535\uff0c\u901a\u8fc7\u6fc0\u6d3b\u7ea7\u8bb0\u5fc6\u5e93\u52a8\u6001\u68c0\u7d22\u3001\u91cd\u7528\u548c\u66f4\u65b0\u5173\u952e\u7f51\u7edc\u72b6\u6001\uff0c\u89e3\u51b3\u4e0a\u4e0b\u6587\u8870\u51cf\uff1b(2) \u76ae\u5c42\u4e13\u5bb6\u805a\u7c7b\uff08CE\uff09\uff0c\u6a21\u62df\u4efb\u52a1\u9002\u5e94\u6027\u795e\u7ecf\u7279\u5316\uff0c\u5c06\u7f51\u7edc\u6743\u91cd\u91cd\u7ec4\u4e3a\u8bed\u4e49\u6a21\u5757\uff0c\u5efa\u7acb\u8de8\u4ee4\u724c\u4f9d\u8d56\u5e76\u51cf\u5c11\u788e\u7247\u5316\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPaceLLM\u5728LongBench\u591a\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e0a\u63d0\u53476%\uff0c\u5728Infinite-Bench\u4efb\u52a1\u4e0a\u63d0\u534712.5-17.5%\uff0c\u5e76\u5728\u201c\u5927\u6d77\u635e\u9488\u201d\u6d4b\u8bd5\u4e2d\u5c06\u53ef\u6d4b\u91cf\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u81f320\u4e07\u4ee4\u724c\u3002\u8be5\u7814\u7a76\u5f00\u521b\u4e86\u8111\u542f\u53d1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u4e0e\u5176\u4ed6\u5de5\u4f5c\u4e92\u8865\uff0c\u4e14\u65e0\u9700\u7ed3\u6784\u6539\u9020\u5373\u53ef\u63d0\u5347\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.18164", "pdf": "https://arxiv.org/pdf/2506.18164", "abs": "https://arxiv.org/abs/2506.18164", "authors": ["Varun Belagali", "Pierre Marza", "Srikar Yellapragada", "Zilinghan Li", "Tarak Nath Nandi", "Ravi K Madduri", "Joel Saltz", "Stergios Christodoulidis", "Maria Vakalopoulou", "Dimitris Samaras"], "title": "CDG-MAE: Learning Correspondences from Diffusion Generated Views", "categories": ["cs.CV"], "comment": null, "summary": "Learning dense correspondences, critical for application such as video label\npropagation, is hindered by tedious and unscalable manual annotation.\nSelf-supervised methods address this by using a cross-view pretext task, often\nmodeled with a masked autoencoder, where a masked target view is reconstructed\nfrom an anchor view. However, acquiring effective training data remains a\nchallenge - collecting diverse video datasets is difficult and costly, while\nsimple image crops lack necessary pose variations. This paper introduces\nCDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic\nviews generated from static images via an image-conditioned diffusion model.\nThese generated views exhibit substantial changes in pose and perspective,\nproviding a rich training signal that overcomes the limitations of video and\ncrop-based anchors. We present a quantitative method to evaluate local and\nglobal consistency of generated images, discussing their use for cross-view\nself-supervised pretraining. Furthermore, we enhance the standard single-anchor\nMAE setting to a multi-anchor strategy to effectively modulate the difficulty\nof pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods\nreliant only on images and substantially narrows the performance gap to\nvideo-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCDG-MAE\uff0c\u4e00\u79cd\u57fa\u4e8eMAE\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5408\u6210\u89c6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u5bc6\u96c6\u5bf9\u5e94\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5bc6\u96c6\u5bf9\u5e94\u5b66\u4e60\u4f9d\u8d56\u7e41\u7410\u4e14\u4e0d\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff08\u89c6\u9891\u6570\u636e\u96c6\u6536\u96c6\u56f0\u96be\uff0c\u56fe\u50cf\u88c1\u526a\u7f3a\u4e4f\u59ff\u6001\u53d8\u5316\uff09\u800c\u53d7\u9650\u3002", "method": "CDG-MAE\u901a\u8fc7\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5408\u6210\u89c6\u56fe\uff0c\u589e\u5f3a\u8bad\u7ec3\u4fe1\u53f7\uff1b\u63d0\u51fa\u591a\u951a\u70b9\u7b56\u7565\u8c03\u8282\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u91cf\u5316\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "CDG-MAE\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684MAE\u65b9\u6cd5\uff0c\u5927\u5e45\u7f29\u5c0f\u4e0e\u57fa\u4e8e\u89c6\u9891\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "CDG-MAE\u901a\u8fc7\u5408\u6210\u89c6\u56fe\u548c\u591a\u951a\u70b9\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u4e3a\u5bc6\u96c6\u5bf9\u5e94\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "CDG-MAE\uff1a\u4ece\u6269\u6563\u751f\u6210\u89c6\u56fe\u4e2d\u5b66\u4e60\u5bf9\u5e94\u5173\u7cfb", "abstract_zh": "\u5b66\u4e60\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\u5bf9\u4e8e\u89c6\u9891\u6807\u7b7e\u4f20\u64ad\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f9d\u8d56\u7e41\u7410\u4e14\u4e0d\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u6807\u6ce8\u3002\u81ea\u76d1\u7763\u65b9\u6cd5\u901a\u8fc7\u8de8\u89c6\u56fe\u501f\u53e3\u4efb\u52a1\uff08\u5982\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff09\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u83b7\u53d6\u6709\u6548\u8bad\u7ec3\u6570\u636e\u4ecd\u5177\u6311\u6218\u6027\u2014\u2014\u6536\u96c6\u591a\u6837\u89c6\u9891\u6570\u636e\u96c6\u56f0\u96be\u4e14\u6210\u672c\u9ad8\uff0c\u800c\u7b80\u5355\u56fe\u50cf\u88c1\u526a\u7f3a\u4e4f\u5fc5\u8981\u7684\u59ff\u6001\u53d8\u5316\u3002\u672c\u6587\u63d0\u51faCDG-MAE\uff0c\u4e00\u79cd\u57fa\u4e8eMAE\u7684\u65b0\u578b\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4ece\u9759\u6001\u56fe\u50cf\u751f\u6210\u591a\u6837\u5408\u6210\u89c6\u56fe\u3002\u8fd9\u4e9b\u751f\u6210\u89c6\u56fe\u5728\u59ff\u6001\u548c\u89c6\u89d2\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u53d8\u5316\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u514b\u670d\u4e86\u89c6\u9891\u548c\u88c1\u526a\u951a\u70b9\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u65b9\u6cd5\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u8de8\u89c6\u56fe\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u6807\u51c6\u7684\u5355\u951a\u70b9MAE\u8bbe\u7f6e\u6269\u5c55\u4e3a\u591a\u951a\u70b9\u7b56\u7565\uff0c\u4ee5\u6709\u6548\u8c03\u8282\u501f\u53e3\u4efb\u52a1\u7684\u96be\u5ea6\u3002CDG-MAE\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684\u6700\u5148\u8fdbMAE\u65b9\u6cd5\uff0c\u5e76\u5927\u5e45\u7f29\u5c0f\u4e86\u4e0e\u57fa\u4e8e\u89c6\u9891\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2506.17304", "pdf": "https://arxiv.org/pdf/2506.17304", "abs": "https://arxiv.org/abs/2506.17304", "authors": ["Jasper Yao"], "title": "AlgoSelect: Universal Algorithm Selection via the Comb Operator", "categories": ["cs.LG", "cs.AI", "cs.DS"], "comment": "24 pages, 4 figures, 1 repository, 1 supplementary document", "summary": "We introduce AlgoSelect, a principled framework for learning optimal\nalgorithm selection from data, centered around the novel Comb Operator. Given a\nset of algorithms and a feature representation of problems, AlgoSelect learns\nto interpolate between diverse computational approaches. For pairs of\nalgorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,\nfacilitates this interpolation. We extend this to an N-Path Comb for multiple\nalgorithms. We prove that this framework is universal (can approximate any\nalgorithm selector), information-theoretically optimal in its learnability\n(thresholds for selection converge almost surely, demonstrated via\nBorel-Cantelli arguments), computationally efficient, and robust. Key\ntheoretical contributions include: (1) a universal approximation theorem\ndemonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)\ninformation-theoretic learnability for selection thresholds; (3) formalization\nof the Comb Operator within linear operator theory, detailing its boundedness\nand spectral properties; (4) an N-Path Comb generalization for multi-algorithm\nselection; and (5) a practical learning framework for the adaptive seeding\nfunctions that guide the Comb Operator. Empirical validation on a comprehensive\n20$\\times$20 problem-algorithm study demonstrates near-perfect selection\n(99.9\\%+ accuracy) with remarkably few samples and rapid convergence, revealing\nthat $H(\\text{Algorithm}|\\text{Problem}) \\approx 0$ in structured domains.\nAlgoSelect provides a theoretically grounded, practically deployable solution\nto automated algorithm selection with provable optimality and learnability\nguarantees, with significant implications for AI and adaptive systems.", "AI": {"tldr": "AlgoSelect\u662f\u4e00\u79cd\u57fa\u4e8eComb Operator\u7684\u901a\u7528\u7b97\u6cd5\u9009\u62e9\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u4ece\u6570\u636e\u4e2d\u9009\u62e9\u6700\u4f18\u7b97\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4e0a\u7684\u666e\u9002\u6027\u3001\u4fe1\u606f\u8bba\u6700\u4f18\u6027\u548c\u8ba1\u7b97\u9ad8\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u9009\u62e9\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\u7f3a\u4e4f\u4e00\u79cd\u65e2\u80fd\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6700\u4f18\u6027\u548c\u53ef\u5b66\u4e60\u6027\uff0c\u53c8\u80fd\u5b9e\u9645\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002AlgoSelect\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7Comb Operator\u5b9e\u73b0\u901a\u7528\u4e14\u9ad8\u6548\u7684\u7b97\u6cd5\u9009\u62e9\u3002", "method": "AlgoSelect\u7684\u6838\u5fc3\u662fComb Operator\uff0c\u901a\u8fc7sigmoid\u95e8\u63a7\u9009\u62e9\u5668\u5728\u7b97\u6cd5\u4e4b\u95f4\u63d2\u503c\uff0c\u5e76\u6269\u5c55\u4e3aN-Path Comb\u4ee5\u652f\u6301\u591a\u7b97\u6cd5\u9009\u62e9\u3002\u7406\u8bba\u8d21\u732e\u5305\u62ec\u666e\u9002\u903c\u8fd1\u5b9a\u7406\u3001\u4fe1\u606f\u8bba\u53ef\u5b66\u4e60\u6027\u3001Comb Operator\u7684\u7ebf\u6027\u7b97\u5b50\u7406\u8bba\u5f62\u5f0f\u5316\u3001N-Path Comb\u7684\u6cdb\u5316\u4ee5\u53ca\u81ea\u9002\u5e94\u79cd\u5b50\u51fd\u6570\u7684\u5b9e\u7528\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u572820\u00d720\u7684\u95ee\u9898-\u7b97\u6cd5\u5b9e\u9a8c\u4e2d\uff0cAlgoSelect\u5b9e\u73b0\u4e8699.9%\u4ee5\u4e0a\u7684\u9009\u62e9\u51c6\u786e\u7387\uff0c\u6837\u672c\u9700\u6c42\u6781\u5c11\u4e14\u6536\u655b\u8fc5\u901f\uff0c\u8868\u660e\u5728\u7ed3\u6784\u5316\u9886\u57df\u4e2d\u7b97\u6cd5\u9009\u62e9\u7684\u6761\u4ef6\u71b5\u63a5\u8fd1\u4e8e\u96f6\u3002", "conclusion": "AlgoSelect\u4e3a\u81ea\u52a8\u5316\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u624e\u5b9e\u3001\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u666e\u9002\u6027\u3001\u6700\u4f18\u6027\u548c\u53ef\u5b66\u4e60\u6027\u4fdd\u8bc1\uff0c\u5bf9AI\u548c\u81ea\u9002\u5e94\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "paper_title_zh": "AlgoSelect\uff1a\u901a\u8fc7Comb Operator\u5b9e\u73b0\u901a\u7528\u7b97\u6cd5\u9009\u62e9", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86AlgoSelect\uff0c\u4e00\u79cd\u57fa\u4e8e\u65b0\u9896Comb Operator\u7684\u7b97\u6cd5\u9009\u62e9\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6700\u4f18\u7b97\u6cd5\u9009\u62e9\u3002\u7ed9\u5b9a\u4e00\u7ec4\u7b97\u6cd5\u548c\u95ee\u9898\u7684\u7279\u5f81\u8868\u793a\uff0cAlgoSelect\u5b66\u4f1a\u5728\u4e0d\u540c\u8ba1\u7b97\u65b9\u6cd5\u4e4b\u95f4\u63d2\u503c\u3002\u5bf9\u4e8e\u7b97\u6cd5\u5bf9\uff0c\u4e00\u4e2a\u7b80\u5355\u7684sigmoid\u95e8\u63a7\u9009\u62e9\u5668\uff08Comb Operator\u7684\u5b9e\u4f8b\uff09\u5b9e\u73b0\u4e86\u8fd9\u79cd\u63d2\u503c\u3002\u6211\u4eec\u5c06\u5176\u6269\u5c55\u4e3a\u652f\u6301\u591a\u7b97\u6cd5\u9009\u62e9\u7684N-Path Comb\u3002\u6211\u4eec\u8bc1\u660e\u8be5\u6846\u67b6\u5177\u6709\u666e\u9002\u6027\uff08\u53ef\u4ee5\u903c\u8fd1\u4efb\u4f55\u7b97\u6cd5\u9009\u62e9\u5668\uff09\u3001\u4fe1\u606f\u8bba\u6700\u4f18\u6027\uff08\u9009\u62e9\u9608\u503c\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\uff0c\u901a\u8fc7Borel-Cantelli\u8bba\u8bc1\u5c55\u793a\uff09\u3001\u8ba1\u7b97\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u5173\u952e\u7406\u8bba\u8d21\u732e\u5305\u62ec\uff1a(1) \u666e\u9002\u903c\u8fd1\u5b9a\u7406\uff0c\u8bc1\u660e\u57fa\u4e8eComb\u7684\u9009\u62e9\u5668\u53ef\u4ee5\u5b9e\u73b0\u4efb\u610f\u7cbe\u5ea6\uff1b(2) \u9009\u62e9\u9608\u503c\u7684\u4fe1\u606f\u8bba\u53ef\u5b66\u4e60\u6027\uff1b(3) \u5728\u7ebf\u6027\u7b97\u5b50\u7406\u8bba\u4e2d\u5f62\u5f0f\u5316Comb Operator\uff0c\u8be6\u8ff0\u5176\u6709\u754c\u6027\u548c\u8c31\u6027\u8d28\uff1b(4) \u591a\u7b97\u6cd5\u9009\u62e9\u7684N-Path Comb\u6cdb\u5316\uff1b(5) \u6307\u5bfcComb Operator\u7684\u81ea\u9002\u5e94\u79cd\u5b50\u51fd\u6570\u7684\u5b9e\u7528\u5b66\u4e60\u6846\u67b6\u3002\u5728\u5168\u9762\u768420\u00d720\u95ee\u9898-\u7b97\u6cd5\u7814\u7a76\u4e2d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660eAlgoSelect\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u9009\u62e9\u51c6\u786e\u7387\uff0899.9%\u4ee5\u4e0a\uff09\uff0c\u6837\u672c\u9700\u6c42\u6781\u5c11\u4e14\u6536\u655b\u8fc5\u901f\uff0c\u63ed\u793a\u4e86\u5728\u7ed3\u6784\u5316\u9886\u57df\u4e2d\u7b97\u6cd5\u9009\u62e9\u7684\u6761\u4ef6\u71b5\u63a5\u8fd1\u4e8e\u96f6\u3002AlgoSelect\u4e3a\u81ea\u52a8\u5316\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u624e\u5b9e\u3001\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6700\u4f18\u6027\u548c\u53ef\u5b66\u4e60\u6027\u4fdd\u8bc1\uff0c\u5bf9AI\u548c\u81ea\u9002\u5e94\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2506.17331", "pdf": "https://arxiv.org/pdf/2506.17331", "abs": "https://arxiv.org/abs/2506.17331", "authors": ["Craig Steven Wright"], "title": "Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems", "categories": ["cs.LO", "cs.CL", "math.LO", "68T27, 03B70", "I.2.4; I.2.3"], "comment": "126 pages, 0 figures, includes formal frameworks and architecture\n  blueprint; no prior version; suitable for submission under AI and Logic\n  categories", "summary": "This paper develops a comprehensive framework for artificial intelligence\nsystems that operate under strict epistemic constraints, moving beyond\nstochastic language prediction to support structured reasoning, propositional\ncommitment, and contradiction detection. It formalises belief representation,\nmetacognitive processes, and normative verification, integrating symbolic\ninference, knowledge graphs, and blockchain-based justification to ensure\ntruth-preserving, auditably rational epistemic agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u8ba4\u77e5\u7ea6\u675f\u4e0b\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6846\u67b6\uff0c\u8d85\u8d8a\u968f\u673a\u8bed\u8a00\u9884\u6d4b\uff0c\u652f\u6301\u7ed3\u6784\u5316\u63a8\u7406\u3001\u547d\u9898\u627f\u8bfa\u548c\u77db\u76fe\u68c0\u6d4b\uff0c\u786e\u4fdd\u771f\u7406\u4fdd\u6301\u548c\u53ef\u5ba1\u8ba1\u7684\u7406\u6027\u8ba4\u77e5\u4ee3\u7406\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u968f\u673a\u8bed\u8a00\u9884\u6d4b\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8ba4\u77e5\u7ea6\u675f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6784\u5efa\u4e00\u4e2a\u652f\u6301\u4e25\u683c\u8ba4\u77e5\u7ea6\u675f\u3001\u771f\u7406\u4fdd\u6301\u548c\u53ef\u5ba1\u8ba1\u6027\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u4fe1\u5ff5\u8868\u793a\u3001\u5143\u8ba4\u77e5\u8fc7\u7a0b\u548c\u89c4\u8303\u6027\u9a8c\u8bc1\uff0c\u6574\u5408\u7b26\u53f7\u63a8\u7406\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u8bba\u8bc1\uff0c\u786e\u4fdd\u7cfb\u7edf\u7684\u771f\u7406\u4fdd\u6301\u548c\u7406\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u652f\u6301\u7ed3\u6784\u5316\u63a8\u7406\u3001\u547d\u9898\u627f\u8bfa\u548c\u77db\u76fe\u68c0\u6d4b\uff0c\u540c\u65f6\u5177\u5907\u771f\u7406\u4fdd\u6301\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u4e25\u683c\u8ba4\u77e5\u7ea6\u675f\u4e0b\u7684\u8fd0\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u7406\u6027\u8ba4\u77e5\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "paper_title_zh": "\u8d85\u8d8a\u9884\u6d4b\u2014\u2014\u6784\u5efa\u4eba\u5de5\u667a\u80fd\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684\u8ba4\u77e5\u5b8c\u6574\u6027", "abstract_zh": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6846\u67b6\uff0c\u5728\u4e25\u683c\u7684\u8ba4\u77e5\u7ea6\u675f\u4e0b\u8fd0\u884c\uff0c\u8d85\u8d8a\u968f\u673a\u8bed\u8a00\u9884\u6d4b\uff0c\u652f\u6301\u7ed3\u6784\u5316\u63a8\u7406\u3001\u547d\u9898\u627f\u8bfa\u548c\u77db\u76fe\u68c0\u6d4b\u3002\u5b83\u5f62\u5f0f\u5316\u4e86\u4fe1\u5ff5\u8868\u793a\u3001\u5143\u8ba4\u77e5\u8fc7\u7a0b\u548c\u89c4\u8303\u6027\u9a8c\u8bc1\uff0c\u6574\u5408\u4e86\u7b26\u53f7\u63a8\u7406\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u8bba\u8bc1\uff0c\u4ee5\u786e\u4fdd\u771f\u7406\u4fdd\u6301\u3001\u53ef\u5ba1\u8ba1\u7684\u7406\u6027\u8ba4\u77e5\u4ee3\u7406\u3002"}}
{"id": "2506.18172", "pdf": "https://arxiv.org/pdf/2506.18172", "abs": "https://arxiv.org/abs/2506.18172", "authors": ["Irsyad Adam", "Tengyue Zhang", "Shrayes Raman", "Zhuyu Qiu", "Brandon Taraku", "Hexiang Feng", "Sile Wang", "Ashwath Radhachandran", "Shreeram Athreya", "Vedrana Ivezic", "Peipei Ping", "Corey Arnold", "William Speier"], "title": "STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Thyroid cancer is among the most common cancers in the United States. Thyroid\nnodules are frequently detected through ultrasound (US) imaging, and some\nrequire further evaluation via fine-needle aspiration (FNA) biopsy. Despite its\neffectiveness, FNA often leads to unnecessary biopsies of benign nodules,\ncausing patient discomfort and anxiety. To address this, the American College\nof Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been\ndeveloped to reduce benign biopsies. However, such systems are limited by\ninterobserver variability. Recent deep learning approaches have sought to\nimprove risk stratification, but they often fail to utilize the rich temporal\nand spatial context provided by US cine clips, which contain dynamic global\ninformation and surrounding structural changes across various views. In this\nwork, we propose the Spatio-Temporal Cross Attention for Cine Thyroid\nUltrasound Time Series Classification (STACT-Time) model, a novel\nrepresentation learning framework that integrates imaging features from US cine\nclips with features from segmentation masks automatically generated by a\npretrained model. By leveraging self-attention and cross-attention mechanisms,\nour model captures the rich temporal and spatial context of US cine clips while\nenhancing feature representation through segmentation-guided learning. Our\nmodel improves malignancy prediction compared to state-of-the-art models,\nachieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1\nscore of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign\nnodules while maintaining high sensitivity for malignancy detection, our model\nhas the potential to enhance clinical decision-making and improve patient\noutcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTACT-Time\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u7532\u72b6\u817a\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u7ed3\u5408\u5206\u5272\u63a9\u6a21\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7532\u72b6\u817a\u7ed3\u8282\u6076\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u6d3b\u68c0\u3002", "motivation": "\u7532\u72b6\u817a\u764c\u662f\u7f8e\u56fd\u5e38\u89c1\u764c\u75c7\u4e4b\u4e00\uff0c\u8d85\u58f0\u68c0\u67e5\u5e38\u53d1\u73b0\u7532\u72b6\u817a\u7ed3\u8282\uff0c\u4f46\u7ec6\u9488\u7a7f\u523a\u6d3b\u68c0\uff08FNA\uff09\u5bf9\u826f\u6027\u7ed3\u8282\u7684\u4e0d\u5fc5\u8981\u68c0\u67e5\u589e\u52a0\u4e86\u60a3\u8005\u8d1f\u62c5\u3002\u73b0\u6709\u7cfb\u7edf\u5982TI-RADS\u53d7\u9650\u4e8e\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u7684\u65f6\u7a7a\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u6709\u6548\u5229\u7528\u65f6\u7a7a\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6a21\u578b\uff0c\u63d0\u5347\u6076\u6027\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faSTACT-Time\u6a21\u578b\uff0c\u7ed3\u5408\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u7684\u65f6\u7a7a\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u7684\u5206\u5272\u63a9\u6a21\u7279\u5f81\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u52a8\u6001\u89c6\u9891\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff0c\u5e76\u5229\u7528\u5206\u5272\u5f15\u5bfc\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTACT-Time\u6a21\u578b\u5728\u6076\u6027\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u7cbe\u786e\u5ea6\u4e3a0.91\uff08\u00b10.02\uff09\uff0cF1\u5206\u6570\u4e3a0.89\uff08\u00b10.02\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u826f\u6027\u7ed3\u8282\u7684\u4e0d\u5fc5\u8981\u6d3b\u68c0\u3002", "conclusion": "STACT-Time\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5229\u7528\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7532\u72b6\u817a\u7ed3\u8282\u6076\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u6709\u671b\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u5e76\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "paper_title_zh": "STACT-Time\uff1a\u57fa\u4e8e\u65f6\u7a7a\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7532\u72b6\u817a\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b", "abstract_zh": "\u7532\u72b6\u817a\u764c\u662f\u7f8e\u56fd\u6700\u5e38\u89c1\u7684\u764c\u75c7\u4e4b\u4e00\u3002\u7532\u72b6\u817a\u7ed3\u8282\u901a\u5e38\u901a\u8fc7\u8d85\u58f0\uff08US\uff09\u6210\u50cf\u68c0\u6d4b\uff0c\u90e8\u5206\u9700\u901a\u8fc7\u7ec6\u9488\u7a7f\u523a\uff08FNA\uff09\u6d3b\u68c0\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002\u5c3d\u7ba1FNA\u6709\u6548\uff0c\u4f46\u5e38\u5bfc\u81f4\u5bf9\u826f\u6027\u7ed3\u8282\u7684\u4e0d\u5fc5\u8981\u6d3b\u68c0\uff0c\u5f15\u53d1\u60a3\u8005\u4e0d\u9002\u548c\u7126\u8651\u3002\u4e3a\u6b64\uff0c\u7f8e\u56fd\u653e\u5c04\u5b66\u4f1a\u5f00\u53d1\u4e86\u7532\u72b6\u817a\u6210\u50cf\u62a5\u544a\u4e0e\u6570\u636e\u7cfb\u7edf\uff08TI-RADS\uff09\u4ee5\u51cf\u5c11\u826f\u6027\u6d3b\u68c0\uff0c\u4f46\u5176\u53d7\u9650\u4e8e\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u3002\u8fd1\u671f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8bd5\u56fe\u6539\u8fdb\u98ce\u9669\u5206\u5c42\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u63d0\u4f9b\u7684\u4e30\u5bcc\u65f6\u7a7a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8fd9\u4e9b\u89c6\u9891\u5305\u542b\u52a8\u6001\u5168\u5c40\u4fe1\u606f\u548c\u591a\u89c6\u89d2\u4e0b\u7684\u5468\u56f4\u7ed3\u6784\u53d8\u5316\u3002\u672c\u6587\u63d0\u51faSTACT-Time\u6a21\u578b\uff0c\u4e00\u79cd\u65b0\u578b\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u7684\u6210\u50cf\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u81ea\u52a8\u751f\u6210\u7684\u5206\u5272\u63a9\u6a21\u7279\u5f81\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u578b\u6355\u6349\u8d85\u58f0\u52a8\u6001\u89c6\u9891\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u5206\u5272\u5f15\u5bfc\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002\u76f8\u6bd4\u73b0\u6709\u6a21\u578b\uff0cSTACT-Time\u663e\u8457\u63d0\u5347\u4e86\u6076\u6027\u9884\u6d4b\u80fd\u529b\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u7cbe\u786e\u5ea6\u4e3a0.91\uff08\u00b10.02\uff09\uff0cF1\u5206\u6570\u4e3a0.89\uff08\u00b10.02\uff09\u3002\u901a\u8fc7\u51cf\u5c11\u826f\u6027\u7ed3\u8282\u7684\u4e0d\u5fc5\u8981\u6d3b\u68c0\u5e76\u4fdd\u6301\u9ad8\u6076\u6027\u68c0\u6d4b\u654f\u611f\u6027\uff0c\u8be5\u6a21\u578b\u6709\u671b\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u5e76\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2506.17312", "pdf": "https://arxiv.org/pdf/2506.17312", "abs": "https://arxiv.org/abs/2506.17312", "authors": ["Huan Liu", "Pengfei Jiao", "Mengzhou Gao", "Chaochao Chen", "Di Jin"], "title": "Heterogeneous Temporal Hypergraph Neural Network", "categories": ["cs.SI", "cs.AI", "cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Graph representation learning (GRL) has emerged as an effective technique for\nmodeling graph-structured data. When modeling heterogeneity and dynamics in\nreal-world complex networks, GRL methods designed for complex heterogeneous\ntemporal graphs (HTGs) have been proposed and have achieved successful\napplications in various fields. However, most existing GRL methods mainly focus\non preserving the low-order topology information while ignoring higher-order\ngroup interaction relationships, which are more consistent with real-world\nnetworks. In addition, most existing hypergraph methods can only model static\nhomogeneous graphs, limiting their ability to model high-order interactions in\nHTGs. Therefore, to simultaneously enable the GRL model to capture high-order\ninteraction relationships in HTGs, we first propose a formal definition of\nheterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge\nconstruction algorithm that does not rely on additional information. Then, a\nnovel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to\nfully capture higher-order interactions in HTGs. HTHGN contains a hierarchical\nattention mechanism module that simultaneously performs temporal\nmessage-passing between heterogeneous nodes and hyperedges to capture rich\nsemantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN\nperforms contrastive learning by maximizing the consistency between low-order\ncorrelated heterogeneous node pairs on HTG to avoid the low-order structural\nambiguity issue. Detailed experimental results on three real-world HTG datasets\nverify the effectiveness of the proposed HTHGN for modeling high-order\ninteractions in HTGs and demonstrate significant performance improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u65f6\u5e8f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HTHGN\uff09\uff0c\u7528\u4e8e\u6355\u6349\u590d\u6742\u5f02\u6784\u65f6\u5e8f\u56fe\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f4e\u9636\u62d3\u6251\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u9ad8\u9636\u7fa4\u4f53\u4ea4\u4e92\u5173\u7cfb\uff0c\u4e14\u73b0\u6709\u8d85\u56fe\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u9759\u6001\u540c\u6784\u56fe\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u540c\u65f6\u5efa\u6a21\u5f02\u6784\u65f6\u5e8f\u56fe\u4e2d\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u4e86\u5f02\u6784\u65f6\u5e8f\u8d85\u56fe\u7684\u5f62\u5f0f\u5316\u8868\u793a\u53ca\u4e0d\u4f9d\u8d56\u989d\u5916\u4fe1\u606f\u7684$P$-\u5747\u5300\u8d85\u8fb9\u6784\u5efa\u7b97\u6cd5\uff1b\u968f\u540e\u63d0\u51faHTHGN\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u6a21\u5757\u5b9e\u73b0\u5f02\u6784\u8282\u70b9\u4e0e\u8d85\u8fb9\u95f4\u7684\u65f6\u5e8f\u6d88\u606f\u4f20\u9012\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u907f\u514d\u4f4e\u9636\u7ed3\u6784\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u5f02\u6784\u65f6\u5e8f\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHTHGN\u80fd\u6709\u6548\u5efa\u6a21\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "HTHGN\u4e3a\u5f02\u6784\u65f6\u5e8f\u56fe\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "paper_title_zh": "\u5f02\u6784\u65f6\u5e8f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc", "abstract_zh": "\u56fe\u8868\u793a\u5b66\u4e60\uff08GRL\uff09\u5df2\u6210\u4e3a\u5efa\u6a21\u56fe\u7ed3\u6784\u6570\u636e\u7684\u6709\u6548\u6280\u672f\u3002\u5728\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u65f6\uff0c\u9488\u5bf9\u590d\u6742\u5f02\u6784\u65f6\u5e8f\u56fe\uff08HTG\uff09\u7684GRL\u65b9\u6cd5\u5df2\u88ab\u63d0\u51fa\u5e76\u5728\u591a\u4e2a\u9886\u57df\u6210\u529f\u5e94\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709GRL\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4fdd\u7559\u4f4e\u9636\u62d3\u6251\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u66f4\u7b26\u5408\u771f\u5b9e\u7f51\u7edc\u7684\u9ad8\u9636\u7fa4\u4f53\u4ea4\u4e92\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u73b0\u6709\u8d85\u56fe\u65b9\u6cd5\u4ec5\u80fd\u5efa\u6a21\u9759\u6001\u540c\u6784\u56fe\uff0c\u9650\u5236\u4e86\u5176\u5728HTG\u4e2d\u5efa\u6a21\u9ad8\u9636\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4e3a\u4f7fGRL\u6a21\u578b\u540c\u65f6\u6355\u6349HTG\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\uff0c\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u5f02\u6784\u65f6\u5e8f\u8d85\u56fe\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u53ca\u4e0d\u4f9d\u8d56\u989d\u5916\u4fe1\u606f\u7684$P$-\u5747\u5300\u5f02\u6784\u8d85\u8fb9\u6784\u5efa\u7b97\u6cd5\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5f02\u6784\u65f6\u5e8f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HTHGN\uff09\uff0c\u4ee5\u5168\u9762\u6355\u6349HTG\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\u3002HTHGN\u5305\u542b\u4e00\u4e2a\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u6a21\u5757\uff0c\u53ef\u540c\u65f6\u5b9e\u73b0\u5f02\u6784\u8282\u70b9\u4e0e\u8d85\u8fb9\u95f4\u7684\u65f6\u5e8f\u6d88\u606f\u4f20\u9012\uff0c\u4ee5\u6355\u6349\u8d85\u8fb9\u5e26\u6765\u7684\u66f4\u5e7f\u611f\u53d7\u91ce\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u3002\u6b64\u5916\uff0cHTHGN\u901a\u8fc7\u6700\u5927\u5316HTG\u4e0a\u4f4e\u9636\u76f8\u5173\u5f02\u6784\u8282\u70b9\u5bf9\u7684\u4e00\u81f4\u6027\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u907f\u514d\u4f4e\u9636\u7ed3\u6784\u6a21\u7cca\u95ee\u9898\u3002\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754cHTG\u6570\u636e\u96c6\u4e0a\u7684\u8be6\u7ec6\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86HTHGN\u5728\u5efa\u6a21\u9ad8\u9636\u4ea4\u4e92\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.17351", "pdf": "https://arxiv.org/pdf/2506.17351", "abs": "https://arxiv.org/abs/2506.17351", "authors": ["Mostafa Shahin", "Beena Ahmed", "Julien Epps"], "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": null, "summary": "Cognitive impairment (CI) is of growing public health concern, and early\ndetection is vital for effective intervention. Speech has gained attention as a\nnon-invasive and easily collectible biomarker for assessing cognitive decline.\nTraditional CI detection methods typically rely on supervised models trained on\nacoustic and linguistic features extracted from speech, which often require\nmanual annotation and may not generalise well across datasets and languages. In\nthis work, we propose the first zero-shot speech-based CI detection method\nusing the Qwen2- Audio AudioLLM, a model capable of processing both audio and\ntext inputs. By designing prompt-based instructions, we guide the model in\nclassifying speech samples as indicative of normal cognition or cognitive\nimpairment. We evaluate our approach on two datasets: one in English and\nanother multilingual, spanning different cognitive assessment tasks. Our\nresults show that the zero-shot AudioLLM approach achieves performance\ncomparable to supervised methods and exhibits promising generalizability and\nconsistency across languages, tasks, and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u97f3\u7684\u96f6\u6837\u672c\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528Qwen2-Audio AudioLLM\u6a21\u578b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u63d0\u793a\u6307\u4ee4\u5b9e\u73b0\u5206\u7c7b\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\uff0c\u4e14\u5728\u8de8\u8bed\u8a00\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8ba4\u77e5\u969c\u788d\uff08CI\uff09\u662f\u65e5\u76ca\u4e25\u91cd\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u9700\u8981\u624b\u52a8\u6807\u6ce8\u4e14\u6cdb\u5316\u6027\u5dee\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Qwen2-Audio AudioLLM\u6a21\u578b\uff0c\u8bbe\u8ba1\u63d0\u793a\u6307\u4ee4\u5bf9\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u5206\u7c7b\uff0c\u5224\u65ad\u5176\u662f\u5426\u663e\u793a\u8ba4\u77e5\u969c\u788d\u3002\u6a21\u578b\u652f\u6301\u97f3\u9891\u548c\u6587\u672c\u8f93\u5165\uff0c\u5e76\u5728\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u8de8\u8bed\u8a00\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u57fa\u4e8eAudioLLM\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u4e3a\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8eAudioLLM\u7684\u96f6\u6837\u672c\u8bed\u97f3\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b", "abstract_zh": "\u8ba4\u77e5\u969c\u788d\uff08CI\uff09\u662f\u65e5\u76ca\u4e25\u91cd\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u6709\u6548\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u8bed\u97f3\u4f5c\u4e3a\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u4e14\u6613\u4e8e\u6536\u96c6\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u4f20\u7edf\u7684CI\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4ece\u8bed\u97f3\u4e2d\u63d0\u53d6\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\u7684\u76d1\u7763\u6a21\u578b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u6807\u6ce8\u4e14\u96be\u4ee5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u8bed\u8a00\u4e2d\u6cdb\u5316\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQwen2-Audio AudioLLM\u7684\u96f6\u6837\u672c\u8bed\u97f3CI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5904\u7406\u97f3\u9891\u548c\u6587\u672c\u8f93\u5165\u3002\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u4ee4\uff0c\u6211\u4eec\u5f15\u5bfc\u6a21\u578b\u5c06\u8bed\u97f3\u6837\u672c\u5206\u7c7b\u4e3a\u6b63\u5e38\u8ba4\u77e5\u6216\u8ba4\u77e5\u969c\u788d\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u4e00\u4e2a\u82f1\u8bed\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff09\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e0d\u540c\u7684\u8ba4\u77e5\u8bc4\u4f30\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u96f6\u6837\u672cAudioLLM\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u5728\u8bed\u8a00\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.18173", "pdf": "https://arxiv.org/pdf/2506.18173", "abs": "https://arxiv.org/abs/2506.18173", "authors": ["Sabbir Ahmed", "Md. Bakhtiar Hasan", "Tasnim Ahmed", "Md. Hasanul Kabir"], "title": "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data", "categories": ["cs.CV"], "comment": "Submitted to ACPR Springer, 15 pages, 1 Figure, 7 Tables, and lots of\n  efforts :)", "summary": "While deep learning-based architectures have been widely used for correctly\ndetecting and classifying plant diseases, they require large-scale datasets to\nlearn generalized features and achieve state-of-the-art performance. This poses\na challenge for such models to obtain satisfactory performance in classifying\nleaf diseases with limited samples. This work proposes a few-shot learning\nframework, Domain-adapted Expert Network (DExNet), for plant disease\nclassification that compensates for the lack of sufficient training data by\ncombining observations of a number of expert critics. It starts with extracting\nthe feature embeddings as 'observations' from nine 'critics' that are\nstate-of-the-art pre-trained CNN-based architectures. These critics are 'domain\nadapted' using a publicly available leaf disease dataset having no overlapping\nclasses with the specific downstream task of interest. The observations are\nthen passed to the 'Feature Fusion Block' and finally to a classifier network\nconsisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10\nclasses of tomato leaf images from the PlantVillage dataset, achieving\npromising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,\n10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%\nhas been achieved in 80-shot classification, which is only 1.2% less than\nstate-of-the-art, allowing a 94.5% reduction in the training data requirement.\nThe proposed pipeline also outperforms existing works on leaf disease\nclassification with limited data in both laboratory and real-life conditions in\nsingle-domain, mixed-domain, and cross-domain scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDExNet\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3CNN\u67b6\u6784\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u690d\u7269\u53f6\u7247\u75c5\u5bb3\u5206\u7c7b\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u690d\u7269\u75c5\u5bb3\u5206\u7c7b\u4e2d\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4f46\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u4e0b\u9ad8\u6548\u5206\u7c7b\u53f6\u7247\u75c5\u5bb3\u7684\u65b9\u6cd5\u3002", "method": "DExNet\u6846\u67b6\u9996\u5148\u4ece\u4e5d\u4e2a\u9884\u8bad\u7ec3\u7684CNN\u67b6\u6784\u4e2d\u63d0\u53d6\u7279\u5f81\u5d4c\u5165\u4f5c\u4e3a\u89c2\u5bdf\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u7684\u53f6\u7247\u75c5\u5bb3\u6570\u636e\u96c6\u8fdb\u884c\u9886\u57df\u9002\u5e94\u3002\u968f\u540e\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u5757\u548cBi-LSTM\u5206\u7c7b\u5668\u7f51\u7edc\u5b8c\u6210\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728PlantVillage\u6570\u636e\u96c6\u7684\u756a\u8304\u53f6\u7247\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cDExNet\u57285-shot\u300110-shot\u548c15-shot\u5206\u7c7b\u4e2d\u5206\u522b\u8fbe\u523089.06%\u300192.46%\u548c94.07%\u7684\u51c6\u786e\u7387\uff0c80-shot\u5206\u7c7b\u4e2d\u8fbe\u523098.09%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u4f4e1.2%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8694.5%\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002", "conclusion": "DExNet\u5728\u5c0f\u6837\u672c\u53f6\u7247\u75c5\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u9700\u6c42\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DExNet\uff1a\u7ed3\u5408\u9886\u57df\u9002\u5e94\u8bc4\u8bba\u5668\u7684\u89c2\u5bdf\u7ed3\u679c\u7528\u4e8e\u5c0f\u6837\u672c\u53f6\u7247\u75c5\u5bb3\u5206\u7c7b", "abstract_zh": "\u5c3d\u7ba1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u67b6\u6784\u5df2\u5e7f\u6cdb\u7528\u4e8e\u690d\u7269\u75c5\u5bb3\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u5b66\u4e60\u901a\u7528\u7279\u5f81\u5e76\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002\u8fd9\u4e3a\u6a21\u578b\u5728\u6709\u9650\u6837\u672c\u4e0b\u5206\u7c7b\u53f6\u7247\u75c5\u5bb3\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c0f\u6837\u672c\u5b66\u4e60\u6846\u67b6\u2014\u2014\u9886\u57df\u9002\u5e94\u4e13\u5bb6\u7f51\u7edc\uff08DExNet\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u4e13\u5bb6\u8bc4\u8bba\u5668\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u5f25\u8865\u4e86\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u9996\u5148\u4ece\u4e5d\u4e2a\u9884\u8bad\u7ec3\u7684CNN\u67b6\u6784\u4e2d\u63d0\u53d6\u7279\u5f81\u5d4c\u5165\u4f5c\u4e3a\u89c2\u5bdf\u7ed3\u679c\uff0c\u5e76\u4f7f\u7528\u4e0e\u4e0b\u6e38\u4efb\u52a1\u65e0\u91cd\u53e0\u7c7b\u522b\u7684\u516c\u5f00\u53f6\u7247\u75c5\u5bb3\u6570\u636e\u96c6\u8fdb\u884c\u9886\u57df\u9002\u5e94\u3002\u968f\u540e\uff0c\u89c2\u5bdf\u7ed3\u679c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u5757\u4f20\u9012\u81f3\u7531Bi-LSTM\u5c42\u7ec4\u6210\u7684\u5206\u7c7b\u5668\u7f51\u7edc\u3002\u8be5\u6846\u67b6\u5728PlantVillage\u6570\u636e\u96c6\u768410\u7c7b\u756a\u8304\u53f6\u7247\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5206\u522b\u57285-shot\u300110-shot\u548c15-shot\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e8689.06%\u300192.46%\u548c94.07%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c80-shot\u5206\u7c7b\u7684\u51c6\u786e\u7387\u8fbe\u523098.09\u00b10.7%\uff0c\u4ec5\u6bd4\u6700\u4f18\u65b9\u6cd5\u4f4e1.2%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8694.5%\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002\u8be5\u6846\u67b6\u5728\u5b9e\u9a8c\u5ba4\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5355\u57df\u3001\u6df7\u5408\u57df\u548c\u8de8\u57df\u6761\u4ef6\u4e0b\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5c0f\u6837\u672c\u53f6\u7247\u75c5\u5bb3\u5206\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2506.17318", "pdf": "https://arxiv.org/pdf/2506.17318", "abs": "https://arxiv.org/abs/2506.17318", "authors": ["Atharv Singh Patlan", "Ashwin Hebbar", "Pramod Viswanath", "Prateek Mittal"], "title": "Context manipulation attacks : Web agents are susceptible to corrupted memory", "categories": ["cs.CR", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Autonomous web navigation agents, which translate natural language\ninstructions into sequences of browser actions, are increasingly deployed for\ncomplex tasks across e-commerce, information retrieval, and content discovery.\nDue to the stateless nature of large language models (LLMs), these agents rely\nheavily on external memory systems to maintain context across interactions.\nUnlike centralized systems where context is securely stored server-side, agent\nmemory is often managed client-side or by third-party applications, creating\nsignificant security vulnerabilities. This was recently exploited to attack\nproduction systems.\n  We introduce and formalize \"plan injection,\" a novel context manipulation\nattack that corrupts these agents' internal task representations by targeting\nthis vulnerable context. Through systematic evaluation of two popular web\nagents, Browser-use and Agent-E, we show that plan injections bypass robust\nprompt injection defenses, achieving up to 3x higher attack success rates than\ncomparable prompt-based attacks. Furthermore, \"context-chained injections,\"\nwhich craft logical bridges between legitimate user goals and attacker\nobjectives, lead to a 17.7% increase in success rate for privacy exfiltration\ntasks. Our findings highlight that secure memory handling must be a first-class\nconcern in agentic systems.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u81ea\u4e3b\u7f51\u9875\u5bfc\u822a\u4ee3\u7406\u56e0\u4f9d\u8d56\u5916\u90e8\u5185\u5b58\u7cfb\u7edf\u800c\u9762\u4e34\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u201c\u8ba1\u5212\u6ce8\u5165\u201d\u653b\u51fb\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7f51\u9875\u5bfc\u822a\u4ee3\u7406\u5728\u7535\u5b50\u5546\u52a1\u3001\u4fe1\u606f\u68c0\u7d22\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4f9d\u8d56\u7684\u5916\u90e8\u5185\u5b58\u7cfb\u7edf\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u64cd\u7eb5\u4e0a\u4e0b\u6587\u8fdb\u884c\u653b\u51fb\uff0c\u5a01\u80c1\u7cfb\u7edf\u5b89\u5168\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8ba1\u5212\u6ce8\u5165\u201d\u7684\u4e0a\u4e0b\u6587\u64cd\u7eb5\u653b\u51fb\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e24\u79cd\u6d41\u884c\u7684\u7f51\u9875\u4ee3\u7406\uff08Browser-use\u548cAgent-E\uff09\uff0c\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u201c\u4e0a\u4e0b\u6587\u94fe\u5f0f\u6ce8\u5165\u201d\u4ee5\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u201c\u8ba1\u5212\u6ce8\u5165\u201d\u653b\u51fb\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u673a\u5236\uff0c\u653b\u51fb\u6210\u529f\u7387\u6bd4\u4f20\u7edf\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u9ad8\u51fa3\u500d\uff1b\u201c\u4e0a\u4e0b\u6587\u94fe\u5f0f\u6ce8\u5165\u201d\u5728\u9690\u79c1\u6cc4\u9732\u4efb\u52a1\u4e2d\u4f7f\u653b\u51fb\u6210\u529f\u7387\u63d0\u534717.7%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5b89\u5168\u5185\u5b58\u5904\u7406\u7684\u91cd\u8981\u6027\uff0c\u9700\u5c06\u5176\u4f5c\u4e3a\u9996\u8981\u5173\u6ce8\u70b9\u4ee5\u9632\u6b62\u7c7b\u4f3c\u653b\u51fb\u3002", "paper_title_zh": "\u4e0a\u4e0b\u6587\u64cd\u7eb5\u653b\u51fb\uff1a\u7f51\u9875\u4ee3\u7406\u6613\u53d7\u5185\u5b58\u6c61\u67d3\u5f71\u54cd", "abstract_zh": "\u81ea\u4e3b\u7f51\u9875\u5bfc\u822a\u4ee3\u7406\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u6d4f\u89c8\u5668\u64cd\u4f5c\u5e8f\u5217\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u3001\u4fe1\u606f\u68c0\u7d22\u548c\u5185\u5bb9\u53d1\u73b0\u7b49\u590d\u6742\u4efb\u52a1\u3002\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65e0\u72b6\u6001\u7279\u6027\uff0c\u8fd9\u4e9b\u4ee3\u7406\u4e25\u91cd\u4f9d\u8d56\u5916\u90e8\u5185\u5b58\u7cfb\u7edf\u6765\u7ef4\u62a4\u4ea4\u4e92\u95f4\u7684\u4e0a\u4e0b\u6587\u3002\u4e0e\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u4e2d\u4e0a\u4e0b\u6587\u5b89\u5168\u5b58\u50a8\u5728\u670d\u52a1\u5668\u7aef\u4e0d\u540c\uff0c\u4ee3\u7406\u5185\u5b58\u901a\u5e38\u7531\u5ba2\u6237\u7aef\u6216\u7b2c\u4e09\u65b9\u5e94\u7528\u7ba1\u7406\uff0c\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u8fd1\u671f\u5df2\u6709\u653b\u51fb\u8005\u5229\u7528\u6b64\u6f0f\u6d1e\u653b\u51fb\u751f\u4ea7\u7cfb\u7edf\u3002\n\n\u6211\u4eec\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8ba1\u5212\u6ce8\u5165\u201d\u7684\u65b0\u578b\u4e0a\u4e0b\u6587\u64cd\u7eb5\u653b\u51fb\uff0c\u901a\u8fc7\u9488\u5bf9\u8fd9\u4e00\u8106\u5f31\u4e0a\u4e0b\u6587\uff0c\u7834\u574f\u4ee3\u7406\u7684\u5185\u90e8\u4efb\u52a1\u8868\u793a\u3002\u901a\u8fc7\u5bf9\u4e24\u79cd\u6d41\u884c\u7684\u7f51\u9875\u4ee3\u7406\uff08Browser-use\u548cAgent-E\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6211\u4eec\u53d1\u73b0\u8ba1\u5212\u6ce8\u5165\u80fd\u591f\u7ed5\u8fc7\u5f3a\u5927\u7684\u63d0\u793a\u6ce8\u5165\u9632\u5fa1\u673a\u5236\uff0c\u653b\u51fb\u6210\u529f\u7387\u6bd4\u540c\u7c7b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u9ad8\u51fa3\u500d\u3002\u6b64\u5916\uff0c\u201c\u4e0a\u4e0b\u6587\u94fe\u5f0f\u6ce8\u5165\u201d\u901a\u8fc7\u5728\u5408\u6cd5\u7528\u6237\u76ee\u6807\u548c\u653b\u51fb\u8005\u76ee\u6807\u4e4b\u95f4\u6784\u5efa\u903b\u8f91\u6865\u6881\uff0c\u4f7f\u9690\u79c1\u6cc4\u9732\u4efb\u52a1\u7684\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u4e8617.7%\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5b89\u5168\u5185\u5b58\u5904\u7406\u5fc5\u987b\u6210\u4e3a\u4ee3\u7406\u7cfb\u7edf\u7684\u9996\u8981\u5173\u6ce8\u70b9\u3002"}}
{"id": "2506.18204", "pdf": "https://arxiv.org/pdf/2506.18204", "abs": "https://arxiv.org/abs/2506.18204", "authors": ["Youjie Zhou", "Guofeng Mei", "Yiming Wang", "Yi Wan", "Fabio Poiesi"], "title": "Multimodal Fusion SLAM with Fourier Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual SLAM is particularly challenging in environments affected by noise,\nvarying lighting conditions, and darkness. Learning-based optical flow\nalgorithms can leverage multiple modalities to address these challenges, but\ntraditional optical flow-based visual SLAM approaches often require significant\ncomputational resources.To overcome this limitation, we propose FMF-SLAM, an\nefficient multimodal fusion SLAM method that utilizes fast Fourier transform\n(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel\nFourier-based self-attention and cross-attention mechanism to extract features\nfrom RGB and depth signals. We further enhance the interaction of multimodal\nfeatures by incorporating multi-scale knowledge distillation across modalities.\nWe also demonstrate the practical feasibility of FMF-SLAM in real-world\nscenarios with real time performance by integrating it with a security robot by\nfusing with a global positioning module GNSS-RTK and global Bundle Adjustment.\nOur approach is validated using video sequences from TUM, TartanAir, and our\nreal-world datasets, showcasing state-of-the-art performance under noisy,\nvarying lighting, and dark conditions.Our code and datasets are available at\nhttps://github.com/youjie-zhou/FMF-SLAM.git.", "AI": {"tldr": "FMF-SLAM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408SLAM\u65b9\u6cd5\uff0c\u5229\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u63d0\u5347\u7b97\u6cd5\u6548\u7387\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u81ea\u6ce8\u610f\u529b\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6RGB\u4e0e\u6df1\u5ea6\u4fe1\u53f7\u7279\u5f81\uff0c\u5e76\u5728\u566a\u58f0\u3001\u5149\u7167\u53d8\u5316\u548c\u9ed1\u6697\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5149\u6d41\u7684\u89c6\u89c9SLAM\u65b9\u6cd5\u5728\u566a\u58f0\u3001\u5149\u7167\u53d8\u5316\u548c\u9ed1\u6697\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faFMF-SLAM\uff0c\u91c7\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u63d0\u5347\u6548\u7387\uff0c\u5f15\u5165\u5085\u91cc\u53f6\u81ea\u6ce8\u610f\u529b\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6RGB\u4e0e\u6df1\u5ea6\u4fe1\u53f7\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728TUM\u3001TartanAir\u53ca\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cFMF-SLAM\u5728\u566a\u58f0\u3001\u5149\u7167\u53d8\u5316\u548c\u9ed1\u6697\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5177\u5907\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "FMF-SLAM\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u5085\u91cc\u53f6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9SLAM\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "paper_title_zh": "\u57fa\u4e8e\u5085\u91cc\u53f6\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u878d\u5408SLAM", "abstract_zh": "\u89c6\u89c9SLAM\u5728\u566a\u58f0\u3001\u5149\u7167\u53d8\u5316\u548c\u9ed1\u6697\u73af\u5883\u4e2d\u5c24\u4e3a\u56f0\u96be\u3002\u57fa\u4e8e\u5b66\u4e60\u7684\u5149\u6d41\u7b97\u6cd5\u53ef\u4ee5\u5229\u7528\u591a\u6a21\u6001\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8e\u5149\u6d41\u7684\u89c6\u89c9SLAM\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u4e3a\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51faFMF-SLAM\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408SLAM\u65b9\u6cd5\uff0c\u5229\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u63d0\u5347\u7b97\u6cd5\u6548\u7387\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u81ea\u6ce8\u610f\u529b\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4eceRGB\u548c\u6df1\u5ea6\u4fe1\u53f7\u4e2d\u63d0\u53d6\u7279\u5f81\u3002\u901a\u8fc7\u8de8\u6a21\u6001\u7684\u591a\u5c3a\u5ea6\u77e5\u8bc6\u84b8\u998f\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u7279\u5f81\u7684\u4ea4\u4e92\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c06FMF-SLAM\u4e0e\u5b89\u5168\u673a\u5668\u4eba\u96c6\u6210\uff0c\u878d\u5408\u5168\u7403\u5b9a\u4f4d\u6a21\u5757GNSS-RTK\u548c\u5168\u5c40Bundle Adjustment\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728TUM\u3001TartanAir\u53ca\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5728\u566a\u58f0\u3001\u5149\u7167\u53d8\u5316\u548c\u9ed1\u6697\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728https://github.com/youjie-zhou/FMF-SLAM.git\u83b7\u53d6\u3002"}}
{"id": "2506.17323", "pdf": "https://arxiv.org/pdf/2506.17323", "abs": "https://arxiv.org/abs/2506.17323", "authors": ["Tamas Bisztray", "Bilel Cherif", "Richard A. Dubniczky", "Nils Gruschka", "Bertalan Borsos", "Mohamed Amine Ferrag", "Attila Kovacs", "Vasileios Mavroeidis", "Norbert Tihanyi"], "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCodeT5-Authorship\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u7531\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684C\u7a0b\u5e8f\u4ee3\u7801\u7684\u4f5c\u8005\u3002\u901a\u8fc7\u5f15\u5165LLM-AuthorBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u578b\u5728\u4e8c\u8fdb\u5236\u548c\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523097.56%\u548c95.40%\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u65e5\u76ca\u666e\u904d\uff0c\u8bc6\u522b\u4ee3\u7801\u80cc\u540e\u7684\u5177\u4f53\u6a21\u578b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u65b0\u5174\u7814\u7a76\u6311\u6218\uff0c\u4e3aLLM\u751f\u6210\u7684\u4ee3\u7801\u63d0\u4f9b\u4f5c\u8005\u5f52\u5c5e\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51faCodeT5-Authorship\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528CodeT5\u7684\u7f16\u7801\u5668\u5c42\uff0c\u5e76\u901a\u8fc7\u4e24\u5c42\u5206\u7c7b\u5934\u751f\u6210\u4f5c\u8005\u6982\u7387\u5206\u5e03\u3002\u4f7f\u7528LLM-AuthorBench\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b32,000\u4e2aC\u7a0b\u5e8f\uff09\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e0e\u591a\u79cd\u4f20\u7edf\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u4e8c\u8fdb\u5236\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u533a\u5206GPT-4.1\u548cGPT-4o\u751f\u6210\u7684\u4ee3\u7801\u51c6\u786e\u7387\u8fbe97.56%\uff1b\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5bf9\u4e94\u79cd\u4e3b\u6d41LLM\u7684\u51c6\u786e\u7387\u4e3a95.40%\u3002", "conclusion": "CodeT5-Authorship\u6a21\u578b\u5728LLM\u751f\u6210\u7684\u4ee3\u7801\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u57fa\u51c6\u3002\u6240\u6709\u4ee3\u7801\u548c\u811a\u672c\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "\u6211\u77e5\u9053\u53bb\u5e74\u590f\u5929\u662f\u8c01\u7528LLM\u5199\u4e86\u4f60\u7684\u4ee3\u7801\uff1a\u57fa\u4e8e\u4ee3\u7801\u98ce\u683c\u5b66\u7684LLM\u751f\u6210\u4ee3\u7801\u4f5c\u8005\u5f52\u5c5e", "abstract_zh": "\u68c0\u6d4bAI\u751f\u6210\u7684\u4ee3\u7801\u3001\u6df1\u5ea6\u4f2a\u9020\u548c\u5176\u4ed6\u5408\u6210\u5185\u5bb9\u662f\u65b0\u5174\u7684\u7814\u7a76\u6311\u6218\u3002\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u4ee3\u7801\u65e5\u76ca\u666e\u904d\uff0c\u8bc6\u522b\u6bcf\u4e2a\u6837\u672c\u80cc\u540e\u7684\u5177\u4f53\u6a21\u578b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u751f\u6210\u7684C\u7a0b\u5e8f\u7684\u4f5c\u8005\u5f52\u5c5e\u95ee\u9898\u3002\u6211\u4eec\u53d1\u5e03\u4e86CodeT5-Authorship\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ec5\u4f7f\u7528\u539f\u59cbCodeT5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u7684\u7f16\u7801\u5668\u5c42\uff0c\u820d\u5f03\u89e3\u7801\u5668\u4ee5\u4e13\u6ce8\u4e8e\u5206\u7c7b\u4efb\u52a1\u3002\u6a21\u578b\u7684\u7f16\u7801\u5668\u8f93\u51fa\uff08\u9996\u4e2a\u6807\u8bb0\uff09\u901a\u8fc7\u4e00\u4e2a\u5e26\u6709GELU\u6fc0\u6d3b\u548cDropout\u7684\u4e24\u5c42\u5206\u7c7b\u5934\uff0c\u751f\u6210\u53ef\u80fd\u7684\u4f5c\u8005\u6982\u7387\u5206\u5e03\u3002\u4e3a\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6211\u4eec\u5f15\u5165\u4e86LLM-AuthorBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b32,000\u4e2a\u7531\u516b\u79cd\u5148\u8fdbLLM\u751f\u6210\u7684C\u7a0b\u5e8f\uff0c\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u3002\u6211\u4eec\u5c06\u6a21\u578b\u4e0e\u4e03\u79cd\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548c\u516b\u79cd\u5fae\u8c03\u7684Transformer\u6a21\u578b\uff08\u5305\u62ecBERT\u3001RoBERTa\u3001CodeBERT\u3001ModernBERT\u3001DistilBERT\u3001DeBERTa-V3\u3001Longformer\u548cLoRA\u5fae\u8c03\u7684Qwen2-1.5B\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u4e8c\u8fdb\u5236\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u533a\u5206GPT-4.1\u548cGPT-4o\u751f\u6210\u7684\u4ee3\u7801\u51c6\u786e\u7387\u8fbe97.56%\uff1b\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5bf9\u4e94\u79cd\u4e3b\u6d41LLM\uff08Gemini 2.5 Flash\u3001Claude 3.5 Haiku\u3001GPT-4.1\u3001Llama 3.3\u548cDeepSeek-V3\uff09\u7684\u51c6\u786e\u7387\u4e3a95.40%\u3002\u4e3a\u652f\u6301\u5f00\u653e\u79d1\u5b66\uff0c\u6211\u4eec\u5f00\u6e90\u4e86CodeT5-Authorship\u67b6\u6784\u3001LLM-AuthorBench\u57fa\u51c6\u6d4b\u8bd5\u53ca\u76f8\u5173Google Colab\u811a\u672c\uff1ahttps://github.com/LLMauthorbench/\u3002"}}
{"id": "2506.18208", "pdf": "https://arxiv.org/pdf/2506.18208", "abs": "https://arxiv.org/abs/2506.18208", "authors": ["Ankit Sanjyal"], "title": "Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction", "categories": ["cs.CV"], "comment": "5 pages, 1 table, 2 figures. First submission. Code available at:\n  \\url{https://github.com/ANKITSANJYAL/nerf-few-shot-limitations}", "summary": "Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction\nfrom sparse image collections. Recent work has explored integrating pre-trained\nvision features, particularly from DINO, to enhance few-shot reconstruction\ncapabilities. However, the effectiveness of such approaches remains unclear,\nespecially in extreme few-shot scenarios. In this paper, we present a\nsystematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,\nfrozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.\nSurprisingly, our experiments reveal that all DINO variants perform worse than\nthe baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the\nbaseline's 14.71. This counterintuitive result suggests that pre-trained vision\nfeatures may not be beneficial for few-shot 3D reconstruction and may even\nintroduce harmful biases. We analyze potential causes including feature-task\nmismatch, overfitting to limited data, and integration challenges. Our findings\nchallenge common assumptions in the field and suggest that simpler\narchitectures focusing on geometric consistency may be more effective for\nfew-shot scenarios.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86DINO\u589e\u5f3a\u7684NeRF\u6a21\u578b\u5728\u5c11\u6837\u672c3D\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\uff08\u5982DINO\uff09\u4e0d\u4ec5\u672a\u63d0\u5347\u6027\u80fd\uff0c\u53cd\u800c\u8868\u73b0\u66f4\u5dee\uff0c\u6311\u6218\u4e86\u8be5\u9886\u57df\u7684\u5e38\u89c1\u5047\u8bbe\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0cNeRF\u5728\u7a00\u758f\u56fe\u50cf\u96c6\u76843D\u573a\u666f\u91cd\u5efa\u4e2d\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\uff08\u5982DINO\uff09\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e9b\u7279\u5f81\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u57fa\u7ebfNeRF\u3001\u51bb\u7ed3DINO\u7279\u5f81\u3001LoRA\u5fae\u8c03\u7279\u5f81\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u7cfb\u7edf\u8bc4\u4f30DINO\u589e\u5f3aNeRF\u6a21\u578b\u5728\u5c11\u6837\u672c3D\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u6709DINO\u53d8\u4f53\u7684PSNR\u503c\uff0812.9-13.0\uff09\u5747\u4f4e\u4e8e\u57fa\u7ebfNeRF\uff0814.71\uff09\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u7279\u5f81\u53ef\u80fd\u5f15\u5165\u6709\u5bb3\u504f\u5dee\u3002", "conclusion": "\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u5bf9\u5c11\u6837\u672c3D\u91cd\u5efa\u65e0\u76ca\uff0c\u751a\u81f3\u53ef\u80fd\u6709\u5bb3\uff0c\u5efa\u8bae\u91c7\u7528\u66f4\u7b80\u5355\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u67b6\u6784\u3002", "paper_title_zh": "\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u5728\u5c11\u6837\u672c3D\u91cd\u5efa\u4e2d\u5bf9NeRF\u7684\u9650\u5236", "abstract_zh": "\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u901a\u8fc7\u7a00\u758f\u56fe\u50cf\u96c6\u5b9e\u73b0\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u9769\u547d\u6027\u8fdb\u5c55\u3002\u8fd1\u671f\u7814\u7a76\u63a2\u7d22\u4e86\u6574\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\uff08\u5c24\u5176\u662fDINO\uff09\u4ee5\u589e\u5f3a\u5c11\u6837\u672c\u91cd\u5efa\u80fd\u529b\uff0c\u4f46\u5176\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u5728\u6781\u7aef\u5c11\u6837\u672c\u573a\u666f\u4e2d\u3002\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86DINO\u589e\u5f3a\u7684NeRF\u6a21\u578b\uff0c\u5bf9\u6bd4\u4e86\u57fa\u7ebfNeRF\u3001\u51bb\u7ed3DINO\u7279\u5f81\u3001LoRA\u5fae\u8c03\u7279\u5f81\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002\u51fa\u4e4e\u610f\u6599\u7684\u662f\uff0c\u5b9e\u9a8c\u8868\u660e\u6240\u6709DINO\u53d8\u4f53\u8868\u73b0\u5747\u900a\u4e8e\u57fa\u7ebfNeRF\uff0cPSNR\u503c\u7ea6\u4e3a12.9\u81f313.0\uff0c\u800c\u57fa\u7ebf\u4e3a14.71\u3002\u8fd9\u4e00\u53cd\u76f4\u89c9\u7684\u7ed3\u679c\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u53ef\u80fd\u5bf9\u5c11\u6837\u672c3D\u91cd\u5efa\u65e0\u76ca\uff0c\u751a\u81f3\u5f15\u5165\u6709\u5bb3\u504f\u5dee\u3002\u6211\u4eec\u5206\u6790\u4e86\u6f5c\u5728\u539f\u56e0\uff0c\u5305\u62ec\u7279\u5f81\u4e0e\u4efb\u52a1\u4e0d\u5339\u914d\u3001\u5bf9\u6709\u9650\u6570\u636e\u7684\u8fc7\u62df\u5408\u4ee5\u53ca\u6574\u5408\u6311\u6218\u3002\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u8be5\u9886\u57df\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u5e76\u8868\u660e\u4e13\u6ce8\u4e8e\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u66f4\u7b80\u5355\u67b6\u6784\u53ef\u80fd\u66f4\u9002\u5408\u5c11\u6837\u672c\u573a\u666f\u3002"}}
{"id": "2506.18209", "pdf": "https://arxiv.org/pdf/2506.18209", "abs": "https://arxiv.org/abs/2506.18209", "authors": ["Zhisen Hu", "Dominic Cullen", "Peter Thompson", "David Johnson", "Chang Bian", "Aleksei Tiulpin", "Timothy Cootes", "Claudia Lindner"], "title": "Deep Learning-based Alignment Measurement in Knee Radiographs", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to MICCAI 2025", "summary": "Radiographic knee alignment (KA) measurement is important for predicting\njoint health and surgical outcomes after total knee replacement. Traditional\nmethods for KA measurements are manual, time-consuming and require long-leg\nradiographs. This study proposes a deep learning-based method to measure KA in\nanteroposterior knee radiographs via automatically localized knee anatomical\nlandmarks. Our method builds on hourglass networks and incorporates an\nattention gate structure to enhance robustness and focus on key anatomical\nfeatures. To our knowledge, this is the first deep learning-based method to\nlocalize over 100 knee anatomical landmarks to fully outline the knee shape\nwhile integrating KA measurements on both pre-operative and post-operative\nimages. It provides highly accurate and reliable anatomical varus/valgus KA\nmeasurements using the anatomical tibiofemoral angle, achieving mean absolute\ndifferences ~1{\\deg} when compared to clinical ground truth measurements.\nAgreement between automated and clinical measurements was excellent\npre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good\npost-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can\nbe automated with high accuracy, creating opportunities for digitally enhanced\nclinical workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u819d\u5173\u8282\u524d\u540e\u4f4dX\u5149\u7247\u4e2d\u81ea\u52a8\u5b9a\u4f4d\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u5e76\u6d4b\u91cf\u819d\u5173\u8282\u5bf9\u7ebf\uff08KA\uff09\uff0c\u4e0e\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u819d\u5173\u8282\u5bf9\u7ebf\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u8017\u65f6\u4e14\u9700\u8981\u957f\u817fX\u5149\u7247\u3002\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u9ad8\u7cbe\u5ea6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u6d4b\u91cf\u819d\u5173\u8282\u5bf9\u7ebf\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6c99\u6f0f\u7f51\u7edc\uff08hourglass networks\uff09\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u95e8\u7ed3\u6784\uff0c\u81ea\u52a8\u5b9a\u4f4d\u8d85\u8fc7100\u4e2a\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\uff0c\u5168\u9762\u52fe\u52d2\u819d\u5173\u8282\u5f62\u72b6\uff0c\u5e76\u6574\u5408\u672f\u524d\u548c\u672f\u540e\u56fe\u50cf\u7684KA\u6d4b\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u89e3\u5256\u80eb\u80a1\u89d2\u6d4b\u91cf\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u4e0e\u4e34\u5e8a\u771f\u5b9e\u6d4b\u91cf\u7ed3\u679c\u7684\u7edd\u5bf9\u5e73\u5747\u5dee\u5f02\u7ea6\u4e3a1\u5ea6\u3002\u672f\u524d\u548c\u672f\u540e\u81ea\u52a8\u6d4b\u91cf\u4e0e\u4e34\u5e8a\u6d4b\u91cf\u7684\u4e00\u81f4\u6027\u5206\u522b\u4e3a\u4f18\u5f02\uff08ICC=0.97\uff09\u548c\u826f\u597d\uff08ICC=0.86\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u819d\u5173\u8282\u5bf9\u7ebf\u8bc4\u4f30\u53ef\u4ee5\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5316\uff0c\u4e3a\u6570\u5b57\u5316\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "paper_title_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u819d\u5173\u8282X\u5149\u7247\u5bf9\u7ebf\u6d4b\u91cf", "abstract_zh": "\u819d\u5173\u8282\u5bf9\u7ebf\uff08KA\uff09\u7684X\u5149\u6d4b\u91cf\u5bf9\u4e8e\u9884\u6d4b\u5173\u8282\u5065\u5eb7\u548c\u5168\u819d\u5173\u8282\u7f6e\u6362\u672f\u540e\u6548\u679c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4f20\u7edf\u7684KA\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u8017\u65f6\u4e14\u9700\u8981\u957f\u817fX\u5149\u7247\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5b9a\u4f4d\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\uff0c\u5728\u524d\u540e\u4f4d\u819d\u5173\u8282X\u5149\u7247\u4e2d\u6d4b\u91cfKA\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6c99\u6f0f\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u95e8\u7ed3\u6784\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u5e76\u805a\u7126\u5173\u952e\u89e3\u5256\u7279\u5f81\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9a\u4f4d\u8d85\u8fc7100\u4e2a\u819d\u5173\u8282\u89e3\u5256\u6807\u5fd7\u4ee5\u5168\u9762\u52fe\u52d2\u819d\u5173\u8282\u5f62\u72b6\uff0c\u5e76\u6574\u5408\u672f\u524d\u548c\u672f\u540e\u56fe\u50cf\u7684KA\u6d4b\u91cf\u3002\u901a\u8fc7\u89e3\u5256\u80eb\u80a1\u89d2\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u51c6\u786e\u548c\u53ef\u9760\u7684\u89e3\u5256\u5185\u7ffb/\u5916\u7ffbKA\u6d4b\u91cf\uff0c\u4e0e\u4e34\u5e8a\u771f\u5b9e\u6d4b\u91cf\u7ed3\u679c\u7684\u7edd\u5bf9\u5e73\u5747\u5dee\u5f02\u7ea6\u4e3a1\u5ea6\u3002\u81ea\u52a8\u6d4b\u91cf\u4e0e\u4e34\u5e8a\u6d4b\u91cf\u7684\u4e00\u81f4\u6027\u5728\u672f\u524d\u8868\u73b0\u4f18\u5f02\uff08\u7ec4\u5185\u76f8\u5173\u7cfb\u6570ICC=0.97\uff09\uff0c\u672f\u540e\u8868\u73b0\u826f\u597d\uff08ICC=0.86\uff09\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cKA\u8bc4\u4f30\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5316\uff0c\u4e3a\u6570\u5b57\u5316\u589e\u5f3a\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u521b\u9020\u4e86\u673a\u4f1a\u3002"}}
{"id": "2506.17329", "pdf": "https://arxiv.org/pdf/2506.17329", "abs": "https://arxiv.org/abs/2506.17329", "authors": ["Pedro H. Lui", "Lucas P. Siqueira", "Juliano F. Kazienko", "Vagner E. Quincozes", "Silvio E. Quincozes", "Daniel Welfer"], "title": "On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 7 figures, conference", "summary": "Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of\nThings (IoT), real-time monitoring, and human-centered design toward\npersonalized medicine and predictive diagnostics. However, the increasing\nreliance on interconnected medical technologies exposes them to cyber threats.\nMeanwhile, current AI-driven cybersecurity models often neglect biomedical\ndata, limiting their effectiveness and interpretability. This study addresses\nthis gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that\nintegrates network traffic and biomedical sensor data. Classification outputs\nindicate that XGBoost achieved 99% F1-score for benign and data alteration, and\n81% for spoofing. Explainability findings reveal that network data play a\ndominant role in intrusion detection whereas biomedical features contributed to\nspoofing detection, with temperature reaching a Shapley values magnitude of\n0.37.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728Healthcare 5.0\u4e2d\u7ed3\u5408\u7f51\u7edc\u6d41\u91cf\u548c\u751f\u7269\u533b\u5b66\u6570\u636e\u7684\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6a21\u578bXGBoost\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u751f\u7269\u533b\u5b66\u7279\u5f81\u5728\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "Healthcare 5.0\u4f9d\u8d56AI\u548c\u7269\u8054\u7f51\u6280\u672f\uff0c\u4f46\u73b0\u6709\u7f51\u7edc\u5b89\u5168\u6a21\u578b\u5e38\u5ffd\u7565\u751f\u7269\u533b\u5b66\u6570\u636e\uff0c\u5bfc\u81f4\u6548\u679c\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u7ed3\u5408\u7f51\u7edc\u548c\u751f\u7269\u533b\u5b66\u6570\u636e\u7684\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6a21\u578bXGBoost\uff0c\u5206\u6790\u6574\u5408\u4e86\u7f51\u7edc\u6d41\u91cf\u548c\u751f\u7269\u533b\u5b66\u4f20\u611f\u5668\u6570\u636e\u7684Healthcare 5.0\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5176\u5728\u5165\u4fb5\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "result": "XGBoost\u5728\u826f\u6027\u6570\u636e\u7be1\u6539\u68c0\u6d4b\u4e2dF1-score\u8fbe99%\uff0c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u4e3a81%\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u663e\u793a\u7f51\u7edc\u6570\u636e\u5728\u5165\u4fb5\u68c0\u6d4b\u4e2d\u5360\u4e3b\u5bfc\uff0c\u800c\u751f\u7269\u533b\u5b66\u7279\u5f81\uff08\u5982\u6e29\u5ea6\uff09\u5728\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8d21\u732e\u663e\u8457\uff08Shapley\u503c\u8fbe0.37\uff09\u3002", "conclusion": "\u7ed3\u5408\u7f51\u7edc\u548c\u751f\u7269\u533b\u5b66\u6570\u636e\u7684XAI\u6a21\u578b\u5728Healthcare 5.0\u5165\u4fb5\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u7269\u533b\u5b66\u7279\u5f81\u5bf9\u6b3a\u9a97\u68c0\u6d4b\u5c24\u4e3a\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u5173\u4e8e\u7f51\u7edc\u751f\u7269\u533b\u5b66\u7279\u5f81\u5728Healthcare 5.0\u5165\u4fb5\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u7814\u7a76", "abstract_zh": "Healthcare 5.0\u6574\u5408\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u3001\u7269\u8054\u7f51\uff08IoT\uff09\u3001\u5b9e\u65f6\u76d1\u6d4b\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\uff0c\u65e8\u5728\u5b9e\u73b0\u4e2a\u6027\u5316\u533b\u7597\u548c\u9884\u6d4b\u6027\u8bca\u65ad\u3002\u7136\u800c\uff0c\u5bf9\u4e92\u8054\u533b\u7597\u6280\u672f\u7684\u4f9d\u8d56\u589e\u52a0\u4f7f\u5176\u9762\u4e34\u7f51\u7edc\u5a01\u80c1\u3002\u76ee\u524d\u57fa\u4e8eAI\u7684\u7f51\u7edc\u5b89\u5168\u6a21\u578b\u5e38\u5ffd\u7565\u751f\u7269\u533b\u5b66\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6548\u679c\u548c\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5e94\u7528\u4e8e\u6574\u5408\u7f51\u7edc\u6d41\u91cf\u548c\u751f\u7269\u533b\u5b66\u4f20\u611f\u5668\u6570\u636e\u7684Healthcare 5.0\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u5206\u7c7b\u7ed3\u679c\u663e\u793a\uff0cXGBoost\u5728\u826f\u6027\u6570\u636e\u7be1\u6539\u68c0\u6d4b\u4e2dF1-score\u8fbe99%\uff0c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u4e3a81%\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8868\u660e\uff0c\u7f51\u7edc\u6570\u636e\u5728\u5165\u4fb5\u68c0\u6d4b\u4e2d\u5360\u4e3b\u5bfc\uff0c\u800c\u751f\u7269\u533b\u5b66\u7279\u5f81\uff08\u5982\u6e29\u5ea6\uff09\u5728\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8d21\u732e\u663e\u8457\uff08Shapley\u503c\u8fbe0.37\uff09\u3002"}}
{"id": "2506.17673", "pdf": "https://arxiv.org/pdf/2506.17673", "abs": "https://arxiv.org/abs/2506.17673", "authors": ["Seonglae Cho", "Harryn Oh", "Donghyun Lee", "Luis Eduardo Rodrigues Vieira", "Andrew Bermingham", "Ziad El Sayed"], "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "18 pages, 18 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.", "AI": {"tldr": "FaithfulSAE\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSAE\u56e0\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u96c6\u800c\u5bfc\u81f4\u7684\u7279\u5f81\u4e0d\u7a33\u5b9a\u548c\u865a\u5047\u7279\u5f81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u6355\u83b7\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5728\u8bad\u7ec3\u65f6\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u96c6\uff08\u5982\u7f51\u7edc\u6570\u636e\u6216\u5176\u4ed6\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u7279\u5f81\u4e0d\u7a33\u5b9a\u6216\u6355\u83b7\u865a\u5047\u7279\u5f81\uff08Fake Features\uff09\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86FaithfulSAE\u65b9\u6cd5\u3002", "method": "FaithfulSAE\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3SAE\uff0c\u907f\u514d\u4e86\u5916\u90e8\u6570\u636e\u96c6\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u7a33\u5b9a\u5730\u6355\u83b7\u6a21\u578b\u5185\u90e8\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFaithfulSAE\u57285/7\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u865a\u5047\u7279\u5f81\u6bd4\u4f8b\uff0c\u5e76\u5728SAE\u63a2\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u7f51\u7edc\u6570\u636e\u8bad\u7ec3\u7684SAE\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u521d\u59cb\u5316\u79cd\u5b50\u4e0b\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "FaithfulSAE\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5bf9\u6a21\u578b\u5185\u90e8\u7279\u5f81\u7684\u6355\u83b7\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002", "paper_title_zh": "FaithfulSAE\uff1a\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6355\u83b7\u771f\u5b9e\u7279\u5f81", "abstract_zh": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u5206\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u5f81\u4e3a\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002\u7136\u800c\uff0cPaulo\u548cBelrose\uff082025\uff09\u6307\u51fa\u4e0d\u540c\u521d\u59cb\u5316\u79cd\u5b50\u4e0bSAE\u7684\u4e0d\u7a33\u5b9a\u6027\uff0cHeap\u7b49\u4eba\uff082025\uff09\u5219\u53d1\u73b0SAE\u53ef\u80fd\u65e0\u6cd5\u6355\u83b7\u6a21\u578b\u5185\u90e8\u7279\u5f81\u3002\u8fd9\u4e9b\u95ee\u9898\u53ef\u80fd\u6e90\u4e8eSAE\u5728\u5916\u90e8\u6570\u636e\u96c6\uff08\u5982\u7f51\u7edc\u6570\u636e\u6216\u5176\u4ed6\u6a21\u578b\u751f\u6210\u6570\u636e\uff09\u4e0a\u7684\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u5305\u542b\u8d85\u51fa\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\uff0c\u5bfc\u81f4\u865a\u5047SAE\u7279\u5f81\uff08\u5373\u201c\u865a\u5047\u7279\u5f81\u201d\uff09\uff0c\u4ece\u800c\u9519\u8bef\u8868\u5f81\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faFaithfulSAE\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u5408\u6210\u7684\u6570\u636e\u96c6\u8bad\u7ec3SAE\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8f83\u5c11OOD\u7684\u6307\u4ee4\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684FaithfulSAE\u5728\u4e0d\u540c\u79cd\u5b50\u4e0b\u8868\u73b0\u66f4\u7a33\u5b9a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cFaithfulSAE\u5728SAE\u63a2\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u7f51\u7edc\u6570\u636e\u8bad\u7ec3\u7684SAE\uff0c\u5e76\u57285/7\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u865a\u5047\u7279\u5f81\u6bd4\u4f8b\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u66f4\u597d\u5730\u6355\u83b7\u6a21\u578b\u5185\u90e8\u7279\u5f81\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86SAE\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.18217", "pdf": "https://arxiv.org/pdf/2506.18217", "abs": "https://arxiv.org/abs/2506.18217", "authors": ["Kazuma Kitazawa", "Tsuyoshi Takatani"], "title": "Shape from Polarization of Thermal Emission and Reflection", "categories": ["cs.CV"], "comment": "ICCP2025", "summary": "Shape estimation for transparent objects is challenging due to their complex\nlight transport. To circumvent these difficulties, we leverage the Shape from\nPolarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where\nmost materials are opaque and emissive. While a few prior studies have explored\nLWIR SfP, these attempts suffered from significant errors due to inadequate\npolarimetric modeling, particularly the neglect of reflection. Addressing this\ngap, we formulated a polarization model that explicitly accounts for the\ncombined effects of emission and reflection. Based on this model, we estimated\nsurface normals using not only a direct model-based method but also a\nlearning-based approach employing a neural network trained on a\nphysically-grounded synthetic dataset. Furthermore, we modeled the LWIR\npolarimetric imaging process, accounting for inherent systematic errors to\nensure accurate polarimetry. We implemented a prototype system and created\nThermoPol, the first real-world benchmark dataset for LWIR SfP. Through\ncomprehensive experiments, we demonstrated the high accuracy and broad\napplicability of our method across various materials, including those\ntransparent in the visible spectrum.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u957f\u6ce2\u7ea2\u5916\uff08LWIR\uff09\u504f\u632f\u7684\u5f62\u72b6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u53d1\u5c04\u548c\u53cd\u5c04\u7684\u504f\u632f\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u900f\u660e\u7269\u4f53\u5f62\u72b6\u4f30\u8ba1\u7684\u96be\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754cLWIR\u504f\u632f\u6570\u636e\u96c6ThermoPol\u3002", "motivation": "\u900f\u660e\u7269\u4f53\u7684\u5f62\u72b6\u4f30\u8ba1\u56e0\u590d\u6742\u7684\u5149\u4f20\u8f93\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u6ce2\u7ea2\u5916\u5149\u8c31\u4e2d\u56e0\u5ffd\u7565\u53cd\u5c04\u6548\u5e94\u5bfc\u81f4\u8bef\u5dee\u8f83\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u504f\u632f\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u632f\u6a21\u578b\uff0c\u660e\u786e\u8003\u8651\u4e86\u53d1\u5c04\u548c\u53cd\u5c04\u7684\u8054\u5408\u6548\u5e94\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u548c\u57fa\u4e8e\u5b66\u4e60\uff08\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u65b9\u6cd5\u4f30\u8ba1\u8868\u9762\u6cd5\u7ebf\u3002\u540c\u65f6\uff0c\u5efa\u7acb\u4e86LWIR\u504f\u632f\u6210\u50cf\u8fc7\u7a0b\u7684\u6a21\u578b\u4ee5\u6d88\u9664\u7cfb\u7edf\u8bef\u5dee\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6750\u6599\uff08\u5305\u62ec\u53ef\u89c1\u5149\u900f\u660e\u7684\u6750\u6599\uff09\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754cLWIR\u504f\u632f\u6570\u636e\u96c6ThermoPol\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u957f\u6ce2\u7ea2\u5916\u504f\u632f\u5f62\u72b6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u900f\u660e\u7269\u4f53\u7684\u5f62\u72b6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u70ed\u53d1\u5c04\u4e0e\u53cd\u5c04\u504f\u632f\u7684\u5f62\u72b6\u4f30\u8ba1", "abstract_zh": "\u900f\u660e\u7269\u4f53\u7684\u5f62\u72b6\u4f30\u8ba1\u56e0\u5176\u590d\u6742\u7684\u5149\u4f20\u8f93\u7279\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5229\u7528\u957f\u6ce2\u7ea2\u5916\uff08LWIR\uff09\u5149\u8c31\u4e2d\u7684\u504f\u632f\u5f62\u72b6\u4f30\u8ba1\uff08SfP\uff09\u6280\u672f\uff0c\u5176\u4e2d\u5927\u591a\u6570\u6750\u6599\u662f\u4e0d\u900f\u660e\u4e14\u5177\u6709\u53d1\u5c04\u6027\u7684\u3002\u5c3d\u7ba1\u5df2\u6709\u5c11\u6570\u7814\u7a76\u63a2\u7d22\u4e86LWIR SfP\uff0c\u4f46\u8fd9\u4e9b\u5c1d\u8bd5\u56e0\u504f\u632f\u5efa\u6a21\u4e0d\u8db3\uff08\u5c24\u5176\u662f\u5ffd\u7565\u4e86\u53cd\u5c04\u6548\u5e94\uff09\u800c\u5b58\u5728\u663e\u8457\u8bef\u5dee\u3002\u9488\u5bf9\u8fd9\u4e00\u7f3a\u9677\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u632f\u6a21\u578b\uff0c\u660e\u786e\u8003\u8651\u4e86\u53d1\u5c04\u548c\u53cd\u5c04\u7684\u8054\u5408\u6548\u5e94\u3002\u57fa\u4e8e\u8be5\u6a21\u578b\uff0c\u6211\u4eec\u4e0d\u4ec5\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u4f30\u8ba1\u8868\u9762\u6cd5\u7ebf\uff0c\u8fd8\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5728\u7269\u7406\u57fa\u7840\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5efa\u6a21\u4e86LWIR\u504f\u632f\u6210\u50cf\u8fc7\u7a0b\uff0c\u4ee5\u6d88\u9664\u56fa\u6709\u7684\u7cfb\u7edf\u8bef\u5dee\uff0c\u786e\u4fdd\u504f\u632f\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754cLWIR SfP\u57fa\u51c6\u6570\u636e\u96c6ThermoPol\u3002\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6750\u6599\uff08\u5305\u62ec\u53ef\u89c1\u5149\u900f\u660e\u7684\u6750\u6599\uff09\u4e0a\u7684\u9ad8\u7cbe\u5ea6\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.17686", "pdf": "https://arxiv.org/pdf/2506.17686", "abs": "https://arxiv.org/abs/2506.17686", "authors": ["Alican Gok", "Oguzhan Buyuksolak", "Osman Erman Okman", "Murat Saraclar"], "title": "Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "To be submitted to IEEE Signal Processing Letters, 5 pages, 3 figures", "summary": "Keyword Spotting plays a critical role in enabling hands-free interaction for\nbattery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the\nscalability and adaptability challenges of traditional systems by enabling\nrecognition of custom keywords with only a few examples. However, existing\nFS-KWS systems achieve subpar accuracy at desirable false acceptance rates,\nparticularly in resource-constrained edge environments. To address these\nissues, we propose a training scheme that leverages self-supervised learning\nmodels for robust feature extraction, dimensionality reduction, and knowledge\ndistillation. The teacher model, based on Wav2Vec 2.0 is trained using\nSub-center ArcFace loss, which enhances inter-class separability and\nintra-class compactness. To enable efficient deployment on edge devices, we\nintroduce attention-based dimensionality reduction and train a standard\nlightweight ResNet15 student model. We evaluate the proposed approach on the\nEnglish portion of the Multilingual Spoken Words Corpus (MSWC) and the Google\nSpeech Commands (GSC) datasets. Notably, the proposed training method improves\nthe 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%\nfalse alarm accuracy on the GSC dataset, thus making it significantly\nbetter-suited for a real use case scenario.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u3001\u964d\u7ef4\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\uff08FS-KWS\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u7cfb\u7edf\u5728\u4f4e\u8bef\u62a5\u7387\u4e0b\u51c6\u786e\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8eWav2Vec 2.0\u7684\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u7528Sub-center ArcFace\u635f\u5931\u589e\u5f3a\u7c7b\u95f4\u5206\u79bb\u6027\u548c\u7c7b\u5185\u7d27\u51d1\u6027\uff1b\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7ResNet15\u5b66\u751f\u6a21\u578b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "result": "\u5728GSC\u6570\u636e\u96c6\u4e0a\uff0c10\u6837\u672c\u5206\u7c7b\u51c6\u786e\u7387\u4ece33.4%\u63d0\u5347\u81f374.1%\uff0811\u7c7b\uff0c1%\u8bef\u62a5\u7387\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\u573a\u666f\u3002", "paper_title_zh": "\u901a\u8fc7\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u63d0\u5347\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u6027\u80fd", "abstract_zh": "\u5173\u952e\u8bcd\u68c0\u6d4b\u5728\u7535\u6c60\u4f9b\u7535\u7684\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5b9e\u73b0\u514d\u63d0\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\uff08FS-KWS\uff09\u901a\u8fc7\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\u8bc6\u522b\u81ea\u5b9a\u4e49\u5173\u952e\u8bcd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u6311\u6218\u3002\u7136\u800c\uff0c\u73b0\u6709FS-KWS\u7cfb\u7edf\u5728\u7406\u60f3\u8bef\u62a5\u7387\u4e0b\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65b9\u6848\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9c81\u68d2\u7279\u5f81\u63d0\u53d6\u3001\u964d\u7ef4\u548c\u77e5\u8bc6\u84b8\u998f\u3002\u57fa\u4e8eWav2Vec 2.0\u7684\u6559\u5e08\u6a21\u578b\u4f7f\u7528Sub-center ArcFace\u635f\u5931\u8bad\u7ec3\uff0c\u589e\u5f3a\u4e86\u7c7b\u95f4\u5206\u79bb\u6027\u548c\u7c7b\u5185\u7d27\u51d1\u6027\u3002\u4e3a\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u7684\u9ad8\u6548\u90e8\u7f72\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u5e76\u8bad\u7ec3\u4e86\u8f7b\u91cf\u7ea7ResNet15\u5b66\u751f\u6a21\u578b\u3002\u6211\u4eec\u5728\u591a\u8bed\u8a00\u53e3\u8bed\u8bcd\u5e93\uff08MSWC\uff09\u7684\u82f1\u8bed\u90e8\u5206\u548cGoogle\u8bed\u97f3\u547d\u4ee4\uff08GSC\uff09\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u5728GSC\u6570\u636e\u96c6\u4e0a\u5c0610\u6837\u672c\u5206\u7c7b\u51c6\u786e\u7387\u4ece33.4%\u63d0\u5347\u81f374.1%\uff0811\u7c7b\uff0c1%\u8bef\u62a5\u7387\uff09\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.18220", "pdf": "https://arxiv.org/pdf/2506.18220", "abs": "https://arxiv.org/abs/2506.18220", "authors": ["Berk Yilmaz", "Aniruddh Aiyengar"], "title": "Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07", "I.2.6; I.5.1; J.3"], "comment": "15 pages, 10 figures. Berk Yilmaz and Aniruddh Aiyengar contributed\n  equally to this work", "summary": "Early and accurate identification of retinal ailments is crucial for averting\nocular decline; however, access to dependable diagnostic devices is not often\navailable in low-resourced settings. This project proposes to solve that by\ndeveloping a lightweight, edge-device deployable disease classifier using\ncross-architecture knowledge distilling. We first train a high-capacity vision\ntransformer (ViT) teacher model, pre-trained using I-JEPA self-supervised\nlearning, to classify fundus images into four classes: Normal, Diabetic\nRetinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus\nwhen compressing to a CNN-based student model for deployment in\nresource-limited conditions, such as the NVIDIA Jetson Nano. This was\naccomplished using a novel framework which included a Partitioned\nCross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a\nmulti-view robust training method. The teacher model has 97.4 percent more\nparameters than the student model, with it achieving 89 percent classification\nwith a roughly 93 percent retention of the teacher model's diagnostic\nperformance. The retention of clinical classification behavior supports our\nmethod's initial aim: compression of the ViT while retaining accuracy. Our work\nserves as an example of a scalable, AI-driven triage solution for retinal\ndisorders in under-resourced areas.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982NVIDIA Jetson Nano\uff09\u4e0a\u90e8\u7f72\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u3002\u901a\u8fc7\u9ad8\u5bb9\u91cfViT\u6559\u5e08\u6a21\u578b\u548cCNN\u5b66\u751f\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u8bca\u65ad\u6027\u80fd\u7684\u9ad8\u4fdd\u7559\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u89c6\u7f51\u819c\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u8bca\u65ad\u8bbe\u5907\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u75be\u75c5\u5206\u7c7b\u5668\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8eViT\u7684\u9ad8\u5bb9\u91cf\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u7528I-JEPA\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\uff0c\u5206\u7c7b\u773c\u5e95\u56fe\u50cf\u4e3a\u56db\u7c7b\uff08\u6b63\u5e38\u3001\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u9752\u5149\u773c\u548c\u767d\u5185\u969c\uff09\u3002\u7136\u540e\u901a\u8fc7\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u6a21\u578b\u538b\u7f29\u4e3aCNN\u5b66\u751f\u6a21\u578b\uff0c\u91c7\u7528\u5206\u533a\u4ea4\u53c9\u6ce8\u610f\u529b\u6295\u5f71\u5668\u3001\u5206\u7ec4\u7ebf\u6027\u6295\u5f71\u5668\u548c\u591a\u89c6\u56fe\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u6559\u5e08\u6a21\u578b\u53c2\u6570\u6bd4\u5b66\u751f\u6a21\u578b\u591a97.4%\uff0c\u5b66\u751f\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u4e3a89%\uff0c\u4fdd\u7559\u4e86\u6559\u5e08\u6a21\u578b93%\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86ViT\u6a21\u578b\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89c6\u7f51\u819c\u75be\u75c5AI\u5206\u8bca\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5728NVIDIA Jetson Nano\u4e0a\u7684\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b", "abstract_zh": "\u65e9\u671f\u51c6\u786e\u8bc6\u522b\u89c6\u7f51\u819c\u75be\u75c5\u5bf9\u9632\u6b62\u89c6\u529b\u4e0b\u964d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\uff0c\u53ef\u9760\u7684\u8bca\u65ad\u8bbe\u5907\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002\u672c\u9879\u76ee\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u75be\u75c5\u5206\u7c7b\u5668\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u91c7\u7528\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8eViT\u7684\u9ad8\u5bb9\u91cf\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u7528I-JEPA\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\uff0c\u5206\u7c7b\u773c\u5e95\u56fe\u50cf\u4e3a\u56db\u7c7b\uff08\u6b63\u5e38\u3001\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u9752\u5149\u773c\u548c\u767d\u5185\u969c\uff09\u3002\u5728\u538b\u7f29\u4e3aCNN\u5b66\u751f\u6a21\u578b\u65f6\uff0c\u4fdd\u6301\u4e86\u7269\u8054\u7f51\uff08IoT\uff09\u7684\u7126\u70b9\uff0c\u4ee5\u4fbf\u5728\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\uff08\u5982NVIDIA Jetson Nano\uff09\u4e0a\u90e8\u7f72\u3002\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5305\u62ec\u5206\u533a\u4ea4\u53c9\u6ce8\u610f\u529b\u6295\u5f71\u5668\u3001\u5206\u7ec4\u7ebf\u6027\u6295\u5f71\u5668\u548c\u591a\u89c6\u56fe\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\u3002\u6559\u5e08\u6a21\u578b\u53c2\u6570\u6bd4\u5b66\u751f\u6a21\u578b\u591a97.4%\uff0c\u5b66\u751f\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u4e3a89%\uff0c\u4fdd\u7559\u4e86\u6559\u5e08\u6a21\u578b93%\u7684\u8bca\u65ad\u6027\u80fd\u3002\u4e34\u5e8a\u5206\u7c7b\u884c\u4e3a\u7684\u4fdd\u7559\u652f\u6301\u4e86\u6211\u4eec\u7684\u521d\u59cb\u76ee\u6807\uff1a\u5728\u538b\u7f29ViT\u6a21\u578b\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u672c\u7814\u7a76\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u89c6\u7f51\u819c\u75be\u75c5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684AI\u5206\u8bca\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17335", "pdf": "https://arxiv.org/pdf/2506.17335", "abs": "https://arxiv.org/abs/2506.17335", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable potential in\nadvancing scientific discovery. However, their capability in the fundamental\nyet crucial task of reproducing code from research papers, especially in the\nNLP domain, remains underexplored. This task includes unique complex reasoning\nchallenges in the intellectual synthesis of abstract concepts and the\ncomprehension of code repositories with interdependent files. Motivated by this\ngap, we present LMR-BENCH, a benchmark designed to systematically evaluate the\ncapability of LLM agents on code reproduction from Language Modeling Research.\nIt consists of 28 code reproduction tasks derived from 23 research papers\npublished in top-tier NLP venues over the past five years, spanning nine\nfundamental categories. Models are provided with a research paper, a code\nrepository containing one or more masked functions, and instructions for\nimplementing these functions. We conduct extensive experiments in standard\nprompting and LLM agent settings with state-of-the-art LLMs, evaluating the\naccuracy of unit tests and performing LLM-based evaluation of code correctness.\nExperimental results reveal that even the most advanced models still exhibit\npersistent limitations in scientific reasoning and code synthesis, highlighting\ncritical gaps in LLM agents' ability to autonomously reproduce scientific\nresearch", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLMR-BENCH\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u590d\u73b0\u8bed\u8a00\u5efa\u6a21\u7814\u7a76\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u5408\u6210\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1LLM\u4ee3\u7406\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u590d\u73b0\u7814\u7a76\u8bba\u6587\u4ee3\u7801\uff08\u5c24\u5176\u662fNLP\u9886\u57df\uff09\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u8fd9\u4e00\u4efb\u52a1\u6d89\u53ca\u62bd\u8c61\u6982\u5ff5\u7684\u7efc\u5408\u7406\u89e3\u548c\u5bf9\u4ee3\u7801\u5e93\u7684\u590d\u6742\u63a8\u7406\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86LMR-BENCH\u57fa\u51c6\uff0c\u5305\u542b28\u4e2a\u4ee3\u7801\u590d\u73b0\u4efb\u52a1\uff0c\u6e90\u81ea\u8fc7\u53bb\u4e94\u5e74\u9876\u7ea7NLP\u4f1a\u8bae\u768423\u7bc7\u8bba\u6587\uff0c\u6db5\u76d6\u4e5d\u7c7b\u57fa\u7840\u4efb\u52a1\u3002\u6a21\u578b\u9700\u6839\u636e\u8bba\u6587\u548c\u90e8\u5206\u63a9\u7801\u7684\u4ee3\u7801\u5e93\u5b9e\u73b0\u7f3a\u5931\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u548cLLM\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684LLM\u5728\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u5408\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5b8c\u5168\u81ea\u4e3b\u590d\u73b0\u79d1\u5b66\u7814\u7a76\u3002", "conclusion": "LMR-BENCH\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u79d1\u5b66\u4ee3\u7801\u590d\u73b0\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "paper_title_zh": "LMR-BENCH\uff1a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u590d\u73b0\u8bed\u8a00\u5efa\u6a21\u7814\u7a76\u4e2d\u7684\u80fd\u529b", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u590d\u73b0\u7814\u7a76\u8bba\u6587\u4ee3\u7801\uff08\u5c24\u5176\u662fNLP\u9886\u57df\uff09\u8fd9\u4e00\u57fa\u7840\u800c\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u8be5\u4efb\u52a1\u6d89\u53ca\u62bd\u8c61\u6982\u5ff5\u7684\u7efc\u5408\u7406\u89e3\u548c\u5bf9\u4f9d\u8d56\u6587\u4ef6\u7684\u4ee3\u7801\u5e93\u7684\u590d\u6742\u63a8\u7406\u3002\u57fa\u4e8e\u6b64\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51faLMR-BENCH\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u8bed\u8a00\u5efa\u6a21\u7814\u7a76\u4ee3\u7801\u590d\u73b0\u4e2d\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b28\u4e2a\u4ee3\u7801\u590d\u73b0\u4efb\u52a1\uff0c\u6e90\u81ea\u8fc7\u53bb\u4e94\u5e74\u9876\u7ea7NLP\u4f1a\u8bae\u768423\u7bc7\u8bba\u6587\uff0c\u6db5\u76d6\u4e5d\u7c7b\u57fa\u7840\u4efb\u52a1\u3002\u6a21\u578b\u9700\u6839\u636e\u8bba\u6587\u3001\u90e8\u5206\u63a9\u7801\u7684\u4ee3\u7801\u5e93\u53ca\u5b9e\u73b0\u6307\u4ee4\u5b8c\u6210\u4efb\u52a1\u3002\u6211\u4eec\u5728\u6807\u51c6\u63d0\u793a\u548cLLM\u4ee3\u7406\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u548cLLM\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u5408\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u51f8\u663e\u4e86LLM\u4ee3\u7406\u5728\u81ea\u4e3b\u590d\u73b0\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5173\u952e\u4e0d\u8db3\u3002"}}
{"id": "2506.17781", "pdf": "https://arxiv.org/pdf/2506.17781", "abs": "https://arxiv.org/abs/2506.17781", "authors": ["Miguel Romero", "Shuoyang Ding", "Corey D. Barret", "Georgiana Dinu", "George Karypis"], "title": "Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Dense embeddings are fundamental to modern machine learning systems, powering\nRetrieval-Augmented Generation (RAG), information retrieval, and representation\nlearning. While instruction-conditioning has become the dominant approach for\nembedding specialization, its direct application to low-capacity models imposes\nfundamental representational constraints that limit the performance gains\nderived from specialization. In this paper, we analyze these limitations and\nintroduce the Mixture of Task Experts (MoTE) transformer block, which leverages\ntask-specialized parameters trained with Task-Aware Contrastive Learning\n(\\tacl) to enhance the model ability to generate specialized embeddings.\nEmpirical results show that MoTE achieves $64\\%$ higher performance gains in\nretrieval datasets ($+3.27 \\rightarrow +5.21$) and $43\\%$ higher performance\ngains across all datasets ($+1.81 \\rightarrow +2.60$). Critically, these gains\nare achieved without altering instructions, training data, inference time, or\nnumber of active parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MoTE\uff08\u4efb\u52a1\u4e13\u5bb6\u6df7\u5408\uff09\u53d8\u6362\u5757\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u6a21\u578b\u751f\u6210\u4e13\u7528\u5d4c\u5165\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u4e0d\u6539\u53d8\u6307\u4ee4\u3001\u8bad\u7ec3\u6570\u636e\u6216\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u8c03\u8282\u7684\u4f4e\u5bb9\u91cf\u6a21\u578b\u5728\u5d4c\u5165\u4e13\u4e1a\u5316\u4e2d\u5b58\u5728\u8868\u793a\u9650\u5236\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4efb\u52a1\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5f15\u5165MoTE\u53d8\u6362\u5757\uff0c\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff08TACL\uff09\u8bad\u7ec3\u4efb\u52a1\u4e13\u7528\u53c2\u6570\uff0c\u4ee5\u751f\u6210\u66f4\u4e13\u4e1a\u7684\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMoTE\u5728\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534764%\uff08\u4ece+3.27\u5230+5.21\uff09\uff0c\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u63d0\u534743%\uff08\u4ece+1.81\u5230+2.60\uff09\u3002", "conclusion": "MoTE\u5728\u4e0d\u6539\u53d8\u6307\u4ee4\u3001\u8bad\u7ec3\u6570\u636e\u6216\u63a8\u7406\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5d4c\u5165\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u4efb\u52a1\u5d4c\u5165\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u8d85\u8d8a\u6307\u4ee4\u8c03\u8282\uff1aMoTE\u2014\u2014\u591a\u4efb\u52a1\u5d4c\u5165\u6a21\u578b\u7684\u4efb\u52a1\u4e13\u5bb6\u6df7\u5408", "abstract_zh": "\u5bc6\u96c6\u5d4c\u5165\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u4fe1\u606f\u68c0\u7d22\u548c\u8868\u793a\u5b66\u4e60\u3002\u5c3d\u7ba1\u6307\u4ee4\u8c03\u8282\u5df2\u6210\u4e3a\u5d4c\u5165\u4e13\u4e1a\u5316\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u4f4e\u5bb9\u91cf\u6a21\u578b\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u8868\u793a\u9650\u5236\uff0c\u9650\u5236\u4e86\u4e13\u4e1a\u5316\u7684\u6027\u80fd\u63d0\u5347\u3002\u672c\u6587\u5206\u6790\u4e86\u8fd9\u4e9b\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4efb\u52a1\u4e13\u5bb6\u6df7\u5408\uff08MoTE\uff09\u53d8\u6362\u5757\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff08TACL\uff09\u8bad\u7ec3\u4efb\u52a1\u4e13\u7528\u53c2\u6570\uff0c\u589e\u5f3a\u6a21\u578b\u751f\u6210\u4e13\u7528\u5d4c\u5165\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoTE\u5728\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8664%\u7684\u6027\u80fd\u63d0\u5347\uff08\u4ece+3.27\u5230+5.21\uff09\uff0c\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e8643%\uff08\u4ece+1.81\u5230+2.60\uff09\u3002\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e9b\u63d0\u5347\u672a\u6539\u53d8\u6307\u4ee4\u3001\u8bad\u7ec3\u6570\u636e\u3001\u63a8\u7406\u65f6\u95f4\u6216\u6d3b\u52a8\u53c2\u6570\u6570\u91cf\u3002"}}
{"id": "2506.18226", "pdf": "https://arxiv.org/pdf/2506.18226", "abs": "https://arxiv.org/abs/2506.18226", "authors": ["Xunzhi Xiang", "Qi Fan"], "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u81ea\u9002\u5e94\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\uff08ADSA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11GPU\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fc7\u957f\u7684\u4e0a\u4e0b\u6587\u5bfc\u81f4KV\u7f13\u5b58\u5360\u7528\u5927\u91cf\u5185\u5b58\u548c\u8ba1\u7b97\u5ef6\u8fdf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\uff08ADSA\uff09\uff0c\u52a8\u6001\u8bc6\u522b\u5bf9\u5c40\u90e8\u7eb9\u7406\u4e00\u81f4\u6027\u548c\u5168\u5c40\u8bed\u4e49\u8fde\u8d2f\u6027\u81f3\u5173\u91cd\u8981\u7684\u5386\u53f2\u6807\u8bb0\uff0c\u5e76\u5f15\u5165\u52a8\u6001KV\u7f13\u5b58\u66f4\u65b0\u673a\u5236\uff0c\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cADSA\u5728\u751f\u6210\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\uff0cGPU\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u7ea650%\u3002", "conclusion": "ADSA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "paper_title_zh": "\u9ad8\u6548\u5316\uff1a\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b", "abstract_zh": "\u81ea\u56de\u5f52\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5df2\u6210\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7684\u4e3b\u8981\u8303\u5f0f\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u4e00\u7ef4\u6807\u8bb0\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3001\u5efa\u6a21\u5168\u5c40\u4e0a\u4e0b\u6587\u5e76\u786e\u4fdd\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u7136\u800c\uff0c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fc7\u957f\u7684\u4e0a\u4e0b\u6587\u5bfc\u81f4KV\u7f13\u5b58\u5360\u7528\u5927\u91cf\u5185\u5b58\u548c\u8ba1\u7b97\u5ef6\u8fdf\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u7cfb\u7edf\u5206\u6790\u4e86\u5168\u5c40\u8bed\u4e49\u3001\u7a7a\u95f4\u5e03\u5c40\u548c\u7ec6\u7c92\u5ea6\u7eb9\u7406\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5f62\u6210\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u81ea\u9002\u5e94\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\uff08ADSA\uff09\u3002ADSA\u52a8\u6001\u8bc6\u522b\u5bf9\u5c40\u90e8\u7eb9\u7406\u4e00\u81f4\u6027\u548c\u5168\u5c40\u8bed\u4e49\u8fde\u8d2f\u6027\u81f3\u5173\u91cd\u8981\u7684\u5386\u53f2\u6807\u8bb0\uff0c\u4ece\u800c\u9ad8\u6548\u7b80\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3aADSA\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001KV\u7f13\u5b58\u66f4\u65b0\u673a\u5236\uff0c\u5c06\u63a8\u7406\u65f6\u7684GPU\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u7ea650%\u3002\u5927\u91cf\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.17337", "pdf": "https://arxiv.org/pdf/2506.17337", "abs": "https://arxiv.org/abs/2506.17337", "authors": ["Yuan Zhong", "Ruinan Jin", "Xiaoxiao Li", "Qi Dou"], "title": "Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical vision-language models (VLMs) leverage large-scale pretraining for\ndiverse imaging tasks but require substantial computational and data resources.\nMeanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not\ntrained for medical use, show promise with fine-tuning. This raises a key\nquestion: Can efficient fine-tuned common VLMs rival generalist medical VLMs\nfor solving specific medical imaging tasks? This study systematically evaluates\ncommon and medical VLMs across disease diagnosis and visual question answering\n(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf\nperformance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges\nthese gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen\nmedical modalities. While medical-specific pretraining provides advantages in\nID settings, common VLMs match or surpass medical-specific models after\nlightweight fine-tuning, with LoRA-based adaptation proving highly effective\namong different tasks. In OOD tasks, common VLMs demonstrate strong\nadaptability in some tasks, challenging the assumption that medical-specific\npre-training is essential. These findings suggest that leveraging common VLMs\nwith fine-tuning offers a scalable and cost-effective alternative to developing\nlarge-scale medical VLMs, providing crucial insights for future research in the\nmedical imaging field.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u662f\u5426\u53ef\u901a\u8fc7\u5fae\u8c03\u5339\u654c\u533b\u7597\u4e13\u7528VLMs\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8f7b\u91cf\u5fae\u8c03\u540e\uff0c\u901a\u7528VLMs\u5728\u7279\u5b9a\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u533b\u7597\u4e13\u7528\u6a21\u578b\uff0c\u5c24\u5176\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u9002\u5e94\u6027\u3002", "motivation": "\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\uff0c\u800c\u901a\u7528VLMs\u867d\u672a\u7ecf\u533b\u7597\u8bad\u7ec3\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u80fd\u5177\u5907\u7ade\u4e89\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u901a\u7528VLMs\u662f\u5426\u80fd\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u66ff\u4ee3\u4e13\u7528\u6a21\u578b\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8eCLIP\u548cLLaVA\u7684\u901a\u7528\u4e0e\u533b\u7597VLMs\uff0c\u6bd4\u8f83\u4e86\u5176\u5728\u57df\u5185\uff08ID\uff09\u548c\u57df\u5916\uff08OOD\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u4e86\u8f7b\u91cf\u5fae\u8c03\uff08\u5982LoRA\uff09\u7684\u6548\u679c\u3002", "result": "\u533b\u7597\u4e13\u7528\u9884\u8bad\u7ec3\u5728\u57df\u5185\u4efb\u52a1\u4e2d\u5360\u4f18\uff0c\u4f46\u901a\u7528VLMs\u7ecf\u5fae\u8c03\u540e\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\uff1b\u5728\u57df\u5916\u4efb\u52a1\u4e2d\uff0c\u901a\u7528VLMs\u5c55\u73b0\u51fa\u8f83\u5f3a\u9002\u5e94\u6027\uff0c\u6311\u6218\u4e86\u533b\u7597\u4e13\u7528\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u901a\u7528VLMs\u7ed3\u5408\u5fae\u8c03\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u533b\u7597\u5f71\u50cf\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002", "paper_title_zh": "\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u5339\u654c\u533b\u7597\u4e13\u7528\u6a21\u578b\uff1f\u8bc4\u4f30\u4e0e\u7b56\u7565\u6d1e\u5bdf", "abstract_zh": "\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u9002\u7528\u4e8e\u591a\u79cd\u5f71\u50cf\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u901a\u7528VLMs\uff08\u5982CLIP\u3001LLaVA\uff09\u867d\u672a\u7ecf\u533b\u7597\u8bad\u7ec3\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u9ad8\u6548\u5fae\u8c03\u7684\u901a\u7528VLMs\u80fd\u5426\u5728\u7279\u5b9a\u533b\u7597\u5f71\u50cf\u4efb\u52a1\u4e2d\u5339\u654c\u901a\u7528\u533b\u7597VLMs\uff1f\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u901a\u7528\u4e0e\u533b\u7597VLMs\u5728\u75be\u75c5\u8bca\u65ad\u548c\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u7684\u8868\u73b0\u3002\u57fa\u4e8eCLIP\u548cLLaVA\u7684\u6a21\u578b\uff0c\u6211\u4eec\u8003\u5bdf\u4e86\uff081\uff09\u57df\u5185\uff08ID\uff09\u4efb\u52a1\u4e2d\u7684\u73b0\u6210\u6027\u80fd\u5dee\u8ddd\uff0c\uff082\uff09\u5fae\u8c03\u662f\u5426\u80fd\u5f25\u5408\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u4ee5\u53ca\uff083\uff09\u5bf9\u672a\u89c1\u533b\u7597\u6a21\u6001\u7684\u57df\u5916\uff08OOD\uff09\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u533b\u7597\u4e13\u7528\u9884\u8bad\u7ec3\u5728ID\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u901a\u7528VLMs\u7ecf\u8f7b\u91cf\u5fae\u8c03\u540e\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5176\u4e2dLoRA\u9002\u914d\u65b9\u6cd5\u5c24\u4e3a\u6709\u6548\u3002\u5728OOD\u4efb\u52a1\u4e2d\uff0c\u901a\u7528VLMs\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u9002\u5e94\u6027\uff0c\u6311\u6218\u4e86\u533b\u7597\u4e13\u7528\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u7ed3\u5408\u5fae\u8c03\u7684\u901a\u7528VLMs\u4e3a\u5f00\u53d1\u5927\u89c4\u6a21\u533b\u7597VLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u533b\u7597\u5f71\u50cf\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2506.18234", "pdf": "https://arxiv.org/pdf/2506.18234", "abs": "https://arxiv.org/abs/2506.18234", "authors": ["Yue Li", "Meng Tian", "Dechang Zhu", "Jiangtong Zhu", "Zhenyu Lin", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Large vision-language models (VLMs) for autonomous driving (AD) are evolving\nbeyond perception and cognition tasks toward motion planning. However, we\nidentify two critical challenges in this direction: (1) VLMs tend to learn\nshortcuts by relying heavily on history input information, achieving seemingly\nstrong planning results without genuinely understanding the visual inputs; and\n(2) the chain-ofthought (COT) reasoning processes are always misaligned with\nthe motion planning outcomes, and how to effectively leverage the complex\nreasoning capability to enhance planning remains largely underexplored. In this\npaper, we start from a small-scale domain-specific VLM and propose Drive-R1\ndesigned to bridges the scenario reasoning and motion planning for AD. Drive-R1\nfirst undergoes the supervised finetuning on a elaborate dataset containing\nboth long and short COT data. Drive-R1 is encouraged to reason step-by-step\nfrom visual input to final planning decisions. Subsequently, Drive-R1 is\ntrained within a reinforcement learning framework that incentivizes the\ndiscovery of reasoning paths that are more informative for planning, guided by\nrewards based on predicted trajectories and meta actions. Experimental\nevaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that\nDrive-R1 achieves superior performance compared to existing state-of-the-art\nVLMs. We believe that Drive-R1 presents a promising direction for bridging\nreasoning and planning in AD, offering methodological insights for future\nresearch and applications.", "AI": {"tldr": "Drive-R1\u662f\u4e00\u79cd\u7ed3\u5408\u63a8\u7406\u4e0e\u89c4\u5212\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u8def\u5f84\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\uff0c\u5728nuScenes\u548cDriveLM-nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4e3b\u8981\u4f9d\u8d56\u5386\u53f2\u8f93\u5165\u4fe1\u606f\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u771f\u6b63\u7406\u89e3\uff0c\u4e14\u63a8\u7406\u8fc7\u7a0b\u4e0e\u8fd0\u52a8\u89c4\u5212\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002Drive-R1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u63a8\u7406\u4e0e\u89c4\u5212\u7d27\u5bc6\u7ed3\u5408\u3002", "method": "Drive-R1\u9996\u5148\u5728\u5305\u542b\u957f\u77ed\u94fe\u5f0f\u63a8\u7406\uff08COT\uff09\u6570\u636e\u7684\u7cbe\u7ec6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u9010\u6b65\u4ece\u89c6\u89c9\u8f93\u5165\u63a8\u7406\u5230\u6700\u7ec8\u89c4\u5212\u51b3\u7b56\u3002\u968f\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u4ee5\u9884\u6d4b\u8f68\u8ff9\u548c\u5143\u52a8\u4f5c\u4e3a\u5956\u52b1\uff0c\u4f18\u5316\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728nuScenes\u548cDriveLM-nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDrive-R1\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u63a8\u7406\u4e0e\u89c4\u5212\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "Drive-R1\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u63a8\u7406\u4e0e\u89c4\u5212\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u542f\u793a\u3002", "paper_title_zh": "Drive-R1\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6865\u63a5\u63a8\u7406\u4e0e\u89c4\u5212", "abstract_zh": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6b63\u4ece\u611f\u77e5\u548c\u8ba4\u77e5\u4efb\u52a1\u5411\u8fd0\u52a8\u89c4\u5212\u53d1\u5c55\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\uff081\uff09VLMs\u503e\u5411\u4e8e\u4f9d\u8d56\u5386\u53f2\u8f93\u5165\u4fe1\u606f\u8d70\u6377\u5f84\uff0c\u770b\u4f3c\u89c4\u5212\u7ed3\u679c\u826f\u597d\u5374\u672a\u771f\u6b63\u7406\u89e3\u89c6\u89c9\u8f93\u5165\uff1b\uff082\uff09\u94fe\u5f0f\u63a8\u7406\uff08COT\uff09\u8fc7\u7a0b\u4e0e\u8fd0\u52a8\u89c4\u5212\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u5347\u89c4\u5212\u4ecd\u5f85\u63a2\u7d22\u3002\u672c\u6587\u4ece\u5c0f\u89c4\u6a21\u9886\u57df\u4e13\u7528VLM\u51fa\u53d1\uff0c\u63d0\u51faDrive-R1\uff0c\u65e8\u5728\u6865\u63a5\u81ea\u52a8\u9a7e\u9a76\u7684\u573a\u666f\u63a8\u7406\u4e0e\u8fd0\u52a8\u89c4\u5212\u3002Drive-R1\u9996\u5148\u5728\u5305\u542b\u957f\u77edCOT\u6570\u636e\u7684\u7cbe\u7ec6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u9f13\u52b1\u5176\u4ece\u89c6\u89c9\u8f93\u5165\u9010\u6b65\u63a8\u7406\u81f3\u6700\u7ec8\u89c4\u5212\u51b3\u7b56\u3002\u968f\u540e\uff0cDrive-R1\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u8bad\u7ec3\uff0c\u4ee5\u9884\u6d4b\u8f68\u8ff9\u548c\u5143\u52a8\u4f5c\u4e3a\u5956\u52b1\uff0c\u6fc0\u52b1\u53d1\u73b0\u5bf9\u89c4\u5212\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u63a8\u7406\u8def\u5f84\u3002\u5728nuScenes\u548cDriveLM-nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDrive-R1\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684VLMs\u3002\u6211\u4eec\u8ba4\u4e3a\uff0cDrive-R1\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u63a8\u7406\u4e0e\u89c4\u5212\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u542f\u793a\u3002"}}
{"id": "2506.17338", "pdf": "https://arxiv.org/pdf/2506.17338", "abs": "https://arxiv.org/abs/2506.17338", "authors": ["Duong Bach"], "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning", "categories": ["cs.DC", "cs.AI", "cs.MA"], "comment": "13 pages", "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo-Forgetting Protocol\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u540c\u6b65\u4fee\u526a\u5171\u4eab\u8bb0\u5fc6\uff0c\u7ed3\u5408\u8bed\u4e49\u6295\u7968\u3001\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u548cPBFT\u5171\u8bc6\u673a\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u5e76\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u9700\u8981\u9ad8\u6548\u7ba1\u7406\u5171\u4eab\u77e5\u8bc6\uff0c\u907f\u514d\u8fc7\u65f6\u6216\u65e0\u5173\u6570\u636e\u7684\u79ef\u7d2f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u540c\u6b65\u4fee\u526a\u8bb0\u5fc6\u7684\u534f\u8bae\u3002", "method": "\u534f\u8bae\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a(1) \u57fa\u4e8eDistilBERT\u7684\u8bed\u4e49\u6295\u7968\uff0c\u8bc4\u4f30\u8bb0\u5fc6\u9879\u7684\u76f8\u5173\u6027\uff1b(2) \u591a\u5c3a\u5ea6\u65f6\u95f4\u8870\u51cf\u51fd\u6570\uff0c\u6839\u636e\u8bb0\u5fc6\u7684\u5e74\u9f84\u548c\u8bbf\u95ee\u9891\u7387\u8c03\u6574\u91cd\u8981\u6027\uff1b(3) \u57fa\u4e8ePBFT\u7684\u5171\u8bc6\u673a\u5236\uff0c\u786e\u4fdd\u5728\u5b58\u5728\u6076\u610f\u4ee3\u7406\u65f6\u4ecd\u80fd\u8fbe\u6210\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u534f\u8bae\u5728500\u4e2a\u5468\u671f\u5185\u51cf\u5c1152%\u5185\u5b58\u5360\u7528\uff0c\u9057\u5fd8\u51b3\u7b56\u51c6\u786e\u7387\u8fbe88%\uff0cPBFT\u5171\u8bc6\u6210\u529f\u7387\u4e3a92%\uff0c\u5185\u5b58\u8bbf\u95ee\u7f13\u5b58\u547d\u4e2d\u7387\u4e3a82%\u3002", "conclusion": "Co-Forgetting Protocol\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u8bb0\u5fc6\u540c\u6b65\u4fee\u526a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "paper_title_zh": "\u57fa\u4e8ePBFT\u7684\u8bed\u4e49\u6295\u7968\u7528\u4e8e\u591a\u4ee3\u7406\u8bb0\u5fc6\u4fee\u526a", "abstract_zh": "\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u9ad8\u6548\u7ba1\u7406\u5171\u4eab\u77e5\u8bc6\uff0c\u786e\u4fdd\u5206\u5e03\u5f0f\u8bb0\u5fc6\u540c\u6b65\u3001\u76f8\u5173\u4e14\u907f\u514d\u8fc7\u65f6\u6216\u65e0\u5173\u6570\u636e\u7684\u79ef\u7d2f\u2014\u2014\u8fd9\u4e00\u8fc7\u7a0b\u7c7b\u4f3c\u4e8e\u751f\u7269\u9057\u5fd8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo-Forgetting Protocol\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u4fee\u526aMAS\u4e2d\u7684\u8bb0\u5fc6\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002\u8be5\u534f\u8bae\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bed\u4e49\u6295\u7968\uff0c\u4ee3\u7406\u4f7f\u7528\u8f7b\u91cf\u7ea7DistilBERT\u6a21\u578b\u8bc4\u4f30\u8bb0\u5fc6\u9879\u7684\u5185\u5bb9\u548c\u5f53\u524d\u64cd\u4f5c\u4e0a\u4e0b\u6587\u7684\u76f8\u5173\u6027\uff1b(2) \u591a\u5c3a\u5ea6\u65f6\u95f4\u8870\u51cf\u51fd\u6570\uff0c\u6839\u636e\u8bb0\u5fc6\u7684\u5e74\u9f84\u548c\u8bbf\u95ee\u9891\u7387\u5728\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u5185\u5206\u914d\u9012\u51cf\u7684\u91cd\u8981\u6027\uff1b(3) \u57fa\u4e8e\u5b9e\u7528\u62dc\u5360\u5ead\u5bb9\u9519\uff08PBFT\uff09\u7684\u5171\u8bc6\u673a\u5236\uff0c\u786e\u4fdd\u4fdd\u7559\u6216\u4e22\u5f03\u8bb0\u5fc6\u9879\u7684\u51b3\u5b9a\u7531\u5408\u683c\u4e14\u5bb9\u9519\u7684\u591a\u6570\u4ee3\u7406\u8fbe\u6210\uff0c\u5373\u4f7f\u7cfb\u7edf\u4e2d\u5b58\u5728\u6700\u591af\u4e2a\u62dc\u5360\u5ead\uff08\u6076\u610f\u6216\u6545\u969c\uff09\u4ee3\u7406\uff08N\u22653f+1\uff09\u3002\u534f\u8bae\u5229\u7528gRPC\u5b9e\u73b0\u9ad8\u6548\u7684\u4ee3\u7406\u95f4\u901a\u4fe1\uff0cPinecone\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u5411\u91cf\u5d4c\u5165\u5b58\u50a8\u548c\u76f8\u4f3c\u6027\u641c\u7d22\uff0cSQLite\u7ba1\u7406\u5143\u6570\u636e\u3002\u5728\u6a21\u62dfMAS\u73af\u5883\u4e2d\u5bf9\u56db\u4e2a\u4ee3\u7406\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u534f\u8bae\u5728500\u4e2a\u5468\u671f\u5185\u51cf\u5c1152%\u5185\u5b58\u5360\u7528\uff0c\u9057\u5fd8\u51b3\u7b56\u51c6\u786e\u7387\u8fbe88%\uff08\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\uff09\uff0cPBFT\u5171\u8bc6\u6210\u529f\u7387\u4e3a92%\uff08\u6a21\u62df\u62dc\u5360\u5ead\u6761\u4ef6\u4e0b\uff09\uff0c\u5185\u5b58\u8bbf\u95ee\u7f13\u5b58\u547d\u4e2d\u7387\u4e3a82%\u3002"}}
{"id": "2506.17828", "pdf": "https://arxiv.org/pdf/2506.17828", "abs": "https://arxiv.org/abs/2506.17828", "authors": ["Xinnan Zhang", "Chenliang Li", "Siliang Zeng", "Jiaxiang Li", "Zhongruo Wang", "Kaixiang Lin", "Songtao Lu", "Alfredo Garcia", "Mingyi Hong"], "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIRO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u9f50\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\uff0c\u901a\u8fc7\u8fed\u4ee3\u91cd\u52a0\u6743\u548c\u4f18\u5316\u65b9\u6cd5\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982RLHF\u548cDPO\u9700\u8981\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff0c\u65e0\u6cd5\u5728\u6d4b\u8bd5\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u4e0d\u9002\u7528\u4e8e\u65e0\u6cd5\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u3002\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u867d\u907f\u514d\u6743\u91cd\u66f4\u65b0\uff0c\u4f46\u4f9d\u8d56\u4e0d\u5b8c\u7f8e\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5bfc\u81f4\u8f93\u51fa\u6b21\u4f18\u4e14\u63a8\u7406\u6210\u672c\u9ad8\u3002", "method": "IRO\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5b9e\u73b0\u5bf9\u9f50\uff1a(i) \u4ece\u57fa\u7840\u6a21\u578b\u4e2d\u91c7\u6837\u5019\u9009\u8f93\u51fa\uff0c(ii) \u4f7f\u7528\u5f53\u524d\u4ef7\u503c\u51fd\u6570\u91cd\u91c7\u6837\uff0c(iii) \u8bad\u7ec3\u65b0\u7684\u8f7b\u91cf\u7ea7\u4ef7\u503c\u51fd\u6570\u4ee5\u6307\u5bfc\u4e0b\u4e00\u6b21\u89e3\u7801\u3002\u6d4b\u8bd5\u65f6\uff0c\u901a\u8fc7\u641c\u7d22\u4f18\u5316\u8fc7\u7a0b\u5f15\u5bfc\u751f\u6210\u3002", "result": "IRO\u80fd\u591f\u5728\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u51bb\u7ed3LLM\u7684\u5bf9\u9f50\uff0c\u7c7b\u4f3c\u4e8eOpenAI\u7684RFT\uff0c\u4f46\u66f4\u5177\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "IRO\u4e3a\u5bf9\u9f50\u51bb\u7ed3LLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7528\u6237\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u4e14\u65e0\u9700\u6a21\u578b\u6743\u91cd\u8bbf\u95ee\u6743\u9650\u3002", "paper_title_zh": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u51bb\u7ed3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff1a\u4e00\u79cd\u8fed\u4ee3\u91cd\u52a0\u6743\u4f18\u5316\u65b9\u6cd5", "abstract_zh": "\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u901a\u5e38\u9700\u8981\u5fae\u8c03\u65b9\u6cd5\uff0c\u5982RLHF\u548cDPO\u3002\u8fd9\u4e9b\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff0c\u56e0\u6b64\u65e0\u6cd5\u5728\u6d4b\u8bd5\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u4e5f\u4e0d\u9002\u7528\u4e8e\u65e0\u6cd5\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u8f93\u51fa\u8d28\u91cf\u63d0\u5347\uff0c\u907f\u514d\u4e86\u6743\u91cd\u66f4\u65b0\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u4e0d\u5b8c\u7f8e\u5956\u52b1\u51fd\u6570\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u6b21\u4f18\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8fed\u4ee3\u91cd\u52a0\u6743\u4f18\u5316\uff08IRO\uff09\u7684\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u53ef\u5728\u4e0d\u4fee\u6539\u57fa\u7840\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bf9\u9f50\u3002\u8bad\u7ec3\u65f6\uff0c\u6bcf\u8f6e\u8fed\u4ee3\uff08i\uff09\u4ece\u57fa\u7840\u6a21\u578b\u4e2d\u91c7\u6837\u5019\u9009\u8f93\u51fa\uff0c\uff08ii\uff09\u4f7f\u7528\u5f53\u524d\u4ef7\u503c\u51fd\u6570\u91cd\u91c7\u6837\uff0c\uff08iii\uff09\u8bad\u7ec3\u65b0\u7684\u8f7b\u91cf\u7ea7\u4ef7\u503c\u51fd\u6570\u4ee5\u6307\u5bfc\u4e0b\u4e00\u6b21\u89e3\u7801\u3002\u6d4b\u8bd5\u65f6\uff0c\u901a\u8fc7\u641c\u7d22\u4f18\u5316\u8fc7\u7a0b\u5229\u7528\u4ef7\u503c\u51fd\u6570\u5f15\u5bfc\u57fa\u7840\u6a21\u578b\u751f\u6210\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7528\u6237\u53ef\u7c7b\u4f3cOpenAI\u7684\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u5728\u81ea\u6709\u6570\u636e\u96c6\u4e0a\u5e94\u7528IRO\uff0c\u4f46\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u3002"}}
{"id": "2506.18246", "pdf": "https://arxiv.org/pdf/2506.18246", "abs": "https://arxiv.org/abs/2506.18246", "authors": ["Xiangzhao Hao", "Kuan Zhu", "Hongyu Guo", "Haiyun Guo", "Ming Tang", "JinQiao Wang"], "title": "Referring Expression Instance Retrieval and A Strong End-to-End Baseline", "categories": ["cs.CV"], "comment": null, "summary": "Natural language querying of visual content underpins many vision-language\ntasks, typically categorized by text granularity and visual search scope.\nText-Image Retrieval (TIR) retrieves whole images using coarse descriptions,\nwhile Referring Expression Comprehension (REC) localizes objects using\nfine-grained expressions within a single image. However, real-world scenarios\noften require both instance-level retrieval and localization across large\ngalleries -- tasks where TIR lacks precision and REC lacks scalability. To\naddress this gap, we propose a new task: Referring Expression Instance\nRetrieval (REIR), which jointly supports instance-level retrieval and\nlocalization. We introduce REIRCOCO, a large-scale benchmark constructed by\nprompting vision-language models to generate fine-grained expressions for\nMSCOCO and RefCOCO instances. We also present a baseline method, CLARE,\nfeaturing a dual-stream architecture with a Mix of Relation Experts (MORE)\nmodule for capturing inter-instance relationships. CLARE integrates object\ndetection and REC pretraining with Contrastive Language-Instance Alignment\n(CLIA) for end-to-end optimization. Experiments show that CLARE achieves\nstate-of-the-art performance on REIR and generalizes well to TIR and REC,\nhighlighting its effectiveness and versatility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\uff1a\u6307\u4ee3\u8868\u8fbe\u5b9e\u4f8b\u68c0\u7d22\uff08REIR\uff09\uff0c\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u548c\u5b9a\u4f4d\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6REIRCOCO\u3002\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5CLARE\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u548c\u5173\u7cfb\u4e13\u5bb6\u6a21\u5757\uff08MORE\uff09\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5728REIR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5982\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\uff08TIR\uff09\u548c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08REC\uff09\u5206\u522b\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u548c\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u573a\u666f\u4e2d\u540c\u65f6\u9700\u8981\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u548c\u5b9a\u4f4d\u7684\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51faREIR\u4efb\u52a1\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51faCLARE\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u548c\u5173\u7cfb\u4e13\u5bb6\u6a21\u5757\uff08MORE\uff09\u6355\u6349\u5b9e\u4f8b\u95f4\u5173\u7cfb\uff0c\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u548cREC\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u8bed\u8a00-\u5b9e\u4f8b\u5bf9\u9f50\uff08CLIA\uff09\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCLARE\u5728REIR\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230TIR\u548cREC\u4efb\u52a1\uff0c\u5c55\u73b0\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "REIR\u4efb\u52a1\u548cCLARE\u65b9\u6cd5\u4e3a\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u548c\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u5e94\u7528\u573a\u666f\u3002", "paper_title_zh": "\u6307\u4ee3\u8868\u8fbe\u5b9e\u4f8b\u68c0\u7d22\u4e0e\u4e00\u79cd\u5f3a\u7aef\u5230\u7aef\u57fa\u7ebf\u65b9\u6cd5", "abstract_zh": "\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u89c6\u89c9\u5185\u5bb9\u662f\u8bb8\u591a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u57fa\u7840\uff0c\u901a\u5e38\u6839\u636e\u6587\u672c\u7c92\u5ea6\u548c\u89c6\u89c9\u641c\u7d22\u8303\u56f4\u5206\u7c7b\u3002\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\uff08TIR\uff09\u901a\u8fc7\u7c97\u7c92\u5ea6\u63cf\u8ff0\u68c0\u7d22\u6574\u5f20\u56fe\u50cf\uff0c\u800c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08REC\uff09\u5219\u5728\u5355\u5f20\u56fe\u50cf\u4e2d\u4f7f\u7528\u7ec6\u7c92\u5ea6\u8868\u8fbe\u5b9a\u4f4d\u5bf9\u8c61\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u573a\u666f\u5f80\u5f80\u9700\u8981\u540c\u65f6\u8fdb\u884c\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u548c\u5927\u89c4\u6a21\u56fe\u5e93\u4e2d\u7684\u5b9a\u4f4d\u2014\u2014TIR\u7f3a\u4e4f\u7cbe\u5ea6\uff0c\u800cREC\u7f3a\u4e4f\u6269\u5c55\u6027\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u65b0\u4efb\u52a1\uff1a\u6307\u4ee3\u8868\u8fbe\u5b9e\u4f8b\u68c0\u7d22\uff08REIR\uff09\uff0c\u652f\u6301\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u548c\u5b9a\u4f4d\u3002\u6211\u4eec\u6784\u5efa\u4e86REIRCOCO\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3aMSCOCO\u548cRefCOCO\u5b9e\u4f8b\u751f\u6210\u7ec6\u7c92\u5ea6\u8868\u8fbe\u3002\u540c\u65f6\u63d0\u51fa\u57fa\u7ebf\u65b9\u6cd5CLARE\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u548c\u5173\u7cfb\u4e13\u5bb6\u6a21\u5757\uff08MORE\uff09\u6355\u6349\u5b9e\u4f8b\u95f4\u5173\u7cfb\uff0c\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u548cREC\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u8bed\u8a00-\u5b9e\u4f8b\u5bf9\u9f50\uff08CLIA\uff09\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCLARE\u5728REIR\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u6cdb\u5316\u5230TIR\u548cREC\u4efb\u52a1\uff0c\u5c55\u73b0\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2506.17342", "pdf": "https://arxiv.org/pdf/2506.17342", "abs": "https://arxiv.org/abs/2506.17342", "authors": ["Zijian Long", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "title": "Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.MM", "cs.NI"], "comment": "Accepted by IEEE Transactions on Computational Social Systems", "summary": "The social metaverse is a growing digital ecosystem that blends virtual and\nphysical worlds. It allows users to interact socially, work, shop, and enjoy\nentertainment. However, privacy remains a major challenge, as immersive\ninteractions require continuous collection of biometric and behavioral data. At\nthe same time, ensuring high-quality, low-latency streaming is difficult due to\nthe demands of real-time interaction, immersive rendering, and bandwidth\noptimization. To address these issues, we propose ASMS (Adaptive Social\nMetaverse Streaming), a novel streaming system based on Federated Multi-Agent\nProximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which\nintegrates federated learning (FL) and deep reinforcement learning (DRL) to\ndynamically adjust streaming bit rates while preserving user privacy.\nExperimental results show that ASMS improves user experience by at least 14%\ncompared to existing streaming methods across various network conditions.\nTherefore, ASMS enhances the social metaverse experience by providing seamless\nand immersive streaming, even in dynamic and resource-constrained networks,\nwhile ensuring that sensitive user data remains on local devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08F-MAPPO\uff09\u7684\u81ea\u9002\u5e94\u793e\u4ea4\u5143\u5b87\u5b99\u6d41\u5a92\u4f53\u7cfb\u7edfASMS\uff0c\u65e8\u5728\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u548c\u9ad8\u8d28\u91cf\u4f4e\u5ef6\u8fdf\u6d41\u5a92\u4f53\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u8868\u660e\uff0cASMS\u5728\u5404\u79cd\u7f51\u7edc\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u81f3\u5c1114%\u3002", "motivation": "\u793e\u4ea4\u5143\u5b87\u5b99\u4f5c\u4e3a\u865a\u62df\u4e0e\u7269\u7406\u4e16\u754c\u878d\u5408\u7684\u6570\u5b57\u751f\u6001\u7cfb\u7edf\uff0c\u9762\u4e34\u9690\u79c1\u4fdd\u62a4\u548c\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6d41\u5a92\u4f53\u7684\u53cc\u91cd\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u9690\u79c1\u4e0e\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ASMS\u91c7\u7528\u8054\u90a6\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08F-MAPPO\uff09\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6d41\u5a92\u4f53\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cASMS\u5728\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u6d41\u5a92\u4f53\u65b9\u6cd5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u81f3\u5c1114%\uff0c\u5b9e\u73b0\u4e86\u65e0\u7f1d\u4e14\u6c89\u6d78\u5f0f\u7684\u6d41\u5a92\u4f53\u4f53\u9a8c\u3002", "conclusion": "ASMS\u901a\u8fc7F-MAPPO\u6280\u672f\uff0c\u5728\u52a8\u6001\u548c\u8d44\u6e90\u53d7\u9650\u7684\u7f51\u7edc\u4e2d\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6d41\u5a92\u4f53\u670d\u52a1\uff0c\u540c\u65f6\u786e\u4fdd\u7528\u6237\u654f\u611f\u6570\u636e\u672c\u5730\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u5143\u5b87\u5b99\u7684\u7528\u6237\u4f53\u9a8c\u3002", "paper_title_zh": "\u57fa\u4e8e\u8054\u90a6\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u793e\u4ea4\u5143\u5b87\u5b99\u6d41\u5a92\u4f53", "abstract_zh": "\u793e\u4ea4\u5143\u5b87\u5b99\u662f\u4e00\u4e2a\u878d\u5408\u865a\u62df\u4e0e\u7269\u7406\u4e16\u754c\u7684\u6570\u5b57\u751f\u6001\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u5176\u4e2d\u8fdb\u884c\u793e\u4ea4\u4e92\u52a8\u3001\u5de5\u4f5c\u3001\u8d2d\u7269\u548c\u5a31\u4e50\u3002\u7136\u800c\uff0c\u9690\u79c1\u95ee\u9898\u4ecd\u662f\u4e3b\u8981\u6311\u6218\uff0c\u56e0\u4e3a\u6c89\u6d78\u5f0f\u4ea4\u4e92\u9700\u8981\u6301\u7eed\u6536\u96c6\u751f\u7269\u7279\u5f81\u548c\u884c\u4e3a\u6570\u636e\u3002\u540c\u65f6\uff0c\u7531\u4e8e\u5b9e\u65f6\u4ea4\u4e92\u3001\u6c89\u6d78\u5f0f\u6e32\u67d3\u548c\u5e26\u5bbd\u4f18\u5316\u7684\u9700\u6c42\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u6d41\u5a92\u4f53\u670d\u52a1\u5341\u5206\u56f0\u96be\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ASMS\uff08\u81ea\u9002\u5e94\u793e\u4ea4\u5143\u5b87\u5b99\u6d41\u5a92\u4f53\u7cfb\u7edf\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08F-MAPPO\uff09\u7684\u65b0\u578b\u6d41\u5a92\u4f53\u7cfb\u7edf\u3002ASMS\u5229\u7528F-MAPPO\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6d41\u5a92\u4f53\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cASMS\u5728\u5404\u79cd\u7f51\u7edc\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u6d41\u5a92\u4f53\u65b9\u6cd5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u81f3\u5c1114%\u3002\u56e0\u6b64\uff0cASMS\u901a\u8fc7\u63d0\u4f9b\u65e0\u7f1d\u4e14\u6c89\u6d78\u5f0f\u7684\u6d41\u5a92\u4f53\u4f53\u9a8c\uff0c\u589e\u5f3a\u4e86\u793e\u4ea4\u5143\u5b87\u5b99\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u5373\u4f7f\u5728\u52a8\u6001\u548c\u8d44\u6e90\u53d7\u9650\u7684\u7f51\u7edc\u4e2d\uff0c\u4e5f\u80fd\u786e\u4fdd\u654f\u611f\u7528\u6237\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u3002"}}
{"id": "2506.18248", "pdf": "https://arxiv.org/pdf/2506.18248", "abs": "https://arxiv.org/abs/2506.18248", "authors": ["Jongoh Jeong", "Hunmin Yang", "Jaeseok Jeong", "Kuk-Jin Yoon"], "title": "Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generative adversarial attacks train a perturbation generator on a white-box\nsurrogate model and subsequently apply the crafted perturbations to unseen\nblack-box victim models. In contrast to iterative attacks, these methods\ndeliver superior inference-time efficiency, scalability, and transferability;\nhowever, up until now, existing studies have not fully exploited the\nrepresentational capacity of generative models to preserve and harness semantic\ninformation. Specifically, the intermediate activations of the generator encode\nrich semantic features--object boundaries and coarse shapes--that remain\nunder-exploited, thereby limiting the alignment of perturbations with\nobject-salient regions which are critical for adversarial transferability. To\nremedy this, we introduce a semantic structure-aware attack framework based on\nthe Mean Teacher, which serves as a temporally smoothed feature reference. With\nthis smoothed reference, we further direct semantic consistency between the\nearly-layer activations in the student and those of the semantically rich\nteacher by feature distillation. By anchoring perturbation synthesis to the\nsemantically salient early intermediate blocks within the generator based on\nempirical findings, our method guides progressive adversarial perturbation on\nregions that substantially enhance adversarial transferability. We conduct\nextensive experiments over diverse models, domains and tasks to demonstrate\nconsistent improvements relative to state-of-the-art generative attacks,\ncomprehensively evaluated using conventional metrics and our newly proposed\nAccidental Correction Rate (ACR).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7ed3\u6784\u611f\u77e5\u7684\u751f\u6210\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u4e2d\u95f4\u6fc0\u6d3b\u7279\u5f81\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u7ed3\u6784\u611f\u77e5\u7684\u65b9\u6cd5\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMean Teacher\u7684\u8bed\u4e49\u7ed3\u6784\u611f\u77e5\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u4fdd\u6301\u5b66\u751f\u6a21\u578b\u4e0e\u6559\u5e08\u6a21\u578b\u5728\u65e9\u671f\u5c42\u6fc0\u6d3b\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u57fa\u4e8e\u7ecf\u9a8c\u53d1\u73b0\u5c06\u6270\u52a8\u5408\u6210\u951a\u5b9a\u5728\u751f\u6210\u5668\u7684\u8bed\u4e49\u663e\u8457\u533a\u57df\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u3001\u9886\u57df\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u8fc1\u79fb\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u63d0\u51fa\u7684Accidental Correction Rate (ACR)\u6307\u6807\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u7ed3\u6784\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u672c\u6587\u6210\u529f\u63d0\u5347\u4e86\u751f\u6210\u5bf9\u6297\u653b\u51fb\u7684\u8fc1\u79fb\u6027\uff0c\u4e3a\u5bf9\u6297\u653b\u51fb\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u5411\u3002", "paper_title_zh": "\u8bed\u4e49\u7ed3\u6784\u611f\u77e5\u7684\u751f\u6210\u5bf9\u6297\u653b\u51fb\u4ee5\u589e\u5f3a\u5bf9\u6297\u8fc1\u79fb\u6027", "abstract_zh": "\u751f\u6210\u5bf9\u6297\u653b\u51fb\u901a\u8fc7\u5728\u767d\u76d2\u4ee3\u7406\u6a21\u578b\u4e0a\u8bad\u7ec3\u6270\u52a8\u751f\u6210\u5668\uff0c\u968f\u540e\u5c06\u751f\u6210\u7684\u6270\u52a8\u5e94\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u9ed1\u76d2\u53d7\u5bb3\u8005\u6a21\u578b\u3002\u4e0e\u8fed\u4ee3\u653b\u51fb\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u63a8\u7406\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u8fc1\u79fb\u6027\uff1b\u7136\u800c\uff0c\u8fc4\u4eca\u4e3a\u6b62\uff0c\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u5145\u5206\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\u6765\u4fdd\u7559\u548c\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u3002\u5177\u4f53\u800c\u8a00\uff0c\u751f\u6210\u5668\u7684\u4e2d\u95f4\u6fc0\u6d3b\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u7279\u5f81\uff08\u5982\u7269\u4f53\u8fb9\u754c\u548c\u7c97\u7565\u5f62\u72b6\uff09\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6270\u52a8\u4e0e\u7269\u4f53\u663e\u8457\u533a\u57df\u7684\u5bf9\u9f50\uff0c\u800c\u8fd9\u4e9b\u533a\u57df\u5bf9\u5bf9\u6297\u8fc1\u79fb\u6027\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMean Teacher\u7684\u8bed\u4e49\u7ed3\u6784\u611f\u77e5\u653b\u51fb\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f5c\u4e3a\u65f6\u95f4\u5e73\u6ed1\u7684\u7279\u5f81\u53c2\u8003\u3002\u901a\u8fc7\u8fd9\u4e00\u5e73\u6ed1\u53c2\u8003\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u7684\u65e9\u671f\u5c42\u6fc0\u6d3b\u4e0e\u8bed\u4e49\u4e30\u5bcc\u7684\u6559\u5e08\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\u3002\u57fa\u4e8e\u7ecf\u9a8c\u53d1\u73b0\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6270\u52a8\u5408\u6210\u951a\u5b9a\u5728\u751f\u6210\u5668\u7684\u8bed\u4e49\u663e\u8457\u65e9\u671f\u4e2d\u95f4\u5757\u4e0a\uff0c\u4ece\u800c\u5f15\u5bfc\u6e10\u8fdb\u5f0f\u5bf9\u6297\u6270\u52a8\u4f5c\u7528\u4e8e\u663e\u8457\u63d0\u5347\u5bf9\u6297\u8fc1\u79fb\u6027\u7684\u533a\u57df\u3002\u6211\u4eec\u5728\u591a\u79cd\u6a21\u578b\u3001\u9886\u57df\u548c\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u751f\u6210\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f20\u7edf\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684Accidental Correction Rate (ACR)\u4e0a\u5747\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2506.17942", "pdf": "https://arxiv.org/pdf/2506.17942", "abs": "https://arxiv.org/abs/2506.17942", "authors": ["Marco Cognetta", "Cyril Allauzen"], "title": "Tutorial: $\\varphi$-Transductions in OpenFst via the Gallic Semiring", "categories": ["cs.FL", "cs.CL"], "comment": "8 pages, 2 figures, code included", "summary": "OpenFst, a popular finite-state transducer library, supports\n$\\varphi$-transitions but, due to an implementation constraint, they cannot be\nused with transducers in a straightforward way.\n  In this short tutorial, we describe how one can use other functionality\nprovided by OpenFst (namely, the Gallic semiring) to correctly implement\n$\\varphi$-transductions and demonstrate it by implementing the MaxMatch\n(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).\nAccompanying self-contained code examples are provided.\nhttps://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "paper_title_zh": "Error", "abstract_zh": "Error"}}
{"id": "2506.18261", "pdf": "https://arxiv.org/pdf/2506.18261", "abs": "https://arxiv.org/abs/2506.18261", "authors": ["Rui Su", "Dong Xu", "Luping Zhou", "Wanli Ouyang"], "title": "Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Weakly supervised temporal action localization is a challenging task as only\nthe video-level annotation is available during the training process. To address\nthis problem, we propose a two-stage approach to fully exploit multi-resolution\ninformation in the temporal domain and generate high quality frame-level pseudo\nlabels based on both appearance and motion streams. Specifically, in the first\nstage, we generate reliable initial frame-level pseudo labels, and in the\nsecond stage, we iteratively refine the pseudo labels and use a set of selected\nframes with highly confident pseudo labels to train neural networks and better\npredict action class scores at each frame. We fully exploit temporal\ninformation at multiple scales to improve temporal action localization\nperformance. Specifically, in order to obtain reliable initial frame-level\npseudo labels, in the first stage, we propose an Initial Label Generation (ILG)\nmodule, which leverages temporal multi-resolution consistency to generate high\nquality class activation sequences (CASs), which consist of a number of\nsequences with each sequence measuring how likely each video frame belongs to\none specific action class. In the second stage, we propose a Progressive\nTemporal Label Refinement (PTLR) framework. In our PTLR framework, two networks\ncalled Network-OTS and Network-RTS, which are respectively used to generate\nCASs for the original temporal scale and the reduced temporal scales, are used\nas two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo\nlabels in turn. By this way, the multi-resolution information in the temporal\ndomain is exchanged at the pseudo label level, and our work can help improve\neach stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels\nfrom another stream (i.e., the RTS/OTS stream).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf\u5e27\u7ea7\u4f2a\u6807\u7b7e\uff0c\u4ee5\u6539\u8fdb\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u3002", "motivation": "\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u4ec5\u4f9d\u8d56\u89c6\u9891\u7ea7\u6807\u6ce8\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u5e27\u7ea7\u76d1\u7763\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u4ece\u800c\u6539\u8fdb\u5b9a\u4f4d\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u521d\u59cb\u6807\u7b7e\u751f\u6210\u9636\u6bb5\uff0c\u5229\u7528\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf\u7c7b\u522b\u6fc0\u6d3b\u5e8f\u5217\uff1b2) \u6e10\u8fdb\u5f0f\u6807\u7b7e\u7ec6\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u4e24\u4e2a\u7f51\u7edc\uff08OTS\u548cRTS\u6d41\uff09\u4ea4\u66ff\u4f18\u5316\u4f2a\u6807\u7b7e\uff0c\u5b9e\u73b0\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u4ea4\u6362\u548c\u4f2a\u6807\u7b7e\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "paper_title_zh": "\u901a\u8fc7\u5229\u7528\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u6539\u8fdb\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d", "abstract_zh": "\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ec5\u63d0\u4f9b\u89c6\u9891\u7ea7\u6807\u6ce8\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5145\u5206\u5229\u7528\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u5e76\u57fa\u4e8e\u5916\u89c2\u548c\u8fd0\u52a8\u6d41\u751f\u6210\u9ad8\u8d28\u91cf\u5e27\u7ea7\u4f2a\u6807\u7b7e\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u53ef\u9760\u7684\u521d\u59cb\u5e27\u7ea7\u4f2a\u6807\u7b7e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fed\u4ee3\u4f18\u5316\u4f2a\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u7684\u5e27\u96c6\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u66f4\u597d\u5730\u9884\u6d4b\u6bcf\u5e27\u7684\u52a8\u4f5c\u7c7b\u522b\u5206\u6570\u3002\u6211\u4eec\u5145\u5206\u6316\u6398\u591a\u5c3a\u5ea6\u65f6\u5e8f\u4fe1\u606f\u4ee5\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u521d\u59cb\u6807\u7b7e\u751f\u6210\uff08ILG\uff09\u6a21\u5757\uff0c\u5229\u7528\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7c7b\u522b\u6fc0\u6d3b\u5e8f\u5217\uff08CASs\uff09\uff0c\u6bcf\u4e2a\u5e8f\u5217\u8861\u91cf\u89c6\u9891\u5e27\u5c5e\u4e8e\u7279\u5b9a\u52a8\u4f5c\u7c7b\u522b\u7684\u53ef\u80fd\u6027\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u6e10\u8fdb\u5f0f\u65f6\u5e8f\u6807\u7b7e\u7ec6\u5316\uff08PTLR\uff09\u6846\u67b6\uff0c\u5176\u4e2d\u4e24\u4e2a\u7f51\u7edc\uff08Network-OTS\u548cNetwork-RTS\uff09\u5206\u522b\u7528\u4e8e\u751f\u6210\u539f\u59cb\u65f6\u5e8f\u5c3a\u5ea6\u548c\u7f29\u51cf\u65f6\u5e8f\u5c3a\u5ea6\u7684CASs\uff0c\u4f5c\u4e3a\u4e24\u4e2a\u6d41\uff08OTS\u6d41\u548cRTS\u6d41\uff09\u4ea4\u66ff\u4f18\u5316\u4f2a\u6807\u7b7e\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u65f6\u57df\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u5728\u4f2a\u6807\u7b7e\u5c42\u9762\u4ea4\u6362\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u901a\u8fc7\u5229\u7528\u53e6\u4e00\u6d41\uff08RTS/OTS\u6d41\uff09\u7684\u4f18\u5316\u4f2a\u6807\u7b7e\u6765\u63d0\u5347\u6bcf\u4e2a\u6d41\uff08OTS/RTS\u6d41\uff09\u7684\u6027\u80fd\u3002"}}
{"id": "2506.17347", "pdf": "https://arxiv.org/pdf/2506.17347", "abs": "https://arxiv.org/abs/2506.17347", "authors": ["Jennifer Wang", "Andrew Selbst", "Solon Barocas", "Suresh Venkatasubramanian"], "title": "Distinguishing Predictive and Generative AI in Regulation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Over the past decade, policymakers have developed a set of regulatory tools\nto ensure AI development aligns with key societal goals. Many of these tools\nwere initially developed in response to concerns with predictive AI and\ntherefore encode certain assumptions about the nature of AI systems and the\nutility of certain regulatory approaches. With the advent of generative AI,\nhowever, some of these assumptions no longer hold, even as policymakers attempt\nto maintain a single regulatory target that covers both types of AI.\n  In this paper, we identify four distinct aspects of generative AI that call\nfor meaningfully different policy responses. These are the generality and\nadaptability of generative AI that make it a poor regulatory target, the\ndifficulty of designing effective evaluations, new legal concerns that change\nthe ecosystem of stakeholders and sources of expertise, and the distributed\nstructure of the generative AI value chain.\n  In light of these distinctions, policymakers will need to evaluate where the\npast decade of policy work remains relevant and where new policies, designed to\naddress the unique risks posed by generative AI, are necessary. We outline\nthree recommendations for policymakers to more effectively identify regulatory\ntargets and leverage constraints across the broader ecosystem to govern\ngenerative AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u4e0e\u9884\u6d4b\u5f0fAI\u5728\u76d1\u7ba1\u4e0a\u7684\u5dee\u5f02\uff0c\u6307\u51fa\u73b0\u6709\u76d1\u7ba1\u5de5\u5177\u5bf9\u751f\u6210\u5f0fAI\u7684\u9002\u7528\u6027\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u56db\u70b9\u5173\u952e\u533a\u522b\u53ca\u4e09\u6761\u653f\u7b56\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5174\u8d77\uff0c\u73b0\u6709\u57fa\u4e8e\u9884\u6d4b\u5f0fAI\u7684\u76d1\u7ba1\u5de5\u5177\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u653f\u7b56\u5236\u5b9a\u8005\u9700\u91cd\u65b0\u8bc4\u4f30\u76d1\u7ba1\u6846\u67b6\u4ee5\u9002\u5e94\u65b0\u6280\u672f\u7684\u7279\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u6790\u751f\u6210\u5f0fAI\u7684\u56db\u4e2a\u72ec\u7279\u65b9\u9762\uff08\u901a\u7528\u6027\u3001\u8bc4\u4f30\u96be\u5ea6\u3001\u6cd5\u5f8b\u95ee\u9898\u53ca\u4ef7\u503c\u94fe\u7ed3\u6784\uff09\uff0c\u63d0\u51fa\u5176\u4e0e\u9884\u6d4b\u5f0fAI\u7684\u5dee\u5f02\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u653f\u7b56\u5efa\u8bae\u3002", "result": "\u8bc6\u522b\u51fa\u751f\u6210\u5f0fAI\u7684\u56db\u4e2a\u5173\u952e\u7279\u5f81\uff0c\u8868\u660e\u73b0\u6709\u76d1\u7ba1\u5de5\u5177\u5bf9\u5176\u4e0d\u9002\u7528\uff0c\u5e76\u5efa\u8bae\u653f\u7b56\u5236\u5b9a\u8005\u8c03\u6574\u76d1\u7ba1\u76ee\u6807\u548c\u7ea6\u675f\u673a\u5236\u3002", "conclusion": "\u653f\u7b56\u5236\u5b9a\u8005\u9700\u533a\u5206\u751f\u6210\u5f0fAI\u4e0e\u9884\u6d4b\u5f0fAI\u7684\u76d1\u7ba1\u9700\u6c42\uff0c\u91cd\u65b0\u8bbe\u8ba1\u653f\u7b56\u4ee5\u5e94\u5bf9\u751f\u6210\u5f0fAI\u7684\u72ec\u7279\u98ce\u9669\u3002", "paper_title_zh": "\u76d1\u7ba1\u4e2d\u533a\u5206\u9884\u6d4b\u5f0fAI\u4e0e\u751f\u6210\u5f0fAI", "abstract_zh": "\u8fc7\u53bb\u5341\u5e74\uff0c\u653f\u7b56\u5236\u5b9a\u8005\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u76d1\u7ba1\u5de5\u5177\uff0c\u4ee5\u786e\u4fddAI\u53d1\u5c55\u7b26\u5408\u793e\u4f1a\u76ee\u6807\u3002\u8fd9\u4e9b\u5de5\u5177\u6700\u521d\u662f\u4e3a\u5e94\u5bf9\u9884\u6d4b\u5f0fAI\u7684\u95ee\u9898\u800c\u8bbe\u8ba1\uff0c\u56e0\u6b64\u9690\u542b\u4e86\u5bf9AI\u7cfb\u7edf\u6027\u8d28\u548c\u76d1\u7ba1\u6548\u7528\u7684\u5047\u8bbe\u3002\u7136\u800c\uff0c\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u51fa\u73b0\uff0c\u8fd9\u4e9b\u5047\u8bbe\u4e0d\u518d\u6210\u7acb\uff0c\u5c3d\u7ba1\u653f\u7b56\u5236\u5b9a\u8005\u8bd5\u56fe\u5c06\u4e24\u8005\u7eb3\u5165\u540c\u4e00\u76d1\u7ba1\u6846\u67b6\u3002\u672c\u6587\u6307\u51fa\u751f\u6210\u5f0fAI\u7684\u56db\u4e2a\u72ec\u7279\u65b9\u9762\uff0c\u9700\u8981\u4e0d\u540c\u7684\u653f\u7b56\u5e94\u5bf9\uff1a\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u4f7f\u5176\u96be\u4ee5\u6210\u4e3a\u76d1\u7ba1\u76ee\u6807\u3001\u8bc4\u4f30\u8bbe\u8ba1\u7684\u56f0\u96be\u3001\u65b0\u7684\u6cd5\u5f8b\u95ee\u9898\u6539\u53d8\u4e86\u5229\u76ca\u76f8\u5173\u8005\u548c\u4e13\u5bb6\u6765\u6e90\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u4ee5\u53ca\u751f\u6210\u5f0fAI\u4ef7\u503c\u94fe\u7684\u5206\u5e03\u5f0f\u7ed3\u6784\u3002\u57fa\u4e8e\u8fd9\u4e9b\u5dee\u5f02\uff0c\u653f\u7b56\u5236\u5b9a\u8005\u9700\u8bc4\u4f30\u8fc7\u53bb\u5341\u5e74\u7684\u76d1\u7ba1\u5de5\u4f5c\u662f\u5426\u4ecd\u9002\u7528\uff0c\u5e76\u8bbe\u8ba1\u65b0\u653f\u7b56\u4ee5\u5e94\u5bf9\u751f\u6210\u5f0fAI\u7684\u72ec\u7279\u98ce\u9669\u3002\u672c\u6587\u63d0\u51fa\u4e09\u6761\u5efa\u8bae\uff0c\u5e2e\u52a9\u653f\u7b56\u5236\u5b9a\u8005\u66f4\u6709\u6548\u5730\u8bc6\u522b\u76d1\u7ba1\u76ee\u6807\u5e76\u5229\u7528\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u7ea6\u675f\u6765\u6cbb\u7406\u751f\u6210\u5f0fAI\u3002"}}
{"id": "2506.18266", "pdf": "https://arxiv.org/pdf/2506.18266", "abs": "https://arxiv.org/abs/2506.18266", "authors": ["Haoming Chen", "Lichen Yuan", "TianFang Sun", "Jingyu Gong", "Xin Tan", "Zhizhong Zhang", "Yuan Xie"], "title": "YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction in the past was considered to require\nprecise geometric relationships in order to enable effective training. However,\nin complex indoor environments, the large-scale and widespread collection of\ndata, along with the necessity for fine-grained annotations, becomes\nimpractical due to the complexity of data acquisition setups and privacy\nconcerns. In this paper, we demonstrate that 3D spatially-accurate training can\nbe achieved using only indoor Internet data, without the need for any\npre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we\ncollect a web dataset, YouTube-Occ, which comprises house tour videos from\nYouTube, providing abundant real house scenes for 3D representation learning.\nUpon on this web dataset, we establish a fully self-supervised model to\nleverage accessible 2D prior knowledge for reaching powerful 3D indoor\nperception. Specifically, we harness the advantages of the prosperous vision\nfoundation models, distilling the 2D region-level knowledge into the occupancy\nnetwork by grouping the similar pixels into superpixels. Experimental results\nshow that our method achieves state-of-the-art zero-shot performance on two\npopular benchmarks (NYUv2 and OccScanNet", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528YouTube\u5ba4\u5185\u89c6\u9891\u8fdb\u884c3D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u76f8\u673a\u53c2\u6570\uff0c\u901a\u8fc72D\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u9ad8\u6548\u76843D\u611f\u77e5\u3002", "motivation": "\u4f20\u7edf3D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u4f9d\u8d56\u7cbe\u786e\u51e0\u4f55\u5173\u7cfb\uff0c\u4f46\u590d\u6742\u5ba4\u5185\u73af\u5883\u7684\u6570\u636e\u91c7\u96c6\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u95ee\u9898\u7a81\u51fa\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u4e92\u8054\u7f51\u89c6\u9891\u6570\u636e\uff08\u5982YouTube\u623f\u5c4b\u6e38\u89c8\u89c6\u9891\uff09\u5b9e\u73b0\u65e0\u9700\u76f8\u673a\u53c2\u6570\u76843D\u8bad\u7ec3\u3002", "method": "\u6536\u96c6YouTube\u623f\u5c4b\u6e38\u89c8\u89c6\u9891\u6784\u5efa\u6570\u636e\u96c6YouTube-Occ\uff0c\u63d0\u51fa\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5c062D\u533a\u57df\u7ea7\u77e5\u8bc6\uff08\u5982\u8d85\u50cf\u7d20\u5206\u7ec4\uff09\u84b8\u998f\u5230\u5360\u636e\u7f51\u7edc\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728NYUv2\u548cOccScanNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u72b6\u6001\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4e92\u8054\u7f51\u89c6\u9891\u6570\u636e\uff0c\u65e0\u9700\u76f8\u673a\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\uff0c\u4e3a\u590d\u6742\u5ba4\u5185\u73af\u5883\u76843D\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "YouTube-Occ\uff1a\u4eceYouTube\u89c6\u9891\u5b66\u4e60\u5ba4\u51853D\u8bed\u4e49\u5360\u636e\u9884\u6d4b", "abstract_zh": "\u8fc7\u53bb\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u88ab\u8ba4\u4e3a\u9700\u8981\u7cbe\u786e\u7684\u51e0\u4f55\u5173\u7cfb\u4ee5\u5b9e\u73b0\u6709\u6548\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u5927\u89c4\u6a21\u6570\u636e\u91c7\u96c6\u548c\u7ec6\u7c92\u5ea6\u6807\u6ce8\u56e0\u6570\u636e\u83b7\u53d6\u590d\u6742\u6027\u548c\u9690\u79c1\u95ee\u9898\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u6587\u8bc1\u660e\uff0c\u4ec5\u4f7f\u7528\u5ba4\u5185\u4e92\u8054\u7f51\u6570\u636e\u5373\u53ef\u5b9e\u73b03D\u7a7a\u95f4\u7cbe\u786e\u8bad\u7ec3\uff0c\u65e0\u9700\u4efb\u4f55\u76f8\u673a\u5185\u5916\u53c2\u6570\u5148\u9a8c\u77e5\u8bc6\u3002\u6211\u4eec\u6784\u5efa\u4e86YouTube-Occ\u6570\u636e\u96c6\uff0c\u5305\u542bYouTube\u623f\u5c4b\u6e38\u89c8\u89c6\u9891\uff0c\u4e3a3D\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e30\u5bcc\u771f\u5b9e\u573a\u666f\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u5b8c\u5168\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u5229\u7528\u53ef\u8bbf\u95ee\u76842D\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u5f3a\u5927\u76843D\u5ba4\u5185\u611f\u77e5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5c06\u76f8\u4f3c\u50cf\u7d20\u5206\u7ec4\u4e3a\u8d85\u50cf\u7d20\uff0c\u5c062D\u533a\u57df\u7ea7\u77e5\u8bc6\u84b8\u998f\u5230\u5360\u636e\u7f51\u7edc\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\uff08NYUv2\u548cOccScanNet\uff09\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u72b6\u6001\u7684\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2506.17348", "pdf": "https://arxiv.org/pdf/2506.17348", "abs": "https://arxiv.org/abs/2506.17348", "authors": ["Pavel Malinovskiy"], "title": "Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook", "categories": ["cs.MA", "cs.AI", "I.2.11; F.2.2"], "comment": "43 pages, 7 figures, 30 references", "summary": "This paper presents a substantially reworked examination of how advanced\ngame-theoretic paradigms can serve as a foundation for the next-generation\nchallenges in Artificial Intelligence (AI), forecasted to arrive in or around\n2025. Our focus extends beyond traditional models by incorporating dynamic\ncoalition formation, language-based utilities, sabotage risks, and partial\nobservability. We provide a set of mathematical formalisms, simulations, and\ncoding schemes that illustrate how multi-agent AI systems may adapt and\nnegotiate in complex environments. Key elements include repeated games,\nBayesian updates for adversarial detection, and moral framing within payoff\nstructures. This work aims to equip AI researchers with robust theoretical\ntools for aligning strategic interaction in uncertain, partially adversarial\ncontexts.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e862025\u5e74\u591a\u667a\u80fd\u4f53AI\u6311\u6218\u4e2d\u9ad8\u7ea7\u535a\u5f08\u8bba\u6846\u67b6\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u52a8\u6001\u8054\u76df\u5f62\u6210\u3001\u8bed\u8a00\u6548\u7528\u3001\u7834\u574f\u98ce\u9669\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u65b0\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u5de5\u5177\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6218\u7565\u4ea4\u4e92\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9ad8\u7ea7\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u4e3a2025\u5e74\u53ef\u80fd\u51fa\u73b0\u7684\u591a\u667a\u80fd\u4f53AI\u6311\u6218\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u7ed3\u5408\u52a8\u6001\u8054\u76df\u5f62\u6210\u3001\u8bed\u8a00\u6548\u7528\u3001\u7834\u574f\u98ce\u9669\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u65b0\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6570\u5b66\u5f62\u5f0f\u5316\u65b9\u6cd5\u3001\u6a21\u62df\u5b9e\u9a8c\u548c\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u91cd\u590d\u535a\u5f08\u3001\u8d1d\u53f6\u65af\u66f4\u65b0\u548c\u9053\u5fb7\u6846\u67b6\u7b49\u5de5\u5177\u5206\u6790\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9002\u5e94\u4e0e\u534f\u5546\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u535a\u5f08\u8bba\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u548c\u90e8\u5206\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u6218\u7565\u4ea4\u4e92\uff0c\u4e3aAI\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7406\u8bba\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u4e3a2025\u5e74\u591a\u667a\u80fd\u4f53AI\u6311\u6218\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u7684\u535a\u5f08\u8bba\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u52a8\u6001\u8054\u76df\u548c\u9053\u5fb7\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "paper_title_zh": "\u9762\u5411\u591a\u667a\u80fd\u4f53AI\u6311\u6218\u7684\u9ad8\u7ea7\u535a\u5f08\u8bba\u6846\u67b6\uff1a2025\u5e74\u5c55\u671b", "abstract_zh": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u9ad8\u7ea7\u535a\u5f08\u8bba\u8303\u5f0f\u5982\u4f55\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6311\u6218\u7684\u57fa\u7840\uff0c\u9884\u8ba1\u8fd9\u4e9b\u6311\u6218\u5c06\u57282025\u5e74\u5de6\u53f3\u51fa\u73b0\u3002\u6211\u4eec\u7684\u7814\u7a76\u8d85\u8d8a\u4e86\u4f20\u7edf\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u52a8\u6001\u8054\u76df\u5f62\u6210\u3001\u8bed\u8a00\u6548\u7528\u3001\u7834\u574f\u98ce\u9669\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u65b0\u6982\u5ff5\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u5957\u6570\u5b66\u5f62\u5f0f\u5316\u65b9\u6cd5\u3001\u6a21\u62df\u5b9e\u9a8c\u548c\u7f16\u7801\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5982\u4f55\u5728\u590d\u6742\u73af\u5883\u4e2d\u9002\u5e94\u548c\u534f\u5546\u3002\u5173\u952e\u8981\u7d20\u5305\u62ec\u91cd\u590d\u535a\u5f08\u3001\u7528\u4e8e\u5bf9\u6297\u68c0\u6d4b\u7684\u8d1d\u53f6\u65af\u66f4\u65b0\u4ee5\u53ca\u6536\u76ca\u7ed3\u6784\u4e2d\u7684\u9053\u5fb7\u6846\u67b6\u3002\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u4e3aAI\u7814\u7a76\u8005\u63d0\u4f9b\u5f3a\u5927\u7684\u7406\u8bba\u5de5\u5177\uff0c\u4ee5\u5728\u4e0d\u786e\u5b9a\u548c\u90e8\u5206\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5bf9\u9f50\u6218\u7565\u4ea4\u4e92\u3002"}}
{"id": "2506.18045", "pdf": "https://arxiv.org/pdf/2506.18045", "abs": "https://arxiv.org/abs/2506.18045", "authors": ["I. Loaiza", "R. Vestrelli", "A. Fronzetti Colladon", "R. Rigobon"], "title": "The Democratic Paradox in Large Language Models' Underestimation of Press Freedom", "categories": ["cs.CY", "cs.AI", "cs.CL", "K.4; I.2.7; I.2.0"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly mediate global information\naccess for millions of users worldwide, their alignment and biases have the\npotential to shape public understanding and trust in fundamental democratic\ninstitutions, such as press freedom. In this study, we uncover three systematic\ndistortions in the way six popular LLMs evaluate press freedom in 180 countries\ncompared to expert assessments of the World Press Freedom Index (WPFI). The six\nLLMs exhibit a negative misalignment, consistently underestimating press\nfreedom, with individual models rating between 71% to 93% of countries as less\nfree. We also identify a paradoxical pattern we term differential misalignment:\nLLMs disproportionately underestimate press freedom in countries where it is\nstrongest. Additionally, five of the six LLMs exhibit positive home bias,\nrating their home countries' press freedoms more favorably than would be\nexpected given their negative misalignment with the human benchmark. In some\ncases, LLMs rate their home countries between 7% to 260% more positively than\nexpected. If LLMs are set to become the next search engines and some of the\nmost important cultural tools of our time, they must ensure accurate\nrepresentations of the state of our human and civic rights globally.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u516d\u79cd\u6d41\u884c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc4\u4f30180\u4e2a\u56fd\u5bb6\u65b0\u95fb\u81ea\u7531\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u666e\u904d\u4f4e\u4f30\u65b0\u95fb\u81ea\u7531\uff0c\u4e14\u5bf9\u672c\u56fd\u5b58\u5728\u6b63\u5411\u504f\u89c1\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5168\u7403\u4fe1\u606f\u83b7\u53d6\u4e2d\u7684\u89d2\u8272\u65e5\u76ca\u91cd\u8981\uff0c\u5176\u504f\u89c1\u53ef\u80fd\u5f71\u54cd\u516c\u4f17\u5bf9\u65b0\u95fb\u81ea\u7531\u7b49\u6c11\u4e3b\u5236\u5ea6\u7684\u7406\u89e3\u4e0e\u4fe1\u4efb\uff0c\u56e0\u6b64\u9700\u7814\u7a76\u5176\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cdLLMs\u5bf9180\u4e2a\u56fd\u5bb6\u65b0\u95fb\u81ea\u7531\u7684\u8bc4\u4f30\u4e0e\u4e16\u754c\u65b0\u95fb\u81ea\u7531\u6307\u6570\uff08WPFI\uff09\u7684\u4e13\u5bb6\u8bc4\u4f30\uff0c\u5206\u6790\u5176\u504f\u5dee\u6a21\u5f0f\u3002", "result": "LLMs\u666e\u904d\u4f4e\u4f30\u65b0\u95fb\u81ea\u7531\uff0871%-93%\u56fd\u5bb6\u88ab\u4f4e\u4f30\uff09\uff0c\u4e14\u5728\u65b0\u95fb\u81ea\u7531\u8f83\u9ad8\u7684\u56fd\u5bb6\u4f4e\u4f30\u66f4\u4e25\u91cd\uff1b\u4e94\u79cdLLMs\u5bf9\u672c\u56fd\u5b58\u5728\u6b63\u5411\u504f\u89c1\uff08\u9ad8\u4f307%-260%\uff09\u3002", "conclusion": "\u82e5LLMs\u6210\u4e3a\u4e0b\u4e00\u4ee3\u641c\u7d22\u5f15\u64ce\u548c\u6587\u5316\u5de5\u5177\uff0c\u5fc5\u987b\u786e\u4fdd\u5176\u5bf9\u5168\u7403\u4eba\u6743\u4e0e\u516c\u6c11\u6743\u5229\u7684\u51c6\u786e\u8bc4\u4f30\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f4e\u4f30\u65b0\u95fb\u81ea\u7531\u7684\u6c11\u4e3b\u6096\u8bba", "abstract_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5168\u7403\u8303\u56f4\u5185\u4e3a\u6570\u767e\u4e07\u4eba\u63d0\u4f9b\u4fe1\u606f\u4e2d\u4ecb\u670d\u52a1\uff0c\u5176\u504f\u89c1\u53ef\u80fd\u5f71\u54cd\u516c\u4f17\u5bf9\u65b0\u95fb\u81ea\u7531\u7b49\u6c11\u4e3b\u5236\u5ea6\u7684\u7406\u89e3\u4e0e\u4fe1\u4efb\u3002\u672c\u7814\u7a76\u63ed\u793a\u4e86\u516d\u79cd\u6d41\u884cLLMs\u5728\u8bc4\u4f30180\u4e2a\u56fd\u5bb6\u65b0\u95fb\u81ea\u7531\u65f6\u4e0e\u4e16\u754c\u65b0\u95fb\u81ea\u7531\u6307\u6570\uff08WPFI\uff09\u4e13\u5bb6\u8bc4\u4f30\u76f8\u6bd4\u5b58\u5728\u7684\u4e09\u79cd\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u8fd9\u4e9bLLMs\u666e\u904d\u4f4e\u4f30\u65b0\u95fb\u81ea\u7531\uff0c71%\u81f393%\u7684\u56fd\u5bb6\u88ab\u4f4e\u4f30\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5dee\u5f02\u504f\u5dee\u201d\u7684\u6096\u8bba\u6a21\u5f0f\uff1aLLMs\u5bf9\u65b0\u95fb\u81ea\u7531\u8f83\u9ad8\u7684\u56fd\u5bb6\u4f4e\u4f30\u66f4\u4e25\u91cd\u3002\u6b64\u5916\uff0c\u516d\u79cd\u6a21\u578b\u4e2d\u6709\u4e94\u79cd\u5bf9\u672c\u56fd\u5b58\u5728\u6b63\u5411\u504f\u89c1\uff0c\u9ad8\u4f30\u672c\u56fd\u65b0\u95fb\u81ea\u7531\uff087%\u81f3260%\uff09\u3002\u82e5LLMs\u6210\u4e3a\u4e0b\u4e00\u4ee3\u641c\u7d22\u5f15\u64ce\u548c\u6587\u5316\u5de5\u5177\uff0c\u5fc5\u987b\u786e\u4fdd\u5176\u5bf9\u5168\u7403\u4eba\u6743\u4e0e\u516c\u6c11\u6743\u5229\u7684\u51c6\u786e\u8bc4\u4f30\u3002"}}
{"id": "2506.18268", "pdf": "https://arxiv.org/pdf/2506.18268", "abs": "https://arxiv.org/abs/2506.18268", "authors": ["Yu Liu", "Yangtao Meng", "Xianfei Pan", "Jie Jiang", "Changhao Chen"], "title": "ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments", "categories": ["cs.CV"], "comment": "8 pages, 3 figures, accepted to IROS 2025", "summary": "Thermal cameras capture environmental data through heat emission, a\nfundamentally different mechanism compared to visible light cameras, which rely\non pinhole imaging. As a result, traditional visual relocalization methods\ndesigned for visible light images are not directly applicable to thermal\nimages. Despite significant advancements in deep learning for camera\nrelocalization, approaches specifically tailored for thermal camera-based\nrelocalization remain underexplored. To address this gap, we introduce\nThermalLoc, a novel end-to-end deep learning method for thermal image\nrelocalization. ThermalLoc effectively extracts both local and global features\nfrom thermal images by integrating EfficientNet with Transformers, and performs\nabsolute pose regression using two MLP networks. We evaluated ThermalLoc on\nboth the publicly available thermal-odometry dataset and our own dataset. The\nresults demonstrate that ThermalLoc outperforms existing representative methods\nemployed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,\nand RobustLoc, achieving superior accuracy and robustness.", "AI": {"tldr": "ThermalLoc\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u70ed\u6210\u50cf\u76f8\u673a\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u91cd\u5b9a\u4f4d\uff0c\u7ed3\u5408EfficientNet\u548cTransformer\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7MLP\u7f51\u7edc\u5b9e\u73b0\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u57fa\u4e8e\u53ef\u89c1\u5149\u56fe\u50cf\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u70ed\u6210\u50cf\u56fe\u50cf\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u76f8\u673a\u91cd\u5b9a\u4f4d\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u70ed\u6210\u50cf\u76f8\u673a\u7684\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "ThermalLoc\u7ed3\u5408EfficientNet\u548cTransformer\u63d0\u53d6\u70ed\u6210\u50cf\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2aMLP\u7f51\u7edc\u8fdb\u884c\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cThermalLoc\u5728\u70ed\u6210\u50cf\u76f8\u673a\u91cd\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f18\u4e8eAtLoc\u3001MapNet\u3001PoseNet\u548cRobustLoc\u7b49\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ThermalLoc\u586b\u8865\u4e86\u70ed\u6210\u50cf\u76f8\u673a\u91cd\u5b9a\u4f4d\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u91cd\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "ThermalLoc\uff1a\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u70ed\u6210\u50cf\u76f8\u673a\u9c81\u68d2\u91cd\u5b9a\u4f4d\u65b9\u6cd5", "abstract_zh": "\u70ed\u6210\u50cf\u76f8\u673a\u901a\u8fc7\u70ed\u8f90\u5c04\u6355\u6349\u73af\u5883\u6570\u636e\uff0c\u5176\u673a\u5236\u4e0e\u57fa\u4e8e\u9488\u5b54\u6210\u50cf\u7684\u53ef\u89c1\u5149\u76f8\u673a\u622a\u7136\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u4f20\u7edf\u9488\u5bf9\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u89c6\u89c9\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u70ed\u6210\u50cf\u56fe\u50cf\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u76f8\u673a\u91cd\u5b9a\u4f4d\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e13\u95e8\u9488\u5bf9\u70ed\u6210\u50cf\u76f8\u673a\u91cd\u5b9a\u4f4d\u7684\u65b9\u6cd5\u4ecd\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ThermalLoc\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u70ed\u6210\u50cf\u56fe\u50cf\u7684\u91cd\u5b9a\u4f4d\u3002ThermalLoc\u901a\u8fc7\u7ed3\u5408EfficientNet\u548cTransformer\uff0c\u6709\u6548\u63d0\u53d6\u70ed\u6210\u50cf\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4e24\u4e2aMLP\u7f51\u7edc\u5b9e\u73b0\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\u3002\u6211\u4eec\u5728\u516c\u5f00\u7684\u70ed\u6210\u50cf-\u91cc\u7a0b\u8ba1\u6570\u636e\u96c6\u548c\u81ea\u6709\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86ThermalLoc\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0cThermalLoc\u5728\u70ed\u6210\u50cf\u76f8\u673a\u91cd\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\uff08\u5982AtLoc\u3001MapNet\u3001PoseNet\u548cRobustLoc\uff09\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.17350", "pdf": "https://arxiv.org/pdf/2506.17350", "abs": "https://arxiv.org/abs/2506.17350", "authors": ["Yinghao Wu", "Liyan Zhang"], "title": "CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Backdoor attacks have emerged as a critical security threat against deep\nneural networks in recent years. The majority of existing backdoor attacks\nfocus on targeted backdoor attacks, where trigger is strongly associated to\nspecific malicious behavior. Various backdoor detection methods depend on this\ninherent property and shows effective results in identifying and mitigating\nsuch targeted attacks. However, a purely untargeted attack in backdoor\nscenarios is, in some sense, self-weakening, since the target nature is what\nmakes backdoor attacks so powerful. In light of this, we introduce a novel\nConstrained Untargeted Backdoor Attack (CUBA), which combines the flexibility\nof untargeted attacks with the intentionality of targeted attacks. The\ncompromised model, when presented with backdoor images, will classify them into\nrandom classes within a constrained range of target classes selected by the\nattacker. This combination of randomness and determinedness enables the\nproposed untargeted backdoor attack to natively circumvent existing backdoor\ndefense methods. To implement the untargeted backdoor attack under controlled\nflexibility, we propose to apply logit normalization on cross-entropy loss with\nflipped one-hot labels. By constraining the logit during training, the\ncompromised model will show a uniform distribution across selected target\nclasses, resulting in controlled untargeted attack. Extensive experiments\ndemonstrate the effectiveness of the proposed CUBA on different datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53d7\u63a7\u65e0\u76ee\u6807\u540e\u95e8\u653b\u51fb\uff08CUBA\uff09\uff0c\u7ed3\u5408\u65e0\u76ee\u6807\u653b\u51fb\u7684\u7075\u6d3b\u6027\u548c\u6709\u76ee\u6807\u653b\u51fb\u7684\u610f\u56fe\u6027\uff0c\u901a\u8fc7\u7ea6\u675f\u76ee\u6807\u7c7b\u522b\u8303\u56f4\u5b9e\u73b0\u968f\u673a\u5206\u7c7b\uff0c\u6709\u6548\u89c4\u907f\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u591a\u4e3a\u6709\u76ee\u6807\u653b\u51fb\uff0c\u5176\u89e6\u53d1\u673a\u5236\u4e0e\u7279\u5b9a\u6076\u610f\u884c\u4e3a\u5f3a\u5173\u8054\uff0c\u800c\u7eaf\u7cb9\u7684\u65e0\u76ee\u6807\u653b\u51fb\u56e0\u7f3a\u4e4f\u76ee\u6807\u6027\u800c\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u51fa\u4e00\u79cd\u65e2\u80fd\u7075\u6d3b\u653b\u51fb\u53c8\u80fd\u89c4\u907f\u9632\u5fa1\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u5bf9\u6570\u5f52\u4e00\u5316\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u4e2d\u4f7f\u7528\u7ffb\u8f6c\u7684\u72ec\u70ed\u6807\u7b7e\uff0c\u7ea6\u675f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u6570\u5206\u5e03\uff0c\u4f7f\u53d7\u611f\u67d3\u6a21\u578b\u5728\u9009\u5b9a\u76ee\u6807\u7c7b\u522b\u8303\u56f4\u5185\u5448\u73b0\u5747\u5300\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u53d7\u63a7\u7684\u65e0\u76ee\u6807\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCUBA\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u6709\u6548\u5b9e\u73b0\u53d7\u63a7\u65e0\u76ee\u6807\u653b\u51fb\uff0c\u6210\u529f\u89c4\u907f\u73b0\u6709\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "CUBA\u901a\u8fc7\u7ed3\u5408\u65e0\u76ee\u6807\u653b\u51fb\u7684\u7075\u6d3b\u6027\u548c\u6709\u76ee\u6807\u653b\u51fb\u7684\u610f\u56fe\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u4e3a\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\u3002", "paper_title_zh": "CUBA\uff1a\u9488\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53d7\u63a7\u65e0\u76ee\u6807\u540e\u95e8\u653b\u51fb", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u540e\u95e8\u653b\u51fb\u5df2\u6210\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9762\u4e34\u7684\u91cd\u8981\u5b89\u5168\u5a01\u80c1\u3002\u73b0\u6709\u540e\u95e8\u653b\u51fb\u591a\u4e3a\u6709\u76ee\u6807\u653b\u51fb\uff0c\u5176\u89e6\u53d1\u673a\u5236\u4e0e\u7279\u5b9a\u6076\u610f\u884c\u4e3a\u5f3a\u5173\u8054\u3002\u591a\u6570\u540e\u95e8\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u8fd9\u4e00\u56fa\u6709\u7279\u6027\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u548c\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\u3002\u7136\u800c\uff0c\u7eaf\u7cb9\u7684\u65e0\u76ee\u6807\u653b\u51fb\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u81ea\u6211\u524a\u5f31\uff0c\u56e0\u4e3a\u76ee\u6807\u6027\u6b63\u662f\u540e\u95e8\u653b\u51fb\u7684\u5f3a\u5927\u4e4b\u5904\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53d7\u63a7\u65e0\u76ee\u6807\u540e\u95e8\u653b\u51fb\uff08CUBA\uff09\uff0c\u7ed3\u5408\u4e86\u65e0\u76ee\u6807\u653b\u51fb\u7684\u7075\u6d3b\u6027\u548c\u6709\u76ee\u6807\u653b\u51fb\u7684\u610f\u56fe\u6027\u3002\u53d7\u611f\u67d3\u7684\u6a21\u578b\u5728\u9762\u5bf9\u540e\u95e8\u56fe\u50cf\u65f6\uff0c\u4f1a\u5c06\u5176\u5206\u7c7b\u5230\u653b\u51fb\u8005\u9009\u5b9a\u7684\u76ee\u6807\u7c7b\u522b\u8303\u56f4\u5185\u7684\u968f\u673a\u7c7b\u522b\u3002\u8fd9\u79cd\u968f\u673a\u6027\u4e0e\u786e\u5b9a\u6027\u7684\u7ed3\u5408\u4f7f\u8be5\u65e0\u76ee\u6807\u540e\u95e8\u653b\u51fb\u80fd\u5929\u7136\u89c4\u907f\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002\u4e3a\u5b9e\u73b0\u53d7\u63a7\u7075\u6d3b\u6027\u4e0b\u7684\u65e0\u76ee\u6807\u653b\u51fb\uff0c\u6211\u4eec\u63d0\u51fa\u5728\u5bf9\u6570\u5f52\u4e00\u5316\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u4e2d\u4f7f\u7528\u7ffb\u8f6c\u7684\u72ec\u70ed\u6807\u7b7e\u3002\u901a\u8fc7\u7ea6\u675f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u6570\u5206\u5e03\uff0c\u53d7\u611f\u67d3\u6a21\u578b\u5c06\u5728\u9009\u5b9a\u76ee\u6807\u7c7b\u522b\u8303\u56f4\u5185\u5448\u73b0\u5747\u5300\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u53d7\u63a7\u65e0\u76ee\u6807\u653b\u51fb\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86CUBA\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18088", "pdf": "https://arxiv.org/pdf/2506.18088", "abs": "https://arxiv.org/abs/2506.18088", "authors": ["Tianxing Chen", "Zanxin Chen", "Baijun Chen", "Zijian Cai", "Yibin Liu", "Qiwei Liang", "Zixuan Li", "Xianliang Lin", "Yiheng Ge", "Zhenyu Gu", "Weiliang Deng", "Yubin Guo", "Tian Nian", "Xuanbing Xie", "Qiangyu Chen", "Kailun Su", "Tianling Xu", "Guodong Liu", "Mengkang Hu", "Huan-ang Gao", "Kaixuan Wang", "Zhixuan Liang", "Yusen Qin", "Xiaokang Yang", "Ping Luo", "Yao Mu"], "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "comment": "Project Page: https://robotwin-platform.github.io/", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "AI": {"tldr": "RoboTwin 2.0\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u9886\u57df\u968f\u673a\u5316\u63d0\u5347\u7b56\u7565\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u5728\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u7f3a\u4e4f\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548c\u8fc7\u4e8e\u7b80\u5316\u7684\u4eff\u771f\u73af\u5883\u3002", "method": "\u6784\u5efaRoboTwin-OD\u5bf9\u8c61\u5e93\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eff\u771f\u5faa\u73af\u4f18\u5316\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u4e94\u8f74\u9886\u57df\u968f\u673a\u5316\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\u63d0\u534710.9%\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0cVLA\u6a21\u578b\u76f8\u5bf9\u6539\u8fdb367%\u3002", "conclusion": "RoboTwin 2.0\u4e3a\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "paper_title_zh": "RoboTwin 2.0\uff1a\u4e00\u79cd\u5177\u6709\u5f3a\u9886\u57df\u968f\u673a\u5316\u7684\u53ef\u6269\u5c55\u6570\u636e\u751f\u6210\u5668\u4e0e\u53cc\u673a\u68b0\u81c2\u9c81\u68d2\u64cd\u4f5c\u57fa\u51c6", "abstract_zh": "\u57fa\u4e8e\u4eff\u771f\u7684\u6570\u636e\u5408\u6210\u5df2\u6210\u4e3a\u589e\u5f3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6709\u529b\u8303\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u5728\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u4e2d\u4ecd\u663e\u4e0d\u8db3\uff0c\u539f\u56e0\u6709\u4e8c\uff1a(1) \u7f3a\u4e4f\u9488\u5bf9\u65b0\u4efb\u52a1\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff1b(2) \u4eff\u771f\u73af\u5883\u8fc7\u4e8e\u7b80\u5316\uff0c\u672a\u80fd\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u63d0\u51faRoboTwin 2.0\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u4e14\u771f\u5b9e\u7684\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\u3002\u6211\u4eec\u9996\u5148\u6784\u5efaRoboTwin-OD\uff0c\u4e00\u4e2a\u5305\u542b147\u4e2a\u7c7b\u522b\u3001731\u4e2a\u5b9e\u4f8b\u7684\u5927\u89c4\u6a21\u5bf9\u8c61\u5e93\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u5747\u6807\u6ce8\u4e86\u8bed\u4e49\u548c\u64cd\u4f5c\u76f8\u5173\u6807\u7b7e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u4eff\u771f\u5faa\u73af\u4f18\u5316\u7684\u4e13\u5bb6\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u7ea7\u6267\u884c\u4ee3\u7801\u3002\u4e3a\u63d0\u5347\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\uff0cRoboTwin 2.0\u5f15\u5165\u4e86\u4e94\u8f74\u7ed3\u6784\u5316\u9886\u57df\u968f\u673a\u5316\uff08\u6742\u4e71\u5ea6\u3001\u5149\u7167\u3001\u80cc\u666f\u3001\u684c\u9762\u9ad8\u5ea6\u548c\u8bed\u8a00\u6307\u4ee4\uff09\uff0c\u4ece\u800c\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u548c\u7b56\u7565\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728\u4e94\u79cd\u673a\u5668\u4eba\u5b9e\u4f53\u4e0a\u5b9e\u4f8b\u5316\u4e8650\u4e2a\u53cc\u673a\u68b0\u81c2\u4efb\u52a1\uff0c\u5e76\u9884\u6536\u96c6\u4e86\u8d85\u8fc7100,000\u6761\u9886\u57df\u968f\u673a\u5316\u7684\u4e13\u5bb6\u8f68\u8ff9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\u63d0\u534710.9%\uff0c\u5e76\u5728\u65b0\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u6211\u4eec\u6570\u636e\u96c6\u5fae\u8c03\u7684VLA\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4efb\u52a1\u4e2d\u76f8\u5bf9\u6539\u8fdb367%\uff0842.0% vs. 9.0%\uff09\uff0c\u800c\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u6a21\u578b\u76f8\u5bf9\u63d0\u5347228%\uff0c\u663e\u793a\u51fa\u65e0\u9700\u771f\u5b9e\u76d1\u7763\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u6570\u636e\u751f\u6210\u5668\u3001\u57fa\u51c6\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u652f\u6301\u53cc\u673a\u68b0\u81c2\u9c81\u68d2\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u7814\u7a76\u3002"}}
{"id": "2506.18270", "pdf": "https://arxiv.org/pdf/2506.18270", "abs": "https://arxiv.org/abs/2506.18270", "authors": ["Qinrong Cai", "Yu Guan", "Zhibo Chen", "Dong Liang", "Qiuyun Fan", "Qiegen Liu"], "title": "Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "As the deep learning revolution marches on, masked modeling has emerged as a\ndistinctive approach that involves predicting parts of the original data that\nare proportionally masked during training, and has demonstrated exceptional\nperformance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction\nis a critical task in medical imaging that seeks to recover high-quality images\nfrom under-sampled k-space data. However, previous MRI reconstruction\nstrategies usually optimized the entire image domain or k-space, without\nconsidering the importance of different frequency regions in the k-space This\nwork introduces a diffusion model based on adaptive masks (AMDM), which\nutilizes the adaptive adjustment of frequency distribution based on k-space\ndata to develop a hybrid masks mechanism that adapts to different k-space\ninputs. This enables the effective separation of high-frequency and\nlow-frequency components, producing diverse frequency-specific representations.\nAdditionally, the k-space frequency distribution informs the generation of\nadaptive masks, which, in turn, guide a closed-loop diffusion process.\nExperimental results verified the ability of this method to learn specific\nfrequency information and thereby improved the quality of MRI reconstruction,\nproviding a flexible framework for optimizing k-space data using masks in the\nfuture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u63a9\u7801\u7684k\u7a7a\u95f4\u6269\u6563\u6a21\u578b\uff08AMDM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574k\u7a7a\u95f4\u6570\u636e\u7684\u9891\u7387\u5206\u5e03\uff0c\u6709\u6548\u5206\u79bb\u9ad8\u4f4e\u9891\u6210\u5206\uff0c\u4ece\u800c\u63d0\u5347MRI\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfMRI\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u4f18\u5316\u6574\u4e2a\u56fe\u50cf\u57df\u6216k\u7a7a\u95f4\uff0c\u672a\u8003\u8651\u4e0d\u540c\u9891\u7387\u533a\u57df\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u7801\u673a\u5236\uff0c\u66f4\u6709\u6548\u5730\u5229\u7528k\u7a7a\u95f4\u6570\u636e\u4e2d\u7684\u9891\u7387\u4fe1\u606f\uff0c\u63d0\u5347\u91cd\u5efa\u6548\u679c\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u63a9\u7801\u5f15\u5bfc\u7684k\u7a7a\u95f4\u6269\u6563\u6a21\u578b\uff08AMDM\uff09\uff0c\u5229\u7528k\u7a7a\u95f4\u6570\u636e\u7684\u9891\u7387\u5206\u5e03\u751f\u6210\u81ea\u9002\u5e94\u63a9\u7801\uff0c\u6307\u5bfc\u95ed\u73af\u6269\u6563\u8fc7\u7a0b\uff0c\u5206\u79bb\u9ad8\u4f4e\u9891\u6210\u5206\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u9891\u7387\u7279\u5b9a\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7279\u5b9a\u9891\u7387\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8MRI\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u63a9\u7801\u7684k\u7a7a\u95f4\u6570\u636e\u4f18\u5316\u63d0\u4f9b\u4e86\u7075\u6d3b\u6846\u67b6\u3002", "conclusion": "AMDM\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u7801\u673a\u5236\u4f18\u5316k\u7a7a\u95f4\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86MRI\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u601d\u8def\u3002", "paper_title_zh": "\u81ea\u9002\u5e94\u63a9\u7801\u5f15\u5bfc\u7684k\u7a7a\u95f4\u6269\u6563\u7528\u4e8e\u52a0\u901fMRI\u91cd\u5efa", "abstract_zh": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u63a9\u7801\u5efa\u6a21\u6210\u4e3a\u4e00\u79cd\u72ec\u7279\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8bad\u7ec3\u4e2d\u88ab\u90e8\u5206\u63a9\u7801\u7684\u539f\u59cb\u6570\u636e\uff0c\u5df2\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u91cd\u5efa\u662f\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u6b20\u91c7\u6837\u7684k\u7a7a\u95f4\u6570\u636e\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7684MRI\u91cd\u5efa\u7b56\u7565\u901a\u5e38\u4f18\u5316\u6574\u4e2a\u56fe\u50cf\u57df\u6216k\u7a7a\u95f4\uff0c\u672a\u8003\u8651k\u7a7a\u95f4\u4e2d\u4e0d\u540c\u9891\u7387\u533a\u57df\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u63a9\u7801\u7684\u6269\u6563\u6a21\u578b\uff08AMDM\uff09\uff0c\u5229\u7528k\u7a7a\u95f4\u6570\u636e\u7684\u9891\u7387\u5206\u5e03\u52a8\u6001\u8c03\u6574\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u9002\u5e94\u4e0d\u540ck\u7a7a\u95f4\u8f93\u5165\u7684\u6df7\u5408\u63a9\u7801\u673a\u5236\u3002\u8fd9\u6709\u6548\u5206\u79bb\u4e86\u9ad8\u9891\u548c\u4f4e\u9891\u6210\u5206\uff0c\u751f\u6210\u4e86\u591a\u6837\u5316\u7684\u9891\u7387\u7279\u5b9a\u8868\u793a\u3002\u6b64\u5916\uff0ck\u7a7a\u95f4\u9891\u7387\u5206\u5e03\u6307\u5bfc\u751f\u6210\u81ea\u9002\u5e94\u63a9\u7801\uff0c\u8fdb\u800c\u5f15\u5bfc\u95ed\u73af\u6269\u6563\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5b66\u4e60\u7279\u5b9a\u9891\u7387\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86MRI\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u5229\u7528\u63a9\u7801\u4f18\u5316k\u7a7a\u95f4\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2506.18272", "pdf": "https://arxiv.org/pdf/2506.18272", "abs": "https://arxiv.org/abs/2506.18272", "authors": ["Debjyoti Das Adhikary", "Aritra Hazra", "Partha Pratim Chakrabarti"], "title": "ReFrame: Rectification Framework for Image Explaining Architectures", "categories": ["cs.CV"], "comment": "Accepted in CODS-COMAD December 2024", "summary": "Image explanation has been one of the key research interests in the Deep\nLearning field. Throughout the years, several approaches have been adopted to\nexplain an input image fed by the user. From detecting an object in a given\nimage to explaining it in human understandable sentence, to having a\nconversation describing the image, this problem has seen an immense change\nthroughout the years, However, the existing works have been often found to (a)\nhallucinate objects that do not exist in the image and/or (b) lack identifying\nthe complete set of objects present in the image. In this paper, we propose a\nnovel approach to mitigate these drawbacks of inconsistency and incompleteness\nof the objects recognized during the image explanation. To enable this, we\npropose an interpretable framework that can be plugged atop diverse image\nexplaining frameworks including Image Captioning, Visual Question Answering\n(VQA) and Prompt-based AI using LLMs, thereby enhancing their explanation\ncapabilities by rectifying the incorrect or missing objects. We further measure\nthe efficacy of the rectified explanations generated through our proposed\napproaches leveraging object based precision metrics, and showcase the\nimprovements in the inconsistency and completeness of image explanations.\nQuantitatively, the proposed framework is able to improve the explanations over\nthe baseline architectures of Image Captioning (improving the completeness by\n81.81% and inconsistency by 37.10%), Visual Question Answering(average of 9.6%\nand 37.10% in completeness and inconsistency respectively) and Prompt-based AI\nmodel (0.01% and 5.2% for completeness and inconsistency respectively)\nsurpassing the current state-of-the-art by a substantial margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReFrame\u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u50cf\u89e3\u91ca\u4efb\u52a1\u4e2d\u5bf9\u8c61\u8bc6\u522b\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u63cf\u8ff0\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u57fa\u4e8e\u63d0\u793a\u7684AI\u6a21\u578b\u7684\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u89e3\u91ca\u65b9\u6cd5\u5e38\u5b58\u5728\u5e7b\u89c9\u5bf9\u8c61\u6216\u9057\u6f0f\u771f\u5b9e\u5bf9\u8c61\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u89e3\u91ca\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u5b8c\u6574\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u53ef\u63d2\u62d4\u7684\u6846\u67b6\uff0c\u589e\u5f3a\u73b0\u6709\u56fe\u50cf\u89e3\u91ca\u67b6\u6784\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faReFrame\u6846\u67b6\uff0c\u53ef\u96c6\u6210\u4e8e\u56fe\u50cf\u63cf\u8ff0\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u57fa\u4e8e\u63d0\u793a\u7684AI\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u4fee\u6b63\u9519\u8bef\u6216\u7f3a\u5931\u7684\u5bf9\u8c61\u6765\u63d0\u5347\u89e3\u91ca\u80fd\u529b\u3002\u91c7\u7528\u57fa\u4e8e\u5bf9\u8c61\u7684\u7cbe\u5ea6\u6307\u6807\u8bc4\u4f30\u6539\u8fdb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReFrame\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u63cf\u8ff0\u7684\u5b8c\u6574\u6027\uff0881.81%\uff09\u548c\u4e0d\u4e00\u81f4\u6027\uff0837.10%\uff09\u3001\u89c6\u89c9\u95ee\u7b54\u7684\u5b8c\u6574\u6027\uff08\u5e73\u57479.6%\uff09\u548c\u4e0d\u4e00\u81f4\u6027\uff0837.10%\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8e\u63d0\u793a\u7684AI\u6a21\u578b\u7684\u5b8c\u6574\u6027\uff080.01%\uff09\u548c\u4e0d\u4e00\u81f4\u6027\uff085.2%\uff09\u3002", "conclusion": "ReFrame\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u89e3\u91ca\u4e2d\u7684\u5bf9\u8c61\u8bc6\u522b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u4efb\u52a1\u7684\u4e00\u81f4\u6027\u548c\u5b8c\u6574\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002", "paper_title_zh": "ReFrame\uff1a\u56fe\u50cf\u89e3\u91ca\u67b6\u6784\u7684\u4fee\u6b63\u6846\u67b6", "abstract_zh": "\u56fe\u50cf\u89e3\u91ca\u4e00\u76f4\u662f\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u4e4b\u4e00\u3002\u591a\u5e74\u6765\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u65b9\u6cd5\u6765\u89e3\u91ca\u7528\u6237\u8f93\u5165\u7684\u56fe\u50cf\uff0c\u4ece\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u5230\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u53e5\u5b50\u63cf\u8ff0\uff0c\u518d\u5230\u901a\u8fc7\u5bf9\u8bdd\u63cf\u8ff0\u56fe\u50cf\u5185\u5bb9\uff0c\u8fd9\u4e00\u9886\u57df\u7ecf\u5386\u4e86\u5de8\u5927\u53d8\u9769\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a\uff08a\uff09\u5e7b\u89c9\u51fa\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\uff1b\uff08b\uff09\u672a\u80fd\u8bc6\u522b\u56fe\u50cf\u4e2d\u6240\u6709\u5bf9\u8c61\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u50cf\u89e3\u91ca\u4e2d\u5bf9\u8c61\u8bc6\u522b\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u53ef\u96c6\u6210\u4e8e\u591a\u79cd\u56fe\u50cf\u89e3\u91ca\u67b6\u6784\uff08\u5982\u56fe\u50cf\u63cf\u8ff0\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u57fa\u4e8e\u63d0\u793a\u7684AI\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u4fee\u6b63\u9519\u8bef\u6216\u7f3a\u5931\u7684\u5bf9\u8c61\u6765\u589e\u5f3a\u5176\u89e3\u91ca\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5229\u7528\u57fa\u4e8e\u5bf9\u8c61\u7684\u7cbe\u5ea6\u6307\u6807\u8bc4\u4f30\u4fee\u6b63\u540e\u89e3\u91ca\u7684\u6548\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u56fe\u50cf\u89e3\u91ca\u5728\u4e00\u81f4\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u7684\u6539\u8fdb\u3002\u5b9a\u91cf\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728\u56fe\u50cf\u63cf\u8ff0\uff08\u5b8c\u6574\u6027\u63d0\u534781.81%\uff0c\u4e0d\u4e00\u81f4\u6027\u964d\u4f4e37.10%\uff09\u3001\u89c6\u89c9\u95ee\u7b54\uff08\u5b8c\u6574\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u5e73\u5747\u63d0\u53479.6%\u548c37.10%\uff09\u548c\u57fa\u4e8e\u63d0\u793a\u7684AI\u6a21\u578b\uff08\u5b8c\u6574\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u5206\u522b\u63d0\u53470.01%\u548c5.2%\uff09\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.18284", "pdf": "https://arxiv.org/pdf/2506.18284", "abs": "https://arxiv.org/abs/2506.18284", "authors": ["Kasra Moazzami", "Seoyoun Son", "John Lin", "Sun Min Lee", "Daniel Son", "Hayeon Lee", "Jeongho Lee", "Seongji Lee"], "title": "Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 3 figures, 3 tables", "summary": "Endoscopic image classification plays a pivotal role in medical diagnostics\nby identifying anatomical landmarks and pathological findings. However,\nconventional closed-set classification frameworks are inherently limited in\nopen-world clinical settings, where previously unseen conditions can arise\nandcompromise model reliability. To address this, we explore the application of\nOpen Set Recognition (OSR) techniques on the Kvasir dataset, a publicly\navailable and diverse endoscopic image collection. In this study, we evaluate\nand compare the OSR capabilities of several representative deep learning\narchitectures, including ResNet-50, Swin Transformer, and a hybrid\nResNet-Transformer model, under both closed-set and open-set conditions.\nOpenMax is adopted as a baseline OSR method to assess the ability of these\nmodels to distinguish known classes from previously unseen categories. This\nwork represents one of the first efforts to apply open set recognition to the\nKvasir dataset and provides a foundational benchmark for evaluating OSR\nperformance in medical image analysis. Our results offer practical insights\ninto model behavior in clinically realistic settings and highlight the\nimportance of OSR techniques for the safe deployment of AI systems in\nendoscopy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728Kvasir\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982ResNet-50\u3001Swin Transformer\u548c\u6df7\u5408\u6a21\u578b\uff09\u5728\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u91c7\u7528OpenMax\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u7814\u7a76\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684OSR\u6027\u80fd\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edf\u5c01\u95ed\u96c6\u5206\u7c7b\u6846\u67b6\u5728\u4e34\u5e8a\u5f00\u653e\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u672a\u77e5\u7c7b\u522b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u653e\u96c6\u8bc6\u522b\u6280\u672f\u63d0\u5347\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u7c7b\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u5728Kvasir\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86ResNet-50\u3001Swin Transformer\u548c\u6df7\u5408ResNet-Transformer\u6a21\u578b\u7684OSR\u80fd\u529b\uff0c\u5e76\u91c7\u7528OpenMax\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u5408\u6a21\u578b\u5728\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684OSR\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u7279\u70b9\u3002", "conclusion": "\u5f00\u653e\u96c6\u8bc6\u522b\u6280\u672f\u5bf9AI\u7cfb\u7edf\u5728\u5185\u7aa5\u955c\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u672c\u7814\u7a76\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u53c2\u8003\u3002", "paper_title_zh": "\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u7c7b\u7684\u5f00\u653e\u96c6\u8bc6\u522b\uff1a\u57fa\u4e8eKvasir\u6570\u636e\u96c6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5", "abstract_zh": "\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u7c7b\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u8bc6\u522b\u89e3\u5256\u6807\u5fd7\u548c\u75c5\u7406\u53d1\u73b0\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u5c01\u95ed\u96c6\u5206\u7c7b\u6846\u67b6\u5728\u5f00\u653e\u4e16\u754c\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5e94\u5bf9\u672a\u77e5\u60c5\u51b5\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5728\u516c\u5f00\u4e14\u591a\u6837\u5316\u7684Kvasir\u5185\u7aa5\u955c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u63a2\u7d22\u4e86\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u6280\u672f\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u4ee3\u8868\u6027\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5305\u62ecResNet-50\u3001Swin Transformer\u548c\u6df7\u5408ResNet-Transformer\u6a21\u578b\uff09\u5728\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684OSR\u80fd\u529b\u3002\u91c7\u7528OpenMax\u4f5c\u4e3a\u57fa\u7ebfOSR\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u533a\u5206\u5df2\u77e5\u7c7b\u522b\u4e0e\u672a\u77e5\u7c7b\u522b\u7684\u80fd\u529b\u3002\u8fd9\u662f\u9996\u6b21\u5c06\u5f00\u653e\u96c6\u8bc6\u522b\u5e94\u7528\u4e8eKvasir\u6570\u636e\u96c6\u7684\u7814\u7a76\u4e4b\u4e00\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684OSR\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u57fa\u51c6\u3002\u6211\u4eec\u7684\u7ed3\u679c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u7279\u70b9\uff0c\u5e76\u5f3a\u8c03\u4e86OSR\u6280\u672f\u5728\u5185\u7aa5\u955cAI\u7cfb\u7edf\u5b89\u5168\u90e8\u7f72\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.17353", "pdf": "https://arxiv.org/pdf/2506.17353", "abs": "https://arxiv.org/abs/2506.17353", "authors": ["Zongjie Li", "Daoyuan Wu", "Shuai Wang", "Zhendong Su"], "title": "Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs", "categories": ["cs.CR", "cs.AI"], "comment": "In Proceedings of the 2025 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS'25), October 13-17, 2025, Taipei, Taiwan, China.\n  ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3719027.3744856", "summary": "The increasing demand for domain-specific and human-aligned Large Language\nModels (LLMs) has led to the widespread adoption of Supervised Fine-Tuning\n(SFT) techniques. SFT datasets often comprise valuable instruction-response\npairs, making them highly valuable targets for potential extraction. This paper\nstudies this critical research problem for the first time. We start by formally\ndefining and formulating the problem, then explore various attack goals, types,\nand variants based on the unique properties of SFT data in real-world\nscenarios. Based on our analysis of extraction behaviors of direct extraction,\nwe develop a novel extraction method specifically designed for SFT models,\ncalled Differentiated Data Extraction (DDE), which exploits the confidence\nlevels of fine-tuned models and their behavioral differences from pre-trained\nbase models. Through extensive experiments across multiple domains and\nscenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our\nresults show that DDE consistently outperforms existing extraction baselines in\nall attack settings. To counter this new attack, we propose a defense mechanism\nthat mitigates DDE attacks with minimal impact on model performance. Overall,\nour research reveals hidden data leak risks in fine-tuned LLMs and provides\ninsights for developing more secure models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u4ece\u7ecf\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u63d0\u53d6\u4e13\u6709\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5dee\u5f02\u5316\u6570\u636e\u63d0\u53d6\uff08DDE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5fae\u8c03\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u53ca\u5176\u4e0e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u673a\u5236\u4ee5\u51cf\u8f7b\u6b64\u7c7b\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u9886\u57df\u7279\u5b9a\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9700\u6c42\u7684\u589e\u52a0\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6280\u672f\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002SFT\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u9ad8\u4ef7\u503c\u7684\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff0c\u6210\u4e3a\u6f5c\u5728\u63d0\u53d6\u7684\u76ee\u6807\u3002\u672c\u6587\u9996\u6b21\u5bf9\u8fd9\u4e00\u5173\u952e\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002", "method": "\u8bba\u6587\u9996\u5148\u6b63\u5f0f\u5b9a\u4e49\u5e76\u5f62\u5f0f\u5316\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u968f\u540e\u57fa\u4e8eSFT\u6570\u636e\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u63a2\u7d22\u4e86\u591a\u79cd\u653b\u51fb\u76ee\u6807\u3001\u7c7b\u578b\u548c\u53d8\u4f53\u3002\u901a\u8fc7\u5206\u6790\u76f4\u63a5\u63d0\u53d6\u7684\u884c\u4e3a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9SFT\u6a21\u578b\u7684\u63d0\u53d6\u65b9\u6cd5\u2014\u2014\u5dee\u5f02\u5316\u6570\u636e\u63d0\u53d6\uff08DDE\uff09\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5fae\u8c03\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u53ca\u5176\u4e0e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u901a\u8fc7\u8de8\u591a\u4e2a\u9886\u57df\u548c\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528DDE\u63d0\u53d6SFT\u6570\u636e\u7684\u53ef\u884c\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0cDDE\u5728\u6240\u6709\u653b\u51fb\u8bbe\u7f6e\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u6027\u80fd\u5f71\u54cd\u51cf\u8f7bDDE\u653b\u51fb\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5fae\u8c03LLM\u4e2d\u9690\u85cf\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "paper_title_zh": "\u57fa\u4e8e\u5dee\u5f02\u5316\u7684\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e13\u6709\u6570\u636e\u63d0\u53d6", "abstract_zh": "\u968f\u7740\u9886\u57df\u7279\u5b9a\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9700\u6c42\u7684\u589e\u52a0\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6280\u672f\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002SFT\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u9ad8\u4ef7\u503c\u7684\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff0c\u6210\u4e3a\u6f5c\u5728\u63d0\u53d6\u7684\u76ee\u6807\u3002\u672c\u6587\u9996\u6b21\u5bf9\u8fd9\u4e00\u5173\u952e\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002\u6211\u4eec\u9996\u5148\u6b63\u5f0f\u5b9a\u4e49\u5e76\u5f62\u5f0f\u5316\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u968f\u540e\u57fa\u4e8eSFT\u6570\u636e\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u63a2\u7d22\u4e86\u591a\u79cd\u653b\u51fb\u76ee\u6807\u3001\u7c7b\u578b\u548c\u53d8\u4f53\u3002\u901a\u8fc7\u5206\u6790\u76f4\u63a5\u63d0\u53d6\u7684\u884c\u4e3a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9SFT\u6a21\u578b\u7684\u63d0\u53d6\u65b9\u6cd5\u2014\u2014\u5dee\u5f02\u5316\u6570\u636e\u63d0\u53d6\uff08DDE\uff09\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5fae\u8c03\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u53ca\u5176\u4e0e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\u3002\u901a\u8fc7\u8de8\u591a\u4e2a\u9886\u57df\u548c\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4f7f\u7528DDE\u63d0\u53d6SFT\u6570\u636e\u7684\u53ef\u884c\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0cDDE\u5728\u6240\u6709\u653b\u51fb\u8bbe\u7f6e\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e00\u65b0\u578b\u653b\u51fb\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u6027\u80fd\u5f71\u54cd\u51cf\u8f7bDDE\u653b\u51fb\u3002\u603b\u4f53\u800c\u8a00\uff0c\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5fae\u8c03LLM\u4e2d\u9690\u85cf\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2506.18203", "pdf": "https://arxiv.org/pdf/2506.18203", "abs": "https://arxiv.org/abs/2506.18203", "authors": ["Jon Saad-Falcon", "E. Kelly Buchanan", "Mayee F. Chen", "Tzu-Heng Huang", "Brendan McLaughlin", "Tanvir Bhathal", "Shang Zhu", "Ben Athiwaratkun", "Frederic Sala", "Scott Linderman", "Azalia Mirhoseini", "Christopher R\u00e9"], "title": "Shrinking the Generation-Verification Gap with Weak Verifiers", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWeaver\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5f31\u9a8c\u8bc1\u5668\u6765\u7f29\u5c0f\u751f\u6210\u4e0e\u9a8c\u8bc1\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5019\u9009\u54cd\u5e94\u7684\u9009\u62e9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u9ad8\u8d28\u91cf\u9a8c\u8bc1\u5668\uff08\u5982\u4eba\u7c7b\u6216\u5de5\u5177\uff09\u96be\u4ee5\u6269\u5c55\u6216\u6548\u7528\u6709\u9650\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u5668\u4e0e\u7406\u60f3\u9a8c\u8bc1\u5668\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5f31\u9a8c\u8bc1\u5668\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "Weaver\u6846\u67b6\u901a\u8fc7\u52a0\u6743\u96c6\u6210\u591a\u4e2a\u5f31\u9a8c\u8bc1\u5668\uff0c\u5229\u7528\u5f31\u76d1\u7763\u4f30\u8ba1\u5404\u9a8c\u8bc1\u5668\u51c6\u786e\u6027\uff0c\u5e76\u5f52\u4e00\u5316\u8f93\u51fa\u683c\u5f0f\u4ee5\u89e3\u51b3\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u7edf\u8ba1\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u9a8c\u8bc1\u5668\uff0c\u6700\u7ec8\u751f\u6210\u7edf\u4e00\u8bc4\u5206\u4ee5\u53cd\u6620\u771f\u5b9e\u54cd\u5e94\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWeaver\u5728\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8ePass@1\u6027\u80fd\uff0c\u4f7f\u7528Llama 3.3 70B Instruct\u751f\u6210\u5668\u65f6\uff0c\u9a8c\u8bc1\u5668\u96c6\u6210\u8fbe\u523087.7%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u7406\u60f3\u9a8c\u8bc1\u5668\u6c34\u5e73\u3002", "conclusion": "Weaver\u901a\u8fc7\u7ed3\u5408\u5f31\u9a8c\u8bc1\u5668\u6709\u6548\u7f29\u5c0f\u4e86\u751f\u6210\u4e0e\u9a8c\u8bc1\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u901a\u8fc7\u8bad\u7ec3\u5c0f\u578b\u8de8\u7f16\u7801\u5668\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "paper_title_zh": "\u901a\u8fc7\u5f31\u9a8c\u8bc1\u5668\u7f29\u5c0f\u751f\u6210\u4e0e\u9a8c\u8bc1\u4e4b\u95f4\u7684\u5dee\u8ddd", "abstract_zh": "\u9a8c\u8bc1\u5668\u53ef\u4ee5\u901a\u8fc7\u8bc4\u5206\u548c\u6392\u5e8f\u751f\u6210\u7684\u5019\u9009\u54cd\u5e94\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002\u76ee\u524d\uff0c\u9ad8\u8d28\u91cf\u7684\u9a8c\u8bc1\u5668\u8981\u4e48\u96be\u4ee5\u6269\u5c55\uff08\u5982\u4eba\u7c7b\uff09\uff0c\u8981\u4e48\u6548\u7528\u6709\u9650\uff08\u5982Lean\u7b49\u5de5\u5177\uff09\u3002\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u8bc4\u5224\u5668\u548c\u5956\u52b1\u6a21\u578b\u5df2\u6210\u4e3a\u901a\u7528\u9a8c\u8bc1\u5668\uff0c\u4f46\u5176\u4e0e\u7406\u60f3\u9a8c\u8bc1\u5668\uff08\u51c6\u786e\u7387\u5b8c\u7f8e\uff09\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002\u4e3a\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Weaver\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5f31\u4e14\u4e0d\u5b8c\u7f8e\u7684\u9a8c\u8bc1\u5668\u6765\u8bbe\u8ba1\u5f3a\u9a8c\u8bc1\u5668\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u52a0\u6743\u96c6\u6210\u9a8c\u8bc1\u5668\uff08\u901a\u5e38\u9700\u8981\u4ece\u6807\u6ce8\u6570\u636e\u4e2d\u5b66\u4e60\uff09\u7531\u4e8e\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\u5dee\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u672a\u52a0\u6743\u7ec4\u5408\u3002\u4e3a\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0cWeaver\u5229\u7528\u5f31\u76d1\u7763\u4f30\u8ba1\u5404\u9a8c\u8bc1\u5668\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u8f93\u51fa\u7ed3\u5408\u4e3a\u7edf\u4e00\u8bc4\u5206\u4ee5\u66f4\u771f\u5b9e\u53cd\u6620\u54cd\u5e94\u8d28\u91cf\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528\u5f31\u76d1\u7763\u7b97\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u9a8c\u8bc1\u5668\u8f93\u51fa\u683c\u5f0f\u4e0d\u4e00\u81f4\u548c\u5904\u7406\u4f4e\u8d28\u91cf\u9a8c\u8bc1\u5668\u3002Weaver\u901a\u8fc7\u6570\u636e\u96c6\u7edf\u8ba1\u5f52\u4e00\u5316\u8f93\u51fa\u5e76\u8fc7\u6ee4\u7279\u5b9a\u9a8c\u8bc1\u5668\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u6211\u4eec\u7814\u7a76\u4e86Weaver\u5728\u6d4b\u8bd5\u65f6\u91cd\u590d\u91c7\u6837\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5373\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5019\u9009\u54cd\u5e94\u5e76\u9009\u62e9\u5176\u4e00\u3002\u8bc4\u4f30\u663e\u793a\uff0cWeaver\u5728\u9009\u62e9\u9996\u4e2a\u5019\u9009\u54cd\u5e94\u65f6\u663e\u8457\u4f18\u4e8ePass@1\u6027\u80fd\uff0c\u5728\u63a8\u7406\u548c\u6570\u5b66\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528Llama 3.3 70B Instruct\u4f5c\u4e3a\u751f\u6210\u5668\uff0c\u4ee5\u53ca70B\u6216\u66f4\u5c0f\u7684\u8bc4\u5224\u5668\u548c\u5956\u52b1\u6a21\u578b\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u96c6\u6210\u65f6\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523087.7%\u3002\u8fd9\u4e00\u63d0\u5347\u4e0eGPT-4o\u548co3-mini\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0869.0% vs. 86.7%\uff09\u76f8\u5f53\uff0c\u540e\u8005\u9700\u8981\u5927\u91cf\u5fae\u8c03\u548c\u540e\u8bad\u7ec3\u3002\u4e3a\u964d\u4f4e\u9a8c\u8bc1\u5668\u96c6\u6210\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6211\u4eec\u4f7f\u7528Weaver\u7684\u7efc\u5408\u8f93\u51fa\u8bc4\u5206\u8bad\u7ec3\u4e86\u4e00\u4e2a400M\u7684\u8de8\u7f16\u7801\u5668\u3002"}}
{"id": "2506.18291", "pdf": "https://arxiv.org/pdf/2506.18291", "abs": "https://arxiv.org/abs/2506.18291", "authors": ["Yota Urano", "Hiromu Taketsugu", "Norimichi Ukita"], "title": "Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "MIRU 2025", "summary": "This paper presents an architecture for selecting important neighboring\npeople to predict the primary person's trajectory. To achieve effective\nneighboring people selection, we propose a people selection module called the\nImportance Estimator which outputs the importance of each neighboring person\nfor predicting the primary person's future trajectory. To prevent gradients\nfrom being blocked by non-differentiable operations when sampling surrounding\npeople based on their importance, we employ the Gumbel Softmax for training.\nExperiments conducted on the JRDB dataset show that our method speeds up the\nprocess with competitive prediction accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9009\u62e9\u91cd\u8981\u90bb\u5c45\u6765\u9884\u6d4b\u4e3b\u8981\u4eba\u7269\u8f68\u8ff9\u7684\u67b6\u6784\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u4f30\u8ba1\u6a21\u5757\u548cGumbel Softmax\u6280\u672f\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u52a0\u5feb\u4e86\u901f\u5ea6\u3002", "motivation": "\u9884\u6d4b\u4eba\u7c7b\u8f68\u8ff9\u65f6\uff0c\u5468\u56f4\u4eba\u7fa4\u7684\u5f71\u54cd\u662f\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6240\u6709\u90bb\u5c45\u5e73\u7b49\u5904\u7406\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9009\u62e9\u91cd\u8981\u90bb\u5c45\u6765\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u91cd\u8981\u6027\u4f30\u8ba1\u6a21\u5757\uff08Importance Estimator\uff09\u8bc4\u4f30\u6bcf\u4e2a\u90bb\u5c45\u5bf9\u4e3b\u8981\u4eba\u7269\u8f68\u8ff9\u9884\u6d4b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4f7f\u7528Gumbel Softmax\u89e3\u51b3\u975e\u53ef\u5fae\u5206\u64cd\u4f5c\u5bfc\u81f4\u7684\u68af\u5ea6\u963b\u585e\u95ee\u9898\u3002", "result": "\u5728JRDB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u5feb\u4e86\u9884\u6d4b\u901f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u793e\u4ea4\u4ea4\u4e92\u548c\u91cd\u8981\u6027\u4f30\u8ba1\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5e73\u8861\u3002", "paper_title_zh": "\u57fa\u4e8e\u4e2a\u4f53\u91cd\u8981\u6027\u7684\u9009\u62e9\u6027\u793e\u4ea4\u4ea4\u4e92\u7528\u4e8e\u5feb\u901f\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\uff0c\u7528\u4e8e\u9009\u62e9\u91cd\u8981\u7684\u90bb\u5c45\u6765\u9884\u6d4b\u4e3b\u8981\u4eba\u7269\u7684\u8f68\u8ff9\u3002\u4e3a\u5b9e\u73b0\u6709\u6548\u7684\u90bb\u5c45\u9009\u62e9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u79f0\u4e3a\u91cd\u8981\u6027\u4f30\u8ba1\u5668\u7684\u4eba\u5458\u9009\u62e9\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u8f93\u51fa\u6bcf\u4e2a\u90bb\u5c45\u5bf9\u9884\u6d4b\u4e3b\u8981\u4eba\u7269\u672a\u6765\u8f68\u8ff9\u7684\u91cd\u8981\u6027\u3002\u4e3a\u9632\u6b62\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u5468\u56f4\u4eba\u5458\u65f6\u56e0\u975e\u53ef\u5fae\u5206\u64cd\u4f5c\u800c\u963b\u585e\u68af\u5ea6\uff0c\u6211\u4eec\u91c7\u7528\u4e86Gumbel Softmax\u8fdb\u884c\u8bad\u7ec3\u3002\u5728JRDB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u52a0\u5feb\u4e86\u9884\u6d4b\u8fc7\u7a0b\u3002"}}
{"id": "2506.17356", "pdf": "https://arxiv.org/pdf/2506.17356", "abs": "https://arxiv.org/abs/2506.17356", "authors": ["Jionghao Lin", "Jiarui Rao", "Yiyang Zhao", "Yuting Wang", "Ashish Gurung", "Amanda Barany", "Jaclyn Ocumpaugh", "Ryan S. Baker", "Kenneth R. Koedinger"], "title": "Automatic Large Language Models Creation of Interactive Learning Lessons", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Full Research Paper, 15 pages, In Proceedings of 20th European\n  Conference on Technology Enhanced Learning (ECTEL2025)", "summary": "We explore the automatic generation of interactive, scenario-based lessons\ndesigned to train novice human tutors who teach middle school mathematics\nonline. Employing prompt engineering through a Retrieval-Augmented Generation\napproach with GPT-4o, we developed a system capable of creating structured\ntutor training lessons. Our study generated lessons in English for three key\ntopics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,\nand Turning on Cameras, using a task decomposition prompting strategy that\nbreaks lesson generation into sub-tasks. The generated lessons were evaluated\nby two human evaluators, who provided both quantitative and qualitative\nevaluations using a comprehensive rubric informed by lesson design research.\nResults demonstrate that the task decomposition strategy led to higher-rated\nlessons compared to single-step generation. Human evaluators identified several\nstrengths in the LLM-generated lessons, including well-structured content and\ntime-saving potential, while also noting limitations such as generic feedback\nand a lack of clarity in some instructional sections. These findings underscore\nthe potential of hybrid human-AI approaches for generating effective lessons in\ntutor training.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ea4\u4e92\u5f0f\u5b66\u4e60\u8bfe\u7a0b\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u57f9\u8bad\u5728\u7ebf\u521d\u4e2d\u6570\u5b66\u65b0\u624b\u5bfc\u5e08\u3002\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u63d0\u793a\u7b56\u7565\uff0c\u7cfb\u7edf\u751f\u6210\u7684\u8bfe\u7a0b\u5728\u7ed3\u6784\u548c\u5185\u5bb9\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u5b58\u5728\u53cd\u9988\u6cdb\u5316\u548c\u90e8\u5206\u6559\u5b66\u73af\u8282\u4e0d\u6e05\u6670\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u65b0\u624b\u5bfc\u5e08\u57f9\u8bad\u4e2d\u7f3a\u4e4f\u9ad8\u6548\u3001\u7ed3\u6784\u5316\u8bfe\u7a0b\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4ea4\u4e92\u5f0f\u5b66\u4e60\u8bfe\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u57f9\u8bad\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff08RAG\uff09\u7ed3\u5408GPT-4o\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u63d0\u793a\u7b56\u7565\u5c06\u8bfe\u7a0b\u751f\u6210\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u751f\u6210\u4e09\u4e2a\u5173\u952e\u4e3b\u9898\u7684\u82f1\u8bed\u8bfe\u7a0b\u3002", "result": "\u4efb\u52a1\u5206\u89e3\u7b56\u7565\u751f\u6210\u7684\u8bfe\u7a0b\u8bc4\u5206\u9ad8\u4e8e\u5355\u6b65\u751f\u6210\uff0c\u5185\u5bb9\u7ed3\u6784\u826f\u597d\u4e14\u8282\u7701\u65f6\u95f4\uff0c\u4f46\u5b58\u5728\u53cd\u9988\u6cdb\u5316\u548c\u90e8\u5206\u6559\u5b66\u73af\u8282\u4e0d\u6e05\u6670\u7684\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e0eAI\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u6548\u5bfc\u5e08\u57f9\u8bad\u8bfe\u7a0b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u53cd\u9988\u548c\u6559\u5b66\u73af\u8282\u7684\u6e05\u6670\u5ea6\u3002", "paper_title_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ea4\u4e92\u5f0f\u5b66\u4e60\u8bfe\u7a0b", "abstract_zh": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u57fa\u4e8e\u573a\u666f\u7684\u8bfe\u7a0b\uff0c\u65e8\u5728\u57f9\u8bad\u5728\u7ebf\u521d\u4e2d\u6570\u5b66\u65b0\u624b\u5bfc\u5e08\u3002\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff08RAG\uff09\u7ed3\u5408GPT-4o\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u5bfc\u5e08\u57f9\u8bad\u8bfe\u7a0b\u7684\u7cfb\u7edf\u3002\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u4efb\u52a1\u5206\u89e3\u63d0\u793a\u7b56\u7565\uff0c\u5c06\u8bfe\u7a0b\u751f\u6210\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u751f\u6210\u4e86\u4e09\u4e2a\u5173\u952e\u4e3b\u9898\u7684\u82f1\u8bed\u8bfe\u7a0b\uff1a\u9f13\u52b1\u5b66\u751f\u72ec\u7acb\u6027\u3001\u9f13\u52b1\u6c42\u52a9\u884c\u4e3a\u548c\u5f00\u542f\u6444\u50cf\u5934\u3002\u751f\u6210\u7684\u8bfe\u7a0b\u7531\u4e24\u540d\u4eba\u7c7b\u8bc4\u4f30\u8005\u6839\u636e\u8bfe\u7a0b\u8bbe\u8ba1\u7814\u7a76\u5236\u5b9a\u7684\u7efc\u5408\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4efb\u52a1\u5206\u89e3\u7b56\u7565\u751f\u6210\u7684\u8bfe\u7a0b\u8bc4\u5206\u9ad8\u4e8e\u5355\u6b65\u751f\u6210\u3002\u8bc4\u4f30\u8005\u6307\u51fa\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u8bfe\u7a0b\u5728\u5185\u5bb9\u7ed3\u6784\u548c\u8282\u7701\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u5b58\u5728\u53cd\u9988\u6cdb\u5316\u548c\u90e8\u5206\u6559\u5b66\u73af\u8282\u4e0d\u6e05\u6670\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u4eba\u673a\u6df7\u5408\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u6548\u5bfc\u5e08\u57f9\u8bad\u8bfe\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.18237", "pdf": "https://arxiv.org/pdf/2506.18237", "abs": "https://arxiv.org/abs/2506.18237", "authors": ["Xu Wan", "Wei Wang", "Wenyue Xu", "Wotao Yin", "Jie Song", "Mingyang Sun"], "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL)-based post-training has significantly advanced\nthe complex reasoning capabilities of language models, fostering sophisticated\nself-reflection processes. However, this ``slow thinking'' paradigm presents a\ncritical challenge to reasoning efficiency: models may expend excessive\ncomputation on simple questions and shift reasoning prematurely for complex\nones. Previous mechanisms typically rely on static length budgets or predefined\nrules, lacking the adaptability for varying question complexities and models'\nevolving capabilities. To this end, we propose AdapThink, an adaptive\npost-training framework designed to induce more efficient thinking while\nmaintaining the performance of reasoning language models. Specifically,\nAdapThink incorporates two key mechanisms: 1) A group-relative reward function\nthat leverages model confidence and response's characteristic to dynamically\nadjust the preference of reflection-related transition words without resorting\nto a fixed length preference. 2) A diversity-aware sampling mechanism that\nbalances the training group's solution accuracy with reasoning diversity via an\nentropy-guided score. Experiments on several mathematical reasoning datasets\nwith DeepSeek-distilled models demonstrate AdapThink's advantages in enabling\nadaptive reasoning patterns and mitigating the inefficiencies.", "AI": {"tldr": "AdapThink\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u53cd\u601d\u504f\u597d\u548c\u591a\u6837\u6027\u611f\u77e5\u91c7\u6837\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u867d\u7136\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9759\u6001\u9884\u7b97\u6216\u89c4\u5219\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u95ee\u9898\u590d\u6742\u5ea6\u548c\u6a21\u578b\u80fd\u529b\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "AdapThink\u5305\u542b\u4e24\u79cd\u673a\u5236\uff1a1\uff09\u57fa\u4e8e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u54cd\u5e94\u7279\u5f81\u7684\u7fa4\u7ec4\u76f8\u5bf9\u5956\u52b1\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u53cd\u601d\u504f\u597d\uff1b2\uff09\u901a\u8fc7\u71b5\u5f15\u5bfc\u5206\u6570\u5e73\u8861\u8bad\u7ec3\u7fa4\u7684\u89e3\u51c6\u786e\u6027\u548c\u63a8\u7406\u591a\u6837\u6027\u7684\u591a\u6837\u6027\u611f\u77e5\u91c7\u6837\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdapThink\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u6a21\u5f0f\u5e76\u7f13\u89e3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "conclusion": "AdapThink\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u4e3a\u540e\u8bad\u7ec3\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "AdapThink\uff1a\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u601d\u7ef4\u504f\u597d", "abstract_zh": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u540e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u7cbe\u7ec6\u7684\u81ea\u6211\u53cd\u601d\u8fc7\u7a0b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u201c\u6162\u601d\u8003\u201d\u8303\u5f0f\u5bf9\u63a8\u7406\u6548\u7387\u63d0\u51fa\u4e86\u5173\u952e\u6311\u6218\uff1a\u6a21\u578b\u53ef\u80fd\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8017\u8d39\u8fc7\u591a\u8ba1\u7b97\uff0c\u6216\u5728\u590d\u6742\u95ee\u9898\u4e0a\u8fc7\u65e9\u5207\u6362\u63a8\u7406\u3002\u4ee5\u5f80\u7684\u673a\u5236\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u957f\u5ea6\u9884\u7b97\u6216\u9884\u5b9a\u4e49\u89c4\u5219\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u95ee\u9898\u590d\u6742\u5ea6\u548c\u6a21\u578b\u80fd\u529b\u52a8\u6001\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AdapThink\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u8bf1\u5bfc\u66f4\u9ad8\u6548\u7684\u601d\u7ef4\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0cAdapThink\u5305\u542b\u4e24\u79cd\u5173\u952e\u673a\u5236\uff1a1\uff09\u4e00\u79cd\u7fa4\u7ec4\u76f8\u5bf9\u5956\u52b1\u51fd\u6570\uff0c\u5229\u7528\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u54cd\u5e94\u7279\u5f81\u52a8\u6001\u8c03\u6574\u53cd\u601d\u76f8\u5173\u8fc7\u6e21\u8bcd\u7684\u504f\u597d\uff0c\u800c\u65e0\u9700\u56fa\u5b9a\u957f\u5ea6\u504f\u597d\uff1b2\uff09\u4e00\u79cd\u591a\u6837\u6027\u611f\u77e5\u91c7\u6837\u673a\u5236\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u5206\u6570\u5e73\u8861\u8bad\u7ec3\u7fa4\u7684\u89e3\u51c6\u786e\u6027\u4e0e\u63a8\u7406\u591a\u6837\u6027\u3002\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u4f7f\u7528DeepSeek\u84b8\u998f\u6a21\u578b\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86AdapThink\u5728\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u6a21\u5f0f\u548c\u7f13\u89e3\u6548\u7387\u4f4e\u4e0b\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.18292", "pdf": "https://arxiv.org/pdf/2506.18292", "abs": "https://arxiv.org/abs/2506.18292", "authors": ["Ziyue Guo", "Xin Yang", "Yutao Shen", "Yang Zhu", "Lixi Jiang", "Haiyan Cen"], "title": "Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture", "categories": ["cs.CV"], "comment": null, "summary": "Quantitative descriptions of complete canopy architecture are crucial for\nevaluating crop photosynthesis and yield to guide ideotype design. Although\nthree-dimensional (3D) sensing technologies have been developed for plant and\ncanopy reconstruction, severe occlusion and complex architectures hinder\naccurate canopy descriptions. In this study, we propose a point cloud\ncompletion model for 3D reconstruction of rapeseed populations from seeding to\nsilique stages using multi-view imaging. A complete point cloud generation\nframework was developed with the virtual-real integration (VRI) simulation\nmethod and occlusion point detection algorithm to annotate the training dataset\nby distinguishing surface from occluded points. The rapeseed population point\ncloud completion network (RP-PCN) was designed with a multi-resolution dynamic\ngraph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict\noccluded points based on input surface point clouds. A dynamic graph\nconvolutional feature extractor (DGCFE) was introduced to capture structural\nvariations across the growth period. The effectiveness of point cloud\ncompletion was validated by predicting yield using architectural indicators\nfrom complete point clouds of rapeseed population. The results demonstrated\nthat RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm,\nand 4.51 cm at the seedling, bolting, flowering, and silique stages,\nrespectively. Ablation studies showed the effectiveness of the MRDG and DGCFE\nmodules, reducing CD values by 10% and 23%, respectively. The silique\nefficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2%\ncompared to incomplete point clouds. The RP-PCN pipeline proposed in this study\nhas the potential to be extended to other crops, significantly enhancing the\nanalysis of population canopy architectures in field environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u56fe\u5377\u79ef\u7684\u6cb9\u83dc\u7fa4\u4f53\u70b9\u4e91\u8865\u5168\u7f51\u7edc\uff08RP-PCN\uff09\uff0c\u7528\u4e8e\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u91cd\u5efa\u6cb9\u83dc\u7fa4\u4f53\u4ece\u82d7\u671f\u5230\u89d2\u679c\u671f\u7684\u4e09\u7ef4\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u865a\u62df-\u73b0\u5b9e\u878d\u5408\u6a21\u62df\u548c\u906e\u6321\u70b9\u68c0\u6d4b\u7b97\u6cd5\u751f\u6210\u5b8c\u6574\u70b9\u4e91\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u591a\u5206\u8fa8\u7387\u52a8\u6001\u56fe\u5377\u79ef\u7f16\u7801\u5668\u548c\u70b9\u91d1\u5b57\u5854\u89e3\u7801\u5668\u9884\u6d4b\u906e\u6321\u70b9\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRP-PCN\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u7cbe\u5ea6\u548c\u4ea7\u91cf\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f5c\u7269\u51a0\u5c42\u7ed3\u6784\u7684\u5b8c\u6574\u5b9a\u91cf\u63cf\u8ff0\u5bf9\u8bc4\u4f30\u5149\u5408\u4f5c\u7528\u548c\u4ea7\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e25\u91cd\u7684\u906e\u6321\u548c\u590d\u6742\u7ed3\u6784\u963b\u788d\u4e86\u51c6\u786e\u63cf\u8ff0\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u70b9\u4e91\u8865\u5168\u6280\u672f\u89e3\u51b3\u6cb9\u83dc\u7fa4\u4f53\u51a0\u5c42\u4e09\u7ef4\u91cd\u5efa\u4e2d\u7684\u906e\u6321\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u865a\u62df-\u73b0\u5b9e\u878d\u5408\uff08VRI\uff09\u6a21\u62df\u548c\u906e\u6321\u70b9\u68c0\u6d4b\u7b97\u6cd5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u96c6\uff1b2. \u8bbe\u8ba1RP-PCN\u7f51\u7edc\uff0c\u5305\u542b\u591a\u5206\u8fa8\u7387\u52a8\u6001\u56fe\u5377\u79ef\u7f16\u7801\u5668\uff08MRDG\uff09\u548c\u70b9\u91d1\u5b57\u5854\u89e3\u7801\u5668\uff08PPD\uff09\uff1b3. \u5f15\u5165\u52a8\u6001\u56fe\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u5668\uff08DGCFE\uff09\u6355\u6349\u751f\u957f\u5468\u671f\u4e2d\u7684\u7ed3\u6784\u53d8\u5316\u3002", "result": "RP-PCN\u5728\u82d7\u671f\u3001\u62bd\u85b9\u671f\u3001\u82b1\u671f\u548c\u89d2\u679c\u671f\u7684Chamfer\u8ddd\u79bb\uff08CD\uff09\u5206\u522b\u4e3a3.35 cm\u30013.46 cm\u30014.32 cm\u548c4.51 cm\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0cMRDG\u548cDGCFE\u6a21\u5757\u5206\u522b\u964d\u4f4eCD\u503c10%\u548c23%\u3002RP-PCN\u7684\u89d2\u679c\u6548\u7387\u6307\u6570\uff08SEI\uff09\u5c06\u4ea7\u91cf\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e8611.2%\u3002", "conclusion": "RP-PCN\u80fd\u6709\u6548\u8865\u5168\u6cb9\u83dc\u7fa4\u4f53\u51a0\u5c42\u7684\u70b9\u4e91\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4ea7\u91cf\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u6709\u671b\u63a8\u5e7f\u81f3\u5176\u4ed6\u4f5c\u7269\uff0c\u4e3a\u7530\u95f4\u7fa4\u4f53\u51a0\u5c42\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "paper_title_zh": "\u57fa\u4e8e\u52a8\u6001\u56fe\u5377\u79ef\u7684\u6cb9\u83dc\u7fa4\u4f53\u70b9\u4e91\u8865\u5168\u7f51\u7edc\uff08RP-PCN\uff09\u7528\u4e8e\u4f5c\u7269\u51a0\u5c42\u906e\u6321\u7ed3\u6784\u7684\u4e09\u7ef4\u91cd\u5efa", "abstract_zh": "\u5b8c\u6574\u7684\u51a0\u5c42\u7ed3\u6784\u5b9a\u91cf\u63cf\u8ff0\u5bf9\u8bc4\u4f30\u4f5c\u7269\u5149\u5408\u4f5c\u7528\u548c\u4ea7\u91cf\u4ee5\u6307\u5bfc\u7406\u60f3\u578b\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u5f00\u53d1\u51fa\u7528\u4e8e\u690d\u7269\u548c\u51a0\u5c42\u91cd\u5efa\u7684\u4e09\u7ef4\uff083D\uff09\u4f20\u611f\u6280\u672f\uff0c\u4f46\u4e25\u91cd\u7684\u906e\u6321\u548c\u590d\u6742\u7ed3\u6784\u963b\u788d\u4e86\u51c6\u786e\u7684\u51a0\u5c42\u63cf\u8ff0\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u89d2\u6210\u50cf\u7684\u70b9\u4e91\u8865\u5168\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u82d7\u671f\u5230\u89d2\u679c\u671f\u7684\u6cb9\u83dc\u7fa4\u4f53\u4e09\u7ef4\u91cd\u5efa\u3002\u901a\u8fc7\u865a\u62df-\u73b0\u5b9e\u878d\u5408\uff08VRI\uff09\u6a21\u62df\u65b9\u6cd5\u548c\u906e\u6321\u70b9\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5f00\u53d1\u4e86\u5b8c\u6574\u70b9\u4e91\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u533a\u5206\u8868\u9762\u70b9\u548c\u906e\u6321\u70b9\u5e76\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u8bbe\u8ba1\u7684\u6cb9\u83dc\u7fa4\u4f53\u70b9\u4e91\u8865\u5168\u7f51\u7edc\uff08RP-PCN\uff09\u91c7\u7528\u591a\u5206\u8fa8\u7387\u52a8\u6001\u56fe\u5377\u79ef\u7f16\u7801\u5668\uff08MRDG\uff09\u548c\u70b9\u91d1\u5b57\u5854\u89e3\u7801\u5668\uff08PPD\uff09\uff0c\u57fa\u4e8e\u8f93\u5165\u8868\u9762\u70b9\u4e91\u9884\u6d4b\u906e\u6321\u70b9\u3002\u5f15\u5165\u52a8\u6001\u56fe\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u5668\uff08DGCFE\uff09\u4ee5\u6355\u6349\u751f\u957f\u5468\u671f\u4e2d\u7684\u7ed3\u6784\u53d8\u5316\u3002\u901a\u8fc7\u4ece\u5b8c\u6574\u70b9\u4e91\u4e2d\u63d0\u53d6\u7684\u51a0\u5c42\u7ed3\u6784\u6307\u6807\u9884\u6d4b\u4ea7\u91cf\uff0c\u9a8c\u8bc1\u4e86\u70b9\u4e91\u8865\u5168\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cRP-PCN\u5728\u82d7\u671f\u3001\u62bd\u85b9\u671f\u3001\u82b1\u671f\u548c\u89d2\u679c\u671f\u7684Chamfer\u8ddd\u79bb\uff08CD\uff09\u5206\u522b\u4e3a3.35 cm\u30013.46 cm\u30014.32 cm\u548c4.51 cm\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0cMRDG\u548cDGCFE\u6a21\u5757\u5206\u522b\u964d\u4f4eCD\u503c10%\u548c23%\u3002RP-PCN\u7684\u89d2\u679c\u6548\u7387\u6307\u6570\uff08SEI\uff09\u4e0e\u4e0d\u5b8c\u6574\u70b9\u4e91\u76f8\u6bd4\uff0c\u5c06\u4ea7\u91cf\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e8611.2%\u3002\u672c\u7814\u7a76\u4e2d\u63d0\u51fa\u7684RP-PCN\u6d41\u7a0b\u6709\u671b\u63a8\u5e7f\u81f3\u5176\u4ed6\u4f5c\u7269\uff0c\u663e\u8457\u63d0\u5347\u7530\u95f4\u7fa4\u4f53\u51a0\u5c42\u7ed3\u6784\u7684\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2506.17357", "pdf": "https://arxiv.org/pdf/2506.17357", "abs": "https://arxiv.org/abs/2506.17357", "authors": ["Zhenyu Lei", "Jin-Kao Hao", "Qinghua Wu"], "title": "Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Local search plays a central role in many effective heuristic algorithms for\nthe vehicle routing problem (VRP) and its variants. However, neighborhood\nexploration is known to be computationally expensive and time consuming,\nespecially for large instances or problems with complex constraints. In this\nstudy, we explore a promising direction to address this challenge by\nintroducing an original tensor-based GPU acceleration method designed to speed\nup the commonly used local search operators in vehicle routing. By using an\nattribute-based representation, the method offers broad extensibility, making\nit applicable to different VRP variants. Its low-coupling architecture, with\nintensive computations completely offloaded to the GPU, ensures seamless\nintegration in various local search-based algorithms and frameworks, leading to\nsignificant improvements in computational efficiency and potentially improved\nsolution quality. Through comparative experiments on benchmark instances of\nthree routing problems, we demonstrate the substantial computational advantages\nof the proposed approach over traditional CPU-based implementations. We also\nprovide a detailed analysis of the strengths and limitations of the method,\nproviding valuable insights into its performance characteristics and\nidentifying potential bottlenecks in practical applications. These findings\ncontribute to a better understanding and suggest directions for future\nimprovements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7684GPU\u52a0\u901f\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u4e2d\u5c40\u90e8\u641c\u7d22\u7b97\u5b50\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u5e76\u53ef\u80fd\u63d0\u9ad8\u89e3\u7684\u8d28\u91cf\u3002", "motivation": "\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e2d\u7684\u5c40\u90e8\u641c\u7d22\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6216\u590d\u6742\u7ea6\u675f\u95ee\u9898\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7GPU\u52a0\u901f\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5c5e\u6027\u7684\u5f20\u91cf\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5bc6\u96c6\u8ba1\u7b97\u5b8c\u5168\u5378\u8f7d\u5230GPU\u4e0a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u8026\u5408\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u591a\u79cdVRP\u53d8\u4f53\u3002", "result": "\u5728\u4e09\u79cd\u8def\u7531\u95ee\u9898\u7684\u57fa\u51c6\u5b9e\u4f8b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfCPU\u5b9e\u73b0\u5c55\u73b0\u51fa\u663e\u8457\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6027\u80fd\u7279\u70b9\u548c\u6f5c\u5728\u74f6\u9888\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u9650\u5236\u3002", "paper_title_zh": "\u57fa\u4e8e\u5f20\u91cf\u7684GPU\u52a0\u901f\u5728\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e2d\u52a0\u901f\u5c40\u90e8\u4f18\u5316", "abstract_zh": "\u5c40\u90e8\u641c\u7d22\u5728\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u53ca\u5176\u53d8\u4f53\u7684\u8bb8\u591a\u6709\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e2d\u626e\u6f14\u7740\u6838\u5fc3\u89d2\u8272\u3002\u7136\u800c\uff0c\u90bb\u57df\u63a2\u7d22\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u6216\u590d\u6742\u7ea6\u675f\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7684GPU\u52a0\u901f\u65b9\u6cd5\uff0c\u65e8\u5728\u52a0\u901f\u8f66\u8f86\u8def\u5f84\u4e2d\u5e38\u7528\u7684\u5c40\u90e8\u641c\u7d22\u7b97\u5b50\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e\u5c5e\u6027\u7684\u8868\u793a\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684VRP\u53d8\u4f53\u3002\u5176\u4f4e\u8026\u5408\u67b6\u6784\u5c06\u5bc6\u96c6\u8ba1\u7b97\u5b8c\u5168\u5378\u8f7d\u5230GPU\u4e0a\uff0c\u786e\u4fdd\u4e86\u5728\u5404\u79cd\u57fa\u4e8e\u5c40\u90e8\u641c\u7d22\u7684\u7b97\u6cd5\u548c\u6846\u67b6\u4e2d\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u5e76\u53ef\u80fd\u6539\u5584\u89e3\u7684\u8d28\u91cf\u3002\u901a\u8fc7\u5728\u4e09\u79cd\u8def\u7531\u95ee\u9898\u7684\u57fa\u51c6\u5b9e\u4f8b\u4e0a\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfCPU\u5b9e\u73b0\u7684\u663e\u8457\u8ba1\u7b97\u4f18\u52bf\u3002\u6211\u4eec\u8fd8\u8be6\u7ec6\u5206\u6790\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u5176\u6027\u80fd\u7279\u70b9\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u74f6\u9888\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u8be5\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.18254", "pdf": "https://arxiv.org/pdf/2506.18254", "abs": "https://arxiv.org/abs/2506.18254", "authors": ["Tianyu Yu", "Bo Ji", "Shouli Wang", "Shu Yao", "Zefan Wang", "Ganqu Cui", "Lifan Yuan", "Ning Ding", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun", "Tat-Seng Chua"], "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project Website: https://github.com/openbmb/RLPR", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRLPR\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528LLM\u81ea\u8eab\u751f\u6210\u7b54\u6848\u7684\u6982\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u5668\uff0c\u5c06RLVR\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u901a\u7528\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u6570\u5b66\u548c\u4ee3\u7801\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u56e0\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u5668\u800c\u96be\u4ee5\u6269\u5c55\u5230\u901a\u7528\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5\u3002", "method": "RLPR\u6846\u67b6\u5229\u7528LLM\u751f\u6210\u7b54\u6848\u7684\u81ea\u8eab\u6982\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7prob-to-reward\u548c\u7a33\u5b9a\u5316\u65b9\u6cd5\u964d\u4f4e\u566a\u58f0\u5956\u52b1\u7684\u65b9\u5dee\uff0c\u4ece\u800c\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u901a\u7528\u9886\u57df\u548c\u4e09\u4e2a\u6570\u5b66\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRLPR\u663e\u8457\u63d0\u5347\u4e86Gemma\u3001Llama\u548cQwen\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u751a\u81f3\u4f18\u4e8e\u4f9d\u8d56\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5\u3002", "conclusion": "RLPR\u901a\u8fc7LLM\u81ea\u8eab\u6982\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u6210\u529f\u5c06RLVR\u6269\u5c55\u5230\u901a\u7528\u9886\u57df\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "RLPR\uff1a\u65e0\u9700\u9a8c\u8bc1\u5668\u5c06RLVR\u63a8\u5e7f\u81f3\u901a\u7528\u9886\u57df", "abstract_zh": "\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u6210\u529f\u4e3b\u8981\u5c40\u9650\u4e8e\u6570\u5b66\u548c\u4ee3\u7801\u9886\u57df\uff0c\u539f\u56e0\u5728\u4e8e\u5bf9\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u5668\u7684\u4f9d\u8d56\u5bfc\u81f4\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u89c2\u5bdf\u5230LLM\u751f\u6210\u6b63\u786e\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u7684\u5185\u5728\u6982\u7387\u76f4\u63a5\u53cd\u6620\u4e86\u5176\u5bf9\u63a8\u7406\u5956\u52b1\u7684\u8bc4\u4f30\uff08\u5373\u63a8\u7406\u8fc7\u7a0b\u5bfc\u5411\u6b63\u786e\u7b54\u6848\u7684\u7a0b\u5ea6\uff09\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51faRLPR\uff0c\u4e00\u79cd\u7b80\u5355\u7684\u65e0\u9a8c\u8bc1\u5668\u6846\u67b6\uff0c\u5c06RLVR\u63a8\u5e7f\u81f3\u66f4\u5e7f\u6cdb\u7684\u901a\u7528\u9886\u57df\u3002RLPR\u5229\u7528LLM\u5bf9\u53c2\u8003\u7b54\u6848\u7684\u6807\u8bb0\u6982\u7387\u5206\u6570\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u6700\u5927\u5316\u671f\u671b\u5956\u52b1\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u89e3\u51b3\u8fd9\u4e00\u566a\u58f0\u6982\u7387\u5956\u52b1\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86prob-to-reward\u548c\u7a33\u5b9a\u5316\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u4eceLLM\u5185\u5728\u6982\u7387\u4e2d\u83b7\u53d6\u7cbe\u786e\u4e14\u7a33\u5b9a\u7684\u5956\u52b1\u3002\u5728\u56db\u4e2a\u901a\u7528\u9886\u57df\u57fa\u51c6\u548c\u4e09\u4e2a\u6570\u5b66\u9886\u57df\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cRLPR\u663e\u8457\u63d0\u5347\u4e86Gemma\u3001Llama\u548cQwen\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cRLPR\u5728TheoremQA\u4e0a\u6bd4VeriFree\u9ad8\u51fa7.6\u5206\uff0c\u5728Minerva\u4e0a\u9ad8\u51fa7.5\u5206\uff0c\u751a\u81f3\u5728\u4e03\u4e2a\u57fa\u51c6\u4e0a\u5e73\u5747\u4f18\u4e8e\u4f9d\u8d56\u9a8c\u8bc1\u5668\u7684General-Reasoner\u65b9\u6cd51.6\u5206\u3002"}}
{"id": "2506.18321", "pdf": "https://arxiv.org/pdf/2506.18321", "abs": "https://arxiv.org/abs/2506.18321", "authors": ["Zeeshan Ramzan", "Nisar Ahmed", "Qurat-ul-Ain Akram", "Shahzad Asif", "Muhammad Shahbaz", "Rabin Chakrabortty", "Ahmed F. Elaksher"], "title": "Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion", "categories": ["cs.CV"], "comment": "Under review in Earth Systems and Environment", "summary": "Remote sensing offers a highly effective method for obtaining accurate\ninformation on total cropped area and crop types. The study focuses on crop\ncover identification for irrigated regions of Central Punjab. Data collection\nwas executed in two stages: the first involved identifying and geocoding six\ntarget crops through field surveys conducted in January and February 2023. The\nsecond stage involved acquiring Landsat 8-9 imagery for each geocoded field to\nconstruct a labelled dataset. The satellite imagery underwent extensive\npre-processing, including radiometric calibration for reflectance values,\natmospheric correction, and georeferencing verification to ensure consistency\nwithin a common coordinate system. Subsequently, image fusion techniques were\napplied to combine Landsat 8 and 9 spectral bands, creating a composite image\nwith enhanced spectral information, followed by contrast enhancement. During\ndata acquisition, farmers were interviewed, and fields were meticulously mapped\nusing GPS instruments, resulting in a comprehensive dataset of 50,835 data\npoints. This dataset facilitated the extraction of vegetation indices such as\nNDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were\nutilized for classification modeling using conventional classifiers, ensemble\nlearning, and artificial neural networks. A feature selection approach was also\nincorporated to identify the optimal feature set for classification learning.\nThis study demonstrates the effectiveness of combining remote sensing data and\nadvanced modeling techniques to improve crop classification accuracy in\nirrigated agricultural regions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528Landsat 8-9\u878d\u5408\u6570\u636e\u548c\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u690d\u88ab\u6307\u6570\u548c\u53cd\u5c04\u7387\u503c\uff0c\u63d0\u9ad8\u4e86\u5df4\u57fa\u65af\u5766\u65c1\u906e\u666e\u4e2d\u90e8\u704c\u6e89\u533a\u4f5c\u7269\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9065\u611f\u6280\u672f\u4e3a\u4f5c\u7269\u9762\u79ef\u548c\u7c7b\u578b\u7684\u9ad8\u6548\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\u548c\u5148\u8fdb\u5efa\u6a21\u6280\u672f\uff0c\u63d0\u5347\u704c\u6e89\u519c\u4e1a\u533a\u7684\u4f5c\u7269\u5206\u7c7b\u7cbe\u5ea6\u3002", "method": "\u7814\u7a76\u5206\u4e24\u9636\u6bb5\uff1a1) \u901a\u8fc7\u5b9e\u5730\u8c03\u67e5\u548cGPS\u6d4b\u7ed8\u6536\u96c6\u76ee\u6807\u4f5c\u7269\u7684\u5730\u7406\u7f16\u7801\u6570\u636e\uff1b2) \u83b7\u53d6Landsat 8-9\u5f71\u50cf\uff0c\u8fdb\u884c\u8f90\u5c04\u6821\u51c6\u3001\u5927\u6c14\u6821\u6b63\u548c\u56fe\u50cf\u878d\u5408\u3002\u63d0\u53d6\u690d\u88ab\u6307\u6570\u540e\uff0c\u91c7\u7528\u4f20\u7edf\u5206\u7c7b\u5668\u3001\u96c6\u6210\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u9009\u62e9\u4f18\u5316\u5206\u7c7b\u7279\u5f81\u3002", "result": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b50,835\u4e2a\u6570\u636e\u70b9\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u878d\u5408Landsat 8-9\u5149\u8c31\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f5c\u7269\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u5408\u9065\u611f\u6570\u636e\u548c\u5148\u8fdb\u5efa\u6a21\u6280\u672f\uff0c\u53ef\u6709\u6548\u63d0\u9ad8\u704c\u6e89\u519c\u4e1a\u533a\u7684\u4f5c\u7269\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e3a\u519c\u4e1a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u4e8e\u6ce8\u610f\u529b\u96c6\u6210\u5b66\u4e60\u7684Landsat 8-9\u878d\u5408\u6570\u636e\u4f5c\u7269\u5206\u7c7b\u65b9\u6cd5", "abstract_zh": "\u9065\u611f\u6280\u672f\u4e3a\u83b7\u53d6\u4f5c\u7269\u603b\u9762\u79ef\u548c\u7c7b\u578b\u7684\u7cbe\u786e\u4fe1\u606f\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u805a\u7126\u5df4\u57fa\u65af\u5766\u65c1\u906e\u666e\u4e2d\u90e8\u704c\u6e89\u533a\u7684\u4f5c\u7269\u8986\u76d6\u8bc6\u522b\u3002\u6570\u636e\u6536\u96c6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc72023\u5e741\u6708\u81f32\u6708\u7684\u5b9e\u5730\u8c03\u67e5\uff0c\u5bf9\u516d\u79cd\u76ee\u6807\u4f5c\u7269\u8fdb\u884c\u5730\u7406\u7f16\u7801\uff1b\u7b2c\u4e8c\u9636\u6bb5\u83b7\u53d6\u6bcf\u4e2a\u5730\u7406\u7f16\u7801\u7530\u5757\u7684Landsat 8-9\u5f71\u50cf\uff0c\u6784\u5efa\u6807\u8bb0\u6570\u636e\u96c6\u3002\u536b\u661f\u5f71\u50cf\u7ecf\u8fc7\u8f90\u5c04\u6821\u51c6\uff08\u53cd\u5c04\u7387\u503c\uff09\u3001\u5927\u6c14\u6821\u6b63\u548c\u5730\u7406\u914d\u51c6\u9a8c\u8bc1\uff0c\u786e\u4fdd\u5176\u5728\u7edf\u4e00\u5750\u6807\u7cfb\u4e2d\u7684\u4e00\u81f4\u6027\u3002\u968f\u540e\uff0c\u5e94\u7528\u56fe\u50cf\u878d\u5408\u6280\u672f\u7ed3\u5408Landsat 8\u548c9\u7684\u5149\u8c31\u6ce2\u6bb5\uff0c\u751f\u6210\u5177\u6709\u589e\u5f3a\u5149\u8c31\u4fe1\u606f\u7684\u590d\u5408\u56fe\u50cf\uff0c\u5e76\u8fdb\u884c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u3002\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u8bbf\u8c08\u519c\u6c11\u548c\u4f7f\u7528GPS\u4eea\u5668\u7cbe\u7ec6\u6d4b\u7ed8\u7530\u5757\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u5305\u542b50,835\u4e2a\u6570\u636e\u70b9\u7684\u7efc\u5408\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u652f\u6301\u63d0\u53d6NDVI\u3001SAVO\u3001RECI\u548cNDRE\u7b49\u690d\u88ab\u6307\u6570\u3002\u8fd9\u4e9b\u6307\u6570\u548c\u539f\u59cb\u53cd\u5c04\u7387\u503c\u88ab\u7528\u4e8e\u4f20\u7edf\u5206\u7c7b\u5668\u3001\u96c6\u6210\u5b66\u4e60\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u5efa\u6a21\u3002\u7814\u7a76\u8fd8\u7ed3\u5408\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u786e\u5b9a\u5206\u7c7b\u5b66\u4e60\u7684\u6700\u4f73\u7279\u5f81\u96c6\u3002\u672c\u7814\u7a76\u5c55\u793a\u4e86\u7ed3\u5408\u9065\u611f\u6570\u636e\u548c\u5148\u8fdb\u5efa\u6a21\u6280\u672f\uff0c\u5728\u63d0\u5347\u704c\u6e89\u519c\u4e1a\u533a\u4f5c\u7269\u5206\u7c7b\u7cbe\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.17363", "pdf": "https://arxiv.org/pdf/2506.17363", "abs": "https://arxiv.org/abs/2506.17363", "authors": ["Sunjun Kweon", "Sooyohn Nam", "Hyunseung Lim", "Hwajung Hong", "Edward Choi"], "title": "A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant", "categories": ["cs.CY", "cs.AI"], "comment": "ACL 2025 Industry Track", "summary": "Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs)\nhave the potential to enhance student learning by providing instant feedback\nand facilitating multi-turn interactions. However, empirical studies on their\neffectiveness and acceptance in real-world classrooms are limited, leaving\ntheir practical impact uncertain. In this study, we develop an LLM-based VTA\nand deploy it in an introductory AI programming course with 477 graduate\nstudents. To assess how student perceptions of the VTA's performance evolve\nover time, we conduct three rounds of comprehensive surveys at different stages\nof the course. Additionally, we analyze 3,869 student--VTA interaction pairs to\nidentify common question types and engagement patterns. We then compare these\ninteractions with traditional student--human instructor interactions to\nevaluate the VTA's role in the learning process. Through a large-scale\nempirical study and interaction analysis, we assess the feasibility of\ndeploying VTAs in real-world classrooms and identify key challenges for broader\nadoption. Finally, we release the source code of our VTA system, fostering\nfuture advancements in AI-driven education:\n\\texttt{https://github.com/sean0042/VTA}.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u865a\u62df\u6559\u5b66\u52a9\u624b\uff08VTA\uff09\u5728\u5b9e\u9645\u8bfe\u5802\u4e2d\u7684\u6548\u679c\u548c\u63a5\u53d7\u5ea6\uff0c\u5206\u6790\u4e86\u5b66\u751f\u4e92\u52a8\u6a21\u5f0f\uff0c\u5e76\u4e0e\u4f20\u7edf\u5e08\u751f\u4e92\u52a8\u5bf9\u6bd4\uff0c\u63d0\u51fa\u4e86\u63a8\u5e7fVTA\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u865a\u62df\u6559\u5b66\u52a9\u624b\uff08VTA\uff09\u5728\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u4f53\u9a8c\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u679c\u548c\u63a5\u53d7\u5ea6\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30VTA\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684VTA\uff0c\u5e76\u5728477\u540d\u7814\u7a76\u751f\u7684AI\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u90e8\u7f72\u3002\u901a\u8fc7\u4e09\u8f6e\u95ee\u5377\u8c03\u67e5\u548c3,869\u7ec4\u5b66\u751f-VTA\u4e92\u52a8\u6570\u636e\u5206\u6790\uff0c\u8bc4\u4f30VTA\u7684\u8868\u73b0\u548c\u4e92\u52a8\u6a21\u5f0f\uff0c\u5e76\u4e0e\u4f20\u7edf\u5e08\u751f\u4e92\u52a8\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b66\u751f\u5bf9VTA\u7684\u63a5\u53d7\u5ea6\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u4e92\u52a8\u5206\u6790\u63ed\u793a\u4e86\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\u548c\u53c2\u4e0e\u6a21\u5f0f\u3002VTA\u5728\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u95ee\u9898\u4e0a\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cVTA\u5728\u5b9e\u9645\u8bfe\u5802\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u9700\u89e3\u51b3\u590d\u6742\u95ee\u9898\u5904\u7406\u7b49\u6311\u6218\u3002\u7814\u7a76\u5f00\u6e90\u4e86VTA\u7cfb\u7edf\u4ee3\u7801\uff0c\u63a8\u52a8AI\u9a71\u52a8\u6559\u80b2\u7684\u672a\u6765\u53d1\u5c55\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u865a\u62df\u6559\u5b66\u52a9\u624b\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u8bfe\u5802\u4e2d\u7684\u8bc4\u4f30", "abstract_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u865a\u62df\u6559\u5b66\u52a9\u624b\uff08VTA\uff09\u80fd\u591f\u901a\u8fc7\u5373\u65f6\u53cd\u9988\u548c\u591a\u8f6e\u4e92\u52a8\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u8bfe\u5802\u4e2d\u7684\u6548\u679c\u548c\u63a5\u53d7\u5ea6\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\u3002\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684VTA\uff0c\u5e76\u5728477\u540d\u7814\u7a76\u751f\u7684AI\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u90e8\u7f72\u3002\u901a\u8fc7\u4e09\u8f6e\u95ee\u5377\u8c03\u67e5\u548c3,869\u7ec4\u5b66\u751f-VTA\u4e92\u52a8\u6570\u636e\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5b66\u751f\u5bf9VTA\u7684\u611f\u77e5\u53d8\u5316\u3001\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\u548c\u4e92\u52a8\u6a21\u5f0f\uff0c\u5e76\u4e0e\u4f20\u7edf\u5e08\u751f\u4e92\u52a8\u8fdb\u884c\u5bf9\u6bd4\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u548c\u4e92\u52a8\u5206\u6790\uff0c\u672c\u7814\u7a76\u8bc4\u4f30\u4e86VTA\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u63a8\u5e7f\u7684\u5173\u952e\u6311\u6218\u3002\u6700\u540e\uff0c\u7814\u7a76\u5f00\u6e90\u4e86VTA\u7cfb\u7edf\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdbAI\u9a71\u52a8\u6559\u80b2\u7684\u672a\u6765\u53d1\u5c55\uff1a\\texttt{https://github.com/sean0042/VTA}\u3002"}}
{"id": "2506.18311", "pdf": "https://arxiv.org/pdf/2506.18311", "abs": "https://arxiv.org/abs/2506.18311", "authors": ["Hoang-An Trieu", "Dinh-Truong Do", "Chau Nguyen", "Vu Tran", "Minh Le Nguyen"], "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction", "categories": ["cs.IR", "cs.CL"], "comment": "In the Proceedings of SCIDOCA 2024", "summary": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u53d6COVID-19\u6587\u732e\u4e2d\u9690\u85cf\u5173\u7cfb\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u68c0\u7d22\u7cfb\u7edfCovrelex-SE\u7684\u8d28\u91cf\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u66f4\u9ad8\u6548\u5730\u83b7\u53d6\u6709\u7528\u4fe1\u606f\u3002", "motivation": "COVID-19\u75ab\u60c5\u671f\u95f4\uff0c\u5927\u91cf\u76f8\u5173\u6587\u732e\u6d8c\u73b0\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u672a\u6807\u6ce8\u6587\u732e\u4e2d\u7684\u9690\u85cf\u5173\u7cfb\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u8d28\u91cf\u4e0d\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7LLMs\u63d0\u5347\u68c0\u7d22\u7cfb\u7edf\u7684\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u53d6\u672a\u6807\u6ce8COVID-19\u6587\u732e\u4e2d\u7684\u9690\u85cf\u5173\u7cfb\uff0c\u8865\u5145\u73b0\u6709\u89e3\u6790\u5de5\u5177\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u4e3a\u68c0\u7d22\u7cfb\u7edfCovrelex-SE\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4fe1\u606f\u652f\u6301\u3002", "result": "\u901a\u8fc7LLMs\u63d0\u53d6\u7684\u9690\u85cf\u5173\u7cfb\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u7cfb\u7edf\u7684\u68c0\u7d22\u8d28\u91cf\uff0c\u4f7f\u5176\u80fd\u591f\u63d0\u4f9b\u66f4\u591a\u9ad8\u4ef7\u503c\u7684\u4fe1\u606f\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86LLMs\u5728\u63d0\u53d6\u9690\u85cf\u5173\u7cfb\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7a81\u53d1\u516c\u5171\u536b\u751f\u4e8b\u4ef6\u4e2d\u7684\u6587\u732e\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u652f\u6301\u3002", "paper_title_zh": "\u63d0\u5347COVID-19\u7814\u7a76\u4e2d\u7684\u6587\u732e\u68c0\u7d22\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u9690\u85cf\u5173\u7cfb", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u968f\u7740COVID-19\u5927\u6d41\u884c\u7684\u51fa\u73b0\uff0c\u5927\u91cf\u4e0e\u8be5\u75be\u75c5\u76f8\u5173\u7684\u6587\u732e\u88ab\u53d1\u8868\u3002\u7531\u4e8e\u6587\u732e\u6570\u91cf\u5e9e\u5927\uff0c\u9700\u8981\u4e00\u4e2a\u9ad8\u6548\u7684\u68c0\u7d22\u7cfb\u7edf\uff0c\u4ee5\u4fbf\u5728\u7c7b\u4f3cCOVID-19\u8fd9\u6837\u7684\u7a81\u53d1\u75ab\u60c5\u4e2d\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6709\u7528\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5e2e\u52a9\u68c0\u7d22\u7cfb\u7edfCovrelex-SE\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u641c\u7d22\u7ed3\u679c\u3002\u6211\u4eec\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u53d6\u672a\u6807\u6ce8\u6587\u732e\u4e2d\u73b0\u6709\u89e3\u6790\u5de5\u5177\u65e0\u6cd5\u53d1\u73b0\u7684\u9690\u85cf\u5173\u7cfb\uff0c\u4ece\u800c\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u4e3a\u7cfb\u7edf\u63d0\u4f9b\u66f4\u591a\u6709\u7528\u4fe1\u606f\u3002"}}
{"id": "2506.18322", "pdf": "https://arxiv.org/pdf/2506.18322", "abs": "https://arxiv.org/abs/2506.18322", "authors": ["Yiwei Yang", "Chung Peng Lee", "Shangbin Feng", "Dora Zhao", "Bingbing Wen", "Anthony Z. Liu", "Yulia Tsvetkov", "Bill Howe"], "title": "Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Finetuning can cause spurious correlations to arise between non-essential\nfeatures and the target labels, but benchmarks to study these effects involve\ncontrived settings and narrow tasks. In contrast, we consider spurious\ncorrelations in multi-modal Large Vision Language Models (LVLMs) pretrained on\nextensive and diverse datasets without explicit task supervision. We develop a\nbenchmark by sourcing GPT-4o errors on real-world visual-question-answering\n(VQA) benchmarks, then curating a subset through LVLM-human annotation and\nsynthetic counterfactual evaluation to identify errors caused by spurious\ncorrelations. This process yields SpuriVerse, a novel benchmark comprised of\n124 distinct types of spurious correlations extracted from real-world datasets,\neach containing 1 realistic and 10 synthetic VQA samples for a total of 1364\nmultiple choice questions. We evaluate 15 open and closed-source LVLMs on\nSpuriVerse, finding that even state-of-the-art closed-source models struggle\nsignificantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic\nexamples that emphasize the spurious correlation improves performance to\n78.40%, suggesting that training on diverse spurious patterns generalizes to\nunseen situations: models appear to learn to avoid \"shortcuts\" and attend to\nthe overall image context.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86SpuriVerse\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u907f\u514d\u865a\u5047\u76f8\u5173\u6027\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7a84\u4efb\u52a1\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u591a\u6a21\u6001\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5e7f\u6cdb\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u540e\u662f\u5426\u4ecd\u53d7\u865a\u5047\u76f8\u5173\u6027\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u901a\u8fc7\u5206\u6790GPT-4o\u5728\u771f\u5b9e\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548c\u5408\u6210\u53cd\u4e8b\u5b9e\u8bc4\u4f30\uff0c\u6784\u5efa\u4e86SpuriVerse\u57fa\u51c6\uff0c\u5305\u542b124\u7c7b\u865a\u5047\u76f8\u5173\u6027\u548c1364\u4e2a\u591a\u9009\u95ee\u9898\u3002\u968f\u540e\u8bc4\u4f30\u4e8615\u79cd\u5f00\u6e90\u548c\u95ed\u6e90LVLMs\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u9876\u7ea7\u95ed\u6e90\u6a21\u578b\u5728SpuriVerse\u4e0a\u7684\u51c6\u786e\u7387\u6700\u9ad8\u4ec5\u4e3a37.1%\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u865a\u5047\u76f8\u5173\u6027\u7684\u5fae\u8c03\uff0c\u6027\u80fd\u53ef\u63d0\u5347\u81f378.40%\uff0c\u8868\u660e\u6a21\u578b\u80fd\u5b66\u4f1a\u907f\u514d\u201c\u6377\u5f84\u201d\u5e76\u5173\u6ce8\u6574\u4f53\u56fe\u50cf\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLVLMs\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4ecd\u6613\u53d7\u865a\u5047\u76f8\u5173\u6027\u5f71\u54cd\uff0c\u4f46\u901a\u8fc7\u591a\u6837\u5316\u8bad\u7ec3\u53ef\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u4f9d\u8d56\u865a\u5047\u7279\u5f81\u3002", "paper_title_zh": "\u9003\u79bb\u865a\u5047\u5b87\u5b99\uff1a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u8d85\u8d8a\u6240\u89c1\u865a\u5047\u76f8\u5173\u6027\u5b9e\u73b0\u6cdb\u5316\uff1f", "abstract_zh": "\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u975e\u5fc5\u8981\u7279\u5f81\u4e0e\u76ee\u6807\u6807\u7b7e\u4e4b\u95f4\u4ea7\u751f\u865a\u5047\u76f8\u5173\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u4eba\u4e3a\u8bbe\u7f6e\u7684\u7a84\u4efb\u52a1\u3002\u672c\u6587\u5219\u5173\u6ce8\u591a\u6a21\u6001\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5e7f\u6cdb\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u540e\u662f\u5426\u4ecd\u53d7\u865a\u5047\u76f8\u5173\u6027\u5f71\u54cd\u3002\u6211\u4eec\u901a\u8fc7\u5206\u6790GPT-4o\u5728\u771f\u5b9e\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548c\u5408\u6210\u53cd\u4e8b\u5b9e\u8bc4\u4f30\uff0c\u6784\u5efa\u4e86SpuriVerse\u57fa\u51c6\uff0c\u5305\u542b124\u7c7b\u865a\u5047\u76f8\u5173\u6027\u548c1364\u4e2a\u591a\u9009\u95ee\u9898\u3002\u8bc4\u4f3015\u79cd\u5f00\u6e90\u548c\u95ed\u6e90LVLMs\u540e\u53d1\u73b0\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u4e5f\u8f83\u5dee\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4ec5\u4e3a37.1%\u3002\u7136\u800c\uff0c\u901a\u8fc7\u9488\u5bf9\u865a\u5047\u76f8\u5173\u6027\u7684\u5fae\u8c03\uff0c\u6027\u80fd\u53ef\u63d0\u5347\u81f378.40%\uff0c\u8868\u660e\u6a21\u578b\u80fd\u5b66\u4f1a\u907f\u514d\u201c\u6377\u5f84\u201d\u5e76\u5173\u6ce8\u6574\u4f53\u56fe\u50cf\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2506.17364", "pdf": "https://arxiv.org/pdf/2506.17364", "abs": "https://arxiv.org/abs/2506.17364", "authors": ["Alvaro Becerra", "Roberto Daza", "Ruth Cobos", "Aythami Morales", "Mutlu Cukurova", "Julian Fierrez"], "title": "AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning", "categories": ["cs.CY", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted in EC-TEL25: 20th European Conference on Technology Enhanced\n  Learning, Newcastle and Durham, UK, 15-19 September 2025", "summary": "This work investigates the use of multimodal biometrics to detect\ndistractions caused by smartphone use during tasks that require sustained\nattention, with a focus on computer-based online learning. Although the methods\nare applicable to various domains, such as autonomous driving, we concentrate\non the challenges learners face in maintaining engagement amid internal (e.g.,\nmotivation), system-related (e.g., course design) and contextual (e.g.,\nsmartphone use) factors. Traditional learning platforms often lack detailed\nbehavioral data, but Multimodal Learning Analytics (MMLA) and biosensors\nprovide new insights into learner attention. We propose an AI-based approach\nthat leverages physiological signals and head pose data to detect phone use.\nOur results show that single biometric signals, such as brain waves or heart\nrate, offer limited accuracy, while head pose alone achieves 87%. A multimodal\nmodel combining all signals reaches 91% accuracy, highlighting the benefits of\nintegration. We conclude by discussing the implications and limitations of\ndeploying these models for real-time support in online learning environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u591a\u6a21\u6001\u751f\u7269\u7279\u5f81\u68c0\u6d4b\u5728\u7ebf\u5b66\u4e60\u4e2d\u56e0\u667a\u80fd\u624b\u673a\u4f7f\u7528\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u5206\u6563\uff0c\u7ed3\u5408\u751f\u7406\u4fe1\u53f7\u548c\u5934\u90e8\u59ff\u6001\u6570\u636e\uff0c\u591a\u6a21\u6001\u6a21\u578b\u51c6\u786e\u7387\u8fbe91%\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u4e2d\uff0c\u5b66\u4e60\u8005\u5e38\u56e0\u667a\u80fd\u624b\u673a\u4f7f\u7528\u7b49\u56e0\u7d20\u5206\u5fc3\uff0c\u4f20\u7edf\u5b66\u4e60\u5e73\u53f0\u7f3a\u4e4f\u8be6\u7ec6\u884c\u4e3a\u6570\u636e\u3002\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\u548c\u751f\u7269\u4f20\u611f\u5668\u4e3a\u6ce8\u610f\u529b\u76d1\u6d4b\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u751f\u7406\u4fe1\u53f7\uff08\u5982\u8111\u7535\u6ce2\u3001\u5fc3\u7387\uff09\u548c\u5934\u90e8\u59ff\u6001\u6570\u636e\uff0c\u68c0\u6d4b\u667a\u80fd\u624b\u673a\u4f7f\u7528\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u5206\u6563\u3002", "result": "\u5355\u4e00\u751f\u7269\u4fe1\u53f7\uff08\u5982\u8111\u7535\u6ce2\u6216\u5fc3\u7387\uff09\u51c6\u786e\u7387\u6709\u9650\uff0c\u5934\u90e8\u59ff\u6001\u5355\u72ec\u51c6\u786e\u7387\u4e3a87%\uff0c\u591a\u6a21\u6001\u6a21\u578b\u7ed3\u5408\u6240\u6709\u4fe1\u53f7\u540e\u51c6\u786e\u7387\u8fbe91%\u3002", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u9700\u8ba8\u8bba\u5176\u5728\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u5b9e\u65f6\u652f\u6301\u7684\u53ef\u884c\u6027\u4e0e\u5c40\u9650\u6027\u3002", "paper_title_zh": "\u57fa\u4e8eAI\u7684\u591a\u6a21\u6001\u751f\u7269\u7279\u5f81\u68c0\u6d4b\u667a\u80fd\u624b\u673a\u5206\u5fc3\uff1a\u5e94\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u6a21\u6001\u751f\u7269\u7279\u5f81\u68c0\u6d4b\u56e0\u667a\u80fd\u624b\u673a\u4f7f\u7528\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u673a\u5728\u7ebf\u5b66\u4e60\u573a\u666f\u3002\u5c3d\u7ba1\u65b9\u6cd5\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\uff0c\u4f46\u672c\u7814\u7a76\u805a\u7126\u5b66\u4e60\u8005\u5728\u5185\u90e8\uff08\u5982\u52a8\u673a\uff09\u3001\u7cfb\u7edf\u76f8\u5173\uff08\u5982\u8bfe\u7a0b\u8bbe\u8ba1\uff09\u548c\u60c5\u5883\uff08\u5982\u667a\u80fd\u624b\u673a\u4f7f\u7528\uff09\u56e0\u7d20\u4e0b\u7ef4\u6301\u6ce8\u610f\u529b\u7684\u6311\u6218\u3002\u4f20\u7edf\u5b66\u4e60\u5e73\u53f0\u5e38\u7f3a\u4e4f\u8be6\u7ec6\u884c\u4e3a\u6570\u636e\uff0c\u800c\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\uff08MMLA\uff09\u548c\u751f\u7269\u4f20\u611f\u5668\u4e3a\u5b66\u4e60\u8005\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u7406\u4fe1\u53f7\u548c\u5934\u90e8\u59ff\u6001\u6570\u636e\u68c0\u6d4b\u624b\u673a\u4f7f\u7528\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5355\u4e00\u751f\u7269\u4fe1\u53f7\uff08\u5982\u8111\u7535\u6ce2\u6216\u5fc3\u7387\uff09\u51c6\u786e\u7387\u6709\u9650\uff0c\u800c\u5934\u90e8\u59ff\u6001\u5355\u72ec\u51c6\u786e\u7387\u8fbe87%\u3002\u7ed3\u5408\u6240\u6709\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u6a21\u578b\u51c6\u786e\u7387\u8fbe91%\uff0c\u51f8\u663e\u4e86\u6574\u5408\u7684\u4f18\u52bf\u3002\u6700\u540e\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u5b9e\u65f6\u652f\u6301\u7684\u5e94\u7528\u4e0e\u5c40\u9650\u6027\u3002"}}
{"id": "2506.18316", "pdf": "https://arxiv.org/pdf/2506.18316", "abs": "https://arxiv.org/abs/2506.18316", "authors": ["Trieu An", "Long Nguyen", "Minh Le Nguyen"], "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "In the Proceedings of SCIDOCA 2025", "summary": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.", "AI": {"tldr": "\u56e2\u961fLA\u5728SCIDOCA 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u7cfb\u7279\u5f81\u7684\u96f6\u6837\u672c\u68c0\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5019\u9009\u6c60\u4e2d\u53d1\u73b0\u6b63\u786e\u7684\u5f15\u6587\u3002\u901a\u8fc7\u63d0\u53d6\u6bb5\u843d\u7684\u5173\u7cfb\u7279\u5f81\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u7cbe\u51c6\u5339\u914d\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f15\u6587\u53d1\u73b0\u4efb\u52a1\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u6bb5\u843d\u957f\u5ea6\u548c\u5019\u9009\u6458\u8981\u4e4b\u95f4\u7684\u9ad8\u5ea6\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u786e\u5b9a\u5f15\u6587\u3002\u56e2\u961fLA\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u57fa\u4e8e\u6bb5\u843d\u63d0\u53d6\u7684\u5173\u7cfb\u7279\u5f81\u68c0\u7d22\u51fa\u524dk\u4e2a\u6700\u76f8\u4f3c\u7684\u6458\u8981\uff0c\u7136\u540e\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u5b50\u96c6\u4e2d\u7cbe\u51c6\u8bc6\u522b\u6700\u76f8\u5173\u7684\u5f15\u6587\u3002", "result": "\u5728SCIDOCA 2025\u63d0\u4f9b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5f15\u6587\u9884\u6d4b\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5173\u7cfb\u7279\u5f81\u68c0\u7d22\u548cLLM\u7684\u7cbe\u51c6\u5339\u914d\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f15\u6587\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "paper_title_zh": "\u56e2\u961fLA\u5728SCIDOCA 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u57fa\u4e8e\u5173\u7cfb\u7279\u5f81\u7684\u96f6\u6837\u672c\u68c0\u7d22\u5b9e\u73b0\u5f15\u6587\u53d1\u73b0", "abstract_zh": "\u5f15\u6587\u53d1\u73b0\u5171\u4eab\u4efb\u52a1\u7684\u76ee\u6807\u662f\u4ece\u7ed9\u5b9a\u7684\u5019\u9009\u6c60\u4e2d\u9884\u6d4b\u51fa\u6bb5\u843d\u7684\u6b63\u786e\u5f15\u6587\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u6458\u8981\u6bb5\u843d\u7684\u957f\u5ea6\u548c\u5019\u9009\u6458\u8981\u4e4b\u95f4\u7684\u9ad8\u5ea6\u76f8\u4f3c\u6027\uff0c\u8fd9\u4f7f\u5f97\u786e\u5b9a\u5177\u4f53\u5f15\u6587\u53d8\u5f97\u56f0\u96be\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7cfb\u7edf\uff0c\u9996\u5148\u57fa\u4e8e\u6bb5\u843d\u63d0\u53d6\u7684\u5173\u7cfb\u7279\u5f81\u68c0\u7d22\u51fa\u524dk\u4e2a\u6700\u76f8\u4f3c\u7684\u6458\u8981\uff0c\u7136\u540e\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u5b50\u96c6\u4e2d\u7cbe\u51c6\u8bc6\u522b\u6700\u76f8\u5173\u7684\u5f15\u6587\u3002\u6211\u4eec\u5728SCIDOCA 2025\u7ec4\u7ec7\u8005\u63d0\u4f9b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5f15\u6587\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18323", "pdf": "https://arxiv.org/pdf/2506.18323", "abs": "https://arxiv.org/abs/2506.18323", "authors": ["Muhammad Azeem Aslam", "Hassan Khalid", "Nisar Ahmed"], "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Low-light image enhancement remains a challenging task, particularly in the\nabsence of paired training data. In this study, we present LucentVisionNet, a\nnovel zero-shot learning framework that addresses the limitations of\ntraditional and deep learning-based enhancement methods. The proposed approach\nintegrates multi-scale spatial attention with a deep curve estimation network,\nenabling fine-grained enhancement while preserving semantic and perceptual\nfidelity. To further improve generalization, we adopt a recurrent enhancement\nstrategy and optimize the model using a composite loss function comprising six\ntailored components, including a novel no-reference image quality loss inspired\nby human visual perception. Extensive experiments on both paired and unpaired\nbenchmark datasets demonstrate that LucentVisionNet consistently outperforms\nstate-of-the-art supervised, unsupervised, and zero-shot methods across\nmultiple full-reference and no-reference image quality metrics. Our framework\nachieves high visual quality, structural consistency, and computational\nefficiency, making it well-suited for deployment in real-world applications\nsuch as mobile photography, surveillance, and autonomous navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6LucentVisionNet\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u589e\u5f3a\u3002", "motivation": "\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\u3002", "method": "LucentVisionNet\u7ed3\u5408\u4e86\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u548c\u6df1\u5ea6\u66f2\u7ebf\u4f30\u8ba1\u7f51\u7edc\uff0c\u91c7\u7528\u5faa\u73af\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5305\u542b\u516d\u4e2a\u5b9a\u5236\u7ec4\u4ef6\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLucentVisionNet\u5728\u591a\u9879\u5168\u53c2\u8003\u548c\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u3001\u65e0\u76d1\u7763\u548c\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u89c6\u89c9\u8d28\u91cf\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LucentVisionNet\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u6444\u5f71\u3001\u76d1\u63a7\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a", "abstract_zh": "\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLucentVisionNet\u7684\u65b0\u578b\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u589e\u5f3a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u5c06\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u4e0e\u6df1\u5ea6\u66f2\u7ebf\u4f30\u8ba1\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u589e\u5f3a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u548c\u611f\u77e5\u7684\u4fdd\u771f\u5ea6\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u5faa\u73af\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5305\u542b\u516d\u4e2a\u5b9a\u5236\u7ec4\u4ef6\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u79cd\u53d7\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u542f\u53d1\u7684\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u635f\u5931\u3002\u5728\u591a\u4e2a\u914d\u5bf9\u548c\u975e\u914d\u5bf9\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLucentVisionNet\u5728\u5168\u53c2\u8003\u548c\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u3001\u65e0\u76d1\u7763\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6846\u67b6\u5177\u6709\u9ad8\u89c6\u89c9\u8d28\u91cf\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u975e\u5e38\u9002\u5408\u5e94\u7528\u4e8e\u79fb\u52a8\u6444\u5f71\u3001\u76d1\u63a7\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2506.18330", "pdf": "https://arxiv.org/pdf/2506.18330", "abs": "https://arxiv.org/abs/2506.18330", "authors": ["Lixin Wu", "Na Cai", "Qiao Cheng", "Jiachen Wang", "Yitao Duan"], "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.", "AI": {"tldr": "Confucius3-Math\u662f\u4e00\u6b3e\u5f00\u6e90\u768414B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u4e2d\u56fdK-12\u6570\u5b66\u5b66\u4e60\u8bbe\u8ba1\uff0c\u80fd\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e09\u9879\u6280\u672f\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528AI\u6280\u672f\u63d0\u5347\u6559\u80b2\u8d28\u91cf\uff0c\u7279\u522b\u662f\u4e3a\u4e2d\u56fdK-12\u5b66\u751f\u548c\u6559\u5e08\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u6570\u5b66\u5b66\u4e60\u5de5\u5177\u3002\u901a\u8fc7\u5f00\u53d1\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u89e3\u51b3\u4e3b\u6d41\u6570\u5b66\u95ee\u9898\uff0c\u5e76\u63a8\u52a8\u77e5\u8bc6\u4f20\u64ad\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e09\u9879\u6280\u672f\u521b\u65b0\uff1a\u76ee\u6807\u71b5\u6b63\u5219\u5316\u3001\u8fd1\u671f\u6837\u672c\u6062\u590d\u548c\u7b56\u7565\u7279\u5b9a\u96be\u5ea6\u52a0\u6743\u3002\u8fd9\u4e9b\u6280\u672f\u6539\u8fdb\u4e86\u71b5\u6b63\u5219\u5316\u3001\u6570\u636e\u8c03\u5ea6\u7b56\u7565\u548c\u4f18\u52bf\u4f30\u8ba1\u5668\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "result": "Confucius3-Math\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4f18\u4e8e\u8bb8\u591a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u4e14\u80fd\u5728\u5355\u5757\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6280\u672f\u521b\u65b0\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982K-12\u6570\u5b66\uff09\u4ee5\u4f4e\u6210\u672c\u6784\u5efa\u9ad8\u6027\u80fd\u63a8\u7406\u6a21\u578b\u3002\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "paper_title_zh": "Confucius3-Math\uff1a\u4e00\u6b3e\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u4e2d\u56fdK-12\u6570\u5b66\u5b66\u4e60\u8bbe\u8ba1", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86Confucius3-Math\uff0c\u8fd9\u662f\u4e00\u6b3e\u5f00\u6e90\u768414B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a\uff081\uff09\u80fd\u5728\u5355\u5757\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\uff1b\uff082\uff09\u5728\u591a\u9879\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u89c4\u6a21\u66f4\u5927\u7684\u6a21\u578b\u3002\u4f5c\u4e3a\u5229\u7528AI\u63d0\u5347\u6559\u80b2\u548c\u77e5\u8bc6\u4f20\u64ad\u7684\u4f7f\u547d\u7684\u4e00\u90e8\u5206\uff0cConfucius3-Math\u4e13\u6ce8\u4e8e\u4e3a\u4e2d\u56fdK-12\u5b66\u751f\u548c\u6559\u5e08\u63d0\u4f9b\u6570\u5b66\u5b66\u4e60\u652f\u6301\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u4e0e\u56fd\u5bb6\u8bfe\u7a0b\u5bf9\u9f50\uff0c\u5e76\u80fd\u4f4e\u6210\u672c\u89e3\u51b3\u4e3b\u6d41K-12\u6570\u5b66\u95ee\u9898\u3002\u672c\u6587\u5206\u4eab\u4e86\u5f00\u53d1\u65b9\u6cd5\u3001\u9047\u5230\u7684\u6311\u6218\u53ca\u89e3\u51b3\u6280\u672f\uff0c\u7279\u522b\u662f\u4e09\u9879\u6280\u672f\u521b\u65b0\uff1a\u76ee\u6807\u71b5\u6b63\u5219\u5316\u3001\u8fd1\u671f\u6837\u672c\u6062\u590d\u548c\u7b56\u7565\u7279\u5b9a\u96be\u5ea6\u52a0\u6743\u3002\u8fd9\u4e9b\u521b\u65b0\u5305\u62ec\u65b0\u7684\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u3001\u6570\u636e\u8c03\u5ea6\u7b56\u7565\u548c\u6539\u8fdb\u7684\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u5668\uff0c\u663e\u8457\u7a33\u5b9a\u4e86RL\u8bad\u7ec3\u3001\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u9886\u57df\u4ee5\u4f4e\u6210\u672c\u6784\u5efa\u9ad8\u6027\u80fd\u63a8\u7406\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u5728https://github.com/netease-youdao/Confucius3-Math\u5f00\u6e90\u3002"}}
{"id": "2506.18325", "pdf": "https://arxiv.org/pdf/2506.18325", "abs": "https://arxiv.org/abs/2506.18325", "authors": ["Yu Xie", "Chengjie Zeng", "Lingyun Zhang", "Yanwei Fu"], "title": "NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of text-to-image (T2I) models, such as Stable\nDiffusion, has enhanced their capability to synthesize images from textual\nprompts. However, this progress also raises significant risks of misuse,\nincluding the generation of harmful content (e.g., pornography, violence,\ndiscrimination), which contradicts the ethical goals of T2I technology and\nhinders its sustainable development. Inspired by \"jailbreak\" attacks in large\nlanguage models, which bypass restrictions through subtle prompt modifications,\nthis paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a\nnovel approach to detoxify harmful prompts without altering model architecture\nor degrading generation capability. PromptSan includes two variants:\nPromptSan-Modify, which iteratively identifies and replaces harmful tokens in\ninput prompts using text NSFW classifiers during inference, and\nPromptSan-Suffix, which trains an optimized suffix token sequence to neutralize\nharmful intent while passing both text and image NSFW classifier checks.\nExtensive experiments demonstrate that PromptSan achieves state-of-the-art\nperformance in reducing harmful content generation across multiple metrics,\neffectively balancing safety and usability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptSan\u65b9\u6cd5\uff0c\u901a\u8fc7NSFW\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u63d0\u793a\u51c0\u5316\u6280\u672f\uff0c\u786e\u4fdd\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5b89\u5168\u6027\uff0c\u907f\u514d\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u751f\u6210\u80fd\u529b\u63d0\u5347\u7684\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u6ee5\u7528\u98ce\u9669\uff0c\u5982\u751f\u6210\u8272\u60c5\u3001\u66b4\u529b\u7b49\u6709\u5bb3\u5185\u5bb9\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u786e\u4fddT2I\u6280\u672f\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "method": "\u63d0\u51faPromptSan\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1aPromptSan-Modify\u901a\u8fc7\u8fed\u4ee3\u66ff\u6362\u6709\u5bb3\u6807\u8bb0\u51c0\u5316\u8f93\u5165\u63d0\u793a\uff1bPromptSan-Suffix\u8bad\u7ec3\u4f18\u5316\u7684\u540e\u7f00\u6807\u8bb0\u5e8f\u5217\u4e2d\u548c\u6709\u5bb3\u610f\u56fe\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cfNSFW\u5206\u7c7b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPromptSan\u5728\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5e73\u8861\u4e86\u5b89\u5168\u6027\u4e0e\u53ef\u7528\u6027\u3002", "conclusion": "PromptSan\u4e3aT2I\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u751f\u6210\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "paper_title_zh": "\u57fa\u4e8eNSFW\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u63d0\u793a\u51c0\u5316\u6280\u672f\u5b9e\u73b0\u5b89\u5168\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210", "abstract_zh": "\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u7684\u5feb\u901f\u53d1\u5c55\u63d0\u5347\u4e86\u5176\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u6ee5\u7528\u98ce\u9669\uff0c\u5982\u751f\u6210\u8272\u60c5\u3001\u66b4\u529b\u6216\u6b67\u89c6\u6027\u5185\u5bb9\uff0c\u8fd9\u4e0eT2I\u6280\u672f\u7684\u4f26\u7406\u76ee\u6807\u76f8\u6096\uff0c\u963b\u788d\u4e86\u5176\u53ef\u6301\u7eed\u53d1\u5c55\u3002\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u201c\u8d8a\u72f1\u201d\u653b\u51fb\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51faNSFW\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u63d0\u793a\u51c0\u5316\u6280\u672f\uff08PromptSan\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u964d\u4f4e\u751f\u6210\u80fd\u529b\u5373\u53ef\u51c0\u5316\u6709\u5bb3\u63d0\u793a\u7684\u65b0\u65b9\u6cd5\u3002PromptSan\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1aPromptSan-Modify\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u6587\u672cNSFW\u5206\u7c7b\u5668\u8fed\u4ee3\u8bc6\u522b\u5e76\u66ff\u6362\u8f93\u5165\u63d0\u793a\u4e2d\u7684\u6709\u5bb3\u6807\u8bb0\uff1bPromptSan-Suffix\u8bad\u7ec3\u4f18\u5316\u7684\u540e\u7f00\u6807\u8bb0\u5e8f\u5217\u4ee5\u4e2d\u548c\u6709\u5bb3\u610f\u56fe\uff0c\u540c\u65f6\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cfNSFW\u5206\u7c7b\u5668\u68c0\u67e5\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPromptSan\u5728\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5b89\u5168\u6027\u4e0e\u53ef\u7528\u6027\u3002"}}
{"id": "2506.17368", "pdf": "https://arxiv.org/pdf/2506.17368", "abs": "https://arxiv.org/abs/2506.17368", "authors": ["Zhenglin Lai", "Mengyao Liao", "Dong Xu", "Zebin Zhao", "Zhihang Yuan", "Chao Fan", "Jianqiang Li", "Bingzhe Wu"], "title": "SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "9 pages, 7 figures", "summary": "Large language models based on Mixture-of-Experts have achieved substantial\ngains in efficiency and scalability, yet their architectural uniqueness\nintroduces underexplored safety alignment challenges. Existing safety alignment\nstrategies, predominantly designed for dense models, are ill-suited to address\nMoE-specific vulnerabilities. In this work, we formalize and systematically\nstudy MoE model's positional vulnerability - the phenomenon where\nsafety-aligned behaviors rely on specific expert modules, revealing critical\nrisks inherent to MoE architectures. To this end, we present SAFEx, an\nanalytical framework that robustly identifies, characterizes, and validates the\nsafety-critical experts using a novel Stability-based Expert Selection (SES)\nalgorithm. Notably, our approach enables the explicit decomposition of\nsafety-critical experts into distinct functional groups, including those\nresponsible for harmful content detection and those controlling safe response\ngeneration. Extensive experiments on mainstream MoE models, such as the\nrecently released Qwen3-MoE, demonstrated that their intrinsic safety\nmechanisms heavily rely on a small subset of positional experts. Disabling\nthese experts significantly compromised the models' ability to refuse harmful\nrequests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that\ndisabling as few as 12 identified safety-critical experts can cause the refusal\nrate to drop by 22%, demonstrating the disproportionate impact of a small set\nof experts on overall model safety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAFEx\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u5206\u6790\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u6a21\u5757\uff0c\u63ed\u793a\u5176\u4f4d\u7f6e\u8106\u5f31\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5c11\u6570\u4e13\u5bb6\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6548\u7387\u548c\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u72ec\u7279\u7ed3\u6784\u5e26\u6765\u4e86\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u5bf9\u9f50\u6311\u6218\u3002\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u5bc6\u96c6\u6a21\u578b\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3MoE\u7279\u6709\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faSAFEx\u5206\u6790\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u4e13\u5bb6\u9009\u62e9\uff08SES\uff09\u7b97\u6cd5\uff0c\u8bc6\u522b\u5e76\u9a8c\u8bc1\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u6a21\u5757\uff0c\u5e76\u5c06\u5176\u529f\u80fd\u5206\u89e3\u4e3a\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u548c\u5b89\u5168\u54cd\u5e94\u751f\u6210\u7b49\u4e0d\u540c\u7ec4\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u6d41MoE\u6a21\u578b\uff08\u5982Qwen3-MoE\uff09\u7684\u5b89\u5168\u673a\u5236\u9ad8\u5ea6\u4f9d\u8d56\u5c11\u6570\u4f4d\u7f6e\u4e13\u5bb6\u3002\u7981\u7528\u8fd9\u4e9b\u4e13\u5bb6\uff08\u5982Qwen3-MoE\u4e2d\u768412\u4e2a\u4e13\u5bb6\uff09\u4f1a\u5bfc\u81f4\u6a21\u578b\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u7684\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff08\u62d2\u7edd\u7387\u964d\u4f4e22%\uff09\u3002", "conclusion": "MoE\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u4f4d\u7f6e\u8106\u5f31\u6027\uff0c\u5c11\u6570\u4e13\u5bb6\u5bf9\u6574\u4f53\u5b89\u5168\u6027\u5177\u6709\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u9700\u9488\u5bf9MoE\u67b6\u6784\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u5b89\u5168\u7b56\u7565\u3002", "paper_title_zh": "SAFEx\uff1a\u901a\u8fc7\u7a33\u5b9a\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u8bc6\u522b\u5206\u6790\u57fa\u4e8eMoE\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8106\u5f31\u6027", "abstract_zh": "\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6548\u7387\u548c\u6269\u5c55\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u67b6\u6784\u72ec\u7279\u6027\u5f15\u5165\u4e86\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u5bf9\u9f50\u6311\u6218\u3002\u73b0\u6709\u7684\u5b89\u5168\u5bf9\u9f50\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u5bc6\u96c6\u6a21\u578b\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5e94\u5bf9MoE\u7279\u6709\u7684\u8106\u5f31\u6027\u3002\u672c\u6587\u5f62\u5f0f\u5316\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86MoE\u6a21\u578b\u7684\u4f4d\u7f6e\u8106\u5f31\u6027\u73b0\u8c61\uff0c\u5373\u5b89\u5168\u5bf9\u9f50\u884c\u4e3a\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e13\u5bb6\u6a21\u5757\uff0c\u63ed\u793a\u4e86MoE\u67b6\u6784\u7684\u56fa\u6709\u98ce\u9669\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SAFEx\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u4e13\u5bb6\u9009\u62e9\uff08SES\uff09\u7b97\u6cd5\uff0c\u7a33\u5065\u5730\u8bc6\u522b\u3001\u8868\u5f81\u548c\u9a8c\u8bc1\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5c06\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u660e\u786e\u5206\u89e3\u4e3a\u4e0d\u540c\u529f\u80fd\u7ec4\uff0c\u5305\u62ec\u8d1f\u8d23\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u548c\u63a7\u5236\u5b89\u5168\u54cd\u5e94\u751f\u6210\u7684\u4e13\u5bb6\u3002\u5728\u4e3b\u6d41MoE\u6a21\u578b\uff08\u5982\u6700\u65b0\u53d1\u5e03\u7684Qwen3-MoE\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5185\u5728\u5b89\u5168\u673a\u5236\u9ad8\u5ea6\u4f9d\u8d56\u5c11\u6570\u4f4d\u7f6e\u4e13\u5bb6\u3002\u7981\u7528\u8fd9\u4e9b\u4e13\u5bb6\u4f1a\u663e\u8457\u524a\u5f31\u6a21\u578b\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u7684\u80fd\u529b\u3002\u5bf9\u4e8e\u62e5\u67096144\u4e2a\u4e13\u5bb6\uff08\u5728FNN\u5c42\uff09\u7684Qwen3-MoE\uff0c\u6211\u4eec\u53d1\u73b0\u4ec5\u7981\u752812\u4e2a\u5df2\u8bc6\u522b\u7684\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u5373\u53ef\u5bfc\u81f4\u62d2\u7edd\u7387\u4e0b\u964d22%\uff0c\u8bc1\u660e\u4e86\u5c11\u6570\u4e13\u5bb6\u5bf9\u6a21\u578b\u6574\u4f53\u5b89\u5168\u6027\u7684\u4e0d\u6210\u6bd4\u4f8b\u5f71\u54cd\u3002"}}
{"id": "2506.18349", "pdf": "https://arxiv.org/pdf/2506.18349", "abs": "https://arxiv.org/abs/2506.18349", "authors": ["Zichong Li", "Chen Liang", "Zixuan Zhang", "Ilgee Hong", "Young Jin Kim", "Weizhu Chen", "Tuo Zhao"], "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .", "AI": {"tldr": "SlimMoE\u662f\u4e00\u79cd\u591a\u9636\u6bb5\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u7626\u8eab\u548c\u84b8\u998f\u6280\u672f\u5c06\u5927\u578bMoE\u6a21\u578b\u538b\u7f29\u4e3a\u66f4\u5c0f\u3001\u9ad8\u6548\u7684\u53d8\u4f53\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u3002", "motivation": "\u5927\u578bMoE\u6a21\u578b\u867d\u7136\u80fd\u9ad8\u6548\u6269\u5c55\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5185\u5b58\u9700\u6c42\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u96be\u4ee5\u5fae\u8c03\u6216\u90e8\u7f72\u3002SlimMoE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "SlimMoE\u901a\u8fc7\u591a\u9636\u6bb5\u538b\u7f29\u6846\u67b6\uff0c\u9010\u6b65\u51cf\u5c11\u4e13\u5bb6\u53c2\u6570\u6570\u91cf\uff0c\u5e76\u901a\u8fc7\u4e2d\u95f4\u9636\u6bb5\u77e5\u8bc6\u8f6c\u79fb\uff0c\u907f\u514d\u4e00\u6b21\u6027\u526a\u679d\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\uff08\u5982400B tokens\uff09\u3002", "result": "\u538b\u7f29\u540e\u7684\u6a21\u578b\uff08\u5982Phi-mini-MoE\u548cPhi-tiny-MoE\uff09\u5728\u5355GPU\u4e0a\u5373\u53ef\u5fae\u8c03\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u5c0f\u6a21\u578b\uff0c\u751a\u81f3\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u7684\u8868\u73b0\u3002\u4f8b\u5982\uff0cPhi-mini-MoE\u5728MMLU\u5206\u6570\u4e0a\u4e0eLlama 3.1 8B\u76f8\u5f53\uff0c\u4f46\u5ef6\u8fdf\u66f4\u4f4e\u3002", "conclusion": "SlimMoE\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u526a\u679d\u4e0e\u5206\u9636\u6bb5\u84b8\u998f\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf\u3001\u7d27\u51d1\u7684MoE\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u63a8\u52a8\u4e86MoE\u67b6\u6784\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "paper_title_zh": "SlimMoE\uff1a\u901a\u8fc7\u4e13\u5bb6\u7626\u8eab\u4e0e\u84b8\u998f\u5b9e\u73b0\u5927\u578bMoE\u6a21\u578b\u7684\u7ed3\u6784\u5316\u538b\u7f29", "abstract_zh": "\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5df2\u6210\u4e3a\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u5f3a\u5927\u8303\u5f0f\u3002\u7136\u800c\uff0c\u5176\u5de8\u5927\u7684\u5185\u5b58\u9700\u6c42\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u96be\u4ee5\u5fae\u8c03\u6216\u90e8\u7f72\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SlimMoE\uff0c\u4e00\u79cd\u591a\u9636\u6bb5\u538b\u7f29\u6846\u67b6\uff0c\u53ef\u5c06\u5927\u578bMoE\u6a21\u578b\u8f6c\u6362\u4e3a\u66f4\u5c0f\u3001\u9ad8\u6548\u7684\u53d8\u4f53\uff0c\u800c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u7684\u9ad8\u6602\u6210\u672c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7626\u8eab\u4e13\u5bb6\u548c\u5206\u9636\u6bb5\u77e5\u8bc6\u8f6c\u79fb\uff0c\u7cfb\u7edf\u6027\u5730\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4e00\u5200\u5207\u526a\u679d\u65b9\u6cd5\u5e38\u89c1\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u5229\u7528\u8be5\u6846\u67b6\uff0c\u6211\u4eec\u4ec5\u4f7f\u7528400B tokens\uff08\u4e0d\u5230\u539f\u59cb\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u768410%\uff09\u5c06Phi 3.5-MoE\uff0841.9B\u603b\u53c2\u6570/6.6B\u6fc0\u6d3b\u53c2\u6570\uff09\u538b\u7f29\u4e3aPhi-mini-MoE\uff087.6B\u603b\u53c2\u6570/2.4B\u6fc0\u6d3b\u53c2\u6570\uff09\u548cPhi-tiny-MoE\uff083.8B\u603b\u53c2\u6570/1.1B\u6fc0\u6d3b\u53c2\u6570\uff09\u3002\u8fd9\u4e9b\u538b\u7f29\u6a21\u578b\u53ef\u5728\u5355GPU\uff08A100\u7528\u4e8ePhi-mini-MoE\uff0cA6000\u7528\u4e8ePhi-tiny-MoE\uff09\u4e0a\u5fae\u8c03\uff0c\u975e\u5e38\u9002\u5408\u5b66\u672f\u548c\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u538b\u7f29\u6a21\u578b\u7684\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u5c0f\u6a21\u578b\uff0c\u5e76\u4e0e\u66f4\u5927\u6a21\u578b\u7ade\u4e89\u3002\u4f8b\u5982\uff0cPhi-mini-MoE\u4ec5\u4f7f\u75282/3\u7684\u6fc0\u6d3b\u53c2\u6570\u5373\u8fbe\u5230\u6216\u8d85\u8fc7Phi-3-mini\u7684\u6027\u80fd\uff0c\u4e14\u5728MMLU\u5206\u6570\u4e0a\u4e0eLlama 3.1 8B\u76f8\u5f53\uff0c\u540c\u65f6\u5ef6\u8fdf\u663e\u8457\u66f4\u4f4e\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u6784\u5316\u526a\u679d\u4e0e\u5206\u9636\u6bb5\u84b8\u998f\u76f8\u7ed3\u5408\uff0c\u4e3a\u521b\u5efa\u9ad8\u8d28\u91cf\u3001\u7d27\u51d1\u7684MoE\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63a8\u52a8\u4e86MoE\u67b6\u6784\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u6a21\u578b\u5df2\u516c\u5f00\u4e8ehttps://huggingface.co/microsoft/Phi-mini-MoE-instruct\u548chttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct\u3002"}}
{"id": "2506.18331", "pdf": "https://arxiv.org/pdf/2506.18331", "abs": "https://arxiv.org/abs/2506.18331", "authors": ["AmirHossein Zamani", "Tianhao Xie", "Amir G. Aghdam", "Tiberiu Popa", "Eugene Belilovsky"], "title": "Geometry-Aware Preference Learning for 3D Texture Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D generative models have achieved impressive results but\n3D contents generated by these models may not align with subjective human\npreferences or task-specific criteria. Moreover, a core challenge in the 3D\ntexture generation domain remains: most existing approaches rely on repeated\ncalls to 2D text-to-image generative models, which lack an inherent\nunderstanding of the 3D structure of the input 3D mesh object. To address this,\nwe propose an end-to-end differentiable preference learning framework that\nback-propagates human preferences, represented by differentiable reward\nfunctions, through the entire 3D generative pipeline, making the process\ninherently geometry-aware. We demonstrate the effectiveness of our framework\nusing four proposed novel geometry-aware reward functions, offering a more\ncontrollable and interpretable pathway for high-quality 3D content creation\nfrom natural language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\u5c06\u4eba\u7c7b\u504f\u597d\u53cd\u5411\u4f20\u64ad\u81f33D\u751f\u6210\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u7eb9\u7406\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf93D\u7ed3\u6784\u7406\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u751f\u6210\u6a21\u578b\u867d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u76843D\u5185\u5bb9\u5e38\u4e0e\u4eba\u7c7b\u4e3b\u89c2\u504f\u597d\u6216\u4efb\u52a1\u7279\u5b9a\u6807\u51c6\u4e0d\u7b26\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u7f3a\u4e4f\u5bf93D\u7ed3\u6784\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\u5c06\u4eba\u7c7b\u504f\u597d\u53cd\u5411\u4f20\u64ad\u81f3\u6574\u4e2a3D\u751f\u6210\u6d41\u7a0b\uff0c\u4f7f\u5176\u5177\u5907\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u56db\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u5956\u52b1\u51fd\u6570\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf3D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u548c\u53ef\u89e3\u91ca\u7684\u9014\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a3D\u7eb9\u7406\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u51e0\u4f55\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "paper_title_zh": "\u51e0\u4f55\u611f\u77e5\u7684\u504f\u597d\u5b66\u4e60\u7528\u4e8e3D\u7eb9\u7406\u751f\u6210", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c3D\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u751f\u6210\u76843D\u5185\u5bb9\u53ef\u80fd\u4e0d\u7b26\u5408\u4eba\u7c7b\u4e3b\u89c2\u504f\u597d\u6216\u4efb\u52a1\u7279\u5b9a\u6807\u51c6\u3002\u6b64\u5916\uff0c3D\u7eb9\u7406\u751f\u6210\u9886\u57df\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u662f\uff1a\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf92D\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u91cd\u590d\u8c03\u7528\uff0c\u8fd9\u4e9b\u6a21\u578b\u7f3a\u4e4f\u5bf9\u8f93\u51653D\u7f51\u683c\u5bf9\u8c61\u7ed3\u6784\u7684\u56fa\u6709\u7406\u89e3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5956\u52b1\u51fd\u6570\u5c06\u4eba\u7c7b\u504f\u597d\u53cd\u5411\u4f20\u64ad\u81f3\u6574\u4e2a3D\u751f\u6210\u6d41\u7a0b\uff0c\u4f7f\u5176\u5177\u5907\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u3002\u6211\u4eec\u901a\u8fc7\u56db\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u5956\u52b1\u51fd\u6570\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u9ad8\u8d28\u91cf3D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u548c\u53ef\u89e3\u91ca\u7684\u9014\u5f84\u3002"}}
{"id": "2506.17369", "pdf": "https://arxiv.org/pdf/2506.17369", "abs": "https://arxiv.org/abs/2506.17369", "authors": ["Zhiyuan Pan", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u793a\u654f\u611f\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u5fae\u5c0f\u7684\u63d0\u793a\u53d8\u5316\u4e5f\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u6ce2\u52a8\uff0c\u5e76\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6392\u540d\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u63d0\u793a\u6a21\u677f\uff0c\u5bb9\u6613\u56e0\u63d0\u793a\u654f\u611f\u6027\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u4e0d\u53ef\u9760\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u4ee3\u7801\u4efb\u52a1\u4e2d\u63d0\u793a\u654f\u611f\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u751f\u6210\u8bed\u4e49\u548c\u7ed3\u6784\u76f8\u4f3c\u7684\u63d0\u793a\u6a21\u677f\uff0c\u5e76\u57288\u4e2a\u4ee3\u7801\u57fa\u51c6\u4efb\u52a1\u548c10\u4e2a\u4ee3\u8868\u6027\u5f00\u6e90LLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u6027\u80fd\u53d8\u5316\u548c\u6a21\u578b\u6392\u540d\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u5c0f\u7684\u63d0\u793a\u53d8\u5316\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u6ce2\u52a8\uff0c\u5e76\u53ef\u80fd\u6539\u53d8\u6a21\u578b\u6392\u540d\uff0c\u51f8\u663e\u4e86\u63d0\u793a\u654f\u611f\u6027\u5bf9\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u672a\u6765\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\u9700\u8003\u8651\u63d0\u793a\u654f\u611f\u6027\uff0c\u4ee5\u786e\u4fdd\u5bf9LLM\u80fd\u529b\u7684\u8bc4\u4f30\u66f4\u53ef\u9760\u548c\u51c6\u786e\u3002", "paper_title_zh": "\u5728\u8bed\u4e49\u53d8\u5f02\u4e0b\u91cd\u65b0\u8bc4\u4f30\u4ee3\u7801LLM\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u4ee3\uff0c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u5df2\u6210\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91cd\u8981\u7814\u7a76\u9886\u57df\uff0c\u5e76\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5b9e\u8df5\u3002\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLM\u5728\u7279\u5b9a\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\uff09\u4e0a\u7684\u6027\u80fd\u3002\u6784\u5efa\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u7684\u5173\u952e\u6b65\u9aa4\u662f\u63d0\u793a\u8bbe\u8ba1\u3002\u7136\u800c\uff0c\u73b0\u6709\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4f9d\u8d56\u6bcf\u4e2a\u4efb\u52a1\u7684\u5355\u4e00\u63d0\u793a\u6a21\u677f\uff0c\u5bb9\u6613\u56e0\u63d0\u793a\u654f\u611f\u6027\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u4e0d\u53ef\u9760\uff0c\u5373\u5fae\u5c0f\u7684\u63d0\u793a\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u6ce2\u52a8\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u63d0\u793a\u654f\u611f\u6027\uff0c\u4f46\u5176\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u53d1\u73b0\u4ec5\u9650\u4e8e\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63a2\u7a76\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u63d0\u793a\u654f\u611f\u6027\u3002\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5c3d\u53ef\u80fd\u4fdd\u7559\u8bed\u4e49\u548c\u7ed3\u6784\u5730\u4fee\u6539\u63d0\u793a\u6a21\u677f\u3002\u57fa\u4e8e\u8be5\u6846\u67b6\uff0c\u6211\u4eec\u57288\u4e2a\u4ee3\u7801\u57fa\u51c6\u4efb\u52a1\u548c10\u4e2a\u4ee3\u8868\u6027\u5f00\u6e90LLM\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5305\u542b100\u4e2a\u8bed\u4e49\u76f8\u4f3c\u7684\u63d0\u793a\u6a21\u677f\u3002\u968f\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u79cd\u7edf\u8ba1\u6307\u6807\u5206\u6790\u8bc4\u4f30\u7ed3\u679c\uff0c\u91cd\u70b9\u5173\u6ce8\u7edd\u5bf9\u548c\u76f8\u5bf9\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5fae\u5c0f\u7684\u63d0\u793a\u53d8\u5316\u4e5f\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u6ce2\u52a8\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u6027\u80fd\u6392\u540d\u4e0d\u4e00\u81f4\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u672a\u6765\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u65f6\u9700\u8003\u8651\u63d0\u793a\u654f\u611f\u6027\uff0c\u4ee5\u786e\u4fdd\u5bf9LLM\u80fd\u529b\u7684\u8bc4\u4f30\u66f4\u53ef\u9760\u548c\u51c6\u786e\u3002"}}
{"id": "2506.18488", "pdf": "https://arxiv.org/pdf/2506.18488", "abs": "https://arxiv.org/abs/2506.18488", "authors": ["Markus Frohmann", "Elena V. Epure", "Gabriel Meseguer-Brocal", "Markus Schedl", "Romain Hennequin"], "title": "AI-Generated Song Detection via Lyrics Transcripts", "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "Accepted to ISMIR 2025", "summary": "The recent rise in capabilities of AI-based music generation tools has\ncreated an upheaval in the music industry, necessitating the creation of\naccurate methods to detect such AI-generated content. This can be done using\naudio-based detectors; however, it has been shown that they struggle to\ngeneralize to unseen generators or when the audio is perturbed. Furthermore,\nrecent work used accurate and cleanly formatted lyrics sourced from a lyrics\nprovider database to detect AI-generated music. However, in practice, such\nperfect lyrics are not available (only the audio is); this leaves a substantial\ngap in applicability in real-life use cases. In this work, we instead propose\nsolving this gap by transcribing songs using general automatic speech\nrecognition (ASR) models. We do this using several detectors. The results on\ndiverse, multi-genre, and multi-lingual lyrics show generally strong detection\nperformance across languages and genres, particularly for our best-performing\nmodel using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that\nour method is more robust than state-of-the-art audio-based ones when the audio\nis perturbed in different ways and when evaluated on different music\ngenerators. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6b4c\u8bcd\u8f6c\u5f55\u68c0\u6d4bAI\u751f\u6210\u6b4c\u66f2\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u901a\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u548c\u591a\u79cd\u68c0\u6d4b\u5668\uff0c\u5728\u591a\u8bed\u8a00\u548c\u591a\u6d41\u6d3e\u6b4c\u8bcd\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u97f3\u9891\u88ab\u5e72\u6270\u65f6\u4f18\u4e8e\u73b0\u6709\u97f3\u9891\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u97f3\u4e50\u751f\u6210\u5de5\u5177\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u97f3\u4e50\u884c\u4e1a\u4e9f\u9700\u51c6\u786e\u68c0\u6d4bAI\u751f\u6210\u5185\u5bb9\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u97f3\u9891\u68c0\u6d4b\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u6297\u5e72\u6270\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4f9d\u8d56\u5b8c\u7f8e\u6b4c\u8bcd\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u4e2d\u4e0d\u53ef\u884c\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u901a\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u8f6c\u5f55\u6b4c\u66f2\u6b4c\u8bcd\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u68c0\u6d4b\u5668\uff08\u5982Whisper large-v2\u548cLLM2Vec\u5d4c\u5165\uff09\u8fdb\u884cAI\u751f\u6210\u6b4c\u66f2\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u8bed\u8a00\u3001\u591a\u6d41\u6d3e\u7684\u6b4c\u8bcd\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u97f3\u9891\u88ab\u5e72\u6270\u65f6\u4f18\u4e8e\u73b0\u6709\u97f3\u9891\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e14\u5bf9\u4e0d\u540c\u97f3\u4e50\u751f\u6210\u5668\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6b4c\u8bcd\u8f6c\u5f55\u548c\u591a\u79cd\u68c0\u6d4b\u5668\u7684\u7ed3\u5408\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u68c0\u6d4bAI\u751f\u6210\u6b4c\u66f2\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u586b\u8865\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u57fa\u4e8e\u6b4c\u8bcd\u8f6c\u5f55\u7684AI\u751f\u6210\u6b4c\u66f2\u68c0\u6d4b", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8eAI\u7684\u97f3\u4e50\u751f\u6210\u5de5\u5177\u80fd\u529b\u5927\u5e45\u63d0\u5347\uff0c\u5bf9\u97f3\u4e50\u884c\u4e1a\u4ea7\u751f\u4e86\u5de8\u5927\u51b2\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u51c6\u786e\u68c0\u6d4bAI\u751f\u6210\u5185\u5bb9\u7684\u65b9\u6cd5\u3002\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u97f3\u9891\u68c0\u6d4b\u5668\u5b9e\u73b0\uff0c\u4f46\u5b83\u4eec\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u751f\u6210\u5668\u6216\u97f3\u9891\u5e72\u6270\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5df2\u6709\u7814\u7a76\u5229\u7528\u6b4c\u8bcd\u63d0\u4f9b\u6570\u636e\u5e93\u4e2d\u7684\u7cbe\u786e\u6b4c\u8bcd\u68c0\u6d4bAI\u751f\u6210\u97f3\u4e50\uff0c\u4f46\u5b9e\u9645\u4e2d\u5b8c\u7f8e\u6b4c\u8bcd\u96be\u4ee5\u83b7\u53d6\uff08\u4ec5\u6709\u97f3\u9891\uff09\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7559\u4e0b\u4e86\u5de8\u5927\u7a7a\u767d\u3002\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u901a\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u8f6c\u5f55\u6b4c\u66f2\u6b4c\u8bcd\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u79cd\u68c0\u6d4b\u5668\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u8bed\u8a00\u3001\u591a\u6d41\u6d3e\u7684\u6b4c\u8bcd\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528Whisper large-v2\u548cLLM2Vec\u5d4c\u5165\u7684\u6700\u4f73\u6a21\u578b\u4e2d\u3002\u6b64\u5916\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u97f3\u9891\u88ab\u4e0d\u540c\u65b9\u5f0f\u5e72\u6270\u65f6\uff0c\u4ee5\u53ca\u5bf9\u4e0d\u540c\u97f3\u4e50\u751f\u6210\u5668\u7684\u68c0\u6d4b\u4e2d\uff0c\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u97f3\u9891\u68c0\u6d4b\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/deezer/robust-AI-lyrics-detection\u3002"}}
{"id": "2506.18335", "pdf": "https://arxiv.org/pdf/2506.18335", "abs": "https://arxiv.org/abs/2506.18335", "authors": ["Saad Wazir", "Daeyoung Kim"], "title": "Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention", "categories": ["cs.CV"], "comment": "Proceedings of the Computer Vision and Pattern Recognition Conference\n  (CVPR), 2025, pp. 30861-30871", "summary": "Segmenting biomarkers in medical images is crucial for various biotech\napplications. Despite advances, Transformer and CNN based methods often\nstruggle with variations in staining and morphology, limiting feature\nextraction. In medical image segmentation, where datasets often have limited\nsample availability, recent state-of-the-art (SOTA) methods achieve higher\naccuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to\nunderperform. This is due to challenges in effectively transferring rich\nmultiscale features from encoders to decoders, as well as limitations in\ndecoder efficiency. To address these issues, we propose an architecture that\ncaptures multi-scale local and global contextual information and a novel\ndecoder design, which effectively integrates features from the encoder,\nemphasizes important channels and regions, and reconstructs spatial dimensions\nto enhance segmentation accuracy. Our method, compatible with various encoders,\noutperforms SOTA methods, as demonstrated by experiments on four datasets and\nablation studies. Specifically, our method achieves absolute performance gains\nof 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on\nTNBC datasets compared to existing SOTA methods. Code:\nhttps://github.com/saadwazir/MCADS-Decoder", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6df1\u5ea6\u5230\u7a7a\u95f4\u6062\u590d\u548c\u6b8b\u5dee\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u6807\u5fd7\u7269\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u548cCNN\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5728\u67d3\u8272\u548c\u5f62\u6001\u53d8\u5316\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7aef\u5230\u7aef\u65b9\u6cd5\u56e0\u591a\u5c3a\u5ea6\u7279\u5f81\u4f20\u9012\u548c\u89e3\u7801\u5668\u6548\u7387\u95ee\u9898\u800c\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u6df1\u5ea6\u5230\u7a7a\u95f4\u6062\u590d\u548c\u6b8b\u5dee\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u6574\u5408\u7f16\u7801\u5668\u7279\u5f81\u5e76\u63d0\u5347\u7a7a\u95f4\u7ef4\u5ea6\u91cd\u5efa\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08MoNuSeg\u3001DSB\u3001\u7535\u5b50\u663e\u5fae\u955c\u548cTNBC\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u5206\u522b\u4e3a2.76%\u30013.12%\u30012.87%\u548c4.03%\u3002", "conclusion": "\u672c\u6587\u7684\u89e3\u7801\u5668\u8bbe\u8ba1\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5bb9\u591a\u79cd\u7f16\u7801\u5668\uff0c\u4e3a\u751f\u7269\u6807\u5fd7\u7269\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u91cd\u65b0\u601d\u8003\u89e3\u7801\u5668\u8bbe\u8ba1\uff1a\u5229\u7528\u6df1\u5ea6\u5230\u7a7a\u95f4\u6062\u590d\u548c\u6b8b\u5dee\u7ebf\u6027\u6ce8\u610f\u529b\u63d0\u5347\u751f\u7269\u6807\u5fd7\u7269\u5206\u5272", "abstract_zh": "\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u751f\u7269\u6807\u5fd7\u7269\u5206\u5272\u5bf9\u591a\u79cd\u751f\u7269\u6280\u672f\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u6709\u8fdb\u5c55\uff0c\u57fa\u4e8eTransformer\u548cCNN\u7684\u65b9\u6cd5\u5728\u67d3\u8272\u548c\u5f62\u6001\u53d8\u5316\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u9650\u5236\u4e86\u7279\u5f81\u63d0\u53d6\u3002\u5728\u6837\u672c\u6709\u9650\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709SOTA\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u800c\u7aef\u5230\u7aef\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u6e90\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u4ece\u7f16\u7801\u5668\u5230\u89e3\u7801\u5668\u7684\u4f20\u9012\u6548\u7387\u95ee\u9898\u4ee5\u53ca\u89e3\u7801\u5668\u8bbe\u8ba1\u4e0d\u8db3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\uff0c\u80fd\u591f\u6355\u83b7\u591a\u5c3a\u5ea6\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u89e3\u7801\u5668\uff0c\u6709\u6548\u6574\u5408\u7f16\u7801\u5668\u7279\u5f81\u3001\u7a81\u51fa\u91cd\u8981\u901a\u9053\u548c\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u7ef4\u5ea6\u91cd\u5efa\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u517c\u5bb9\u591a\u79cd\u7f16\u7801\u5668\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u6d88\u878d\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u5728MoNuSeg\u3001DSB\u3001\u7535\u5b50\u663e\u5fae\u955c\u548cTNBC\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e862.76%\u30013.12%\u30012.87%\u548c4.03%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u3002\u4ee3\u7801\uff1ahttps://github.com/saadwazir/MCADS-Decoder"}}
{"id": "2506.17370", "pdf": "https://arxiv.org/pdf/2506.17370", "abs": "https://arxiv.org/abs/2506.17370", "authors": ["Aditi Madhusudan Jain", "Ayush Jain"], "title": "AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As e-commerce rapidly integrates artificial intelligence for content creation\nand product recommendations, these technologies offer significant benefits in\npersonalization and efficiency. AI-driven systems automate product\ndescriptions, generate dynamic advertisements, and deliver tailored\nrecommendations based on consumer behavior, as seen in major platforms like\nAmazon and Shopify. However, the widespread use of AI in e-commerce raises\ncrucial ethical challenges, particularly around data privacy, algorithmic bias,\nand consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic\n-- can be inadvertently embedded in AI models, leading to inequitable product\nrecommendations and reinforcing harmful stereotypes. This paper examines the\nethical implications of AI-driven content creation and product recommendations,\nemphasizing the need for frameworks to ensure fairness, transparency, and need\nfor more established and robust ethical standards. We propose actionable best\npractices to remove bias and ensure inclusivity, such as conducting regular\naudits of algorithms, diversifying training data, and incorporating fairness\nmetrics into AI models. Additionally, we discuss frameworks for ethical\nconformance that focus on safeguarding consumer data privacy, promoting\ntransparency in decision-making processes, and enhancing consumer autonomy. By\naddressing these issues, we provide guidelines for responsibly utilizing AI in\ne-commerce applications for content creation and product recommendations,\nensuring that these technologies are both effective and ethically sound.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7535\u5b50\u5546\u52a1\u4e2d\u57fa\u4e8eAI\u7684\u5185\u5bb9\u521b\u4f5c\u548c\u4ea7\u54c1\u63a8\u8350\u7684\u4f26\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6d88\u9664\u504f\u89c1\u548c\u786e\u4fdd\u516c\u5e73\u7684\u5b9e\u8df5\u5efa\u8bae\u3002", "motivation": "\u968f\u7740AI\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u5185\u5bb9\u521b\u4f5c\u548c\u4ea7\u54c1\u63a8\u8350\u65b9\u9762\u7684\u4f18\u52bf\u663e\u8457\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u6570\u636e\u9690\u79c1\u3001\u7b97\u6cd5\u504f\u89c1\u548c\u6d88\u8d39\u8005\u81ea\u4e3b\u6743\u7b49\u4f26\u7406\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u5e94\u7528\u6848\u4f8b\uff08\u5982\u4e9a\u9a6c\u900a\u548cShopify\uff09\uff0c\u8bc6\u522b\u6f5c\u5728\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6d88\u9664\u504f\u89c1\u3001\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u7684\u5177\u4f53\u65b9\u6cd5\uff0c\u5982\u7b97\u6cd5\u5ba1\u8ba1\u3001\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u548c\u516c\u5e73\u6027\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u6a21\u578b\u53ef\u80fd\u5d4c\u5165\u6587\u5316\u3001\u6027\u522b\u6216\u793e\u4f1a\u7ecf\u6d4e\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u4ea7\u54c1\u63a8\u8350\u3002\u672c\u6587\u63d0\u51fa\u4e86\u6d88\u9664\u8fd9\u4e9b\u504f\u89c1\u7684\u5b9e\u8df5\u5efa\u8bae\uff0c\u5e76\u8ba8\u8bba\u4e86\u4fdd\u62a4\u6d88\u8d39\u8005\u9690\u79c1\u548c\u589e\u5f3a\u900f\u660e\u5ea6\u7684\u4f26\u7406\u6846\u67b6\u3002", "conclusion": "\u4e3a\u786e\u4fddAI\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u4f26\u7406\u4f7f\u7528\uff0c\u9700\u8981\u5efa\u7acb\u516c\u5e73\u3001\u900f\u660e\u7684\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u5316\u6570\u636e\u548c\u5b9a\u671f\u5ba1\u8ba1\u6d88\u9664\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u62a4\u6d88\u8d39\u8005\u9690\u79c1\u548c\u81ea\u4e3b\u6743\u3002", "paper_title_zh": "\u7535\u5b50\u5546\u52a1\u4e2d\u57fa\u4e8eAI\u7684\u5185\u5bb9\u521b\u4f5c\u4e0e\u4ea7\u54c1\u63a8\u8350\u5e94\u7528\u7684\u4f26\u7406\u6982\u8ff0", "abstract_zh": "\u968f\u7740\u7535\u5b50\u5546\u52a1\u8fc5\u901f\u6574\u5408\u4eba\u5de5\u667a\u80fd\u7528\u4e8e\u5185\u5bb9\u521b\u4f5c\u548c\u4ea7\u54c1\u63a8\u8350\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u4e2a\u6027\u5316\u548c\u6548\u7387\u65b9\u9762\u5e26\u6765\u4e86\u663e\u8457\u4f18\u52bf\u3002AI\u9a71\u52a8\u7684\u7cfb\u7edf\u81ea\u52a8\u5316\u751f\u6210\u4ea7\u54c1\u63cf\u8ff0\u3001\u52a8\u6001\u5e7f\u544a\uff0c\u5e76\u6839\u636e\u6d88\u8d39\u8005\u884c\u4e3a\u63d0\u4f9b\u5b9a\u5236\u63a8\u8350\uff0c\u5982\u4e9a\u9a6c\u900a\u548cShopify\u7b49\u4e3b\u8981\u5e73\u53f0\u6240\u793a\u3002\u7136\u800c\uff0cAI\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5173\u952e\u7684\u4f26\u7406\u6311\u6218\uff0c\u5c24\u5176\u662f\u56f4\u7ed5\u6570\u636e\u9690\u79c1\u3001\u7b97\u6cd5\u504f\u89c1\u548c\u6d88\u8d39\u8005\u81ea\u4e3b\u6743\u7684\u95ee\u9898\u3002\u504f\u89c1\u2014\u2014\u65e0\u8bba\u662f\u6587\u5316\u3001\u6027\u522b\u8fd8\u662f\u793e\u4f1a\u7ecf\u6d4e\u5c42\u9762\u7684\u2014\u2014\u53ef\u80fd\u65e0\u610f\u4e2d\u5d4c\u5165AI\u6a21\u578b\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u4ea7\u54c1\u63a8\u8350\u5e76\u5f3a\u5316\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\u3002\u672c\u6587\u63a2\u8ba8\u4e86AI\u9a71\u52a8\u7684\u5185\u5bb9\u521b\u4f5c\u548c\u4ea7\u54c1\u63a8\u8350\u7684\u4f26\u7406\u5f71\u54cd\uff0c\u5f3a\u8c03\u9700\u8981\u5efa\u7acb\u6846\u67b6\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u66f4\u5b8c\u5584\u7684\u4f26\u7406\u6807\u51c6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u6d88\u9664\u504f\u89c1\u548c\u786e\u4fdd\u5305\u5bb9\u6027\u7684\u53ef\u884c\u6700\u4f73\u5b9e\u8df5\uff0c\u5982\u5b9a\u671f\u5ba1\u8ba1\u7b97\u6cd5\u3001\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u5c06\u516c\u5e73\u6027\u6307\u6807\u7eb3\u5165AI\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u4ee5\u4fdd\u62a4\u6d88\u8d39\u8005\u6570\u636e\u9690\u79c1\u3001\u4fc3\u8fdb\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\u5ea6\u548c\u589e\u5f3a\u6d88\u8d39\u8005\u81ea\u4e3b\u6743\u4e3a\u6838\u5fc3\u7684\u4f26\u7406\u5408\u89c4\u6846\u67b6\u3002\u901a\u8fc7\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u4e3a\u7535\u5b50\u5546\u52a1\u4e2d\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528AI\u8fdb\u884c\u5185\u5bb9\u521b\u4f5c\u548c\u4ea7\u54c1\u63a8\u8350\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u786e\u4fdd\u8fd9\u4e9b\u6280\u672f\u65e2\u9ad8\u6548\u53c8\u7b26\u5408\u4f26\u7406\u3002"}}
{"id": "2506.18510", "pdf": "https://arxiv.org/pdf/2506.18510", "abs": "https://arxiv.org/abs/2506.18510", "authors": ["Duygu Altinok"], "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted to INTERSPEECH2025 workshop DISS2025", "summary": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u4e0d\u5b8c\u7f8e\u7684\u8bed\u97f3\u63d0\u793a\u8f6c\u5316\u4e3a\u5305\u542b\u4e0d\u6d41\u7545\u6807\u8bb0\u7684\u8f6c\u5f55\u672c\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86LLMs\u5728\u5904\u7406\u4e0d\u5b8c\u7f8e\u8f93\u5165\u65f6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u51c6\u786e\u68c0\u6d4b\u53e3\u8bed\u4e2d\u7684\u4e0d\u6d41\u7545\u73b0\u8c61\u5bf9\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u548c\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4e5f\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5177\u5305\u5bb9\u6027\u7684\u8bed\u97f3\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLMs\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\uff0c\u751f\u6210\u5305\u542b\u4e0d\u6d41\u7545\u6807\u8bb0\u548c\u65f6\u95f4\u6233\u7684\u5b8c\u6574\u8f6c\u5f55\u672c\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u97f3\u9891\u7f16\u7801\u5668\u63d0\u53d6\u7684\u58f0\u5b66\u8868\u5f81\u4e0e\u4e0d\u540c\u8d28\u91cf\u7684\u6587\u672c\u8f93\u5165\uff08\u5982\u65e0\u7455\u75b5\u7684\u8f6c\u5f55\u3001\u65f6\u95f4\u5bf9\u9f50\u7684\u8f6c\u5f55\u6216\u57fa\u4e8e\u97f3\u7d20\u7684ASR\u6a21\u578b\u8f93\u51fa\uff09\u7ed3\u5408\uff0c\u5229\u7528LLMs\u751f\u6210\u5305\u542b\u4e0d\u6d41\u7545\u6807\u8bb0\u548c\u65f6\u95f4\u6233\u7684\u8f6c\u5f55\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6587\u672c\u8f93\u5165\u4e0d\u5b8c\u7f8e\uff0c\u53ea\u8981\u5305\u542b\u65f6\u95f4\u6233\u76f8\u5173\u7ebf\u7d22\uff0cLLMs\u4ecd\u80fd\u6709\u6548\u5e73\u6ed1\u8f93\u5165\u5e76\u751f\u6210\u5b8c\u6574\u7684\u4e0d\u6d41\u7545\u6807\u8bb0\u8f6c\u5f55\u672c\uff0c\u8bc1\u660e\u4e86\u5176\u5904\u7406\u4e0d\u5b8c\u7f8e\u63d0\u793a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u5b8c\u7f8e\u7684\u8bed\u97f3\u63d0\u793a\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e0d\u6d41\u7545\u6807\u8bb0\u8f6c\u5f55\u672c\uff0c\u4e3a\u8bed\u97f3\u548c\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u6d41\u7545\u7684\u64cd\u4f5c\u8005\uff1aLLMs\u5c06\u4e0d\u5b8c\u7f8e\u63d0\u793a\u8f6c\u5316\u4e3a\u5bcc\u542b\u4e0d\u6d41\u7545\u6807\u8bb0\u7684\u8f6c\u5f55\u672c", "abstract_zh": "\u51c6\u786e\u68c0\u6d4b\u53e3\u8bed\u4e2d\u7684\u4e0d\u6d41\u7545\u73b0\u8c61\u5bf9\u4e8e\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u548c\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4e5f\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5177\u5305\u5bb9\u6027\u7684\u8bed\u97f3\u548c\u8bed\u8a00\u6280\u672f\u3002\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u80fd\u591f\u5904\u7406\u8bcd\u6c47\u548c\u975e\u8bcd\u6c47\u8f93\u5165\uff08\u5982\u97f3\u9891\u548c\u89c6\u9891\uff09\u7684\u591a\u529f\u80fd\u5b66\u4e60\u8005\u7684\u8d8b\u52bf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e0d\u6d41\u7545\u73b0\u8c61\u4f5c\u4e3a\u663e\u5f0f\u6807\u8bb0\u548c\u65f6\u95f4\u6233\u8f6c\u5f55\u7684\u65b0\u65b9\u6cd5\uff0c\u4ece\u800c\u751f\u6210\u5b8c\u5168\u6ce8\u91ca\u7684\u5bcc\u542b\u4e0d\u6d41\u7545\u6807\u8bb0\u7684\u8f6c\u5f55\u672c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4ece\u97f3\u9891\u7f16\u7801\u5668\u63d0\u53d6\u7684\u58f0\u5b66\u8868\u5f81\u4e0e\u4e0d\u540c\u8d28\u91cf\u7684\u6587\u672c\u8f93\u5165\uff08\u5982\u65e0\u7455\u75b5\u7684\u8f6c\u5f55\u3001\u65f6\u95f4\u5bf9\u9f50\u7684\u8f6c\u5f55\u6216\u57fa\u4e8e\u97f3\u7d20\u7684ASR\u6a21\u578b\u8f93\u51fa\uff09\u7ed3\u5408\uff0c\u8fd9\u4e9b\u8f93\u5165\u53ef\u80fd\u5305\u542b\u7455\u75b5\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6587\u672c\u8f93\u5165\u65e0\u9700\u5b8c\u7f8e\u65e0\u7f3a\u3002\u53ea\u8981\u5305\u542b\u65f6\u95f4\u6233\u76f8\u5173\u7ebf\u7d22\uff0cLLMs\u5c31\u80fd\u6709\u6548\u5e73\u6ed1\u8f93\u5165\u5e76\u751f\u6210\u5b8c\u6574\u7684\u4e0d\u6d41\u7545\u6807\u8bb0\u8f6c\u5f55\u672c\uff0c\u51f8\u663e\u4e86\u5176\u5728\u5904\u7406\u4e0d\u5b8c\u7f8e\u63d0\u793a\u65f6\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.18346", "pdf": "https://arxiv.org/pdf/2506.18346", "abs": "https://arxiv.org/abs/2506.18346", "authors": ["Tongshun Zhang", "Pingping Liu", "Mengen Cai", "Zijian Zhang", "Yubing Lu", "Qiuzhan Zhou"], "title": "BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Current low-light image enhancement (LLIE) methods face significant\nlimitations in simultaneously improving brightness while preserving semantic\nconsistency, fine details, and computational efficiency. With the emergence of\nstate-space models, particularly Mamba, image restoration has achieved\nremarkable performance, yet existing visual Mamba approaches flatten 2D images\ninto 1D token sequences using fixed scanning rules, critically limiting\ninteractions between distant tokens with causal relationships and constraining\ntheir ability to capture meaningful long-range dependencies. To address these\nfundamental limitations, we propose BSMamba, a novel visual Mamba architecture\ncomprising two specially designed components: Brightness Mamba and Semantic\nMamba. The Brightness Mamba revolutionizes token interaction patterns by\nprioritizing connections between distant tokens with similar brightness levels,\neffectively addressing the challenge of brightness restoration in LLIE tasks\nthrough brightness-guided selective attention. Complementing this, the Semantic\nMamba establishes priority interactions between tokens sharing similar semantic\nmeanings, allowing the model to maintain contextual consistency by connecting\nsemantically related regions across the image, thus preserving the hierarchical\nnature of image semantics during enhancement. By intelligently modeling tokens\nbased on brightness and semantic similarity rather than arbitrary scanning\npatterns, BSMamba transcends the constraints of conventional token sequencing\nwhile adhering to the principles of causal modeling. Extensive experiments\ndemonstrate that BSMamba achieves state-of-the-art performance in LLIE while\npreserving semantic consistency.", "AI": {"tldr": "BSMamba\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9Mamba\u67b6\u6784\uff0c\u901a\u8fc7\u4eae\u5ea6Mamba\u548c\u8bed\u4e49Mamba\u5206\u522b\u5efa\u6a21\u4eae\u5ea6\u76f8\u4f3c\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u4eae\u5ea6\u6062\u590d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728\u63d0\u5347\u4eae\u5ea6\u7684\u540c\u65f6\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7ec6\u8282\uff0c\u4e14\u73b0\u6709\u89c6\u89c9Mamba\u65b9\u6cd5\u56e0\u56fa\u5b9a\u626b\u63cf\u89c4\u5219\u9650\u5236\u4e86\u8fdc\u8ddd\u79bbtoken\u7684\u4ea4\u4e92\u80fd\u529b\u3002BSMamba\u65e8\u5728\u901a\u8fc7\u4eae\u5ea6\u4e0e\u8bed\u4e49\u5efa\u6a21\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BSMamba\u5305\u542b\u4eae\u5ea6Mamba\u548c\u8bed\u4e49Mamba\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u4eae\u5ea6Mamba\u901a\u8fc7\u4eae\u5ea6\u76f8\u4f3c\u6027\u4f18\u5148\u8fde\u63a5\u8fdc\u8ddd\u79bbtoken\uff0c\u8bed\u4e49Mamba\u5219\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002\u4e24\u8005\u7ed3\u5408\u7a81\u7834\u4e86\u4f20\u7edftoken\u5e8f\u5217\u7684\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBSMamba\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u4fdd\u6301\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "BSMamba\u901a\u8fc7\u4eae\u5ea6\u4e0e\u8bed\u4e49\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6027\u80fd\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e3a\u89c6\u89c9Mamba\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002", "paper_title_zh": "BSMamba\uff1a\u57fa\u4e8e\u4eae\u5ea6\u4e0e\u8bed\u4e49\u5efa\u6a21\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u8fdc\u8ddd\u79bb\u4ea4\u4e92\u65b9\u6cd5", "abstract_zh": "\u5f53\u524d\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u65b9\u6cd5\u5728\u63d0\u5347\u4eae\u5ea6\u7684\u540c\u65f6\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u548c\u8ba1\u7b97\u6548\u7387\u3002\u968f\u7740\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u7684\u51fa\u73b0\uff0c\u56fe\u50cf\u4fee\u590d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u89c6\u89c9Mamba\u65b9\u6cd5\u901a\u8fc7\u56fa\u5b9a\u626b\u63cf\u89c4\u5219\u5c062D\u56fe\u50cf\u5c55\u5e73\u4e3a1D token\u5e8f\u5217\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5177\u6709\u56e0\u679c\u5173\u7cfb\u7684\u8fdc\u8ddd\u79bbtoken\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u96be\u4ee5\u6355\u6349\u6709\u610f\u4e49\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6839\u672c\u6027\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86BSMamba\uff0c\u4e00\u79cd\u65b0\u578b\u89c6\u89c9Mamba\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u7ec4\u4ef6\uff1a\u4eae\u5ea6Mamba\u548c\u8bed\u4e49Mamba\u3002\u4eae\u5ea6Mamba\u901a\u8fc7\u4f18\u5148\u8fde\u63a5\u4eae\u5ea6\u76f8\u4f3c\u7684\u8fdc\u8ddd\u79bbtoken\uff0c\u5f7b\u5e95\u6539\u53d8\u4e86token\u4ea4\u4e92\u6a21\u5f0f\uff0c\u901a\u8fc7\u4eae\u5ea6\u5f15\u5bfc\u7684\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6709\u6548\u89e3\u51b3\u4e86LLIE\u4efb\u52a1\u4e2d\u7684\u4eae\u5ea6\u6062\u590d\u6311\u6218\u3002\u4e0e\u4e4b\u4e92\u8865\uff0c\u8bed\u4e49Mamba\u4f18\u5148\u8fde\u63a5\u8bed\u4e49\u76f8\u4f3c\u7684token\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u8fde\u63a5\u56fe\u50cf\u4e2d\u8bed\u4e49\u76f8\u5173\u7684\u533a\u57df\u6765\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5728\u589e\u5f3a\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u56fe\u50cf\u8bed\u4e49\u7684\u5c42\u6b21\u6027\u3002\u901a\u8fc7\u57fa\u4e8e\u4eae\u5ea6\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u800c\u975e\u4efb\u610f\u626b\u63cf\u6a21\u5f0f\u5bf9token\u8fdb\u884c\u667a\u80fd\u5efa\u6a21\uff0cBSMamba\u7a81\u7834\u4e86\u4f20\u7edftoken\u5e8f\u5217\u7684\u9650\u5236\uff0c\u540c\u65f6\u9075\u5faa\u56e0\u679c\u5efa\u6a21\u7684\u539f\u5219\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBSMamba\u5728LLIE\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.17372", "pdf": "https://arxiv.org/pdf/2506.17372", "abs": "https://arxiv.org/abs/2506.17372", "authors": ["Cedric Bernard", "Xavier Pleimling", "Amun Kharel", "Chase Vickery"], "title": "Multimodal Political Bias Identification and Neutralization", "categories": ["cs.CY", "cs.AI", "cs.CV"], "comment": null, "summary": "Due to the presence of political echo chambers, it becomes imperative to\ndetect and remove subjective bias and emotionally charged language from both\nthe text and images of political articles. However, prior work has focused on\nsolely the text portion of the bias rather than both the text and image\nportions. This is a problem because the images are just as powerful of a medium\nto communicate information as text is. To that end, we present a model that\nleverages both text and image bias which consists of four different steps.\nImage Text Alignment focuses on semantically aligning images based on their\nbias through CLIP models. Image Bias Scoring determines the appropriate bias\nscore of images via a ViT classifier. Text De-Biasing focuses on detecting\nbiased words and phrases and neutralizing them through BERT models. These three\nsteps all culminate to the final step of debiasing, which replaces the text and\nthe image with neutralized or reduced counterparts, which for images is done by\ncomparing the bias scores. The results so far indicate that this approach is\npromising, with the text debiasing strategy being able to identify many\npotential biased words and phrases, and the ViT model showcasing effective\ntraining. The semantic alignment model also is efficient. However, more time,\nparticularly in training, and resources are needed to obtain better results. A\nhuman evaluation portion was also proposed to ensure semantic consistency of\nthe newly generated text and images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u653f\u6cbb\u504f\u89c1\u8bc6\u522b\u4e0e\u4e2d\u548c\u6a21\u578b\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u5904\u7406\uff0c\u901a\u8fc7\u56db\u6b65\u65b9\u6cd5\u5b9e\u73b0\u504f\u89c1\u68c0\u6d4b\u4e0e\u4e2d\u548c\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a\u6548\u679c\u826f\u597d\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u653f\u6cbb\u56de\u97f3\u5ba4\u73b0\u8c61\u4f7f\u5f97\u4ece\u653f\u6cbb\u6587\u7ae0\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5e76\u6d88\u9664\u4e3b\u89c2\u504f\u89c1\u548c\u60c5\u7eea\u5316\u8bed\u8a00\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u5f80\u7814\u7a76\u4ec5\u5173\u6ce8\u6587\u672c\u504f\u89c1\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u4f5c\u4e3a\u4fe1\u606f\u4f20\u9012\u5a92\u4ecb\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6a21\u578b\u5206\u4e3a\u56db\u6b65\uff1a1) \u56fe\u50cf\u6587\u672c\u5bf9\u9f50\uff08\u901a\u8fc7CLIP\u6a21\u578b\u8bed\u4e49\u5bf9\u9f50\u56fe\u50cf\u504f\u89c1\uff09\uff1b2) \u56fe\u50cf\u504f\u89c1\u8bc4\u5206\uff08\u901a\u8fc7ViT\u5206\u7c7b\u5668\u8bc4\u5206\uff09\uff1b3) \u6587\u672c\u53bb\u504f\u89c1\uff08\u901a\u8fc7BERT\u6a21\u578b\u68c0\u6d4b\u5e76\u4e2d\u548c\u504f\u89c1\u8bcd\u53e5\uff09\uff1b4) \u6700\u7ec8\u53bb\u504f\u89c1\uff08\u66ff\u6362\u4e3a\u4e2d\u548c\u540e\u7684\u6587\u672c\u548c\u56fe\u50cf\uff09\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u6587\u672c\u53bb\u504f\u89c1\u7b56\u7565\u80fd\u6709\u6548\u8bc6\u522b\u6f5c\u5728\u504f\u89c1\u8bcd\u53e5\uff0cViT\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u826f\u597d\uff0c\u8bed\u4e49\u5bf9\u9f50\u6a21\u578b\u6548\u7387\u9ad8\uff0c\u4f46\u9700\u66f4\u591a\u65f6\u95f4\u548c\u8d44\u6e90\u4f18\u5316\u7ed3\u679c\u3002\u8fd8\u63d0\u51fa\u4eba\u5de5\u8bc4\u4f30\u4ee5\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u653f\u6cbb\u504f\u89c1\u8bc6\u522b\u4e0e\u4e2d\u548c\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bad\u7ec3\u548c\u8d44\u6e90\u6295\u5165\u4ee5\u63d0\u5347\u6548\u679c\u3002", "paper_title_zh": "\u591a\u6a21\u6001\u653f\u6cbb\u504f\u89c1\u8bc6\u522b\u4e0e\u4e2d\u548c", "abstract_zh": "\u7531\u4e8e\u653f\u6cbb\u56de\u97f3\u5ba4\u7684\u5b58\u5728\uff0c\u4ece\u653f\u6cbb\u6587\u7ae0\u7684\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5e76\u6d88\u9664\u4e3b\u89c2\u504f\u89c1\u548c\u60c5\u7eea\u5316\u8bed\u8a00\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7814\u7a76\u4ec5\u5173\u6ce8\u6587\u672c\u90e8\u5206\u7684\u504f\u89c1\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u4f5c\u4e3a\u4fe1\u606f\u4f20\u9012\u5a92\u4ecb\u7684\u540c\u7b49\u91cd\u8981\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u504f\u89c1\u7684\u6a21\u578b\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\uff08\u901a\u8fc7CLIP\u6a21\u578b\u8bed\u4e49\u5bf9\u9f50\u56fe\u50cf\u504f\u89c1\uff09\u3001\u56fe\u50cf\u504f\u89c1\u8bc4\u5206\uff08\u901a\u8fc7ViT\u5206\u7c7b\u5668\u8bc4\u5206\uff09\u3001\u6587\u672c\u53bb\u504f\u89c1\uff08\u901a\u8fc7BERT\u6a21\u578b\u68c0\u6d4b\u5e76\u4e2d\u548c\u504f\u89c1\u8bcd\u53e5\uff09\u4ee5\u53ca\u6700\u7ec8\u7684\u53bb\u504f\u89c1\u6b65\u9aa4\uff08\u66ff\u6362\u4e3a\u4e2d\u548c\u540e\u7684\u6587\u672c\u548c\u56fe\u50cf\uff0c\u56fe\u50cf\u90e8\u5206\u901a\u8fc7\u6bd4\u8f83\u504f\u89c1\u5206\u6570\u5b9e\u73b0\uff09\u3002\u521d\u6b65\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\uff0c\u6587\u672c\u53bb\u504f\u89c1\u7b56\u7565\u80fd\u8bc6\u522b\u5927\u91cf\u6f5c\u5728\u504f\u89c1\u8bcd\u53e5\uff0cViT\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u826f\u597d\uff0c\u8bed\u4e49\u5bf9\u9f50\u6a21\u578b\u6548\u7387\u9ad8\uff0c\u4f46\u4ecd\u9700\u66f4\u591a\u65f6\u95f4\u548c\u8d44\u6e90\u4ee5\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4eba\u5de5\u8bc4\u4f30\u4ee5\u786e\u4fdd\u751f\u6210\u6587\u672c\u548c\u56fe\u50cf\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.18364", "pdf": "https://arxiv.org/pdf/2506.18364", "abs": "https://arxiv.org/abs/2506.18364", "authors": ["Wenqing Zhao", "Guojia Xie", "Han Pan", "Biao Yang", "Weichuan Zhang"], "title": "Spatial frequency information fusion network for few-shot learning", "categories": ["cs.CV"], "comment": null, "summary": "The objective of Few-shot learning is to fully leverage the limited data\nresources for exploring the latent correlations within the data by applying\nalgorithms and training a model with outstanding performance that can\nadequately meet the demands of practical applications. In practical\napplications, the number of images in each category is usually less than that\nin traditional deep learning, which can lead to over-fitting and poor\ngeneralization performance. Currently, many Few-shot classification models pay\nmore attention to spatial domain information while neglecting frequency domain\ninformation, which contains more feature information. Ignoring frequency domain\ninformation will prevent the model from fully exploiting feature information,\nwhich would effect the classification performance. Based on conventional data\naugmentation, this paper proposes an SFIFNet with innovative data\npreprocessing. The key of this method is enhancing the accuracy of image\nfeature representation by integrating frequency domain information with spatial\ndomain information. The experimental results demonstrate the effectiveness of\nthis method in enhancing classification performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFIFNet\u7684\u7a7a\u95f4\u9891\u7387\u4fe1\u606f\u878d\u5408\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u9891\u57df\u548c\u7a7a\u95f4\u57df\u4fe1\u606f\u63d0\u5347\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u6bcf\u7c7b\u56fe\u50cf\u6570\u91cf\u6709\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u6613\u8fc7\u62df\u5408\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u73b0\u6709\u6a21\u578b\u591a\u5173\u6ce8\u7a7a\u95f4\u57df\u4fe1\u606f\u800c\u5ffd\u7565\u9891\u57df\u4fe1\u606f\uff0c\u5bfc\u81f4\u7279\u5f81\u5229\u7528\u4e0d\u5145\u5206\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u878d\u5408\u9891\u57df\u4fe1\u606f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u51faSFIFNet\u7f51\u7edc\uff0c\u521b\u65b0\u6027\u5730\u7ed3\u5408\u9891\u57df\u548c\u7a7a\u95f4\u57df\u4fe1\u606f\uff0c\u4f18\u5316\u56fe\u50cf\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "SFIFNet\u901a\u8fc7\u878d\u5408\u9891\u57df\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u5206\u7c7b\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u7a7a\u95f4\u9891\u7387\u4fe1\u606f\u878d\u5408\u7f51\u7edc", "abstract_zh": "\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u76ee\u6807\u662f\u901a\u8fc7\u7b97\u6cd5\u5145\u5206\u5229\u7528\u6709\u9650\u6570\u636e\u8d44\u6e90\uff0c\u6316\u6398\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u5173\u8054\uff0c\u5e76\u8bad\u7ec3\u51fa\u6027\u80fd\u4f18\u5f02\u7684\u6a21\u578b\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6bcf\u7c7b\u56fe\u50cf\u6570\u91cf\u901a\u5e38\u5c11\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\uff0c\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u6027\u80fd\u5dee\u3002\u76ee\u524d\uff0c\u8bb8\u591a\u5c0f\u6837\u672c\u5206\u7c7b\u6a21\u578b\u66f4\u5173\u6ce8\u7a7a\u95f4\u57df\u4fe1\u606f\u800c\u5ffd\u7565\u9891\u57df\u4fe1\u606f\uff0c\u540e\u8005\u5305\u542b\u66f4\u591a\u7279\u5f81\u4fe1\u606f\u3002\u5ffd\u7565\u9891\u57df\u4fe1\u606f\u4f1a\u963b\u788d\u6a21\u578b\u5145\u5206\u5229\u7528\u7279\u5f81\u4fe1\u606f\uff0c\u4ece\u800c\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u3002\u57fa\u4e8e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6570\u636e\u9884\u5904\u7406\u65b9\u6cd5SFIFNet\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u901a\u8fc7\u878d\u5408\u9891\u57df\u548c\u7a7a\u95f4\u57df\u4fe1\u606f\u63d0\u5347\u56fe\u50cf\u7279\u5f81\u8868\u793a\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18598", "pdf": "https://arxiv.org/pdf/2506.18598", "abs": "https://arxiv.org/abs/2506.18598", "authors": ["Aviral Gupta", "Armaan Sethi", "Ameesh Sethi"], "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u591a\u6570\u4e0e\u5c11\u6570\u7fa4\u4f53\u6fc0\u6d3b\u5747\u503c\u7684\u5dee\u5f02\u5b9a\u4e49\u201c\u504f\u5dee\u5411\u91cf\u201d\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4ece\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u4e2d\u51cf\u53bb\u8be5\u5411\u91cf\uff0c\u4ee5\u51cf\u5c11\u5206\u7c7b\u504f\u5dee\u5e76\u63d0\u5347\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7fa4\u4f53\u5206\u5e03\u4e0d\u5747\u65f6\uff0c\u5bb9\u6613\u7ee7\u627f\u7c7b\u522b\u504f\u5dee\u5e76\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5728\u975e\u5178\u578b\u7fa4\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u884c\u4e3a\u7f16\u8f91\u7684\u542f\u53d1\uff0c\u8ba1\u7b97\u591a\u6570\u4e0e\u5c11\u6570\u7fa4\u4f53\u6fc0\u6d3b\u5747\u503c\u7684\u5dee\u5f02\uff0c\u5b9a\u4e49\u201c\u504f\u5dee\u5411\u91cf\u201d\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4ece\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u4e2d\u51cf\u53bb\u8be5\u5411\u91cf\uff0c\u4ee5\u7ea0\u6b63\u5206\u7c7b\u504f\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5206\u7c7b\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u6700\u5dee\u7fa4\u4f53\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5728\u7c7b\u4f3cTransformer\u7684\u5206\u7c7b\u5668\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u6781\u4f4e\u6210\u672c\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u4ec5\u9700\u63a8\u7406\u65f6\u64cd\u4f5c\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u5206\u7c7b\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u504f\u5dee\u6821\u6b63\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u65e0\u9700\u8bad\u7ec3\u8f6e\uff1a\u63a8\u7406\u65f6\u7528\u4e8e\u504f\u5dee\u6821\u6b63\u7684\u5bfc\u5411\u5411\u91cf", "abstract_zh": "\u5728\u7fa4\u4f53\u5206\u5e03\u4e0d\u5747\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u901a\u5e38\u4f1a\u7ee7\u627f\u7c7b\u522b\u504f\u5dee\u5e76\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\u3002\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5728\u5e73\u5747\u8868\u73b0\u4e0a\u826f\u597d\uff0c\u4f46\u5728\u975e\u5178\u578b\u7fa4\u4f53\u4e0a\u6301\u7eed\u5931\u8d25\u3002\u4f8b\u5982\uff0c\u5728\u5934\u53d1\u989c\u8272\u5206\u7c7b\u4e2d\uff0c\u6570\u636e\u96c6\u53ef\u80fd\u8fc7\u5ea6\u4ee3\u8868\u91d1\u53d1\u5973\u6027\uff0c\u5f3a\u5316\u4e86\u523b\u677f\u5370\u8c61\u3002\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u7b97\u6cd5\u548c\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u88ab\u63d0\u51fa\u4ee5\u89e3\u51b3\u6b64\u7c7b\u504f\u5dee\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u7528\u4e8e\u7f16\u8f91\u5927\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u5bfc\u5411\u5411\u91cf\u3002\u6211\u4eec\u8ba1\u7b97\u591a\u6570\u4e0e\u5c11\u6570\u7fa4\u4f53\u6fc0\u6d3b\u5747\u503c\u7684\u5dee\u5f02\uff0c\u5b9a\u4e49\u201c\u504f\u5dee\u5411\u91cf\u201d\uff0c\u5e76\u5c06\u5176\u4ece\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u4e2d\u51cf\u53bb\u3002\u8fd9\u51cf\u5c11\u4e86\u5206\u7c7b\u504f\u5dee\u5e76\u63d0\u5347\u4e86\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u5728\u7c7b\u4f3cTransformer\u7684\u5206\u7c7b\u5668\u4e2d\u63d0\u53d6\u548c\u5e94\u7528\u8fd9\u4e9b\u5411\u91cf\u7684\u591a\u79cd\u7b56\u7565\uff0c\u8868\u660e\u4f20\u7edf\u7528\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5bfc\u5411\u5411\u91cf\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u540c\u6837\u6709\u6548\u3002\u66f4\u5e7f\u6cdb\u5730\u8bf4\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u79cd\u6781\u4f4e\u6210\u672c\u3001\u4ec5\u9700\u63a8\u7406\u65f6\u64cd\u4f5c\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e3\u5206\u7c7b\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2506.18368", "pdf": "https://arxiv.org/pdf/2506.18368", "abs": "https://arxiv.org/abs/2506.18368", "authors": ["Anja Deli\u0107", "Matej Grci\u0107", "Sini\u0161a \u0160egvi\u0107"], "title": "Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting anomalous human behaviour is an important visual task in\nsafety-critical applications such as healthcare monitoring, workplace safety,\nor public surveillance. In these contexts, abnormalities are often reflected\nwith unusual human poses. Thus, we propose SeeKer, a method for detecting\nanomalies in sequences of human skeletons. Our method formulates the skeleton\nsequence density through autoregressive factorization at the keypoint level.\nThe corresponding conditional distributions represent probable keypoint\nlocations given prior skeletal motion. We formulate the joint distribution of\nthe considered skeleton as causal prediction of conditional Gaussians across\nits constituent keypoints. A skeleton is flagged as anomalous if its keypoint\nlocations surprise our model (i.e. receive a low density). In practice, our\nanomaly score is a weighted sum of per-keypoint log-conditionals, where the\nweights account for the confidence of the underlying keypoint detector. Despite\nits conceptual simplicity, SeeKer surpasses all previous methods on the\nUBnormal and MSAD-HR datasets while delivering competitive performance on the\nShanghaiTech dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9aa8\u67b6\u5e8f\u5217\u7684\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u65b9\u6cd5SeeKer\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u56e0\u5b50\u5316\u5efa\u6a21\u5173\u952e\u70b9\u5bc6\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u533b\u7597\u76d1\u63a7\u3001\u5de5\u4f5c\u573a\u6240\u5b89\u5168\u548c\u516c\u5171\u76d1\u63a7\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u5f02\u5e38\u901a\u5e38\u8868\u73b0\u4e3a\u4e0d\u5bfb\u5e38\u7684\u4eba\u4f53\u59ff\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u8fd9\u4e9b\u5f02\u5e38\u3002", "method": "SeeKer\u65b9\u6cd5\u901a\u8fc7\u81ea\u56de\u5f52\u56e0\u5b50\u5316\u5728\u5173\u952e\u70b9\u7ea7\u522b\u5efa\u6a21\u9aa8\u67b6\u5e8f\u5217\u5bc6\u5ea6\uff0c\u5229\u7528\u6761\u4ef6\u9ad8\u65af\u5206\u5e03\u8868\u793a\u5173\u952e\u70b9\u4f4d\u7f6e\u7684\u6982\u7387\u3002\u5f02\u5e38\u5206\u6570\u662f\u5173\u952e\u70b9\u5bf9\u6570\u6761\u4ef6\u6982\u7387\u7684\u52a0\u6743\u548c\uff0c\u6743\u91cd\u8003\u8651\u4e86\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u7f6e\u4fe1\u5ea6\u3002", "result": "SeeKer\u5728UBnormal\u548cMSAD-HR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728ShanghaiTech\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u5c3d\u7ba1\u65b9\u6cd5\u7b80\u5355\uff0cSeeKer\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9aa8\u67b6\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "paper_title_zh": "\u5e8f\u5217\u5173\u952e\u70b9\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff1a\u9aa8\u67b6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u88ab\u5ffd\u89c6\u7684\u57fa\u7ebf\u65b9\u6cd5", "abstract_zh": "\u68c0\u6d4b\u5f02\u5e38\u4eba\u7c7b\u884c\u4e3a\u662f\u533b\u7597\u76d1\u63a7\u3001\u5de5\u4f5c\u573a\u6240\u5b89\u5168\u548c\u516c\u5171\u76d1\u63a7\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u89c6\u89c9\u4efb\u52a1\u3002\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u5f02\u5e38\u901a\u5e38\u8868\u73b0\u4e3a\u4e0d\u5bfb\u5e38\u7684\u4eba\u4f53\u59ff\u6001\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SeeKer\uff0c\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u4eba\u7c7b\u9aa8\u67b6\u5e8f\u5217\u5f02\u5e38\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5173\u952e\u70b9\u7ea7\u522b\u7684\u81ea\u56de\u5f52\u56e0\u5b50\u5316\u5efa\u6a21\u9aa8\u67b6\u5e8f\u5217\u5bc6\u5ea6\uff0c\u76f8\u5e94\u7684\u6761\u4ef6\u5206\u5e03\u8868\u793a\u7ed9\u5b9a\u5148\u524d\u9aa8\u9abc\u8fd0\u52a8\u7684\u5173\u952e\u70b9\u4f4d\u7f6e\u6982\u7387\u3002\u6211\u4eec\u5c06\u9aa8\u67b6\u7684\u8054\u5408\u5206\u5e03\u5efa\u6a21\u4e3a\u5176\u5173\u952e\u70b9\u6761\u4ef6\u9ad8\u65af\u7684\u56e0\u679c\u9884\u6d4b\u3002\u5982\u679c\u9aa8\u67b6\u7684\u5173\u952e\u70b9\u4f4d\u7f6e\u51fa\u4e4e\u6a21\u578b\u610f\u6599\uff08\u5373\u5bc6\u5ea6\u8f83\u4f4e\uff09\uff0c\u5219\u88ab\u6807\u8bb0\u4e3a\u5f02\u5e38\u3002\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u7684\u5f02\u5e38\u5206\u6570\u662f\u5173\u952e\u70b9\u5bf9\u6570\u6761\u4ef6\u6982\u7387\u7684\u52a0\u6743\u548c\uff0c\u6743\u91cd\u8003\u8651\u4e86\u5e95\u5c42\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u7f6e\u4fe1\u5ea6\u3002\u5c3d\u7ba1\u6982\u5ff5\u7b80\u5355\uff0cSeeKer\u5728UBnormal\u548cMSAD-HR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u5728ShanghaiTech\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.17375", "pdf": "https://arxiv.org/pdf/2506.17375", "abs": "https://arxiv.org/abs/2506.17375", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Challenges in Grounding Language in the Real World", "categories": ["q-bio.NC", "cs.AI"], "comment": "14 pages, 2 figures", "summary": "A long-term goal of Artificial Intelligence is to build a language\nunderstanding system that allows a human to collaborate with a physical robot\nusing language that is natural to the human. In this paper we highlight some of\nthe challenges in doing this, and propose a solution that integrates the\nabilities of a cognitive agent capable of interactive task learning in a\nphysical robot with the linguistic abilities of a large language model. We also\npoint the way to an initial implementation of this approach.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u4e2d\u8bed\u8a00\u4e0e\u73b0\u5b9e\u4e16\u754c\u7ed3\u5408\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba4\u77e5\u4ee3\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u957f\u671f\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\uff0c\u4f7f\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u7269\u7406\u673a\u5668\u4eba\u534f\u4f5c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8ba4\u77e5\u4ee3\u7406\u7684\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u80fd\u529b\u76f8\u7ed3\u5408\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u521d\u6b65\u5b9e\u73b0\u4e86\u4e00\u79cd\u6574\u5408\u8ba4\u77e5\u4ee3\u7406\u548c\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8ba4\u77e5\u4ee3\u7406\u4e0e\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u8bed\u8a00\u4e0e\u73b0\u5b9e\u4e16\u754c\u7ed3\u5408\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "paper_title_zh": "\u8bed\u8a00\u4e0e\u73b0\u5b9e\u4e16\u754c\u7ed3\u5408\u7684\u6311\u6218", "abstract_zh": "\u4eba\u5de5\u667a\u80fd\u7684\u957f\u671f\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\uff0c\u4f7f\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u7269\u7406\u673a\u5668\u4eba\u534f\u4f5c\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u4e00\u4e9b\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5c06\u5177\u5907\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\u7684\u8ba4\u77e5\u4ee3\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u80fd\u529b\u76f8\u7ed3\u5408\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8fd8\u6307\u51fa\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u521d\u6b65\u5b9e\u73b0\u65b9\u5411\u3002"}}
{"id": "2506.18369", "pdf": "https://arxiv.org/pdf/2506.18369", "abs": "https://arxiv.org/abs/2506.18369", "authors": ["Yeongtak Oh", "Jisoo Mok", "Dohyun Chung", "Juhyeon Shin", "Sangha Park", "Johan Barthelemy", "Sungroh Yoon"], "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/oyt9306/RePIC", "summary": "Recent multi-modal large language models (MLLMs) often struggle to generate\npersonalized image captions, even when trained on high-quality captions. In\nthis work, we observe that such limitations persist in existing\npost-training-based MLLM personalization methods. Specifically, despite being\npost-tuned with large-scale caption data through supervised fine-tuning (SFT),\nthese models frequently fail to produce faithful descriptions in real-world\nscenarios, such as multi-concept image captioning. However, acquiring\nlarge-scale, high-quality captions for such complex settings is both costly and\ndifficult. To address the data-centric nature of SFT, we propose a\nreinforcement learning (RL)-based post-training framework. To the best of our\nknowledge, this is the first RL-based approach to post-train MLLMs for\npersonalized image captioning. Our method significantly enhances both visual\nrecognition and personalized generation capabilities of MLLMs, and consistently\noutperforms existing SFT-based baselines, especially in the challenging\nmulti-concept image captioning task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6RePIC\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e2a\u6027\u5316\u56fe\u50cf\u63cf\u8ff0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u591a\u6982\u5ff5\u56fe\u50cf\u63cf\u8ff0\u7b49\u590d\u6742\u573a\u666f\u65f6\uff0c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u6709\u9650\u4e14\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6RePIC\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\uff0c\u800c\u975e\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3002", "result": "RePIC\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u89c6\u89c9\u8bc6\u522b\u548c\u4e2a\u6027\u5316\u751f\u6210\u80fd\u529b\uff0c\u5728\u591a\u6982\u5ff5\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u5316\u751f\u6210\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "paper_title_zh": "RePIC\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u7528\u4e8e\u4e2a\u6027\u5316\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u751f\u6210\u4e2a\u6027\u5316\u56fe\u50cf\u63cf\u8ff0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u7ecf\u8fc7\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u8bad\u7ec3\u3002\u672c\u6587\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u540e\u8bad\u7ec3\u7684MLLM\u4e2a\u6027\u5316\u65b9\u6cd5\u4ecd\u5b58\u5728\u8fd9\u4e00\u5c40\u9650\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5c3d\u7ba1\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4f7f\u7528\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u540e\u8c03\u4f18\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982\u591a\u6982\u5ff5\u56fe\u50cf\u63cf\u8ff0\uff09\u4e2d\u4ecd\u96be\u4ee5\u751f\u6210\u51c6\u786e\u7684\u63cf\u8ff0\u3002\u7136\u800c\uff0c\u83b7\u53d6\u6b64\u7c7b\u590d\u6742\u573a\u666f\u4e0b\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u4e14\u56f0\u96be\u3002\u4e3a\u89e3\u51b3SFT\u7684\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8eRL\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u56fe\u50cf\u63cf\u8ff0\u7684MLLM\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u89c6\u89c9\u8bc6\u522b\u548c\u4e2a\u6027\u5316\u751f\u6210\u80fd\u529b\uff0c\u5728\u591a\u6982\u5ff5\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SFT\u57fa\u7ebf\u3002"}}
{"id": "2506.18631", "pdf": "https://arxiv.org/pdf/2506.18631", "abs": "https://arxiv.org/abs/2506.18631", "authors": ["Chenxing Wei", "Jiarui Yu", "Ying Tiffany He", "Hande Dong", "Yao Shu", "Fei Yu"], "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 15 figures", "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReDit\uff08\u5956\u52b1\u6296\u52a8\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u79bb\u6563\u5956\u52b1\u4fe1\u53f7\u6dfb\u52a0\u968f\u673a\u566a\u58f0\uff0c\u89e3\u51b3\u79bb\u6563\u5956\u52b1\u5bfc\u81f4\u7684\u68af\u5ea6\u5f02\u5e38\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347LLM\u7b56\u7565\u4f18\u5316\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "DeepSeek-R1\u7684\u79bb\u6563\u5956\u52b1\u7cfb\u7edf\u867d\u7136\u6709\u6548\u907f\u514d\u4e86\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u4f46\u4f1a\u5bfc\u81f4\u68af\u5ea6\u5f02\u5e38\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u7f13\u6162\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86ReDit\u65b9\u6cd5\u3002", "method": "ReDit\u901a\u8fc7\u5728\u79bb\u6563\u5956\u52b1\u4fe1\u53f7\u4e2d\u6dfb\u52a0\u7b80\u5355\u7684\u968f\u673a\u566a\u58f0\uff0c\u751f\u6210\u6296\u52a8\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u4f9b\u8fde\u7eed\u7684\u63a2\u7d22\u68af\u5ea6\uff0c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u68af\u5ea6\u66f4\u65b0\u548c\u66f4\u5feb\u7684\u6536\u655b\u3002\u566a\u58f0\u8fd8\u5f15\u5165\u968f\u673a\u6027\uff0c\u5e2e\u52a9\u6a21\u578b\u63a2\u7d22\u65b0\u7b56\u7565\u5e76\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReDit\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u7ea610%\u7684\u8bad\u7ec3\u6b65\u6570\u5373\u53ef\u8fbe\u5230\u4e0e\u666e\u901aGRPO\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u5728\u76f8\u540c\u8bad\u7ec3\u65f6\u957f\u4e0b\u6027\u80fd\u63d0\u53474%\u3002\u53ef\u89c6\u5316\u7ed3\u679c\u8bc1\u5b9eReDit\u663e\u8457\u7f13\u89e3\u4e86\u68af\u5ea6\u95ee\u9898\u3002", "conclusion": "ReDit\u901a\u8fc7\u6296\u52a8\u79bb\u6563\u5956\u52b1\u4fe1\u53f7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u5f02\u5e38\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7b56\u7565\u4f18\u5316\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "paper_title_zh": "ReDit\uff1a\u901a\u8fc7\u5956\u52b1\u6296\u52a8\u6539\u8fdbLLM\u7b56\u7565\u4f18\u5316", "abstract_zh": "DeepSeek-R1\u901a\u8fc7\u5176\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u7cfb\u7edf\u6210\u529f\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u5c3d\u7ba1\u8fd9\u662f\u4e00\u79cd\u201c\u5b8c\u7f8e\u201d\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u4f46\u6b64\u7c7b\u5956\u52b1\u51fd\u6570\u901a\u5e38\u662f\u79bb\u6563\u7684\u3002\u5b9e\u9a8c\u89c2\u5bdf\u8868\u660e\uff0c\u79bb\u6563\u5956\u52b1\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u5f02\u5e38\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u7f13\u6162\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faReDit\uff08\u5956\u52b1\u6296\u52a8\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6dfb\u52a0\u7b80\u5355\u968f\u673a\u566a\u58f0\u5bf9\u79bb\u6563\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u6296\u52a8\u3002\u8fd9\u79cd\u6270\u52a8\u540e\u7684\u5956\u52b1\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u6301\u7eed\u63d0\u4f9b\u63a2\u7d22\u68af\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u68af\u5ea6\u66f4\u65b0\u548c\u66f4\u5feb\u7684\u6536\u655b\u3002\u6ce8\u5165\u7684\u566a\u58f0\u8fd8\u5728\u5e73\u5766\u5956\u52b1\u533a\u57df\u5f15\u5165\u968f\u673a\u6027\uff0c\u9f13\u52b1\u6a21\u578b\u63a2\u7d22\u65b0\u7b56\u7565\u5e76\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u3002\u591a\u6837\u4efb\u52a1\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86ReDit\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002\u5e73\u5747\u800c\u8a00\uff0cReDit\u4ec5\u9700\u7ea610%\u7684\u8bad\u7ec3\u6b65\u6570\u5373\u53ef\u8fbe\u5230\u4e0e\u666e\u901aGRPO\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u5728\u76f8\u540c\u8bad\u7ec3\u65f6\u957f\u4e0b\u4ecd\u6bd4\u666e\u901aGRPO\u6027\u80fd\u63d0\u53474%\u3002\u53ef\u89c6\u5316\u7ed3\u679c\u8bc1\u5b9eReDit\u663e\u8457\u7f13\u89e3\u4e86\u68af\u5ea6\u95ee\u9898\u3002\u6b64\u5916\uff0c\u7406\u8bba\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u4f18\u52bf\u3002"}}
{"id": "2506.18372", "pdf": "https://arxiv.org/pdf/2506.18372", "abs": "https://arxiv.org/abs/2506.18372", "authors": ["Hieu Nguyen", "Phuc-Tan Nguyen", "Thien-Phuc Tran", "Minh-Quang Nguyen", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce OpenEvents V1, a large-scale benchmark dataset aimed at\nadvancing event-centric vision-language understanding. Unlike conventional\nimage captioning and retrieval datasets that emphasize surface-level\ndescriptions, OpenEvents V1 focuses on contextual and temporal grounding\nthrough two primary tasks: (1) generating rich, event-aware image captions and\n(2) retrieving event-relevant images based on narrative-style textual queries.\nThe dataset contains over 200,000 news articles and 400,000 associated images\nsourced from CNN and The Guardian, spanning diverse domains and time periods.\nWe provide extensive baseline results and standardized evaluation protocols for\nboth tasks. OpenEvents V1 establishes a robust foundation for developing\nmultimodal models capable of deep reasoning over complex real-world events. The\ndataset is available at https://ltnghia.github.io/eventa/openevents-v1", "AI": {"tldr": "OpenEvents V1\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u4e8b\u4ef6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u4e8b\u4ef6\u611f\u77e5\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u548c\u57fa\u4e8e\u53d9\u4e8b\u6587\u672c\u67e5\u8be2\u7684\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\uff0c\u5305\u542b20\u4e07\u7bc7\u65b0\u95fb\u6587\u7ae0\u548c40\u4e07\u5f20\u76f8\u5173\u56fe\u7247\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u63cf\u8ff0\u548c\u68c0\u7d22\u6570\u636e\u96c6\u4ec5\u5173\u6ce8\u8868\u9762\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u6df1\u5165\u7406\u89e3\u3002OpenEvents V1\u65e8\u5728\u63a8\u52a8\u4e8b\u4ef6\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u6765\u81eaCNN\u548cThe Guardian\u768420\u4e07\u7bc7\u65b0\u95fb\u6587\u7ae0\u548c40\u4e07\u5f20\u56fe\u7247\uff0c\u8986\u76d6\u591a\u6837\u9886\u57df\u548c\u65f6\u95f4\u6bb5\u3002\u63d0\u4f9b\u4e24\u79cd\u4efb\u52a1\uff1a\u751f\u6210\u4e8b\u4ef6\u611f\u77e5\u7684\u56fe\u50cf\u63cf\u8ff0\u548c\u57fa\u4e8e\u53d9\u4e8b\u6587\u672c\u7684\u56fe\u50cf\u68c0\u7d22\uff0c\u5e76\u5236\u5b9a\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u3002", "result": "OpenEvents V1\u4e3a\u5f00\u53d1\u80fd\u591f\u6df1\u5ea6\u63a8\u7406\u590d\u6742\u73b0\u5b9e\u4e8b\u4ef6\u7684\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u7ed3\u679c\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "OpenEvents V1\u901a\u8fc7\u5176\u5927\u89c4\u6a21\u548c\u4efb\u52a1\u8bbe\u8ba1\uff0c\u4e3a\u591a\u6a21\u6001\u4e8b\u4ef6\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "paper_title_zh": "OpenEvents V1\uff1a\u7528\u4e8e\u591a\u6a21\u6001\u4e8b\u4ef6\u5b9a\u4f4d\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86OpenEvents V1\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u63a8\u52a8\u4e8b\u4ef6\u4e2d\u5fc3\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002\u4e0e\u4f20\u7edf\u5f3a\u8c03\u8868\u9762\u63cf\u8ff0\u7684\u56fe\u50cf\u63cf\u8ff0\u548c\u68c0\u7d22\u6570\u636e\u96c6\u4e0d\u540c\uff0cOpenEvents V1\u901a\u8fc7\u4e24\u9879\u4e3b\u8981\u4efb\u52a1\u805a\u7126\u4e8e\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u5b9a\u4f4d\uff1a(1)\u751f\u6210\u4e30\u5bcc\u7684\u4e8b\u4ef6\u611f\u77e5\u56fe\u50cf\u63cf\u8ff0\uff1b(2)\u57fa\u4e8e\u53d9\u4e8b\u98ce\u683c\u6587\u672c\u67e5\u8be2\u68c0\u7d22\u4e8b\u4ef6\u76f8\u5173\u56fe\u50cf\u3002\u6570\u636e\u96c6\u5305\u542b\u6765\u81eaCNN\u548cThe Guardian\u768420\u4e07\u7bc7\u65b0\u95fb\u6587\u7ae0\u548c40\u4e07\u5f20\u76f8\u5173\u56fe\u7247\uff0c\u6db5\u76d6\u591a\u6837\u9886\u57df\u548c\u65f6\u95f4\u6bb5\u3002\u6211\u4eec\u4e3a\u4e24\u9879\u4efb\u52a1\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u7ed3\u679c\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u3002OpenEvents V1\u4e3a\u5f00\u53d1\u80fd\u591f\u6df1\u5ea6\u63a8\u7406\u590d\u6742\u73b0\u5b9e\u4e8b\u4ef6\u7684\u591a\u6a21\u6001\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u6570\u636e\u96c6\u53ef\u901a\u8fc7https://ltnghia.github.io/eventa/openevents-v1\u83b7\u53d6\u3002"}}
{"id": "2506.18716", "pdf": "https://arxiv.org/pdf/2506.18716", "abs": "https://arxiv.org/abs/2506.18716", "authors": ["Jie Li", "Shifei Ding", "Lili Guo", "Xuan Li"], "title": "Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation", "categories": ["cs.LG", "cs.CL"], "comment": "This paper has been accepted by IJCAI2025", "summary": "Emotion Recognition in Conversation (ERC) aims to detect the emotions of\nindividual utterances within a conversation. Generating efficient and\nmodality-specific representations for each utterance remains a significant\nchallenge. Previous studies have proposed various models to integrate features\nextracted using different modality-specific encoders. However, they neglect the\nvarying contributions of modalities to this task and introduce high complexity\nby aligning modalities at the frame level. To address these challenges, we\npropose the Multi-modal Anchor Gated Transformer with Knowledge Distillation\n(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance\ntextual modality representations, while knowledge distillation is utilized to\nstrengthen representations of weaker modalities. Furthermore, we introduce a\nmulti-modal anchor gated transformer to effectively integrate utterance-level\nrepresentations across modalities. Extensive experiments on the IEMOCAP and\nMELD datasets demonstrate the effectiveness of knowledge distillation in\nenhancing modality representations and achieve state-of-the-art performance in\nemotion recognition. Our code is available at:\nhttps://github.com/JieLi-dd/MAGTKD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u6362\u5668\u4e0e\u77e5\u8bc6\u84b8\u998f\u7ed3\u5408\u7684\u65b9\u6cd5\uff08MAGTKD\uff09\uff0c\u7528\u4e8e\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u6001\u8868\u793a\u548c\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff08ERC\uff09\u9700\u8981\u4e3a\u6bcf\u4e2a\u8bdd\u8bed\u751f\u6210\u9ad8\u6548\u7684\u6a21\u6001\u7279\u5b9a\u8868\u793a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6a21\u6001\u5bf9\u4efb\u52a1\u7684\u4e0d\u540c\u8d21\u732e\uff0c\u4e14\u5e27\u7ea7\u5bf9\u9f50\u589e\u52a0\u4e86\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMAGTKD\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528\u63d0\u793a\u5b66\u4e60\u589e\u5f3a\u6587\u672c\u6a21\u6001\u8868\u793a\uff0c\u77e5\u8bc6\u84b8\u998f\u5f3a\u5316\u8f83\u5f31\u6a21\u6001\u8868\u793a\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u6362\u5668\u6574\u5408\u8de8\u6a21\u6001\u8bdd\u8bed\u7ea7\u8868\u793a\u3002", "result": "\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u6001\u8868\u793a\uff0c\u5e76\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MAGTKD\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u6362\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u6362\u5668\u5728\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff08ERC\uff09\u65e8\u5728\u68c0\u6d4b\u5bf9\u8bdd\u4e2d\u5355\u4e2a\u8bdd\u8bed\u7684\u60c5\u611f\u3002\u4e3a\u6bcf\u4e2a\u8bdd\u8bed\u751f\u6210\u9ad8\u6548\u4e14\u6a21\u6001\u7279\u5b9a\u7684\u8868\u793a\u4ecd\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u5148\u524d\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u79cd\u6a21\u578b\u6765\u6574\u5408\u4e0d\u540c\u6a21\u6001\u7f16\u7801\u5668\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u5ffd\u89c6\u4e86\u6a21\u6001\u5bf9\u6b64\u4efb\u52a1\u7684\u4e0d\u540c\u8d21\u732e\uff0c\u5e76\u901a\u8fc7\u5e27\u7ea7\u5bf9\u9f50\u5f15\u5165\u4e86\u9ad8\u590d\u6742\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u6362\u5668\uff08MAGTKD\uff09\u7528\u4e8eERC\u4efb\u52a1\u3002\u5177\u4f53\u800c\u8a00\uff0c\u91c7\u7528\u63d0\u793a\u5b66\u4e60\u589e\u5f3a\u6587\u672c\u6a21\u6001\u8868\u793a\uff0c\u540c\u65f6\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u5f3a\u5316\u8f83\u5f31\u6a21\u6001\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u6362\u5668\uff0c\u6709\u6548\u6574\u5408\u8de8\u6a21\u6001\u7684\u8bdd\u8bed\u7ea7\u8868\u793a\u3002\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u6001\u8868\u793a\uff0c\u5e76\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53d1\u5e03\u4e8e\uff1ahttps://github.com/JieLi-dd/MAGTKD\u3002"}}
{"id": "2506.18385", "pdf": "https://arxiv.org/pdf/2506.18385", "abs": "https://arxiv.org/abs/2506.18385", "authors": ["Nianchen Deng", "Lixin Gu", "Shenglong Ye", "Yinan He", "Zhe Chen", "Songze Li", "Haomin Wang", "Xingguang Wei", "Tianshuo Yang", "Min Dou", "Tong He", "Wenqi Shao", "Kaipeng Zhang", "Yi Wang", "Botian Shi", "Yanting Zhang", "Jifeng Dai", "Yu Qiao", "Hongjie Zhang", "Wenhai Wang"], "title": "InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent benchmarks and datasets have been proposed to improve spatial\nreasoning in vision-language models (VLMs), yet existing open resources remain\nlimited in scale, visual diversity, and instruction expressiveness. In this\nwork, we introduce InternSpatial, the largest open-source dataset for spatial\nreasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation\nbenchmark designed to assess spatial understanding under diverse instruction\nformats. InternSpatial comprises 12 million QA pairs spanning both single-view\nand multi-view settings, drawn from diverse visual environments and supporting\n19 instruction formats that reflect varied query styles. For evaluation, we\npropose InternSpatial-Bench for single-view tasks and expand multi-view\nreasoning by introducing a novel rotation angle prediction task that has not\nbeen explored in prior work. Experimental results show that models trained on\nInternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on\nVSI-Bench, while maintaining strong performance on general-purpose benchmarks.\nWe hope these resources will support the development of spatially capable VLMs\nin practical applications such as robotics and embodied AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InternSpatial\uff0c\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u6700\u5927\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5e76\u914d\u5957\u4e86\u8bc4\u4f30\u57fa\u51c6InternSpatial-Bench\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u89c6\u89c9\u591a\u6837\u6027\u4e0d\u8db3\u4e14\u6307\u4ee4\u8868\u8fbe\u6709\u9650\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1200\u4e07\u95ee\u7b54\u5bf9\u7684InternSpatial\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5355\u89c6\u89d2\u548c\u591a\u89c6\u89d2\u573a\u666f\uff0c\u652f\u630119\u79cd\u6307\u4ee4\u683c\u5f0f\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86InternSpatial-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u9896\u7684\u65cb\u8f6c\u89d2\u5ea6\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528InternSpatial\u8bad\u7ec3\u7684\u6a21\u578b\u5728InternSpatial-Bench\u4e0a\u63d0\u5347\u4e8612.1%\uff0c\u5728VSI-Bench\u4e0a\u63d0\u5347\u4e8610.7%\uff0c\u540c\u65f6\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "conclusion": "InternSpatial\u53ca\u5176\u8bc4\u4f30\u57fa\u51c6\u4e3a\u5f00\u53d1\u5177\u6709\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u548c\u5177\u8eabAI\u7b49\u5b9e\u9645\u5e94\u7528\u7684\u53d1\u5c55\u3002", "paper_title_zh": "InternSpatial\uff1a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u7684\u7efc\u5408\u6570\u636e\u96c6", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5c3d\u7ba1\u5df2\u6709\u4e00\u4e9b\u57fa\u51c6\u548c\u6570\u636e\u96c6\u88ab\u63d0\u51fa\u4ee5\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7684\u5f00\u653e\u8d44\u6e90\u5728\u89c4\u6a21\u3001\u89c6\u89c9\u591a\u6837\u6027\u548c\u6307\u4ee4\u8868\u8fbe\u6027\u65b9\u9762\u4ecd\u663e\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86InternSpatial\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u7684\u5f00\u6e90\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u96c6\uff0c\u540c\u65f6\u914d\u5957\u4e86InternSpatial-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6837\u5316\u6307\u4ee4\u683c\u5f0f\u4e0b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002InternSpatial\u5305\u542b1200\u4e07\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u5355\u89c6\u89d2\u548c\u591a\u89c6\u89d2\u573a\u666f\uff0c\u6e90\u81ea\u591a\u6837\u5316\u7684\u89c6\u89c9\u73af\u5883\uff0c\u5e76\u652f\u630119\u79cd\u53cd\u6620\u4e0d\u540c\u67e5\u8be2\u98ce\u683c\u7684\u6307\u4ee4\u683c\u5f0f\u3002\u5728\u8bc4\u4f30\u65b9\u9762\uff0c\u6211\u4eec\u4e3a\u5355\u89c6\u89d2\u4efb\u52a1\u8bbe\u8ba1\u4e86InternSpatial-Bench\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u65cb\u8f6c\u89d2\u5ea6\u9884\u6d4b\u4efb\u52a1\u6269\u5c55\u4e86\u591a\u89c6\u89d2\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u5728\u5148\u524d\u7684\u7814\u7a76\u4e2d\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528InternSpatial\u8bad\u7ec3\u7684\u6a21\u578b\u5728InternSpatial-Bench\u4e0a\u63d0\u5347\u4e8612.1%\uff0c\u5728VSI-Bench\u4e0a\u63d0\u5347\u4e8610.7%\uff0c\u540c\u65f6\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e86\u5f3a\u52b2\u6027\u80fd\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u8d44\u6e90\u80fd\u591f\u652f\u6301\u5f00\u53d1\u5177\u6709\u7a7a\u95f4\u80fd\u529b\u7684VLMs\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u548c\u5177\u8eabAI\u7b49\u5b9e\u9645\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.17462", "pdf": "https://arxiv.org/pdf/2506.17462", "abs": "https://arxiv.org/abs/2506.17462", "authors": ["Bernard Lange", "Anil Yildiz", "Mansur Arief", "Shehryar Khattak", "Mykel Kochenderfer", "Georgios Georgakis"], "title": "General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Developing general-purpose navigation policies for unknown environments\nremains a core challenge in robotics. Most existing systems rely on\ntask-specific neural networks and fixed data flows, limiting generalizability.\nLarge Vision-Language Models (LVLMs) offer a promising alternative by embedding\nhuman-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot\nintegrations typically depend on pre-mapped spaces, hard-coded representations,\nand myopic exploration. We introduce the Agentic Robotic Navigation\nArchitecture (ARNA), a general-purpose navigation framework that equips an\nLVLM-based agent with a library of perception, reasoning, and navigation tools\navailable within modern robotic stacks. At runtime, the agent autonomously\ndefines and executes task-specific workflows that iteratively query the robotic\nmodules, reason over multimodal inputs, and select appropriate navigation\nactions. This approach enables robust navigation and reasoning in previously\nunmapped environments, providing a new perspective on robotic stack design.\nEvaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves\nstate-of-the-art performance, demonstrating effective exploration, navigation,\nand embodied question answering without relying on handcrafted plans, fixed\ninput representations, or pre-existing maps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u901a\u7528\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6ARNA\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u611f\u77e5\u3001\u63a8\u7406\u548c\u5bfc\u822a\u5de5\u5177\uff0c\u5b9e\u73b0\u672a\u77e5\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u548c\u63a8\u7406\uff0c\u5e76\u5728HM-EQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u56fa\u5b9a\u6570\u636e\u6d41\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5177\u5907\u7c7b\u4eba\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u63a8\u7406\u548c\u89c4\u5212\uff0c\u4f46\u73b0\u6709LVLM\u4e0e\u673a\u5668\u4eba\u96c6\u6210\u591a\u4f9d\u8d56\u9884\u5efa\u5730\u56fe\u548c\u786c\u7f16\u7801\u8868\u793a\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u5bfc\u822a\u6846\u67b6\uff0c\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86Agentic Robotic Navigation Architecture\uff08ARNA\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8eLVLM\u7684\u901a\u7528\u5bfc\u822a\u6846\u67b6\u3002ARNA\u52a8\u6001\u8c03\u7528\u73b0\u4ee3\u673a\u5668\u4eba\u6280\u672f\u6808\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u5bfc\u822a\u5de5\u5177\uff0c\u5728\u8fd0\u884c\u65f6\u81ea\u4e3b\u5b9a\u4e49\u4efb\u52a1\u7279\u5b9a\u5de5\u4f5c\u6d41\uff0c\u8fed\u4ee3\u67e5\u8be2\u673a\u5668\u4eba\u6a21\u5757\u3001\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u9009\u62e9\u5bfc\u822a\u52a8\u4f5c\u3002", "result": "\u5728Habitat Lab\u7684HM-EQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARNA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5728\u672a\u6620\u5c04\u73af\u5883\u4e2d\u6709\u6548\u63a2\u7d22\u3001\u5bfc\u822a\u548c\u56de\u7b54\u5177\u8eab\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u624b\u5de5\u8ba1\u5212\u3001\u56fa\u5b9a\u8f93\u5165\u8868\u793a\u6216\u9884\u5efa\u5730\u56fe\u3002", "conclusion": "ARNA\u4e3a\u673a\u5668\u4eba\u6280\u672f\u6808\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86LVLM\u5728\u901a\u7528\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u6d41\u7a0b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u5bfc\u822a\u548c\u63a8\u7406\u3002", "paper_title_zh": "\u57fa\u4e8eLVLM\u534f\u8c03\u611f\u77e5\u3001\u63a8\u7406\u4e0e\u884c\u52a8\u7684\u901a\u7528\u673a\u5668\u4eba\u5bfc\u822a", "abstract_zh": "\u5f00\u53d1\u9002\u7528\u4e8e\u672a\u77e5\u73af\u5883\u7684\u901a\u7528\u5bfc\u822a\u7b56\u7565\u4ecd\u662f\u673a\u5668\u4eba\u6280\u672f\u7684\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u7cfb\u7edf\u591a\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u56fa\u5b9a\u6570\u636e\u6d41\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u901a\u8fc7\u5d4c\u5165\u7c7b\u4eba\u77e5\u8bc6\uff0c\u4e3a\u63a8\u7406\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684LVLM-\u673a\u5668\u4eba\u96c6\u6210\u901a\u5e38\u4f9d\u8d56\u9884\u5efa\u5730\u56fe\u3001\u786c\u7f16\u7801\u8868\u793a\u548c\u77ed\u89c6\u63a2\u7d22\u3002\u672c\u6587\u63d0\u51fa\u4e86Agentic Robotic Navigation Architecture\uff08ARNA\uff09\uff0c\u4e00\u79cd\u901a\u7528\u5bfc\u822a\u6846\u67b6\uff0c\u4e3a\u57fa\u4e8eLVLM\u7684\u667a\u80fd\u4f53\u914d\u5907\u4e86\u73b0\u4ee3\u673a\u5668\u4eba\u6280\u672f\u6808\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u5bfc\u822a\u5de5\u5177\u5e93\u3002\u5728\u8fd0\u884c\u65f6\uff0c\u667a\u80fd\u4f53\u81ea\u4e3b\u5b9a\u4e49\u5e76\u6267\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u5de5\u4f5c\u6d41\uff0c\u8fed\u4ee3\u67e5\u8be2\u673a\u5668\u4eba\u6a21\u5757\u3001\u63a8\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u9009\u62e9\u9002\u5f53\u7684\u5bfc\u822a\u52a8\u4f5c\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u672a\u6620\u5c04\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u5bfc\u822a\u548c\u63a8\u7406\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u672f\u6808\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u5728Habitat Lab\u7684HM-EQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARNA\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u624b\u5de5\u8ba1\u5212\u3001\u56fa\u5b9a\u8f93\u5165\u8868\u793a\u6216\u9884\u5efa\u5730\u56fe\u7684\u6709\u6548\u63a2\u7d22\u3001\u5bfc\u822a\u548c\u5177\u8eab\u95ee\u9898\u56de\u7b54\u80fd\u529b\u3002"}}
{"id": "2506.18764", "pdf": "https://arxiv.org/pdf/2506.18764", "abs": "https://arxiv.org/abs/2506.18764", "authors": ["Csaba Zsolnai", "Niels L\u00f6rch", "Julian Arnold"], "title": "Neural Total Variation Distance Estimators for Changepoint Detection in News Data", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "comment": "16 pages, 3 figures", "summary": "Detecting when public discourse shifts in response to major events is crucial\nfor understanding societal dynamics. Real-world data is high-dimensional,\nsparse, and noisy, making changepoint detection in this domain a challenging\nendeavor. In this paper, we leverage neural networks for changepoint detection\nin news data, introducing a method based on the so-called learning-by-confusion\nscheme, which was originally developed for detecting phase transitions in\nphysical systems. We train classifiers to distinguish between articles from\ndifferent time periods. The resulting classification accuracy is used to\nestimate the total variation distance between underlying content distributions,\nwhere significant distances highlight changepoints. We demonstrate the\neffectiveness of this method on both synthetic datasets and real-world data\nfrom The Guardian newspaper, successfully identifying major historical events\nincluding 9/11, the COVID-19 pandemic, and presidential elections. Our approach\nrequires minimal domain knowledge, can autonomously discover significant shifts\nin public discourse, and yields a quantitative measure of change in content,\nmaking it valuable for journalism, policy analysis, and crisis monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u53d8\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7c7b\u5668\u533a\u5206\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u65b0\u95fb\u5185\u5bb9\uff0c\u4f30\u8ba1\u5185\u5bb9\u5206\u5e03\u7684\u603b\u53d8\u5dee\u8ddd\u79bb\u4ee5\u8bc6\u522b\u91cd\u5927\u4e8b\u4ef6\u3002\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u65b0\u95fb\u6570\u636e\u4e2d\u5747\u6709\u6548\uff0c\u6210\u529f\u68c0\u6d4b\u52309/11\u3001COVID-19\u7b49\u4e8b\u4ef6\u3002", "motivation": "\u516c\u5171\u8bdd\u8bed\u7684\u8f6c\u53d8\u5bf9\u7406\u89e3\u793e\u4f1a\u52a8\u6001\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u7ef4\u3001\u7a00\u758f\u4e14\u5608\u6742\u7684\u771f\u5b9e\u6570\u636e\u4f7f\u53d8\u70b9\u68c0\u6d4b\u6210\u4e3a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u81ea\u52a8\u8bc6\u522b\u65b0\u95fb\u6570\u636e\u4e2d\u7684\u91cd\u5927\u4e8b\u4ef6\u8f6c\u6298\u70b9\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u201c\u5b66\u4e60-\u6df7\u6dc6\u201d\u65b9\u6848\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u533a\u5206\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u65b0\u95fb\u6587\u7ae0\uff0c\u5229\u7528\u5206\u7c7b\u51c6\u786e\u7387\u4f30\u8ba1\u5185\u5bb9\u5206\u5e03\u7684\u603b\u53d8\u5dee\u8ddd\u79bb\uff0c\u4ece\u800c\u8bc6\u522b\u53d8\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u300a\u536b\u62a5\u300b\u7684\u771f\u5b9e\u65b0\u95fb\u6570\u636e\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u6210\u529f\u68c0\u6d4b\u52309/11\u3001COVID-19\u5927\u6d41\u884c\u548c\u603b\u7edf\u9009\u4e3e\u7b49\u91cd\u5927\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8fc7\u591a\u9886\u57df\u77e5\u8bc6\uff0c\u80fd\u81ea\u52a8\u53d1\u73b0\u516c\u5171\u8bdd\u8bed\u7684\u91cd\u5927\u8f6c\u53d8\uff0c\u5e76\u63d0\u4f9b\u5185\u5bb9\u53d8\u5316\u7684\u91cf\u5316\u6307\u6807\uff0c\u5bf9\u65b0\u95fb\u4e1a\u3001\u653f\u7b56\u5206\u6790\u548c\u5371\u673a\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "paper_title_zh": "\u57fa\u4e8e\u795e\u7ecf\u603b\u53d8\u5dee\u8ddd\u79bb\u4f30\u8ba1\u5668\u7684\u65b0\u95fb\u6570\u636e\u53d8\u70b9\u68c0\u6d4b\u65b9\u6cd5", "abstract_zh": "\u68c0\u6d4b\u516c\u5171\u8bdd\u8bed\u56e0\u91cd\u5927\u4e8b\u4ef6\u800c\u53d1\u751f\u7684\u8f6c\u53d8\u5bf9\u4e8e\u7406\u89e3\u793e\u4f1a\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u5177\u6709\u9ad8\u7ef4\u5ea6\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\uff0c\u4f7f\u5f97\u8be5\u9886\u57df\u7684\u53d8\u70b9\u68c0\u6d4b\u6210\u4e3a\u4e00\u9879\u6311\u6218\u6027\u4efb\u52a1\u3002\u672c\u6587\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u65b0\u95fb\u6570\u636e\u4e2d\u7684\u53d8\u70b9\u68c0\u6d4b\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u5b66\u4e60-\u6df7\u6dc6\u201d\u65b9\u6848\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6700\u521d\u662f\u4e3a\u68c0\u6d4b\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u76f8\u53d8\u800c\u5f00\u53d1\u7684\u3002\u6211\u4eec\u8bad\u7ec3\u5206\u7c7b\u5668\u4ee5\u533a\u5206\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u6587\u7ae0\uff0c\u5229\u7528\u5206\u7c7b\u51c6\u786e\u7387\u4f30\u8ba1\u6f5c\u5728\u5185\u5bb9\u5206\u5e03\u7684\u603b\u53d8\u5dee\u8ddd\u79bb\uff0c\u663e\u8457\u7684\u8ddd\u79bb\u53d8\u5316\u5373\u4e3a\u53d8\u70b9\u3002\u6211\u4eec\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u300a\u536b\u62a5\u300b\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u8bc6\u522b\u4e86\u5305\u62ec9/11\u4e8b\u4ef6\u3001COVID-19\u5927\u6d41\u884c\u548c\u603b\u7edf\u9009\u4e3e\u5728\u5185\u7684\u91cd\u5927\u5386\u53f2\u4e8b\u4ef6\u3002\u8be5\u65b9\u6cd5\u5bf9\u9886\u57df\u77e5\u8bc6\u8981\u6c42\u6781\u4f4e\uff0c\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u516c\u5171\u8bdd\u8bed\u7684\u91cd\u5927\u8f6c\u53d8\uff0c\u5e76\u63d0\u4f9b\u5185\u5bb9\u53d8\u5316\u7684\u91cf\u5316\u6307\u6807\uff0c\u5bf9\u65b0\u95fb\u4e1a\u3001\u653f\u7b56\u5206\u6790\u548c\u5371\u673a\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.18397", "pdf": "https://arxiv.org/pdf/2506.18397", "abs": "https://arxiv.org/abs/2506.18397", "authors": ["\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez", "Giorgio Battistelli"], "title": "Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection", "categories": ["cs.CV", "math.ST", "stat.TH"], "comment": null, "summary": "This paper presents the distributed Poisson multi-Bernoulli (PMB) filter\nbased on the generalised covariance intersection (GCI) fusion rule for\ndistributed multi-object filtering. Since the exact GCI fusion of two PMB\ndensities is intractable, we derive a principled approximation. Specifically,\nwe approximate the power of a PMB density as an unnormalised PMB density, which\ncorresponds to an upper bound of the PMB density. Then, the GCI fusion rule\ncorresponds to the normalised product of two unnormalised PMB densities. We\nshow that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be\nexpressed in closed form. Future prediction and update steps in each filter\npreserve the PMBM form, which can be projected back to a PMB density before the\nnext fusion step. Experimental results show the benefits of this approach\ncompared to other distributed multi-object filters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u534f\u65b9\u5dee\u4ea4\u96c6\uff08GCI\uff09\u878d\u5408\u89c4\u5219\u7684\u5206\u5e03\u5f0f\u6cca\u677e\u591a\u4f2f\u52aa\u5229\uff08PMB\uff09\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u591a\u76ee\u6807\u6ee4\u6ce2\u3002\u7531\u4e8e\u7cbe\u786e\u7684GCI\u878d\u5408\u96be\u4ee5\u5b9e\u73b0\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u591a\u76ee\u6807\u6ee4\u6ce2\u5728\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u6027\u548c\u7cbe\u5ea6\u4e0a\u5b58\u5728\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7GCI\u878d\u5408\u89c4\u5219\u6539\u8fdbPMB\u6ee4\u6ce2\u5668\u7684\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5c06PMB\u5bc6\u5ea6\u7684\u5e42\u8fd1\u4f3c\u4e3a\u672a\u5f52\u4e00\u5316\u7684PMB\u5bc6\u5ea6\uff0c\u5e76\u901a\u8fc7GCI\u878d\u5408\u89c4\u5219\u5c06\u5176\u8f6c\u5316\u4e3a\u6cca\u677e\u591a\u4f2f\u52aa\u5229\u6df7\u5408\uff08PMBM\uff09\u5f62\u5f0f\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86PMBM\u7684\u95ed\u5f0f\u8868\u8fbe\uff0c\u5e76\u5728\u6ee4\u6ce2\u6b65\u9aa4\u4e2d\u4fdd\u6301\u5176\u5f62\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u591a\u76ee\u6807\u6ee4\u6ce2\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6ee4\u6ce2\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eGCI\u878d\u5408\u7684\u5206\u5e03\u5f0fPMB\u6ee4\u6ce2\u5668\u4e3a\u591a\u76ee\u6807\u6ee4\u6ce2\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u5e7f\u4e49\u534f\u65b9\u5dee\u4ea4\u96c6\u7684\u5206\u5e03\u5f0f\u6cca\u677e\u591a\u4f2f\u52aa\u5229\u6ee4\u6ce2", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u534f\u65b9\u5dee\u4ea4\u96c6\uff08GCI\uff09\u878d\u5408\u89c4\u5219\u7684\u5206\u5e03\u5f0f\u6cca\u677e\u591a\u4f2f\u52aa\u5229\uff08PMB\uff09\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u591a\u76ee\u6807\u6ee4\u6ce2\u3002\u7531\u4e8e\u4e24\u4e2aPMB\u5bc6\u5ea6\u7684\u7cbe\u786eGCI\u878d\u5408\u96be\u4ee5\u5b9e\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5c06PMB\u5bc6\u5ea6\u7684\u5e42\u8fd1\u4f3c\u4e3a\u672a\u5f52\u4e00\u5316\u7684PMB\u5bc6\u5ea6\uff0c\u8fd9\u5bf9\u5e94\u4e8ePMB\u5bc6\u5ea6\u7684\u4e0a\u754c\u3002\u7136\u540e\uff0cGCI\u878d\u5408\u89c4\u5219\u5bf9\u5e94\u4e8e\u4e24\u4e2a\u672a\u5f52\u4e00\u5316PMB\u5bc6\u5ea6\u7684\u5f52\u4e00\u5316\u4e58\u79ef\u3002\u6211\u4eec\u8bc1\u660e\u7ed3\u679c\u4e3a\u6cca\u677e\u591a\u4f2f\u52aa\u5229\u6df7\u5408\uff08PMBM\uff09\uff0c\u53ef\u4ee5\u95ed\u5f0f\u8868\u8fbe\u3002\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u4e2d\u7684\u672a\u6765\u9884\u6d4b\u548c\u66f4\u65b0\u6b65\u9aa4\u4fdd\u7559\u4e86PMBM\u5f62\u5f0f\uff0c\u53ef\u4ee5\u5728\u4e0b\u4e00\u6b21\u878d\u5408\u6b65\u9aa4\u4e4b\u524d\u5c06\u5176\u6295\u5f71\u56dePMB\u5bc6\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u5206\u5e03\u5f0f\u591a\u76ee\u6807\u6ee4\u6ce2\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.17466", "pdf": "https://arxiv.org/pdf/2506.17466", "abs": "https://arxiv.org/abs/2506.17466", "authors": ["Amitash Nanda", "Sree Bhargavi Balija", "Debashis Sahoo"], "title": "FedNAMs: Performing Interpretability Analysis in Federated Learning Context", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 6 figures", "summary": "Federated learning continues to evolve but faces challenges in\ninterpretability and explainability. To address these challenges, we introduce\na novel approach that employs Neural Additive Models (NAMs) within a federated\nlearning framework. This new Federated Neural Additive Models (FedNAMs)\napproach merges the advantages of NAMs, where individual networks concentrate\non specific input features, with the decentralized approach of federated\nlearning, ultimately producing interpretable analysis results. This integration\nenhances privacy by training on local data across multiple devices, thereby\nminimizing the risks associated with data centralization and improving model\nrobustness and generalizability. FedNAMs maintain detailed, feature-specific\nlearning, making them especially valuable in sectors such as finance and\nhealthcare. They facilitate the training of client-specific models to integrate\nlocal updates, preserve privacy, and mitigate concerns related to\ncentralization. Our studies on various text and image classification tasks,\nusing datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show\nthat FedNAMs deliver strong interpretability with minimal accuracy loss\ncompared to traditional Federated Deep Neural Networks (DNNs). The research\ninvolves notable findings, including the identification of critical predictive\nfeatures at both client and global levels. Volatile acidity, sulfates, and\nchlorides for wine quality. Chest pain type, maximum heart rate, and number of\nvessels for heart disease. Petal length and width for iris classification. This\napproach strengthens privacy and model efficiency and improves interpretability\nand robustness across diverse datasets. Finally, FedNAMs generate insights on\ncauses of highly and low interpretable features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedNAMs\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff08NAMs\uff09\u4e0e\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u7279\u5f81\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff08NAMs\uff09\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u63d0\u51fa\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u63d0\u4f9b\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedNAMs\u7684\u65b9\u6cd5\uff0c\u5c06\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff08NAMs\uff09\u4e0e\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u3002NAMs\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8f93\u5165\u7279\u5f81\u5206\u914d\u72ec\u7acb\u7684\u5b50\u7f51\u7edc\uff0c\u5b9e\u73b0\u7279\u5f81\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cFedNAMs\u5141\u8bb8\u5ba2\u6237\u7aef\u5728\u672c\u5730\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u805a\u5408\u5168\u5c40\u6a21\u578b\u6765\u66f4\u65b0\u53c2\u6570\uff0c\u4ece\u800c\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u4e86OpenFetch ML Wine\u3001UCI Heart Disease\u548cIris\u7b49\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u8868\u660eFedNAMs\u5728\u4fdd\u6301\u4e0e\u4f20\u7edf\u8054\u90a6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u76f8\u8fd1\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u4e86\u591a\u4e2a\u5173\u952e\u9884\u6d4b\u7279\u5f81\uff0c\u5982\u8461\u8404\u9152\u8d28\u91cf\u4e2d\u7684\u6325\u53d1\u6027\u9178\u5ea6\u548c\u786b\u9178\u76d0\u542b\u91cf\uff0c\u5fc3\u810f\u75c5\u9884\u6d4b\u4e2d\u7684\u80f8\u75db\u7c7b\u578b\u548c\u6700\u5927\u5fc3\u7387\u7b49\u3002", "conclusion": "FedNAMs\u6210\u529f\u5730\u5c06\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\u4e0eNAMs\u7684\u53ef\u89e3\u91ca\u6027\u7ed3\u5408\u8d77\u6765\uff0c\u4e3a\u91d1\u878d\u548c\u533b\u7597\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u4e3a\u7279\u5f81\u7ea7\u522b\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "paper_title_zh": "FedNAMs\uff1a\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u6267\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790", "abstract_zh": "\u8054\u90a6\u5b66\u4e60\u4e0d\u65ad\u53d1\u5c55\uff0c\u4f46\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff08NAMs\uff09\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u3002\u8fd9\u79cd\u540d\u4e3a\u8054\u90a6\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff08FedNAMs\uff09\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86NAMs\u7684\u4f18\u52bf\uff08\u6bcf\u4e2a\u5b50\u7f51\u7edc\u4e13\u6ce8\u4e8e\u7279\u5b9a\u8f93\u5165\u7279\u5f81\uff09\u548c\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u70b9\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5206\u6790\u7ed3\u679c\u3002\u8fd9\u79cd\u96c6\u6210\u901a\u8fc7\u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u8bad\u7ec3\u672c\u5730\u6570\u636e\u6765\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u4ece\u800c\u964d\u4f4e\u6570\u636e\u96c6\u4e2d\u5316\u7684\u98ce\u9669\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002FedNAMs\u4fdd\u7559\u4e86\u8be6\u7ec6\u7684\u7279\u5f81\u7279\u5b9a\u5b66\u4e60\uff0c\u4f7f\u5176\u5728\u91d1\u878d\u548c\u533b\u7597\u7b49\u9886\u57df\u5c24\u4e3a\u6709\u4ef7\u503c\u3002\u5b83\u4eec\u652f\u6301\u8bad\u7ec3\u5ba2\u6237\u7aef\u7279\u5b9a\u6a21\u578b\u4ee5\u6574\u5408\u672c\u5730\u66f4\u65b0\uff0c\u4fdd\u62a4\u9690\u79c1\u5e76\u7f13\u89e3\u96c6\u4e2d\u5316\u5e26\u6765\u7684\u95ee\u9898\u3002\u6211\u4eec\u5728\u591a\u79cd\u6587\u672c\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff08\u5982OpenFetch ML Wine\u3001UCI Heart Disease\u548cIris\u6570\u636e\u96c6\uff09\u4e0a\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u8054\u90a6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u76f8\u6bd4\uff0cFedNAMs\u4ee5\u6700\u5c0f\u7684\u51c6\u786e\u7387\u635f\u5931\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u53ef\u89e3\u91ca\u6027\u3002\u7814\u7a76\u8fd8\u5305\u62ec\u91cd\u8981\u53d1\u73b0\uff0c\u4f8b\u5982\u5728\u5ba2\u6237\u7aef\u548c\u5168\u5c40\u5c42\u9762\u8bc6\u522b\u5173\u952e\u9884\u6d4b\u7279\u5f81\uff08\u5982\u8461\u8404\u9152\u8d28\u91cf\u4e2d\u7684\u6325\u53d1\u6027\u9178\u5ea6\u3001\u786b\u9178\u76d0\u548c\u6c2f\u5316\u7269\u542b\u91cf\uff0c\u5fc3\u810f\u75c5\u9884\u6d4b\u4e2d\u7684\u80f8\u75db\u7c7b\u578b\u3001\u6700\u5927\u5fc3\u7387\u548c\u8840\u7ba1\u6570\u91cf\uff0c\u4ee5\u53ca\u9e22\u5c3e\u82b1\u5206\u7c7b\u4e2d\u7684\u82b1\u74e3\u957f\u5ea6\u548c\u5bbd\u5ea6\uff09\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7387\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8de8\u6570\u636e\u96c6\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0cFedNAMs\u751f\u6210\u4e86\u5173\u4e8e\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u4f4e\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u6210\u56e0\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.18414", "pdf": "https://arxiv.org/pdf/2506.18414", "abs": "https://arxiv.org/abs/2506.18414", "authors": ["Ciro Listone", "Aniello Murano"], "title": "Latent Space Analysis for Melanoma Prevention", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, under review", "summary": "Melanoma represents a critical health risk due to its aggressive progression\nand high mortality, underscoring the need for early, interpretable diagnostic\ntools. While deep learning has advanced in skin lesion classification, most\nexisting models provide only binary outputs, offering limited clinical insight.\nThis work introduces a novel approach that extends beyond classification,\nenabling interpretable risk modelling through a Conditional Variational\nAutoencoder. The proposed method learns a structured latent space that captures\nsemantic relationships among lesions, allowing for a nuanced, continuous\nassessment of morphological differences. An SVM is also trained on this\nrepresentation effectively differentiating between benign nevi and melanomas,\ndemonstrating strong and consistent performance. More importantly, the learned\nlatent space supports visual and geometric interpretation of malignancy, with\nthe spatial proximity of a lesion to known melanomas serving as a meaningful\nindicator of risk. This approach bridges predictive performance with clinical\napplicability, fostering early detection, highlighting ambiguous cases, and\nenhancing trust in AI-assisted diagnosis through transparent and interpretable\ndecision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u9ed1\u8272\u7d20\u7624\u7684\u53ef\u89e3\u91ca\u98ce\u9669\u8bc4\u4f30\uff0c\u7ed3\u5408SVM\u5206\u7c7b\u5668\u63d0\u5347\u8bca\u65ad\u6027\u80fd\uff0c\u5e76\u652f\u6301\u89c6\u89c9\u548c\u51e0\u4f55\u89e3\u91ca\u3002", "motivation": "\u9ed1\u8272\u7d20\u7624\u56e0\u5176\u4fb5\u88ad\u6027\u548c\u9ad8\u6b7b\u4ea1\u7387\u6210\u4e3a\u91cd\u5927\u5065\u5eb7\u5a01\u80c1\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u591a\u4e3a\u4e8c\u5143\u5206\u7c7b\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u65e9\u671f\u8bca\u65ad\u548c\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u6355\u6349\u76ae\u80a4\u75c5\u53d8\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u5b9e\u73b0\u8fde\u7eed\u98ce\u9669\u8bc4\u4f30\uff1b\u540c\u65f6\u8bad\u7ec3SVM\u5206\u7c7b\u5668\u533a\u5206\u826f\u6027\u75e3\u548c\u9ed1\u8272\u7d20\u7624\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6f5c\u5728\u7a7a\u95f4\u652f\u6301\u89c6\u89c9\u548c\u51e0\u4f55\u89e3\u91ca\uff0c\u75c5\u53d8\u4e0e\u5df2\u77e5\u9ed1\u8272\u7d20\u7624\u7684\u7a7a\u95f4\u63a5\u8fd1\u6027\u53ef\u4f5c\u4e3a\u98ce\u9669\u6307\u6807\uff0c\u589e\u5f3a\u4e86AI\u8f85\u52a9\u8bca\u65ad\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u9884\u6d4b\u6027\u80fd\u4e0e\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u901a\u8fc7\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u65e9\u671f\u68c0\u6d4b\uff0c\u5e76\u7a81\u51fa\u6a21\u7cca\u75c5\u4f8b\uff0c\u4e3aAI\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u9ed1\u8272\u7d20\u7624\u9884\u9632\u5206\u6790", "abstract_zh": "\u9ed1\u8272\u7d20\u7624\u56e0\u5176\u4fb5\u88ad\u6027\u8fdb\u5c55\u548c\u9ad8\u6b7b\u4ea1\u7387\u6210\u4e3a\u91cd\u5927\u5065\u5eb7\u98ce\u9669\uff0c\u4e9f\u9700\u65e9\u671f\u4e14\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u5de5\u5177\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u591a\u4ec5\u63d0\u4f9b\u4e8c\u5143\u8f93\u51fa\uff0c\u4e34\u5e8a\u6d1e\u5bdf\u6709\u9650\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u8d85\u8d8a\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u5efa\u6a21\u3002\u8be5\u65b9\u6cd5\u5b66\u4e60\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u6355\u6349\u75c5\u53d8\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u652f\u6301\u5bf9\u5f62\u6001\u5dee\u5f02\u7684\u7ec6\u81f4\u8fde\u7eed\u8bc4\u4f30\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u8be5\u8868\u793a\u8bad\u7ec3\u7684SVM\u80fd\u6709\u6548\u533a\u5206\u826f\u6027\u75e3\u548c\u9ed1\u8272\u7d20\u7624\uff0c\u8868\u73b0\u51fa\u7a33\u5065\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b66\u4e60\u7684\u6f5c\u5728\u7a7a\u95f4\u652f\u6301\u5bf9\u6076\u6027\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u89e3\u91ca\uff0c\u75c5\u53d8\u4e0e\u5df2\u77e5\u9ed1\u8272\u7d20\u7624\u7684\u7a7a\u95f4\u63a5\u8fd1\u6027\u53ef\u4f5c\u4e3a\u98ce\u9669\u7684\u6709\u610f\u4e49\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u5c06\u9884\u6d4b\u6027\u80fd\u4e0e\u4e34\u5e8a\u9002\u7528\u6027\u7ed3\u5408\uff0c\u4fc3\u8fdb\u65e9\u671f\u68c0\u6d4b\uff0c\u7a81\u51fa\u6a21\u7cca\u75c5\u4f8b\uff0c\u5e76\u901a\u8fc7\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u589e\u5f3a\u5bf9AI\u8f85\u52a9\u8bca\u65ad\u7684\u4fe1\u4efb\u3002"}}
{"id": "2506.18434", "pdf": "https://arxiv.org/pdf/2506.18434", "abs": "https://arxiv.org/abs/2506.18434", "authors": ["Filippo Ruffini", "Elena Mulero Ayllon", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi"], "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) holds significant promise for improving\nprognosis prediction in medical imaging, yet its effective application remains\nchallenging. In this work, we introduce a structured benchmark explicitly\ndesigned to evaluate and compare the transferability of Convolutional Neural\nNetworks and Foundation Models in predicting clinical outcomes in COVID-19\npatients, leveraging diverse publicly available Chest X-ray datasets. Our\nexperimental methodology extensively explores a wide set of fine-tuning\nstrategies, encompassing traditional approaches such as Full Fine-Tuning and\nLinear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods\nincluding Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were\nconducted across multiple learning paradigms, including both extensive\nfull-data scenarios and more clinically realistic Few-Shot Learning settings,\nwhich are critical for modeling rare disease outcomes and rapidly emerging\nhealth threats. By implementing a large-scale comparative analysis involving a\ndiverse selection of pretrained models, including general-purpose architectures\npretrained on large-scale datasets such as CLIP and DINOv2, to\nbiomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we\nrigorously assess each model's capacity to effectively adapt and generalize to\nprognosis tasks, particularly under conditions of severe data scarcity and\npronounced class imbalance. The benchmark was designed to capture critical\nconditions common in prognosis tasks, including variations in dataset size and\nclass distribution, providing detailed insights into the strengths and\nlimitations of each fine-tuning strategy. This extensive and structured\nevaluation aims to inform the practical deployment and adoption of robust,\nefficient, and generalizable AI-driven solutions in real-world clinical\nprognosis prediction workflows.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u7840\u6a21\u578b\u5728COVID-19\u60a3\u8005\u9884\u540e\u9884\u6d4b\u4e2d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u5fae\u8c03\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4f20\u7edf\u65b9\u6cd5\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u533b\u5b66\u5f71\u50cf\u9884\u540e\u9884\u6d4b\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u9ad8\u6548\u3001\u9c81\u68d2\u7684AI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5229\u7528\u516c\u5f00\u7684\u80f8\u90e8X\u5149\u6570\u636e\u96c6\uff0c\u5bf9\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u6bd4\u8f83\u5206\u6790\u3002\u5b9e\u9a8c\u6db5\u76d6\u4e86\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u5305\u62ec\u5168\u5fae\u8c03\u3001\u7ebf\u6027\u63a2\u6d4b\u4ee5\u53ca\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982\u4f4e\u79e9\u9002\u5e94\u3001BitFit\u3001VeRA\u548cIA3\uff09\uff0c\u5e76\u5728\u5168\u6570\u636e\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u548cDINOv2\uff09\u5728\u9884\u540e\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u533b\u5b66\u5f71\u50cf\u9884\u540e\u9884\u6d4b\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u5fae\u8c03\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u9ad8\u6548\u3001\u9c81\u68d2\u7684AI\u89e3\u51b3\u65b9\u6848\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "\u57fa\u7840\u6a21\u578b\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u533b\u5b66\u5f71\u50cf\u9884\u540e\u9884\u6d4b\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u6539\u5584\u533b\u5b66\u5f71\u50cf\u9884\u540e\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u6709\u6548\u5e94\u7528\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u548c\u6bd4\u8f83\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4bCOVID-19\u60a3\u8005\u4e34\u5e8a\u7ed3\u5c40\u4e2d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5229\u7528\u4e86\u591a\u79cd\u516c\u5f00\u7684\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u65b9\u6cd5\u5e7f\u6cdb\u63a2\u7d22\u4e86\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u5168\u5fae\u8c03\u548c\u7ebf\u6027\u63a2\u6d4b\uff0c\u4ee5\u53ca\u5148\u8fdb\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982\u4f4e\u79e9\u9002\u5e94\u3001BitFit\u3001VeRA\u548cIA3\uff09\u3002\u8bc4\u4f30\u5728\u591a\u79cd\u5b66\u4e60\u8303\u5f0f\u4e0b\u8fdb\u884c\uff0c\u5305\u62ec\u5168\u6570\u636e\u573a\u666f\u548c\u66f4\u7b26\u5408\u4e34\u5e8a\u5b9e\u9645\u7684\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\uff0c\u8fd9\u5bf9\u5efa\u6a21\u7f55\u89c1\u75be\u75c5\u7ed3\u5c40\u548c\u5feb\u901f\u51fa\u73b0\u7684\u5065\u5eb7\u5a01\u80c1\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u6bd4\u8f83\u5206\u6790\uff0c\u6db5\u76d6\u4e86\u4ece\u901a\u7528\u67b6\u6784\uff08\u5982CLIP\u548cDINOv2\uff09\u5230\u751f\u7269\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff08\u5982MedCLIP\u3001BioMedCLIP\u548cPubMedCLIP\uff09\u7684\u591a\u6837\u5316\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6211\u4eec\u4e25\u683c\u8bc4\u4f30\u4e86\u6bcf\u4e2a\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u9002\u5e94\u548c\u6cdb\u5316\u5230\u9884\u540e\u4efb\u52a1\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u65e8\u5728\u6355\u6349\u9884\u540e\u4efb\u52a1\u4e2d\u5e38\u89c1\u7684\u5173\u952e\u6761\u4ef6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u89c4\u6a21\u548c\u7c7b\u522b\u5206\u5e03\u7684\u53d8\u5316\uff0c\u4e3a\u6bcf\u79cd\u5fae\u8c03\u7b56\u7565\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u8be6\u7ec6\u89c1\u89e3\u3002\u8fd9\u4e00\u5168\u9762\u4e14\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u65e8\u5728\u4e3a\u5b9e\u9645\u4e34\u5e8a\u9884\u540e\u9884\u6d4b\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u7684\u90e8\u7f72\u548c\u91c7\u7528\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2506.17486", "pdf": "https://arxiv.org/pdf/2506.17486", "abs": "https://arxiv.org/abs/2506.17486", "authors": ["Zachary Ravichandran", "Ignacio Hounie", "Fernando Cladera", "Alejandro Ribeiro", "George J. Pappas", "Vijay Kumar"], "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) provide robots with powerful contextual\nreasoning abilities and a natural human interface. Yet, current LLM-enabled\nrobots typically depend on cloud-hosted models, limiting their usability in\nenvironments with unreliable communication infrastructure, such as outdoor or\nindustrial settings. We present PRISM, a framework for distilling small\nlanguage model (SLM)-enabled robot planners that run on-device with minimal\nhuman supervision. Starting from an existing LLM-enabled planner, PRISM\nautomatically synthesizes diverse tasks and environments, elicits plans from\nthe LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in\nreplacement of the source model. We apply PRISM to three LLM-enabled planners\nfor mapping and exploration, manipulation, and household assistance, and we\ndemonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of\nGPT-4o's performance to over 93% - using only synthetic data. We further\ndemonstrate that the distilled planners generalize across heterogeneous robotic\nplatforms (ground and aerial) and diverse environments (indoor and outdoor). We\nrelease all software, trained models, and datasets at\nhttps://zacravichandran.github.io/PRISM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPRISM\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u4efb\u52a1\u548c\u73af\u5883\u6570\u636e\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u84b8\u998f\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u5b9e\u73b0\u8bbe\u5907\u7aef\u8fd0\u884c\u7684\u673a\u5668\u4eba\u89c4\u5212\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4o\u768493%\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u5f53\u524d\u4f9d\u8d56\u4e91\u7aef\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u673a\u5668\u4eba\u89c4\u5212\u5728\u901a\u4fe1\u4e0d\u53ef\u9760\u7684\u73af\u5883\uff08\u5982\u6237\u5916\u6216\u5de5\u4e1a\u573a\u666f\uff09\u4e2d\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8bbe\u5907\u7aef\u8fd0\u884c\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u51cf\u5c11\u5bf9\u4e91\u7aef\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u5347\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\u3002", "method": "PRISM\u6846\u67b6\u4ece\u73b0\u6709LLM\u89c4\u5212\u5668\u51fa\u53d1\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u548c\u73af\u5883\u6570\u636e\uff0c\u5229\u7528LLM\u751f\u6210\u89c4\u5212\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u84b8\u998f\u51fa\u7d27\u51d1\u7684SLM\uff0c\u4f5c\u4e3aLLM\u7684\u76f4\u63a5\u66ff\u4ee3\u3002", "result": "PRISM\u5c06Llama-3.2-3B\u7684\u6027\u80fd\u4eceGPT-4o\u768410-20%\u63d0\u5347\u81f393%\u4ee5\u4e0a\uff0c\u4e14\u84b8\u998f\u540e\u7684\u89c4\u5212\u5668\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5730\u9762\u548c\u7a7a\u4e2d\uff09\u53ca\u591a\u6837\u5316\u73af\u5883\uff08\u5ba4\u5185\u548c\u6237\u5916\uff09\u3002", "conclusion": "PRISM\u901a\u8fc7\u5408\u6210\u6570\u636e\u84b8\u998fSLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u5907\u7aef\u673a\u5668\u4eba\u89c4\u5212\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\u5728\u8bbe\u5907\u7aef\u84b8\u998f\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u673a\u5668\u4eba\u89c4\u5212", "abstract_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u548c\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u3002\u7136\u800c\uff0c\u76ee\u524d\u4f9d\u8d56\u4e91\u7aefLLM\u7684\u673a\u5668\u4eba\u5728\u901a\u4fe1\u4e0d\u53ef\u9760\u7684\u73af\u5883\uff08\u5982\u6237\u5916\u6216\u5de5\u4e1a\u573a\u666f\uff09\u4e2d\u5b9e\u7528\u6027\u53d7\u9650\u3002\u6211\u4eec\u63d0\u51fa\u4e86PRISM\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u5de5\u76d1\u7763\uff0c\u84b8\u998f\u51fa\u53ef\u5728\u8bbe\u5907\u7aef\u8fd0\u884c\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u89c4\u5212\u5668\u3002PRISM\u4ece\u73b0\u6709LLM\u89c4\u5212\u5668\u51fa\u53d1\uff0c\u81ea\u52a8\u5408\u6210\u591a\u6837\u5316\u4efb\u52a1\u548c\u73af\u5883\u6570\u636e\uff0c\u5229\u7528LLM\u751f\u6210\u89c4\u5212\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u84b8\u998f\u51fa\u7d27\u51d1\u7684SLM\u4f5c\u4e3a\u76f4\u63a5\u66ff\u4ee3\u3002\u6211\u4eec\u5c06PRISM\u5e94\u7528\u4e8e\u4e09\u79cdLLM\u89c4\u5212\u5668\uff08\u7528\u4e8e\u5730\u56fe\u63a2\u7d22\u3001\u64cd\u4f5c\u548c\u5bb6\u5ead\u8f85\u52a9\uff09\uff0c\u7ed3\u679c\u663e\u793aPRISM\u5c06Llama-3.2-3B\u7684\u6027\u80fd\u4eceGPT-4o\u768410-20%\u63d0\u5347\u81f393%\u4ee5\u4e0a\uff0c\u4e14\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u3002\u6b64\u5916\uff0c\u84b8\u998f\u540e\u7684\u89c4\u5212\u5668\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5730\u9762\u548c\u7a7a\u4e2d\uff09\u53ca\u591a\u6837\u5316\u73af\u5883\uff08\u5ba4\u5185\u548c\u6237\u5916\uff09\u3002\u6240\u6709\u8f6f\u4ef6\u3001\u8bad\u7ec3\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u53d1\u5e03\u4e8ehttps://zacravichandran.github.io/PRISM\u3002"}}
{"id": "2506.18843", "pdf": "https://arxiv.org/pdf/2506.18843", "abs": "https://arxiv.org/abs/2506.18843", "authors": ["Heng-Jui Chang", "Saurabhchand Bhati", "James Glass", "Alexander H. Liu"], "title": "USAD: Universal Speech and Audio Representation via Distillation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Preprint", "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUSAD\u7684\u7edf\u4e00\u97f3\u9891\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5c06\u8bed\u97f3\u3001\u58f0\u97f3\u548c\u97f3\u4e50\u7b49\u591a\u79cd\u97f3\u9891\u7c7b\u578b\u6574\u5408\u5230\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u7684\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u591a\u4e3a\u9886\u57df\u4e13\u7528\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8bed\u97f3\u548c\u975e\u8bed\u97f3\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7USAD\u65b9\u6cd5\uff0c\u6253\u7834\u9886\u57df\u9650\u5236\uff0c\u5b9e\u73b0\u901a\u7528\u97f3\u9891\u8868\u793a\u5b66\u4e60\u3002", "method": "USAD\u91c7\u7528\u5c42\u5230\u5c42\u84b8\u998f\u6280\u672f\uff0c\u4ece\u9886\u57df\u4e13\u7528SSL\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5305\u542b\u591a\u79cd\u97f3\u9891\u7c7b\u578b\u7684\u7efc\u5408\u6570\u636e\u96c6\u3002", "result": "USAD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u5e27\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u3001\u97f3\u9891\u6807\u8bb0\u548c\u58f0\u97f3\u5206\u7c7b\uff0c\u5728SUPERB\u548cHEAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "USAD\u901a\u8fc7\u7edf\u4e00\u6a21\u578b\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u97f3\u9891\u8868\u793a\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u7ade\u4e89\u529b\u548c\u901a\u7528\u6027\u3002", "paper_title_zh": "USAD\uff1a\u901a\u8fc7\u84b8\u998f\u5b9e\u73b0\u901a\u7528\u8bed\u97f3\u548c\u97f3\u9891\u8868\u793a", "abstract_zh": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u97f3\u9891\u8868\u793a\uff0c\u4f46\u6a21\u578b\u901a\u5e38\u4ecd\u662f\u9886\u57df\u4e13\u7528\u7684\uff0c\u4e13\u6ce8\u4e8e\u8bed\u97f3\u6216\u975e\u8bed\u97f3\u4efb\u52a1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u901a\u7528\u8bed\u97f3\u548c\u97f3\u9891\u84b8\u998f\uff08USAD\uff09\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u97f3\u9891\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u8bed\u97f3\u3001\u58f0\u97f3\u548c\u97f3\u4e50\u7b49\u591a\u79cd\u97f3\u9891\u7c7b\u578b\u6574\u5408\u5230\u4e00\u4e2a\u6a21\u578b\u4e2d\u3002USAD\u91c7\u7528\u9ad8\u6548\u7684\u5c42\u5230\u5c42\u84b8\u998f\u6280\u672f\uff0c\u4ece\u9886\u57df\u4e13\u7528SSL\u6a21\u578b\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u7efc\u5408\u97f3\u9891\u6570\u636e\u96c6\u3002USAD\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u5e27\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u3001\u97f3\u9891\u6807\u8bb0\u548c\u58f0\u97f3\u5206\u7c7b\uff0c\u5728SUPERB\u548cHEAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2506.18437", "pdf": "https://arxiv.org/pdf/2506.18437", "abs": "https://arxiv.org/abs/2506.18437", "authors": ["Sijin He", "Guangfeng Lin", "Tao Li", "Yajun Chen"], "title": "Frequency-Domain Fusion Transformer for Image Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting plays a vital role in restoring missing image regions and\nsupporting high-level vision tasks, but traditional methods struggle with\ncomplex textures and large occlusions. Although Transformer-based approaches\nhave demonstrated strong global modeling capabilities, they often fail to\npreserve high-frequency details due to the low-pass nature of self-attention\nand suffer from high computational costs. To address these challenges, this\npaper proposes a Transformer-based image inpainting method incorporating\nfrequency-domain fusion. Specifically, an attention mechanism combining wavelet\ntransform and Gabor filtering is introduced to enhance multi-scale structural\nmodeling and detail preservation. Additionally, a learnable frequency-domain\nfilter based on the fast Fourier transform is designed to replace the\nfeedforward network, enabling adaptive noise suppression and detail retention.\nThe model adopts a four-level encoder-decoder structure and is guided by a\nnovel loss strategy to balance global semantics and fine details. Experimental\nresults demonstrate that the proposed method effectively improves the quality\nof image inpainting by preserving more high-frequency information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u57df\u878d\u5408\u589e\u5f3a\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7eb9\u7406\u548c\u5927\u906e\u6321\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7eb9\u7406\u548c\u5927\u906e\u6321\uff0c\u800c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u867d\u5177\u6709\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9891\u57df\u878d\u5408\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548cGabor\u6ee4\u6ce2\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u591a\u5c3a\u5ea6\u7ed3\u6784\u5efa\u6a21\u548c\u7ec6\u8282\u4fdd\u7559\uff1b\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u7684\u53ef\u5b66\u4e60\u9891\u57df\u6ee4\u6ce2\u5668\uff0c\u66ff\u4ee3\u524d\u9988\u7f51\u7edc\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u566a\u58f0\u6291\u5236\u548c\u7ec6\u8282\u4fdd\u7559\uff1b\u91c7\u7528\u56db\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u635f\u5931\u7b56\u7565\u5e73\u8861\u5168\u5c40\u8bed\u4e49\u548c\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559\u66f4\u591a\u9ad8\u9891\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9891\u57df\u878d\u5408Transformer\u65b9\u6cd5\u5728\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5168\u5c40\u8bed\u4e49\u548c\u9ad8\u9891\u7ec6\u8282\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u9891\u57df\u878d\u5408Transformer\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5", "abstract_zh": "\u56fe\u50cf\u4fee\u590d\u5728\u6062\u590d\u7f3a\u5931\u56fe\u50cf\u533a\u57df\u548c\u652f\u6301\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7eb9\u7406\u548c\u5927\u906e\u6321\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u4f4e\u901a\u7279\u6027\uff0c\u5b83\u4eec\u5f80\u5f80\u96be\u4ee5\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9891\u57df\u878d\u5408\u7684Transformer\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548cGabor\u6ee4\u6ce2\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u591a\u5c3a\u5ea6\u7ed3\u6784\u5efa\u6a21\u548c\u7ec6\u8282\u4fdd\u7559\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u7684\u53ef\u5b66\u4e60\u9891\u57df\u6ee4\u6ce2\u5668\uff0c\u66ff\u4ee3\u524d\u9988\u7f51\u7edc\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u566a\u58f0\u6291\u5236\u548c\u7ec6\u8282\u4fdd\u7559\u3002\u6a21\u578b\u91c7\u7528\u56db\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u635f\u5931\u7b56\u7565\u5e73\u8861\u5168\u5c40\u8bed\u4e49\u548c\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559\u66f4\u591a\u9ad8\u9891\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\u3002"}}
{"id": "2506.17491", "pdf": "https://arxiv.org/pdf/2506.17491", "abs": "https://arxiv.org/abs/2506.17491", "authors": ["Hao Peng", "Steve Jiang", "Robert Timmerman"], "title": "Exploring Strategies for Personalized Radiation Therapy Part II Predicting Tumor Drift Patterns with Diffusion Models", "categories": ["physics.med-ph", "cs.AI"], "comment": null, "summary": "Radiation therapy outcomes are decided by two key parameters, dose and\ntiming, whose best values vary substantially across patients. This variability\nis especially critical in the treatment of brain cancer, where fractionated or\nstaged stereotactic radiosurgery improves safety compared to single fraction\napproaches, but complicates the ability to predict treatment response. To\naddress this challenge, we employ Personalized Ultra-fractionated Stereotactic\nAdaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment\nbased on how each tumor evolves over time. However, the success of PULSAR and\nother adaptive approaches depends on predictive tools that can guide early\ntreatment decisions and avoid both overtreatment and undertreatment. However,\ncurrent radiomics and dosiomics models offer limited insight into the evolving\nspatial and temporal patterns of tumor response. To overcome these limitations,\nwe propose a novel framework using Denoising Diffusion Implicit Models (DDIM),\nwhich learns data-driven mappings from pre to post treatment imaging. In this\nstudy, we developed single step and iterative denoising strategies and compared\ntheir performance. The results show that diffusion models can effectively\nsimulate patient specific tumor evolution and localize regions associated with\ntreatment response. The proposed strategy provides a promising foundation for\nmodeling heterogeneous treatment response and enabling early, adaptive\ninterventions, paving the way toward more personalized and biologically\ninformed radiotherapy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b\uff08DDIM\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u8111\u764c\u653e\u7597\u4e2d\u80bf\u7624\u7684\u52a8\u6001\u6f14\u53d8\uff0c\u652f\u6301\u4e2a\u6027\u5316\u8d85\u5206\u5272\u7acb\u4f53\u5b9a\u5411\u81ea\u9002\u5e94\u653e\u7597\uff08PULSAR\uff09\u7684\u65e9\u671f\u51b3\u7b56\u3002", "motivation": "\u8111\u764c\u653e\u7597\u7684\u5242\u91cf\u548c\u65f6\u95f4\u53c2\u6570\u56e0\u60a3\u8005\u800c\u5f02\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9884\u6d4b\u80bf\u7624\u7684\u52a8\u6001\u54cd\u5e94\u3002PULSAR\u7b56\u7565\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u9884\u6d4b\u5de5\u5177\u4ee5\u907f\u514d\u8fc7\u5ea6\u6216\u4e0d\u8db3\u6cbb\u7597\u3002", "method": "\u91c7\u7528\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b\uff08DDIM\uff09\uff0c\u4ece\u6cbb\u7597\u524d\u540e\u7684\u5f71\u50cf\u6570\u636e\u4e2d\u5b66\u4e60\u80bf\u7624\u6f14\u53d8\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5f00\u53d1\u4e86\u5355\u6b65\u548c\u8fed\u4ee3\u53bb\u566a\u7b56\u7565\u5e76\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u60a3\u8005\u7279\u5f02\u6027\u80bf\u7624\u6f14\u53d8\uff0c\u5e76\u5b9a\u4f4d\u4e0e\u6cbb\u7597\u54cd\u5e94\u76f8\u5173\u7684\u533a\u57df\uff0c\u4e3a\u4e2a\u6027\u5316\u653e\u7597\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5efa\u6a21\u5f02\u8d28\u6027\u6cbb\u7597\u54cd\u5e94\u548c\u65e9\u671f\u81ea\u9002\u5e94\u5e72\u9884\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u66f4\u4e2a\u6027\u5316\u548c\u751f\u7269\u5b66\u5bfc\u5411\u7684\u653e\u7597\u53d1\u5c55\u3002", "paper_title_zh": "\u63a2\u7d22\u4e2a\u6027\u5316\u653e\u7597\u7b56\u7565\u7b2c\u4e8c\u90e8\u5206\uff1a\u5229\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u80bf\u7624\u6f02\u79fb\u6a21\u5f0f", "abstract_zh": "\u653e\u7597\u6548\u679c\u7531\u5242\u91cf\u548c\u65f6\u95f4\u4e24\u4e2a\u5173\u952e\u53c2\u6570\u51b3\u5b9a\uff0c\u5176\u6700\u4f73\u503c\u56e0\u60a3\u8005\u5dee\u5f02\u663e\u8457\u3002\u5728\u8111\u764c\u6cbb\u7597\u4e2d\uff0c\u5206\u9636\u6bb5\u7acb\u4f53\u5b9a\u5411\u653e\u5c04\u5916\u79d1\u6bd4\u5355\u6b21\u6cbb\u7597\u66f4\u5b89\u5168\uff0c\u4f46\u589e\u52a0\u4e86\u9884\u6d4b\u6cbb\u7597\u54cd\u5e94\u7684\u96be\u5ea6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u91c7\u7528\u4e2a\u6027\u5316\u8d85\u5206\u5272\u7acb\u4f53\u5b9a\u5411\u81ea\u9002\u5e94\u653e\u7597\uff08PULSAR\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6cbb\u7597\u65b9\u6848\u4ee5\u9002\u5e94\u80bf\u7624\u7684\u6f14\u53d8\u3002\u7136\u800c\uff0cPULSAR\u7b49\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u80fd\u6307\u5bfc\u65e9\u671f\u51b3\u7b56\u7684\u9884\u6d4b\u5de5\u5177\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u6216\u4e0d\u8db3\u6cbb\u7597\u3002\u5f53\u524d\u653e\u5c04\u7ec4\u5b66\u548c\u5242\u91cf\u7ec4\u5b66\u6a21\u578b\u5bf9\u80bf\u7624\u54cd\u5e94\u7684\u65f6\u7a7a\u6f14\u53d8\u6a21\u5f0f\u63d0\u4f9b\u6709\u9650\u4fe1\u606f\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b\uff08DDIM\uff09\u7684\u65b0\u6846\u67b6\uff0c\u4ece\u6cbb\u7597\u524d\u540e\u5f71\u50cf\u4e2d\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684\u6620\u5c04\u5173\u7cfb\u3002\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u5355\u6b65\u548c\u8fed\u4ee3\u53bb\u566a\u7b56\u7565\u5e76\u6bd4\u8f83\u5176\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u60a3\u8005\u7279\u5f02\u6027\u80bf\u7624\u6f14\u53d8\uff0c\u5e76\u5b9a\u4f4d\u4e0e\u6cbb\u7597\u54cd\u5e94\u76f8\u5173\u7684\u533a\u57df\u3002\u8be5\u7b56\u7565\u4e3a\u5efa\u6a21\u5f02\u8d28\u6027\u6cbb\u7597\u54cd\u5e94\u548c\u5b9e\u73b0\u65e9\u671f\u81ea\u9002\u5e94\u5e72\u9884\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u66f4\u4e2a\u6027\u5316\u548c\u751f\u7269\u5b66\u5bfc\u5411\u7684\u653e\u7597\u53d1\u5c55\u3002"}}
{"id": "2506.18871", "pdf": "https://arxiv.org/pdf/2506.18871", "abs": "https://arxiv.org/abs/2506.18871", "authors": ["Chenyuan Wu", "Pengfei Zheng", "Ruiran Yan", "Shitao Xiao", "Xin Luo", "Yueze Wang", "Wanli Li", "Xiyan Jiang", "Yexin Liu", "Junjie Zhou", "Ze Liu", "Ziyi Xia", "Chaofan Li", "Haoge Deng", "Jiahao Wang", "Kun Luo", "Bo Zhang", "Defu Lian", "Xinlong Wang", "Zhongyuan Wang", "Tiejun Huang", "Zheng Liu"], "title": "OmniGen2: Exploration to Advanced Multimodal Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2", "AI": {"tldr": "OmniGen2\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u5f00\u6e90\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u7b49\u4efb\u52a1\u3002\u901a\u8fc7\u5206\u79bb\u7684\u6587\u672c\u548c\u56fe\u50cf\u89e3\u7801\u8def\u5f84\u53ca\u89e3\u8026\u7684\u56fe\u50cf\u6807\u8bb0\u5668\uff0c\u5b83\u5728\u4fdd\u6301\u6587\u672c\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002\u8bad\u7ec3\u4e2d\u5f15\u5165\u4e86\u53cd\u5c04\u673a\u5236\u548c\u4e13\u7528\u6570\u636e\u96c6\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u5728\u7edf\u4e00\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\u4efb\u52a1\u65f6\u5b58\u5728\u6027\u80fd\u6298\u8877\u3002OmniGen2\u65e8\u5728\u901a\u8fc7\u5206\u79bb\u7684\u89e3\u7801\u8def\u5f84\u548c\u4e13\u7528\u8bbe\u8ba1\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u91cd\u65b0\u9002\u5e94VAE\u8f93\u5165\u3002", "method": "OmniGen2\u91c7\u7528\u5206\u79bb\u7684\u6587\u672c\u548c\u56fe\u50cf\u89e3\u7801\u8def\u5f84\uff0c\u4f7f\u7528\u975e\u5171\u4eab\u53c2\u6570\u548c\u89e3\u8026\u7684\u56fe\u50cf\u6807\u8bb0\u5668\u3002\u8bad\u7ec3\u4e2d\u5f00\u53d1\u4e86\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u5e76\u5f15\u5165\u53cd\u5c04\u673a\u5236\u548c\u4e13\u7528\u6570\u636e\u96c6\u3002\u65b0\u57fa\u51c6OmniContext\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\u3002", "result": "OmniGen2\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u65b0\u57fa\u51c6OmniContext\u4e0a\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u6700\u5148\u8fdb\u4e00\u81f4\u6027\u3002\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u652f\u6301\u7814\u7a76\u3002", "conclusion": "OmniGen2\u901a\u8fc7\u5206\u79bb\u89e3\u7801\u8def\u5f84\u548c\u4e13\u7528\u8bbe\u8ba1\uff0c\u5728\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u652f\u6301\u3002", "paper_title_zh": "OmniGen2\uff1a\u63a2\u7d22\u9ad8\u7ea7\u591a\u6a21\u6001\u751f\u6210", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86OmniGen2\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u529f\u80fd\u5f00\u6e90\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u7b49\u591a\u6837\u5316\u4efb\u52a1\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u4e0eOmniGen v1\u4e0d\u540c\uff0cOmniGen2\u91c7\u7528\u5206\u79bb\u7684\u6587\u672c\u548c\u56fe\u50cf\u89e3\u7801\u8def\u5f84\uff0c\u4f7f\u7528\u975e\u5171\u4eab\u53c2\u6570\u548c\u89e3\u8026\u7684\u56fe\u50cf\u6807\u8bb0\u5668\u3002\u8fd9\u4e00\u8bbe\u8ba1\u4f7fOmniGen2\u80fd\u591f\u57fa\u4e8e\u73b0\u6709\u591a\u6a21\u6001\u7406\u89e3\u6a21\u578b\u6784\u5efa\uff0c\u65e0\u9700\u91cd\u65b0\u9002\u5e94VAE\u8f93\u5165\uff0c\u4ece\u800c\u4fdd\u7559\u539f\u59cb\u6587\u672c\u751f\u6210\u80fd\u529b\u3002\u4e3a\u652f\u6301OmniGen2\u7684\u8bad\u7ec3\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u6db5\u76d6\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u6570\u636e\u7684\u7efc\u5408\u6570\u636e\u6784\u5efa\u7ba1\u9053\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u5b9a\u5236\u4e86\u53cd\u5c04\u673a\u5236\uff0c\u5e76\u57fa\u4e8eOmniGen2\u7b56\u5212\u4e86\u4e13\u7528\u53cd\u5c04\u6570\u636e\u96c6\u3002\u5c3d\u7ba1\u53c2\u6570\u89c4\u6a21\u76f8\u5bf9\u8f83\u5c0f\uff0cOmniGen2\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u591a\u9879\u4efb\u52a1\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7ed3\u679c\u3002\u4e3a\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e0a\u4e0b\u6587\u751f\u6210\uff08\u4e5f\u79f0\u4e3a\u4e3b\u9898\u9a71\u52a8\u4efb\u52a1\uff09\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u540d\u4e3aOmniContext\u7684\u65b0\u57fa\u51c6\u3002\u5728\u4e00\u81f4\u6027\u65b9\u9762\uff0cOmniGen2\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6a21\u578b\u3001\u8bad\u7ec3\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u4ee5\u652f\u6301\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://vectorspacelab.github.io/OmniGen2\uff1bGitHub\u94fe\u63a5\uff1ahttps://github.com/VectorSpaceLab/OmniGen2"}}
{"id": "2506.18438", "pdf": "https://arxiv.org/pdf/2506.18438", "abs": "https://arxiv.org/abs/2506.18438", "authors": ["Dinh-Khoi Vo", "Thanh-Toan Do", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Editing natural images using textual descriptions in text-to-image diffusion\nmodels remains a significant challenge, particularly in achieving consistent\ngeneration and handling complex, non-rigid objects. Existing methods often\nstruggle to preserve textures and identity, require extensive fine-tuning, and\nexhibit limitations in editing specific spatial regions or objects while\nretaining background details. This paper proposes Context-Preserving Adaptive\nManipulation (CPAM), a novel zero-shot framework for complicated, non-rigid\nreal image editing. Specifically, we propose a preservation adaptation module\nthat adjusts self-attention mechanisms to preserve and independently control\nthe object and background effectively. This ensures that the objects' shapes,\ntextures, and identities are maintained while keeping the background\nundistorted during the editing process using the mask guidance technique.\nAdditionally, we develop a localized extraction module to mitigate the\ninterference with the non-desired modified regions during conditioning in\ncross-attention mechanisms. We also introduce various mask-guidance strategies\nto facilitate diverse image manipulation tasks in a simple manner. Extensive\nexperiments on our newly constructed Image Manipulation BenchmArk (IMBA), a\nrobust benchmark dataset specifically designed for real image editing,\ndemonstrate that our proposed method is the preferred choice among human\nraters, outperforming existing state-of-the-art editing techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPAM\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u975e\u521a\u6027\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u5c40\u90e8\u63d0\u53d6\u6a21\u5757\uff0c\u6709\u6548\u4fdd\u7559\u5bf9\u8c61\u548c\u80cc\u666f\u7ec6\u8282\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684IMBA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u7f16\u8f91\u81ea\u7136\u56fe\u50cf\u65f6\u96be\u4ee5\u4fdd\u6301\u7eb9\u7406\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e14\u9700\u8981\u5927\u91cf\u5fae\u8c03\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u590d\u6742\u975e\u521a\u6027\u5bf9\u8c61\u7684\u7f16\u8f91\u3002", "method": "CPAM\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u4fdd\u62a4\u9002\u5e94\u6a21\u5757\uff0c\u901a\u8fc7\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u673a\u5236\u72ec\u7acb\u63a7\u5236\u5bf9\u8c61\u548c\u80cc\u666f\uff1b2) \u5c40\u90e8\u63d0\u53d6\u6a21\u5757\uff0c\u51cf\u5c11\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5bf9\u975e\u76ee\u6807\u533a\u57df\u7684\u5e72\u6270\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u591a\u79cd\u63a9\u7801\u5f15\u5bfc\u7b56\u7565\u4ee5\u7b80\u5316\u591a\u6837\u5316\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002", "result": "\u5728\u4e13\u95e8\u8bbe\u8ba1\u7684IMBA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCPAM\u65b9\u6cd5\u5728\u4eba\u7c7b\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u4fdd\u7559\u5bf9\u8c61\u5f62\u72b6\u3001\u7eb9\u7406\u548c\u80cc\u666f\u7ec6\u8282\u3002", "conclusion": "CPAM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u6837\u672c\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u548c\u5c40\u90e8\u63d0\u53d6\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u975e\u521a\u6027\u5bf9\u8c61\u7f16\u8f91\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "CPAM\uff1a\u4fdd\u7559\u4e0a\u4e0b\u6587\u7684\u81ea\u9002\u5e94\u64cd\u4f5c\u96f6\u6837\u672c\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91", "abstract_zh": "\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7f16\u8f91\u81ea\u7136\u56fe\u50cf\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u73b0\u4e00\u81f4\u751f\u6210\u548c\u5904\u7406\u590d\u6742\u975e\u521a\u6027\u5bf9\u8c61\u65b9\u9762\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u4fdd\u6301\u7eb9\u7406\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9700\u8981\u5927\u91cf\u5fae\u8c03\uff0c\u5e76\u4e14\u5728\u7f16\u8f91\u7279\u5b9a\u7a7a\u95f4\u533a\u57df\u6216\u5bf9\u8c61\u65f6\u4fdd\u7559\u80cc\u666f\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4fdd\u7559\u4e0a\u4e0b\u6587\u7684\u81ea\u9002\u5e94\u64cd\u4f5c\uff08CPAM\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u590d\u6742\u975e\u521a\u6027\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u7684\u65b0\u578b\u96f6\u6837\u672c\u6846\u67b6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u9002\u5e94\u6a21\u5757\uff0c\u901a\u8fc7\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u4fdd\u7559\u5e76\u72ec\u7acb\u63a7\u5236\u5bf9\u8c61\u548c\u80cc\u666f\u3002\u8fd9\u786e\u4fdd\u4e86\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u63a9\u7801\u5f15\u5bfc\u6280\u672f\u65f6\uff0c\u5bf9\u8c61\u7684\u5f62\u72b6\u3001\u7eb9\u7406\u548c\u8eab\u4efd\u5f97\u4ee5\u4fdd\u6301\uff0c\u540c\u65f6\u80cc\u666f\u4e0d\u53d7\u5e72\u6270\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u5c40\u90e8\u63d0\u53d6\u6a21\u5757\uff0c\u4ee5\u51cf\u5c11\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5bf9\u975e\u76ee\u6807\u4fee\u6539\u533a\u57df\u7684\u5e72\u6270\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u591a\u79cd\u63a9\u7801\u5f15\u5bfc\u7b56\u7565\uff0c\u4ee5\u7b80\u5355\u7684\u65b9\u5f0f\u4fc3\u8fdb\u591a\u6837\u5316\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002\u5728\u6211\u4eec\u65b0\u6784\u5efa\u7684\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6570\u636e\u96c6\uff08IMBA\uff09\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u4eba\u7c7b\u8bc4\u5206\u8005\u7684\u9996\u9009\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7f16\u8f91\u6280\u672f\u3002"}}
{"id": "2506.17497", "pdf": "https://arxiv.org/pdf/2506.17497", "abs": "https://arxiv.org/abs/2506.17497", "authors": ["Mingyang Yao", "Ke Chen"], "title": "From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Proceedings of the 6th Conference on AI Music Creativity, AIMC 2025", "summary": "Despite progress in controllable symbolic music generation, data scarcity\nremains a challenge for certain control modalities. Composer-style music\ngeneration is a prime example, as only a few pieces per composer are available,\nlimiting the modeling of both styles and fundamental music elements (e.g.,\nmelody, chord, rhythm). In this paper, we investigate how general music\nknowledge learned from a broad corpus can enhance the mastery of specific\ncomposer styles, with a focus on piano piece generation. Our approach follows a\ntwo-stage training paradigm. First, we pre-train a REMI-based music generation\nmodel on a large corpus of pop, folk, and classical music. Then, we fine-tune\nit on a small, human-verified dataset from four renowned composers, namely\nBach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to\ncondition the model on style indicators. To evaluate the effectiveness of our\napproach, we conduct both objective and subjective evaluations on style\naccuracy and musicality. Experimental results demonstrate that our method\noutperforms ablations and baselines, achieving more precise composer-style\nmodeling and better musical aesthetics. Additionally, we provide observations\non how the model builds music concepts from the generality pre-training and\nrefines its stylistic understanding through the mastery fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u4ece\u901a\u7528\u97f3\u4e50\u77e5\u8bc6\u5230\u7279\u5b9a\u4f5c\u66f2\u5bb6\u98ce\u683c\u7684\u7cbe\u786e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u94a2\u7434\u66f2\u7684\u98ce\u683c\u51c6\u786e\u6027\u548c\u97f3\u4e50\u7f8e\u611f\u3002", "motivation": "\u5f53\u524d\u53ef\u63a7\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u4e2d\uff0c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u9650\u5236\u4e86\u7279\u5b9a\u4f5c\u66f2\u5bb6\u98ce\u683c\u7684\u5efa\u6a21\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u901a\u7528\u97f3\u4e50\u77e5\u8bc6\u589e\u5f3a\u5bf9\u5c11\u6570\u4f5c\u66f2\u5bb6\u4f5c\u54c1\u7684\u98ce\u683c\u638c\u63e1\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728\u5927\u89c4\u6a21\u6d41\u884c\u3001\u6c11\u8c23\u548c\u53e4\u5178\u97f3\u4e50\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u57fa\u4e8eREMI\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff1b\u968f\u540e\u5728\u5df4\u8d6b\u3001\u83ab\u624e\u7279\u3001\u8d1d\u591a\u82ac\u548c\u8096\u90a6\u7684\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u5757\u5fae\u8c03\u6a21\u578b\uff0c\u4ee5\u6761\u4ef6\u5316\u98ce\u683c\u6307\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u51c6\u786e\u6027\u548c\u97f3\u4e50\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u4f5c\u66f2\u5bb6\u98ce\u683c\u5efa\u6a21\u548c\u66f4\u597d\u7684\u97f3\u4e50\u7f8e\u5b66\u6548\u679c\u3002\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u4ece\u901a\u7528\u9884\u8bad\u7ec3\u4e2d\u6784\u5efa\u97f3\u4e50\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u7ec6\u5316\u98ce\u683c\u7406\u89e3\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u9884\u8bad\u7ec3\u548c\u7279\u5b9a\u98ce\u683c\u5fae\u8c03\uff0c\u672c\u6587\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u4f5c\u66f2\u5bb6\u98ce\u683c\u97f3\u4e50\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u4ece\u901a\u7528\u5230\u7cbe\u901a\uff1a\u57fa\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u4f5c\u66f2\u5bb6\u98ce\u683c\u7b26\u53f7\u97f3\u4e50\u751f\u6210", "abstract_zh": "\u5c3d\u7ba1\u53ef\u63a7\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u4ecd\u662f\u67d0\u4e9b\u63a7\u5236\u6a21\u6001\u7684\u6311\u6218\u3002\u4f5c\u66f2\u5bb6\u98ce\u683c\u97f3\u4e50\u751f\u6210\u662f\u4e00\u4e2a\u5178\u578b\u4f8b\u5b50\uff0c\u56e0\u4e3a\u6bcf\u4f4d\u4f5c\u66f2\u5bb6\u7684\u53ef\u7528\u4f5c\u54c1\u6781\u5c11\uff0c\u9650\u5236\u4e86\u98ce\u683c\u548c\u57fa\u7840\u97f3\u4e50\u5143\u7d20\uff08\u5982\u65cb\u5f8b\u3001\u548c\u5f26\u3001\u8282\u594f\uff09\u7684\u5efa\u6a21\u3002\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u4ece\u5e7f\u6cdb\u8bed\u6599\u5e93\u4e2d\u5b66\u5230\u7684\u901a\u7528\u97f3\u4e50\u77e5\u8bc6\u589e\u5f3a\u5bf9\u7279\u5b9a\u4f5c\u66f2\u5bb6\u98ce\u683c\u7684\u638c\u63e1\uff0c\u91cd\u70b9\u5173\u6ce8\u94a2\u7434\u66f2\u751f\u6210\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u9996\u5148\u5728\u5927\u89c4\u6a21\u6d41\u884c\u3001\u6c11\u8c23\u548c\u53e4\u5178\u97f3\u4e50\u8bed\u6599\u5e93\u4e0a\u9884\u8bad\u7ec3\u57fa\u4e8eREMI\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff1b\u968f\u540e\u5728\u5df4\u8d6b\u3001\u83ab\u624e\u7279\u3001\u8d1d\u591a\u82ac\u548c\u8096\u90a6\u7684\u5c0f\u578b\u4eba\u5de5\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u5757\u5bf9\u6a21\u578b\u8fdb\u884c\u98ce\u683c\u6307\u793a\u7684\u6761\u4ef6\u5316\u5fae\u8c03\u3002\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u98ce\u683c\u51c6\u786e\u6027\u548c\u97f3\u4e50\u6027\u4e0a\u8fdb\u884c\u4e86\u4e3b\u5ba2\u89c2\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6d88\u878d\u5b9e\u9a8c\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u4f5c\u66f2\u5bb6\u98ce\u683c\u5efa\u6a21\u548c\u66f4\u597d\u7684\u97f3\u4e50\u7f8e\u5b66\u6548\u679c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u4e86\u6a21\u578b\u5982\u4f55\u4ece\u901a\u7528\u9884\u8bad\u7ec3\u4e2d\u6784\u5efa\u97f3\u4e50\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u7cbe\u901a\u5fae\u8c03\u7ec6\u5316\u5176\u98ce\u683c\u7406\u89e3\u3002"}}
{"id": "2506.18898", "pdf": "https://arxiv.org/pdf/2506.18898", "abs": "https://arxiv.org/abs/2506.18898", "authors": ["Jiaming Han", "Hao Chen", "Yang Zhao", "Hanyu Wang", "Qi Zhao", "Ziyan Yang", "Hao He", "Xiangyu Yue", "Lu Jiang"], "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Project page: https://tar.csuhan.com", "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7684\u79bb\u6563\u8bed\u4e49\u8868\u793a\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u3002\u6838\u5fc3\u662f\u6587\u672c\u5bf9\u9f50\u5206\u8bcd\u5668\uff08TA-Tok\uff09\uff0c\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u8bcd\u6c47\u8868\u5b9e\u73b0\u8de8\u6a21\u6001\u8f93\u5165\u8f93\u51fa\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001LLM\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u901a\u5e38\u9700\u8981\u7279\u5b9a\u6a21\u6001\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8868\u793a\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7684\u79bb\u6563\u8bed\u4e49\u8868\u793a\uff0c\u5b9e\u73b0\u89c6\u89c9\u4e0e\u6587\u672c\u7684\u7edf\u4e00\u5904\u7406\uff0c\u7b80\u5316\u591a\u6a21\u6001\u4efb\u52a1\u7684\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u6587\u672c\u5bf9\u9f50\u5206\u8bcd\u5668\uff08TA-Tok\uff09\uff0c\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u5e76\u6269\u5c55\u8bcd\u6c47\u8868\u4ee5\u652f\u6301\u8de8\u6a21\u6001\u8f93\u5165\u8f93\u51fa\u3002\u91c7\u7528\u5c3a\u5ea6\u81ea\u9002\u5e94\u7f16\u7801\u548c\u89e3\u7801\u5e73\u8861\u6548\u7387\u4e0e\u7ec6\u8282\uff0c\u5e76\u5229\u7528\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u7801\u5668\uff08\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\uff09\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u3002\u901a\u8fc7\u9ad8\u7ea7\u9884\u8bad\u7ec3\u4efb\u52a1\u589e\u5f3a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u4e14\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001LLM\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u8868\u793a\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\uff0c\u7b80\u5316\u4e86\u8de8\u6a21\u6001\u4efb\u52a1\u8bbe\u8ba1\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "paper_title_zh": "\u89c6\u89c9\u4f5c\u4e3a\u4e00\u79cd\u65b9\u8a00\uff1a\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u8868\u793a\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u4e0e\u751f\u6210", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u8bd5\u56fe\u901a\u8fc7\u5171\u4eab\u7684\u79bb\u6563\u8bed\u4e49\u8868\u793a\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u3002\u5176\u6838\u5fc3\u662f\u6587\u672c\u5bf9\u9f50\u5206\u8bcd\u5668\uff08TA-Tok\uff09\uff0c\u5b83\u5229\u7528\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bcd\u6c47\u8868\u4e2d\u6295\u5f71\u7684\u6587\u672c\u5bf9\u9f50\u7801\u672c\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u79bb\u6563\u6807\u8bb0\u3002\u901a\u8fc7\u5c06\u89c6\u89c9\u548c\u6587\u672c\u6574\u5408\u5230\u5177\u6709\u6269\u5c55\u8bcd\u6c47\u8868\u7684\u7edf\u4e00\u7a7a\u95f4\u4e2d\uff0c\u6211\u4eec\u7684\u591a\u6a21\u6001LLM\uff08Tar\uff09\u5b9e\u73b0\u4e86\u901a\u8fc7\u5171\u4eab\u63a5\u53e3\u7684\u8de8\u6a21\u6001\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u65e0\u9700\u7279\u5b9a\u6a21\u6001\u8bbe\u8ba1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c3a\u5ea6\u81ea\u9002\u5e94\u7f16\u7801\u548c\u89e3\u7801\u4ee5\u5e73\u8861\u6548\u7387\u548c\u89c6\u89c9\u7ec6\u8282\uff0c\u4ee5\u53ca\u751f\u6210\u5f0f\u89e3\u7801\u5668\u4ee5\u4ea7\u751f\u9ad8\u4fdd\u771f\u89c6\u89c9\u8f93\u51fa\u3002\u4e3a\u6ee1\u8db3\u591a\u6837\u5316\u89e3\u7801\u9700\u6c42\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u7801\u5668\uff1a\u5feb\u901f\u81ea\u56de\u5f52\u6a21\u578b\u548c\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u3002\u4e3a\u589e\u5f3a\u6a21\u6001\u878d\u5408\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u9ad8\u7ea7\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5728\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u7684\u6539\u8fdb\u3002\u8de8\u57fa\u51c6\u5b9e\u9a8c\u8868\u660e\uff0cTar\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u591a\u6a21\u6001LLM\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u53ef\u5728https://tar.csuhan.com\u83b7\u53d6\u3002"}}
{"id": "2506.18463", "pdf": "https://arxiv.org/pdf/2506.18463", "abs": "https://arxiv.org/abs/2506.18463", "authors": ["Sophia Sirko-Galouchenko", "Spyros Gidaris", "Antonin Vobecky", "Andrei Bursuc", "Nicolas Thome"], "title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations", "categories": ["cs.CV"], "comment": null, "summary": "We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP", "AI": {"tldr": "DIP\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u76d1\u7763\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u4e0b\u6e38\u573a\u666f\u4efb\u52a1\u589e\u5f3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5bc6\u96c6\u56fe\u50cf\u8868\u793a\uff0c\u65e0\u9700\u590d\u6742\u81ea\u84b8\u998f\u67b6\u6784\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u81ea\u84b8\u998f\u67b6\u6784\uff0c\u96be\u4ee5\u9ad8\u6548\u63d0\u5347\u5bc6\u96c6\u56fe\u50cf\u8868\u793a\u3002DIP\u65e8\u5728\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\uff0c\u5229\u7528\u4f2a\u4efb\u52a1\u6a21\u62df\u4e0b\u6e38\u573a\u666f\uff0c\u63d0\u5347\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6027\u80fd\u3002", "method": "DIP\u91c7\u7528\u5143\u5b66\u4e60\u601d\u60f3\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u7f16\u7801\u5668\u81ea\u52a8\u751f\u6210\u4f2a\u4efb\u52a1\uff0c\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u4ee5\u6a21\u62df\u4e0b\u6e38\u573a\u666f\u4efb\u52a1\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "result": "DIP\u5728\u591a\u79cd\u4e0b\u6e38\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u521d\u59cb\u89c6\u89c9\u7f16\u7801\u5668\u548c\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff08\u5355A100 GPU\u4e0d\u52309\u5c0f\u65f6\uff09\u3002", "conclusion": "DIP\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u65e0\u76d1\u7763\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f2a\u4efb\u52a1\u5b66\u4e60\u5bc6\u96c6\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u7f16\u7801\u5668\u5728\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "paper_title_zh": "DIP\uff1a\u89c6\u89c9\u8868\u793a\u7684\u4e0a\u4e0b\u6587\u5bc6\u96c6\u65e0\u76d1\u7763\u540e\u8bad\u7ec3", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86DIP\uff0c\u4e00\u79cd\u65b0\u578b\u65e0\u76d1\u7763\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4e0a\u4e0b\u6587\u573a\u666f\u4efb\u52a1\u589e\u5f3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5bc6\u96c6\u56fe\u50cf\u8868\u793a\u3002\u4e0e\u4f9d\u8d56\u590d\u6742\u81ea\u84b8\u998f\u67b6\u6784\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cDIP\u53d7\u5143\u5b66\u4e60\u542f\u53d1\uff0c\u901a\u8fc7\u4f2a\u4efb\u52a1\u6a21\u62df\u4e0b\u6e38\u573a\u666f\u4efb\u52a1\u6765\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u3002\u4e3a\u5b9e\u73b0\u65e0\u6807\u6ce8\u6570\u636e\u7684\u540e\u8bad\u7ec3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u81ea\u52a8\u4efb\u52a1\u751f\u6210\u673a\u5236\u3002DIP\u7b80\u5355\u3001\u65e0\u76d1\u7763\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u5355A100 GPU\u8bad\u7ec3\u65f6\u95f4\u4e0d\u8db39\u5c0f\u65f6\u3002\u901a\u8fc7\u4f2a\u4e0a\u4e0b\u6587\u4efb\u52a1\u5b66\u4e60\u5bc6\u96c6\u8868\u793a\uff0cDIP\u5728\u591a\u79cd\u4e0b\u6e38\u5b9e\u9645\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u521d\u59cb\u89c6\u89c9\u7f16\u7801\u5668\u548c\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u63d0\u5347\u5bc6\u96c6\u8868\u793a\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u89c1\uff1ahttps://github.com/sirkosophia/DIP"}}
{"id": "2506.17508", "pdf": "https://arxiv.org/pdf/2506.17508", "abs": "https://arxiv.org/abs/2506.17508", "authors": ["Sajratul Y. Rubaiat", "Syed N. Sakib", "Hasan M. Jamil"], "title": "Mapping the Evolution of Research Contributions using KnoVo", "categories": ["cs.DL", "cs.AI", "cs.DB", "cs.ET", "cs.IR"], "comment": null, "summary": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework\ndesigned for quantifying and analyzing the evolution of research novelty in the\nscientific literature. Moving beyond traditional citation analysis, which\nprimarily measures impact, KnoVo determines a paper's novelty relative to both\nprior and subsequent work within its multilayered citation network. Given a\ntarget paper's abstract, KnoVo utilizes Large Language Models (LLMs) to\ndynamically extract dimensions of comparison (e.g., methodology, application,\ndataset). The target paper is then compared to related publications along these\nsame extracted dimensions. This comparative analysis, inspired by tournament\nselection, yields quantitative novelty scores reflecting the relative\nimprovement, equivalence, or inferiority of the target paper in specific\naspects. By aggregating these scores and visualizing their progression, for\ninstance, through dynamic evolution graphs and comparative radar charts, KnoVo\nfacilitates researchers not only to assess originality and identify similar\nwork, but also to track knowledge evolution along specific research dimensions,\nuncover research gaps, and explore cross-disciplinary connections. We\ndemonstrate these capabilities through a detailed analysis of 20 diverse papers\nfrom multiple scientific fields and report on the performance of various\nopen-source LLMs within the KnoVo framework.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86KnoVo\uff08\u77e5\u8bc6\u6f14\u5316\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u6790\u79d1\u5b66\u6587\u732e\u4e2d\u7814\u7a76\u65b0\u9896\u6027\u7684\u6f14\u53d8\u3002\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u6bd4\u8f83\u7ef4\u5ea6\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0cKnoVo\u80fd\u8bc4\u4f30\u76ee\u6807\u8bba\u6587\u5728\u7279\u5b9a\u65b9\u9762\u7684\u76f8\u5bf9\u65b0\u9896\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u5de5\u5177\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8ffd\u8e2a\u77e5\u8bc6\u6f14\u5316\u3002", "motivation": "\u4f20\u7edf\u5f15\u7528\u5206\u6790\u4e3b\u8981\u8861\u91cf\u7814\u7a76\u7684\u5f71\u54cd\u529b\uff0c\u800c\u5ffd\u7565\u4e86\u65b0\u9896\u6027\u7684\u52a8\u6001\u6f14\u53d8\u3002KnoVo\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u6bd4\u8f83\u548c\u91cf\u5316\u5206\u6790\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u539f\u521b\u6027\u3001\u53d1\u73b0\u7814\u7a76\u7a7a\u767d\u5e76\u63a2\u7d22\u8de8\u5b66\u79d1\u8054\u7cfb\u3002", "method": "KnoVo\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u52a8\u6001\u63d0\u53d6\u76ee\u6807\u8bba\u6587\u7684\u6bd4\u8f83\u7ef4\u5ea6\uff08\u5982\u65b9\u6cd5\u3001\u5e94\u7528\u3001\u6570\u636e\u96c6\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u76f8\u5173\u6587\u732e\u8fdb\u884c\u5bf9\u6bd4\u3002\u901a\u8fc7\u7c7b\u4f3c\u9526\u6807\u8d5b\u9009\u62e9\u7684\u91cf\u5316\u8bc4\u5206\uff0c\u751f\u6210\u53cd\u6620\u76f8\u5bf9\u65b0\u9896\u6027\u7684\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6f14\u5316\u56fe\u548c\u96f7\u8fbe\u56fe\u53ef\u89c6\u5316\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u5bf920\u7bc7\u8de8\u5b66\u79d1\u8bba\u6587\u7684\u8be6\u7ec6\u5206\u6790\uff0cKnoVo\u5c55\u793a\u4e86\u5176\u5728\u8bc4\u4f30\u65b0\u9896\u6027\u3001\u8ffd\u8e2a\u77e5\u8bc6\u6f14\u5316\u548c\u53d1\u73b0\u7814\u7a76\u7a7a\u767d\u65b9\u9762\u7684\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u5f00\u6e90LLMs\u5728\u6846\u67b6\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "KnoVo\u4e3a\u7814\u7a76\u65b0\u9896\u6027\u7684\u91cf\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u521b\u65b0\u5de5\u5177\uff0c\u4e0d\u4ec5\u8865\u5145\u4e86\u4f20\u7edf\u5f15\u7528\u5206\u6790\u7684\u4e0d\u8db3\uff0c\u8fd8\u80fd\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u66f4\u5168\u9762\u5730\u7406\u89e3\u77e5\u8bc6\u6f14\u5316\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u53d1\u5c55\u3002", "paper_title_zh": "\u5229\u7528KnoVo\u6620\u5c04\u7814\u7a76\u8d21\u732e\u7684\u6f14\u53d8", "abstract_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86KnoVo\uff08\u77e5\u8bc6\u6f14\u5316\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u91cf\u5316\u5206\u6790\u79d1\u5b66\u6587\u732e\u4e2d\u7814\u7a76\u65b0\u9896\u6027\u6f14\u53d8\u7684\u667a\u80fd\u6846\u67b6\u3002\u4e0e\u4f20\u7edf\u5f15\u7528\u5206\u6790\u4e0d\u540c\uff0cKnoVo\u901a\u8fc7\u591a\u5c42\u5f15\u7528\u7f51\u7edc\u8bc4\u4f30\u76ee\u6807\u8bba\u6587\u76f8\u5bf9\u4e8e\u5148\u524d\u548c\u540e\u7eed\u5de5\u4f5c\u7684\u65b0\u9896\u6027\u3002\u7ed9\u5b9a\u76ee\u6807\u8bba\u6587\u7684\u6458\u8981\u540e\uff0cKnoVo\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u52a8\u6001\u63d0\u53d6\u6bd4\u8f83\u7ef4\u5ea6\uff08\u5982\u65b9\u6cd5\u3001\u5e94\u7528\u3001\u6570\u636e\u96c6\uff09\uff0c\u5e76\u5c06\u76ee\u6807\u8bba\u6587\u4e0e\u76f8\u5173\u6587\u732e\u8fdb\u884c\u5bf9\u6bd4\u3002\u8fd9\u79cd\u53d7\u9526\u6807\u8d5b\u9009\u62e9\u542f\u53d1\u7684\u5206\u6790\u751f\u6210\u5b9a\u91cf\u65b0\u9896\u6027\u5206\u6570\uff0c\u53cd\u6620\u76ee\u6807\u8bba\u6587\u5728\u7279\u5b9a\u65b9\u9762\u7684\u76f8\u5bf9\u6539\u8fdb\u3001\u7b49\u6548\u6216\u52a3\u52bf\u3002\u901a\u8fc7\u805a\u5408\u8fd9\u4e9b\u5206\u6570\u5e76\u53ef\u89c6\u5316\u5176\u6f14\u53d8\uff08\u5982\u52a8\u6001\u6f14\u5316\u56fe\u548c\u96f7\u8fbe\u56fe\uff09\uff0cKnoVo\u4e0d\u4ec5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u539f\u521b\u6027\u548c\u53d1\u73b0\u7c7b\u4f3c\u5de5\u4f5c\uff0c\u8fd8\u80fd\u8ffd\u8e2a\u7279\u5b9a\u7814\u7a76\u7ef4\u5ea6\u7684\u77e5\u8bc6\u6f14\u5316\u3001\u63ed\u793a\u7814\u7a76\u7a7a\u767d\u5e76\u63a2\u7d22\u8de8\u5b66\u79d1\u8054\u7cfb\u3002\u6211\u4eec\u901a\u8fc7\u5bf920\u7bc7\u8de8\u5b66\u79d1\u8bba\u6587\u7684\u8be6\u7ec6\u5206\u6790\u5c55\u793a\u4e86\u8fd9\u4e9b\u529f\u80fd\uff0c\u5e76\u62a5\u544a\u4e86KnoVo\u6846\u67b6\u4e2d\u591a\u79cd\u5f00\u6e90LLMs\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2506.18472", "pdf": "https://arxiv.org/pdf/2506.18472", "abs": "https://arxiv.org/abs/2506.18472", "authors": ["Gengyuan Zhang", "Tanveer Hannan", "Hermine Kleiner", "Beste Aydemir", "Xinyu Xie", "Jian Lan", "Thomas Seidl", "Volker Tresp", "Jindong Gu"], "title": "AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction", "categories": ["cs.CV"], "comment": "preprint version; 23 pages (including references and appendix)", "summary": "An ideal vision-language agent serves as a bridge between the human users and\ntheir surrounding physical world in real-world applications like autonomous\ndriving and embodied agents, and proactively provides accurate and timely\nresponses given user intents. An intriguing challenge arises when agents\ninteract with the world as a dynamic data stream and ad-hoc queries from users:\nsupporting knowledge for queries, namely evidence, usually appears\nasynchronously with the arrival time of queries, and agents need to ground\ntheir responses in historical data, present observations, and even future\nstreams. We frame this challenge as Query-Evidence Asynchrony, where user\nqueries and their supporting evidence typically arrive asynchronously in the\nstreaming setting. This setting requires not only strong reasoning capabilities\nbut also the ability to retain past observations and respond to queries with\ntemporal awareness. In this paper, we introduce a diagnostic benchmark that\nevaluates Multimodal Large Language Models (MLLMs) on their ability to handle\ninteraction with streaming data. Further, we present AViLA, Asynchronous\nVideo-Language Agent for streaming data interaction that can handle ad-hoc\nqueries and give time-aware responses. For this purpose, AViLA consists of\nthree key modules: comprehensive memory retention, evidence identification, and\nevidence-grounded trigger, that are designed to maintain a general-purpose\nmemory and respond readily and timely to queries. Our experiments show that\nexisting models often fail to respond at appropriate times, while AViLA\nsignificantly improves both accuracy and temporal awareness. Our code and\ndataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406AViLA\uff0c\u7528\u4e8e\u5904\u7406\u6d41\u5f0f\u591a\u6a21\u6001\u6570\u636e\u4ea4\u4e92\u4e2d\u7684\u67e5\u8be2-\u8bc1\u636e\u5f02\u6b65\u95ee\u9898\uff0c\u901a\u8fc7\u7efc\u5408\u8bb0\u5fc6\u4fdd\u7559\u3001\u8bc1\u636e\u8bc6\u522b\u548c\u8bc1\u636e\u89e6\u53d1\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u51c6\u786e\u6027\u548c\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u4ee3\u7406\u7b49\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u7406\u60f3\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u9700\u8981\u5b9e\u65f6\u8fde\u63a5\u7528\u6237\u4e0e\u7269\u7406\u4e16\u754c\uff0c\u5e76\u57fa\u4e8e\u52a8\u6001\u6570\u636e\u6d41\u548c\u7528\u6237\u4e34\u65f6\u67e5\u8be2\u63d0\u4f9b\u51c6\u786e\u53ca\u65f6\u7684\u54cd\u5e94\u3002\u7136\u800c\uff0c\u67e5\u8be2\u4e0e\u652f\u6301\u8bc1\u636e\u901a\u5e38\u5f02\u6b65\u5230\u8fbe\uff0c\u4ee3\u7406\u9700\u57fa\u4e8e\u5386\u53f2\u3001\u5f53\u524d\u751a\u81f3\u672a\u6765\u6570\u636e\u4f5c\u51fa\u54cd\u5e94\uff0c\u8fd9\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86AViLA\uff08\u5f02\u6b65\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff09\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u7efc\u5408\u8bb0\u5fc6\u4fdd\u7559\u3001\u8bc1\u636e\u8bc6\u522b\u548c\u8bc1\u636e\u89e6\u53d1\u3002\u8fd9\u4e9b\u6a21\u5757\u8bbe\u8ba1\u7528\u4e8e\u7ef4\u62a4\u901a\u7528\u8bb0\u5fc6\uff0c\u5e76\u5728\u9002\u5f53\u65f6\u95f4\u54cd\u5e94\u67e5\u8be2\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5904\u7406\u6d41\u5f0f\u6570\u636e\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u5728\u9002\u5f53\u65f6\u95f4\u54cd\u5e94\uff0c\u800cAViLA\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u51c6\u786e\u6027\u548c\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "AViLA\u901a\u8fc7\u5176\u72ec\u7279\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u67e5\u8be2-\u8bc1\u636e\u5f02\u6b65\u95ee\u9898\uff0c\u4e3a\u6d41\u5f0f\u591a\u6a21\u6001\u6570\u636e\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "AViLA\uff1a\u7528\u4e8e\u6d41\u5f0f\u591a\u6a21\u6001\u6570\u636e\u4ea4\u4e92\u7684\u5f02\u6b65\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406", "abstract_zh": "\u7406\u60f3\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u4ee3\u7406\u7b49\u73b0\u5b9e\u5e94\u7528\u4e2d\u5145\u5f53\u7528\u6237\u4e0e\u5468\u56f4\u7269\u7406\u4e16\u754c\u7684\u6865\u6881\uff0c\u5e76\u6839\u636e\u7528\u6237\u610f\u56fe\u4e3b\u52a8\u63d0\u4f9b\u51c6\u786e\u53ca\u65f6\u7684\u54cd\u5e94\u3002\u5f53\u4ee3\u7406\u4e0e\u52a8\u6001\u6570\u636e\u6d41\u548c\u7528\u6237\u4e34\u65f6\u67e5\u8be2\u4ea4\u4e92\u65f6\uff0c\u4e00\u4e2a\u6709\u8da3\u7684\u6311\u6218\u51fa\u73b0\u4e86\uff1a\u67e5\u8be2\u7684\u652f\u6301\u77e5\u8bc6\uff08\u5373\u8bc1\u636e\uff09\u901a\u5e38\u4e0e\u67e5\u8be2\u5230\u8fbe\u65f6\u95f4\u5f02\u6b65\u51fa\u73b0\uff0c\u4ee3\u7406\u9700\u8981\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u3001\u5f53\u524d\u89c2\u5bdf\u751a\u81f3\u672a\u6765\u6570\u636e\u6d41\u4f5c\u51fa\u54cd\u5e94\u3002\u6211\u4eec\u5c06\u8fd9\u4e00\u6311\u6218\u79f0\u4e3a\u67e5\u8be2-\u8bc1\u636e\u5f02\u6b65\u6027\uff0c\u5176\u4e2d\u7528\u6237\u67e5\u8be2\u53ca\u5176\u652f\u6301\u8bc1\u636e\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u901a\u5e38\u5f02\u6b65\u5230\u8fbe\u3002\u8fd9\u4e00\u8bbe\u7f6e\u4e0d\u4ec5\u9700\u8981\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u4fdd\u7559\u8fc7\u53bb\u89c2\u5bdf\u5e76\u5177\u6709\u65f6\u95f4\u611f\u77e5\u7684\u54cd\u5e94\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5904\u7406\u6d41\u5f0f\u6570\u636e\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AViLA\uff08\u5f02\u6b65\u89c6\u9891\u8bed\u8a00\u4ee3\u7406\uff09\uff0c\u7528\u4e8e\u5904\u7406\u6d41\u5f0f\u6570\u636e\u4ea4\u4e92\u4e2d\u7684\u4e34\u65f6\u67e5\u8be2\u5e76\u63d0\u4f9b\u65f6\u95f4\u611f\u77e5\u54cd\u5e94\u3002\u4e3a\u6b64\uff0cAViLA\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u7efc\u5408\u8bb0\u5fc6\u4fdd\u7559\u3001\u8bc1\u636e\u8bc6\u522b\u548c\u8bc1\u636e\u89e6\u53d1\uff0c\u65e8\u5728\u7ef4\u62a4\u901a\u7528\u8bb0\u5fc6\u5e76\u968f\u65f6\u51c6\u5907\u53ca\u65f6\u54cd\u5e94\u67e5\u8be2\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u5728\u9002\u5f53\u65f6\u95f4\u54cd\u5e94\uff0c\u800cAViLA\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2506.17518", "pdf": "https://arxiv.org/pdf/2506.17518", "abs": "https://arxiv.org/abs/2506.17518", "authors": ["Ayoub Echchahed", "Pablo Samuel Castro"], "title": "A Survey of State Representation Learning for Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Representation learning methods are an important tool for addressing the\nchallenges posed by complex observations spaces in sequential decision making\nproblems. Recently, many methods have used a wide variety of types of\napproaches for learning meaningful state representations in reinforcement\nlearning, allowing better sample efficiency, generalization, and performance.\nThis survey aims to provide a broad categorization of these methods within a\nmodel-free online setting, exploring how they tackle the learning of state\nrepresentations differently. We categorize the methods into six main classes,\ndetailing their mechanisms, benefits, and limitations. Through this taxonomy,\nour aim is to enhance the understanding of this field and provide a guide for\nnew researchers. We also discuss techniques for assessing the quality of\nrepresentations, and detail relevant future directions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u72b6\u6001\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u516d\u7c7b\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u673a\u5236\u3001\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\uff0c\u65e8\u5728\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u6e05\u6670\u5206\u7c7b\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u590d\u6742\u89c2\u6d4b\u7a7a\u95f4\u5728\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\u5e26\u6765\u6311\u6218\uff0c\u72b6\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u5de5\u5177\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u7406\u89e3\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u672c\u6587\u5728\u65e0\u6a21\u578b\u5728\u7ebf\u8bbe\u7f6e\u4e0b\uff0c\u5c06\u72b6\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5206\u4e3a\u516d\u7c7b\uff0c\u8be6\u7ec6\u5206\u6790\u6bcf\u7c7b\u65b9\u6cd5\u7684\u673a\u5236\u3001\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u8868\u793a\u8d28\u91cf\u8bc4\u4f30\u6280\u672f\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u4e0e\u6bd4\u8f83\uff0c\u672c\u6587\u63ed\u793a\u4e86\u4e0d\u540c\u72b6\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u7684\u7279\u70b9\u4e0e\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u4e0e\u8bc4\u4f30\uff0c\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u72b6\u6001\u8868\u793a\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u6e05\u6670\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002", "paper_title_zh": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u72b6\u6001\u8868\u793a\u5b66\u4e60\u7684\u7efc\u8ff0", "abstract_zh": "\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u662f\u89e3\u51b3\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\u590d\u6742\u89c2\u6d4b\u7a7a\u95f4\u6311\u6218\u7684\u91cd\u8981\u5de5\u5177\u3002\u8fd1\u5e74\u6765\uff0c\u8bb8\u591a\u65b9\u6cd5\u91c7\u7528\u591a\u79cd\u65b9\u5f0f\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\u4e2d\u6709\u610f\u4e49\u7684\u72b6\u6001\u8868\u793a\uff0c\u4ece\u800c\u63d0\u5347\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u65e0\u6a21\u578b\u5728\u7ebf\u8bbe\u7f6e\u4e0b\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u5206\u7c7b\uff0c\u63a2\u8ba8\u5176\u5b66\u4e60\u72b6\u6001\u8868\u793a\u7684\u4e0d\u540c\u65b9\u5f0f\u3002\u6211\u4eec\u5c06\u65b9\u6cd5\u5206\u4e3a\u516d\u5927\u7c7b\uff0c\u8be6\u7ec6\u9610\u8ff0\u5176\u673a\u5236\u3001\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u3002\u901a\u8fc7\u8fd9\u4e00\u5206\u7c7b\u4f53\u7cfb\uff0c\u6211\u4eec\u65e8\u5728\u589e\u5f3a\u5bf9\u8be5\u9886\u57df\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u65b0\u7814\u7a76\u8005\u63d0\u4f9b\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\u7684\u6280\u672f\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u76f8\u5173\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2506.18476", "pdf": "https://arxiv.org/pdf/2506.18476", "abs": "https://arxiv.org/abs/2506.18476", "authors": ["Yaokun Zhong", "Siyu Jiang", "Jian Zhu", "Jian-Fang Hu"], "title": "Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Semi-Supervised Video Paragraph Grounding (SSVPG) aims to localize multiple\nsentences in a paragraph from an untrimmed video with limited temporal\nannotations. Existing methods focus on teacher-student consistency learning and\nvideo-level contrastive loss, but they overlook the importance of perturbing\nquery contexts to generate strong supervisory signals. In this work, we propose\na novel Context Consistency Learning (CCL) framework that unifies the paradigms\nof consistency regularization and pseudo-labeling to enhance semi-supervised\nlearning. Specifically, we first conduct teacher-student learning where the\nstudent model takes as inputs strongly-augmented samples with sentences removed\nand is enforced to learn from the adequately strong supervisory signals from\nthe teacher model. Afterward, we conduct model retraining based on the\ngenerated pseudo labels, where the mutual agreement between the original and\naugmented views' predictions is utilized as the label confidence. Extensive\nexperiments show that CCL outperforms existing methods by a large margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5b66\u4e60\uff08CCL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53e5\u5b50\u79fb\u9664\u589e\u5f3a\u534a\u76d1\u7763\u89c6\u9891\u6bb5\u843d\u5b9a\u4f4d\uff08SSVPG\uff09\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u4f2a\u6807\u7b7e\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u76d1\u7763\u89c6\u9891\u6bb5\u843d\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e08\u751f\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u89c6\u9891\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u4f46\u5ffd\u7565\u4e86\u901a\u8fc7\u6270\u52a8\u67e5\u8be2\u4e0a\u4e0b\u6587\u751f\u6210\u5f3a\u76d1\u7763\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u8fdb\u884c\u5e08\u751f\u5b66\u4e60\uff0c\u5b66\u751f\u6a21\u578b\u8f93\u5165\u7ecf\u8fc7\u5f3a\u589e\u5f3a\uff08\u53e5\u5b50\u79fb\u9664\uff09\u7684\u6837\u672c\uff0c\u5e76\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u5f3a\u76d1\u7763\u4fe1\u53f7\u3002\u968f\u540e\u57fa\u4e8e\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u6a21\u578b\u91cd\u8bad\u7ec3\uff0c\u5229\u7528\u539f\u59cb\u89c6\u56fe\u548c\u589e\u5f3a\u89c6\u56fe\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u4f5c\u4e3a\u6807\u7b7e\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCCL\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u660e\u663e\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u4f2a\u6807\u7b7e\u751f\u6210\uff0cCCL\u6846\u67b6\u5728\u534a\u76d1\u7763\u89c6\u9891\u6bb5\u843d\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u53e5\u5b50\u79fb\u9664\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5b66\u4e60\u7528\u4e8e\u534a\u76d1\u7763\u89c6\u9891\u6bb5\u843d\u5b9a\u4f4d", "abstract_zh": "\u534a\u76d1\u7763\u89c6\u9891\u6bb5\u843d\u5b9a\u4f4d\uff08SSVPG\uff09\u65e8\u5728\u4ece\u672a\u4fee\u526a\u7684\u89c6\u9891\u4e2d\u5b9a\u4f4d\u6bb5\u843d\u4e2d\u7684\u591a\u4e2a\u53e5\u5b50\uff0c\u4e14\u4ec5\u9700\u6709\u9650\u7684\u65f6\u5e8f\u6807\u6ce8\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e08\u751f\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u89c6\u9891\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u4f46\u5ffd\u7565\u4e86\u901a\u8fc7\u6270\u52a8\u67e5\u8be2\u4e0a\u4e0b\u6587\u751f\u6210\u5f3a\u76d1\u7763\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5b66\u4e60\uff08CCL\uff09\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u4f2a\u6807\u7b7e\u751f\u6210\u7684\u8303\u5f0f\u4ee5\u589e\u5f3a\u534a\u76d1\u7763\u5b66\u4e60\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u8fdb\u884c\u5e08\u751f\u5b66\u4e60\uff0c\u5b66\u751f\u6a21\u578b\u8f93\u5165\u7ecf\u8fc7\u5f3a\u589e\u5f3a\uff08\u53e5\u5b50\u79fb\u9664\uff09\u7684\u6837\u672c\uff0c\u5e76\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u5f3a\u76d1\u7763\u4fe1\u53f7\u3002\u968f\u540e\u57fa\u4e8e\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u6a21\u578b\u91cd\u8bad\u7ec3\uff0c\u5229\u7528\u539f\u59cb\u89c6\u56fe\u548c\u589e\u5f3a\u89c6\u56fe\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u4f5c\u4e3a\u6807\u7b7e\u7f6e\u4fe1\u5ea6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCCL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18484", "pdf": "https://arxiv.org/pdf/2506.18484", "abs": "https://arxiv.org/abs/2506.18484", "authors": ["Pascal Kl\u00f6ckner", "Jos\u00e9 Teixeira", "Diana Montezuma", "Jaime S. Cardoso", "Hugo M. Horlings", "Sara P. Oliveira"], "title": "GANs vs. Diffusion Models for virtual staining with the HER2match dataset", "categories": ["cs.CV"], "comment": null, "summary": "Virtual staining is a promising technique that uses deep generative models to\nrecreate histological stains, providing a faster and more cost-effective\nalternative to traditional tissue chemical staining. Specifically for H&E-HER2\nstaining transfer, despite a rising trend in publications, the lack of\nsufficient public datasets has hindered progress in the topic. Additionally, it\nis currently unclear which model frameworks perform best for this particular\ntask. In this paper, we introduce the HER2match dataset, the first publicly\navailable dataset with the same breast cancer tissue sections stained with both\nH&E and HER2. Furthermore, we compare the performance of several Generative\nAdversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel\nBrownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate\nthat, overall, GANs perform better than DMs, with only the BBDM achieving\ncomparable results. Furthermore, we emphasize the importance of data alignment,\nas all models trained on HER2match produced vastly improved visuals compared to\nthe widely used consecutive-slide BCI dataset. This research provides a new\nhigh-quality dataset ([available upon publication acceptance]), improving both\nmodel training and evaluation. In addition, our comparison of frameworks offers\nvaluable guidance for researchers working on the topic.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u516c\u5f00\u7684H&E\u548cHER2\u53cc\u67d3\u8272\u4e73\u817a\u764c\u7ec4\u7ec7\u6570\u636e\u96c6HER2match\uff0c\u5e76\u6bd4\u8f83\u4e86GANs\u548c\u6269\u6563\u6a21\u578b\u5728H&E-HER2\u67d3\u8272\u8f6c\u6362\u4e2d\u7684\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0GANs\u6574\u4f53\u8868\u73b0\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4ec5BBDM\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u3002\u6570\u636e\u5bf9\u9f50\u5bf9\u6a21\u578b\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u865a\u62df\u67d3\u8272\u6280\u672f\u901a\u8fc7\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u91cd\u5efa\u7ec4\u7ec7\u67d3\u8272\uff0c\u6bd4\u4f20\u7edf\u5316\u5b66\u67d3\u8272\u66f4\u5feb\u4e14\u6210\u672c\u66f4\u4f4e\u3002\u7136\u800c\uff0cH&E-HER2\u67d3\u8272\u8f6c\u6362\u9886\u57df\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4e14\u4e0d\u6e05\u695a\u54ea\u79cd\u6a21\u578b\u6846\u67b6\u6700\u9002\u5408\u8be5\u4efb\u52a1\u3002", "method": "\u672c\u6587\u5f15\u5165HER2match\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u591a\u79cdGANs\u548c\u6269\u6563\u6a21\u578b\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684BBDM\u6a21\u578b\uff09\u5728H&E-HER2\u8f6c\u6362\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGANs\u6574\u4f53\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4ec5BBDM\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u3002\u4f7f\u7528HER2match\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u89c6\u89c9\u6548\u679c\u663e\u8457\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684BCI\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6bd4\u8f83\u7ed3\u679c\uff0c\u4e3aH&E-HER2\u67d3\u8272\u8f6c\u6362\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "GANs\u4e0e\u6269\u6563\u6a21\u578b\u5728HER2match\u6570\u636e\u96c6\u4e0a\u7684\u865a\u62df\u67d3\u8272\u6027\u80fd\u6bd4\u8f83", "abstract_zh": "\u865a\u62df\u67d3\u8272\u662f\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u91cd\u5efa\u7ec4\u7ec7\u67d3\u8272\u7684\u6280\u672f\uff0c\u4e3a\u4f20\u7edf\u7ec4\u7ec7\u5316\u5b66\u67d3\u8272\u63d0\u4f9b\u4e86\u66f4\u5feb\u4e14\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u9488\u5bf9H&E-HER2\u67d3\u8272\u8f6c\u6362\uff0c\u5c3d\u7ba1\u76f8\u5173\u7814\u7a76\u9010\u6e10\u589e\u591a\uff0c\u4f46\u516c\u5f00\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u54ea\u79cd\u6a21\u578b\u6846\u67b6\u6700\u9002\u5408\u8be5\u4efb\u52a1\u3002\u672c\u6587\u4ecb\u7ecd\u4e86HER2match\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u516c\u5f00\u7684\u540c\u4e00\u4e73\u817a\u764c\u7ec4\u7ec7\u5207\u7247\u540c\u65f6\u8fdb\u884cH&E\u548cHER2\u67d3\u8272\u7684\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u591a\u79cd\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u548c\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\uff08BBDM\uff09\u7528\u4e8eH&E-HER2\u8f6c\u6362\u3002\u7ed3\u679c\u8868\u660e\uff0c\u603b\u4f53\u4e0aGANs\u4f18\u4e8eDMs\uff0c\u4ec5BBDM\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u3002\u6211\u4eec\u8fd8\u5f3a\u8c03\u4e86\u6570\u636e\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u6240\u6709\u57fa\u4e8eHER2match\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u89c6\u89c9\u6548\u679c\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u8fde\u7eed\u5207\u7247BCI\u6570\u636e\u96c6\u3002\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u65b0\u6570\u636e\u96c6\uff08\u5f85\u53d1\u8868\u540e\u516c\u5f00\uff09\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u6bd4\u8f83\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2506.17536", "pdf": "https://arxiv.org/pdf/2506.17536", "abs": "https://arxiv.org/abs/2506.17536", "authors": ["Hao Peng", "Steve Jiang", "Robert Timmerman"], "title": "Exploring Strategies for Personalized Radiation Therapy Part I Unlocking Response-Related Tumor Subregions with Class Activation Mapping", "categories": ["physics.med-ph", "cs.AI"], "comment": null, "summary": "Personalized precision radiation therapy requires more than simple\nclassification, it demands the identification of prognostic, spatially\ninformative features and the ability to adapt treatment based on individual\nresponse. This study compares three approaches for predicting treatment\nresponse: standard radiomics, gradient based features, and convolutional neural\nnetworks enhanced with Class Activation Mapping. We analyzed 69 brain\nmetastases from 39 patients treated with Gamma Knife radiosurgery. An\nintegrated autoencoder classifier model was used to predict whether tumor\nvolume would shrink by more than 20 percent at a three months follow up, framed\nas a binary classification task. The results highlight their strength in\nhierarchical feature extraction and the classifiers discriminative capacity.\nAmong the models, pixel wise CAM provides the most detailed spatial insight,\nidentifying lesion specific regions rather than relying on fixed patterns,\ndemonstrating strong generalization. In non responding lesions, the activated\nregions may indicate areas of radio resistance. Pixel wise CAM outperformed\nboth radiomics and gradient based methods in classification accuracy. Moreover,\nits fine grained spatial features allow for alignment with cellular level data,\nsupporting biological validation and deeper understanding of heterogeneous\ntreatment responses. Although further validation is necessary, these findings\nunderscore the promise in guiding personalized and adaptive radiotherapy\nstrategies for both photon and particle therapies.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u9884\u6d4b\u653e\u7597\u53cd\u5e94\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u50cf\u7d20\u7ea7CAM\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u4e2a\u6027\u5316\u653e\u7597\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u4e2a\u6027\u5316\u7cbe\u51c6\u653e\u7597\u9700\u8981\u8bc6\u522b\u5177\u6709\u9884\u540e\u610f\u4e49\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u4e2a\u4f53\u53cd\u5e94\u8c03\u6574\u6cbb\u7597\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8639\u540d\u60a3\u8005\u768469\u4e2a\u8111\u8f6c\u79fb\u7624\uff0c\u4f7f\u7528\u6807\u51c6\u653e\u5c04\u7ec4\u5b66\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u7279\u5f81\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u7ed3\u5408CAM\uff09\u9884\u6d4b\u80bf\u7624\u4f53\u79ef\u662f\u5426\u5728\u4e09\u4e2a\u6708\u968f\u8bbf\u65f6\u7f29\u5c0f\u8d85\u8fc720%\u3002", "result": "\u50cf\u7d20\u7ea7CAM\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u653e\u5c04\u7ec4\u5b66\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u8bc6\u522b\u75c5\u7076\u7279\u5f02\u6027\u533a\u57df\uff0c\u63ed\u793a\u53ef\u80fd\u7684\u653e\u7597\u62b5\u6297\u533a\u57df\u3002", "conclusion": "\u50cf\u7d20\u7ea7CAM\u4e3a\u4e2a\u6027\u5316\u653e\u7597\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u652f\u6301\u751f\u7269\u5b66\u9a8c\u8bc1\uff0c\u672a\u6765\u6709\u671b\u6307\u5bfc\u5149\u5b50\u53ca\u7c92\u5b50\u7597\u6cd5\u7684\u9002\u5e94\u6027\u7b56\u7565\u3002", "paper_title_zh": "\u63a2\u7d22\u4e2a\u6027\u5316\u653e\u7597\u7b56\u7565\u7b2c\u4e00\u90e8\u5206\uff1a\u5229\u7528\u7c7b\u522b\u6fc0\u6d3b\u6620\u5c04\u89e3\u9501\u4e0e\u53cd\u5e94\u76f8\u5173\u7684\u80bf\u7624\u4e9a\u533a\u57df", "abstract_zh": "\u4e2a\u6027\u5316\u7cbe\u51c6\u653e\u7597\u4e0d\u4ec5\u9700\u8981\u7b80\u5355\u5206\u7c7b\uff0c\u8fd8\u9700\u8bc6\u522b\u5177\u6709\u9884\u540e\u610f\u4e49\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u6839\u636e\u4e2a\u4f53\u53cd\u5e94\u8c03\u6574\u6cbb\u7597\u3002\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u9884\u6d4b\u6cbb\u7597\u53cd\u5e94\u7684\u65b9\u6cd5\uff1a\u6807\u51c6\u653e\u5c04\u7ec4\u5b66\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u7279\u5f81\u4ee5\u53ca\u7ed3\u5408\u7c7b\u522b\u6fc0\u6d3b\u6620\u5c04\uff08CAM\uff09\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u5206\u6790\u4e8639\u540d\u60a3\u8005\u7ecf\u4f3d\u739b\u5200\u653e\u5c04\u5916\u79d1\u6cbb\u7597\u768469\u4e2a\u8111\u8f6c\u79fb\u7624\uff0c\u4f7f\u7528\u96c6\u6210\u81ea\u7f16\u7801\u5668\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u80bf\u7624\u4f53\u79ef\u5728\u4e09\u4e2a\u6708\u968f\u8bbf\u65f6\u662f\u5426\u7f29\u5c0f\u8d85\u8fc720%\uff08\u4f5c\u4e3a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u5224\u522b\u80fd\u529b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002\u5176\u4e2d\uff0c\u50cf\u7d20\u7ea7CAM\u63d0\u4f9b\u4e86\u6700\u8be6\u7ec6\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u80fd\u591f\u8bc6\u522b\u75c5\u7076\u7279\u5f02\u6027\u533a\u57df\u800c\u975e\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u5728\u65e0\u53cd\u5e94\u75c5\u7076\u4e2d\uff0c\u6fc0\u6d3b\u533a\u57df\u53ef\u80fd\u6307\u793a\u653e\u7597\u62b5\u6297\u533a\u57df\u3002\u50cf\u7d20\u7ea7CAM\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u653e\u5c04\u7ec4\u5b66\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u5176\u7cbe\u7ec6\u7a7a\u95f4\u7279\u5f81\u8fd8\u53ef\u4e0e\u7ec6\u80de\u6c34\u5e73\u6570\u636e\u5bf9\u9f50\uff0c\u652f\u6301\u751f\u7269\u5b66\u9a8c\u8bc1\u548c\u6df1\u5165\u7406\u89e3\u5f02\u8d28\u6027\u6cbb\u7597\u53cd\u5e94\u3002\u5c3d\u7ba1\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5149\u5b50\u53ca\u7c92\u5b50\u7597\u6cd5\u7684\u4e2a\u6027\u5316\u9002\u5e94\u6027\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2506.18493", "pdf": "https://arxiv.org/pdf/2506.18493", "abs": "https://arxiv.org/abs/2506.18493", "authors": ["Trong-Vu Hoang", "Quang-Binh Nguyen", "Thanh-Toan Do", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation", "categories": ["cs.CV"], "comment": null, "summary": "Customizing image generation remains a core challenge in controllable image\nsynthesis. For single-concept generation, maintaining both identity\npreservation and prompt alignment is challenging. In multi-concept scenarios,\nrelying solely on a prompt without additional conditions like layout boxes or\nsemantic masks, often leads to identity loss and concept omission. In this\npaper, we introduce ShowFlow, a comprehensive framework designed to tackle\nthese challenges. We propose ShowFlow-S for single-concept image generation,\nand ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a\nKronA-WED adapter, which integrates a Kronecker adapter with weight and\nembedding decomposition, and employs a disentangled learning approach with a\nnovel attention regularization objective to enhance single-concept generation.\nBuilding on this foundation, ShowFlow-M directly reuses the learned models from\nShowFlow-S to support multi-concept generation without extra conditions,\nincorporating a Subject-Adaptive Matching Attention (SAMA) and a layout\nconsistency strategy as the plug-and-play module. Extensive experiments and\nuser studies validate ShowFlow's effectiveness, highlighting its potential in\nreal-world applications like advertising and virtual dressing.", "AI": {"tldr": "ShowFlow\u662f\u4e00\u4e2a\u5168\u9762\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u5206\u4e3aShowFlow-S\u548cShowFlow-M\u4e24\u90e8\u5206\uff0c\u5206\u522b\u89e3\u51b3\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u751f\u6210\u7684\u6311\u6218\u3002\u901a\u8fc7KronA-WED\u9002\u914d\u5668\u548cSAMA\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u4fdd\u6301\u548c\u63d0\u793a\u5bf9\u9f50\uff0c\u65e0\u9700\u989d\u5916\u6761\u4ef6\u5373\u53ef\u751f\u6210\u591a\u6982\u5ff5\u56fe\u50cf\u3002", "motivation": "\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u5728\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002\u5355\u6982\u5ff5\u751f\u6210\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u548c\u63d0\u793a\u5bf9\u9f50\uff0c\u800c\u591a\u6982\u5ff5\u751f\u6210\u4f9d\u8d56\u989d\u5916\u6761\u4ef6\uff08\u5982\u5e03\u5c40\u6846\u6216\u8bed\u4e49\u63a9\u7801\uff09\u6613\u5bfc\u81f4\u8eab\u4efd\u4e22\u5931\u548c\u6982\u5ff5\u9057\u6f0f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ShowFlow-S\u91c7\u7528KronA-WED\u9002\u914d\u5668\uff08\u7ed3\u5408Kronecker\u9002\u914d\u5668\u4e0e\u6743\u91cd\u548c\u5d4c\u5165\u5206\u89e3\uff09\u548c\u89e3\u8026\u5b66\u4e60\u65b9\u6cd5\uff0c\u589e\u5f3a\u5355\u6982\u5ff5\u751f\u6210\u3002ShowFlow-M\u76f4\u63a5\u590d\u7528ShowFlow-S\u7684\u6a21\u578b\uff0c\u901a\u8fc7SAMA\u6a21\u5757\u548c\u5e03\u5c40\u4e00\u81f4\u6027\u7b56\u7565\u5b9e\u73b0\u65e0\u989d\u5916\u6761\u4ef6\u7684\u591a\u6982\u5ff5\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cShowFlow\u5728\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u751f\u6210\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5e7f\u544a\u548c\u865a\u62df\u8bd5\u8863\u7b49\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "ShowFlow\u901a\u8fc7\u521b\u65b0\u7684\u9002\u914d\u5668\u548c\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u751f\u6210\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "ShowFlow\uff1a\u4ece\u9c81\u68d2\u7684\u5355\u6982\u5ff5\u751f\u6210\u5230\u65e0\u6761\u4ef6\u7684\u591a\u6982\u5ff5\u751f\u6210", "abstract_zh": "\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u4ecd\u662f\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u7684\u6838\u5fc3\u6311\u6218\u3002\u5728\u5355\u6982\u5ff5\u751f\u6210\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4fdd\u5b58\u548c\u63d0\u793a\u5bf9\u9f50\u5177\u6709\u96be\u5ea6\uff1b\u800c\u5728\u591a\u6982\u5ff5\u573a\u666f\u4e2d\uff0c\u4ec5\u4f9d\u8d56\u63d0\u793a\u800c\u65e0\u989d\u5916\u6761\u4ef6\uff08\u5982\u5e03\u5c40\u6846\u6216\u8bed\u4e49\u63a9\u7801\uff09\u5e38\u5bfc\u81f4\u8eab\u4efd\u4e22\u5931\u548c\u6982\u5ff5\u9057\u6f0f\u3002\u672c\u6587\u63d0\u51faShowFlow\uff0c\u4e00\u4e2a\u5168\u9762\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u6846\u67b6\u3002ShowFlow-S\u7528\u4e8e\u5355\u6982\u5ff5\u56fe\u50cf\u751f\u6210\uff0cShowFlow-M\u7528\u4e8e\u591a\u6982\u5ff5\u751f\u6210\u3002ShowFlow-S\u5f15\u5165KronA-WED\u9002\u914d\u5668\uff0c\u7ed3\u5408Kronecker\u9002\u914d\u5668\u4e0e\u6743\u91cd\u548c\u5d4c\u5165\u5206\u89e3\uff0c\u5e76\u91c7\u7528\u89e3\u8026\u5b66\u4e60\u65b9\u6cd5\u548c\u65b0\u578b\u6ce8\u610f\u529b\u6b63\u5219\u5316\u76ee\u6807\u4ee5\u589e\u5f3a\u5355\u6982\u5ff5\u751f\u6210\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cShowFlow-M\u76f4\u63a5\u590d\u7528ShowFlow-S\u7684\u5b66\u4e60\u6a21\u578b\uff0c\u652f\u6301\u65e0\u9700\u989d\u5916\u6761\u4ef6\u7684\u591a\u6982\u5ff5\u751f\u6210\uff0c\u6574\u5408\u4e86\u4e3b\u9898\u81ea\u9002\u5e94\u5339\u914d\u6ce8\u610f\u529b\uff08SAMA\uff09\u548c\u5e03\u5c40\u4e00\u81f4\u6027\u7b56\u7565\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u3002\u5927\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86ShowFlow\u7684\u6709\u6548\u6027\uff0c\u7a81\u663e\u5176\u5728\u5e7f\u544a\u548c\u865a\u62df\u8bd5\u8863\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17538", "pdf": "https://arxiv.org/pdf/2506.17538", "abs": "https://arxiv.org/abs/2506.17538", "authors": ["Yile Gu", "Rohan Kadekodi", "Hoang Nguyen", "Keisuke Kamahori", "Yiyu Liu", "Baris Kasikci"], "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User Devices", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.OS"], "comment": "The code is available at https://github.com/efeslab/ConsumerBench", "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ConsumerBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7ec8\u7aef\u8bbe\u5907\u4e0a\u751f\u6210\u5f0fAI\u6a21\u578b\u7cfb\u7edf\u6548\u7387\u548c\u54cd\u5e94\u65f6\u95f4\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8d44\u6e90\u5206\u914d\u548c\u8c03\u5ea6\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5e94\u7528\u4ece\u4e91\u7aef\u8fc1\u79fb\u5230\u7ec8\u7aef\u8bbe\u5907\uff0c\u8d44\u6e90\u7ba1\u7406\u3001\u7cfb\u7edf\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u9762\u4e34\u65b0\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5047\u8bbe\u6a21\u578b\u72ec\u5360\u4e13\u7528GPU\uff0c\u65e0\u6cd5\u53cd\u6620\u591a\u5e94\u7528\u5e76\u53d1\u8fd0\u884c\u7684\u5b9e\u9645\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "ConsumerBench\u6a21\u62df\u4e86\u7ec8\u7aef\u8bbe\u5907\u4e0a\u591a\u5e94\u7528\u5e76\u53d1\u8fd0\u884c\u7684\u573a\u666f\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u5de5\u4f5c\u6d41\u4ee5\u6a21\u62df\u590d\u6742\u4efb\u52a1\u3002\u5b83\u6355\u83b7\u5e94\u7528\u7ea7\u6307\u6807\uff08\u5982\u5ef6\u8fdf\u548c\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u8fbe\u6210\u7387\uff09\u548c\u7cfb\u7edf\u7ea7\u6307\u6807\uff08\u5982CPU/GPU\u5229\u7528\u7387\u548c\u5185\u5b58\u5e26\u5bbd\uff09\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u8d44\u6e90\u5171\u4eab\u7684\u4f4e\u6548\u6027\u3001\u8d2a\u5a6a\u5206\u914d\u4e0b\u7684\u4e0d\u516c\u5e73\u8c03\u5ea6\u4ee5\u53ca\u9759\u6001\u6a21\u578b\u670d\u52a1\u5668\u914d\u7f6e\u7684\u6027\u80fd\u7f3a\u9677\u3002\u540c\u65f6\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u9488\u5bf9\u6d88\u8d39\u7ea7GPU\u67b6\u6784\u7684\u5b9a\u5236\u5185\u6838\u548cSLO\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u7684\u4ef7\u503c\u3002", "conclusion": "ConsumerBench\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u548c\u7cfb\u7edf\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u6d1e\u5bdf\uff0c\u5f3a\u8c03\u4e86\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u8c03\u5ea6\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u5347\u7ec8\u7aef\u8bbe\u5907\u4e0a\u751f\u6210\u5f0fAI\u5e94\u7528\u7684\u6027\u80fd\u3002", "paper_title_zh": "ConsumerBench\uff1a\u7ec8\u7aef\u8bbe\u5907\u4e0a\u751f\u6210\u5f0fAI\u5e94\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5e94\u7528\u4ece\u4e91\u7aef\u73af\u5883\u5411\u7ec8\u7aef\u8bbe\u5907\u7684\u8fc1\u79fb\uff0c\u4e3a\u8d44\u6e90\u7ba1\u7406\u3001\u7cfb\u7edf\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86ConsumerBench\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u65e8\u5728\u8bc4\u4f30\u7ec8\u7aef\u8bbe\u5907\u4e0a\u8fd0\u884cGenAI\u6a21\u578b\u7684\u7cfb\u7edf\u6548\u7387\u548c\u54cd\u5e94\u65f6\u95f4\u3002\u4e0e\u73b0\u6709\u5047\u8bbe\u6a21\u578b\u72ec\u5360\u4e13\u7528GPU\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u540c\uff0cConsumerBench\u6a21\u62df\u4e86\u53d7\u9650\u786c\u4ef6\u4e0a\u591a\u5e94\u7528\u5e76\u53d1\u8fd0\u884c\u7684\u73b0\u5b9e\u573a\u666f\u3002\u6b64\u5916\uff0cConsumerBench\u652f\u6301\u81ea\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u6a21\u62df\u9700\u8981\u591a\u5e94\u7528\u534f\u8c03\u7684\u590d\u6742\u4efb\u52a1\u3002\u5b83\u6355\u83b7\u4e86\u5e94\u7528\u7ea7\u6307\u6807\uff08\u5982\u5ef6\u8fdf\u548c\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u8fbe\u6210\u7387\uff09\u548c\u7cfb\u7edf\u7ea7\u6307\u6807\uff08\u5982CPU/GPU\u5229\u7528\u7387\u548c\u5185\u5b58\u5e26\u5bbd\uff09\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cConsumerBench\u63ed\u793a\u4e86\u8d44\u6e90\u5171\u4eab\u7684\u4f4e\u6548\u6027\u3001\u8d2a\u5a6a\u5206\u914d\u4e0b\u7684\u4e0d\u516c\u5e73\u8c03\u5ea6\u4ee5\u53ca\u9759\u6001\u6a21\u578b\u670d\u52a1\u5668\u914d\u7f6e\u7684\u6027\u80fd\u7f3a\u9677\u3002\u672c\u6587\u8fd8\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u548c\u7cfb\u7edf\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u9488\u5bf9\u6d88\u8d39\u7ea7GPU\u67b6\u6784\u7684\u5b9a\u5236\u5185\u6838\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u5b9e\u65bdSLO\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.18496", "pdf": "https://arxiv.org/pdf/2506.18496", "abs": "https://arxiv.org/abs/2506.18496", "authors": ["Seonghak Kim"], "title": "Biased Teacher, Balanced Student", "categories": ["cs.CV"], "comment": "12 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Knowledge Distillation (KD) is a widely adopted model compression technique\nwhere a compact student model learns from the output of a larger, pre-trained\nteacher. While effective in balanced settings, conventional KD suffers\nsignificantly when applied to long-tailed data distributions, as the teacher\nmodel tends to be biased toward head classes and provides limited supervision\nfor tail classes. In this paper, we propose Long-Tailed Knowledge Distillation\n(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by\nreformulating the standard KD objective into two components: inter-group and\nintra-group Kullback-Leibler (KL) divergence, corresponding to the prediction\ndistributions across and within class groups (head, medium, tail),\nrespectively. This decomposition allows us to identify and quantify the sources\nof teacher bias. To address them, we introduce (1) a rebalanced inter-group\nloss that calibrates the teacher's group-level predictions and (2) a uniform\nintra-group loss that ensures equal contribution from all groups during\ndistillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and\nImageNet-LT show that LTKD consistently outperforms existing KD methods,\nachieving significant gains in both overall accuracy and tail-class\nperformance. Our results demonstrate that LTKD enables effective knowledge\ntransfer even from biased teachers, making it a strong candidate for real-world\ndeployment in resource-constrained and imbalanced settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u957f\u5c3e\u6570\u636e\u5206\u5e03\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6LTKD\uff0c\u901a\u8fc7\u5206\u89e3\u6807\u51c6KD\u76ee\u6807\u4e3a\u7ec4\u95f4\u548c\u7ec4\u5185KL\u6563\u5ea6\uff0c\u6821\u51c6\u6559\u5e08\u6a21\u578b\u7684\u9884\u6d4b\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u5728\u5c3e\u7c7b\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5728\u957f\u5c3e\u6570\u636e\u5206\u5e03\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6559\u5e08\u6a21\u578b\u5bf9\u5934\u90e8\u7c7b\u522b\u7684\u9884\u6d4b\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u5c3e\u7c7b\u76d1\u7763\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u573a\u666f\u7684KD\u6846\u67b6\u3002", "method": "LTKD\u5c06\u6807\u51c6KD\u76ee\u6807\u5206\u89e3\u4e3a\u7ec4\u95f4\u548c\u7ec4\u5185KL\u6563\u5ea6\uff0c\u5206\u522b\u5bf9\u5e94\u7c7b\u522b\u7ec4\u95f4\u548c\u7ec4\u5185\u7684\u9884\u6d4b\u5206\u5e03\u3002\u901a\u8fc7\u5f15\u5165\u91cd\u65b0\u5e73\u8861\u7684\u7ec4\u95f4\u635f\u5931\u548c\u5747\u5300\u7684\u7ec4\u5185\u635f\u5931\uff0c\u6821\u51c6\u6559\u5e08\u6a21\u578b\u7684\u9884\u6d4b\u504f\u5dee\u3002", "result": "\u5728CIFAR-100-LT\u3001TinyImageNet-LT\u548cImageNet-LT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLTKD\u663e\u8457\u4f18\u4e8e\u73b0\u6709KD\u65b9\u6cd5\uff0c\u6574\u4f53\u51c6\u786e\u7387\u548c\u5c3e\u7c7b\u6027\u80fd\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LTKD\u80fd\u591f\u6709\u6548\u4ece\u504f\u5dee\u6559\u5e08\u6a21\u578b\u4e2d\u8f6c\u79fb\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u5b9e\u9645\u573a\u666f\u3002", "paper_title_zh": "\u504f\u89c1\u7684\u6559\u5e08\uff0c\u5e73\u8861\u7684\u5b66\u751f", "abstract_zh": "\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u662f\u4e00\u79cd\u5e7f\u6cdb\u91c7\u7528\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u5176\u4e2d\u7d27\u51d1\u7684\u5b66\u751f\u6a21\u578b\u4ece\u9884\u8bad\u7ec3\u7684\u5927\u578b\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u4e2d\u5b66\u4e60\u3002\u5c3d\u7ba1\u5728\u5e73\u8861\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f20\u7edfKD\u5728\u957f\u5c3e\u6570\u636e\u5206\u5e03\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u4e3a\u6559\u5e08\u6a21\u578b\u503e\u5411\u4e8e\u504f\u5411\u5934\u90e8\u7c7b\u522b\uff0c\u5bf9\u5c3e\u7c7b\u522b\u7684\u76d1\u7763\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u957f\u5c3e\u77e5\u8bc6\u84b8\u998f\uff08LTKD\uff09\uff0c\u4e00\u79cd\u4e13\u4e3a\u7c7b\u522b\u4e0d\u5e73\u8861\u573a\u666f\u8bbe\u8ba1\u7684\u65b0\u6846\u67b6\u3002\u6211\u4eec\u9996\u5148\u5c06\u6807\u51c6KD\u76ee\u6807\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u7ec4\u95f4\u548c\u7ec4\u5185Kullback-Leibler\uff08KL\uff09\u6563\u5ea6\uff0c\u5206\u522b\u5bf9\u5e94\u7c7b\u522b\u7ec4\u95f4\u548c\u7ec4\u5185\u7684\u9884\u6d4b\u5206\u5e03\u3002\u8fd9\u79cd\u5206\u89e3\u4f7f\u6211\u4eec\u80fd\u591f\u8bc6\u522b\u548c\u91cf\u5316\u6559\u5e08\u504f\u5dee\u7684\u6765\u6e90\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\uff081\uff09\u91cd\u65b0\u5e73\u8861\u7684\u7ec4\u95f4\u635f\u5931\uff0c\u6821\u51c6\u6559\u5e08\u6a21\u578b\u5728\u7ec4\u7ea7\u522b\u7684\u9884\u6d4b\uff1b\uff082\uff09\u5747\u5300\u7684\u7ec4\u5185\u635f\u5931\uff0c\u786e\u4fdd\u84b8\u998f\u8fc7\u7a0b\u4e2d\u6240\u6709\u7c7b\u522b\u7684\u8d21\u732e\u5747\u7b49\u3002\u5728CIFAR-100-LT\u3001TinyImageNet-LT\u548cImageNet-LT\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLTKD\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709KD\u65b9\u6cd5\uff0c\u5728\u6574\u4f53\u51c6\u786e\u7387\u548c\u5c3e\u7c7b\u522b\u6027\u80fd\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cLTKD\u80fd\u591f\u4ece\u504f\u5dee\u6559\u5e08\u6a21\u578b\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4f7f\u5176\u6210\u4e3a\u8d44\u6e90\u53d7\u9650\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5b9e\u9645\u90e8\u7f72\u7684\u6709\u529b\u5019\u9009\u65b9\u6848\u3002"}}
{"id": "2506.17551", "pdf": "https://arxiv.org/pdf/2506.17551", "abs": "https://arxiv.org/abs/2506.17551", "authors": ["Haowei Yang", "Yu Tian", "Zhongheng Yang", "Zhao Wang", "Chengrui Zhou", "Dannier Li"], "title": "Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "With the rapid adoption of large language models (LLMs) in recommendation\nsystems, the computational and communication bottlenecks caused by their\nmassive parameter sizes and large data volumes have become increasingly\nprominent. This paper systematically investigates two classes of optimization\nmethods-model parallelism and data parallelism-for distributed training of LLMs\nin recommendation scenarios. For model parallelism, we implement both tensor\nparallelism and pipeline parallelism, and introduce an adaptive load-balancing\nmechanism to reduce cross-device communication overhead. For data parallelism,\nwe compare synchronous and asynchronous modes, combining gradient compression\nand sparsification techniques with an efficient aggregation communication\nframework to significantly improve bandwidth utilization. Experiments conducted\non a real-world recommendation dataset in a simulated service environment\ndemonstrate that our proposed hybrid parallelism scheme increases training\nthroughput by over 30% and improves resource utilization by approximately 20%\ncompared to traditional single-mode parallelism, while maintaining strong\nscalability and robustness. Finally, we discuss trade-offs among different\nparallel strategies in online deployment and outline future directions\ninvolving heterogeneous hardware integration and automated scheduling\ntechnologies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5e76\u884c\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5e9e\u5927\u7684\u53c2\u6570\u91cf\u548c\u6570\u636e\u91cf\u5bfc\u81f4\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u74f6\u9888\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u4f18\u5316\u5e76\u884c\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6a21\u578b\u5e76\u884c\uff08\u5305\u62ec\u5f20\u91cf\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\uff09\u548c\u6570\u636e\u5e76\u884c\uff08\u540c\u6b65\u4e0e\u5f02\u6b65\u6a21\u5f0f\uff09\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u548c\u68af\u5ea6\u538b\u7f29\u6280\u672f\uff0c\u4f18\u5316\u901a\u4fe1\u5f00\u9500\u548c\u5e26\u5bbd\u5229\u7528\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u5e76\u884c\u65b9\u6848\u5728\u771f\u5b9e\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534730%\u4ee5\u4e0a\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u9ad8\u7ea620%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u9a8c\u8bc1\u4e86\u6df7\u5408\u5e76\u884c\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u7ebf\u90e8\u7f72\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5f02\u6784\u786c\u4ef6\u96c6\u6210\u548c\u81ea\u52a8\u5316\u8c03\u5ea6\u6280\u672f\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u6a21\u578b\u5e76\u884c\u4e0e\u6570\u636e\u5e76\u884c\u4f18\u5316\u65b9\u6cd5\u7814\u7a76", "abstract_zh": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u5176\u5e9e\u5927\u7684\u53c2\u6570\u91cf\u548c\u6570\u636e\u91cf\u5bfc\u81f4\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u74f6\u9888\u65e5\u76ca\u7a81\u51fa\u3002\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u6a21\u578b\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\u2014\u2014\u7528\u4e8e\u63a8\u8350\u573a\u666f\u4e0bLLMs\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u5728\u6a21\u578b\u5e76\u884c\u65b9\u9762\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5f20\u91cf\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u4ee5\u51cf\u5c11\u8de8\u8bbe\u5907\u901a\u4fe1\u5f00\u9500\u3002\u5728\u6570\u636e\u5e76\u884c\u65b9\u9762\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u540c\u6b65\u548c\u5f02\u6b65\u6a21\u5f0f\uff0c\u7ed3\u5408\u68af\u5ea6\u538b\u7f29\u548c\u7a00\u758f\u5316\u6280\u672f\uff0c\u4ee5\u53ca\u9ad8\u6548\u7684\u805a\u5408\u901a\u4fe1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e26\u5bbd\u5229\u7528\u7387\u3002\u5728\u6a21\u62df\u670d\u52a1\u73af\u5883\u4e2d\u5bf9\u771f\u5b9e\u63a8\u8350\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u5355\u4e00\u5e76\u884c\u6a21\u5f0f\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6df7\u5408\u5e76\u884c\u65b9\u6848\u4f7f\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u8d85\u8fc730%\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u9ad8\u7ea620%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u5728\u7ebf\u90e8\u7f72\u4e2d\u4e0d\u540c\u5e76\u884c\u7b56\u7565\u7684\u6743\u8861\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5f02\u6784\u786c\u4ef6\u96c6\u6210\u548c\u81ea\u52a8\u5316\u8c03\u5ea6\u6280\u672f\u3002"}}
{"id": "2506.18504", "pdf": "https://arxiv.org/pdf/2506.18504", "abs": "https://arxiv.org/abs/2506.18504", "authors": ["Xinyao Li", "Jingjing Li", "Fengling Li", "Lei Zhu", "Yang Yang", "Heng Tao Shen"], "title": "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, vision-language pretraining has emerged as a transformative\ntechnique that integrates the strengths of both visual and textual modalities,\nresulting in powerful vision-language models (VLMs). Leveraging web-scale\npretraining data, these models exhibit strong zero-shot capabilities. However,\ntheir performance often deteriorates when confronted with domain-specific or\nspecialized generalization tasks. To address this, a growing body of research\nfocuses on transferring or generalizing the rich knowledge embedded in VLMs to\nvarious downstream applications. This survey aims to comprehensively summarize\nthe generalization settings, methodologies, benchmarking and results in VLM\nliteratures. Delving into the typical VLM structures, current literatures are\ncategorized into prompt-based, parameter-based and feature-based methods\naccording to the transferred modules. The differences and characteristics in\neach category are furthered summarized and discussed by revisiting the typical\ntransfer learning (TL) settings, providing novel interpretations for TL in the\nera of VLMs. Popular benchmarks for VLM generalization are further introduced\nwith thorough performance comparisons among the reviewed methods. Following the\nadvances in large-scale generalizable pretraining, this survey also discusses\nthe relations and differences between VLMs and up-to-date multimodal large\nlanguage models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the\nsurging literatures in vision-language research from a novel and practical\ngeneralization prospective, this survey contributes to a clear landscape of\ncurrent and future multimodal researches.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u65b0\u9886\u57df\u7684\u6cdb\u5316\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u7684\u8bbe\u7f6e\u3001\u65b9\u6cd5\u3001\u57fa\u51c6\u53ca\u7ed3\u679c\uff0c\u5e76\u63a2\u8ba8\u4e86VLMs\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5173\u7cfb\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u6216\u4e13\u4e1a\u5316\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u603b\u7ed3\u5982\u4f55\u5c06VLMs\u7684\u77e5\u8bc6\u6cdb\u5316\u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6e05\u6670\u7684\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5178\u578bVLM\u7ed3\u6784\uff0c\u5c06\u73b0\u6709\u6587\u732e\u5206\u4e3a\u57fa\u4e8e\u63d0\u793a\u3001\u57fa\u4e8e\u53c2\u6570\u548c\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u6bcf\u7c7b\u65b9\u6cd5\u7684\u5dee\u5f02\u4e0e\u7279\u70b9\u3002\u540c\u65f6\u56de\u987e\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728VLM\u65f6\u4ee3\u7684\u5e94\u7528\uff0c\u5e76\u4ecb\u7ecd\u4e86\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e86VLMs\u6cdb\u5316\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86VLMs\u4e0eMLLMs\uff08\u5982DeepSeek-VL\uff09\u7684\u5173\u7cfb\u4e0e\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u4e3a\u89c6\u89c9-\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6cdb\u5316\u89c6\u89d2\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "paper_title_zh": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u9886\u57df\u7684\u6cdb\u5316\uff1a\u5168\u9762\u7efc\u8ff0", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4f5c\u4e3a\u4e00\u79cd\u6574\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u4f18\u52bf\u7684\u6280\u672f\uff0c\u50ac\u751f\u4e86\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002\u8fd9\u4e9b\u6a21\u578b\u5229\u7528\u7f51\u7edc\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u9762\u5bf9\u7279\u5b9a\u9886\u57df\u6216\u4e13\u4e1a\u5316\u6cdb\u5316\u4efb\u52a1\u65f6\uff0c\u5176\u6027\u80fd\u5f80\u5f80\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u81f4\u529b\u4e8e\u5c06VLMs\u4e2d\u5d4c\u5165\u7684\u4e30\u5bcc\u77e5\u8bc6\u8fc1\u79fb\u6216\u6cdb\u5316\u5230\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u4e2d\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u5168\u9762\u603b\u7ed3VLM\u6587\u732e\u4e2d\u7684\u6cdb\u5316\u8bbe\u7f6e\u3001\u65b9\u6cd5\u3001\u57fa\u51c6\u53ca\u7ed3\u679c\u3002\u901a\u8fc7\u6df1\u5165\u5206\u6790\u5178\u578bVLM\u7ed3\u6784\uff0c\u73b0\u6709\u6587\u732e\u88ab\u5f52\u7c7b\u4e3a\u57fa\u4e8e\u63d0\u793a\u3001\u57fa\u4e8e\u53c2\u6570\u548c\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u8fc1\u79fb\u6a21\u5757\u8fdb\u4e00\u6b65\u603b\u7ed3\u548c\u8ba8\u8bba\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u5dee\u5f02\u4e0e\u7279\u70b9\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u56de\u987e\u5178\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u8bbe\u7f6e\uff0c\u4e3aVLM\u65f6\u4ee3\u7684TL\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u8bfb\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86VLM\u6cdb\u5316\u7684\u6d41\u884c\u57fa\u51c6\uff0c\u5e76\u5bf9\u6240\u7efc\u8ff0\u65b9\u6cd5\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u3002\u968f\u7740\u5927\u89c4\u6a21\u53ef\u6cdb\u5316\u9884\u8bad\u7ec3\u7684\u8fdb\u5c55\uff0c\u672c\u7efc\u8ff0\u8fd8\u63a2\u8ba8\u4e86VLMs\u4e0e\u6700\u65b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff0c\u5982DeepSeek-VL\uff09\u7684\u5173\u7cfb\u4e0e\u5dee\u5f02\u3002\u901a\u8fc7\u4ece\u65b0\u9896\u4e14\u5b9e\u7528\u7684\u6cdb\u5316\u89c6\u89d2\u7cfb\u7edf\u68b3\u7406\u89c6\u89c9-\u8bed\u8a00\u7814\u7a76\u7684\u6fc0\u589e\u6587\u732e\uff0c\u672c\u7efc\u8ff0\u4e3a\u5f53\u524d\u548c\u672a\u6765\u7684\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u56fe\u666f\u3002"}}
{"id": "2506.18512", "pdf": "https://arxiv.org/pdf/2506.18512", "abs": "https://arxiv.org/abs/2506.18512", "authors": ["Yuting Zhang", "Kaishen Yuan", "Hao Lu", "Yutao Yue", "Jintai Chen", "Kaishun Wu"], "title": "MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and interpretable multi-disease diagnosis remains a critical\nchallenge in medical research, particularly when leveraging heterogeneous\nmultimodal medical data. Current approaches often rely on single-modal data,\nlimiting their ability to comprehensively understand complex diseases. To\naddress this, we propose MedTVT-R1, a novel Multimodal Large Language Model\n(MLLM) framework designed to integrate clinical multimodal data for reasoning\nand diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction\ndataset that provides question-answer pairs for physiological-level\ninterpretations and disease-level diagnoses with a Chain of Evidence approach.\nMedTVT-R1 incorporates a modality perception layer to capture inter-modal\ndependencies and adaptively weight modality contributions. Additionally, we\nemploy Group Relative Policy Optimization (GRPO)-based Reinforcement\nFine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.\nExperimental results demonstrate MedTVT-R1's superiority in multimodal feature\nutilization and multi-disease diagnosis, offering significant potential for\nclinical applications such as diagnostic report generation and comorbidity\nreasoning. The dataset and code are available at\nhttps://github.com/keke-nice/MedTVT-R1.", "AI": {"tldr": "MedTVT-R1\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u65e8\u5728\u6574\u5408\u4e34\u5e8a\u591a\u6a21\u6001\u6570\u636e\u4ee5\u8fdb\u884c\u591a\u75be\u75c5\u8bca\u65ad\u548c\u63a8\u7406\u3002\u901a\u8fc7\u6784\u5efaMedTVT-QA\u6570\u636e\u96c6\u5e76\u91c7\u7528GRPO\u5f3a\u5316\u5fae\u8c03\uff0c\u6a21\u578b\u5728\u591a\u6a21\u6001\u7279\u5f81\u5229\u7528\u548c\u591a\u75be\u75c5\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u8bca\u65ad\u65b9\u6cd5\u591a\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\uff0c\u96be\u4ee5\u5168\u9762\u7406\u89e3\u590d\u6742\u75be\u75c5\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86MedTVT-QA\u6570\u636e\u96c6\uff0c\u5305\u542b\u751f\u7406\u7ea7\u89e3\u91ca\u548c\u75be\u75c5\u7ea7\u8bca\u65ad\u7684\u95ee\u7b54\u5bf9\u3002\u6a21\u578b\u91c7\u7528\u6a21\u6001\u611f\u77e5\u5c42\u6355\u6349\u6a21\u6001\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7GRPO\u5f3a\u5316\u5fae\u8c03\u548cJaccard\u5956\u52b1\u51fd\u6570\u4f18\u5316\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMedTVT-R1\u5728\u591a\u6a21\u6001\u7279\u5f81\u5229\u7528\u548c\u591a\u75be\u75c5\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u751f\u6210\u8bca\u65ad\u62a5\u544a\u548c\u5171\u75c5\u63a8\u7406\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "MedTVT-R1\u4e3a\u591a\u6a21\u6001\u533b\u7597\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u5e94\u7528\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "paper_title_zh": "MedTVT-R1\uff1a\u4e00\u79cd\u8d4b\u80fd\u533b\u7597\u63a8\u7406\u4e0e\u8bca\u65ad\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b", "abstract_zh": "\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u75be\u75c5\u8bca\u65ad\u662f\u533b\u7597\u7814\u7a76\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5229\u7528\u5f02\u6784\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u590d\u6742\u75be\u75c5\u7684\u5168\u9762\u7406\u89e3\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MedTVT-R1\uff0c\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6846\u67b6\uff0c\u65e8\u5728\u6574\u5408\u4e34\u5e8a\u591a\u6a21\u6001\u6570\u636e\u4ee5\u8fdb\u884c\u63a8\u7406\u548c\u591a\u75be\u75c5\u8bca\u65ad\u3002\u6211\u4eec\u6784\u5efa\u4e86MedTVT-QA\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u8bc1\u636e\u94fe\u65b9\u6cd5\u63d0\u4f9b\u4e86\u751f\u7406\u7ea7\u89e3\u91ca\u548c\u75be\u75c5\u7ea7\u8bca\u65ad\u7684\u95ee\u7b54\u5bf9\u3002MedTVT-R1\u91c7\u7528\u6a21\u6001\u611f\u77e5\u5c42\u6355\u6349\u6a21\u6001\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u81ea\u9002\u5e94\u52a0\u6743\u6a21\u6001\u8d21\u732e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5f3a\u5316\u5fae\u8c03\u548cJaccard\u5956\u52b1\u51fd\u6570\uff0c\u4f18\u5316\u4e86\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMedTVT-R1\u5728\u591a\u6a21\u6001\u7279\u5f81\u5229\u7528\u548c\u591a\u75be\u75c5\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3a\u8bca\u65ad\u62a5\u544a\u751f\u6210\u548c\u5171\u75c5\u63a8\u7406\u7b49\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6f5c\u529b\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u53d1\u5e03\u4e8ehttps://github.com/keke-nice/MedTVT-R1\u3002"}}
{"id": "2506.17560", "pdf": "https://arxiv.org/pdf/2506.17560", "abs": "https://arxiv.org/abs/2506.17560", "authors": ["Ava Abderezaei", "Chi-Hui Lin", "Joseph Miceli", "Naren Sivagnanadasan", "St\u00e9phane Aroca-Ouellette", "Jake Brawer", "Alessandro Roncone"], "title": "Towards Zero-Shot Coordination between Teams of Agents: The N-XPlay Framework", "categories": ["cs.MA", "cs.AI"], "comment": "Accepted to RSS Workshop on Scalable and Resilient Multi-Robot\n  Systems: Decision-Making, Coordination, and Learning 2025", "summary": "Zero-shot coordination (ZSC) -- the ability to collaborate with unfamiliar\npartners -- is essential to making autonomous agents effective teammates.\nExisting ZSC methods evaluate coordination capabilities between two agents who\nhave not previously interacted. However, these scenarios do not reflect the\ncomplexity of real-world multi-agent systems, where coordination often involves\na hierarchy of sub-groups and interactions between teams of agents, known as\nMulti-Team Systems (MTS). To address this gap, we first introduce N-player\nOvercooked, an N-agent extension of the popular two-agent ZSC benchmark,\nenabling evaluation of ZSC in N-agent scenarios. We then propose N-XPlay for\nZSC in N-agent, multi-team settings. Comparison against Self-Play across two-,\nthree- and five-player Overcooked scenarios, where agents are split between an\n``ego-team'' and a group of unseen collaborators shows that agents trained with\nN-XPlay are better able to simultaneously balance ``intra-team'' and\n``inter-team'' coordination than agents trained with SP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86N-XPlay\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u56e2\u961f\u95f4\u7684\u96f6\u6837\u672c\u534f\u4f5c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u7684N\u73a9\u5bb6Overcooked\u73af\u5883\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u81ea\u535a\u5f08\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u534f\u4f5c\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u4e24\u667a\u80fd\u4f53\u573a\u666f\uff0c\u65e0\u6cd5\u53cd\u6620\u73b0\u5b9e\u591a\u56e2\u961f\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u652f\u6301\u591a\u667a\u80fd\u4f53\u56e2\u961f\u534f\u4f5c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u6269\u5c55\u4e86Overcooked\u73af\u5883\u4e3aN\u73a9\u5bb6\u7248\u672c\uff0c\u968f\u540e\u63d0\u51faN-XPlay\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u667a\u80fd\u4f53\u5728\u591a\u56e2\u961f\u573a\u666f\u4e2d\u540c\u65f6\u4f18\u5316\u56e2\u961f\u5185\u548c\u56e2\u961f\u95f4\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cN-XPlay\u5728\u4e8c\u3001\u4e09\u548c\u4e94\u73a9\u5bb6Overcooked\u573a\u666f\u4e2d\uff0c\u6bd4\u81ea\u535a\u5f08\u65b9\u6cd5\u66f4\u80fd\u5e73\u8861\u56e2\u961f\u5185\u548c\u56e2\u961f\u95f4\u534f\u4f5c\u3002", "conclusion": "N-XPlay\u4e3a\u591a\u667a\u80fd\u4f53\u56e2\u961f\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u96f6\u6837\u672c\u534f\u4f5c\u5728\u591a\u56e2\u961f\u573a\u666f\u4e2d\u7684\u7a7a\u767d\u3002", "paper_title_zh": "\u9762\u5411\u591a\u667a\u80fd\u4f53\u56e2\u961f\u95f4\u96f6\u6837\u672c\u534f\u4f5c\u7684N-XPlay\u6846\u67b6", "abstract_zh": "\u96f6\u6837\u672c\u534f\u4f5c\uff08ZSC\uff09\u2014\u2014\u5373\u4e0e\u964c\u751f\u4f19\u4f34\u534f\u4f5c\u7684\u80fd\u529b\u2014\u2014\u662f\u4f7f\u81ea\u4e3b\u667a\u80fd\u4f53\u6210\u4e3a\u6709\u6548\u961f\u53cb\u7684\u5173\u952e\u3002\u73b0\u6709ZSC\u65b9\u6cd5\u4ec5\u8bc4\u4f30\u4e24\u4e2a\u672a\u4ea4\u4e92\u8fc7\u7684\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u573a\u666f\u65e0\u6cd5\u53cd\u6620\u73b0\u5b9e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u5176\u4e2d\u534f\u4f5c\u901a\u5e38\u6d89\u53ca\u5b50\u7fa4\u5c42\u6b21\u7ed3\u6784\u548c\u591a\u56e2\u961f\u7cfb\u7edf\uff08MTS\uff09\u95f4\u7684\u4ea4\u4e92\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86N\u73a9\u5bb6Overcooked\uff0c\u8fd9\u662f\u6d41\u884c\u4e24\u667a\u80fd\u4f53ZSC\u57fa\u51c6\u7684N\u667a\u80fd\u4f53\u6269\u5c55\uff0c\u652f\u6301\u5728N\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u8bc4\u4f30ZSC\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86N-XPlay\uff0c\u7528\u4e8eN\u667a\u80fd\u4f53\u591a\u56e2\u961f\u573a\u666f\u4e2d\u7684ZSC\u3002\u901a\u8fc7\u5728\u4e8c\u3001\u4e09\u548c\u4e94\u73a9\u5bb6Overcooked\u573a\u666f\u4e2d\u4e0e\u81ea\u535a\u5f08\u65b9\u6cd5\u5bf9\u6bd4\uff08\u667a\u80fd\u4f53\u88ab\u5206\u4e3a\u201c\u81ea\u6211\u56e2\u961f\u201d\u548c\u4e00\u7ec4\u672a\u89c1\u8fc7\u7684\u534f\u4f5c\u8005\uff09\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528N-XPlay\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6bd4\u81ea\u535a\u5f08\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u66f4\u80fd\u540c\u65f6\u5e73\u8861\u201c\u56e2\u961f\u5185\u201d\u548c\u201c\u56e2\u961f\u95f4\u201d\u534f\u4f5c\u3002"}}
{"id": "2506.18520", "pdf": "https://arxiv.org/pdf/2506.18520", "abs": "https://arxiv.org/abs/2506.18520", "authors": ["JiaKui Hu", "Zhengjian Yao", "Lujia Jin", "Hangzhou He", "Yanye Lu"], "title": "Enhancing Image Restoration Transformer via Adaptive Translation Equivariance", "categories": ["cs.CV"], "comment": null, "summary": "Translation equivariance is a fundamental inductive bias in image\nrestoration, ensuring that translated inputs produce translated outputs.\nAttention mechanisms in modern restoration transformers undermine this\nproperty, adversely impacting both training convergence and generalization. To\nalleviate this issue, we propose two key strategies for incorporating\ntranslation equivariance: slide indexing and component stacking. Slide indexing\nmaintains operator responses at fixed positions, with sliding window attention\nbeing a notable example, while component stacking enables the arrangement of\ntranslation-equivariant operators in parallel or sequentially, thereby building\ncomplex architectures while preserving translation equivariance. However, these\nstrategies still create a dilemma in model design between the high\ncomputational cost of self-attention and the fixed receptive field associated\nwith sliding window attention. To address this, we develop an adaptive sliding\nindexing mechanism to efficiently select key-value pairs for each query, which\nare then concatenated in parallel with globally aggregated key-value pairs. The\ndesigned network, called the Translation Equivariance Adaptive Transformer\n(TEAFormer), is assessed across a variety of image restoration tasks. The\nresults highlight its superiority in terms of effectiveness, training\nconvergence, and generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5e73\u79fb\u7b49\u53d8\u6027\u7684\u56fe\u50cf\u6062\u590dTransformer\uff08TEAFormer\uff09\uff0c\u901a\u8fc7\u6ed1\u52a8\u7d22\u5f15\u548c\u7ec4\u4ef6\u5806\u53e0\u7b56\u7565\u89e3\u51b3\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7834\u574f\u5e73\u79fb\u7b49\u53d8\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u56fe\u50cf\u6062\u590dTransformer\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u7834\u574f\u4e86\u5e73\u79fb\u7b49\u53d8\u6027\uff0c\u5f71\u54cd\u8bad\u7ec3\u6536\u655b\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u5e73\u79fb\u7b49\u53d8\u6027\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u6ed1\u52a8\u7d22\u5f15\u548c\u7ec4\u4ef6\u5806\u53e0\u4e24\u79cd\u7b56\u7565\uff1a\u6ed1\u52a8\u7d22\u5f15\u4fdd\u6301\u56fa\u5b9a\u4f4d\u7f6e\u7684\u7b97\u5b50\u54cd\u5e94\uff08\u5982\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff09\uff0c\u7ec4\u4ef6\u5806\u53e0\u5141\u8bb8\u5e76\u884c\u6216\u987a\u5e8f\u6392\u5217\u5e73\u79fb\u7b49\u53d8\u7b97\u5b50\u3002\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u81ea\u9002\u5e94\u6ed1\u52a8\u7d22\u5f15\u673a\u5236\uff0c\u9ad8\u6548\u9009\u62e9\u67e5\u8be2\u7684\u952e\u503c\u5bf9\u5e76\u4e0e\u5168\u5c40\u805a\u5408\u952e\u503c\u5bf9\u5e76\u884c\u62fc\u63a5\u3002", "result": "\u63d0\u51fa\u7684TEAFormer\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6548\u679c\u3001\u8bad\u7ec3\u6536\u655b\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "TEAFormer\u901a\u8fc7\u81ea\u9002\u5e94\u5e73\u79fb\u7b49\u53d8\u6027\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u81ea\u9002\u5e94\u5e73\u79fb\u7b49\u53d8\u6027\u589e\u5f3a\u56fe\u50cf\u6062\u590dTransformer", "abstract_zh": "\u5e73\u79fb\u7b49\u53d8\u6027\u662f\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u57fa\u672c\u5f52\u7eb3\u504f\u7f6e\uff0c\u786e\u4fdd\u5e73\u79fb\u8f93\u5165\u4ea7\u751f\u5e73\u79fb\u8f93\u51fa\u3002\u73b0\u4ee3\u6062\u590dTransformer\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u7834\u574f\u4e86\u8fd9\u4e00\u6027\u8d28\uff0c\u5bf9\u8bad\u7ec3\u6536\u655b\u548c\u6cdb\u5316\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u4e3a\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e24\u79cd\u7ed3\u5408\u5e73\u79fb\u7b49\u53d8\u6027\u7684\u5173\u952e\u7b56\u7565\uff1a\u6ed1\u52a8\u7d22\u5f15\u548c\u7ec4\u4ef6\u5806\u53e0\u3002\u6ed1\u52a8\u7d22\u5f15\u4fdd\u6301\u56fa\u5b9a\u4f4d\u7f6e\u7684\u7b97\u5b50\u54cd\u5e94\uff08\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u662f\u5178\u578b\u4f8b\u5b50\uff09\uff0c\u800c\u7ec4\u4ef6\u5806\u53e0\u5141\u8bb8\u5e76\u884c\u6216\u987a\u5e8f\u6392\u5217\u5e73\u79fb\u7b49\u53d8\u7b97\u5b50\uff0c\u4ece\u800c\u6784\u5efa\u590d\u6742\u67b6\u6784\u7684\u540c\u65f6\u4fdd\u6301\u5e73\u79fb\u7b49\u53d8\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7b56\u7565\u4ecd\u9762\u4e34\u81ea\u6ce8\u610f\u529b\u9ad8\u8ba1\u7b97\u6210\u672c\u4e0e\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u56fa\u5b9a\u611f\u53d7\u91ce\u4e4b\u95f4\u7684\u8bbe\u8ba1\u56f0\u5883\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6ed1\u52a8\u7d22\u5f15\u673a\u5236\uff0c\u9ad8\u6548\u9009\u62e9\u6bcf\u4e2a\u67e5\u8be2\u7684\u952e\u503c\u5bf9\uff0c\u5e76\u4e0e\u5168\u5c40\u805a\u5408\u952e\u503c\u5bf9\u5e76\u884c\u62fc\u63a5\u3002\u8bbe\u8ba1\u7684\u7f51\u7edc\u79f0\u4e3a\u5e73\u79fb\u7b49\u53d8\u81ea\u9002\u5e94Transformer\uff08TEAFormer\uff09\uff0c\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u51f8\u663e\u4e86\u5176\u5728\u6548\u679c\u3001\u8bad\u7ec3\u6536\u655b\u548c\u6cdb\u5316\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.18523", "pdf": "https://arxiv.org/pdf/2506.18523", "abs": "https://arxiv.org/abs/2506.18523", "authors": ["Kei Taguchi", "Kazumasa Ohara", "Tatsuya Yokota", "Hiroaki Miyoshi", "Noriaki Hashimoto", "Ichiro Takeuchi", "Hidekata Hontani"], "title": "Multi-Scale Representation of Follicular Lymphoma Pathology Images in a Single Hyperbolic Space", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "We propose a method for representing malignant lymphoma pathology images,\nfrom high-resolution cell nuclei to low-resolution tissue images, within a\nsingle hyperbolic space using self-supervised learning. To capture\nmorphological changes that occur across scales during disease progression, our\napproach embeds tissue and corresponding nucleus images close to each other\nbased on inclusion relationships. Using the Poincar\\'e ball as the feature\nspace enables effective encoding of this hierarchical structure. The learned\nrepresentations capture both disease state and cell type variations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8868\u793a\u6076\u6027\u6dcb\u5df4\u7624\u75c5\u7406\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u4ece\u9ad8\u5206\u8fa8\u7387\u7ec6\u80de\u6838\u5230\u4f4e\u5206\u8fa8\u7387\u7ec4\u7ec7\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "motivation": "\u4e3a\u4e86\u6355\u6349\u75be\u75c5\u8fdb\u5c55\u4e2d\u8de8\u5c3a\u5ea6\u7684\u5f62\u6001\u53d8\u5316\uff0c\u7814\u7a76\u65e8\u5728\u5c06\u7ec4\u7ec7\u548c\u7ec6\u80de\u6838\u56fe\u50cf\u5d4c\u5165\u540c\u4e00\u53cc\u66f2\u7a7a\u95f4\uff0c\u4ee5\u7f16\u7801\u5176\u5c42\u6b21\u7ed3\u6784\u3002", "method": "\u4f7f\u7528Poincar\u00e9\u7403\u4f5c\u4e3a\u7279\u5f81\u7a7a\u95f4\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5c06\u7ec4\u7ec7\u548c\u7ec6\u80de\u6838\u56fe\u50cf\u57fa\u4e8e\u5305\u542b\u5173\u7cfb\u5d4c\u5165\u76f8\u8fd1\u4f4d\u7f6e\u3002", "result": "\u5b66\u4e60\u5230\u7684\u8868\u5f81\u80fd\u591f\u540c\u65f6\u6355\u6349\u75be\u75c5\u72b6\u6001\u548c\u7ec6\u80de\u7c7b\u578b\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f16\u7801\u4e86\u591a\u5c3a\u5ea6\u75c5\u7406\u56fe\u50cf\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u75be\u75c5\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "paper_title_zh": "\u5728\u5355\u4e00\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8868\u793a\u6ee4\u6ce1\u6027\u6dcb\u5df4\u7624\u75c5\u7406\u56fe\u50cf\u7684\u591a\u5c3a\u5ea6\u7279\u5f81", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5355\u4e00\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8868\u793a\u6076\u6027\u6dcb\u5df4\u7624\u75c5\u7406\u56fe\u50cf\uff0c\u6db5\u76d6\u4ece\u9ad8\u5206\u8fa8\u7387\u7ec6\u80de\u6838\u5230\u4f4e\u5206\u8fa8\u7387\u7ec4\u7ec7\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u3002\u4e3a\u4e86\u6355\u6349\u75be\u75c5\u8fdb\u5c55\u4e2d\u8de8\u5c3a\u5ea6\u7684\u5f62\u6001\u53d8\u5316\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u5305\u542b\u5173\u7cfb\u5c06\u7ec4\u7ec7\u548c\u5bf9\u5e94\u7ec6\u80de\u6838\u56fe\u50cf\u5d4c\u5165\u76f8\u8fd1\u4f4d\u7f6e\u3002\u4f7f\u7528Poincar\u00e9\u7403\u4f5c\u4e3a\u7279\u5f81\u7a7a\u95f4\uff0c\u80fd\u591f\u6709\u6548\u7f16\u7801\u8fd9\u79cd\u5c42\u6b21\u7ed3\u6784\u3002\u5b66\u4e60\u5230\u7684\u8868\u5f81\u540c\u65f6\u6355\u6349\u4e86\u75be\u75c5\u72b6\u6001\u548c\u7ec6\u80de\u7c7b\u578b\u7684\u53d8\u5316\u3002"}}
{"id": "2506.17564", "pdf": "https://arxiv.org/pdf/2506.17564", "abs": "https://arxiv.org/abs/2506.17564", "authors": ["Lakshita Dodeja", "Karl Schmeckpeper", "Shivam Vats", "Thomas Weng", "Mingxi Jia", "George Konidaris", "Stefanie Tellex"], "title": "Accelerating Residual Reinforcement Learning with Uncertainty Estimation", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Residual Reinforcement Learning (RL) is a popular approach for adapting\npretrained policies by learning a lightweight residual policy that provides\ncorrective actions. While Residual RL is more sample-efficient than finetuning\nthe entire base policy, existing methods struggle with sparse rewards and are\ndesigned for deterministic base policies. We propose two improvements to\nResidual RL that further enhance its sample efficiency and make it suitable for\nstochastic base policies. First, we leverage uncertainty estimates of the base\npolicy to focus exploration on regions in which the base policy is not\nconfident. Second, we propose a simple modification to off-policy residual\nlearning that allows it to observe base actions and better handle stochastic\nbase policies. We evaluate our method with both Gaussian-based and\nDiffusion-based stochastic base policies on tasks from Robosuite and D4RL, and\ncompare against state-of-the-art finetuning methods, demo-augmented RL methods,\nand other residual RL methods. Our algorithm significantly outperforms existing\nbaselines in a variety of simulation benchmark environments. We also deploy our\nlearned polices in the real world to demonstrate their robustness with\nzero-shot sim-to-real transfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u57fa\u7840\u7b56\u7565\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u4fee\u6539\u79bb\u7b56\u7565\u6b8b\u5dee\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u5e76\u9002\u7528\u4e8e\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002", "motivation": "\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u7b56\u7565\u8c03\u6574\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u6bd4\u5fae\u8c03\u6574\u4e2a\u57fa\u7840\u7b56\u7565\u66f4\u9ad8\u6548\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7a00\u758f\u5956\u52b1\u4e14\u4ec5\u9002\u7528\u4e8e\u786e\u5b9a\u6027\u57fa\u7840\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u63d0\u5347\u5176\u6837\u672c\u6548\u7387\u5e76\u6269\u5c55\u81f3\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002", "method": "1. \u5229\u7528\u57fa\u7840\u7b56\u7565\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5c06\u63a2\u7d22\u96c6\u4e2d\u5728\u57fa\u7840\u7b56\u7565\u4e0d\u81ea\u4fe1\u7684\u533a\u57df\uff1b2. \u4fee\u6539\u79bb\u7b56\u7565\u6b8b\u5dee\u5b66\u4e60\uff0c\u4f7f\u5176\u80fd\u89c2\u5bdf\u57fa\u7840\u52a8\u4f5c\u5e76\u66f4\u597d\u5904\u7406\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002", "result": "\u5728Robosuite\u548cD4RL\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u9ad8\u65af\u548c\u6269\u6563\u7684\u968f\u673a\u57fa\u7840\u7b56\u7565\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u3001\u6f14\u793a\u589e\u5f3aRL\u65b9\u6cd5\u53ca\u5176\u4ed6\u6b8b\u5deeRL\u65b9\u6cd5\uff0c\u5e76\u5728\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u968f\u673a\u57fa\u7840\u7b56\u7565\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "paper_title_zh": "\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u52a0\u901f", "abstract_zh": "\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u4e00\u79cd\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u7b56\u7565\u63d0\u4f9b\u4fee\u6b63\u52a8\u4f5c\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u6d41\u884c\u65b9\u6cd5\u3002\u5c3d\u7ba1\u6b8b\u5deeRL\u6bd4\u5fae\u8c03\u6574\u4e2a\u57fa\u7840\u7b56\u7565\u66f4\u9ad8\u6548\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7a00\u758f\u5956\u52b1\u4e14\u4ec5\u9002\u7528\u4e8e\u786e\u5b9a\u6027\u57fa\u7840\u7b56\u7565\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6837\u672c\u6548\u7387\u5e76\u4f7f\u5176\u9002\u7528\u4e8e\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002\u9996\u5148\uff0c\u5229\u7528\u57fa\u7840\u7b56\u7565\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5c06\u63a2\u7d22\u96c6\u4e2d\u5728\u57fa\u7840\u7b56\u7565\u4e0d\u81ea\u4fe1\u7684\u533a\u57df\uff1b\u5176\u6b21\uff0c\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u79bb\u7b56\u7565\u6b8b\u5dee\u5b66\u4e60\u4fee\u6539\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u89c2\u5bdf\u57fa\u7840\u52a8\u4f5c\u5e76\u66f4\u597d\u5904\u7406\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002\u6211\u4eec\u5728Robosuite\u548cD4RL\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u9ad8\u65af\u548c\u6269\u6563\u7684\u968f\u673a\u57fa\u7840\u7b56\u7565\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u5fae\u8c03\u65b9\u6cd5\u3001\u6f14\u793a\u589e\u5f3aRL\u65b9\u6cd5\u53ca\u5176\u4ed6\u6b8b\u5deeRL\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u591a\u79cd\u4eff\u771f\u57fa\u51c6\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u90e8\u7f72\u4e86\u5b66\u4e60\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.18527", "pdf": "https://arxiv.org/pdf/2506.18527", "abs": "https://arxiv.org/abs/2506.18527", "authors": ["JiaKui Hu", "Yuxiao Yang", "Jialun Liu", "Jinbo Wu", "Chen Zhao", "Yanye Lu"], "title": "Auto-Regressively Generating Multi-View Consistent Images", "categories": ["cs.CV"], "comment": null, "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u81ea\u56de\u5f52\uff08MV-AR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u9010\u6b65\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u57283D\u5185\u5bb9\u521b\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u5904\u7406\u591a\u6837\u5316\u6761\u4ef6\u65f6\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MV-AR\u65b9\u6cd5\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9010\u8bcd\u9884\u6d4b\u80fd\u529b\uff0c\u9010\u6b65\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u3002\u901a\u8fc7\u8bbe\u8ba1\u7edf\u4e00\u6a21\u578b\u548c\u6761\u4ef6\u6ce8\u5165\u6a21\u5757\uff08\u5982\u6587\u672c\u3001\u76f8\u673a\u4f4d\u59ff\u3001\u56fe\u50cf\u548c\u5f62\u72b6\uff09\uff0c\u5e76\u7ed3\u5408\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u201cShuffle View\u201d\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMV-AR\u80fd\u591f\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4e0e\u9886\u5148\u7684\u57fa\u4e8e\u6269\u6563\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "MV-AR\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u591a\u6837\u5316\u6761\u4ef6\u7684\u95ee\u9898\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "paper_title_zh": "\u81ea\u56de\u5f52\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u56fe\u50cf", "abstract_zh": "\u4ece\u4eba\u7c7b\u6307\u4ee4\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u5bf93D\u5185\u5bb9\u521b\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u4fdd\u6301\u591a\u89c6\u89d2\u95f4\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u5728\u591a\u6837\u5316\u6761\u4ef6\u4e0b\u6709\u6548\u5408\u6210\u5f62\u72b6\u548c\u7eb9\u7406\u3002\u672c\u6587\u63d0\u51fa\u591a\u89c6\u89d2\u81ea\u56de\u5f52\uff08MV-AR\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u9010\u6b65\u751f\u6210\u4e0e\u4efb\u610f\u63d0\u793a\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u3002\u9996\u5148\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9010\u8bcd\u9884\u6d4b\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u6e10\u8fdb\u5f0f\u591a\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u6548\u679c\u3002\u5728\u751f\u6210\u5e7f\u6cdb\u5206\u79bb\u7684\u89c6\u89d2\u65f6\uff0cMV-AR\u53ef\u4ee5\u5229\u7528\u5176\u6240\u6709\u5148\u524d\u89c6\u89d2\u63d0\u53d6\u6709\u6548\u53c2\u8003\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u9002\u5e94\u591a\u79cd\u63d0\u793a\u7684\u7edf\u4e00\u6a21\u578b\u3002\u4e3a\u5e94\u5bf9\u591a\u79cd\u6761\u4ef6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6587\u672c\u3001\u76f8\u673a\u4f4d\u59ff\u3001\u56fe\u50cf\u548c\u5f62\u72b6\u7684\u6761\u4ef6\u6ce8\u5165\u6a21\u5757\u3002\u4e3a\u540c\u65f6\u5904\u7406\u591a\u6a21\u6001\u6761\u4ef6\uff0c\u91c7\u7528\u4e86\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u3002\u8be5\u7b56\u7565\u9996\u5148\u4ee5\u6587\u672c\u5230\u591a\u89c6\u89d2\uff08t2mv\uff09\u6a21\u578b\u4e3a\u57fa\u7ebf\uff0c\u901a\u8fc7\u968f\u673a\u4e22\u5f03\u548c\u7ec4\u5408\u6761\u4ef6\uff0c\u9010\u6b65\u5f00\u53d1\u5168\u9762\u7684X\u5230\u591a\u89c6\u89d2\uff08X2mv\uff09\u6a21\u578b\u3002\u6700\u540e\uff0c\u4e3a\u7f13\u89e3\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201cShuffle View\u201d\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4ece\u800c\u5c06\u8bad\u7ec3\u6570\u636e\u6269\u5c55\u4e86\u6570\u4e2a\u6570\u91cf\u7ea7\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86MV-AR\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\uff0c\u5176\u80fd\u591f\u5728\u4e00\u7cfb\u5217\u6761\u4ef6\u4e0b\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u5e76\u4e0e\u9886\u5148\u7684\u57fa\u4e8e\u6269\u6563\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728https://github.com/MILab-PKU/MVAR\u53d1\u5e03\u3002"}}
{"id": "2506.17577", "pdf": "https://arxiv.org/pdf/2506.17577", "abs": "https://arxiv.org/abs/2506.17577", "authors": ["Meng Xia", "Robin Schmucker", "Conrad Borchers", "Vincent Aleven"], "title": "Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "Full research paper accepted at EC-TEL 2025", "summary": "Mastery learning improves learning proficiency and efficiency. However, the\noverpractice of skills--students spending time on skills they have already\nmastered--remains a fundamental challenge for tutoring systems. Previous\nresearch has reduced overpractice through the development of better problem\nselection algorithms and the authoring of focused practice tasks. However, few\nefforts have concentrated on reducing overpractice through step-level\nadaptivity, which can avoid resource-intensive curriculum redesign. We propose\nand evaluate Fast-Forwarding as a technique that enhances existing problem\nselection algorithms. Based on simulation studies informed by learner models\nand problem-solving pathways derived from real student data, Fast-Forwarding\ncan reduce overpractice by up to one-third, as it does not require students to\ncomplete problem-solving steps if all remaining pathways are fully mastered.\nFast-Forwarding is a flexible method that enhances any problem selection\nalgorithm, though its effectiveness is highest for algorithms that\npreferentially select difficult problems. Therefore, our findings suggest that\nwhile Fast-Forwarding may improve student practice efficiency, the size of its\npractical impact may also depend on students' ability to stay motivated and\nengaged at higher levels of difficulty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5feb\u8fdb\u201d\u7684\u6280\u672f\uff0c\u901a\u8fc7\u8df3\u8fc7\u5b66\u751f\u5df2\u638c\u63e1\u7684\u89e3\u9898\u6b65\u9aa4\uff0c\u51cf\u5c11\u8fc7\u5ea6\u7ec3\u4e60\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c11\u9ad8\u8fbe\u4e09\u5206\u4e4b\u4e00\u7684\u8fc7\u5ea6\u7ec3\u4e60\u3002", "motivation": "\u638c\u63e1\u5b66\u4e60\u867d\u80fd\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u5b66\u751f\u5728\u5df2\u638c\u63e1\u6280\u80fd\u4e0a\u7684\u8fc7\u5ea6\u7ec3\u4e60\u4ecd\u662f\u8f85\u5bfc\u7cfb\u7edf\u7684\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u591a\u901a\u8fc7\u6539\u8fdb\u9898\u76ee\u9009\u62e9\u7b97\u6cd5\u6216\u8bbe\u8ba1\u9488\u5bf9\u6027\u7ec3\u4e60\u4efb\u52a1\u6765\u51cf\u5c11\u8fc7\u5ea6\u7ec3\u4e60\uff0c\u4f46\u9c9c\u6709\u5173\u6ce8\u6b65\u9aa4\u7ea7\u522b\u7684\u9002\u5e94\u6027\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u201c\u5feb\u8fdb\u201d\u6280\u672f\uff0c\u57fa\u4e8e\u5b66\u4e60\u8005\u6a21\u578b\u548c\u771f\u5b9e\u5b66\u751f\u6570\u636e\u7684\u89e3\u9898\u8def\u5f84\u6a21\u62df\uff0c\u8df3\u8fc7\u5b66\u751f\u5df2\u5b8c\u5168\u638c\u63e1\u7684\u89e3\u9898\u6b65\u9aa4\uff0c\u65e0\u9700\u5b8c\u6210\u6240\u6709\u6b65\u9aa4\u5373\u53ef\u7ee7\u7eed\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u9898\u76ee\u9009\u62e9\u7b97\u6cd5\uff0c\u5c24\u5176\u9002\u5408\u504f\u597d\u9009\u62e9\u96be\u9898\u7684\u7b97\u6cd5\u3002", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u201c\u5feb\u8fdb\u201d\u6280\u672f\u53ef\u51cf\u5c11\u9ad8\u8fbe\u4e09\u5206\u4e4b\u4e00\u7684\u8fc7\u5ea6\u7ec3\u4e60\u65f6\u95f4\uff0c\u4e14\u5176\u6548\u679c\u5728\u504f\u597d\u9009\u62e9\u96be\u9898\u7684\u7b97\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u201c\u5feb\u8fdb\u201d\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u679c\u8fd8\u53d6\u51b3\u4e8e\u5b66\u751f\u5728\u9ad8\u96be\u5ea6\u5b66\u4e60\u4e2d\u7684\u6301\u7eed\u52a8\u673a\u548c\u53c2\u4e0e\u5ea6\u3002", "paper_title_zh": "\u901a\u8fc7\u5feb\u8fdb\u8df3\u8fc7\u8fc7\u5ea6\u7ec3\u4e60\u6b65\u9aa4\u4f18\u5316\u638c\u63e1\u5b66\u4e60", "abstract_zh": "\u638c\u63e1\u5b66\u4e60\u80fd\u63d0\u5347\u5b66\u4e60\u719f\u7ec3\u5ea6\u548c\u6548\u7387\uff0c\u4f46\u5b66\u751f\u5728\u5df2\u638c\u63e1\u6280\u80fd\u4e0a\u7684\u8fc7\u5ea6\u7ec3\u4e60\u4ecd\u662f\u8f85\u5bfc\u7cfb\u7edf\u7684\u6838\u5fc3\u6311\u6218\u3002\u4ee5\u5f80\u7814\u7a76\u901a\u8fc7\u6539\u8fdb\u9898\u76ee\u9009\u62e9\u7b97\u6cd5\u6216\u8bbe\u8ba1\u9488\u5bf9\u6027\u7ec3\u4e60\u4efb\u52a1\u6765\u51cf\u5c11\u8fc7\u5ea6\u7ec3\u4e60\uff0c\u4f46\u9c9c\u6709\u5173\u6ce8\u6b65\u9aa4\u7ea7\u522b\u7684\u9002\u5e94\u6027\u8c03\u6574\u3002\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u201c\u5feb\u8fdb\u201d\u6280\u672f\uff0c\u901a\u8fc7\u589e\u5f3a\u73b0\u6709\u9898\u76ee\u9009\u62e9\u7b97\u6cd5\uff0c\u8df3\u8fc7\u5b66\u751f\u5df2\u5b8c\u5168\u638c\u63e1\u7684\u89e3\u9898\u6b65\u9aa4\u3002\u57fa\u4e8e\u771f\u5b9e\u5b66\u751f\u6570\u636e\u7684\u5b66\u4e60\u8005\u6a21\u578b\u548c\u89e3\u9898\u8def\u5f84\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u201c\u5feb\u8fdb\u201d\u6280\u672f\u53ef\u51cf\u5c11\u9ad8\u8fbe\u4e09\u5206\u4e4b\u4e00\u7684\u8fc7\u5ea6\u7ec3\u4e60\u65f6\u95f4\u3002\u8be5\u65b9\u6cd5\u7075\u6d3b\u9002\u7528\u4e8e\u4efb\u4f55\u9898\u76ee\u9009\u62e9\u7b97\u6cd5\uff0c\u4f46\u5728\u504f\u597d\u9009\u62e9\u96be\u9898\u7684\u7b97\u6cd5\u4e2d\u6548\u679c\u6700\u4f73\u3002\u56e0\u6b64\uff0c\u5c3d\u7ba1\u201c\u5feb\u8fdb\u201d\u6280\u672f\u80fd\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff0c\u5176\u5b9e\u9645\u6548\u679c\u8fd8\u53d6\u51b3\u4e8e\u5b66\u751f\u5728\u9ad8\u96be\u5ea6\u5b66\u4e60\u4e2d\u7684\u6301\u7eed\u52a8\u673a\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2506.18529", "pdf": "https://arxiv.org/pdf/2506.18529", "abs": "https://arxiv.org/abs/2506.18529", "authors": ["Pengxiang Li", "Wei Wu", "Zhi Gao", "Xiaomeng Fan", "Peilin Yu", "Yuwei Wu", "Zhipeng Lu", "Yunde Jia", "Mehrtash Harandi"], "title": "A Set-to-Set Distance Measure in Hyperbolic Space", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages", "summary": "We propose a hyperbolic set-to-set distance measure for computing\ndissimilarity between sets in hyperbolic space. While point-to-point distances\nin hyperbolic space effectively capture hierarchical relationships between data\npoints, many real-world applications require comparing sets of hyperbolic data\npoints, where the local structure and the global structure of the sets carry\ncrucial semantic information. The proposed the \\underline{h}yperbolic\n\\underline{s}et-\\underline{to}-\\underline{s}et \\underline{d}istance measure\n(HS2SD) integrates both global and local structural information: global\nstructure through geodesic distances between Einstein midpoints of hyperbolic\nsets, and local structure through topological characteristics of the two sets.\nTo efficiently compute topological differences, we prove that using a finite\nThue-Morse sequence of degree and adjacency matrices can serve as a robust\napproximation to capture the topological structure of a set. In this case, by\nconsidering the topological differences, HS2SD provides a more nuanced\nunderstanding of the relationships between two hyperbolic sets. Empirical\nevaluation on entity matching, standard image classification, and few-shot\nimage classification demonstrates that our distance measure outperforms\nexisting methods by effectively modeling the hierarchical and complex\nrelationships inherent in hyperbolic sets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u96c6\u5408\u95f4\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff08HS2SD\uff09\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\uff0c\u7528\u4e8e\u8ba1\u7b97\u53cc\u66f2\u7a7a\u95f4\u4e2d\u96c6\u5408\u7684\u5dee\u5f02\u6027\uff0c\u5e76\u5728\u5b9e\u4f53\u5339\u914d\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u6bd4\u8f83\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u96c6\u5408\u6570\u636e\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u70b9\u5bf9\u70b9\u8ddd\u79bb\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u96c6\u5408\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6574\u5408\u8fd9\u4e24\u79cd\u4fe1\u606f\u7684\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7684HS2SD\u65b9\u6cd5\u901a\u8fc7\u53cc\u66f2\u96c6\u5408\u7684\u7231\u56e0\u65af\u5766\u4e2d\u70b9\u4e4b\u95f4\u7684\u6d4b\u5730\u8ddd\u79bb\u6355\u6349\u5168\u5c40\u7ed3\u6784\uff0c\u540c\u65f6\u5229\u7528\u6709\u9650Thue-Morse\u5e8f\u5217\u7684\u5ea6\u548c\u90bb\u63a5\u77e9\u9635\u8fd1\u4f3c\u62d3\u6251\u7ed3\u6784\u4ee5\u6355\u6349\u5c40\u90e8\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHS2D\u5728\u5b9e\u4f53\u5339\u914d\u3001\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u548c\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u53cc\u66f2\u96c6\u5408\u7684\u5c42\u6b21\u548c\u590d\u6742\u5173\u7cfb\u3002", "conclusion": "HS2SD\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\uff0c\u4e3a\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u96c6\u5408\u6bd4\u8f83\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u4efb\u52a1\u3002", "paper_title_zh": "\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u96c6\u5408\u95f4\u8ddd\u79bb\u5ea6\u91cf", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u96c6\u5408\u95f4\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff08HS2SD\uff09\uff0c\u7528\u4e8e\u8ba1\u7b97\u53cc\u66f2\u7a7a\u95f4\u4e2d\u96c6\u5408\u7684\u5dee\u5f02\u6027\u3002\u867d\u7136\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u70b9\u5bf9\u70b9\u8ddd\u79bb\u80fd\u6709\u6548\u6355\u6349\u6570\u636e\u70b9\u4e4b\u95f4\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u4f46\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u9700\u8981\u6bd4\u8f83\u53cc\u66f2\u6570\u636e\u70b9\u7684\u96c6\u5408\uff0c\u5176\u4e2d\u96c6\u5408\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\u627f\u8f7d\u4e86\u5173\u952e\u7684\u8bed\u4e49\u4fe1\u606f\u3002HS2SD\u901a\u8fc7\u6574\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\uff1a\u5168\u5c40\u7ed3\u6784\u901a\u8fc7\u53cc\u66f2\u96c6\u5408\u7684\u7231\u56e0\u65af\u5766\u4e2d\u70b9\u4e4b\u95f4\u7684\u6d4b\u5730\u8ddd\u79bb\uff0c\u5c40\u90e8\u7ed3\u6784\u901a\u8fc7\u4e24\u4e2a\u96c6\u5408\u7684\u62d3\u6251\u7279\u5f81\u3002\u4e3a\u4e86\u9ad8\u6548\u8ba1\u7b97\u62d3\u6251\u5dee\u5f02\uff0c\u6211\u4eec\u8bc1\u660e\u4f7f\u7528\u6709\u9650Thue-Morse\u5e8f\u5217\u7684\u5ea6\u548c\u90bb\u63a5\u77e9\u9635\u53ef\u4ee5\u4f5c\u4e3a\u6355\u6349\u96c6\u5408\u62d3\u6251\u7ed3\u6784\u7684\u7a33\u5065\u8fd1\u4f3c\u3002\u901a\u8fc7\u8003\u8651\u62d3\u6251\u5dee\u5f02\uff0cHS2SD\u63d0\u4f9b\u4e86\u5bf9\u4e24\u4e2a\u53cc\u66f2\u96c6\u5408\u5173\u7cfb\u7684\u66f4\u7ec6\u81f4\u7406\u89e3\u3002\u5728\u5b9e\u4f53\u5339\u914d\u3001\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u548c\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u53cc\u66f2\u96c6\u5408\u4e2d\u56fa\u6709\u7684\u5c42\u6b21\u548c\u590d\u6742\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.17580", "pdf": "https://arxiv.org/pdf/2506.17580", "abs": "https://arxiv.org/abs/2506.17580", "authors": ["Sajratul Y. Rubaiat", "Hasan M. Jamil"], "title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.DL", "cs.ET"], "comment": null, "summary": "The exponential growth of scientific literature challenges researchers\nextracting and synthesizing knowledge. Traditional search engines return many\nsources without direct, detailed answers, while general-purpose LLMs may offer\nconcise responses that lack depth or omit current information. LLMs with search\ncapabilities are also limited by context window, yielding short, incomplete\nanswers. This paper introduces WISE (Workflow for Intelligent Scientific\nKnowledge Extraction), a system addressing these limits by using a structured\nworkflow to extract, refine, and rank query-specific knowledge. WISE uses an\nLLM-powered, tree-based architecture to refine data, focusing on query-aligned,\ncontext-aware, and non-redundant information. Dynamic scoring and ranking\nprioritize unique contributions from each source, and adaptive stopping\ncriteria minimize processing overhead. WISE delivers detailed, organized\nanswers by systematically exploring and synthesizing knowledge from diverse\nsources. Experiments on HBB gene-associated diseases demonstrate WISE reduces\nprocessed text by over 80% while achieving significantly higher recall over\nbaselines like search engines and other LLM-based approaches. ROUGE and BLEU\nmetrics reveal WISE's output is more unique than other systems, and a novel\nlevel-based metric shows it provides more in-depth information. We also explore\nhow the WISE workflow can be adapted for diverse domains like drug discovery,\nmaterial science, and social science, enabling efficient knowledge extraction\nand synthesis from unstructured scientific papers and web sources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWISE\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u667a\u80fd\u63d0\u53d6\u77e5\u8bc6\u3002\u901a\u8fc7\u6811\u72b6\u67b6\u6784\u548c\u52a8\u6001\u8bc4\u5206\uff0cWISE\u663e\u8457\u51cf\u5c11\u4e86\u5904\u7406\u6587\u672c\u91cf\u5e76\u63d0\u9ad8\u4e86\u4fe1\u606f\u53ec\u56de\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u548c\u5176\u4ed6\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u4f7f\u7814\u7a76\u8005\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u548c\u6574\u5408\u77e5\u8bc6\u3002\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u8fd4\u56de\u5927\u91cf\u65e0\u5173\u4fe1\u606f\uff0c\u800c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u56de\u7b54\u7f3a\u4e4f\u6df1\u5ea6\u6216\u65f6\u6548\u6027\u3002WISE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7cbe\u51c6\u3001\u6df1\u5165\u7684\u77e5\u8bc6\u63d0\u53d6\u3002", "method": "WISE\u91c7\u7528\u6811\u72b6\u67b6\u6784\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u63d0\u53d6\u3001\u7cbe\u70bc\u548c\u6392\u5e8f\u67e5\u8be2\u76f8\u5173\u77e5\u8bc6\u3002\u52a8\u6001\u8bc4\u5206\u548c\u81ea\u9002\u5e94\u505c\u6b62\u6807\u51c6\u4f18\u5316\u4e86\u5904\u7406\u6548\u7387\uff0c\u786e\u4fdd\u63d0\u53d6\u7684\u4fe1\u606f\u4e0e\u67e5\u8be2\u76f8\u5173\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u65e0\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWISE\u5728HBB\u57fa\u56e0\u76f8\u5173\u75be\u75c5\u7684\u7814\u7a76\u4e2d\uff0c\u5904\u7406\u6587\u672c\u91cf\u51cf\u5c1180%\u4ee5\u4e0a\uff0c\u53ec\u56de\u7387\u663e\u8457\u9ad8\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002ROUGE\u548cBLEU\u6307\u6807\u663e\u793a\u5176\u8f93\u51fa\u66f4\u5177\u72ec\u7279\u6027\uff0c\u4e14\u80fd\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u4fe1\u606f\u3002", "conclusion": "WISE\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u77e5\u8bc6\u63d0\u53d6\u548c\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u6587\u732e\u5206\u6790\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002\u5176\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u836f\u7269\u53d1\u73b0\u3001\u6750\u6599\u79d1\u5b66\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\uff0c\u4e3a\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5173\u8054\u5f00\u653e\u6570\u636e\u4e0a\u4e0b\u6587\u611f\u77e5\u79d1\u5b66\u77e5\u8bc6\u63d0\u53d6", "abstract_zh": "\u79d1\u5b66\u6587\u732e\u7684\u6307\u6570\u7ea7\u589e\u957f\u7ed9\u7814\u7a76\u8005\u63d0\u53d6\u548c\u6574\u5408\u77e5\u8bc6\u5e26\u6765\u4e86\u6311\u6218\u3002\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u8fd4\u56de\u5927\u91cf\u6765\u6e90\u4f46\u7f3a\u4e4f\u76f4\u63a5\u3001\u8be6\u7ec6\u7684\u7b54\u6848\uff0c\u800c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u56de\u7b54\u53ef\u80fd\u7b80\u6d01\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u6216\u9057\u6f0f\u6700\u65b0\u4fe1\u606f\u3002\u5177\u5907\u641c\u7d22\u80fd\u529b\u7684LLM\u4e5f\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5bfc\u81f4\u56de\u7b54\u7b80\u77ed\u4e14\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u4ecb\u7ecd\u4e86WISE\uff08\u667a\u80fd\u79d1\u5b66\u77e5\u8bc6\u63d0\u53d6\u5de5\u4f5c\u6d41\uff09\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u63d0\u53d6\u3001\u7cbe\u70bc\u548c\u6392\u5e8f\u67e5\u8be2\u76f8\u5173\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u9650\u5236\u3002WISE\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6811\u72b6\u67b6\u6784\u7cbe\u70bc\u6570\u636e\uff0c\u4e13\u6ce8\u4e8e\u4e0e\u67e5\u8be2\u5bf9\u9f50\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u65e0\u5197\u4f59\u7684\u4fe1\u606f\u3002\u52a8\u6001\u8bc4\u5206\u548c\u6392\u5e8f\u4f18\u5148\u8003\u8651\u6bcf\u4e2a\u6765\u6e90\u7684\u72ec\u7279\u8d21\u732e\uff0c\u81ea\u9002\u5e94\u505c\u6b62\u6807\u51c6\u6700\u5c0f\u5316\u5904\u7406\u5f00\u9500\u3002WISE\u901a\u8fc7\u7cfb\u7edf\u5316\u63a2\u7d22\u548c\u6574\u5408\u591a\u6837\u5316\u6765\u6e90\u7684\u77e5\u8bc6\uff0c\u63d0\u4f9b\u8be6\u7ec6\u4e14\u7ec4\u7ec7\u5316\u7684\u7b54\u6848\u3002\u5728HBB\u57fa\u56e0\u76f8\u5173\u75be\u75c5\u7684\u5b9e\u9a8c\u4e2d\uff0cWISE\u51cf\u5c11\u4e8680%\u4ee5\u4e0a\u7684\u5904\u7406\u6587\u672c\u91cf\uff0c\u540c\u65f6\u53ec\u56de\u7387\u663e\u8457\u9ad8\u4e8e\u641c\u7d22\u5f15\u64ce\u548c\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u3002ROUGE\u548cBLEU\u6307\u6807\u663e\u793aWISE\u7684\u8f93\u51fa\u66f4\u5177\u72ec\u7279\u6027\uff0c\u800c\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u7ea7\u6307\u6807\u8868\u660e\u5176\u63d0\u4f9b\u7684\u4fe1\u606f\u66f4\u6df1\u5165\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86WISE\u5de5\u4f5c\u6d41\u5982\u4f55\u9002\u5e94\u836f\u7269\u53d1\u73b0\u3001\u6750\u6599\u79d1\u5b66\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u591a\u6837\u5316\u9886\u57df\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u4ece\u975e\u7ed3\u6784\u5316\u79d1\u5b66\u8bba\u6587\u548c\u7f51\u7edc\u6765\u6e90\u4e2d\u63d0\u53d6\u548c\u6574\u5408\u77e5\u8bc6\u3002"}}
{"id": "2506.18533", "pdf": "https://arxiv.org/pdf/2506.18533", "abs": "https://arxiv.org/abs/2506.18533", "authors": ["Pengxiang Li", "Yuwei Wu", "Zhi Gao", "Xiaomeng Fan", "Wei Wu", "Zhipeng Lu", "Yunde Jia", "Mehrtash Harandi"], "title": "Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Learning in hyperbolic spaces has attracted increasing attention due to its\nsuperior ability to model hierarchical structures of data. Most existing\nhyperbolic learning methods use fixed distance measures for all data, assuming\na uniform hierarchy across all data points. However, real-world hierarchical\nstructures exhibit significant diversity, making this assumption overly\nrestrictive. In this paper, we propose a geometry-aware distance measure in\nhyperbolic spaces, which dynamically adapts to varying hierarchical structures.\nOur approach derives the distance measure by generating tailored projections\nand curvatures for each pair of data points, effectively mapping them to an\nappropriate hyperbolic space. We introduce a revised low-rank decomposition\nscheme and a hard-pair mining mechanism to mitigate the computational cost of\npair-wise distance computation without compromising accuracy. We present an\nupper bound on the low-rank approximation error using Talagrand's concentration\ninequality, ensuring theoretical robustness. Extensive experiments on standard\nimage classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical\nclassification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet,\ntiered-ImageNet) demonstrate the effectiveness of our method. Our approach\nconsistently outperforms learning methods that use fixed distance measures,\nwith notable improvements on few-shot learning tasks, where it achieves over\n5\\% gains on mini-ImageNet. The results reveal that adaptive distance measures\nbetter capture diverse hierarchical structures, with visualization showing\nclearer class boundaries and improved prototype separation in hyperbolic\nspaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u53cc\u66f2\u7a7a\u95f4\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u591a\u6837\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u66f2\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6240\u6709\u6570\u636e\u70b9\u5177\u6709\u7edf\u4e00\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u800c\u73b0\u5b9e\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u5177\u6709\u591a\u6837\u6027\uff0c\u8fd9\u79cd\u5047\u8bbe\u8fc7\u4e8e\u5c40\u9650\u3002", "method": "\u901a\u8fc7\u4e3a\u6bcf\u5bf9\u6570\u636e\u70b9\u751f\u6210\u5b9a\u5236\u5316\u7684\u6295\u5f71\u548c\u66f2\u7387\uff0c\u5c06\u5176\u6620\u5c04\u5230\u5408\u9002\u7684\u53cc\u66f2\u7a7a\u95f4\uff0c\u540c\u65f6\u5f15\u5165\u4f4e\u79e9\u5206\u89e3\u65b9\u6848\u548c\u56f0\u96be\u5bf9\u6316\u6398\u673a\u5236\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u3001\u5c42\u6b21\u5206\u7c7b\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u8ddd\u79bb\u5ea6\u91cf\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\u3002", "conclusion": "\u81ea\u9002\u5e94\u8ddd\u79bb\u5ea6\u91cf\u80fd\u66f4\u597d\u5730\u6355\u6349\u591a\u6837\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u663e\u793a\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u7c7b\u522b\u8fb9\u754c\u66f4\u6e05\u6670\uff0c\u539f\u578b\u5206\u79bb\u66f4\u660e\u663e\u3002", "paper_title_zh": "\u53cc\u66f2\u7a7a\u95f4\u4e2d\u591a\u6837\u5316\u5c42\u6b21\u7ed3\u6784\u7684\u51e0\u4f55\u611f\u77e5\u8ddd\u79bb\u5ea6\u91cf", "abstract_zh": "\u7531\u4e8e\u53cc\u66f2\u7a7a\u95f4\u5728\u5efa\u6a21\u6570\u636e\u5c42\u6b21\u7ed3\u6784\u65b9\u9762\u7684\u4f18\u8d8a\u80fd\u529b\uff0c\u53cc\u66f2\u7a7a\u95f4\u5b66\u4e60\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u7684\u5927\u591a\u6570\u53cc\u66f2\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6240\u6709\u6570\u636e\u4f7f\u7528\u56fa\u5b9a\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5047\u8bbe\u6240\u6709\u6570\u636e\u70b9\u5177\u6709\u7edf\u4e00\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u8868\u73b0\u51fa\u663e\u8457\u7684\u591a\u6837\u6027\uff0c\u8fd9\u79cd\u5047\u8bbe\u8fc7\u4e8e\u5c40\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u611f\u77e5\u8ddd\u79bb\u5ea6\u91cf\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4e3a\u6bcf\u5bf9\u6570\u636e\u70b9\u751f\u6210\u5b9a\u5236\u5316\u7684\u6295\u5f71\u548c\u66f2\u7387\uff0c\u5c06\u5176\u6620\u5c04\u5230\u5408\u9002\u7684\u53cc\u66f2\u7a7a\u95f4\u3002\u4e3a\u4e86\u5728\u4e0d\u5f71\u54cd\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e\u6210\u5bf9\u8ddd\u79bb\u8ba1\u7b97\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6539\u8fdb\u7684\u4f4e\u79e9\u5206\u89e3\u65b9\u6848\u548c\u56f0\u96be\u5bf9\u6316\u6398\u673a\u5236\u3002\u5229\u7528Talagrand\u7684\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u79e9\u8fd1\u4f3c\u8bef\u5dee\u7684\u4e0a\u754c\uff0c\u786e\u4fdd\u4e86\u7406\u8bba\u4e0a\u7684\u9c81\u68d2\u6027\u3002\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\uff08MNIST\u3001CIFAR-10\u548cCIFAR-100\uff09\u3001\u5c42\u6b21\u5206\u7c7b\uff085\u7ea7CIFAR-100\uff09\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\uff08mini-ImageNet\u3001tiered-ImageNet\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56fa\u5b9a\u8ddd\u79bb\u5ea6\u91cf\u7684\u5b66\u4e60\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cmini-ImageNet\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\u3002\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u8ddd\u79bb\u5ea6\u91cf\u80fd\u66f4\u597d\u5730\u6355\u6349\u591a\u6837\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u663e\u793a\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u7c7b\u522b\u8fb9\u754c\u66f4\u6e05\u6670\uff0c\u539f\u578b\u5206\u79bb\u66f4\u660e\u663e\u3002"}}
{"id": "2506.18544", "pdf": "https://arxiv.org/pdf/2506.18544", "abs": "https://arxiv.org/abs/2506.18544", "authors": ["Muhao Xu", "Xueying Zhou", "Xizhan Gao", "Weiye Song", "Guang Feng", "Sijie Niu"], "title": "Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, detecting logical anomalies is becoming a more challenging task\ncompared to detecting structural ones. Existing encoder decoder based methods\ntypically compress inputs into low-dimensional bottlenecks on the assumption\nthat the compression process can effectively suppress the transmission of\nlogical anomalies to the decoder. However, logical anomalies present a\nparticular difficulty because, while their local features often resemble normal\nsemantics, their global semantics deviate significantly from normal patterns.\nThanks to the generalisation capabilities inherent in neural networks, these\nabnormal semantic features can propagate through low-dimensional bottlenecks.\nThis ultimately allows the decoder to reconstruct anomalous images with\nmisleading fidelity. To tackle the above challenge, we propose a novel\nnormality prior guided multi-semantic fusion network for unsupervised anomaly\ndetection. Instead of feeding the compressed bottlenecks to the decoder\ndirectly, we introduce the multi-semantic features of normal samples into the\nreconstruction process. To this end, we first extract abstract global semantics\nof normal cases by a pre-trained vision-language network, then the learnable\nsemantic codebooks are constructed to store representative feature vectors of\nnormal samples by vector quantisation. Finally, the above multi-semantic\nfeatures are fused and employed as input to the decoder to guide the\nreconstruction of anomalies to approximate normality. Extensive experiments are\nconducted to validate the effectiveness of our proposed method, and it achieves\nthe SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in\npixel-sPRO and 2.6% in image-AUROC. The source code is available at\nhttps://github.com/Xmh-L/NPGMF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u6001\u6027\u5148\u9a8c\u7684\u591a\u8bed\u4e49\u878d\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u6b63\u5e38\u6837\u672c\u7684\u591a\u8bed\u4e49\u7279\u5f81\u6307\u5bfc\u5f02\u5e38\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u903b\u8f91\u5f02\u5e38\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u903b\u8f91\u5f02\u5e38\u7684\u5c40\u90e8\u7279\u5f81\u4e0e\u6b63\u5e38\u8bed\u4e49\u76f8\u4f3c\uff0c\u4f46\u5168\u5c40\u8bed\u4e49\u5dee\u5f02\u663e\u8457\u3002\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u5bfc\u81f4\u5f02\u5e38\u7279\u5f81\u901a\u8fc7\u4f4e\u7ef4\u74f6\u9888\u4f20\u64ad\uff0c\u89e3\u7801\u5668\u53ef\u80fd\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u91cd\u5efa\u5f02\u5e38\u56fe\u50cf\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u7f51\u7edc\u63d0\u53d6\u6b63\u5e38\u6837\u672c\u7684\u5168\u5c40\u8bed\u4e49\uff1b2) \u6784\u5efa\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u7801\u672c\u5b58\u50a8\u6b63\u5e38\u6837\u672c\u7279\u5f81\uff1b3) \u878d\u5408\u591a\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u89e3\u7801\u5668\u8f93\u5165\uff0c\u6307\u5bfc\u5f02\u5e38\u91cd\u5efa\u903c\u8fd1\u6b63\u5e38\u6a21\u5f0f\u3002", "result": "\u5728MVTec LOCO AD\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7sPRO\u548c\u56fe\u50cf\u7ea7AUROC\u6307\u6807\u4e0a\u5206\u522b\u63d0\u5347\u4e865.7%\u548c2.6%\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6b63\u6001\u6027\u5148\u9a8c\u548c\u591a\u8bed\u4e49\u878d\u5408\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u903b\u8f91\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "paper_title_zh": "\u6b63\u6001\u6027\u5148\u9a8c\u5f15\u5bfc\u7684\u591a\u8bed\u4e49\u878d\u5408\u7f51\u7edc\u7528\u4e8e\u65e0\u76d1\u7763\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u68c0\u6d4b\u903b\u8f91\u5f02\u5e38\u6bd4\u68c0\u6d4b\u7ed3\u6784\u5f02\u5e38\u66f4\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u8f93\u5165\u538b\u7f29\u5230\u4f4e\u7ef4\u74f6\u9888\u4e2d\uff0c\u5047\u8bbe\u538b\u7f29\u8fc7\u7a0b\u53ef\u4ee5\u6709\u6548\u6291\u5236\u903b\u8f91\u5f02\u5e38\u5411\u89e3\u7801\u5668\u7684\u4f20\u9012\u3002\u7136\u800c\uff0c\u903b\u8f91\u5f02\u5e38\u7684\u5c40\u90e8\u7279\u5f81\u901a\u5e38\u4e0e\u6b63\u5e38\u8bed\u4e49\u76f8\u4f3c\uff0c\u4f46\u5176\u5168\u5c40\u8bed\u4e49\u4e0e\u6b63\u5e38\u6a21\u5f0f\u663e\u8457\u504f\u79bb\u3002\u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4e9b\u5f02\u5e38\u8bed\u4e49\u7279\u5f81\u53ef\u80fd\u901a\u8fc7\u4f4e\u7ef4\u74f6\u9888\u4f20\u64ad\uff0c\u6700\u7ec8\u5bfc\u81f4\u89e3\u7801\u5668\u4ee5\u8bef\u5bfc\u6027\u7684\u9ad8\u4fdd\u771f\u5ea6\u91cd\u5efa\u5f02\u5e38\u56fe\u50cf\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u6001\u6027\u5148\u9a8c\u5f15\u5bfc\u7684\u591a\u8bed\u4e49\u878d\u5408\u7f51\u7edc\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u4e0d\u76f4\u63a5\u5411\u89e3\u7801\u5668\u63d0\u4f9b\u538b\u7f29\u74f6\u9888\uff0c\u800c\u662f\u5c06\u6b63\u5e38\u6837\u672c\u7684\u591a\u8bed\u4e49\u7279\u5f81\u5f15\u5165\u91cd\u5efa\u8fc7\u7a0b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u7f51\u7edc\u63d0\u53d6\u6b63\u5e38\u6837\u672c\u7684\u62bd\u8c61\u5168\u5c40\u8bed\u4e49\uff0c\u7136\u540e\u6784\u5efa\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u7801\u672c\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5b58\u50a8\u6b63\u5e38\u6837\u672c\u7684\u4ee3\u8868\u6027\u7279\u5f81\u5411\u91cf\u3002\u6700\u540e\uff0c\u5c06\u4e0a\u8ff0\u591a\u8bed\u4e49\u7279\u5f81\u878d\u5408\u5e76\u4f5c\u4e3a\u89e3\u7801\u5668\u8f93\u5165\uff0c\u6307\u5bfc\u5f02\u5e38\u91cd\u5efa\u903c\u8fd1\u6b63\u5e38\u6a21\u5f0f\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728MVTec LOCO AD\u6570\u636e\u96c6\u4e0a\uff0c\u50cf\u7d20\u7ea7sPRO\u548c\u56fe\u50cf\u7ea7AUROC\u5206\u522b\u63d0\u5347\u4e865.7%\u548c2.6%\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002\u6e90\u4ee3\u7801\u89c1https://github.com/Xmh-L/NPGMF\u3002"}}
{"id": "2506.18557", "pdf": "https://arxiv.org/pdf/2506.18557", "abs": "https://arxiv.org/abs/2506.18557", "authors": ["Sung Jin Um", "Dongjin Kim", "Sangmin Lee", "Jung Uk Kim"], "title": "Object-aware Sound Source Localization via Audio-Visual Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Audio-visual sound source localization task aims to spatially localize\nsound-making objects within visual scenes by integrating visual and audio cues.\nHowever, existing methods struggle with accurately localizing sound-making\nobjects in complex scenes, particularly when visually similar silent objects\ncoexist. This limitation arises primarily from their reliance on simple\naudio-visual correspondence, which does not capture fine-grained semantic\ndifferences between sound-making and silent objects. To address these\nchallenges, we propose a novel sound source localization framework leveraging\nMultimodal Large Language Models (MLLMs) to generate detailed contextual\ninformation that explicitly distinguishes between sound-making foreground\nobjects and silent background objects. To effectively integrate this detailed\ninformation, we introduce two novel loss functions: Object-aware Contrastive\nAlignment (OCA) loss and Object Region Isolation (ORI) loss. Extensive\nexperimental results on MUSIC and VGGSound datasets demonstrate the\neffectiveness of our approach, significantly outperforming existing methods in\nboth single-source and multi-source localization scenarios. Code and generated\ndetailed contextual information are available at:\nhttps://github.com/VisualAIKHU/OA-SSL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u58f0\u97f3\u6e90\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\u533a\u5206\u53d1\u58f0\u7269\u4f53\u4e0e\u9759\u9ed8\u7269\u4f53\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\uff08OCA\u548cORI\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u58f0\u97f3\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5f53\u89c6\u89c9\u76f8\u4f3c\u7684\u9759\u9ed8\u7269\u4f53\u5b58\u5728\u65f6\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u5b83\u4eec\u4f9d\u8d56\u7b80\u5355\u7684\u89c6\u542c\u5bf9\u5e94\u5173\u7cfb\uff0c\u65e0\u6cd5\u533a\u5206\u53d1\u58f0\u7269\u4f53\u4e0e\u9759\u9ed8\u7269\u4f53\u7684\u8bed\u4e49\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528MLLMs\u751f\u6210\u8be6\u7ec6\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6846\u67b6\uff0c\u660e\u786e\u533a\u5206\u53d1\u58f0\u524d\u666f\u7269\u4f53\u4e0e\u9759\u9ed8\u80cc\u666f\u7269\u4f53\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\uff1a\u5bf9\u8c61\u611f\u77e5\u5bf9\u6bd4\u5bf9\u9f50\uff08OCA\uff09\u635f\u5931\u548c\u5bf9\u8c61\u533a\u57df\u9694\u79bb\uff08ORI\uff09\u635f\u5931\u3002", "result": "\u5728MUSIC\u548cVGG-Sound\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6e90\u548c\u591a\u6e90\u5b9a\u4f4d\u573a\u666f\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408MLLMs\u548c\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u672c\u6587\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u573a\u666f\u4e2d\u58f0\u97f3\u6e90\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u4e3a\u89c6\u542c\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u89c6\u542c\u573a\u666f\u7406\u89e3\u7684\u5bf9\u8c61\u611f\u77e5\u58f0\u97f3\u6e90\u5b9a\u4f4d", "abstract_zh": "\u89c6\u542c\u58f0\u97f3\u6e90\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u97f3\u9891\u7ebf\u7d22\uff0c\u5728\u89c6\u89c9\u573a\u666f\u4e2d\u7a7a\u95f4\u5b9a\u4f4d\u53d1\u58f0\u7269\u4f53\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u53d1\u58f0\u7269\u4f53\uff0c\u5c24\u5176\u662f\u5f53\u89c6\u89c9\u76f8\u4f3c\u7684\u9759\u9ed8\u7269\u4f53\u5171\u5b58\u65f6\u3002\u8fd9\u4e00\u5c40\u9650\u6027\u4e3b\u8981\u6e90\u4e8e\u5b83\u4eec\u4f9d\u8d56\u7b80\u5355\u7684\u89c6\u542c\u5bf9\u5e94\u5173\u7cfb\uff0c\u65e0\u6cd5\u6355\u6349\u53d1\u58f0\u7269\u4f53\u4e0e\u9759\u9ed8\u7269\u4f53\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u58f0\u97f3\u6e90\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u751f\u6210\u8be6\u7ec6\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u660e\u786e\u533a\u5206\u53d1\u58f0\u524d\u666f\u7269\u4f53\u4e0e\u9759\u9ed8\u80cc\u666f\u7269\u4f53\u3002\u4e3a\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\uff1a\u5bf9\u8c61\u611f\u77e5\u5bf9\u6bd4\u5bf9\u9f50\uff08OCA\uff09\u635f\u5931\u548c\u5bf9\u8c61\u533a\u57df\u9694\u79bb\uff08ORI\uff09\u635f\u5931\u3002\u5728MUSIC\u548cVGG-Sound\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5355\u6e90\u548c\u591a\u6e90\u5b9a\u4f4d\u573a\u666f\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u751f\u6210\u7684\u8be6\u7ec6\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://github.com/VisualAIKHU/OA-SSL\u3002"}}
{"id": "2506.17601", "pdf": "https://arxiv.org/pdf/2506.17601", "abs": "https://arxiv.org/abs/2506.17601", "authors": ["Rohan Thakker", "Adarsh Patnaik", "Vince Kurtz", "Jonas Frey", "Jonathan Becktor", "Sangwoo Moon", "Rob Royce", "Marcel Kaufmann", "Georgios Georgakis", "Pascal Roth", "Joel Burdick", "Marco Hutter", "Shehryar Khattak"], "title": "Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Safe, reliable navigation in extreme, unfamiliar terrain is required for\nfuture robotic space exploration missions. Recent generative-AI methods learn\nsemantically aware navigation policies from large, cross-embodiment datasets,\nbut offer limited safety guarantees. Inspired by human cognitive science, we\npropose a risk-guided diffusion framework that fuses a fast, learned \"System-1\"\nwith a slow, physics-based \"System-2\", sharing computation at both training and\ninference to couple adaptability with formal safety. Hardware experiments\nconducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our\napproach reduces failure rates by up to $4\\times$ while matching the\ngoal-reaching performance of learning-based robotic models by leveraging\ninference-time compute without any additional training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u5f15\u5bfc\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u5feb\u901f\u5b66\u4e60\u7684\u2018\u7cfb\u7edf1\u2019\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u6162\u901f\u2018\u7cfb\u7edf2\u2019\uff0c\u5728\u6781\u7aef\u5730\u5f62\u4e2d\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6545\u969c\u7387\u964d\u4f4e\u81f34\u500d\u3002", "motivation": "\u672a\u6765\u7684\u592a\u7a7a\u63a2\u7d22\u4efb\u52a1\u9700\u8981\u673a\u5668\u4eba\u5728\u6781\u7aef\u964c\u751f\u5730\u5f62\u4e2d\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u5bfc\u822a\u3002\u5c3d\u7ba1\u73b0\u6709\u751f\u6210\u5f0fAI\u65b9\u6cd5\u80fd\u4ece\u8de8\u672c\u4f53\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u8bed\u4e49\u611f\u77e5\u5bfc\u822a\u7b56\u7565\uff0c\u4f46\u5176\u5b89\u5168\u6027\u4fdd\u969c\u6709\u9650\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u517c\u5177\u9002\u5e94\u6027\u548c\u5f62\u5f0f\u5316\u5b89\u5168\u6027\u7684\u5bfc\u822a\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u98ce\u9669\u5f15\u5bfc\u6269\u6563\u6846\u67b6\uff0c\u878d\u5408\u5feb\u901f\u5b66\u4e60\u7684\u2018\u7cfb\u7edf1\u2019\uff08\u57fa\u4e8e\u751f\u6210\u5f0fAI\uff09\u4e0e\u6162\u901f\u7269\u7406\u6a21\u62df\u7684\u2018\u7cfb\u7edf2\u2019\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5171\u4eab\u8ba1\u7b97\u8d44\u6e90\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u4e0e\u9002\u5e94\u6027\u3002", "result": "\u5728NASA JPL\u7684Mars Yard\u706b\u661f\u6a21\u62df\u8bbe\u65bd\u4e2d\u8fdb\u884c\u7684\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u6545\u969c\u7387\u964d\u4f4e\u81f34\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5b66\u4e60\u578b\u673a\u5668\u4eba\u6a21\u578b\u76f8\u540c\u7684\u76ee\u6807\u8fbe\u6210\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u98ce\u9669\u5f15\u5bfc\u6269\u6563\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0fAI\u4e0e\u7269\u7406\u6a21\u62df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u6781\u7aef\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u5b89\u5168\u6027\uff0c\u4e3a\u672a\u6765\u592a\u7a7a\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u98ce\u9669\u5f15\u5bfc\u6269\u6563\uff1a\u5728\u592a\u7a7a\u90e8\u7f72\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u5931\u8d25\u4e0d\u662f\u9009\u9879", "abstract_zh": "\u672a\u6765\u7684\u673a\u5668\u4eba\u592a\u7a7a\u63a2\u7d22\u4efb\u52a1\u9700\u8981\u5728\u6781\u7aef\u964c\u751f\u5730\u5f62\u4e2d\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u5bfc\u822a\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u751f\u6210\u5f0fAI\u65b9\u6cd5\u80fd\u591f\u4ece\u5927\u578b\u8de8\u672c\u4f53\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u8bed\u4e49\u611f\u77e5\u5bfc\u822a\u7b56\u7565\uff0c\u4f46\u5176\u5b89\u5168\u6027\u4fdd\u969c\u6709\u9650\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u5f15\u5bfc\u6269\u6563\u6846\u67b6\uff0c\u5c06\u5feb\u901f\u5b66\u4e60\u7684\u2018\u7cfb\u7edf1\u2019\u4e0e\u6162\u901f\u57fa\u4e8e\u7269\u7406\u7684\u2018\u7cfb\u7edf2\u2019\u878d\u5408\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5171\u4eab\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u9002\u5e94\u6027\u4e0e\u5f62\u5f0f\u5316\u5b89\u5168\u7684\u7ed3\u5408\u3002\u5728NASA JPL\u7684\u706b\u661f\u6a21\u62df\u8bbe\u65bdMars Yard\u4e2d\u8fdb\u884c\u7684\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6545\u969c\u7387\u964d\u4f4e\u81f34\u500d\uff0c\u540c\u65f6\u901a\u8fc7\u5229\u7528\u63a8\u7406\u8ba1\u7b97\u8d44\u6e90\uff08\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff09\u5339\u914d\u5b66\u4e60\u578b\u673a\u5668\u4eba\u6a21\u578b\u7684\u76ee\u6807\u8fbe\u6210\u6027\u80fd\u3002"}}
{"id": "2506.18564", "pdf": "https://arxiv.org/pdf/2506.18564", "abs": "https://arxiv.org/abs/2506.18564", "authors": ["Xuanyu Zhang", "Weiqi Li", "Shijie Zhao", "Junlin Li", "Li Zhang", "Jian Zhang"], "title": "VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Recent advances in AI-generated content (AIGC) have led to the emergence of\npowerful text-to-video generation models. Despite these successes, evaluating\nthe quality of AIGC-generated videos remains challenging due to limited\ngeneralization, lack of temporal awareness, heavy reliance on large-scale\nannotated datasets, and the lack of effective interaction with generation\nmodels. Most current approaches rely on supervised finetuning of\nvision-language models (VLMs), which often require large-scale annotated\ndatasets and tend to decouple understanding and generation. To address these\nshortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for\nAIGC video quality assessment. Our approach features: (1) a progressive video\nquality learning scheme that combines image quality warm-up, general\ntask-specific temporal learning, and joint optimization with the video\ngeneration model; (2) the design of multi-dimension scoring rewards, preference\ncomparison rewards, and temporal modeling rewards to enhance both\ngeneralization and specialization in video quality evaluation. Extensive\nexperiments demonstrate that VQ-Insight consistently outperforms\nstate-of-the-art baselines in preference comparison, multi-dimension scoring,\nand natural video scoring, bringing significant improvements for video\ngeneration tasks.", "AI": {"tldr": "VQ-Insight\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e10\u8fdb\u5f0f\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u5206\u5956\u52b1\u548c\u65f6\u5e8f\u5efa\u6a21\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u8bc4\u4f30\u9762\u4e34\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u7f3a\u4e4f\u65f6\u5e8f\u611f\u77e5\u3001\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7b49\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u7ed3\u5408\u7406\u89e3\u548c\u751f\u6210\u3002VQ-Insight\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "VQ-Insight\u91c7\u7528\u6e10\u8fdb\u5f0f\u5b66\u4e60\u65b9\u6848\uff0c\u5305\u62ec\u56fe\u50cf\u8d28\u91cf\u9884\u70ed\u3001\u4efb\u52a1\u7279\u5b9a\u65f6\u5e8f\u5b66\u4e60\u548c\u4e0e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8054\u5408\u4f18\u5316\u3002\u8bbe\u8ba1\u4e86\u591a\u7ef4\u5ea6\u8bc4\u5206\u5956\u52b1\u3001\u504f\u597d\u6bd4\u8f83\u5956\u52b1\u548c\u65f6\u5e8f\u5efa\u6a21\u5956\u52b1\uff0c\u4ee5\u63d0\u5347\u8bc4\u4f30\u7684\u6cdb\u5316\u548c\u4e13\u4e1a\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVQ-Insight\u5728\u504f\u597d\u6bd4\u8f83\u3001\u591a\u7ef4\u5ea6\u8bc4\u5206\u548c\u81ea\u7136\u89c6\u9891\u8bc4\u5206\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u6548\u679c\u3002", "conclusion": "VQ-Insight\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u7ef4\u5ea6\u5956\u52b1\u8bbe\u8ba1\uff0c\u4e3aAI\u751f\u6210\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "paper_title_zh": "VQ-Insight\uff1a\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u6559\u6388\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7406\u89e3AI\u751f\u6210\u89c6\u9891\u8d28\u91cf", "abstract_zh": "\u8fd1\u5e74\u6765\uff0cAI\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u7684\u8fdb\u5c55\u50ac\u751f\u4e86\u5f3a\u5927\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u7f3a\u4e4f\u65f6\u5e8f\u611f\u77e5\u3001\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4ee5\u53ca\u4e0e\u751f\u6210\u6a21\u578b\u4ea4\u4e92\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u8bc4\u4f30AIGC\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5f53\u524d\u65b9\u6cd5\u591a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4f46\u9700\u8981\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e14\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u7ed3\u5408\u7406\u89e3\u548c\u751f\u6210\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VQ-Insight\uff0c\u4e00\u79cd\u65b0\u578b\u63a8\u7406\u5f0fVLM\u6846\u67b6\uff0c\u7528\u4e8eAIGC\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\uff1a\uff081\uff09\u6e10\u8fdb\u5f0f\u89c6\u9891\u8d28\u91cf\u5b66\u4e60\u65b9\u6848\uff0c\u7ed3\u5408\u56fe\u50cf\u8d28\u91cf\u9884\u70ed\u3001\u4efb\u52a1\u7279\u5b9a\u65f6\u5e8f\u5b66\u4e60\u548c\u4e0e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8054\u5408\u4f18\u5316\uff1b\uff082\uff09\u8bbe\u8ba1\u591a\u7ef4\u5ea6\u8bc4\u5206\u5956\u52b1\u3001\u504f\u597d\u6bd4\u8f83\u5956\u52b1\u548c\u65f6\u5e8f\u5efa\u6a21\u5956\u52b1\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u7684\u6cdb\u5316\u548c\u4e13\u4e1a\u5316\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVQ-Insight\u5728\u504f\u597d\u6bd4\u8f83\u3001\u591a\u7ef4\u5ea6\u8bc4\u5206\u548c\u81ea\u7136\u89c6\u9891\u8bc4\u5206\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u4efb\u52a1\u5e26\u6765\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2506.17621", "pdf": "https://arxiv.org/pdf/2506.17621", "abs": "https://arxiv.org/abs/2506.17621", "authors": ["Ravishka Rathnasuriya", "Wei Yang"], "title": "Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Proceedings of the 2025 Poster Session of the 10th IEEE European\n  Symposium on Security and Privacy (EuroS&P 2025)", "summary": "The growing deployment of deep learning models in real-world environments has\nintensified the need for efficient inference under strict latency and resource\nconstraints. To meet these demands, dynamic deep learning systems (DDLSs) have\nemerged, offering input-adaptive computation to optimize runtime efficiency.\nWhile these systems succeed in reducing cost, their dynamic nature introduces\nsubtle and underexplored security risks. In particular, input-dependent\nexecution pathways create opportunities for adversaries to degrade efficiency,\nresulting in excessive latency, energy usage, and potential denial-of-service\nin time-sensitive deployments. This work investigates the security implications\nof dynamic behaviors in DDLSs and reveals how current systems expose efficiency\nvulnerabilities exploitable by adversarial inputs. Through a survey of existing\nattack strategies, we identify gaps in the coverage of emerging model\narchitectures and limitations in current defense mechanisms. Building on these\ninsights, we propose to examine the feasibility of efficiency attacks on modern\nDDLSs and develop targeted defenses to preserve robustness under adversarial\nconditions.", "AI": {"tldr": "\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff08DDLSs\uff09\u901a\u8fc7\u8f93\u5165\u81ea\u9002\u5e94\u8ba1\u7b97\u4f18\u5316\u6548\u7387\uff0c\u4f46\u5176\u52a8\u6001\u7279\u6027\u5f15\u5165\u5b89\u5168\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u5229\u7528\u8f93\u5165\u4f9d\u8d56\u6027\u6267\u884c\u8def\u5f84\u964d\u4f4e\u6548\u7387\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u3001\u80fd\u8017\u4e0a\u5347\u751a\u81f3\u62d2\u7edd\u670d\u52a1\u3002\u672c\u6587\u7814\u7a76DDLSs\u7684\u6548\u7387\u6f0f\u6d1e\uff0c\u5206\u6790\u653b\u51fb\u7b56\u7565\u5e76\u63d0\u51fa\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u9ad8\u6548\u63a8\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff08DDLSs\uff09\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5176\u52a8\u6001\u7279\u6027\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\uff0c\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\u751a\u81f3\u670d\u52a1\u4e2d\u65ad\u3002\u672c\u6587\u65e8\u5728\u63ed\u793aDDLSs\u7684\u6548\u7387\u6f0f\u6d1e\u53ca\u5176\u5b89\u5168\u98ce\u9669\u3002", "method": "\u672c\u6587\u9996\u5148\u8c03\u67e5\u73b0\u6709\u653b\u51fb\u7b56\u7565\uff0c\u5206\u6790\u5176\u5bf9\u65b0\u5174\u6a21\u578b\u67b6\u6784\u7684\u8986\u76d6\u4e0d\u8db3\u53ca\u9632\u5fa1\u673a\u5236\u7684\u5c40\u9650\u6027\u3002\u968f\u540e\uff0c\u7814\u7a76\u73b0\u4ee3DDLSs\u4e2d\u6548\u7387\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f00\u53d1\u9488\u5bf9\u6027\u9632\u5fa1\u63aa\u65bd\u4ee5\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cDDLSs\u7684\u52a8\u6001\u884c\u4e3a\u786e\u5b9e\u5b58\u5728\u6548\u7387\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u7279\u5b9a\u8f93\u5165\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u3002\u73b0\u6709\u9632\u5fa1\u673a\u5236\u672a\u80fd\u5b8c\u5168\u8986\u76d6\u8fd9\u4e9b\u6f0f\u6d1e\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "conclusion": "\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u6548\u7387\u6f0f\u6d1e\u662f\u4e00\u4e2a\u88ab\u4f4e\u4f30\u7684\u5b89\u5168\u95ee\u9898\u3002\u672c\u6587\u63ed\u793a\u4e86\u5176\u6f5c\u5728\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u9632\u5fa1\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "paper_title_zh": "\u5229\u7528\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u6f0f\u6d1e", "abstract_zh": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e25\u683c\u5ef6\u8fdf\u548c\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u9ad8\u6548\u63a8\u7406\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\uff0c\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff08DDLSs\uff09\u5e94\u8fd0\u800c\u751f\uff0c\u901a\u8fc7\u8f93\u5165\u81ea\u9002\u5e94\u8ba1\u7b97\u4f18\u5316\u8fd0\u884c\u65f6\u6548\u7387\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u7cfb\u7edf\u6210\u529f\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u4f46\u5176\u52a8\u6001\u7279\u6027\u5f15\u5165\u4e86\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u98ce\u9669\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8f93\u5165\u4f9d\u8d56\u7684\u6267\u884c\u8def\u5f84\u4e3a\u653b\u51fb\u8005\u63d0\u4f9b\u4e86\u964d\u4f4e\u6548\u7387\u7684\u673a\u4f1a\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u3001\u80fd\u8017\u4e0a\u5347\uff0c\u751a\u81f3\u53ef\u80fd\u5bf9\u65f6\u95f4\u654f\u611f\u90e8\u7f72\u9020\u6210\u62d2\u7edd\u670d\u52a1\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86DDLSs\u4e2d\u52a8\u6001\u884c\u4e3a\u7684\u5b89\u5168\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5982\u4f55\u66b4\u9732\u53ef\u88ab\u6076\u610f\u8f93\u5165\u5229\u7528\u7684\u6548\u7387\u6f0f\u6d1e\u3002\u901a\u8fc7\u5bf9\u73b0\u6709\u653b\u51fb\u7b56\u7565\u7684\u8c03\u67e5\uff0c\u6211\u4eec\u53d1\u73b0\u65b0\u5174\u6a21\u578b\u67b6\u6784\u7684\u8986\u76d6\u4e0d\u8db3\u4ee5\u53ca\u5f53\u524d\u9632\u5fa1\u673a\u5236\u7684\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u7814\u7a76\u73b0\u4ee3DDLSs\u4e2d\u6548\u7387\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f00\u53d1\u9488\u5bf9\u6027\u9632\u5fa1\u63aa\u65bd\u4ee5\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u4fdd\u6301\u7cfb\u7edf\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.18569", "pdf": "https://arxiv.org/pdf/2506.18569", "abs": "https://arxiv.org/abs/2506.18569", "authors": ["Oleh Kuzyk", "Zuoyue Li", "Marc Pollefeys", "Xi Wang"], "title": "VisualChef: Generating Visual Aids in Cooking via Mask Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Cooking requires not only following instructions but also understanding,\nexecuting, and monitoring each step - a process that can be challenging without\nvisual guidance. Although recipe images and videos offer helpful cues, they\noften lack consistency in focus, tools, and setup. To better support the\ncooking process, we introduce VisualChef, a method for generating contextual\nvisual aids tailored to cooking scenarios. Given an initial frame and a\nspecified action, VisualChef generates images depicting both the action's\nexecution and the resulting appearance of the object, while preserving the\ninitial frame's environment. Previous work aims to integrate knowledge\nextracted from large language models by generating detailed textual\ndescriptions to guide image generation, which requires fine-grained\nvisual-textual alignment and involves additional annotations. In contrast,\nVisualChef simplifies alignment through mask-based visual grounding. Our key\ninsight is identifying action-relevant objects and classifying them to enable\ntargeted modifications that reflect the intended action and outcome while\nmaintaining a consistent environment. In addition, we propose an automated\npipeline to extract high-quality initial, action, and final state frames. We\nevaluate VisualChef quantitatively and qualitatively on three egocentric video\ndatasets and show its improvements over state-of-the-art methods.", "AI": {"tldr": "VisualChef\u662f\u4e00\u79cd\u901a\u8fc7\u63a9\u7801\u4fee\u590d\u6280\u672f\u751f\u6210\u70f9\u996a\u573a\u666f\u4e2d\u89c6\u89c9\u8f85\u52a9\u5de5\u5177\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u81f4\u7684\u89c6\u89c9\u6307\u5bfc\uff0c\u7b80\u5316\u89c6\u89c9\u4e0e\u6587\u672c\u5bf9\u9f50\u8fc7\u7a0b\u3002", "motivation": "\u70f9\u996a\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u7684\u89c6\u89c9\u6307\u5bfc\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u6587\u672c\u63cf\u8ff0\u548c\u989d\u5916\u6807\u6ce8\uff0cVisualChef\u65e8\u5728\u901a\u8fc7\u89c6\u89c9\u63a9\u7801\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "VisualChef\u901a\u8fc7\u63a9\u7801\u89c6\u89c9\u5b9a\u4f4d\u8bc6\u522b\u52a8\u4f5c\u76f8\u5173\u5bf9\u8c61\u5e76\u5206\u7c7b\uff0c\u751f\u6210\u53cd\u6620\u52a8\u4f5c\u548c\u7ed3\u679c\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u73af\u5883\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u63d0\u53d6\u9ad8\u8d28\u91cf\u5e27\u7684\u6d41\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\uff0cVisualChef\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VisualChef\u901a\u8fc7\u63a9\u7801\u4fee\u590d\u6280\u672f\u7b80\u5316\u89c6\u89c9\u8f85\u52a9\u751f\u6210\uff0c\u4e3a\u70f9\u996a\u573a\u666f\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u89c6\u89c9\u652f\u6301\u3002", "paper_title_zh": "VisualChef\uff1a\u901a\u8fc7\u63a9\u7801\u4fee\u590d\u751f\u6210\u70f9\u996a\u4e2d\u7684\u89c6\u89c9\u8f85\u52a9\u5de5\u5177", "abstract_zh": "\u70f9\u996a\u4e0d\u4ec5\u9700\u8981\u9075\u5faa\u6307\u4ee4\uff0c\u8fd8\u9700\u8981\u7406\u89e3\u3001\u6267\u884c\u548c\u76d1\u63a7\u6bcf\u4e00\u6b65\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u5728\u7f3a\u4e4f\u89c6\u89c9\u6307\u5bfc\u65f6\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002\u5c3d\u7ba1\u98df\u8c31\u56fe\u50cf\u548c\u89c6\u9891\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u63d0\u793a\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5728\u7126\u70b9\u3001\u5de5\u5177\u548c\u8bbe\u7f6e\u4e0a\u7f3a\u4e4f\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u652f\u6301\u70f9\u996a\u8fc7\u7a0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VisualChef\uff0c\u4e00\u79cd\u4e3a\u70f9\u996a\u573a\u666f\u751f\u6210\u4e0a\u4e0b\u6587\u89c6\u89c9\u8f85\u52a9\u5de5\u5177\u7684\u65b9\u6cd5\u3002\u7ed9\u5b9a\u521d\u59cb\u5e27\u548c\u6307\u5b9a\u52a8\u4f5c\uff0cVisualChef\u751f\u6210\u63cf\u7ed8\u52a8\u4f5c\u6267\u884c\u548c\u5bf9\u8c61\u6700\u7ec8\u5916\u89c2\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u7559\u521d\u59cb\u5e27\u7684\u73af\u5883\u3002\u5148\u524d\u7684\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u751f\u6210\u8be6\u7ec6\u7684\u6587\u672c\u63cf\u8ff0\u6765\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u8fd9\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u5e76\u6d89\u53ca\u989d\u5916\u6807\u6ce8\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cVisualChef\u901a\u8fc7\u57fa\u4e8e\u63a9\u7801\u7684\u89c6\u89c9\u5b9a\u4f4d\u7b80\u5316\u4e86\u5bf9\u9f50\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u5173\u952e\u89c1\u89e3\u662f\u8bc6\u522b\u52a8\u4f5c\u76f8\u5173\u5bf9\u8c61\u5e76\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u5b9e\u73b0\u53cd\u6620\u9884\u671f\u52a8\u4f5c\u548c\u7ed3\u679c\u7684\u9488\u5bf9\u6027\u4fee\u6539\uff0c\u540c\u65f6\u4fdd\u6301\u73af\u5883\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u7a0b\u6765\u63d0\u53d6\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u3001\u52a8\u4f5c\u548c\u6700\u7ec8\u72b6\u6001\u5e27\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u5bf9VisualChef\u8fdb\u884c\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6539\u8fdb\u3002"}}
{"id": "2506.18575", "pdf": "https://arxiv.org/pdf/2506.18575", "abs": "https://arxiv.org/abs/2506.18575", "authors": ["Kaifeng Sheng", "Zheng Zhou", "Yingliang Peng", "Qianwei Wang"], "title": "2D Triangle Splatting for Direct Differentiable Mesh Training", "categories": ["cs.CV"], "comment": "13 pages, 8 figures", "summary": "Differentiable rendering with 3D Gaussian primitives has emerged as a\npowerful method for reconstructing high-fidelity 3D scenes from multi-view\nimages. While it offers improvements over NeRF-based methods, this\nrepresentation still encounters challenges with rendering speed and advanced\nrendering effects, such as relighting and shadow rendering, compared to\nmesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a\nnovel method that replaces 3D Gaussian primitives with 2D triangle facelets.\nThis representation naturally forms a discrete mesh-like structure while\nretaining the benefits of continuous volumetric modeling. By incorporating a\ncompactness parameter into the triangle primitives, we enable direct training\nof photorealistic meshes. Our experimental results demonstrate that our\ntriangle-based method, in its vanilla version (without compactness tuning),\nachieves higher fidelity compared to state-of-the-art Gaussian-based methods.\nFurthermore, our approach produces reconstructed meshes with superior visual\nquality compared to existing mesh reconstruction methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a2D\u4e09\u89d2\u5f62\u9762\u7247\uff082DTS\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u66ff\u4ee33D\u9ad8\u65af\u57fa\u5143\uff0c\u76f4\u63a5\u8bad\u7ec3\u9ad8\u4fdd\u771f\u7f51\u683c\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8fde\u7eed\u4f53\u79ef\u5efa\u6a21\u7684\u4f18\u52bf\u548c\u79bb\u6563\u7f51\u683c\u7ed3\u6784\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u9ad8\u65af\u57fa\u5143\u548c\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e3D\u9ad8\u65af\u57fa\u5143\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u56fe\u50cf\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6e32\u67d3\u901f\u5ea6\u548c\u9ad8\u7ea7\u6e32\u67d3\u6548\u679c\uff08\u5982\u91cd\u65b0\u5149\u7167\u548c\u9634\u5f71\u6e32\u67d3\uff09\u4e0a\u4ecd\u4e0d\u53ca\u57fa\u4e8e\u7f51\u683c\u7684\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa2D\u4e09\u89d2\u5f62\u9762\u7247\uff082DTS\uff09\u65b9\u6cd5\uff0c\u75282D\u4e09\u89d2\u5f62\u9762\u7247\u66ff\u4ee33D\u9ad8\u65af\u57fa\u5143\uff0c\u5f62\u6210\u79bb\u6563\u7684\u7c7b\u7f51\u683c\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u8fde\u7eed\u4f53\u79ef\u5efa\u6a21\u7684\u4f18\u52bf\u3002\u901a\u8fc7\u5f15\u5165\u7d27\u51d1\u6027\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u8bad\u7ec3\u9ad8\u4fdd\u771f\u7f51\u683c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c2DTS\u65b9\u6cd5\u5728\u672a\u8c03\u6574\u7d27\u51d1\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u4fdd\u771f\u5ea6\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9ad8\u65af\u57fa\u5143\u65b9\u6cd5\uff0c\u4e14\u91cd\u5efa\u7684\u7f51\u683c\u89c6\u89c9\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "2DTS\u65b9\u6cd5\u7ed3\u5408\u4e86\u8fde\u7eed\u4f53\u79ef\u5efa\u6a21\u548c\u79bb\u6563\u7f51\u683c\u7684\u4f18\u52bf\uff0c\u4e3a\u9ad8\u4fdd\u771f\u7f51\u683c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "paper_title_zh": "\u7528\u4e8e\u76f4\u63a5\u53ef\u5fae\u5206\u7f51\u683c\u8bad\u7ec3\u76842D\u4e09\u89d2\u5f62\u9762\u7247\u65b9\u6cd5", "abstract_zh": "\u57fa\u4e8e3D\u9ad8\u65af\u57fa\u5143\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u5df2\u6210\u4e3a\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u91cd\u5efa\u9ad8\u4fdd\u771f3D\u573a\u666f\u7684\u5f3a\u5927\u65b9\u6cd5\u3002\u5c3d\u7ba1\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\uff0c\u4f46\u5728\u6e32\u67d3\u901f\u5ea6\u548c\u9ad8\u7ea7\u6e32\u67d3\u6548\u679c\uff08\u5982\u91cd\u65b0\u5149\u7167\u548c\u9634\u5f71\u6e32\u67d3\uff09\u65b9\u9762\u4ecd\u4e0d\u53ca\u57fa\u4e8e\u7f51\u683c\u7684\u6a21\u578b\u3002\u672c\u6587\u63d0\u51fa2D\u4e09\u89d2\u5f62\u9762\u7247\uff082DTS\uff09\u65b9\u6cd5\uff0c\u75282D\u4e09\u89d2\u5f62\u9762\u7247\u66ff\u4ee33D\u9ad8\u65af\u57fa\u5143\u3002\u8fd9\u79cd\u8868\u793a\u81ea\u7136\u5f62\u6210\u79bb\u6563\u7684\u7c7b\u7f51\u683c\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8fde\u7eed\u4f53\u79ef\u5efa\u6a21\u7684\u4f18\u52bf\u3002\u901a\u8fc7\u5728\u4e09\u89d2\u5f62\u57fa\u5143\u4e2d\u5f15\u5165\u7d27\u51d1\u6027\u53c2\u6570\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u76f4\u63a5\u8bad\u7ec3\u9ad8\u4fdd\u771f\u7f51\u683c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u4e09\u89d2\u5f62\u65b9\u6cd5\uff08\u672a\u8c03\u6574\u7d27\u51d1\u6027\uff09\u5728\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9ad8\u65af\u57fa\u5143\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91cd\u5efa\u7684\u7f51\u683c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\u3002"}}
{"id": "2506.17631", "pdf": "https://arxiv.org/pdf/2506.17631", "abs": "https://arxiv.org/abs/2506.17631", "authors": ["Zesen Wang", "Yonggang Li", "Lijuan Lan"], "title": "LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting aims to model temporal dependencies among variables\nfor future state inference, holding significant importance and widespread\napplications in real-world scenarios. Although deep learning-based methods have\nachieved remarkable progress, they still exhibit suboptimal performance in\nlong-term forecasting and data-scarce scenarios. Recent research demonstrates\nthat large language models (LLMs) achieve promising performance in time series\nforecasting. However, we find existing LLM-based methods still have\nshortcomings: (1) the absence of a unified paradigm for textual prompt\nformulation and (2) the neglect of modality discrepancies between textual\nprompts and time series. To address this, we propose LLM-Prompt, an LLM-based\ntime series forecasting framework integrating multi-prompt information and\ncross-modal semantic alignment. Specifically, we first construct a unified\ntextual prompt paradigm containing learnable soft prompts and textualized hard\nprompts. Second, to enhance LLMs' comprehensive understanding of the\nforecasting task, we design a semantic space embedding and cross-modal\nalignment module to achieve cross-modal fusion of temporal and textual\ninformation. Finally, the transformed time series from the LLMs are projected\nto obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3\ncarbon emission datasets demonstrate that LLM-Prompt is a powerful framework\nfor time series forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM-Prompt\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u63d0\u793a\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u6587\u672c\u63d0\u793a\u7edf\u4e00\u6027\u548c\u6a21\u6001\u5dee\u5f02\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7f3a\u4e4f\u7edf\u4e00\u7684\u6587\u672c\u63d0\u793a\u8303\u5f0f\uff0c\u4ee5\u53ca\u5ffd\u89c6\u6587\u672c\u63d0\u793a\u4e0e\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "method": "1. \u6784\u5efa\u7edf\u4e00\u7684\u6587\u672c\u63d0\u793a\u8303\u5f0f\uff0c\u5305\u542b\u53ef\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u548c\u6587\u672c\u5316\u7684\u786c\u63d0\u793a\uff1b2. \u8bbe\u8ba1\u8bed\u4e49\u7a7a\u95f4\u5d4c\u5165\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff0c\u5b9e\u73b0\u65f6\u95f4\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u878d\u5408\uff1b3. \u5c06LLM\u8f6c\u6362\u7684\u65f6\u95f4\u5e8f\u5217\u6295\u5f71\u4ee5\u83b7\u53d6\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u57286\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c3\u4e2a\u78b3\u6392\u653e\u6570\u636e\u96c6\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0cLLM-Prompt\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\u3002", "conclusion": "LLM-Prompt\u901a\u8fc7\u6574\u5408\u591a\u63d0\u793a\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u957f\u671f\u9884\u6d4b\u548c\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u3002", "paper_title_zh": "LLM-Prompt\uff1a\u96c6\u6210\u5f02\u6784\u63d0\u793a\u4ee5\u89e3\u9501\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65e8\u5728\u5efa\u6a21\u53d8\u91cf\u4e4b\u95f4\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u4ee5\u63a8\u65ad\u672a\u6765\u72b6\u6001\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u548c\u5e7f\u6cdb\u5e94\u7528\u3002\u5c3d\u7ba1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u957f\u671f\u9884\u6d4b\u548c\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u8868\u73b0\u4ecd\u4e0d\u7406\u60f3\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4ecd\u5b58\u5728\u4e0d\u8db3\uff1a\uff081\uff09\u7f3a\u4e4f\u7edf\u4e00\u7684\u6587\u672c\u63d0\u793a\u8303\u5f0f\uff1b\uff082\uff09\u5ffd\u89c6\u4e86\u6587\u672c\u63d0\u793a\u4e0e\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faLLM-Prompt\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u6574\u5408\u591a\u63d0\u793a\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u63d0\u793a\u8303\u5f0f\uff0c\u5305\u542b\u53ef\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u548c\u6587\u672c\u5316\u7684\u786c\u63d0\u793a\u3002\u5176\u6b21\uff0c\u4e3a\u589e\u5f3aLLM\u5bf9\u9884\u6d4b\u4efb\u52a1\u7684\u5168\u9762\u7406\u89e3\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8bed\u4e49\u7a7a\u95f4\u5d4c\u5165\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff0c\u5b9e\u73b0\u65f6\u95f4\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u878d\u5408\u3002\u6700\u540e\uff0c\u5c06LLM\u8f6c\u6362\u7684\u65f6\u95f4\u5e8f\u5217\u6295\u5f71\u4ee5\u83b7\u53d6\u9884\u6d4b\u7ed3\u679c\u3002\u57286\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c3\u4e2a\u78b3\u6392\u653e\u6570\u636e\u96c6\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0cLLM-Prompt\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\u3002"}}
{"id": "2506.18587", "pdf": "https://arxiv.org/pdf/2506.18587", "abs": "https://arxiv.org/abs/2506.18587", "authors": ["Antoine Saget", "Baptiste Lafabregue", "Antoine Cornu\u00e9jols", "Pierre Gan\u00e7arski"], "title": "Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing", "categories": ["cs.CV"], "comment": "10 pages, 2 figures, accepted at 42nd International Conference on\n  Machine Learning (ICML 2025) Terrabytes workshop", "summary": "Given the abundance of unlabeled Satellite Image Time Series (SITS) and the\nscarcity of labeled data, contrastive self-supervised pretraining emerges as a\nnatural tool to leverage this vast quantity of unlabeled data. However,\ndesigning effective data augmentations for contrastive learning remains\nchallenging for time series. We introduce a novel resampling-based augmentation\nstrategy that generates positive pairs by upsampling time series and extracting\ndisjoint subsequences while preserving temporal coverage. We validate our\napproach on multiple agricultural classification benchmarks using Sentinel-2\nimagery, showing that it outperforms common alternatives such as jittering,\nresizing, and masking. Further, we achieve state-of-the-art performance on the\nS2-Agri100 dataset without employing spatial information or temporal encodings,\nsurpassing more complex masked-based SSL frameworks. Our method offers a\nsimple, yet effective, contrastive learning augmentation for remote sensing\ntime series.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u91c7\u6837\u7684\u65f6\u95f4\u5e8f\u5217\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u9886\u57df\uff0c\u901a\u8fc7\u4e0a\u91c7\u6837\u548c\u63d0\u53d6\u4e0d\u91cd\u53e0\u5b50\u5e8f\u5217\u751f\u6210\u6b63\u6837\u672c\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u519c\u4e1a\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\u6570\u636e\u4e30\u5bcc\u4f46\u6807\u6ce8\u7a00\u7f3a\uff0c\u5bf9\u6bd4\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6210\u4e3a\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u7684\u6570\u636e\u589e\u5f3a\u8bbe\u8ba1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cd\u91c7\u6837\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u4e0a\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u5e76\u63d0\u53d6\u4e0d\u91cd\u53e0\u5b50\u5e8f\u5217\u4ee5\u751f\u6210\u6b63\u6837\u672c\u5bf9\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5728\u591a\u4e2a\u519c\u4e1a\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6296\u52a8\u3001\u8c03\u6574\u5927\u5c0f\u548c\u63a9\u7801\u7b49\u5e38\u89c1\u65b9\u6cd5\uff0c\u5e76\u5728S2-Agri100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f7f\u7528\u7a7a\u95f4\u4fe1\u606f\u6216\u65f6\u95f4\u7f16\u7801\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9065\u611f\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\uff0c\u6027\u80fd\u4f18\u4e8e\u590d\u6742\u7684\u57fa\u4e8e\u63a9\u7801\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002", "paper_title_zh": "\u65f6\u95f4\u5e8f\u5217\u5bf9\u6bd4\u5b66\u4e60\u7684\u91cd\u91c7\u6837\u589e\u5f3a\u65b9\u6cd5\uff1a\u5728\u9065\u611f\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u9274\u4e8e\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\u6570\u636e\u4e30\u5bcc\u800c\u6807\u6ce8\u7a00\u7f3a\uff0c\u5bf9\u6bd4\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6210\u4e3a\u5229\u7528\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u7684\u81ea\u7136\u9009\u62e9\u3002\u7136\u800c\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u8bbe\u8ba1\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4ecd\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u91c7\u6837\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u4e0a\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u5e76\u63d0\u53d6\u4e0d\u91cd\u53e0\u5b50\u5e8f\u5217\u751f\u6210\u6b63\u6837\u672c\u5bf9\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u8986\u76d6\u8303\u56f4\u3002\u6211\u4eec\u5728\u591a\u4e2a\u519c\u4e1a\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528Sentinel-2\u56fe\u50cf\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u6296\u52a8\u3001\u8c03\u6574\u5927\u5c0f\u548c\u63a9\u7801\u7b49\u5e38\u89c1\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728S2-Agri100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u672a\u4f7f\u7528\u7a7a\u95f4\u4fe1\u606f\u6216\u65f6\u95f4\u7f16\u7801\uff0c\u8d85\u8d8a\u4e86\u66f4\u590d\u6742\u7684\u57fa\u4e8e\u63a9\u7801\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u9065\u611f\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\u3002"}}
{"id": "2506.18591", "pdf": "https://arxiv.org/pdf/2506.18591", "abs": "https://arxiv.org/abs/2506.18591", "authors": ["Mauricio Byrd Victorica", "Gy\u00f6rgy D\u00e1n", "Henrik Sandberg"], "title": "SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds", "categories": ["cs.CV", "cs.LG"], "comment": "2025 IEEE Conference on Secure and Trustworthy Machine Learning\n  (SaTML2025)", "summary": "State-of-the-art convolutional neural network models for object detection and\nimage classification are vulnerable to physically realizable adversarial\nperturbations, such as patch attacks. Existing defenses have focused,\nimplicitly or explicitly, on single-patch attacks, leaving their sensitivity to\nthe number of patches as an open question or rendering them computationally\ninfeasible or inefficient against attacks consisting of multiple patches in the\nworst cases. In this work, we propose SpaNN, an attack detector whose\ncomputational complexity is independent of the expected number of adversarial\npatches. The key novelty of the proposed detector is that it builds an ensemble\nof binarized feature maps by applying a set of saliency thresholds to the\nneural activations of the first convolutional layer of the victim model. It\nthen performs clustering on the ensemble and uses the cluster features as the\ninput to a classifier for attack detection. Contrary to existing detectors,\nSpaNN does not rely on a fixed saliency threshold for identifying adversarial\nregions, which makes it robust against white box adversarial attacks. We\nevaluate SpaNN on four widely used data sets for object detection and\nclassification, and our results show that SpaNN outperforms state-of-the-art\ndefenses by up to 11 and 27 percentage points in the case of object detection\nand the case of image classification, respectively. Our code is available at\nhttps://github.com/gerkbyrd/SpaNN.", "AI": {"tldr": "SpaNN\u662f\u4e00\u79cd\u65b0\u578b\u5bf9\u6297\u6027\u8865\u4e01\u68c0\u6d4b\u5668\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u8865\u4e01\u6570\u91cf\u65e0\u5173\uff0c\u901a\u8fc7\u591a\u663e\u8457\u6027\u9608\u503c\u6784\u5efa\u4e8c\u503c\u5316\u7279\u5f81\u56fe\u96c6\u5408\uff0c\u5e76\u5229\u7528\u805a\u7c7b\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u8865\u4e01\u653b\u51fb\uff0c\u5bf9\u591a\u8865\u4e01\u653b\u51fb\u6548\u679c\u4e0d\u4f73\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002SpaNN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u591a\u8865\u4e01\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "SpaNN\u901a\u8fc7\u5e94\u7528\u4e00\u7ec4\u663e\u8457\u6027\u9608\u503c\u5bf9\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\u6fc0\u6d3b\u8fdb\u884c\u4e8c\u503c\u5316\uff0c\u6784\u5efa\u7279\u5f81\u56fe\u96c6\u5408\uff0c\u7136\u540e\u8fdb\u884c\u805a\u7c7b\u5e76\u5c06\u805a\u7c7b\u7279\u5f81\u8f93\u5165\u5206\u7c7b\u5668\u8fdb\u884c\u653b\u51fb\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\uff0cSpaNN\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u9632\u5fa1\u65b9\u6cd5\u9ad8\u51fa11\u548c27\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SpaNN\u901a\u8fc7\u591a\u663e\u8457\u6027\u9608\u503c\u548c\u805a\u7c7b\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8865\u4e01\u653b\u51fb\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u8865\u4e01\u6570\u91cf\u65e0\u5173\u3002", "paper_title_zh": "SpaNN\uff1a\u901a\u8fc7\u8de8\u5ea6\u663e\u8457\u6027\u9608\u503c\u68c0\u6d4bCNN\u4e0a\u7684\u591a\u4e2a\u5bf9\u6297\u6027\u8865\u4e01", "abstract_zh": "\u6700\u5148\u8fdb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5bb9\u6613\u53d7\u5230\u7269\u7406\u53ef\u5b9e\u73b0\u7684\u5bf9\u6297\u6027\u6270\u52a8\uff08\u5982\u8865\u4e01\u653b\u51fb\uff09\u7684\u5f71\u54cd\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u8865\u4e01\u653b\u51fb\uff0c\u5bf9\u591a\u8865\u4e01\u653b\u51fb\u7684\u654f\u611f\u6027\u5c1a\u672a\u89e3\u51b3\uff0c\u6216\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\u6216\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u63d0\u51faSpaNN\uff0c\u4e00\u79cd\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u9884\u671f\u8865\u4e01\u6570\u91cf\u65e0\u5173\u7684\u653b\u51fb\u68c0\u6d4b\u5668\u3002\u8be5\u68c0\u6d4b\u5668\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u901a\u8fc7\u5e94\u7528\u4e00\u7ec4\u663e\u8457\u6027\u9608\u503c\u5bf9\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\u6fc0\u6d3b\u8fdb\u884c\u4e8c\u503c\u5316\uff0c\u6784\u5efa\u7279\u5f81\u56fe\u96c6\u5408\uff0c\u7136\u540e\u8fdb\u884c\u805a\u7c7b\u5e76\u5c06\u805a\u7c7b\u7279\u5f81\u7528\u4e8e\u653b\u51fb\u68c0\u6d4b\u5206\u7c7b\u3002\u4e0e\u73b0\u6709\u68c0\u6d4b\u5668\u4e0d\u540c\uff0cSpaNN\u4e0d\u4f9d\u8d56\u56fa\u5b9a\u663e\u8457\u6027\u9608\u503c\u8bc6\u522b\u5bf9\u6297\u533a\u57df\uff0c\u4ece\u800c\u5bf9\u767d\u76d2\u5bf9\u6297\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30SpaNN\uff0c\u7ed3\u679c\u663e\u793a\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cSpaNN\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u9632\u5fa1\u65b9\u6cd5\u9ad8\u51fa11\u548c27\u4e2a\u767e\u5206\u70b9\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/gerkbyrd/SpaNN\u3002"}}
{"id": "2506.17639", "pdf": "https://arxiv.org/pdf/2506.17639", "abs": "https://arxiv.org/abs/2506.17639", "authors": ["Yuxuan Chen", "Xiao Li"], "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action models (VLA) have demonstrated remarkable capabilities\nand promising potential in solving complex robotic manipulation tasks. However,\ntheir substantial parameter sizes and high inference latency pose significant\nchallenges for real-world deployment, particularly on resource-constrained\nrobotic platforms. To address this issue, we begin by conducting an extensive\nempirical study to explore the effectiveness of model compression techniques\nwhen applied to VLAs. Building on the insights gained from these preliminary\nexperiments, we propose RLRC, a three-stage recovery method for compressed\nVLAs, including structured pruning, performance recovery based on SFT and RL,\nand further quantization. RLRC achieves up to an 8x reduction in memory usage\nand a 2.3x improvement in inference throughput, while maintaining or even\nsurpassing the original VLA's task success rate. Extensive experiments show\nthat RLRC consistently outperforms existing compression baselines,\ndemonstrating strong potential for on-device deployment of VLAs. Project\nwebsite: https://rlrc-vla.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6062\u590d\u65b9\u6cd5RLRC\uff0c\u7528\u4e8e\u538b\u7f29\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u53c2\u6570\u548c\u9ad8\u5ef6\u8fdf\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u538b\u7f29\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RLRC\u91c7\u7528\u4e09\u9636\u6bb5\u6062\u590d\u65b9\u6cd5\uff1a\u7ed3\u6784\u5316\u526a\u679d\u3001\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6027\u80fd\u6062\u590d\uff0c\u4ee5\u53ca\u8fdb\u4e00\u6b65\u91cf\u5316\u3002", "result": "RLRC\u5b9e\u73b0\u4e86\u5185\u5b58\u5360\u7528\u51cf\u5c118\u500d\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53472.3\u500d\uff0c\u540c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u4fdd\u6301\u751a\u81f3\u8d85\u8fc7\u539f\u59cb\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u57fa\u7ebf\u3002", "conclusion": "RLRC\u5c55\u793a\u4e86\u5728\u8bbe\u5907\u7aef\u90e8\u7f72VLA\u7684\u6f5c\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "RLRC\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u538b\u7f29\u6062\u590d\u65b9\u6cd5", "abstract_zh": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u5728\u89e3\u51b3\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\u548c\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5176\u5e9e\u5927\u7684\u53c2\u6570\u89c4\u6a21\u548c\u9ad8\u63a8\u7406\u5ef6\u8fdf\u4e3a\u5b9e\u9645\u90e8\u7f72\u5e26\u6765\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63a2\u7d22\u4e86\u6a21\u578b\u538b\u7f29\u6280\u672f\u5728VLA\u4e0a\u7684\u6709\u6548\u6027\u3002\u57fa\u4e8e\u521d\u6b65\u5b9e\u9a8c\u7684\u6d1e\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RLRC\uff0c\u4e00\u79cd\u9488\u5bf9\u538b\u7f29VLA\u7684\u4e09\u9636\u6bb5\u6062\u590d\u65b9\u6cd5\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u526a\u679d\u3001\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6027\u80fd\u6062\u590d\uff0c\u4ee5\u53ca\u8fdb\u4e00\u6b65\u91cf\u5316\u3002RLRC\u5b9e\u73b0\u4e86\u5185\u5b58\u5360\u7528\u51cf\u5c118\u500d\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53472.3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8fc7\u539f\u59cbVLA\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRLRC\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5728\u8bbe\u5907\u7aef\u90e8\u7f72VLA\u7684\u5f3a\u5927\u6f5c\u529b\u3002\u9879\u76ee\u7f51\u7ad9\uff1ahttps://rlrc-vla.github.io"}}
{"id": "2506.18655", "pdf": "https://arxiv.org/pdf/2506.18655", "abs": "https://arxiv.org/abs/2506.18655", "authors": ["Wenxu Qian", "Chaoyue Wang", "Hou Peng", "Zhiyu Tan", "Hao Li", "Anxiang Zeng"], "title": "RDPO: Real Data Preference Optimization for Physics Consistency Video Generation", "categories": ["cs.CV", "I.2.6; I.2.10"], "comment": "16 pages, 10 figures", "summary": "Video generation techniques have achieved remarkable advancements in visual\nquality, yet faithfully reproducing real-world physics remains elusive.\nPreference-based model post-training may improve physical consistency, but\nrequires costly human-annotated datasets or reward models that are not yet\nfeasible. To address these challenges, we present Real Data Preference\nOptimisation (RDPO), an annotation-free framework that distills physical priors\ndirectly from real-world videos. Specifically, the proposed RDPO\nreverse-samples real video sequences with a pre-trained generator to\nautomatically build preference pairs that are statistically distinguishable in\nterms of physical correctness. A multi-stage iterative training schedule then\nguides the generator to obey physical laws increasingly well. Benefiting from\nthe dynamic information explored from real videos, our proposed RDPO\nsignificantly improves the action coherence and physical realism of the\ngenerated videos. Evaluations on multiple benchmarks and human evaluations have\ndemonstrated that RDPO achieves improvements across multiple dimensions. The\nsource code and demonstration of this paper are available at:\nhttps://wwenxu.github.io/RDPO/", "AI": {"tldr": "RDPO\u662f\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u63d0\u53d6\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u52a8\u4f5c\u8fde\u8d2f\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u771f\u5b9e\u7269\u7406\u89c4\u5f8b\u7684\u518d\u73b0\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5956\u52b1\u6a21\u578b\uff0c\u96be\u4ee5\u5b9e\u73b0\u3002RDPO\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u5b66\u4e60\u7269\u7406\u5148\u9a8c\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RDPO\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u5668\u53cd\u5411\u91c7\u6837\u771f\u5b9e\u89c6\u9891\u5e8f\u5217\uff0c\u81ea\u52a8\u6784\u5efa\u7269\u7406\u6b63\u786e\u6027\u53ef\u533a\u5206\u7684\u504f\u597d\u5bf9\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u8fed\u4ee3\u8bad\u7ec3\u9010\u6b65\u63d0\u5347\u751f\u6210\u5668\u5bf9\u7269\u7406\u89c4\u5f8b\u7684\u9075\u5faa\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\uff0cRDPO\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u52a8\u4f5c\u8fde\u8d2f\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\uff0c\u53d6\u5f97\u4e86\u591a\u7ef4\u5ea6\u6539\u8fdb\u3002", "conclusion": "RDPO\u901a\u8fc7\u76f4\u63a5\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u63d0\u53d6\u52a8\u6001\u4fe1\u606f\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u6807\u6ce8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "RDPO\uff1a\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u504f\u597d\u4f18\u5316\u7684\u7269\u7406\u4e00\u81f4\u6027\u89c6\u9891\u751f\u6210", "abstract_zh": "\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u771f\u5b9e\u7269\u7406\u89c4\u5f8b\u7684\u518d\u73b0\u4ecd\u5177\u6311\u6218\u6027\u3002\u57fa\u4e8e\u504f\u597d\u7684\u6a21\u578b\u540e\u8bad\u7ec3\u53ef\u80fd\u63d0\u5347\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4f46\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5c1a\u4e0d\u53ef\u884c\u7684\u5956\u52b1\u6a21\u578b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u771f\u5b9e\u6570\u636e\u504f\u597d\u4f18\u5316\uff08RDPO\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u63d0\u53d6\u7269\u7406\u5148\u9a8c\u3002\u5177\u4f53\u800c\u8a00\uff0cRDPO\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u5668\u53cd\u5411\u91c7\u6837\u771f\u5b9e\u89c6\u9891\u5e8f\u5217\uff0c\u81ea\u52a8\u6784\u5efa\u7269\u7406\u6b63\u786e\u6027\u53ef\u533a\u5206\u7684\u504f\u597d\u5bf9\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u8fed\u4ee3\u8bad\u7ec3\u9010\u6b65\u63d0\u5347\u751f\u6210\u5668\u5bf9\u7269\u7406\u89c4\u5f8b\u7684\u9075\u5faa\u80fd\u529b\u3002\u5f97\u76ca\u4e8e\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u63a2\u7d22\u7684\u52a8\u6001\u4fe1\u606f\uff0cRDPO\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u52a8\u4f5c\u8fde\u8d2f\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\uff0cRDPO\u5b9e\u73b0\u4e86\u591a\u7ef4\u5ea6\u6539\u8fdb\u3002\u672c\u6587\u7684\u6e90\u4ee3\u7801\u548c\u6f14\u793a\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://wwenxu.github.io/RDPO/"}}
{"id": "2506.18658", "pdf": "https://arxiv.org/pdf/2506.18658", "abs": "https://arxiv.org/abs/2506.18658", "authors": ["Ling Zhang", "Boxiang Yun", "Qingli Li", "Yan Wang"], "title": "Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automated pathology report generation from Whole Slide Images (WSIs) faces\ntwo key challenges: (1) lack of semantic content in visual features and (2)\ninherent information redundancy in WSIs. To address these issues, we propose a\nnovel Historical Report Guided \\textbf{Bi}-modal Concurrent Learning Framework\nfor Pathology Report \\textbf{Gen}eration (BiGen) emulating pathologists'\ndiagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to\nprovide rich semantic content, which retrieves WSI-relevant knowledge from\npre-built medical knowledge bank by matching high-attention patches and (2) A\nbi-modal concurrent learning strategy instantiated via a learnable visual token\nand a learnable textual token to dynamically extract key visual features and\nretrieved knowledge, where weight-shared layers enable cross-modal alignment\nbetween visual features and knowledge features. Our multi-modal decoder\nintegrates both modals for comprehensive diagnostic reports generation.\nExperiments on the PathText (BRCA) dataset demonstrate our framework's\nsuperiority, achieving state-of-the-art performance with 7.4\\% relative\nimprovement in NLP metrics and 19.1\\% enhancement in classification metrics for\nHer-2 prediction versus existing methods. Ablation studies validate the\nnecessity of our proposed modules, highlighting our method's ability to provide\nWSI-relevant rich semantic content and suppress information redundancy in WSIs.\nCode is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u62a5\u544a\u5f15\u5bfc\u7684\u53cc\u6a21\u6001\u5e76\u53d1\u5b66\u4e60\u6846\u67b6\uff08BiGen\uff09\uff0c\u7528\u4e8e\u75c5\u7406\u62a5\u544a\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u4e2d\u8bed\u4e49\u5185\u5bb9\u4e0d\u8db3\u548c\u4fe1\u606f\u5197\u4f59\u7684\u95ee\u9898\u3002\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u673a\u5236\u548c\u53cc\u6a21\u6001\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62a5\u544a\u751f\u6210\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u5316\u75c5\u7406\u62a5\u544a\u751f\u6210\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u89c6\u89c9\u7279\u5f81\u7f3a\u4e4f\u8bed\u4e49\u5185\u5bb9\uff0c\u4ee5\u53caWSI\u4e2d\u5b58\u5728\u4fe1\u606f\u5197\u4f59\u3002\u672c\u6587\u65e8\u5728\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u901a\u8fc7\u5f15\u5165\u5386\u53f2\u62a5\u544a\u548c\u53cc\u6a21\u6001\u5b66\u4e60\uff0c\u63d0\u5347\u62a5\u544a\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "1. \u77e5\u8bc6\u68c0\u7d22\u673a\u5236\uff1a\u4ece\u9884\u5efa\u7684\u533b\u5b66\u77e5\u8bc6\u5e93\u4e2d\u5339\u914d\u9ad8\u6ce8\u610f\u529b\u533a\u57df\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u5185\u5bb9\u30022. \u53cc\u6a21\u6001\u5e76\u53d1\u5b66\u4e60\u7b56\u7565\uff1a\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u89c6\u89c9\u548c\u6587\u672c\u6807\u8bb0\u52a8\u6001\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5229\u7528\u6743\u91cd\u5171\u4eab\u5c42\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u591a\u6a21\u6001\u89e3\u7801\u5668\u6574\u5408\u4e24\u79cd\u6a21\u6001\u751f\u6210\u5168\u9762\u8bca\u65ad\u62a5\u544a\u3002", "result": "\u5728PathText\uff08BRCA\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBiGen\u5728NLP\u6307\u6807\u4e0a\u76f8\u5bf9\u63d0\u5347\u4e867.4%\uff0c\u5728Her-2\u9884\u6d4b\u7684\u5206\u7c7b\u6307\u6807\u4e0a\u63d0\u5347\u4e8619.1%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u5757\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "BiGen\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5386\u53f2\u62a5\u544a\u548c\u53cc\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86WSI\u4e2d\u7684\u8bed\u4e49\u4e0d\u8db3\u548c\u4fe1\u606f\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u62a5\u544a\u751f\u6210\u7684\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002", "paper_title_zh": "\u57fa\u4e8e\u5386\u53f2\u62a5\u544a\u5f15\u5bfc\u7684\u53cc\u6a21\u6001\u5e76\u53d1\u5b66\u4e60\u75c5\u7406\u62a5\u544a\u751f\u6210", "abstract_zh": "\u81ea\u52a8\u5316\u75c5\u7406\u62a5\u544a\u751f\u6210\u9762\u4e34\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u4e2d\u89c6\u89c9\u7279\u5f81\u7f3a\u4e4f\u8bed\u4e49\u5185\u5bb9\u548c\u4fe1\u606f\u5197\u4f59\u4e24\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u8bca\u65ad\u63a8\u7406\u7684\u5386\u53f2\u62a5\u544a\u5f15\u5bfc\u53cc\u6a21\u6001\u5e76\u53d1\u5b66\u4e60\u6846\u67b6\uff08BiGen\uff09\uff0c\u5305\u62ec\uff1a\uff081\uff09\u77e5\u8bc6\u68c0\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u5339\u914d\u9ad8\u6ce8\u610f\u529b\u533a\u57df\u4ece\u9884\u5efa\u533b\u5b66\u77e5\u8bc6\u5e93\u4e2d\u83b7\u53d6\u76f8\u5173\u8bed\u4e49\u5185\u5bb9\uff1b\uff082\uff09\u53cc\u6a21\u6001\u5e76\u53d1\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u89c6\u89c9\u548c\u6587\u672c\u6807\u8bb0\u52a8\u6001\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5229\u7528\u6743\u91cd\u5171\u4eab\u5c42\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u591a\u6a21\u6001\u89e3\u7801\u5668\u6574\u5408\u4e24\u79cd\u6a21\u6001\u751f\u6210\u5168\u9762\u8bca\u65ad\u62a5\u544a\u3002\u5728PathText\uff08BRCA\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBiGen\u5728NLP\u6307\u6807\u4e0a\u76f8\u5bf9\u63d0\u5347\u4e867.4%\uff0c\u5728Her-2\u9884\u6d4b\u7684\u5206\u7c7b\u6307\u6807\u4e0a\u63d0\u5347\u4e8619.1%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u5757\u7684\u5fc5\u8981\u6027\uff0c\u8868\u660eBiGen\u80fd\u591f\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u5185\u5bb9\u5e76\u6291\u5236\u4fe1\u606f\u5197\u4f59\u3002\u4ee3\u7801\u5df2\u516c\u5f00\uff1ahttps://github.com/DeepMed-Lab-ECNU/BiGen\u3002"}}
{"id": "2506.18668", "pdf": "https://arxiv.org/pdf/2506.18668", "abs": "https://arxiv.org/abs/2506.18668", "authors": ["Pablo Meseguer", "Roc\u00edo del Amor", "Valery Naranjo"], "title": "Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping", "categories": ["cs.CV", "cs.AI"], "comment": "Accepeted for oral presentation at Medical Image Understanding and\n  Analysis (MIUA) 2025", "summary": "Pretraining on large-scale, in-domain datasets grants histopathology\nfoundation models (FM) the ability to learn task-agnostic data representations,\nenhancing transfer learning on downstream tasks. In computational pathology,\nautomated whole slide image analysis requires multiple instance learning (MIL)\nframeworks due to the gigapixel scale of the slides. The diversity among\nhistopathology FMs has highlighted the need to design real-world challenges for\nevaluating their effectiveness. To bridge this gap, our work presents a novel\nbenchmark for evaluating histopathology FMs as patch-level feature extractors\nwithin a MIL classification framework. For that purpose, we leverage the\nAI4SkIN dataset, a multi-center cohort encompassing slides with challenging\ncutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -\nSilhouette Index (FM-SI), a novel metric to measure model consistency against\ndistribution shifts. Our experimentation shows that extracting less biased\nfeatures enhances classification performance, especially in similarity-based\nMIL classifiers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u591a\u4e2d\u5fc3\u76ae\u80a4\u764c\u4e9a\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807FM-SI\u6765\u8861\u91cf\u6a21\u578b\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u7684\u591a\u6837\u6027\u4f7f\u5f97\u9700\u8981\u8bbe\u8ba1\u771f\u5b9e\u4e16\u754c\u7684\u6311\u6218\u6765\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u8bc4\u4f30FM\u5728\u76ae\u80a4\u764c\u4e9a\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5229\u7528AI4SkIN\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\uff0c\u5c06FM\u4f5c\u4e3a\u8865\u4e01\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u7ed3\u5408\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5206\u7c7b\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6307\u6807FM-SI\u6765\u8861\u91cf\u6a21\u578b\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u53d6\u8f83\u5c11\u504f\u89c1\u7684\u7279\u5f81\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684MIL\u5206\u7c7b\u5668\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u548cFM-SI\u6307\u6807\u4e3a\u8bc4\u4f30\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u76ae\u80a4\u764c\u4e9a\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e2d\u5bf9\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u76ae\u80a4\u764c\u4e9a\u578b\u5206\u7c7b\u7684\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u901a\u8fc7\u5728\u5927\u578b\u9886\u57df\u5185\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u80fd\u591f\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u8868\u793a\uff0c\u4ece\u800c\u589e\u5f3a\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u3002\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\uff0c\u7531\u4e8e\u5207\u7247\u56fe\u50cf\u7684\u5343\u5146\u50cf\u7d20\u89c4\u6a21\uff0c\u81ea\u52a8\u5316\u7684\u5168\u5207\u7247\u56fe\u50cf\u5206\u6790\u9700\u8981\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6846\u67b6\u3002\u7ec4\u7ec7\u75c5\u7406\u5b66FM\u7684\u591a\u6837\u6027\u51f8\u663e\u4e86\u8bbe\u8ba1\u771f\u5b9e\u4e16\u754c\u6311\u6218\u4ee5\u8bc4\u4f30\u5176\u6709\u6548\u6027\u7684\u5fc5\u8981\u6027\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30FM\u4f5c\u4e3a\u8865\u4e01\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u5728MIL\u5206\u7c7b\u6846\u67b6\u4e2d\u7684\u8868\u73b0\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5229\u7528\u4e86AI4SkIN\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u5177\u6709\u6311\u6218\u6027\u7684\u76ae\u80a4\u68ad\u5f62\u7ec6\u80de\u80bf\u7624\u4e9a\u578b\u7684\u591a\u4e2d\u5fc3\u961f\u5217\u3002\u6211\u4eec\u8fd8\u5b9a\u4e49\u4e86\u57fa\u7840\u6a21\u578b-\u8f6e\u5ed3\u6307\u6570\uff08FM-SI\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u53d6\u8f83\u5c11\u504f\u89c1\u7684\u7279\u5f81\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684MIL\u5206\u7c7b\u5668\u4e2d\u3002"}}
{"id": "2506.17680", "pdf": "https://arxiv.org/pdf/2506.17680", "abs": "https://arxiv.org/abs/2506.17680", "authors": ["Zhengni Yang", "Rui Yang", "Weijian Han", "Qixin Liu"], "title": "Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "accepted by IJCNN2025", "summary": "This paper introduces a novel deep-learning approach to predict true\nstress-strain curves of high-strength steels from small punch test (SPT)\nload-displacement data. The proposed approach uses Gramian Angular Field (GAF)\nto transform load-displacement sequences into images, capturing\nspatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model\nwith an LSTM-based encoder-decoder architecture, enhanced by multi-head\ncross-attention to improved accuracy. Experimental results demonstrate that the\nproposed approach achieves superior prediction accuracy, with minimum and\nmaximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The\nproposed method offers a promising alternative to traditional experimental\ntechniques in materials science, enhancing the accuracy and efficiency of true\nstress-strain relationship predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u51b2\u5b54\u8bd5\u9a8c\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Seq2Seq\u6a21\u578b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u9ad8\u5f3a\u94a2\u7684\u771f\u5b9e\u5e94\u529b-\u5e94\u53d8\u66f2\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u5728\u9884\u6d4b\u9ad8\u5f3a\u94a2\u7684\u771f\u5b9e\u5e94\u529b-\u5e94\u53d8\u66f2\u7ebf\u65f6\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Gramian Angular Field\u5c06\u8f7d\u8377-\u4f4d\u79fb\u5e8f\u5217\u8f6c\u5316\u4e3a\u56fe\u50cf\u4ee5\u6355\u6349\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eLSTM\u7684Seq2Seq\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7684\u6700\u5c0f\u548c\u6700\u5927\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a0.15 MPa\u548c5.58 MPa\uff0c\u9884\u6d4b\u7cbe\u5ea6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u771f\u5b9e\u5e94\u529b-\u5e94\u53d8\u5173\u7cfb\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u5c0f\u51b2\u5b54\u8bd5\u9a8c\u7684Seq2Seq\u4e0e\u4ea4\u53c9\u6ce8\u610f\u529b\u589e\u5f3a\u5e94\u529b-\u5e94\u53d8\u9884\u6d4b", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5c0f\u51b2\u5b54\u8bd5\u9a8c\uff08SPT\uff09\u7684\u8f7d\u8377-\u4f4d\u79fb\u6570\u636e\u4e2d\u9884\u6d4b\u9ad8\u5f3a\u94a2\u7684\u771f\u5b9e\u5e94\u529b-\u5e94\u53d8\u66f2\u7ebf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7Gramian Angular Field\u5c06\u8f7d\u8377-\u4f4d\u79fb\u5e8f\u5217\u8f6c\u5316\u4e3a\u56fe\u50cf\u4ee5\u6355\u6349\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eLSTM\u7684Seq2Seq\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u6700\u5c0f\u548c\u6700\u5927\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a0.15 MPa\u548c5.58 MPa\uff0c\u9884\u6d4b\u7cbe\u5ea6\u663e\u8457\u3002\u8be5\u65b9\u6cd5\u4e3a\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u771f\u5b9e\u5e94\u529b-\u5e94\u53d8\u5173\u7cfb\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.18669", "pdf": "https://arxiv.org/pdf/2506.18669", "abs": "https://arxiv.org/abs/2506.18669", "authors": ["Hao Shao", "Qibin Hou"], "title": "MedSeg-R: Medical Image Segmentation with Clinical Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation is challenging due to overlapping anatomies with\nambiguous boundaries and a severe imbalance between the foreground and\nbackground classes, which particularly affects the delineation of small\nlesions. Existing methods, including encoder-decoder networks and prompt-driven\nvariants of the Segment Anything Model (SAM), rely heavily on local cues or\nuser prompts and lack integrated semantic priors, thus failing to generalize\nwell to low-contrast or overlapping targets. To address these issues, we\npropose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by\nclinical reasoning. Its cognitive stage interprets medical report into\nstructured semantic priors (location, texture, shape), which are fused via\ntransformer block. In the perceptual stage, these priors modulate the SAM\nbackbone: spatial attention highlights likely lesion regions, dynamic\nconvolution adapts feature filters to expected textures, and deformable\nsampling refines spatial support. By embedding this fine-grained guidance\nearly, MedSeg-R disentangles inter-class confusion and amplifies minority-class\ncues, greatly improving sensitivity to small lesions. In challenging\nbenchmarks, MedSeg-R produces large Dice improvements in overlapping and\nambiguous structures, demonstrating plug-and-play compatibility with SAM-based\nsystems.", "AI": {"tldr": "MedSeg-R\u662f\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u63a8\u7406\u7684\u8f7b\u91cf\u7ea7\u53cc\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u533b\u5b66\u62a5\u544a\u7684\u8bed\u4e49\u5148\u9a8c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5bf9\u5c0f\u75c5\u7076\u7684\u654f\u611f\u6027\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u89e3\u5256\u7ed3\u6784\u8fb9\u754c\u6a21\u7cca\u3001\u524d\u666f\u4e0e\u80cc\u666f\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u7ebf\u7d22\u6216\u7528\u6237\u63d0\u793a\uff0c\u7f3a\u4e4f\u8bed\u4e49\u5148\u9a8c\uff0c\u96be\u4ee5\u5904\u7406\u4f4e\u5bf9\u6bd4\u5ea6\u6216\u91cd\u53e0\u76ee\u6807\u3002", "method": "MedSeg-R\u91c7\u7528\u53cc\u9636\u6bb5\u8bbe\u8ba1\uff1a\u8ba4\u77e5\u9636\u6bb5\u89e3\u6790\u533b\u5b66\u62a5\u544a\u751f\u6210\u7ed3\u6784\u5316\u8bed\u4e49\u5148\u9a8c\uff08\u4f4d\u7f6e\u3001\u7eb9\u7406\u3001\u5f62\u72b6\uff09\uff0c\u5e76\u901a\u8fc7Transformer\u878d\u5408\uff1b\u611f\u77e5\u9636\u6bb5\u5229\u7528\u8fd9\u4e9b\u5148\u9a8c\u8c03\u5236SAM\u4e3b\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u7a7a\u95f4\u6ce8\u610f\u529b\u3001\u52a8\u6001\u5377\u79ef\u548c\u53ef\u53d8\u5f62\u91c7\u6837\u4f18\u5316\u5206\u5272\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedSeg-R\u663e\u8457\u63d0\u5347\u4e86\u91cd\u53e0\u548c\u6a21\u7cca\u7ed3\u6784\u7684\u5206\u5272\u6027\u80fd\uff0c\u5bf9\u5c0f\u75c5\u7076\u7684\u654f\u611f\u6027\u5927\u5e45\u63d0\u9ad8\u3002", "conclusion": "MedSeg-R\u901a\u8fc7\u5d4c\u5165\u4e34\u5e8a\u63a8\u7406\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4e0eSAM\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\u3002", "paper_title_zh": "MedSeg-R\uff1a\u57fa\u4e8e\u4e34\u5e8a\u63a8\u7406\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272", "abstract_zh": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u56e0\u89e3\u5256\u7ed3\u6784\u8fb9\u754c\u6a21\u7cca\u53ca\u524d\u666f\u4e0e\u80cc\u666f\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u5f71\u54cd\u5c0f\u75c5\u7076\u7684\u5212\u5206\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u548c\u57fa\u4e8e\u63d0\u793a\u7684Segment Anything Model\u53d8\u4f53\uff09\u8fc7\u5ea6\u4f9d\u8d56\u5c40\u90e8\u7ebf\u7d22\u6216\u7528\u6237\u63d0\u793a\uff0c\u7f3a\u4e4f\u96c6\u6210\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u96be\u4ee5\u6cdb\u5316\u81f3\u4f4e\u5bf9\u6bd4\u5ea6\u6216\u91cd\u53e0\u76ee\u6807\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faMedSeg-R\uff0c\u4e00\u79cd\u53d7\u4e34\u5e8a\u63a8\u7406\u542f\u53d1\u7684\u8f7b\u91cf\u7ea7\u53cc\u9636\u6bb5\u6846\u67b6\u3002\u5176\u8ba4\u77e5\u9636\u6bb5\u5c06\u533b\u5b66\u62a5\u544a\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u8bed\u4e49\u5148\u9a8c\uff08\u4f4d\u7f6e\u3001\u7eb9\u7406\u3001\u5f62\u72b6\uff09\uff0c\u5e76\u901a\u8fc7Transformer\u5757\u878d\u5408\u3002\u5728\u611f\u77e5\u9636\u6bb5\uff0c\u8fd9\u4e9b\u5148\u9a8c\u8c03\u5236SAM\u4e3b\u5e72\uff1a\u7a7a\u95f4\u6ce8\u610f\u529b\u7a81\u51fa\u53ef\u80fd\u7684\u75c5\u7076\u533a\u57df\uff0c\u52a8\u6001\u5377\u79ef\u8c03\u6574\u7279\u5f81\u6ee4\u6ce2\u5668\u4ee5\u9002\u5e94\u9884\u671f\u7eb9\u7406\uff0c\u53ef\u53d8\u5f62\u91c7\u6837\u4f18\u5316\u7a7a\u95f4\u652f\u6301\u3002\u901a\u8fc7\u65e9\u671f\u5d4c\u5165\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u6307\u5bfc\uff0cMedSeg-R\u89e3\u5f00\u4e86\u7c7b\u522b\u95f4\u6df7\u6dc6\u5e76\u653e\u5927\u4e86\u5c11\u6570\u7c7b\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5c0f\u75c5\u7076\u7684\u654f\u611f\u6027\u3002\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedSeg-R\u5728\u91cd\u53e0\u548c\u6a21\u7cca\u7ed3\u6784\u4e0a\u5b9e\u73b0\u4e86\u5927\u5e45Dice\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u4e0e\u57fa\u4e8eSAM\u7cfb\u7edf\u7684\u5373\u63d2\u5373\u7528\u517c\u5bb9\u6027\u3002"}}
{"id": "2506.17682", "pdf": "https://arxiv.org/pdf/2506.17682", "abs": "https://arxiv.org/abs/2506.17682", "authors": ["Zhijian Feng", "Wenhao Zheng", "Xuanji Xiao"], "title": "Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems", "categories": ["cs.IR", "cs.AI", "68T07", "H.3.3"], "comment": null, "summary": "In real-world recommendation systems, users would engage in variety\nscenarios, such as homepages, search pages, and related recommendation pages.\nEach of these scenarios would reflect different aspects users focus on.\nHowever, the user interests may be inconsistent in different scenarios, due to\ndifferences in decision-making processes and preference expression. This\nvariability complicates unified modeling, making multi-scenario learning a\nsignificant challenge. To address this, we propose a novel reinforcement\nlearning approach that models user preferences across scenarios by modeling\nuser interest evolution across multiple scenarios. Our method employs Double\nQ-learning to enhance next-item prediction accuracy and optimizes contrastive\nlearning loss using Q-value to make model performance better. Experimental\nresults demonstrate that our approach surpasses state-of-the-art methods in\nmulti-scenario recommendation tasks. Our work offers a fresh perspective on\nmulti-scenario modeling and highlights promising directions for future\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u5174\u8da3\u5728\u591a\u573a\u666f\u4e2d\u7684\u6f14\u5316\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u573a\u666f\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u5b9e\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u5728\u4e0d\u540c\u573a\u666f\uff08\u5982\u9996\u9875\u3001\u641c\u7d22\u9875\u7b49\uff09\u7684\u5174\u8da3\u53ef\u80fd\u4e0d\u4e00\u81f4\uff0c\u8fd9\u589e\u52a0\u4e86\u7edf\u4e00\u5efa\u6a21\u7684\u96be\u5ea6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u591a\u573a\u666f\u5b66\u4e60\u4e2d\u7528\u6237\u5174\u8da3\u6f14\u5316\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ccQ\u5b66\u4e60\u5efa\u6a21\u7528\u6237\u5174\u8da3\u5728\u591a\u573a\u666f\u4e2d\u7684\u6f14\u5316\uff0c\u5e76\u5229\u7528Q\u503c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u573a\u666f\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002", "paper_title_zh": "\u591a\u573a\u666f\u5b66\u4e60\u4e2d\u5f3a\u5316\u7528\u6237\u5174\u8da3\u6f14\u5316\u7684\u63a8\u8350\u7cfb\u7edf\u7814\u7a76", "abstract_zh": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u4f1a\u53c2\u4e0e\u591a\u79cd\u573a\u666f\uff0c\u5982\u9996\u9875\u3001\u641c\u7d22\u9875\u548c\u76f8\u5173\u63a8\u8350\u9875\u3002\u8fd9\u4e9b\u573a\u666f\u53cd\u6620\u4e86\u7528\u6237\u5173\u6ce8\u7684\u4e0d\u540c\u65b9\u9762\u3002\u7136\u800c\uff0c\u7531\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u548c\u504f\u597d\u8868\u8fbe\u7684\u5dee\u5f02\uff0c\u7528\u6237\u5174\u8da3\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u53ef\u80fd\u4e0d\u4e00\u81f4\uff0c\u8fd9\u589e\u52a0\u4e86\u7edf\u4e00\u5efa\u6a21\u7684\u590d\u6742\u6027\uff0c\u4f7f\u591a\u573a\u666f\u5b66\u4e60\u6210\u4e3a\u4e00\u9879\u91cd\u8981\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u5174\u8da3\u5728\u591a\u573a\u666f\u4e2d\u7684\u6f14\u5316\u6765\u5efa\u6a21\u7528\u6237\u504f\u597d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u53ccQ\u5b66\u4e60\u6765\u63d0\u9ad8\u4e0b\u4e00\u9879\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528Q\u503c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u573a\u666f\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u591a\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2506.18677", "pdf": "https://arxiv.org/pdf/2506.18677", "abs": "https://arxiv.org/abs/2506.18677", "authors": ["Adam Yang", "Nadula Kadawedduwa", "Tianfu Wang", "Maria Molina", "Christopher Metzler"], "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Accurately reconstructing the 3D structure of tornadoes is critically\nimportant for understanding and preparing for this highly destructive weather\nphenomenon. While modern 3D scene reconstruction techniques, such as 3D\nGaussian splatting (3DGS), could provide a valuable tool for reconstructing the\n3D structure of tornados, at present we are critically lacking a controlled\ntornado dataset with which to develop and validate these tools. In this work we\ncapture and release a novel multiview dataset of a small lab-based tornado. We\ndemonstrate one can effectively reconstruct and visualize the 3D structure of\nthis tornado using 3DGS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff083DGS\uff09\u91cd\u5efa\u5b9e\u9a8c\u5ba4\u5c0f\u578b\u9f99\u5377\u98ce\u4e09\u7ef4\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u6570\u636e\u96c6\u7528\u4e8e\u5f00\u53d1\u548c\u9a8c\u8bc1\u76f8\u5173\u5de5\u5177\u3002", "motivation": "\u51c6\u786e\u91cd\u5efa\u9f99\u5377\u98ce\u7684\u4e09\u7ef4\u7ed3\u6784\u5bf9\u7406\u89e3\u548c\u5e94\u5bf9\u8fd9\u4e00\u9ad8\u7834\u574f\u6027\u5929\u6c14\u73b0\u8c61\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u5f00\u53d1\u548c\u9a8c\u8bc13D\u91cd\u5efa\u6280\u672f\u7684\u53d7\u63a7\u9f99\u5377\u98ce\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u6355\u83b7\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u5ba4\u5c0f\u578b\u9f99\u5377\u98ce\u7684\u591a\u89c6\u89d2\u6570\u636e\u96c6\uff0c\u5e76\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff083DGS\uff09\u91cd\u5efa\u5176\u4e09\u7ef4\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c3DGS\u80fd\u591f\u6709\u6548\u91cd\u5efa\u5e76\u53ef\u89c6\u5316\u9f99\u5377\u98ce\u7684\u4e09\u7ef4\u7ed3\u6784\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u9f99\u5377\u98ce\u7684\u4e09\u7ef4\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u9f99\u5377\u98ce\u4e09\u7ef4\u91cd\u5efa", "abstract_zh": "\u51c6\u786e\u91cd\u5efa\u9f99\u5377\u98ce\u7684\u4e09\u7ef4\u7ed3\u6784\u5bf9\u4e8e\u7406\u89e3\u548c\u5e94\u5bf9\u8fd9\u4e00\u9ad8\u7834\u574f\u6027\u5929\u6c14\u73b0\u8c61\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u73b0\u4ee3\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u6280\u672f\uff08\u59823D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c3DGS\uff09\u53ef\u80fd\u4e3a\u9f99\u5377\u98ce\u7684\u4e09\u7ef4\u7ed3\u6784\u91cd\u5efa\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u4f46\u76ee\u524d\u6211\u4eec\u4e25\u91cd\u7f3a\u4e4f\u7528\u4e8e\u5f00\u53d1\u548c\u9a8c\u8bc1\u8fd9\u4e9b\u5de5\u5177\u7684\u53d7\u63a7\u9f99\u5377\u98ce\u6570\u636e\u96c6\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u6355\u83b7\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u5ba4\u5c0f\u578b\u9f99\u5377\u98ce\u7684\u591a\u89c6\u89d2\u6570\u636e\u96c6\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u5229\u75283DGS\u53ef\u4ee5\u6709\u6548\u91cd\u5efa\u5e76\u53ef\u89c6\u5316\u8be5\u9f99\u5377\u98ce\u7684\u4e09\u7ef4\u7ed3\u6784\u3002"}}
{"id": "2506.18678", "pdf": "https://arxiv.org/pdf/2506.18678", "abs": "https://arxiv.org/abs/2506.18678", "authors": ["Tianchen Deng", "Guole Shen", "Xun Chen", "Shenghai Yuan", "Hongming Shen", "Guohao Peng", "Zhenyu Wu", "Jingchuan Wang", "Lihua Xie", "Danwei Wang", "Hesheng Wang", "Weidong Chen"], "title": "MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Neural implicit scene representations have recently shown promising results\nin dense visual SLAM. However, existing implicit SLAM algorithms are\nconstrained to single-agent scenarios, and fall difficulties in large-scale\nscenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks\ncannot meet the constraints of communication bandwidth. To this end, we propose\nthe first distributed multi-agent collaborative neural SLAM framework with\nhybrid scene representation, distributed camera tracking, intra-to-inter loop\nclosure, and online distillation for multiple submap fusion. A novel\ntriplane-grid joint scene representation method is proposed to improve scene\nreconstruction. A novel intra-to-inter loop closure method is designed to\nachieve local (single-agent) and global (multi-agent) consistency. We also\ndesign a novel online distillation method to fuse the information of different\nsubmaps to achieve global consistency. Furthermore, to the best of our\nknowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that\nprovides both continuous-time trajectories groundtruth and high-accuracy 3D\nmeshes groundtruth. To this end, we propose the first real-world Dense slam\n(DES) dataset covering both single-agent and multi-agent scenarios, ranging\nfrom small rooms to large-scale outdoor scenes, with high-accuracy ground truth\nfor both 3D mesh and continuous-time camera trajectory. This dataset can\nadvance the development of the research in both SLAM, 3D reconstruction, and\nvisual foundation model. Experiments on various datasets demonstrate the\nsuperiority of the proposed method in both mapping, tracking, and\ncommunication. The dataset and code will open-source on\nhttps://github.com/dtc111111/mcnslam.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u795e\u7ecfSLAM\u6846\u67b6MCN-SLAM\uff0c\u7ed3\u5408\u6df7\u5408\u9690\u5f0f\u795e\u7ecf\u573a\u666f\u8868\u793a\u3001\u5206\u5e03\u5f0f\u76f8\u673a\u8ddf\u8e2a\u3001\u5c40\u90e8\u5230\u5168\u5c40\u95ed\u73af\u4ee5\u53ca\u5728\u7ebf\u84b8\u998f\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u667a\u80fd\u4f53SLAM\u5728\u5927\u573a\u666f\u548c\u957f\u5e8f\u5217\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u5bc6\u96c6SLAM\u6570\u636e\u96c6DES\u3002", "motivation": "\u73b0\u6709\u9690\u5f0fSLAM\u7b97\u6cd5\u4ec5\u9002\u7528\u4e8e\u5355\u667a\u80fd\u4f53\u573a\u666f\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u573a\u666f\u548c\u957f\u5e8f\u5217\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u800c\u57fa\u4e8eNeRF\u7684\u591a\u667a\u80fd\u4f53SLAM\u6846\u67b6\u65e0\u6cd5\u6ee1\u8db3\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u586b\u8865\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff08triplane-grid\u8054\u5408\u8868\u793a\uff09\uff0c\u8bbe\u8ba1\u4e86\u5c40\u90e8\u5230\u5168\u5c40\u95ed\u73af\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u4e00\u81f4\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u7ebf\u84b8\u998f\u6280\u672f\u7528\u4e8e\u5b50\u5730\u56fe\u878d\u5408\u3002\u6b64\u5916\uff0c\u521b\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u5bc6\u96c6SLAM\u6570\u636e\u96c6DES\uff0c\u6db5\u76d6\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6620\u5c04\u3001\u8ddf\u8e2a\u548c\u901a\u4fe1\u65b9\u9762\u5747\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14DES\u6570\u636e\u96c6\u4e3aSLAM\u30013D\u91cd\u5efa\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u6570\u636e\u652f\u6301\u3002", "conclusion": "MCN-SLAM\u6846\u67b6\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5cSLAM\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6DES\u6570\u636e\u96c6\u7684\u53d1\u5e03\u5c06\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002", "paper_title_zh": "MCN-SLAM\uff1a\u57fa\u4e8e\u6df7\u5408\u9690\u5f0f\u795e\u7ecf\u573a\u666f\u8868\u793a\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u795e\u7ecfSLAM", "abstract_zh": "\u795e\u7ecf\u9690\u5f0f\u573a\u666f\u8868\u793a\u5728\u5bc6\u96c6\u89c6\u89c9SLAM\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u9690\u5f0fSLAM\u7b97\u6cd5\u4ec5\u9002\u7528\u4e8e\u5355\u667a\u80fd\u4f53\u573a\u666f\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u573a\u666f\u548c\u957f\u5e8f\u5217\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u57fa\u4e8eNeRF\u7684\u591a\u667a\u80fd\u4f53SLAM\u6846\u67b6\u65e0\u6cd5\u6ee1\u8db3\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9996\u4e2a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u795e\u7ecfSLAM\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u573a\u666f\u8868\u793a\u3001\u5206\u5e03\u5f0f\u76f8\u673a\u8ddf\u8e2a\u3001\u5c40\u90e8\u5230\u5168\u5c40\u95ed\u73af\u4ee5\u53ca\u5728\u7ebf\u84b8\u998f\u6280\u672f\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684triplane-grid\u8054\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\u4ee5\u6539\u8fdb\u573a\u666f\u91cd\u5efa\uff0c\u8bbe\u8ba1\u4e86\u5c40\u90e8\u5230\u5168\u5c40\u95ed\u73af\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u4e00\u81f4\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u7ebf\u84b8\u998f\u6280\u672f\u7528\u4e8e\u5b50\u5730\u56fe\u878d\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u5bc6\u96c6SLAM\u6570\u636e\u96c6DES\uff0c\u6db5\u76d6\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u4ece\u5c0f\u623f\u95f4\u5230\u5927\u89c4\u6a21\u6237\u5916\u573a\u666f\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u76843D\u7f51\u683c\u548c\u8fde\u7eed\u65f6\u95f4\u76f8\u673a\u8f68\u8ff9\u771f\u503c\u3002\u8be5\u6570\u636e\u96c6\u5c06\u63a8\u52a8SLAM\u30013D\u91cd\u5efa\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6620\u5c04\u3001\u8ddf\u8e2a\u548c\u901a\u4fe1\u65b9\u9762\u5747\u8868\u73b0\u4f18\u8d8a\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5728https://github.com/dtc111111/mcnslam\u5f00\u6e90\u3002"}}
{"id": "2506.18679", "pdf": "https://arxiv.org/pdf/2506.18679", "abs": "https://arxiv.org/abs/2506.18679", "authors": ["Ruicheng Zhang", "Yu Sun", "Zeyu Zhang", "Jinai Li", "Xiaofan Liu", "Au Hoi Fan", "Haowei Guo", "Puxin Yan"], "title": "MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MARL-MambaContour, the first contour-based medical image\nsegmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our\napproach reframes segmentation as a multi-agent cooperation task focused on\ngenerate topologically consistent object-level contours, addressing the\nlimitations of traditional pixel-based methods which could lack topological\nconstraints and holistic structural awareness of anatomical regions. Each\ncontour point is modeled as an autonomous agent that iteratively adjusts its\nposition to align precisely with the target boundary, enabling adaptation to\nblurred edges and intricate morphologies common in medical images. This\niterative adjustment process is optimized by a contour-specific Soft\nActor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization\nAdjustment Mechanism (ERAM) which dynamically balance agent exploration with\ncontour smoothness. Furthermore, the framework incorporates a Mamba-based\npolicy network featuring a novel Bidirectional Cross-attention Hidden-state\nFusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion\nlimitations associated with long-range modeling in state space models, thereby\nfacilitating more accurate inter-agent information exchange and informed\ndecision-making. Extensive experiments on five diverse medical imaging datasets\ndemonstrate the state-of-the-art performance of MARL-MambaContour, highlighting\nits potential as an accurate and robust clinical application.", "AI": {"tldr": "MARL-MambaContour\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u8f6e\u5ed3\u70b9\u5efa\u6a21\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u4f18\u5316\u8f6e\u5ed3\u751f\u6210\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u50cf\u7d20\u7ea7\u65b9\u6cd5\u5728\u62d3\u6251\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u611f\u77e5\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff08\u5982\u50cf\u7d20\u7ea7\u65b9\u6cd5\uff09\u7f3a\u4e4f\u5bf9\u76ee\u6807\u8fb9\u754c\u7684\u62d3\u6251\u7ea6\u675f\u548c\u6574\u4f53\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6a21\u7cca\u8fb9\u7f18\u548c\u590d\u6742\u5f62\u6001\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u66f4\u7cbe\u786e\u4e14\u62d3\u6251\u4e00\u81f4\u7684\u8f6e\u5ed3\u3002", "method": "MARL-MambaContour\u5c06\u6bcf\u4e2a\u8f6e\u5ed3\u70b9\u5efa\u6a21\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574\u4f4d\u7f6e\u4ee5\u5bf9\u9f50\u76ee\u6807\u8fb9\u754c\u3002\u91c7\u7528\u6539\u8fdb\u7684Soft Actor-Critic\uff08SAC\uff09\u7b97\u6cd5\u4f18\u5316\u8c03\u6574\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u71b5\u6b63\u5219\u5316\u8c03\u6574\u673a\u5236\uff08ERAM\uff09\u5e73\u8861\u63a2\u7d22\u4e0e\u8f6e\u5ed3\u5e73\u6ed1\u6027\u3002\u6b64\u5916\uff0c\u6846\u67b6\u8fd8\u5305\u542b\u57fa\u4e8eMamba\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u91c7\u7528\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u9690\u85cf\u72b6\u6001\u878d\u5408\u673a\u5236\uff08BCHFM\uff09\u63d0\u5347\u957f\u7a0b\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMARL-MambaContour\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u9002\u5e94\u6a21\u7cca\u8fb9\u7f18\u548c\u590d\u6742\u5f62\u6001\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u4f5c\u4e3a\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u5de5\u5177\u7684\u6f5c\u529b\u3002", "conclusion": "MARL-MambaContour\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "MARL-MambaContour\uff1a\u91ca\u653e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4e3b\u52a8\u8f6e\u5ed3\u4f18\u5316\u7684\u6f5c\u529b", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86MARL-MambaContour\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u8f6e\u5ed3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c06\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u62d3\u6251\u4e00\u81f4\u7684\u76ee\u6807\u8f6e\u5ed3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u50cf\u7d20\u7ea7\u65b9\u6cd5\u5728\u62d3\u6251\u7ea6\u675f\u548c\u6574\u4f53\u7ed3\u6784\u611f\u77e5\u4e0a\u7684\u4e0d\u8db3\u3002\u6bcf\u4e2a\u8f6e\u5ed3\u70b9\u88ab\u5efa\u6a21\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574\u4f4d\u7f6e\u4ee5\u7cbe\u786e\u5bf9\u9f50\u76ee\u6807\u8fb9\u754c\uff0c\u4ece\u800c\u9002\u5e94\u533b\u5b66\u56fe\u50cf\u4e2d\u5e38\u89c1\u7684\u6a21\u7cca\u8fb9\u7f18\u548c\u590d\u6742\u5f62\u6001\u3002\u8fd9\u4e00\u8c03\u6574\u8fc7\u7a0b\u7531\u6539\u8fdb\u7684Soft Actor-Critic\uff08SAC\uff09\u7b97\u6cd5\u4f18\u5316\uff0c\u5e76\u8fdb\u4e00\u6b65\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u8c03\u6574\u673a\u5236\uff08ERAM\uff09\u52a8\u6001\u5e73\u8861\u667a\u80fd\u4f53\u63a2\u7d22\u4e0e\u8f6e\u5ed3\u5e73\u6ed1\u6027\u3002\u6b64\u5916\uff0c\u6846\u67b6\u8fd8\u6574\u5408\u4e86\u57fa\u4e8eMamba\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u91c7\u7528\u65b0\u9896\u7684\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u9690\u85cf\u72b6\u6001\u878d\u5408\u673a\u5236\uff08BCHFM\uff09\uff0c\u7f13\u89e3\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u957f\u7a0b\u5efa\u6a21\u53ef\u80fd\u5f15\u53d1\u7684\u8bb0\u5fc6\u6df7\u6dc6\u95ee\u9898\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u51c6\u786e\u7684\u667a\u80fd\u4f53\u95f4\u4fe1\u606f\u4ea4\u6362\u548c\u51b3\u7b56\u5236\u5b9a\u3002\u5728\u4e94\u4e2a\u591a\u6837\u5316\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMARL-MambaContour\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.18682", "pdf": "https://arxiv.org/pdf/2506.18682", "abs": "https://arxiv.org/abs/2506.18682", "authors": ["Imad Ali Shah", "Jiarong Li", "Tim Brophy", "Martin Glavin", "Edward Jones", "Enda Ward", "Brian Deegan"], "title": "Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in autonomous driving (AD) have highlighted the potential of\nHyperspectral Imaging (HSI) for enhanced environmental perception, particularly\nin challenging weather and lighting conditions. However, efficiently processing\nits high-dimensional spectral data remains a significant challenge. This paper\nintroduces a Multi-scale Spectral Attention Module (MSAM) that enhances\nspectral feature extraction through three parallel 1D convolutions with varying\nkernel sizes between 1 to 11, coupled with an adaptive feature aggregation\nmechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our\nproposed UNet-MSAM achieves significant improvements in semantic segmentation\nperformance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and\nHyperspectral City v2. Our comprehensive experiments demonstrate that with\nminimal computational overhead (on average 0.02% in parameters and 0.82%\nGFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average\nimprovements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.\nThrough extensive ablation studies, we have established that multi-scale kernel\ncombinations perform better than single-scale configurations. These findings\ndemonstrate the potential of HSI processing for AD and provide valuable\ninsights into designing robust, multi-scale spectral feature extractors for\nreal-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u5149\u8c31\u6ce8\u610f\u529b\u6a21\u5757\uff08MSAM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u6838\u5927\u5c0f\u76841D\u5377\u79ef\u548c\u81ea\u9002\u5e94\u7279\u5f81\u805a\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u4e2d\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5728\u590d\u6742\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u5177\u6709\u73af\u5883\u611f\u77e5\u6f5c\u529b\uff0c\u4f46\u5176\u9ad8\u7ef4\u5149\u8c31\u6570\u636e\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u5149\u8c31\u6ce8\u610f\u529b\u6a21\u5757\uff08MSAM\uff09\uff0c\u5305\u542b\u4e09\u4e2a\u5e76\u884c1D\u5377\u79ef\uff08\u6838\u5927\u5c0f\u4e3a1\u81f311\uff09\u548c\u81ea\u9002\u5e94\u7279\u5f81\u805a\u5408\u673a\u5236\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230UNet\u7684\u8df3\u8dc3\u8fde\u63a5\u4e2d\uff08UNet-MSAM\uff09\u3002", "result": "\u5728HyKo-VIS v2\u3001HSI-Drive v2\u548cHyperspectral City v2\u6570\u636e\u96c6\u4e0a\uff0cUNet-MSAM\u5e73\u5747\u63d0\u53473.61%\u7684mIoU\u548c3.80%\u7684mF1\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u589e\u52a00.02%\u53c2\u6570\u548c0.82%GFLOPS\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u6838\u7ec4\u5408\u4f18\u4e8e\u5355\u5c3a\u5ea6\u914d\u7f6e\uff0c\u8bc1\u660e\u4e86HSI\u5728AD\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u7684\u591a\u5c3a\u5ea6\u5149\u8c31\u7279\u5f81\u63d0\u53d6\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u5c3a\u5ea6\u5149\u8c31\u6ce8\u610f\u529b\u6a21\u5757\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u9ad8\u5149\u8c31\u5206\u5272", "abstract_zh": "\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u51f8\u663e\u4e86\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5728\u590d\u6742\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u589e\u5f3a\u73af\u5883\u611f\u77e5\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u9ad8\u7ef4\u5149\u8c31\u6570\u636e\u7684\u9ad8\u6548\u5904\u7406\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u5149\u8c31\u6ce8\u610f\u529b\u6a21\u5757\uff08MSAM\uff09\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5927\u5c0f\u4ece1\u523011\u7684\u5e76\u884c1D\u5377\u79ef\u7ed3\u5408\u81ea\u9002\u5e94\u7279\u5f81\u805a\u5408\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u5149\u8c31\u7279\u5f81\u63d0\u53d6\u3002\u5c06MSAM\u96c6\u6210\u5230UNet\u7684\u8df3\u8dc3\u8fde\u63a5\uff08UNet-SC\uff09\u4e2d\uff0c\u63d0\u51fa\u7684UNet-MSAM\u5728\u591a\u4e2aHSI\u6570\u636e\u96c6\uff08HyKo-VIS v2\u3001HSI-Drive v2\u548cHyperspectral City v2\uff09\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6781\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff08\u5e73\u5747\u589e\u52a00.02%\u53c2\u6570\u548c0.82%GFLOPS\uff09\u4e0b\uff0cUNet-MSAM\u59cb\u7ec8\u4f18\u4e8eUNet-SC\uff0c\u5e73\u5747\u63d0\u53473.61%\u7684mIoU\u548c3.80%\u7684mF1\u3002\u901a\u8fc7\u5927\u91cf\u6d88\u878d\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u5b9e\u591a\u5c3a\u5ea6\u6838\u7ec4\u5408\u4f18\u4e8e\u5355\u5c3a\u5ea6\u914d\u7f6e\u3002\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86HSI\u5904\u7406\u5728AD\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u9c81\u68d2\u591a\u5c3a\u5ea6\u5149\u8c31\u7279\u5f81\u63d0\u53d6\u5668\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2506.17719", "pdf": "https://arxiv.org/pdf/2506.17719", "abs": "https://arxiv.org/abs/2506.17719", "authors": ["Timofei Miryashkin", "Olga Klimanova", "Alexander Shapeev"], "title": "Resolving the Ti-V Phase Diagram Discrepancy with First-Principles Calculations and Bayesian Learning", "categories": ["cond-mat.mtrl-sci", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Conflicting experiments disagree on whether the titanium-vanadium (Ti-V)\nbinary alloy exhibits a body-centred cubic (BCC) miscibility gap or remains\ncompletely soluble. A leading hypothesis attributes the miscibility gap to\noxygen contamination during alloy preparation. To resolve this controversy, we\nuse an ab initio + machine-learning workflow that couples an actively-trained\nMoment Tensor Potential to Bayesian thermodynamic inference. Using this\nworkflow, we obtain Ti-V binary system across the entire composition range,\ntogether with confidence intervals in the thermodynamic limit. The resulting\ndiagram reproduces all experimental features, demonstrating the robustness of\nour approach, and clearly favors the variant with a BCC miscibility gap\nterminating at T = 980 K and c = 0.67. Because oxygen was excluded from\nsimulations, the gap cannot be attributed to impurity effects, contradicting\nrecent CALPHAD reassessments.", "AI": {"tldr": "\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u8d1d\u53f6\u65af\u5b66\u4e60\u89e3\u51b3\u4e86\u949b-\u9492\uff08Ti-V\uff09\u4e8c\u5143\u5408\u91d1\u76f8\u56fe\u7684\u4e89\u8bae\uff0c\u786e\u8ba4\u5176\u5b58\u5728\u4f53\u5fc3\u7acb\u65b9\uff08BCC\uff09\u6df7\u6eb6\u95f4\u9699\uff0c\u4e14\u8be5\u95f4\u9699\u4e0e\u6c27\u6c61\u67d3\u65e0\u5173\u3002", "motivation": "\u949b-\u9492\uff08Ti-V\uff09\u4e8c\u5143\u5408\u91d1\u7684\u76f8\u56fe\u5b58\u5728\u4e89\u8bae\uff0c\u5b9e\u9a8c\u6570\u636e\u5bf9\u662f\u5426\u5b58\u5728\u4f53\u5fc3\u7acb\u65b9\uff08BCC\uff09\u6df7\u6eb6\u95f4\u9699\u6216\u5b8c\u5168\u53ef\u6eb6\u6027\u5b58\u5728\u5206\u6b67\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u4e89\u8bae\uff0c\u5e76\u9a8c\u8bc1\u6df7\u6eb6\u95f4\u9699\u662f\u5426\u7531\u6c27\u6c61\u67d3\u5f15\u8d77\u3002", "method": "\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e3b\u52a8\u8bad\u7ec3\u7684\u77e9\u5f20\u91cf\u52bf\uff08Moment Tensor Potential\uff09\u4e0e\u8d1d\u53f6\u65af\u70ed\u529b\u5b66\u63a8\u65ad\uff0c\u751f\u6210\u949b-\u9492\u7cfb\u7edf\u7684\u5b8c\u6574\u76f8\u56fe\uff0c\u5e76\u8ba1\u7b97\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u949b-\u9492\u7cfb\u7edf\u5b58\u5728BCC\u6df7\u6eb6\u95f4\u9699\uff0c\u7ec8\u6b62\u6e29\u5ea6\u4e3a980 K\uff0c\u6210\u5206\u4e3ac = 0.67\u3002\u7531\u4e8e\u6a21\u62df\u4e2d\u6392\u9664\u4e86\u6c27\u7684\u5f71\u54cd\uff0c\u6df7\u6eb6\u95f4\u9699\u4e0d\u80fd\u5f52\u56e0\u4e8e\u6742\u8d28\u6548\u5e94\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u786e\u8ba4\u4e86\u949b-\u9492\u4e8c\u5143\u5408\u91d1\u76f8\u56fe\u4e2dBCC\u6df7\u6eb6\u95f4\u9699\u7684\u5b58\u5728\uff0c\u5e76\u6392\u9664\u4e86\u6c27\u6c61\u67d3\u7684\u5f71\u54cd\uff0c\u4e3a\u76f8\u5173\u4e89\u8bae\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u8d1d\u53f6\u65af\u5b66\u4e60\u89e3\u51b3\u949b-\u9492\u76f8\u56fe\u4e89\u8bae", "abstract_zh": "\u949b-\u9492\uff08Ti-V\uff09\u4e8c\u5143\u5408\u91d1\u7684\u76f8\u56fe\u5b58\u5728\u4e89\u8bae\uff0c\u5b9e\u9a8c\u6570\u636e\u5bf9\u662f\u5426\u5b58\u5728\u4f53\u5fc3\u7acb\u65b9\uff08BCC\uff09\u6df7\u6eb6\u95f4\u9699\u6216\u5b8c\u5168\u53ef\u6eb6\u6027\u5b58\u5728\u5206\u6b67\u3002\u672c\u7814\u7a76\u91c7\u7528\u4ece\u5934\u7b97+\u673a\u5668\u5b66\u4e60\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e3b\u52a8\u8bad\u7ec3\u7684\u77e9\u5f20\u91cf\u52bf\u4e0e\u8d1d\u53f6\u65af\u70ed\u529b\u5b66\u63a8\u65ad\uff0c\u751f\u6210\u4e86\u949b-\u9492\u4e8c\u5143\u7cfb\u7edf\u5728\u6574\u4e2a\u6210\u5206\u8303\u56f4\u5185\u7684\u76f8\u56fe\uff0c\u5e76\u8ba1\u7b97\u4e86\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u7f6e\u4fe1\u533a\u95f4\u3002\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u56fe\u91cd\u73b0\u4e86\u6240\u6709\u5b9e\u9a8c\u7279\u5f81\uff0c\u652f\u6301\u5b58\u5728\u7ec8\u6b62\u4e8eT = 980 K\u548cc = 0.67\u7684BCC\u6df7\u6eb6\u95f4\u9699\u3002\u7531\u4e8e\u6a21\u62df\u4e2d\u6392\u9664\u4e86\u6c27\u7684\u5f71\u54cd\uff0c\u6df7\u6eb6\u95f4\u9699\u4e0d\u80fd\u5f52\u56e0\u4e8e\u6742\u8d28\u6548\u5e94\uff0c\u8fd9\u4e0e\u6700\u8fd1\u7684CALPHAD\u91cd\u65b0\u8bc4\u4f30\u7ed3\u679c\u76f8\u77db\u76fe\u3002"}}
{"id": "2506.18683", "pdf": "https://arxiv.org/pdf/2506.18683", "abs": "https://arxiv.org/abs/2506.18683", "authors": ["Youcef Sklab", "Hanane Ariouat", "Eric Chenin", "Edi Prifti", "Jean-Daniel Zucker"], "title": "SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 9 figures, 14 tables", "summary": "We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image\nclassification architecture that integrates 3D point cloud representations\ninferred directly from RGB images. Our key contribution lies in a\npixel-to-point transformation that converts 2D object masks into 3D point\nclouds, enabling the fusion of texture-based and geometric features for\nenhanced classification performance. SIM-Net is particularly well-suited for\nthe classification of digitized herbarium specimens (a task made challenging by\nheterogeneous backgrounds), non-plant elements, and occlusions that compromise\nconventional image-based models. To address these issues, SIM-Net employs a\nsegmentation-based preprocessing step to extract object masks prior to 3D point\ncloud generation. The architecture comprises a CNN encoder for 2D image\nfeatures and a PointNet-based encoder for geometric features, which are fused\ninto a unified latent space. Experimental evaluations on herbarium datasets\ndemonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of\nup to 9.9% in accuracy and 12.3% in F-score. It also surpasses several\ntransformer-based state-of-the-art architectures, highlighting the benefits of\nincorporating 3D structural reasoning into 2D image classification tasks.", "AI": {"tldr": "SIM-Net\u662f\u4e00\u79cd\u65b0\u578b2D\u56fe\u50cf\u5206\u7c7b\u67b6\u6784\uff0c\u901a\u8fc7\u4eceRGB\u56fe\u50cf\u63a8\u65ad3D\u70b9\u4e91\uff0c\u878d\u5408\u7eb9\u7406\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u80cc\u666f\u548c\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf2D\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u80cc\u666f\u3001\u975e\u76ee\u6807\u5143\u7d20\u548c\u906e\u6321\u65f6\u6027\u80fd\u53d7\u9650\u3002SIM-Net\u65e8\u5728\u901a\u8fc7\u878d\u54083D\u51e0\u4f55\u7279\u5f81\uff0c\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u5b57\u5316\u690d\u7269\u6807\u672c\u7b49\u6311\u6218\u6027\u4efb\u52a1\u3002", "method": "SIM-Net\u901a\u8fc7\u50cf\u7d20\u5230\u70b9\u7684\u8f6c\u6362\u5c062D\u7269\u4f53\u63a9\u7801\u8f6c\u5316\u4e3a3D\u70b9\u4e91\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u76842D\u56fe\u50cf\u7279\u5f81\u548cPointNet\u63d0\u53d6\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u690d\u7269\u6807\u672c\u6570\u636e\u96c6\u4e0a\uff0cSIM-Net\u6bd4ResNet101\u51c6\u786e\u7387\u63d0\u53479.9%\uff0cF-score\u63d0\u534712.3%\uff0c\u5e76\u4f18\u4e8e\u591a\u79cd\u57fa\u4e8eTransformer\u7684\u5148\u8fdb\u67b6\u6784\u3002", "conclusion": "SIM-Net\u901a\u8fc7\u878d\u54083D\u7ed3\u6784\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e862D\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "paper_title_zh": "SIM-Net\uff1a\u4e00\u79cd\u5229\u7528RGB\u56fe\u50cf\u63a8\u65ad\u76843D\u7269\u4f53\u5f62\u72b6\u70b9\u4e91\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u76842D\u5206\u7c7b\u7f51\u7edc", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u5f62\u72b6-\u56fe\u50cf\u591a\u6a21\u6001\u7f51\u7edc\uff08SIM-Net\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u76842D\u56fe\u50cf\u5206\u7c7b\u67b6\u6784\uff0c\u901a\u8fc7\u76f4\u63a5\u4eceRGB\u56fe\u50cf\u63a8\u65ad3D\u70b9\u4e91\u8868\u793a\u6765\u878d\u5408\u7eb9\u7406\u548c\u51e0\u4f55\u7279\u5f81\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5c062D\u7269\u4f53\u63a9\u7801\u8f6c\u6362\u4e3a3D\u70b9\u4e91\u7684\u50cf\u7d20\u5230\u70b9\u53d8\u6362\uff0c\u4ece\u800c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002SIM-Net\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u80cc\u666f\u590d\u6742\u3001\u975e\u76ee\u6807\u5143\u7d20\u548c\u906e\u6321\u7684\u6570\u5b57\u5316\u690d\u7269\u6807\u672c\u5206\u7c7b\u4efb\u52a1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0cSIM-Net\u91c7\u7528\u57fa\u4e8e\u5206\u5272\u7684\u9884\u5904\u7406\u6b65\u9aa4\u63d0\u53d6\u7269\u4f53\u63a9\u7801\uff0c\u518d\u751f\u62103D\u70b9\u4e91\u3002\u67b6\u6784\u5305\u62ec\u7528\u4e8e2D\u56fe\u50cf\u7279\u5f81\u7684CNN\u7f16\u7801\u5668\u548c\u7528\u4e8e\u51e0\u4f55\u7279\u5f81\u7684\u57fa\u4e8ePointNet\u7684\u7f16\u7801\u5668\uff0c\u4e24\u8005\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u878d\u5408\u3002\u5728\u690d\u7269\u6807\u672c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSIM-Net\u59cb\u7ec8\u4f18\u4e8eResNet101\uff0c\u51c6\u786e\u7387\u548cF-score\u5206\u522b\u63d0\u53479.9%\u548c12.3%\uff0c\u5e76\u8d85\u8d8a\u591a\u79cd\u57fa\u4e8eTransformer\u7684\u5148\u8fdb\u67b6\u6784\uff0c\u51f8\u663e\u4e86\u5c063D\u7ed3\u6784\u63a8\u7406\u878d\u51652D\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.18701", "pdf": "https://arxiv.org/pdf/2506.18701", "abs": "https://arxiv.org/abs/2506.18701", "authors": ["Yifan Zhang", "Chunli Peng", "Boyang Wang", "Puyi Wang", "Qingcheng Zhu", "Fei Kang", "Biao Jiang", "Zedong Gao", "Eric Li", "Yang Liu", "Yahui Zhou"], "title": "Matrix-Game: Interactive World Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report", "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.", "AI": {"tldr": "Matrix-Game\u662f\u4e00\u79cd\u4ea4\u4e92\u5f0f\u4e16\u754c\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u53ef\u63a7\u6e38\u620f\u4e16\u754c\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6e38\u620f\u4e16\u754c\u751f\u6210\u6a21\u578b\u5728\u53ef\u63a7\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cMatrix-Game\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u548c\u53ef\u63a7\u751f\u6210\u8303\u5f0f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u9884\u8bad\u7ec3\u7406\u89e3\u73af\u5883\uff0c\u518d\u901a\u8fc7\u52a8\u4f5c\u6807\u7b7e\u8bad\u7ec3\u751f\u6210\u4ea4\u4e92\u89c6\u9891\u3002\u6a21\u578b\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u3001\u8fd0\u52a8\u4e0a\u4e0b\u6587\u548c\u7528\u6237\u52a8\u4f5c\u8fdb\u884c\u53ef\u63a7\u751f\u6210\u3002", "result": "Matrix-Game\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u3001\u52a8\u4f5c\u53ef\u63a7\u6027\u548c\u7269\u7406\u89c4\u5219\u7406\u89e3\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982Oasis\u548cMineWorld\uff09\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "Matrix-Game\u4e3a\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5230\u4e16\u754c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u6a21\u578b\u548c\u8bc4\u6d4b\u57fa\u51c6\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "paper_title_zh": "Matrix-Game\uff1a\u4ea4\u4e92\u5f0f\u4e16\u754c\u57fa\u7840\u6a21\u578b", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86Matrix-Game\uff0c\u4e00\u79cd\u7528\u4e8e\u53ef\u63a7\u6e38\u620f\u4e16\u754c\u751f\u6210\u7684\u4ea4\u4e92\u5f0f\u4e16\u754c\u57fa\u7840\u6a21\u578b\u3002Matrix-Game\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u9996\u5148\u8fdb\u884c\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u9884\u8bad\u7ec3\u4ee5\u7406\u89e3\u73af\u5883\uff0c\u968f\u540e\u901a\u8fc7\u52a8\u4f5c\u6807\u7b7e\u8bad\u7ec3\u751f\u6210\u4ea4\u4e92\u89c6\u9891\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86Matrix-Game-MC\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc72,700\u5c0f\u65f6\u7684\u65e0\u6807\u7b7e\u6e38\u620f\u89c6\u9891\u7247\u6bb5\u548c1,000\u5c0f\u65f6\u7684\u9ad8\u8d28\u91cf\u6807\u7b7e\u7247\u6bb5\uff08\u9644\u5e26\u7ec6\u7c92\u5ea6\u952e\u76d8\u548c\u9f20\u6807\u52a8\u4f5c\u6807\u6ce8\uff09\u3002\u6a21\u578b\u91c7\u7528\u53ef\u63a7\u7684\u56fe\u50cf\u5230\u4e16\u754c\u751f\u6210\u8303\u5f0f\uff0c\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u3001\u8fd0\u52a8\u4e0a\u4e0b\u6587\u548c\u7528\u6237\u52a8\u4f5c\u8fdb\u884c\u751f\u6210\u3002\u62e5\u6709\u8d85\u8fc7170\u4ebf\u53c2\u6570\u7684Matrix-Game\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u89d2\u8272\u52a8\u4f5c\u548c\u76f8\u673a\u79fb\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002\u4e3a\u8bc4\u4f30\u6027\u80fd\uff0c\u6211\u4eec\u5f00\u53d1\u4e86GameWorld Score\u8bc4\u6d4b\u57fa\u51c6\uff0c\u8861\u91cfMinecraft\u4e16\u754c\u751f\u6210\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u65f6\u95f4\u8d28\u91cf\u3001\u52a8\u4f5c\u53ef\u63a7\u6027\u548c\u7269\u7406\u89c4\u5219\u7406\u89e3\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMatrix-Game\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff08\u5305\u62ecOasis\u548cMineWorld\uff09\uff0c\u5c24\u5176\u5728\u53ef\u63a7\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u53cc\u76f2\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86Matrix-Game\u7684\u4f18\u8d8a\u6027\uff0c\u7a81\u663e\u5176\u5728\u591a\u6837\u5316\u6e38\u620f\u573a\u666f\u4e2d\u751f\u6210\u611f\u77e5\u771f\u5b9e\u4e14\u7cbe\u786e\u53ef\u63a7\u89c6\u9891\u7684\u80fd\u529b\u3002\u4e3a\u63a8\u52a8\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5230\u4e16\u754c\u751f\u6210\u7684\u7814\u7a76\uff0c\u6211\u4eec\u5c06\u5f00\u6e90Matrix-Game\u6a21\u578b\u6743\u91cd\u548cGameWorld Score\u8bc4\u6d4b\u57fa\u51c6\u3002"}}
{"id": "2506.18721", "pdf": "https://arxiv.org/pdf/2506.18721", "abs": "https://arxiv.org/abs/2506.18721", "authors": ["Dustin Aganian", "Erik Franze", "Markus Eisenbach", "Horst-Michael Gross"], "title": "Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "IEEE International Joint Conference on Neural Networks (IJCNN) 2025", "summary": "Effective human action recognition is widely used for cobots in Industry 4.0\nto assist in assembly tasks. However, conventional skeleton-based methods often\nlose keypoint semantics, limiting their effectiveness in complex interactions.\nIn this work, we introduce a novel approach to skeleton-based action\nrecognition that enriches input representations by leveraging word embeddings\nto encode semantic information. Our method replaces one-hot encodings with\nsemantic volumes, enabling the model to capture meaningful relationships\nbetween joints and objects. Through extensive experiments on multiple assembly\ndatasets, we demonstrate that our approach significantly improves\nclassification performance, and enhances generalization capabilities by\nsimultaneously supporting different skeleton types and object classes. Our\nfindings highlight the potential of incorporating semantic information to\nenhance skeleton-based action recognition in dynamic and diverse environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bcd\u5d4c\u5165\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u5de5\u4e1a4.0\u4e2d\uff0c\u4eba\u673a\u534f\u4f5c\u9700\u8981\u9ad8\u6548\u7684\u52a8\u4f5c\u8bc6\u522b\u6280\u672f\u3002\u4f20\u7edf\u9aa8\u67b6\u65b9\u6cd5\u5e38\u5ffd\u7565\u5173\u952e\u70b9\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u590d\u6742\u4ea4\u4e92\u4e2d\u7684\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u8bed\u4e49\u4f53\u79ef\u66ff\u4ee3\u4f20\u7edf\u72ec\u70ed\u7f16\u7801\uff0c\u901a\u8fc7\u8bcd\u5d4c\u5165\u7f16\u7801\u8bed\u4e49\u4fe1\u606f\uff0c\u6355\u6349\u5173\u8282\u4e0e\u7269\u4f53\u95f4\u7684\u6709\u610f\u4e49\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u88c5\u914d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u9aa8\u67b6\u7c7b\u578b\u548c\u7269\u4f53\u7c7b\u522b\uff0c\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u5728\u52a8\u6001\u591a\u6837\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u57fa\u4e8e\u8bcd\u5d4c\u5165\u8bed\u4e49\u4fe1\u606f\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5", "abstract_zh": "\u9ad8\u6548\u7684\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\u5728\u5de5\u4e1a4.0\u4e2d\u5e7f\u6cdb\u5e94\u7528\u4e8e\u534f\u4f5c\u673a\u5668\u4eba\u4ee5\u8f85\u52a9\u88c5\u914d\u4efb\u52a1\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u9aa8\u67b6\u7684\u65b9\u6cd5\u5e38\u4e22\u5931\u5173\u952e\u70b9\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4ea4\u4e92\u4e2d\u7684\u6548\u679c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bcd\u5d4c\u5165\u7f16\u7801\u8bed\u4e49\u4fe1\u606f\u6765\u4e30\u5bcc\u8f93\u5165\u8868\u793a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7528\u8bed\u4e49\u4f53\u79ef\u66ff\u4ee3\u72ec\u70ed\u7f16\u7801\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u5173\u8282\u4e0e\u7269\u4f53\u4e4b\u95f4\u7684\u6709\u610f\u4e49\u5173\u7cfb\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u88c5\u914d\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u540c\u65f6\u652f\u6301\u4e0d\u540c\u9aa8\u67b6\u7c7b\u578b\u548c\u7269\u4f53\u7c7b\u522b\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u53d1\u73b0\u51f8\u663e\u4e86\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\u4ee5\u589e\u5f3a\u52a8\u6001\u591a\u6837\u73af\u5883\u4e2d\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17756", "pdf": "https://arxiv.org/pdf/2506.17756", "abs": "https://arxiv.org/abs/2506.17756", "authors": ["Hosung Lee", "Byeongoh Hwang", "Dasan Kim", "Myungjoo Kang"], "title": "Residual Connection-Enhanced ConvLSTM for Lithium Dendrite Growth Prediction", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": "14pages, 6figures, accepted to Journal of The Electrochemical Society", "summary": "The growth of lithium dendrites significantly impacts the performance and\nsafety of rechargeable batteries, leading to short circuits and capacity\ndegradation. This study proposes a Residual Connection-Enhanced ConvLSTM model\nto predict dendrite growth patterns with improved accuracy and computational\nefficiency. By integrating residual connections into ConvLSTM, the model\nmitigates the vanishing gradient problem, enhances feature retention across\nlayers, and effectively captures both localized dendrite growth dynamics and\nmacroscopic battery behavior. The dataset was generated using a phase-field\nmodel, simulating dendrite evolution under varying conditions. Experimental\nresults show that the proposed model achieves up to 7% higher accuracy and\nsignificantly reduces mean squared error (MSE) compared to conventional\nConvLSTM across different voltage conditions (0.1V, 0.3V, 0.5V). This\nhighlights the effectiveness of residual connections in deep spatiotemporal\nnetworks for electrochemical system modeling. The proposed approach offers a\nrobust tool for battery diagnostics, potentially aiding in real-time monitoring\nand optimization of lithium battery performance. Future research can extend\nthis framework to other battery chemistries and integrate it with real-world\nexperimental data for further validation", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7684ConvLSTM\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u9502\u679d\u6676\u751f\u957f\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u9502\u679d\u6676\u7684\u751f\u957f\u4e25\u91cd\u5f71\u54cd\u53ef\u5145\u7535\u7535\u6c60\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u77ed\u8def\u548c\u5bb9\u91cf\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6a21\u578b\u6765\u9884\u6d4b\u679d\u6676\u751f\u957f\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u5c06\u6b8b\u5dee\u8fde\u63a5\u96c6\u6210\u5230ConvLSTM\u4e2d\uff0c\u6a21\u578b\u7f13\u89e3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u8de8\u5c42\u7279\u5f81\u4fdd\u7559\uff0c\u5e76\u6709\u6548\u6355\u6349\u4e86\u5c40\u90e8\u679d\u6676\u751f\u957f\u52a8\u6001\u548c\u5b8f\u89c2\u7535\u6c60\u884c\u4e3a\u3002\u6570\u636e\u96c6\u901a\u8fc7\u76f8\u573a\u6a21\u578b\u751f\u6210\uff0c\u6a21\u62df\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u679d\u6676\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684ConvLSTM\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u7535\u538b\u6761\u4ef6\u4e0b\uff080.1V\u30010.3V\u30010.5V\uff09\u7684\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u4e867%\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u5728\u6df1\u5ea6\u65f6\u7a7a\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7535\u5316\u5b66\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002\u8be5\u65b9\u6cd5\u6709\u671b\u7528\u4e8e\u7535\u6c60\u8bca\u65ad\u548c\u5b9e\u65f6\u6027\u80fd\u4f18\u5316\u3002\u672a\u6765\u7814\u7a76\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u7535\u6c60\u5316\u5b66\u4f53\u7cfb\uff0c\u5e76\u7ed3\u5408\u5b9e\u9645\u5b9e\u9a8c\u6570\u636e\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "paper_title_zh": "\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7684ConvLSTM\u7528\u4e8e\u9502\u679d\u6676\u751f\u957f\u9884\u6d4b", "abstract_zh": "\u9502\u679d\u6676\u7684\u751f\u957f\u663e\u8457\u5f71\u54cd\u53ef\u5145\u7535\u7535\u6c60\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u77ed\u8def\u548c\u5bb9\u91cf\u4e0b\u964d\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7684ConvLSTM\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u679d\u6676\u751f\u957f\u6a21\u5f0f\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u5c06\u6b8b\u5dee\u8fde\u63a5\u96c6\u6210\u5230ConvLSTM\u4e2d\uff0c\u6a21\u578b\u7f13\u89e3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u8de8\u5c42\u7279\u5f81\u4fdd\u7559\uff0c\u5e76\u6709\u6548\u6355\u6349\u4e86\u5c40\u90e8\u679d\u6676\u751f\u957f\u52a8\u6001\u548c\u5b8f\u89c2\u7535\u6c60\u884c\u4e3a\u3002\u6570\u636e\u96c6\u901a\u8fc7\u76f8\u573a\u6a21\u578b\u751f\u6210\uff0c\u6a21\u62df\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u679d\u6676\u6f14\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684ConvLSTM\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u7535\u538b\u6761\u4ef6\u4e0b\uff080.1V\u30010.3V\u30010.5V\uff09\u7684\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u4e867%\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3002\u8fd9\u51f8\u663e\u4e86\u6b8b\u5dee\u8fde\u63a5\u5728\u6df1\u5ea6\u65f6\u7a7a\u7f51\u7edc\u4e2d\u5bf9\u7535\u5316\u5b66\u7cfb\u7edf\u5efa\u6a21\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u7535\u6c60\u8bca\u65ad\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u6709\u671b\u7528\u4e8e\u5b9e\u65f6\u76d1\u6d4b\u548c\u9502\u7535\u6c60\u6027\u80fd\u4f18\u5316\u3002\u672a\u6765\u7814\u7a76\u53ef\u5c06\u6b64\u6846\u67b6\u6269\u5c55\u81f3\u5176\u4ed6\u7535\u6c60\u5316\u5b66\u4f53\u7cfb\uff0c\u5e76\u7ed3\u5408\u5b9e\u9645\u5b9e\u9a8c\u6570\u636e\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2506.18731", "pdf": "https://arxiv.org/pdf/2506.18731", "abs": "https://arxiv.org/abs/2506.18731", "authors": ["Aman Bhatta", "Michael C. King", "Kevin W. Bowyer"], "title": "Deep CNN Face Matchers Inherently Support Revocable Biometric Templates", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "One common critique of biometric authentication is that if an individual's\nbiometric is compromised, then the individual has no recourse. The concept of\nrevocable biometrics was developed to address this concern. A biometric scheme\nis revocable if an individual can have their current enrollment in the scheme\nrevoked, so that the compromised biometric template becomes worthless, and the\nindividual can re-enroll with a new template that has similar recognition\npower. We show that modern deep CNN face matchers inherently allow for a robust\nrevocable biometric scheme. For a given state-of-the-art deep CNN backbone and\ntraining set, it is possible to generate an unlimited number of distinct face\nmatcher models that have both (1) equivalent recognition power, and (2)\nstrongly incompatible biometric templates. The equivalent recognition power\nextends to the point of generating impostor and genuine distributions that have\nthe same shape and placement on the similarity dimension, meaning that the\nmodels can share a similarity threshold for a 1-in-10,000 false match rate. The\nbiometric templates from different model instances are so strongly incompatible\nthat the cross-instance similarity score for images of the same person is\ntypically lower than the same-instance similarity score for images of different\npersons. That is, a stolen biometric template that is revoked is of less value\nin attempting to match the re-enrolled identity than the average impostor\ntemplate. We also explore the feasibility of using a Vision Transformer (ViT)\nbackbone-based face matcher in the revocable biometric system proposed in this\nwork and demonstrate that it is less suitable compared to typical ResNet-based\ndeep CNN backbones.", "AI": {"tldr": "\u73b0\u4ee3\u6df1\u5ea6CNN\u4eba\u8138\u5339\u914d\u5668\u5929\u751f\u652f\u6301\u53ef\u64a4\u9500\u7684\u751f\u7269\u7279\u5f81\u6a21\u677f\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u4e2d\u6a21\u677f\u88ab\u6cc4\u9732\u540e\u65e0\u6cd5\u64a4\u9500\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u7684\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\u662f\uff0c\u4e00\u65e6\u751f\u7269\u7279\u5f81\u6a21\u677f\u88ab\u6cc4\u9732\uff0c\u7528\u6237\u65e0\u6cd5\u64a4\u9500\u6216\u66f4\u6362\u3002\u53ef\u64a4\u9500\u751f\u7269\u7279\u5f81\u7684\u6982\u5ff5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5177\u6709\u7b49\u6548\u8bc6\u522b\u80fd\u529b\u4f46\u6a21\u677f\u4e0d\u517c\u5bb9\u7684\u6df1\u5ea6CNN\u4eba\u8138\u5339\u914d\u5668\u6a21\u578b\uff0c\u5b9e\u73b0\u751f\u7269\u7279\u5f81\u6a21\u677f\u7684\u53ef\u64a4\u9500\u6027\u3002\u540c\u65f6\u63a2\u7d22\u4e86Vision Transformer\uff08ViT\uff09\u5728\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6df1\u5ea6CNN\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u65e0\u9650\u6570\u91cf\u7684\u7b49\u6548\u8bc6\u522b\u80fd\u529b\u6a21\u578b\uff0c\u4e14\u6a21\u677f\u95f4\u517c\u5bb9\u6027\u6781\u4f4e\uff0c\u6cc4\u9732\u7684\u6a21\u677f\u5728\u64a4\u9500\u540e\u51e0\u4e4e\u65e0\u4ef7\u503c\u3002ViT\u5728\u6b64\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u5982ResNet\u3002", "conclusion": "\u6df1\u5ea6CNN\u4eba\u8138\u5339\u914d\u5668\u5929\u751f\u652f\u6301\u53ef\u64a4\u9500\u751f\u7269\u7279\u5f81\u6a21\u677f\uff0c\u4e3a\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u6df1\u5ea6CNN\u4eba\u8138\u5339\u914d\u5668\u5929\u751f\u652f\u6301\u53ef\u64a4\u9500\u7684\u751f\u7269\u7279\u5f81\u6a21\u677f", "abstract_zh": "\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u7684\u4e00\u4e2a\u5e38\u89c1\u6279\u8bc4\u662f\uff0c\u5982\u679c\u4e2a\u4f53\u7684\u751f\u7269\u7279\u5f81\u88ab\u6cc4\u9732\uff0c\u4e2a\u4f53\u5c06\u65e0\u6cd5\u633d\u56de\u3002\u53ef\u64a4\u9500\u751f\u7269\u7279\u5f81\u7684\u6982\u5ff5\u6b63\u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u800c\u63d0\u51fa\u7684\u3002\u4e00\u4e2a\u751f\u7269\u7279\u5f81\u65b9\u6848\u662f\u53ef\u64a4\u9500\u7684\uff0c\u5982\u679c\u4e2a\u4f53\u53ef\u4ee5\u64a4\u9500\u5f53\u524d\u7684\u6ce8\u518c\uff0c\u4f7f\u6cc4\u9732\u7684\u751f\u7269\u7279\u5f81\u6a21\u677f\u5931\u6548\uff0c\u5e76\u91cd\u65b0\u6ce8\u518c\u4e00\u4e2a\u65b0\u7684\u5177\u6709\u76f8\u4f3c\u8bc6\u522b\u80fd\u529b\u7684\u6a21\u677f\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u73b0\u4ee3\u6df1\u5ea6CNN\u4eba\u8138\u5339\u914d\u5668\u5929\u751f\u652f\u6301\u4e00\u79cd\u5f3a\u5927\u7684\u53ef\u64a4\u9500\u751f\u7269\u7279\u5f81\u65b9\u6848\u3002\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u6700\u5148\u8fdb\u6df1\u5ea6CNN\u4e3b\u5e72\u548c\u8bad\u7ec3\u96c6\uff0c\u53ef\u4ee5\u751f\u6210\u65e0\u9650\u6570\u91cf\u7684\u4e0d\u540c\u4eba\u8138\u5339\u914d\u5668\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\uff081\uff09\u7b49\u6548\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u4ee5\u53ca\uff082\uff09\u6781\u4e0d\u517c\u5bb9\u7684\u751f\u7269\u7279\u5f81\u6a21\u677f\u3002\u8fd9\u79cd\u7b49\u6548\u8bc6\u522b\u80fd\u529b\u751a\u81f3\u6269\u5c55\u5230\u751f\u6210\u5177\u6709\u76f8\u540c\u5f62\u72b6\u548c\u76f8\u4f3c\u5ea6\u7ef4\u5ea6\u5206\u5e03\u7684\u5192\u540d\u9876\u66ff\u548c\u771f\u5b9e\u5206\u5e03\uff0c\u8fd9\u610f\u5473\u7740\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u5171\u4eab\u4e00\u4e2a\u7528\u4e8e1/10,000\u9519\u8bef\u5339\u914d\u7387\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\u3002\u4e0d\u540c\u6a21\u578b\u5b9e\u4f8b\u7684\u751f\u7269\u7279\u5f81\u6a21\u677f\u517c\u5bb9\u6027\u6781\u4f4e\uff0c\u4ee5\u81f3\u4e8e\u540c\u4e00\u4eba\u7269\u7684\u8de8\u5b9e\u4f8b\u76f8\u4f3c\u5ea6\u5f97\u5206\u901a\u5e38\u4f4e\u4e8e\u4e0d\u540c\u4eba\u7269\u7684\u540c\u5b9e\u4f8b\u76f8\u4f3c\u5ea6\u5f97\u5206\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u88ab\u64a4\u9500\u7684\u6cc4\u9732\u751f\u7269\u7279\u5f81\u6a21\u677f\u5728\u5c1d\u8bd5\u5339\u914d\u91cd\u65b0\u6ce8\u518c\u7684\u8eab\u4efd\u65f6\uff0c\u5176\u4ef7\u503c\u4f4e\u4e8e\u5e73\u5747\u5192\u540d\u9876\u66ff\u6a21\u677f\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u5728\u672c\u6587\u63d0\u51fa\u7684\u53ef\u64a4\u9500\u751f\u7269\u7279\u5f81\u7cfb\u7edf\u4e2d\u4f7f\u7528\u57fa\u4e8eVision Transformer\uff08ViT\uff09\u4e3b\u5e72\u7684\u4eba\u8138\u5339\u914d\u5668\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8bc1\u660e\u5176\u4e0d\u5982\u5178\u578b\u7684\u57fa\u4e8eResNet\u7684\u6df1\u5ea6CNN\u4e3b\u5e72\u3002"}}
{"id": "2506.17765", "pdf": "https://arxiv.org/pdf/2506.17765", "abs": "https://arxiv.org/abs/2506.17765", "authors": ["Jiao Chen", "Kehui Yao", "Reza Yousefi Maragheh", "Kai Zhao", "Jianpeng Xu", "Jason Cho", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "CARTS: Collaborative Agents for Recommendation Textual Summarization", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCARTS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6587\u672c\u6458\u8981\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6807\u9898\u76f8\u5173\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u7684\u6587\u672c\u6458\u8981\u4efb\u52a1\uff08\u5982\u751f\u6210\u7b80\u6d01\u6807\u9898\uff09\u9700\u8981\u9ad8\u5ea6\u76f8\u5173\u6027\u548c\u4e25\u683c\u7684\u5b57\u6570\u9650\u5236\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u9002\u7528\u3002", "method": "CARTS\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u589e\u5f3a\u751f\u6210\uff08GAG\uff09\u3001\u8fed\u4ee3\u7cbe\u70bc\u548c\u4ef2\u88c1\uff0c\u5206\u522b\u8d1f\u8d23\u63d0\u53d6\u7279\u5f81\u3001\u4f18\u5316\u6807\u9898\u548c\u6700\u7ec8\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCARTS\u5728\u5927\u89c4\u6a21\u7535\u5546\u6570\u636e\u548cA/B\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u548c\u601d\u7ef4\u94fe\u57fa\u7ebf\uff0c\u6807\u9898\u76f8\u5173\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CARTS\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6587\u672c\u6458\u8981\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "CARTS\uff1a\u534f\u4f5c\u667a\u80fd\u4f53\u7528\u4e8e\u63a8\u8350\u6587\u672c\u6458\u8981", "abstract_zh": "\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u5e38\u9700\u6587\u672c\u6458\u8981\uff08\u5982\u4e3a\u4ea7\u54c1\u8f6e\u64ad\u751f\u6210\u7b80\u6d01\u6807\u9898\uff09\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u9002\u7528\uff0c\u56e0\u5176\u9700\u9ad8\u5ea6\u76f8\u5173\u6027\u548c\u4e25\u683c\u5b57\u6570\u9650\u5236\u3002\u672c\u6587\u63d0\u51faCARTS\uff08\u534f\u4f5c\u667a\u80fd\u4f53\u7528\u4e8e\u63a8\u8350\u6587\u672c\u6458\u8981\uff09\uff0c\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u4e13\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u6458\u8981\u8bbe\u8ba1\u3002CARTS\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u589e\u5f3a\u751f\u6210\uff08GAG\uff09\u3001\u7cbe\u70bc\u5faa\u73af\u548c\u4ef2\u88c1\uff0c\u5206\u522b\u7531\u4e0d\u540c\u667a\u80fd\u4f53\u8d1f\u8d23\u63d0\u53d6\u5173\u952e\u7279\u5f81\u3001\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u957f\u5ea6\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u6807\u9898\uff0c\u5e76\u901a\u8fc7\u534f\u4f5c\u4ef2\u88c1\u9009\u62e9\u6700\u7ec8\u6807\u9898\u3002\u5728\u5927\u89c4\u6a21\u7535\u5546\u6570\u636e\u548c\u5b9e\u65f6A/B\u6d4b\u8bd5\u4e2d\uff0cCARTS\u663e\u8457\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u548c\u601d\u7ef4\u94fe\u57fa\u7ebf\uff0c\u6807\u9898\u76f8\u5173\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u6307\u6807\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2506.18737", "pdf": "https://arxiv.org/pdf/2506.18737", "abs": "https://arxiv.org/abs/2506.18737", "authors": ["Shanliang Yao", "Runwei Guan", "Yi Ni", "Sen Xu", "Yong Yue", "Xiaohui Zhu", "Ryan Wen Liu"], "title": "USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IROS", "summary": "Object tracking in inland waterways plays a crucial role in safe and\ncost-effective applications, including waterborne transportation, sightseeing\ntours, environmental monitoring and surface rescue. Our Unmanned Surface\nVehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU,\ndelivers robust tracking capabilities in complex waterborne environments. By\nleveraging these sensors, our USV collected comprehensive object tracking data,\nwhich we present as USVTrack, the first 4D radar-camera tracking dataset\ntailored for autonomous driving in new generation waterborne transportation\nsystems. Our USVTrack dataset presents rich scenarios, featuring diverse\nvarious waterways, varying times of day, and multiple weather and lighting\nconditions. Moreover, we present a simple but effective radar-camera matching\nmethod, termed RCM, which can be plugged into popular two-stage association\ntrackers. Experimental results utilizing RCM demonstrate the effectiveness of\nthe radar-camera matching in improving object tracking accuracy and reliability\nfor autonomous driving in waterborne environments. The USVTrack dataset is\npublic on https://usvtrack.github.io.", "AI": {"tldr": "USVTrack\u662f\u9996\u4e2a\u4e3a\u65b0\u4e00\u4ee3\u6c34\u4e0a\u4ea4\u901a\u7cfb\u7edf\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\u76844D\u96f7\u8fbe-\u76f8\u673a\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USV\uff09\u5728\u590d\u6742\u6c34\u57df\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u96f7\u8fbe-\u76f8\u673a\u5339\u914d\u65b9\u6cd5\uff08RCM\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5185\u9646\u6c34\u57df\u7684\u76ee\u6807\u8ddf\u8e2a\u5bf9\u6c34\u4e0a\u4ea4\u901a\u3001\u89c2\u5149\u65c5\u6e38\u3001\u73af\u5883\u76d1\u6d4b\u548c\u6551\u63f4\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u6570\u636e\u96c6\u548c\u8ddf\u8e2a\u65b9\u6cd5\u5728\u590d\u6742\u6c34\u57df\u73af\u5883\u4e2d\u7684\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5229\u7528\u914d\u59074D\u96f7\u8fbe\u3001\u5355\u76ee\u76f8\u673a\u3001GPS\u548cIMU\u7684\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USV\uff09\u6536\u96c6\u4e86\u591a\u6837\u5316\u7684\u6c34\u57df\u573a\u666f\u6570\u636e\uff0c\u6784\u5efa\u4e86USVTrack\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96f7\u8fbe-\u76f8\u673a\u5339\u914d\u65b9\u6cd5\uff08RCM\uff09\uff0c\u53ef\u96c6\u6210\u5230\u5e38\u7528\u7684\u4e24\u9636\u6bb5\u5173\u8054\u8ddf\u8e2a\u5668\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRCM\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6c34\u57df\u73af\u5883\u4e2d\u3002USVTrack\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8d44\u6e90\u3002", "conclusion": "USVTrack\u6570\u636e\u96c6\u548cRCM\u65b9\u6cd5\u4e3a\u6c34\u4e0a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u4ee5\u9002\u5e94\u66f4\u591a\u573a\u666f\u3002", "paper_title_zh": "USVTrack\uff1a\u57fa\u4e8e\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\u76844D\u96f7\u8fbe-\u76f8\u673a\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5185\u9646\u6c34\u57df\u81ea\u52a8\u9a7e\u9a76", "abstract_zh": "\u5185\u9646\u6c34\u57df\u7684\u76ee\u6807\u8ddf\u8e2a\u5728\u6c34\u4e0a\u4ea4\u901a\u3001\u89c2\u5149\u65c5\u6e38\u3001\u73af\u5883\u76d1\u6d4b\u548c\u6551\u63f4\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u6211\u4eec\u7684\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USV\uff09\u914d\u5907\u4e864D\u96f7\u8fbe\u3001\u5355\u76ee\u76f8\u673a\u3001GPS\u548cIMU\uff0c\u80fd\u591f\u5728\u590d\u6742\u6c34\u57df\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u76ee\u6807\u8ddf\u8e2a\u3002\u901a\u8fc7\u8fd9\u4e9b\u4f20\u611f\u5668\uff0cUSV\u6536\u96c6\u4e86\u5168\u9762\u7684\u76ee\u6807\u8ddf\u8e2a\u6570\u636e\uff0c\u5e76\u6784\u5efa\u4e86USVTrack\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e3a\u65b0\u4e00\u4ee3\u6c34\u4e0a\u4ea4\u901a\u7cfb\u7edf\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\u76844D\u96f7\u8fbe-\u76f8\u673a\u8ddf\u8e2a\u6570\u636e\u96c6\u3002USVTrack\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u6c34\u57df\u573a\u666f\uff0c\u5305\u62ec\u4e0d\u540c\u65f6\u95f4\u6bb5\u3001\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u96f7\u8fbe-\u76f8\u673a\u5339\u914d\u65b9\u6cd5\uff08RCM\uff09\uff0c\u53ef\u96c6\u6210\u5230\u5e38\u7528\u7684\u4e24\u9636\u6bb5\u5173\u8054\u8ddf\u8e2a\u5668\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRCM\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002USVTrack\u6570\u636e\u96c6\u5df2\u5728https://usvtrack.github.io\u516c\u5f00\u3002"}}
{"id": "2506.17776", "pdf": "https://arxiv.org/pdf/2506.17776", "abs": "https://arxiv.org/abs/2506.17776", "authors": ["Dyuman Aditya", "Colton Payne", "Mario Leiva", "Paulo Shakarian"], "title": "Machine Learning Model Integration with Open World Temporal Logic for Process Automation", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "Recent advancements in Machine Learning (ML) have yielded powerful models\ncapable of extracting structured information from diverse and complex data\nsources. However, a significant challenge lies in translating these perceptual\nor extractive outputs into actionable, reasoned decisions within complex\noperational workflows. To address these challenges, this paper introduces a\nnovel approach that integrates the outputs from various machine learning models\ndirectly with the PyReason framework, an open-world temporal logic programming\nreasoning engine. PyReason's foundation in generalized annotated logic allows\nfor the seamless incorporation of real-valued outputs (e.g., probabilities,\nconfidence scores) from diverse ML models, treating them as truth intervals\nwithin its logical framework. Crucially, PyReason provides mechanisms,\nimplemented in Python, to continuously poll ML model outputs, convert them into\nlogical facts, and dynamically recompute the minimal model, ensuring real-tine\nadaptive decision-making. Furthermore, its native support for temporal\nreasoning, knowledge graph integration, and fully explainable interface traces\nenables sophisticated analysis over time-sensitive process data and existing\norganizational knowledge. By combining the strengths of perception and\nextraction from ML models with the logical deduction and transparency of\nPyReason, we aim to create a powerful system for automating complex processes.\nThis integration finds utility across numerous domains, including\nmanufacturing, healthcare, and business operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8f93\u51fa\u4e0ePyReason\u6846\u67b6\uff08\u4e00\u79cd\u5f00\u653e\u4e16\u754c\u65f6\u5e8f\u903b\u8f91\u7f16\u7a0b\u63a8\u7406\u5f15\u64ce\uff09\u96c6\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u611f\u77e5\u548c\u63d0\u53d6\u80fd\u529b\u4e0e\u903b\u8f91\u63a8\u7406\u7ed3\u5408\uff0c\u5b9e\u73b0\u590d\u6742\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u51b3\u7b56\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u4ece\u590d\u6742\u6570\u636e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4f46\u5c06\u5176\u8f93\u51fa\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u548cPyReason\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u901a\u8fc7PyReason\u6846\u67b6\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8f93\u51fa\uff08\u5982\u6982\u7387\u548c\u7f6e\u4fe1\u5ea6\uff09\u8f6c\u6362\u4e3a\u903b\u8f91\u4e8b\u5b9e\uff0c\u5e76\u52a8\u6001\u8ba1\u7b97\u6700\u5c0f\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u9002\u5e94\u51b3\u7b56\u3002PyReason\u8fd8\u652f\u6301\u65f6\u5e8f\u63a8\u7406\u548c\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u611f\u77e5\u8f93\u51fa\u4e0e\u903b\u8f91\u63a8\u7406\u65e0\u7f1d\u7ed3\u5408\uff0c\u652f\u6301\u590d\u6742\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u51b3\u7b56\uff0c\u9002\u7528\u4e8e\u5236\u9020\u3001\u533b\u7597\u548c\u5546\u4e1a\u8fd0\u8425\u7b49\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u673a\u5668\u5b66\u4e60\u4e0ePyReason\u6846\u67b6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u51b3\u7b56\uff0c\u540c\u65f6\u5177\u5907\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "paper_title_zh": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u5f00\u653e\u4e16\u754c\u65f6\u5e8f\u903b\u8f91\u7684\u96c6\u6210\u4ee5\u5b9e\u73b0\u6d41\u7a0b\u81ea\u52a8\u5316", "abstract_zh": "\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u50ac\u751f\u4e86\u80fd\u591f\u4ece\u591a\u6837\u5316\u548c\u590d\u6742\u6570\u636e\u6e90\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u5f3a\u5927\u6a21\u578b\u3002\u7136\u800c\uff0c\u5c06\u8fd9\u4e9b\u611f\u77e5\u6216\u63d0\u53d6\u8f93\u51fa\u8f6c\u5316\u4e3a\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u53ef\u64cd\u4f5c\u51b3\u7b56\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8f93\u51fa\u76f4\u63a5\u4e0ePyReason\u6846\u67b6\uff08\u4e00\u79cd\u5f00\u653e\u4e16\u754c\u65f6\u5e8f\u903b\u8f91\u7f16\u7a0b\u63a8\u7406\u5f15\u64ce\uff09\u96c6\u6210\u3002PyReason\u57fa\u4e8e\u5e7f\u4e49\u6ce8\u91ca\u903b\u8f91\uff0c\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u6765\u81ea\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b9e\u503c\u8f93\u51fa\uff08\u5982\u6982\u7387\u3001\u7f6e\u4fe1\u5ea6\uff09\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u903b\u8f91\u6846\u67b6\u4e2d\u7684\u771f\u503c\u533a\u95f4\u3002\u5173\u952e\u7684\u662f\uff0cPyReason\u901a\u8fc7Python\u5b9e\u73b0\u7684\u673a\u5236\u6301\u7eed\u8f6e\u8be2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u903b\u8f91\u4e8b\u5b9e\uff0c\u5e76\u52a8\u6001\u91cd\u65b0\u8ba1\u7b97\u6700\u5c0f\u6a21\u578b\uff0c\u786e\u4fdd\u5b9e\u65f6\u81ea\u9002\u5e94\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u5176\u5bf9\u65f6\u5e8f\u63a8\u7406\u3001\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u548c\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u63a5\u53e3\u8ddf\u8e2a\u7684\u539f\u751f\u652f\u6301\uff0c\u4f7f\u5f97\u80fd\u591f\u5bf9\u65f6\u95f4\u654f\u611f\u7684\u6d41\u7a0b\u6570\u636e\u548c\u73b0\u6709\u7ec4\u7ec7\u77e5\u8bc6\u8fdb\u884c\u590d\u6742\u5206\u6790\u3002\u901a\u8fc7\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u611f\u77e5\u548c\u63d0\u53d6\u80fd\u529b\u4e0ePyReason\u7684\u903b\u8f91\u63a8\u7406\u548c\u900f\u660e\u6027\uff0c\u6211\u4eec\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u5f3a\u5927\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u590d\u6742\u6d41\u7a0b\u3002\u8fd9\u79cd\u96c6\u6210\u5728\u5236\u9020\u3001\u533b\u7597\u548c\u5546\u4e1a\u8fd0\u8425\u7b49\u591a\u4e2a\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.18785", "pdf": "https://arxiv.org/pdf/2506.18785", "abs": "https://arxiv.org/abs/2506.18785", "authors": ["Helin Cao", "Rafael Materla", "Sven Behnke"], "title": "SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "under reviewed", "summary": "Perception systems in autonomous driving rely on sensors such as LiDAR and\ncameras to perceive the 3D environment. However, due to occlusions and data\nsparsity, these sensors often fail to capture complete information. Semantic\nOccupancy Prediction (SOP) addresses this challenge by inferring both occupancy\nand semantics of unobserved regions. Existing transformer-based SOP methods\nlack explicit modeling of spatial structure in attention computation, resulting\nin limited geometric awareness and poor performance in sparse or occluded\nareas. To this end, we propose Spatially-aware Window Attention (SWA), a novel\nmechanism that incorporates local spatial context into attention. SWA\nsignificantly improves scene completion and achieves state-of-the-art results\non LiDAR-based SOP benchmarks. We further validate its generality by\nintegrating SWA into a camera-based SOP pipeline, where it also yields\nconsistent gains across modalities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSWA-SOP\u7684\u7a7a\u95f4\u611f\u77e5\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u5360\u7528\u9884\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u6216\u906e\u6321\u533a\u57df\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4f9d\u8d56LiDAR\u548c\u6444\u50cf\u5934\u7b49\u4f20\u611f\u5668\u611f\u77e53D\u73af\u5883\uff0c\u4f46\u7531\u4e8e\u906e\u6321\u548c\u6570\u636e\u7a00\u758f\u6027\uff0c\u8fd9\u4e9b\u4f20\u611f\u5668\u5e38\u65e0\u6cd5\u6355\u83b7\u5b8c\u6574\u4fe1\u606f\u3002\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u7ed3\u6784\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u5728\u7a00\u758f\u6216\u906e\u6321\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u7a97\u53e3\u6ce8\u610f\u529b\uff08SWA\uff09\u673a\u5236\uff0c\u5c06\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\u878d\u5165\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u8865\u5168\u80fd\u529b\u3002\u6b64\u5916\uff0cSWA\u8fd8\u88ab\u96c6\u6210\u5230\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u6d41\u7a0b\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u5176\u8de8\u6a21\u6001\u7684\u901a\u7528\u6027\u3002", "result": "SWA\u5728\u57fa\u4e8eLiDAR\u7684\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u6d41\u7a0b\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SWA-SOP\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u611f\u77e5\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u6216\u906e\u6321\u533a\u57df\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002", "paper_title_zh": "SWA-SOP\uff1a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u7a7a\u95f4\u611f\u77e5\u7a97\u53e3\u6ce8\u610f\u529b", "abstract_zh": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4f9d\u8d56LiDAR\u548c\u6444\u50cf\u5934\u7b49\u4f20\u611f\u5668\u611f\u77e53D\u73af\u5883\uff0c\u4f46\u7531\u4e8e\u906e\u6321\u548c\u6570\u636e\u7a00\u758f\u6027\uff0c\u8fd9\u4e9b\u4f20\u611f\u5668\u5e38\u65e0\u6cd5\u6355\u83b7\u5b8c\u6574\u4fe1\u606f\u3002\u8bed\u4e49\u5360\u7528\u9884\u6d4b\uff08SOP\uff09\u901a\u8fc7\u63a8\u65ad\u672a\u89c2\u6d4b\u533a\u57df\u7684\u5360\u7528\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684SOP\u65b9\u6cd5\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u7ed3\u6784\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u5728\u7a00\u758f\u6216\u906e\u6321\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u7a97\u53e3\u6ce8\u610f\u529b\uff08SWA\uff09\u673a\u5236\uff0c\u5c06\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\u878d\u5165\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u3002SWA\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u8865\u5168\u80fd\u529b\uff0c\u5e76\u5728\u57fa\u4e8eLiDAR\u7684SOP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c06SWA\u96c6\u6210\u5230\u57fa\u4e8e\u6444\u50cf\u5934\u7684SOP\u6d41\u7a0b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u8de8\u6a21\u6001\u4e2d\u4e5f\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.17779", "pdf": "https://arxiv.org/pdf/2506.17779", "abs": "https://arxiv.org/abs/2506.17779", "authors": ["Andrei Cristian Nica", "Akshaya Vishnu Kudlu Shanbhogue", "Harshil Shah", "Aleix Cambray", "Tudor Berariu", "Lucas Maystre", "David Barber"], "title": "Toward Autonomous UI Exploration: The UIExplorer Benchmark", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Autonomous agents must know how to explore user interfaces (UIs) for reliable\ntask solving, yet systematic evaluation of this crucial phase is lacking. We\nintroduce UIExplore-Bench, the first benchmark explicitly dedicated to UI\nexploration. The benchmark evaluates agents with either Structured mode\n(granting access to layout information like DOM trees) or Screen mode (relying\non GUI-only observations such as screenshots and human-like mouse/keyboard\ninteractions) across three levels in a standardized GitLab sandbox environment.\nWe formalize exploration as the process of maximizing the set of actionable UI\ncomponents discovered and propose a metric, human-normalized UI-Functionalities\nObserved (hUFO), to quantify the effectiveness of exploration. Our results show\nthat UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%\nof human performance in Structured mode and 59.0% in Screen mode at 2,000\nsteps, particularly excelling at the Sparse level. The results highlight the\nrelevance of our benchmark, as current agents show a substantial performance\ngap compared to one hour of human expert exploration, indicating ample room for\nfuture advancements. We publicly release the benchmark environment, an\nexploration dataset, and an evaluation suite to catalyze research into\nefficient UI exploration strategies and their downstream applications, such as\nexperience-driven task completion and automated training data generation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8eUI\u63a2\u7d22\u7684\u57fa\u51c6\u6d4b\u8bd5UIExplore-Bench\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u5f0f\u548c\u5c4f\u5e55\u6a21\u5f0f\u8bc4\u4f30\u4ee3\u7406\u5728GitLab\u6c99\u76d2\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fahUFO\u6307\u6807\u91cf\u5316\u63a2\u7d22\u6548\u679c\u3002\u7ed3\u679c\u663e\u793aUIExplore-AlGo\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u81ea\u4e3b\u4ee3\u7406\u5728\u7528\u6237\u754c\u9762\uff08UI\uff09\u63a2\u7d22\u9636\u6bb5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u800c\u8fd9\u4e00\u9636\u6bb5\u5bf9\u4efb\u52a1\u89e3\u51b3\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8UI\u63a2\u7d22\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86UIExplore-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7ed3\u6784\u5316\u6a21\u5f0f\uff08\u57fa\u4e8eDOM\u6811\u7b49\u5e03\u5c40\u4fe1\u606f\uff09\u548c\u5c4f\u5e55\u6a21\u5f0f\uff08\u57fa\u4e8eGUI\u89c2\u5bdf\u5982\u622a\u56fe\u548c\u6a21\u62df\u4eba\u673a\u4ea4\u4e92\uff09\u3002\u63a2\u7d22\u76ee\u6807\u4e3a\u6700\u5927\u5316\u53d1\u73b0\u53ef\u64cd\u4f5c\u7684UI\u7ec4\u4ef6\uff0c\u5e76\u91c7\u7528hUFO\u6307\u6807\u91cf\u5316\u6548\u679c\u3002\u6d4b\u8bd5\u5728GitLab\u6c99\u76d2\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u5206\u4e3a\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002", "result": "UIExplore-AlGo\u5728\u7ed3\u6784\u5316\u6a21\u5f0f\u548c\u5c4f\u5e55\u6a21\u5f0f\u4e0b\u7684hUFO\u5f97\u5206\u5206\u522b\u4e3a77.2%\u548c59.0%\uff0c\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u7a00\u758f\u7ea7\u522b\u4e0a\u7a81\u51fa\u3002\u7136\u800c\uff0c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4e00\u5c0f\u65f6\u7684\u63a2\u7d22\u76f8\u6bd4\uff0c\u73b0\u6709\u4ee3\u7406\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "UIExplore-Bench\u57fa\u51c6\u6d4b\u8bd5\u7684\u63d0\u51fa\u4e3aUI\u63a2\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002\u540c\u65f6\uff0c\u516c\u5f00\u7684\u6d4b\u8bd5\u73af\u5883\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5957\u4ef6\u5c06\u63a8\u52a8\u9ad8\u6548UI\u63a2\u7d22\u7b56\u7565\u53ca\u5176\u5e94\u7528\u7684\u53d1\u5c55\u3002", "paper_title_zh": "\u8fc8\u5411\u81ea\u4e3bUI\u63a2\u7d22\uff1aUIExplorer\u57fa\u51c6\u6d4b\u8bd5", "abstract_zh": "\u81ea\u4e3b\u4ee3\u7406\u9700\u8981\u638c\u63e1\u5982\u4f55\u63a2\u7d22\u7528\u6237\u754c\u9762\uff08UI\uff09\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u4efb\u52a1\u89e3\u51b3\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e00\u5173\u952e\u9636\u6bb5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u6211\u4eec\u63a8\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8eUI\u63a2\u7d22\u7684\u57fa\u51c6\u6d4b\u8bd5UIExplore-Bench\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5728\u6807\u51c6\u5316\u7684GitLab\u6c99\u76d2\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u5f0f\uff08\u63d0\u4f9bDOM\u6811\u7b49\u5e03\u5c40\u4fe1\u606f\uff09\u6216\u5c4f\u5e55\u6a21\u5f0f\uff08\u4f9d\u8d56GUI\u89c2\u5bdf\u5982\u622a\u56fe\u548c\u6a21\u62df\u4eba\u673a\u4ea4\u4e92\uff09\u8bc4\u4f30\u4ee3\u7406\u5728\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u8868\u73b0\u3002\u6211\u4eec\u5c06\u63a2\u7d22\u5b9a\u4e49\u4e3a\u6700\u5927\u5316\u53d1\u73b0\u53ef\u64cd\u4f5c\u7684UI\u7ec4\u4ef6\u7684\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4eba\u7c7b\u6807\u51c6\u5316UI\u529f\u80fd\u89c2\u5bdf\u6307\u6807\uff08hUFO\uff09\u6765\u91cf\u5316\u63a2\u7d22\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\uff0cUIExplore-AlGo\u5728\u7ed3\u6784\u5316\u6a21\u5f0f\u548c\u5c4f\u5e55\u6a21\u5f0f\u4e0b\u7684hUFO\u5f97\u5206\u5206\u522b\u8fbe\u5230\u4eba\u7c7b\u8868\u73b0\u768477.2%\u548c59.0%\uff082000\u6b65\u65f6\uff09\uff0c\u5c24\u5176\u5728\u7a00\u758f\u7ea7\u522b\u8868\u73b0\u7a81\u51fa\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff0c\u56e0\u4e3a\u5f53\u524d\u4ee3\u7406\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4e00\u5c0f\u65f6\u7684\u63a2\u7d22\u76f8\u6bd4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u8868\u660e\u672a\u6765\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u6211\u4eec\u516c\u5f00\u4e86\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\u3001\u63a2\u7d22\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u63a8\u52a8\u9ad8\u6548UI\u63a2\u7d22\u7b56\u7565\u53ca\u5176\u4e0b\u6e38\u5e94\u7528\uff08\u5982\u7ecf\u9a8c\u9a71\u52a8\u7684\u4efb\u52a1\u5b8c\u6210\u548c\u81ea\u52a8\u5316\u8bad\u7ec3\u6570\u636e\u751f\u6210\uff09\u7684\u7814\u7a76\u3002"}}
{"id": "2506.18787", "pdf": "https://arxiv.org/pdf/2506.18787", "abs": "https://arxiv.org/abs/2506.18787", "authors": ["Dylan Ebert"], "title": "3D Arena: An Open Platform for Generative 3D Evaluation", "categories": ["cs.CV"], "comment": "9 pages, 2 figures", "summary": "Evaluating Generative 3D models remains challenging due to misalignment\nbetween automated metrics and human perception of quality. Current benchmarks\nrely on image-based metrics that ignore 3D structure or geometric measures that\nfail to capture perceptual appeal and real-world utility. To address this gap,\nwe present 3D Arena, an open platform for evaluating image-to-3D generation\nmodels through large-scale human preference collection using pairwise\ncomparisons.\n  Since launching in June 2024, the platform has collected 123,243 votes from\n8,096 users across 19 state-of-the-art models, establishing the largest human\npreference evaluation for Generative 3D. We contribute the iso3d dataset of 100\nevaluation prompts and demonstrate quality control achieving 99.75% user\nauthenticity through statistical fraud detection. Our ELO-based ranking system\nprovides reliable model assessment, with the platform becoming an established\nevaluation resource.\n  Through analysis of this preference data, we present insights into human\npreference patterns. Our findings reveal preferences for visual presentation\nfeatures, with Gaussian splat outputs achieving a 16.6 ELO advantage over\nmeshes and textured models receiving a 144.1 ELO advantage over untextured\nmodels. We provide recommendations for improving evaluation methods, including\nmulti-criteria assessment, task-oriented evaluation, and format-aware\ncomparison. The platform's community engagement establishes 3D Arena as a\nbenchmark for the field while advancing understanding of human-centered\nevaluation in Generative 3D.", "AI": {"tldr": "3D Arena\u662f\u4e00\u4e2a\u5f00\u653e\u5e73\u53f0\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6536\u96c6\u8bc4\u4f30\u751f\u6210\u5f0f3D\u6a21\u578b\uff0c\u586b\u8865\u4e86\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6a21\u578b\u6392\u540d\u548c\u4eba\u7c7b\u504f\u597d\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f3D\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u56fe\u50cf\u6307\u6807\u6216\u51e0\u4f55\u6d4b\u91cf\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u611f\u77e5\u5438\u5f15\u529b\u548c\u5b9e\u9645\u6548\u7528\u3002", "method": "3D Arena\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u6536\u96c6\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u4f7f\u7528\u7edf\u8ba1\u6b3a\u8bc8\u68c0\u6d4b\u786e\u4fdd\u7528\u6237\u771f\u5b9e\u6027\uff0c\u5e76\u57fa\u4e8eELO\u6392\u540d\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5e73\u53f0\u6536\u96c6\u4e86123,243\u7968\u7528\u6237\u504f\u597d\u6570\u636e\uff0c\u53d1\u73b0\u9ad8\u65af\u6e85\u5c04\u8f93\u51fa\u6bd4\u7f51\u683c\u6a21\u578b\u66f4\u5177\u4f18\u52bf\uff0816.6 ELO\uff09\uff0c\u5e26\u7eb9\u7406\u6a21\u578b\u6bd4\u65e0\u7eb9\u7406\u6a21\u578b\u66f4\u53d7\u6b22\u8fce\uff08144.1 ELO\uff09\u3002", "conclusion": "3D Arena\u6210\u4e3a\u751f\u6210\u5f0f3D\u8bc4\u4f30\u7684\u6807\u6746\uff0c\u63d0\u51fa\u4e86\u591a\u6807\u51c6\u8bc4\u4f30\u3001\u4efb\u52a1\u5bfc\u5411\u8bc4\u4f30\u548c\u683c\u5f0f\u611f\u77e5\u6bd4\u8f83\u7b49\u6539\u8fdb\u5efa\u8bae\uff0c\u63a8\u52a8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u3002", "paper_title_zh": "3D Arena\uff1a\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5f0f3D\u8bc4\u4f30\u7684\u5f00\u653e\u5e73\u53f0", "abstract_zh": "\u8bc4\u4f30\u751f\u6210\u5f0f3D\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u7c7b\u5bf9\u8d28\u91cf\u7684\u611f\u77e5\u4e4b\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u3002\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u5ffd\u75653D\u7ed3\u6784\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u6307\u6807\uff0c\u6216\u65e0\u6cd5\u6355\u6349\u611f\u77e5\u5438\u5f15\u529b\u548c\u5b9e\u9645\u6548\u7528\u7684\u51e0\u4f55\u6d4b\u91cf\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e863D Arena\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u6536\u96c6\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u7684\u5f00\u653e\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u52303D\u7684\u751f\u6210\u6a21\u578b\u3002\n\u81ea2024\u5e746\u6708\u63a8\u51fa\u4ee5\u6765\uff0c\u8be5\u5e73\u53f0\u5df2\u4ece8,096\u540d\u7528\u6237\u6536\u96c6\u4e86123,243\u7968\uff0c\u8986\u76d619\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u751f\u6210\u5f0f3D\u9886\u57df\u6700\u5927\u89c4\u6a21\u7684\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u3002\u6211\u4eec\u8d21\u732e\u4e86\u5305\u542b100\u4e2a\u8bc4\u4f30\u63d0\u793a\u7684iso3d\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u6b3a\u8bc8\u68c0\u6d4b\u5b9e\u73b0\u4e8699.75%\u7684\u7528\u6237\u771f\u5b9e\u6027\u3002\u57fa\u4e8eELO\u7684\u6392\u540d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u4f7f\u8be5\u5e73\u53f0\u6210\u4e3a\u516c\u8ba4\u7684\u8bc4\u4f30\u8d44\u6e90\u3002\n\u901a\u8fc7\u5bf9\u504f\u597d\u6570\u636e\u7684\u5206\u6790\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u4eba\u7c7b\u504f\u597d\u7684\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u65af\u6e85\u5c04\u8f93\u51fa\u6bd4\u7f51\u683c\u6a21\u578b\u5177\u670916.6 ELO\u7684\u4f18\u52bf\uff0c\u5e26\u7eb9\u7406\u6a21\u578b\u6bd4\u65e0\u7eb9\u7406\u6a21\u578b\u5177\u6709144.1 ELO\u7684\u4f18\u52bf\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u7684\u5efa\u8bae\uff0c\u5305\u62ec\u591a\u6807\u51c6\u8bc4\u4f30\u3001\u4efb\u52a1\u5bfc\u5411\u8bc4\u4f30\u548c\u683c\u5f0f\u611f\u77e5\u6bd4\u8f83\u3002\u5e73\u53f0\u7684\u793e\u533a\u53c2\u4e0e\u4f7f3D Arena\u6210\u4e3a\u8be5\u9886\u57df\u7684\u6807\u6746\uff0c\u540c\u65f6\u63a8\u52a8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u5f0f3D\u8bc4\u4f30\u7684\u7406\u89e3\u3002"}}
{"id": "2506.17782", "pdf": "https://arxiv.org/pdf/2506.17782", "abs": "https://arxiv.org/abs/2506.17782", "authors": ["Catarina Pires", "S\u00e9rgio Nunes", "Lu\u00eds Filipe Teixeira"], "title": "Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs", "categories": ["cs.IR", "cs.AI", "H.3.3; I.2.7"], "comment": "To appear at the Third Workshop on Large Language Models for\n  Evaluation in Information Retrieval (LLM4Eval 2025), co-located with SIGIR\n  2025. 9 pages, 2 figures, 5 tables", "summary": "Evaluating Information Retrieval (IR) systems relies on high-quality manual\nrelevance judgments (qrels), which are costly and time-consuming to obtain.\nWhile pooling reduces the annotation effort, it results in only partially\nlabeled datasets. Large Language Models (LLMs) offer a promising alternative to\nreducing reliance on manual judgments, particularly in complex domains like\nmedical case-based retrieval, where relevance assessment requires analyzing\nboth textual and visual information. In this work, we explore using a\nMultimodal Large Language Model (MLLM) to expand relevance judgments, creating\na new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on\nthe ImageCLEFmed 2013 case-based retrieval task, simulating human assessment\nthrough an iteratively refined, structured prompting strategy that integrates\nbinary scoring, instruction-based evaluation, and few-shot learning. We\nsystematically experimented with various prompt configurations to maximize\nagreement with human judgments. To evaluate agreement between the MLLM and\nhuman judgments, we use Cohen's Kappa, achieving a substantial agreement score\nof 0.6, comparable to inter-annotator agreement typically observed in\nmultimodal retrieval tasks. Starting from the original 15,028 manual judgments\n(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset\nby over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On\naverage, each medical case query received 15,398 new annotations, with\napproximately 99% being non-relevant, reflecting the high sparsity typical in\nthis domain. Our results demonstrate the potential of MLLMs to scale relevance\njudgment collection, offering a promising direction for supporting retrieval\nevaluation in medical and multimodal IR tasks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6269\u5c55\u533b\u5b66\u6848\u4f8b\u68c0\u7d22\u4efb\u52a1\u7684\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\u751f\u6210\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u89c4\u6a21\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u8fbe\u6210\u8f83\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u957f\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5206\u6790\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u533b\u5b66\u6848\u4f8b\u68c0\u7d22\u9886\u57df\u3002", "method": "\u91c7\u7528Gemini 1.5 Pro\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff08\u5305\u62ec\u4e8c\u5143\u8bc4\u5206\u3001\u6307\u4ee4\u8bc4\u4f30\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff09\u6a21\u62df\u4eba\u5de5\u6807\u6ce8\uff0c\u6269\u5c55ImageCLEFmed 2013\u4efb\u52a1\u7684\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684Cohen's Kappa\u4e00\u81f4\u6027\u5f97\u5206\u4e3a0.6\uff0c\u6807\u6ce8\u89c4\u6a21\u4ece15,028\u6269\u5c55\u5230558,653\uff0c\u76f8\u5173\u6807\u6ce8\u4ece4.72%\u589e\u81f35,950\uff0c\u6bcf\u4e2a\u67e5\u8be2\u5e73\u5747\u65b0\u589e15,398\u6761\u6807\u6ce8\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u663e\u8457\u6269\u5c55\u76f8\u5173\u6027\u6807\u6ce8\u89c4\u6a21\uff0c\u4e3a\u533b\u5b66\u548c\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "paper_title_zh": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u533b\u5b66\u6848\u4f8b\u68c0\u7d22\u4efb\u52a1\u7684\u76f8\u5173\u6027\u6807\u6ce8", "abstract_zh": "\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\uff08qrels\uff09\uff0c\u4f46\u83b7\u53d6\u8fd9\u4e9b\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u957f\u3002\u5c3d\u7ba1\u6c60\u5316\u65b9\u6cd5\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u4f46\u4ec5\u80fd\u751f\u6210\u90e8\u5206\u6807\u6ce8\u7684\u6570\u636e\u96c6\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5206\u6790\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u590d\u6742\u9886\u57df\uff08\u5982\u533b\u5b66\u6848\u4f8b\u68c0\u7d22\uff09\u4e2d\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6269\u5c55\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u751f\u6210\u81ea\u52a8\u5316\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5728ImageCLEFmed 2013\u6848\u4f8b\u68c0\u7d22\u4efb\u52a1\u4e2d\u91c7\u7528Gemini 1.5 Pro\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u7684\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff08\u6574\u5408\u4e8c\u5143\u8bc4\u5206\u3001\u6307\u4ee4\u8bc4\u4f30\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff09\u6a21\u62df\u4eba\u5de5\u8bc4\u4f30\u3002\u6211\u4eec\u7cfb\u7edf\u6027\u5730\u5c1d\u8bd5\u4e86\u591a\u79cd\u63d0\u793a\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u5316\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u8bc4\u4f30MLLM\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u4f7f\u7528Cohen's Kappa\uff0c\u83b7\u5f97\u4e860.6\u7684\u663e\u8457\u4e00\u81f4\u6027\u5206\u6570\uff0c\u4e0e\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5e38\u89c1\u7684\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u76f8\u5f53\u3002\u539f\u59cb\u6570\u636e\u96c6\u5305\u542b35\u4e2a\u4e3b\u9898\u768415,028\u6761\u4eba\u5de5\u6807\u6ce8\uff084.72%\u76f8\u5173\uff09\uff0c\u901a\u8fc7MLLM\u65b9\u6cd5\u5c06\u6570\u636e\u96c6\u6269\u5c55\u4e8637\u500d\u4ee5\u4e0a\uff0c\u8fbe\u5230558,653\u6761\u6807\u6ce8\uff0c\u76f8\u5173\u6807\u6ce8\u589e\u81f35,950\u6761\u3002\u5e73\u5747\u6bcf\u4e2a\u533b\u5b66\u6848\u4f8b\u67e5\u8be2\u65b0\u589e15,398\u6761\u6807\u6ce8\uff0c\u5176\u4e2d\u7ea699%\u4e3a\u975e\u76f8\u5173\u6807\u6ce8\uff0c\u53cd\u6620\u4e86\u8be5\u9886\u57df\u7684\u9ad8\u7a00\u758f\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cMLLM\u80fd\u591f\u663e\u8457\u6269\u5c55\u76f8\u5173\u6027\u6807\u6ce8\u89c4\u6a21\uff0c\u4e3a\u533b\u5b66\u548c\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.18791", "pdf": "https://arxiv.org/pdf/2506.18791", "abs": "https://arxiv.org/abs/2506.18791", "authors": ["Suyash Gaurav", "Muhammad Farhan Humayun", "Jukka Heikkonen", "Jatin Chaudhary"], "title": "Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The evolution of Vision Transformers has led to their widespread adaptation\nto different domains. Despite large-scale success, there remain significant\nchallenges including their reliance on extensive computational and memory\nresources for pre-training on huge datasets as well as difficulties in\ntask-specific transfer learning. These limitations coupled with energy\ninefficiencies mainly arise due to the computation-intensive self-attention\nmechanism. To address these issues, we propose a novel Super-Pixel Based Patch\nPooling (SPPP) technique that generates context-aware, semantically rich, patch\nembeddings to effectively reduce the architectural complexity and improve\nefficiency. Additionally, we introduce the Light Latent Attention (LLA) module\nin our pipeline by integrating latent tokens into the attention mechanism\nallowing cross-attention operations to significantly reduce the time and space\ncomplexity of the attention module. By leveraging the data-intuitive patch\nembeddings coupled with dynamic positional encodings, our approach adaptively\nmodulates the cross-attention process to focus on informative regions while\nmaintaining the global semantic structure. This targeted attention improves\ntraining efficiency and accelerates convergence. Notably, the SPPP module is\nlightweight and can be easily integrated into existing transformer\narchitectures. Extensive experiments demonstrate that our proposed architecture\nprovides significant improvements in terms of computational efficiency while\nachieving comparable results with the state-of-the-art approaches, highlighting\nits potential for energy-efficient transformers suitable for edge deployment.\n(The code is available on our GitHub repository:\nhttps://github.com/zser092/Focused-Attention-ViT).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u50cf\u7d20\u7684\u8865\u4e01\u6c60\u5316\uff08SPPP\uff09\u6280\u672f\u548c\u8f7b\u91cf\u6f5c\u5728\u6ce8\u610f\u529b\uff08LLA\uff09\u6a21\u5757\uff0c\u65e8\u5728\u964d\u4f4e\u89c6\u89c9\u53d8\u6362\u5668\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u53d8\u6362\u5668\u5728\u5e7f\u6cdb\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5176\u4f9d\u8d56\u5927\u91cf\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u4e14\u4efb\u52a1\u7279\u5b9a\u8fc1\u79fb\u5b66\u4e60\u56f0\u96be\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bfc\u81f4\u80fd\u6548\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faSPPP\u6280\u672f\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bed\u4e49\u4e30\u5bcc\u8865\u4e01\u5d4c\u5165\uff0c\u964d\u4f4e\u67b6\u6784\u590d\u6742\u6027\uff1b\u5f15\u5165LLA\u6a21\u5757\uff0c\u901a\u8fc7\u6f5c\u5728\u4ee4\u724c\u96c6\u6210\u51cf\u5c11\u6ce8\u610f\u529b\u6a21\u5757\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u7684\u80fd\u6548\u4f18\u5316\u53d8\u6362\u5668\u3002", "conclusion": "SPPP\u548cLLA\u6a21\u5757\u7684\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u53d8\u6362\u5668\u7684\u8d44\u6e90\u9700\u6c42\u95ee\u9898\uff0c\u4e3a\u8f7b\u91cf\u5316\u548c\u9ad8\u6548\u80fd\u53d8\u6362\u5668\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "paper_title_zh": "\u805a\u7126\u6ce8\u610f\u529b\uff1a\u9762\u5411\u6570\u636e\u76f4\u89c2\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u53d8\u6362\u5668", "abstract_zh": "\u89c6\u89c9\u53d8\u6362\u5668\u7684\u53d1\u5c55\u4f7f\u5176\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540c\u9886\u57df\uff0c\u4f46\u5176\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u4ee5\u53ca\u4efb\u52a1\u7279\u5b9a\u8fc1\u79fb\u5b66\u4e60\u7684\u56f0\u96be\uff0c\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002\u8fd9\u4e9b\u95ee\u9898\u4e3b\u8981\u6e90\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u50cf\u7d20\u8865\u4e01\u6c60\u5316\uff08SPPP\uff09\u6280\u672f\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u8865\u4e01\u5d4c\u5165\uff0c\u6709\u6548\u964d\u4f4e\u67b6\u6784\u590d\u6742\u6027\u5e76\u63d0\u5347\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u6d41\u7a0b\u4e2d\u5f15\u5165\u4e86\u8f7b\u91cf\u6f5c\u5728\u6ce8\u610f\u529b\uff08LLA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u6f5c\u5728\u4ee4\u724c\u96c6\u6210\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u6ce8\u610f\u529b\u6a21\u5757\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u76f4\u89c2\u7684\u8865\u4e01\u5d4c\u5165\u548c\u52a8\u6001\u4f4d\u7f6e\u7f16\u7801\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u81ea\u9002\u5e94\u8c03\u8282\u4ea4\u53c9\u6ce8\u610f\u529b\u8fc7\u7a0b\uff0c\u805a\u7126\u4fe1\u606f\u4e30\u5bcc\u533a\u57df\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u7ed3\u6784\u3002\u8fd9\u79cd\u5b9a\u5411\u6ce8\u610f\u529b\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u52a0\u901f\u6536\u655b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSPPP\u6a21\u5757\u8f7b\u91cf\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u53d8\u6362\u5668\u67b6\u6784\u4e2d\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u67b6\u6784\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u7a81\u663e\u4e86\u5176\u9002\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\u7684\u80fd\u6548\u4f18\u5316\u53d8\u6362\u5668\u7684\u6f5c\u529b\u3002\uff08\u4ee3\u7801\u5df2\u53d1\u5e03\u4e8eGitHub\u4ed3\u5e93\uff1ahttps://github.com/zser092/Focused-Attention-ViT\uff09\u3002"}}
{"id": "2506.17807", "pdf": "https://arxiv.org/pdf/2506.17807", "abs": "https://arxiv.org/abs/2506.17807", "authors": ["Lijun Zhang", "Xiao Liu", "Hui Guan"], "title": "Reimagining Parameter Space Exploration with Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICML 2025 EXAIT Workshop", "summary": "Adapting neural networks to new tasks typically requires task-specific\nfine-tuning, which is time-consuming and reliant on labeled data. We explore a\ngenerative alternative that produces task-specific parameters directly from\ntask identity, eliminating the need for task-specific training. To this end, we\npropose using diffusion models to learn the underlying structure of effective\ntask-specific parameter space and synthesize parameters on demand. Once\ntrained, the task-conditioned diffusion model can generate specialized weights\ndirectly from task identifiers. We evaluate this approach across three\nscenarios: generating parameters for a single seen task, for multiple seen\ntasks, and for entirely unseen tasks. Experiments show that diffusion models\ncan generate accurate task-specific parameters and support multi-task\ninterpolation when parameter subspaces are well-structured, but fail to\ngeneralize to unseen tasks, highlighting both the potential and limitations of\nthis generative solution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u6a21\u578b\u76f4\u63a5\u4ece\u4efb\u52a1\u6807\u8bc6\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5fae\u8c03\u7684\u65f6\u95f4\u548c\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5df2\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u672a\u77e5\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u9002\u5e94\u65b0\u4efb\u52a1\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u8017\u65f6\u4e14\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3002\u672c\u6587\u63a2\u7d22\u4e00\u79cd\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u4efb\u52a1\u6807\u8bc6\u751f\u6210\u53c2\u6570\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u7a7a\u95f4\u7684\u7ed3\u6784\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u6807\u8bc6\u751f\u6210\u53c2\u6570\u3002\u6a21\u578b\u8bad\u7ec3\u540e\uff0c\u53ef\u76f4\u63a5\u4e3a\u5df2\u77e5\u4efb\u52a1\u3001\u591a\u4efb\u52a1\u548c\u672a\u77e5\u4efb\u52a1\u751f\u6210\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u80fd\u4e3a\u5df2\u77e5\u4efb\u52a1\u751f\u6210\u51c6\u786e\u53c2\u6570\uff0c\u5e76\u652f\u6301\u591a\u4efb\u52a1\u63d2\u503c\uff0c\u4f46\u5728\u672a\u77e5\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u672a\u77e5\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u6539\u8fdb\u3002", "paper_title_zh": "\u5229\u7528\u6269\u6563\u6a21\u578b\u91cd\u65b0\u63a2\u7d22\u53c2\u6570\u7a7a\u95f4", "abstract_zh": "\u795e\u7ecf\u7f51\u7edc\u9002\u5e94\u65b0\u4efb\u52a1\u901a\u5e38\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u66ff\u4ee3\u65b9\u6848\uff0c\u76f4\u63a5\u4ece\u4efb\u52a1\u6807\u8bc6\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u6709\u6548\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u7a7a\u95f4\u7684\u57fa\u7840\u7ed3\u6784\uff0c\u5e76\u6309\u9700\u5408\u6210\u53c2\u6570\u3002\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u4efb\u52a1\u6761\u4ef6\u6269\u6563\u6a21\u578b\u53ef\u76f4\u63a5\u4ece\u4efb\u52a1\u6807\u8bc6\u751f\u6210\u4e13\u7528\u6743\u91cd\u3002\u6211\u4eec\u5728\u4e09\u79cd\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff1a\u4e3a\u5355\u4e2a\u5df2\u77e5\u4efb\u52a1\u3001\u591a\u4e2a\u5df2\u77e5\u4efb\u52a1\u548c\u5b8c\u5168\u672a\u77e5\u4efb\u52a1\u751f\u6210\u53c2\u6570\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u53c2\u6570\u5b50\u7a7a\u95f4\u7ed3\u6784\u826f\u597d\u65f6\uff0c\u6269\u6563\u6a21\u578b\u80fd\u751f\u6210\u51c6\u786e\u7684\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u5e76\u652f\u6301\u591a\u4efb\u52a1\u63d2\u503c\uff0c\u4f46\u5728\u672a\u77e5\u4efb\u52a1\u4e0a\u65e0\u6cd5\u6cdb\u5316\uff0c\u51f8\u663e\u4e86\u8fd9\u4e00\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002"}}
{"id": "2506.18792", "pdf": "https://arxiv.org/pdf/2506.18792", "abs": "https://arxiv.org/abs/2506.18792", "authors": ["Michal Nazarczuk", "Sibi Catley-Chandar", "Thomas Tanay", "Zhensong Zhang", "Gregory Slabaugh", "Eduardo P\u00e9rez-Pellitero"], "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving\nsubjects from arbitrary viewpoints. This task is particularly challenging when\nrelying on monocular video, where disentangling structure from motion is\nill-posed and supervision is scarce. We introduce Video Diffusion-Aware\nReconstruction (ViDAR), a novel 4D reconstruction framework that leverages\npersonalised diffusion models to synthesise a pseudo multi-view supervision\nsignal for training a Gaussian splatting representation. By conditioning on\nscene-specific features, ViDAR recovers fine-grained appearance details while\nmitigating artefacts introduced by monocular ambiguity. To address the\nspatio-temporal inconsistency of diffusion-based supervision, we propose a\ndiffusion-aware loss function and a camera pose optimisation strategy that\naligns synthetic views with the underlying scene geometry. Experiments on\nDyCheck, a challenging benchmark with extreme viewpoint variation, show that\nViDAR outperforms all state-of-the-art baselines in visual quality and\ngeometric consistency. We further highlight ViDAR's strong improvement over\nbaselines on dynamic regions and provide a new benchmark to compare performance\nin reconstructing motion-rich parts of the scene. Project page:\nhttps://vidar-4d.github.io", "AI": {"tldr": "ViDAR\u662f\u4e00\u79cd\u65b0\u9896\u76844D\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u573a\u666f\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u5728\u5355\u76ee\u89c6\u9891\u8f93\u5165\u4e0b\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4ece\u8fd0\u52a8\u4e2d\u5206\u79bb\u7ed3\u6784\u662f\u4e0d\u9002\u5b9a\u7684\u4e14\u76d1\u7763\u4fe1\u53f7\u7a00\u7f3a\u3002ViDAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "ViDAR\u7ed3\u5408\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u8bad\u7ec3\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u3002\u901a\u8fc7\u573a\u666f\u7279\u5b9a\u7279\u5f81\u6761\u4ef6\u5316\uff0c\u6062\u590d\u7ec6\u8282\u5e76\u51cf\u5c11\u5355\u76ee\u6a21\u7cca\u6027\u5e26\u6765\u7684\u4f2a\u5f71\u3002\u63d0\u51fa\u6269\u6563\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u76f8\u673a\u59ff\u6001\u4f18\u5316\u7b56\u7565\uff0c\u5bf9\u9f50\u5408\u6210\u89c6\u56fe\u4e0e\u573a\u666f\u51e0\u4f55\u3002", "result": "\u5728DyCheck\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViDAR\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u52a8\u6001\u533a\u57df\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u8fd0\u52a8\u4e30\u5bcc\u573a\u666f\u91cd\u5efa\u6027\u80fd\u6bd4\u8f83\u57fa\u51c6\u3002", "conclusion": "ViDAR\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u76844D\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u5355\u76ee\u8f93\u5165\u4e0b\u7684\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "ViDAR\uff1a\u57fa\u4e8e\u5355\u76ee\u8f93\u5165\u7684\u89c6\u9891\u6269\u6563\u611f\u77e54D\u91cd\u5efa", "abstract_zh": "\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u65e8\u5728\u4ece\u4efb\u610f\u89c6\u89d2\u751f\u6210\u8fd0\u52a8\u4e3b\u4f53\u7684\u903c\u771f\u89c6\u56fe\u3002\u5728\u4f9d\u8d56\u5355\u76ee\u89c6\u9891\u65f6\uff0c\u8fd9\u4e00\u4efb\u52a1\u5c24\u4e3a\u56f0\u96be\uff0c\u56e0\u4e3a\u4ece\u8fd0\u52a8\u4e2d\u5206\u79bb\u7ed3\u6784\u662f\u4e0d\u9002\u5b9a\u7684\u4e14\u76d1\u7763\u4fe1\u53f7\u7a00\u7f3a\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u89c6\u9891\u6269\u6563\u611f\u77e5\u91cd\u5efa\uff08ViDAR\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u76844D\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u7528\u4e8e\u8bad\u7ec3\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u3002\u901a\u8fc7\u573a\u666f\u7279\u5b9a\u7279\u5f81\u6761\u4ef6\u5316\uff0cViDAR\u6062\u590d\u4e86\u7ec6\u7c92\u5ea6\u7684\u5916\u89c2\u7ec6\u8282\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5355\u76ee\u6a21\u7cca\u6027\u5f15\u5165\u7684\u4f2a\u5f71\u3002\u4e3a\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u7684\u76d1\u7763\u7684\u65f6\u7a7a\u4e0d\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6269\u6563\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u76f8\u673a\u59ff\u6001\u4f18\u5316\u7b56\u7565\uff0c\u5bf9\u9f50\u5408\u6210\u89c6\u56fe\u4e0e\u5e95\u5c42\u573a\u666f\u51e0\u4f55\u3002\u5728\u5177\u6709\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u7684DyCheck\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViDAR\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u5747\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86ViDAR\u5728\u52a8\u6001\u533a\u57df\u4e0a\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u7528\u4e8e\u6bd4\u8f83\u8fd0\u52a8\u4e30\u5bcc\u573a\u666f\u90e8\u5206\u7684\u91cd\u5efa\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://vidar-4d.github.io"}}
{"id": "2506.17811", "pdf": "https://arxiv.org/pdf/2506.17811", "abs": "https://arxiv.org/abs/2506.17811", "authors": ["Jacky Kwok", "Christopher Agia", "Rohan Sinha", "Matt Foutter", "Shulu Li", "Ion Stoica", "Azalia Mirhoseini", "Marco Pavone"], "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities\nin visuomotor control, yet ensuring their robustness in unstructured real-world\nenvironments remains a persistent challenge. In this paper, we investigate\ntest-time scaling through the lens of sampling and verification as means to\nenhance the robustness and generalization of VLAs. We first demonstrate that\nthe relationship between action error and the number of generated samples\nfollows an exponentiated power law across a range of VLAs, indicating the\nexistence of inference-time scaling laws. Building on these insights, we\nintroduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,\nRoboMonkey samples a small set of actions from a VLA, applies Gaussian\nperturbation and majority voting to construct an action proposal distribution,\nand then uses a Vision Language Model (VLM)-based verifier to select the\noptimal action. We propose a synthetic data generation pipeline for training\nsuch VLM-based action verifiers, and demonstrate that scaling the synthetic\ndataset consistently improves verification and downstream accuracy. Through\nextensive simulated and hardware experiments, we show that pairing existing\nVLAs with RoboMonkey yields significant performance gains, achieving a 25%\nabsolute improvement on out-of-distribution tasks and 8% on in-distribution\ntasks. Additionally, when adapting to new robot setups, we show that\nfine-tuning both VLAs and action verifiers yields a 7% performance increase\ncompared to fine-tuning VLAs alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRoboMonkey\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u91c7\u6837\u548c\u9a8c\u8bc1\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6d4b\u8bd5\u65f6\u91c7\u6837\u548c\u9a8c\u8bc1\u589e\u5f3aVLA\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faRoboMonkey\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u5c11\u91cf\u52a8\u4f5c\u3001\u9ad8\u65af\u6270\u52a8\u548c\u591a\u6570\u6295\u7968\u6784\u5efa\u52a8\u4f5c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9a8c\u8bc1\u5668\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\u8bad\u7ec3VLM\u9a8c\u8bc1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoboMonkey\u663e\u8457\u63d0\u5347VLA\u6027\u80fd\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u7edd\u5bf9\u63d0\u534725%\uff0c\u5206\u5e03\u5185\u4efb\u52a1\u63d0\u53478%\u3002\u9002\u5e94\u65b0\u673a\u5668\u4eba\u8bbe\u7f6e\u65f6\uff0c\u8054\u5408\u5fae\u8c03VLA\u548c\u9a8c\u8bc1\u5668\u6bd4\u5355\u72ec\u5fae\u8c03VLA\u6027\u80fd\u63d0\u53477%\u3002", "conclusion": "RoboMonkey\u901a\u8fc7\u6d4b\u8bd5\u65f6\u91c7\u6837\u548c\u9a8c\u8bc1\u6709\u6548\u63d0\u5347VLA\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "RoboMonkey\uff1a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u91c7\u6837\u4e0e\u9a8c\u8bc1\u6269\u5c55", "abstract_zh": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\uff0c\u4f46\u786e\u4fdd\u5176\u5728\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4ecd\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218\u3002\u672c\u6587\u4ece\u91c7\u6837\u548c\u9a8c\u8bc1\u7684\u89d2\u5ea6\u7814\u7a76\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u4ee5\u589e\u5f3aVLA\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u9996\u5148\u8bc1\u660e\uff0c\u52a8\u4f5c\u8bef\u5dee\u4e0e\u751f\u6210\u6837\u672c\u6570\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u9075\u5faa\u6307\u6570\u5e42\u5f8b\uff0c\u8868\u660e\u5b58\u5728\u63a8\u7406\u65f6\u6269\u5c55\u89c4\u5f8b\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51faRoboMonkey\uff0c\u4e00\u4e2a\u7528\u4e8eVLA\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\u3002\u5728\u90e8\u7f72\u65f6\uff0cRoboMonkey\u4eceVLA\u4e2d\u91c7\u6837\u5c11\u91cf\u52a8\u4f5c\uff0c\u5e94\u7528\u9ad8\u65af\u6270\u52a8\u548c\u591a\u6570\u6295\u7968\u6784\u5efa\u52a8\u4f5c\u63d0\u8bae\u5206\u5e03\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9a8c\u8bc1\u5668\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bad\u7ec3\u6b64\u7c7bVLM\u52a8\u4f5c\u9a8c\u8bc1\u5668\uff0c\u5e76\u8bc1\u660e\u6269\u5c55\u5408\u6210\u6570\u636e\u96c6\u80fd\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u548c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u5c06\u73b0\u6709VLA\u4e0eRoboMonkey\u7ed3\u5408\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u5b9e\u73b025%\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u5206\u5e03\u5185\u4efb\u52a1\u63d0\u53478%\u3002\u6b64\u5916\uff0c\u5728\u9002\u5e94\u65b0\u673a\u5668\u4eba\u8bbe\u7f6e\u65f6\uff0c\u8054\u5408\u5fae\u8c03VLA\u548c\u52a8\u4f5c\u9a8c\u8bc1\u5668\u6bd4\u5355\u72ec\u5fae\u8c03VLA\u6027\u80fd\u63d0\u53477%\u3002"}}
{"id": "2506.18798", "pdf": "https://arxiv.org/pdf/2506.18798", "abs": "https://arxiv.org/abs/2506.18798", "authors": ["Helin Cao", "Sven Behnke"], "title": "OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "under review", "summary": "Autonomous driving perception faces significant challenges due to occlusions\nand incomplete scene data in the environment. To overcome these issues, the\ntask of semantic occupancy prediction (SOP) is proposed, which aims to jointly\ninfer both the geometry and semantic labels of a scene from images. However,\nconventional camera-based methods typically treat all categories equally and\nprimarily rely on local features, leading to suboptimal predictions, especially\nfor dynamic foreground objects. To address this, we propose Object-Centric SOP\n(OC-SOP), a framework that integrates high-level object-centric cues extracted\nvia a detection branch into the semantic occupancy prediction pipeline. This\nobject-centric integration significantly enhances the prediction accuracy for\nforeground objects and achieves state-of-the-art performance among all\ncategories on SemanticKITTI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOC-SOP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u7684\u9ad8\u5c42\u5bf9\u8c61\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u89c6\u89c9\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u52a8\u6001\u524d\u666f\u7269\u4f53\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u9762\u4e34\u73af\u5883\u906e\u6321\u548c\u573a\u666f\u6570\u636e\u4e0d\u5b8c\u6574\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u76f8\u673a\u7684\u65b9\u6cd5\u5bf9\u6240\u6709\u7c7b\u522b\u4e00\u89c6\u540c\u4ec1\uff0c\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u7279\u5f81\uff0c\u5bfc\u81f4\u52a8\u6001\u524d\u666f\u7269\u4f53\u7684\u9884\u6d4b\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u5bf9\u8c61\u4e2d\u5fc3\u4fe1\u606f\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86OC-SOP\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2a\u68c0\u6d4b\u5206\u652f\u63d0\u53d6\u9ad8\u5c42\u5bf9\u8c61\u4e2d\u5fc3\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u6d41\u7a0b\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u524d\u666f\u7269\u4f53\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "OC-SOP\u663e\u8457\u63d0\u5347\u4e86\u524d\u666f\u7269\u4f53\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6240\u6709\u7c7b\u522b\u7684\u9886\u5148\u6027\u80fd\u3002", "conclusion": "OC-SOP\u901a\u8fc7\u6574\u5408\u5bf9\u8c61\u4e2d\u5fc3\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u6001\u524d\u666f\u7269\u4f53\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "OC-SOP\uff1a\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u611f\u77e5\u589e\u5f3a\u57fa\u4e8e\u89c6\u89c9\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b", "abstract_zh": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u56e0\u73af\u5883\u4e2d\u7684\u906e\u6321\u548c\u573a\u666f\u6570\u636e\u4e0d\u5b8c\u6574\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8bed\u4e49\u5360\u636e\u9884\u6d4b\uff08SOP\uff09\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u56fe\u50cf\u4e2d\u8054\u5408\u63a8\u65ad\u573a\u666f\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u6807\u7b7e\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u76f8\u673a\u7684\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u7c7b\u522b\u4e00\u89c6\u540c\u4ec1\uff0c\u5e76\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u7279\u5f81\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5bf9\u52a8\u6001\u524d\u666f\u7269\u4f53\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u8c61\u4e2d\u5fc3SOP\uff08OC-SOP\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u68c0\u6d4b\u5206\u652f\u63d0\u53d6\u9ad8\u5c42\u5bf9\u8c61\u4e2d\u5fc3\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u6d41\u7a0b\u4e2d\u3002\u8fd9\u79cd\u5bf9\u8c61\u4e2d\u5fc3\u7684\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u524d\u666f\u7269\u4f53\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6240\u6709\u7c7b\u522b\u7684\u9886\u5148\u6027\u80fd\u3002"}}
{"id": "2506.17818", "pdf": "https://arxiv.org/pdf/2506.17818", "abs": "https://arxiv.org/abs/2506.17818", "authors": ["Angelos-Nikolaos Kanatas", "Charilaos Papaioannou", "Alexandros Potamianos"], "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "10 pages, 4 figures, accepted to the 26th International Society for\n  Music Information Retrieval conference (ISMIR 2025), to be held in Daejeon,\n  South Korea", "summary": "Recent advances in music foundation models have improved audio representation\nlearning, yet their effectiveness across diverse musical traditions remains\nlimited. We introduce CultureMERT-95M, a multi-culturally adapted foundation\nmodel developed to enhance cross-cultural music representation learning and\nunderstanding. To achieve this, we propose a two-stage continual pre-training\nstrategy that integrates learning rate re-warming and re-decaying, enabling\nstable adaptation even with limited computational resources. Training on a\n650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music\ntraditions, results in an average improvement of 4.9% in ROC-AUC and AP across\ndiverse non-Western music auto-tagging tasks, surpassing prior\nstate-of-the-art, with minimal forgetting on Western-centric benchmarks. We\nfurther investigate task arithmetic, an alternative approach to multi-cultural\nadaptation that merges single-culture adapted models in the weight space. Task\narithmetic performs on par with our multi-culturally trained model on\nnon-Western auto-tagging tasks and shows no regression on Western datasets.\nCross-cultural evaluation reveals that single-culture models transfer with\nvarying effectiveness across musical traditions, whereas the multi-culturally\nadapted model achieves the best overall performance. To support research on\nworld music representation learning, we publicly release CultureMERT-95M and\nCultureMERT-TA-95M, fostering the development of more culturally aware music\nfoundation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCultureMERT-95M\u7684\u591a\u6587\u5316\u9002\u5e94\u97f3\u4e50\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u8de8\u6587\u5316\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u5728\u975e\u897f\u65b9\u97f3\u4e50\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u897f\u65b9\u97f3\u4e50\u6570\u636e\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u4e50\u57fa\u7840\u6a21\u578b\u5728\u8de8\u6587\u5316\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6587\u5316\u9002\u5e94\u7b56\u7565\u63d0\u5347\u6a21\u578b\u5bf9\u591a\u6837\u5316\u97f3\u4e50\u4f20\u7edf\u7684\u7406\u89e3\u548c\u8868\u793a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u5b66\u4e60\u7387\u91cd\u65b0\u5347\u6e29\u4e0e\u8870\u51cf\uff0c\u8bad\u7ec3\u4e8e\u5305\u542b\u5e0c\u814a\u3001\u571f\u8033\u5176\u548c\u5370\u5ea6\u97f3\u4e50\u7684650\u5c0f\u65f6\u591a\u6587\u5316\u6570\u636e\u96c6\u3002\u540c\u65f6\u63a2\u7d22\u4e86\u4efb\u52a1\u7b97\u672f\u65b9\u6cd5\uff0c\u5c06\u5355\u6587\u5316\u9002\u5e94\u6a21\u578b\u5728\u6743\u91cd\u7a7a\u95f4\u4e2d\u5408\u5e76\u3002", "result": "CultureMERT-95M\u5728\u975e\u897f\u65b9\u97f3\u4e50\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u53474.9%\u7684ROC-AUC\u548cAP\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u5bf9\u897f\u65b9\u97f3\u4e50\u6570\u636e\u9057\u5fd8\u6700\u5c0f\u3002\u4efb\u52a1\u7b97\u672f\u65b9\u6cd5\u5728\u975e\u897f\u65b9\u4efb\u52a1\u4e2d\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u5bf9\u897f\u65b9\u6570\u636e\u65e0\u9000\u5316\u3002", "conclusion": "\u591a\u6587\u5316\u9002\u5e94\u6a21\u578b\u5728\u8de8\u6587\u5316\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5355\u6587\u5316\u6a21\u578b\u7684\u8fc1\u79fb\u6548\u679c\u56e0\u4f20\u7edf\u800c\u5f02\u3002\u672c\u6587\u516c\u5f00\u4e86CultureMERT-95M\u548cCultureMERT-TA-95M\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5177\u6587\u5316\u610f\u8bc6\u7684\u97f3\u4e50\u57fa\u7840\u6a21\u578b\u7814\u7a76\u3002", "paper_title_zh": "CultureMERT\uff1a\u8de8\u6587\u5316\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u7684\u6301\u7eed\u9884\u8bad\u7ec3", "abstract_zh": "\u5c3d\u7ba1\u97f3\u4e50\u57fa\u7840\u6a21\u578b\u5728\u97f3\u9891\u8868\u793a\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u591a\u6837\u5316\u97f3\u4e50\u4f20\u7edf\u4e2d\u7684\u8868\u73b0\u4ecd\u6709\u9650\u3002\u6211\u4eec\u63d0\u51fa\u4e86CultureMERT-95M\uff0c\u4e00\u79cd\u591a\u6587\u5316\u9002\u5e94\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u8de8\u6587\u5316\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u4e0e\u7406\u89e3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u5b66\u4e60\u7387\u91cd\u65b0\u5347\u6e29\u4e0e\u8870\u51cf\uff0c\u5373\u4f7f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5b9a\u9002\u5e94\u3002\u901a\u8fc7\u5728\u5305\u542b\u5e0c\u814a\u3001\u571f\u8033\u5176\u548c\u5370\u5ea6\u97f3\u4e50\u7684650\u5c0f\u65f6\u591a\u6587\u5316\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u975e\u897f\u65b9\u97f3\u4e50\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684ROC-AUC\u548cAP\u5e73\u5747\u63d0\u5347\u4e864.9%\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5bf9\u897f\u65b9\u57fa\u51c6\u4efb\u52a1\u7684\u9057\u5fd8\u6700\u5c0f\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u4efb\u52a1\u7b97\u672f\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u6743\u91cd\u7a7a\u95f4\u4e2d\u5408\u5e76\u5355\u6587\u5316\u9002\u5e94\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u4efb\u52a1\u7b97\u672f\u5728\u975e\u897f\u65b9\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u591a\u6587\u5316\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u5bf9\u897f\u65b9\u6570\u636e\u96c6\u65e0\u9000\u5316\u3002\u8de8\u6587\u5316\u8bc4\u4f30\u8868\u660e\uff0c\u5355\u6587\u5316\u6a21\u578b\u5728\u4e0d\u540c\u97f3\u4e50\u4f20\u7edf\u4e2d\u7684\u8fc1\u79fb\u6548\u679c\u5404\u5f02\uff0c\u800c\u591a\u6587\u5316\u9002\u5e94\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u4f73\u3002\u4e3a\u652f\u6301\u4e16\u754c\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u7814\u7a76\uff0c\u6211\u4eec\u516c\u5f00\u4e86CultureMERT-95M\u548cCultureMERT-TA-95M\uff0c\u4ee5\u63a8\u52a8\u66f4\u5177\u6587\u5316\u610f\u8bc6\u7684\u97f3\u4e50\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2506.18807", "pdf": "https://arxiv.org/pdf/2506.18807", "abs": "https://arxiv.org/abs/2506.18807", "authors": ["Pietro Bonazzi", "Nicola Farronato", "Stefan Zihlmann", "Haotong Qi", "Michele Magno"], "title": "PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications", "categories": ["cs.CV"], "comment": null, "summary": "Real-time, on-device segmentation is critical for latency-sensitive and\nprivacy-aware applications like smart glasses and IoT devices. We introduce\nPicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation\nmodel optimized for edge and in-sensor execution, including the Sony IMX500. It\nbuilds on a depthwise separable U-Net, with knowledge distillation and\nfixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).\nOn COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized\nmodel (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it\nthe only model meeting both memory and compute constraints for in-sensor\ndeployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.\nThese results demonstrate that efficient, promptable segmentation is feasible\ndirectly on-camera, enabling privacy-preserving vision without cloud or host\nprocessing.", "AI": {"tldr": "PicoSAM2\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u4f4e\u5ef6\u8fdf\u7684\u5206\u5272\u6a21\u578b\uff0c\u4e13\u4e3a\u8fb9\u7f18\u548c\u4f20\u611f\u5668\u5185\u6267\u884c\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u3002", "motivation": "\u5b9e\u65f6\u3001\u8bbe\u5907\u7aef\u7684\u5206\u5272\u5bf9\u4e8e\u5ef6\u8fdf\u654f\u611f\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5e94\u7528\uff08\u5982\u667a\u80fd\u773c\u955c\u548c\u7269\u8054\u7f51\u8bbe\u5907\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bbU-Net\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u56fa\u5b9a\u70b9\u63d0\u793a\u7f16\u7801\uff0c\u4eceSegment Anything Model 2\uff08SAM2\uff09\u4e2d\u5b66\u4e60\u3002", "result": "\u5728COCO\u548cLVIS\u4e0a\u5206\u522b\u8fbe\u523051.9%\u548c44.9%\u7684mIoU\uff0c\u91cf\u5316\u6a21\u578b\uff081.22MB\uff09\u5728IMX500\u4e0a\u8fd0\u884c\u65f6\u95f4\u4e3a14.3\u6beb\u79d2\uff0c\u6ee1\u8db3\u4f20\u611f\u5668\u5185\u90e8\u7f72\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002", "conclusion": "\u9ad8\u6548\u7684\u3001\u53ef\u63d0\u793a\u7684\u5206\u5272\u6a21\u578b\u53ef\u76f4\u63a5\u5728\u76f8\u673a\u4e0a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u89c6\u89c9\u5904\u7406\uff0c\u65e0\u9700\u4e91\u7aef\u6216\u4e3b\u673a\u5904\u7406\u3002", "paper_title_zh": "PicoSAM2\uff1a\u9762\u5411\u8fb9\u7f18\u89c6\u89c9\u5e94\u7528\u7684\u4f20\u611f\u5668\u5185\u4f4e\u5ef6\u8fdf\u5206\u5272", "abstract_zh": "\u5b9e\u65f6\u3001\u8bbe\u5907\u7aef\u7684\u5206\u5272\u5bf9\u4e8e\u5ef6\u8fdf\u654f\u611f\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5e94\u7528\uff08\u5982\u667a\u80fd\u773c\u955c\u548c\u7269\u8054\u7f51\u8bbe\u5907\uff09\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86PicoSAM2\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\uff081.3M\u53c2\u6570\uff0c336M MACs\uff09\u7684\u53ef\u63d0\u793a\u5206\u5272\u6a21\u578b\uff0c\u4e13\u4e3a\u8fb9\u7f18\u548c\u4f20\u611f\u5668\u5185\u6267\u884c\u4f18\u5316\uff0c\u5305\u62ec\u7d22\u5c3cIMX500\u3002\u5b83\u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bbU-Net\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u56fa\u5b9a\u70b9\u63d0\u793a\u7f16\u7801\u4eceSegment Anything Model 2\uff08SAM2\uff09\u4e2d\u5b66\u4e60\u3002\u5728COCO\u548cLVIS\u4e0a\uff0c\u5206\u522b\u8fbe\u523051.9%\u548c44.9%\u7684mIoU\u3002\u91cf\u5316\u6a21\u578b\uff081.22MB\uff09\u5728IMX500\u4e0a\u8fd0\u884c\u65f6\u95f4\u4e3a14.3\u6beb\u79d2\uff0c\u5b9e\u73b0\u4e8686 MACs/cycle\uff0c\u662f\u552f\u4e00\u6ee1\u8db3\u4f20\u611f\u5668\u5185\u90e8\u7f72\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u7684\u6a21\u578b\u3002\u84b8\u998f\u5c06LVIS\u6027\u80fd\u63d0\u5347\u4e86+3.5% mIoU\u548c+5.1% mAP\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u6548\u7684\u3001\u53ef\u63d0\u793a\u7684\u5206\u5272\u53ef\u76f4\u63a5\u5728\u76f8\u673a\u4e0a\u5b9e\u73b0\uff0c\u65e0\u9700\u4e91\u7aef\u6216\u4e3b\u673a\u5904\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u89c6\u89c9\u5e94\u7528\u3002"}}
{"id": "2506.17823", "pdf": "https://arxiv.org/pdf/2506.17823", "abs": "https://arxiv.org/abs/2506.17823", "authors": ["Kevin Chang", "Rakesh Vivekanandan", "Noah Pragin", "Sean Bullock", "Geoffrey Hollinger"], "title": "Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Advancing Quantitative and Qualitative Simulators for Marine\n  Applications Workshop Paper at International Conference on Robotics and\n  Automation 2025", "summary": "Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain\nenvironments is a critical challenge for underwater robotics. Reinforcement\nlearning is a promising method for developing robust controllers, but the\ndisparity between training simulations and the real world, or the sim2real gap,\noften leads to a significant deterioration in performance. In this work, we\nperform a simulation study on reducing the sim2real gap in autonomous docking\nthrough training various controllers and then evaluating them under realistic\ndisturbances. In particular, we focus on the real-world challenge of docking\nunder different payloads that are potentially outside the original training\ndistribution. We explore existing methods for improving robustness including\nrandomization techniques and history-conditioned controllers. Our findings\nprovide insights into mitigating the sim2real gap when training docking\ncontrollers. Furthermore, our work indicates areas of future research that may\nbe beneficial to the marine robotics community.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u7f29\u5c0f\u81ea\u4e3b\u6c34\u4e0b\u5bf9\u63a5\u4e2d\u7684\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u5bf9\u63a5\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\uff08AUV\uff09\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5bf9\u63a5\u662f\u6c34\u4e0b\u673a\u5668\u4eba\u6280\u672f\u7684\u5173\u952e\u6311\u6218\u3002\u5f3a\u5316\u5b66\u4e60\u662f\u5f00\u53d1\u9c81\u68d2\u63a7\u5236\u5668\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\uff08sim2real gap\uff09\u5e38\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u51cf\u5c11\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u5bf9\u63a5\u95ee\u9898\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3\u591a\u79cd\u63a7\u5236\u5668\u5e76\u5728\u771f\u5b9e\u6270\u52a8\u4e0b\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u63a2\u8ba8\u4e86\u968f\u673a\u5316\u6280\u672f\u548c\u5386\u53f2\u6761\u4ef6\u63a7\u5236\u5668\u7b49\u73b0\u6709\u65b9\u6cd5\u5bf9\u63d0\u5347\u9c81\u68d2\u6027\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u8d1f\u8f7d\u6761\u4ef6\u8d85\u51fa\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bad\u7ec3\u5bf9\u63a5\u63a7\u5236\u5668\u65f6\u7f29\u5c0fsim2real\u5dee\u8ddd\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5bf9\u6d77\u6d0b\u673a\u5668\u4eba\u793e\u533a\u6709\u76ca\u7684\u7814\u7a76\u65b9\u5411\u3002", "paper_title_zh": "\u5b66\u4e60\u5bf9\u63a5\uff1a\u57fa\u4e8e\u6a21\u62df\u7684\u7814\u7a76\u7f29\u5c0f\u81ea\u4e3b\u6c34\u4e0b\u5bf9\u63a5\u4e2d\u7684\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd", "abstract_zh": "\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\uff08AUV\uff09\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5bf9\u63a5\u662f\u6c34\u4e0b\u673a\u5668\u4eba\u6280\u672f\u7684\u5173\u952e\u6311\u6218\u3002\u5f3a\u5316\u5b66\u4e60\u662f\u5f00\u53d1\u9c81\u68d2\u63a7\u5236\u5668\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\uff08sim2real gap\uff09\u5e38\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u5bf9\u63a5\u95ee\u9898\u3002\u6211\u4eec\u8bad\u7ec3\u4e86\u591a\u79cd\u63a7\u5236\u5668\uff0c\u5e76\u5728\u771f\u5b9e\u6270\u52a8\u4e0b\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u8d1f\u8f7d\u6761\u4ef6\u8d85\u51fa\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u7684\u60c5\u51b5\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u5305\u62ec\u968f\u673a\u5316\u6280\u672f\u548c\u5386\u53f2\u6761\u4ef6\u63a7\u5236\u5668\u5728\u5185\u7684\u73b0\u6709\u65b9\u6cd5\u5bf9\u63d0\u5347\u9c81\u68d2\u6027\u7684\u6548\u679c\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u8bad\u7ec3\u5bf9\u63a5\u63a7\u5236\u5668\u65f6\u7f29\u5c0fsim2real\u5dee\u8ddd\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5bf9\u6d77\u6d0b\u673a\u5668\u4eba\u793e\u533a\u6709\u76ca\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.18839", "pdf": "https://arxiv.org/pdf/2506.18839", "abs": "https://arxiv.org/abs/2506.18839", "authors": ["Chaoyang Wang", "Ashkan Mirzaei", "Vidit Goel", "Willi Menapace", "Aliaksandr Siarohin", "Avalon Vinella", "Michael Vasilkovsky", "Ivan Skorokhodov", "Vladislav Shakhrai", "Sergey Korolev", "Sergey Tulyakov", "Peter Wonka"], "title": "4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose the first framework capable of computing a 4D spatio-temporal grid\nof video frames and 3D Gaussian particles for each time step using a\nfeed-forward architecture. Our architecture has two main components, a 4D video\nmodel and a 4D reconstruction model. In the first part, we analyze current 4D\nvideo diffusion architectures that perform spatial and temporal attention\neither sequentially or in parallel within a two-stream design. We highlight the\nlimitations of existing approaches and introduce a novel fused architecture\nthat performs spatial and temporal attention within a single layer. The key to\nour method is a sparse attention pattern, where tokens attend to others in the\nsame frame, at the same timestamp, or from the same viewpoint. In the second\npart, we extend existing 3D reconstruction algorithms by introducing a Gaussian\nhead, a camera token replacement algorithm, and additional dynamic layers and\ntraining. Overall, we establish a new state of the art for 4D generation,\nimproving both visual quality and reconstruction capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4Real-Video-V2\u7684\u6846\u67b6\uff0c\u9996\u6b21\u901a\u8fc7\u524d\u9988\u67b6\u6784\u751f\u62104D\u65f6\u7a7a\u89c6\u9891\u5e27\u548c3D\u9ad8\u65af\u7c92\u5b50\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5305\u62ec\u878d\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u7684\u5355\u5c42\u8bbe\u8ba1\u548c\u6539\u8fdb\u76843D\u91cd\u5efa\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u751f\u6210\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u80fd\u529b\u3002", "motivation": "\u5f53\u524d4D\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u65f6\u7a7a\u6ce8\u610f\u529b\u5904\u7406\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u901a\u5e38\u91c7\u7528\u987a\u5e8f\u6216\u5e76\u884c\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u6548\u7387\u548c\u8d28\u91cf\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u878d\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u548c\u6539\u8fdb\u91cd\u5efa\u7b97\u6cd5\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u53474D\u573a\u666f\u751f\u6210\u7684\u8868\u73b0\u3002", "method": "1. \u63d0\u51fa\u878d\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u7684\u5355\u5c42\u8bbe\u8ba1\uff0c\u91c7\u7528\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4f7ftoken\u5728\u540c\u4e00\u5e27\u3001\u65f6\u95f4\u6233\u6216\u89c6\u89d2\u5185\u4ea4\u4e92\u30022. \u6269\u5c553D\u91cd\u5efa\u7b97\u6cd5\uff0c\u5f15\u5165\u9ad8\u65af\u5934\u3001\u76f8\u673atoken\u66ff\u6362\u7b97\u6cd5\u53ca\u52a8\u6001\u5c42\u548c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4Real-Video-V2\u57284D\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u4f73\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u878d\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u8bbe\u8ba1\u548c\u6539\u8fdb\u91cd\u5efa\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a4D\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "4Real-Video-V2\uff1a\u878d\u5408\u89c6\u89d2-\u65f6\u95f4\u6ce8\u610f\u529b\u4e0e\u524d\u9988\u91cd\u5efa\u76844D\u573a\u666f\u751f\u6210", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u901a\u8fc7\u524d\u9988\u67b6\u6784\u8ba1\u7b974D\u65f6\u7a7a\u89c6\u9891\u5e27\u548c\u6bcf\u65f6\u95f4\u6b653D\u9ad8\u65af\u7c92\u5b50\u7684\u6846\u67b6\u3002\u8be5\u67b6\u6784\u5305\u542b\u4e24\u90e8\u5206\uff1a4D\u89c6\u9891\u6a21\u578b\u548c4D\u91cd\u5efa\u6a21\u578b\u3002\u5728\u7b2c\u4e00\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5f53\u524d4D\u89c6\u9891\u6269\u6563\u67b6\u6784\uff0c\u8fd9\u4e9b\u67b6\u6784\u5728\u53cc\u6d41\u8bbe\u8ba1\u4e2d\u987a\u5e8f\u6216\u5e76\u884c\u6267\u884c\u7a7a\u95f4\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u3002\u6211\u4eec\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u878d\u5408\u67b6\u6784\uff0c\u5728\u5355\u5c42\u5185\u5b8c\u6210\u7a7a\u95f4\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u3002\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u5728\u4e8e\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5176\u4e2dtoken\u5173\u6ce8\u540c\u4e00\u5e27\u3001\u540c\u4e00\u65f6\u95f4\u6233\u6216\u540c\u4e00\u89c6\u89d2\u7684\u5176\u4ed6token\u3002\u5728\u7b2c\u4e8c\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u9ad8\u65af\u5934\u3001\u76f8\u673atoken\u66ff\u6362\u7b97\u6cd5\u4ee5\u53ca\u989d\u5916\u7684\u52a8\u6001\u5c42\u548c\u8bad\u7ec3\uff0c\u6269\u5c55\u4e86\u73b0\u67093D\u91cd\u5efa\u7b97\u6cd5\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u57284D\u751f\u6210\u4efb\u52a1\u4e2d\u786e\u7acb\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2506.17826", "pdf": "https://arxiv.org/pdf/2506.17826", "abs": "https://arxiv.org/abs/2506.17826", "authors": ["Zhongtian Sun", "Anoushka Harit", "Pietro Lio"], "title": "Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While the impact of batch size on generalisation is well studied in vision\ntasks, its causal mechanisms remain underexplored in graph and text domains. We\nintroduce a hypergraph-based causal framework, HGCNet, that leverages deep\nstructural causal models (DSCMs) to uncover how batch size influences\ngeneralisation via gradient noise, minima sharpness, and model complexity.\nUnlike prior approaches based on static pairwise dependencies, HGCNet employs\nhypergraphs to capture higher-order interactions across training dynamics.\nUsing do-calculus, we quantify direct and mediated effects of batch size\ninterventions, providing interpretable, causally grounded insights into\noptimisation. Experiments on citation networks, biomedical text, and e-commerce\nreviews show that HGCNet outperforms strong baselines including GCN, GAT,\nPI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes\ncausally enhance generalisation through increased stochasticity and flatter\nminima, offering actionable interpretability to guide training strategies in\ndeep learning. This work positions interpretability as a driver of principled\narchitectural and optimisation choices beyond post hoc analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u56e0\u679c\u6846\u67b6HGCNet\uff0c\u7528\u4e8e\u63ed\u793a\u6279\u91cf\u5927\u5c0f\u5982\u4f55\u901a\u8fc7\u68af\u5ea6\u566a\u58f0\u3001\u6781\u5c0f\u503c\u9510\u5ea6\u548c\u6a21\u578b\u590d\u6742\u6027\u5f71\u54cd\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5c0f\u6279\u91cf\u80fd\u901a\u8fc7\u589e\u52a0\u968f\u673a\u6027\u548c\u5e73\u5766\u6781\u5c0f\u503c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6279\u91cf\u5927\u5c0f\u5bf9\u89c6\u89c9\u4efb\u52a1\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5176\u5728\u56fe\u548c\u6587\u672c\u9886\u57df\u7684\u56e0\u679c\u673a\u5236\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u5206\u6790\u4ee5\u6307\u5bfc\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u63d0\u51faHGCNet\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08DSCMs\uff09\u548c\u8d85\u56fe\u6355\u6349\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\uff0c\u901a\u8fc7do-\u6f14\u7b97\u91cf\u5316\u6279\u91cf\u5927\u5c0f\u5e72\u9884\u7684\u76f4\u63a5\u548c\u95f4\u63a5\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHGCNet\u5728\u5f15\u6587\u7f51\u7edc\u3001\u751f\u7269\u533b\u5b66\u6587\u672c\u548c\u7535\u5546\u8bc4\u8bba\u6570\u636e\u4e0a\u4f18\u4e8eGCN\u3001GAT\u3001PI-GNN\u3001BERT\u548cRoBERTa\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u63ed\u793a\u5c0f\u6279\u91cf\u901a\u8fc7\u589e\u52a0\u968f\u673a\u6027\u548c\u5e73\u5766\u6781\u5c0f\u503c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u53ef\u89e3\u91ca\u6027\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u4f18\u5316\u9009\u62e9\u7684\u9a71\u52a8\u529b\uff0c\u4e3a\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u57fa\u4e8e\u56e0\u679c\u5206\u6790\u7684\u5b9e\u9645\u6307\u5bfc\u3002", "paper_title_zh": "\u57fa\u4e8e\u56e0\u679c\u8d85\u56fe\u7684\u53ef\u64cd\u4f5c\u53ef\u89e3\u91ca\u6027\uff1a\u63ed\u793a\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6279\u91cf\u5927\u5c0f\u6548\u5e94", "abstract_zh": "\u5c3d\u7ba1\u6279\u91cf\u5927\u5c0f\u5bf9\u89c6\u89c9\u4efb\u52a1\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5176\u5728\u56fe\u548c\u6587\u672c\u9886\u57df\u7684\u56e0\u679c\u673a\u5236\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u56e0\u679c\u6846\u67b6HGCNet\uff0c\u5229\u7528\u6df1\u5ea6\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08DSCMs\uff09\u63ed\u793a\u6279\u91cf\u5927\u5c0f\u5982\u4f55\u901a\u8fc7\u68af\u5ea6\u566a\u58f0\u3001\u6781\u5c0f\u503c\u9510\u5ea6\u548c\u6a21\u578b\u590d\u6742\u6027\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u3002\u4e0e\u4ee5\u5f80\u57fa\u4e8e\u9759\u6001\u6210\u5bf9\u4f9d\u8d56\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cHGCNet\u91c7\u7528\u8d85\u56fe\u6355\u6349\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\u3002\u901a\u8fc7do-\u6f14\u7b97\uff0c\u6211\u4eec\u91cf\u5316\u4e86\u6279\u91cf\u5927\u5c0f\u5e72\u9884\u7684\u76f4\u63a5\u548c\u95f4\u63a5\u6548\u5e94\uff0c\u4e3a\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u5206\u6790\u3002\u5728\u5f15\u6587\u7f51\u7edc\u3001\u751f\u7269\u533b\u5b66\u6587\u672c\u548c\u7535\u5546\u8bc4\u8bba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGCNet\u4f18\u4e8eGCN\u3001GAT\u3001PI-GNN\u3001BERT\u548cRoBERTa\u7b49\u57fa\u7ebf\u6a21\u578b\u3002\u5206\u6790\u63ed\u793a\uff0c\u8f83\u5c0f\u7684\u6279\u91cf\u5927\u5c0f\u901a\u8fc7\u589e\u52a0\u968f\u673a\u6027\u548c\u5e73\u5766\u6781\u5c0f\u503c\u56e0\u679c\u6027\u5730\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u5c06\u53ef\u89e3\u91ca\u6027\u4f5c\u4e3a\u67b6\u6784\u548c\u4f18\u5316\u9009\u62e9\u7684\u9a71\u52a8\u529b\uff0c\u8d85\u8d8a\u4e86\u4e8b\u540e\u5206\u6790\u7684\u8303\u7574\u3002"}}
{"id": "2506.18851", "pdf": "https://arxiv.org/pdf/2506.18851", "abs": "https://arxiv.org/abs/2506.18851", "authors": ["Zhuowei Chen", "Bingchuan Li", "Tianxiang Ma", "Lijie Liu", "Mingcong Liu", "Yi Zhang", "Gen Li", "Xinghui Li", "Siyu Zhou", "Qian He", "Xinglong Wu"], "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset", "categories": ["cs.CV"], "comment": "Project page:https://phantom-video.github.io/Phantom-Data/", "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce \\textbf{Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset}, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Phantom-Data\uff0c\u9996\u4e2a\u901a\u7528\u8de8\u914d\u5bf9\u4e3b\u9898\u4e00\u81f4\u89c6\u9891\u751f\u6210\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u56e0\u914d\u5bf9\u8bad\u7ec3\u5bfc\u81f4\u7684\u4e3b\u9898\u4e0e\u80cc\u666f\u7ea0\u7f20\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u6307\u4ee4\u7684\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u56e0\u914d\u5bf9\u8bad\u7ec3\u8303\u5f0f\u5bfc\u81f4\u4e3b\u9898\u8eab\u4efd\u4e0e\u80cc\u666f\u5c5e\u6027\u7ea0\u7f20\uff0c\u96be\u4ee5\u5fe0\u5b9e\u9075\u5faa\u6587\u672c\u6307\u4ee4\uff08\u5373\u201c\u590d\u5236\u7c98\u8d34\u95ee\u9898\u201d\uff09\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u8de8\u914d\u5bf9\u6570\u636e\u96c6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u6784\u5efaPhantom-Data\u6570\u636e\u96c6\uff1a(1)\u901a\u7528\u8f93\u5165\u5bf9\u9f50\u4e3b\u9898\u68c0\u6d4b\u6a21\u5757\uff1b(2)\u4ece5300\u4e07\u89c6\u9891\u548c30\u4ebf\u56fe\u50cf\u4e2d\u68c0\u7d22\u8de8\u4e0a\u4e0b\u6587\u4e3b\u9898\uff1b(3)\u57fa\u4e8e\u5148\u9a8c\u7684\u8eab\u4efd\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528Phantom-Data\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u914d\u5bf9\u57fa\u7ebf\u76f8\u5f53\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "conclusion": "Phantom-Data\u4e3a\u8de8\u914d\u5bf9\u4e3b\u9898\u4e00\u81f4\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9996\u4e2a\u901a\u7528\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "paper_title_zh": "Phantom-Data\uff1a\u8fc8\u5411\u901a\u7528\u4e3b\u9898\u4e00\u81f4\u89c6\u9891\u751f\u6210\u6570\u636e\u96c6", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u5fe0\u5b9e\u9075\u5faa\u6587\u672c\u6307\u4ee4\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e00\u9650\u5236\uff08\u901a\u5e38\u79f0\u4e3a\u201c\u590d\u5236\u7c98\u8d34\u95ee\u9898\u201d\uff09\u6e90\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u914d\u5bf9\u8bad\u7ec3\u8303\u5f0f\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u76ee\u6807\u89c6\u9891\u540c\u4e00\u573a\u666f\u4e2d\u91c7\u6837\u53c2\u8003\u56fe\u50cf\uff0c\u5c06\u4e3b\u9898\u8eab\u4efd\u4e0e\u80cc\u666f\u548c\u4e0a\u4e0b\u6587\u5c5e\u6027\u7ea0\u7f20\u5728\u4e00\u8d77\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\\textbf{Phantom-Data\uff0c\u9996\u4e2a\u901a\u7528\u8de8\u914d\u5bf9\u4e3b\u9898\u4e00\u81f4\u89c6\u9891\u751f\u6210\u6570\u636e\u96c6}\uff0c\u5305\u542b\u8de8\u591a\u6837\u7c7b\u522b\u7684\u7ea6\u4e00\u767e\u4e07\u8eab\u4efd\u4e00\u81f4\u914d\u5bf9\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u6784\u5efa\uff1a(1)\u901a\u7528\u4e14\u8f93\u5165\u5bf9\u9f50\u7684\u4e3b\u9898\u68c0\u6d4b\u6a21\u5757\uff1b(2)\u4ece\u8d85\u8fc75300\u4e07\u89c6\u9891\u548c30\u4ebf\u56fe\u50cf\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u8de8\u4e0a\u4e0b\u6587\u4e3b\u9898\u68c0\u7d22\uff1b(3)\u57fa\u4e8e\u5148\u9a8c\u7684\u8eab\u4efd\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u4e0a\u4e0b\u6587\u53d8\u5316\u4e0b\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528Phantom-Data\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u914d\u5bf9\u57fa\u7ebf\u76f8\u5f53\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.18856", "pdf": "https://arxiv.org/pdf/2506.18856", "abs": "https://arxiv.org/abs/2506.18856", "authors": ["Kuanning Wang", "Yuqian Fu", "Tianyu Wang", "Yanwei Fu", "Longfei Liang", "Yu-Gang Jiang", "Xiangyang Xue"], "title": "RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "Accurate 6D pose estimation is key for robotic manipulation, enabling precise\nobject localization for tasks like grasping. We present RAG-6DPose, a\nretrieval-augmented approach that leverages 3D CAD models as a knowledge base\nby integrating both visual and geometric cues. Our RAG-6DPose roughly contains\nthree stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D\nvisual features from multi-view CAD rendered images and also attaching 3D\npoints; 2) Retrieving relevant CAD features from the knowledge base based on\nthe current query image via our ReSPC module; and 3) Incorporating retrieved\nCAD information to refine pose predictions via retrieval-augmented decoding.\nExperimental results on standard benchmarks and real-world robotic tasks\ndemonstrate the effectiveness and robustness of our approach, particularly in\nhandling occlusions and novel viewpoints. Supplementary material is available\non our project website: https://sressers.github.io/RAG-6DPose .", "AI": {"tldr": "RAG-6DPose\u662f\u4e00\u79cd\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u76846D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u75283D CAD\u6a21\u578b\u4f5c\u4e3a\u77e5\u8bc6\u5e93\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u51e0\u4f55\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u51c6\u786e\u76846D\u59ff\u6001\u4f30\u8ba1\u5bf9\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff08\u5982\u6293\u53d6\uff09\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u548c\u65b0\u89c6\u89d2\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u5229\u7528CAD\u6a21\u578b\u4f5c\u4e3a\u77e5\u8bc6\u5e93\uff0c\u4ee5\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "method": "RAG-6DPose\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u6784\u5efa\u591a\u6a21\u6001CAD\u77e5\u8bc6\u5e93\uff0c\u4ece\u591a\u89c6\u89d2CAD\u6e32\u67d3\u56fe\u50cf\u4e2d\u63d0\u53d62D\u89c6\u89c9\u7279\u5f81\u5e76\u9644\u52a03D\u70b9\uff1b2) \u901a\u8fc7ReSPC\u6a21\u5757\u4ece\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u4e0e\u5f53\u524d\u67e5\u8be2\u56fe\u50cf\u76f8\u5173\u7684CAD\u7279\u5f81\uff1b3) \u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u89e3\u7801\u5c06\u68c0\u7d22\u5230\u7684CAD\u4fe1\u606f\u7528\u4e8e\u4f18\u5316\u59ff\u6001\u9884\u6d4b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cRAG-6DPose\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5904\u7406\u906e\u6321\u548c\u65b0\u89c6\u89d2\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "RAG-6DPose\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e866D\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "RAG-6DPose\uff1a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u76846D\u59ff\u6001\u4f30\u8ba1\u2014\u2014\u5229\u7528CAD\u6a21\u578b\u4f5c\u4e3a\u77e5\u8bc6\u5e93", "abstract_zh": "\u51c6\u786e\u76846D\u59ff\u6001\u4f30\u8ba1\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5173\u952e\uff0c\u80fd\u591f\u4e3a\u6293\u53d6\u7b49\u4efb\u52a1\u63d0\u4f9b\u7cbe\u786e\u7684\u7269\u4f53\u5b9a\u4f4d\u3002\u6211\u4eec\u63d0\u51fa\u4e86RAG-6DPose\uff0c\u8fd9\u662f\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u51e0\u4f55\u7ebf\u7d22\uff0c\u5229\u75283D CAD\u6a21\u578b\u4f5c\u4e3a\u77e5\u8bc6\u5e93\u3002RAG-6DPose\u5927\u81f4\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u901a\u8fc7\u4ece\u591a\u89c6\u89d2CAD\u6e32\u67d3\u56fe\u50cf\u4e2d\u63d0\u53d62D\u89c6\u89c9\u7279\u5f81\u5e76\u9644\u52a03D\u70b9\uff0c\u6784\u5efa\u591a\u6a21\u6001CAD\u77e5\u8bc6\u5e93\uff1b2) \u901a\u8fc7\u6211\u4eec\u7684ReSPC\u6a21\u5757\u4ece\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u4e0e\u5f53\u524d\u67e5\u8be2\u56fe\u50cf\u76f8\u5173\u7684CAD\u7279\u5f81\uff1b3) \u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u89e3\u7801\u5c06\u68c0\u7d22\u5230\u7684CAD\u4fe1\u606f\u7528\u4e8e\u4f18\u5316\u59ff\u6001\u9884\u6d4b\u3002\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u906e\u6321\u548c\u65b0\u89c6\u89d2\u65f6\u8868\u73b0\u7a81\u51fa\u3002\u8865\u5145\u6750\u6599\u53ef\u5728\u6211\u4eec\u7684\u9879\u76ee\u7f51\u7ad9https://sressers.github.io/RAG-6DPose\u4e0a\u83b7\u53d6\u3002"}}
{"id": "2506.17840", "pdf": "https://arxiv.org/pdf/2506.17840", "abs": "https://arxiv.org/abs/2506.17840", "authors": ["Anoushka Harit", "Zhongtian Sun"], "title": "Causal Spherical Hypergraph Networks for Modelling Social Uncertainty", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Human social behaviour is governed by complex interactions shaped by\nuncertainty, causality, and group dynamics. We propose Causal Spherical\nHypergraph Networks (Causal-SphHN), a principled framework for socially\ngrounded prediction that jointly models higher-order structure, directional\ninfluence, and epistemic uncertainty. Our method represents individuals as\nhyperspherical embeddings and group contexts as hyperedges, capturing semantic\nand relational geometry. Uncertainty is quantified via Shannon entropy over von\nMises-Fisher distributions, while temporal causal dependencies are identified\nusing Granger-informed subgraphs. Information is propagated through an angular\nmessage-passing mechanism that respects belief dispersion and directional\nsemantics. Experiments on SNARE (offline networks), PHEME (online discourse),\nand AMIGOS (multimodal affect) show that Causal-SphHN improves predictive\naccuracy, robustness, and calibration over strong baselines. Moreover, it\nenables interpretable analysis of influence patterns and social ambiguity. This\nwork contributes a unified causal-geometric approach for learning under\nuncertainty in dynamic social environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCausal-SphHN\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u793e\u4f1a\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u8d85\u7403\u9762\u5d4c\u5165\u548c\u8d85\u8fb9\u8868\u793a\u4e2a\u4f53\u4e0e\u7fa4\u4f53\u5173\u7cfb\uff0c\u7ed3\u5408\u71b5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528Granger\u56e0\u679c\u5b50\u56fe\u8bc6\u522b\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4eba\u7c7b\u793e\u4ea4\u884c\u4e3a\u53d7\u590d\u6742\u4ea4\u4e92\u5f71\u54cd\uff0c\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u3001\u56e0\u679c\u5173\u7cfb\u548c\u7fa4\u4f53\u52a8\u6001\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u5efa\u6a21\u8fd9\u4e9b\u56e0\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u6355\u6349\u9ad8\u9636\u7ed3\u6784\u3001\u65b9\u5411\u6027\u5f71\u54cd\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "method": "Causal-SphHN\u5c06\u4e2a\u4f53\u8868\u793a\u4e3a\u8d85\u7403\u9762\u5d4c\u5165\uff0c\u7fa4\u4f53\u4e0a\u4e0b\u6587\u8868\u793a\u4e3a\u8d85\u8fb9\uff0c\u901a\u8fc7von Mises-Fisher\u5206\u5e03\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5229\u7528Granger\u56e0\u679c\u5b50\u56fe\u8bc6\u522b\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u901a\u8fc7\u89d2\u5ea6\u6d88\u606f\u4f20\u9012\u673a\u5236\u4f20\u64ad\u4fe1\u606f\u3002", "result": "\u5728SNARE\u3001PHEME\u548cAMIGOS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCausal-SphHN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6821\u51c6\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5f71\u54cd\u6a21\u5f0f\u548c\u793e\u4ea4\u6a21\u7cca\u6027\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56e0\u679c\u51e0\u4f55\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u793e\u4ea4\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "paper_title_zh": "\u56e0\u679c\u8d85\u7403\u9762\u8d85\u56fe\u7f51\u7edc\uff1a\u7528\u4e8e\u5efa\u6a21\u793e\u4f1a\u4e0d\u786e\u5b9a\u6027", "abstract_zh": "\u4eba\u7c7b\u793e\u4ea4\u884c\u4e3a\u53d7\u590d\u6742\u4ea4\u4e92\u5f71\u54cd\uff0c\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u3001\u56e0\u679c\u5173\u7cfb\u548c\u7fa4\u4f53\u52a8\u6001\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u56e0\u679c\u8d85\u7403\u9762\u8d85\u56fe\u7f51\u7edc\uff08Causal-SphHN\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u793e\u4f1a\u80cc\u666f\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u9ad8\u9636\u7ed3\u6784\u3001\u65b9\u5411\u6027\u5f71\u54cd\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u5c06\u4e2a\u4f53\u8868\u793a\u4e3a\u8d85\u7403\u9762\u5d4c\u5165\uff0c\u7fa4\u4f53\u4e0a\u4e0b\u6587\u8868\u793a\u4e3a\u8d85\u8fb9\uff0c\u6355\u6349\u8bed\u4e49\u548c\u5173\u7cfb\u51e0\u4f55\u3002\u4e0d\u786e\u5b9a\u6027\u901a\u8fc7von Mises-Fisher\u5206\u5e03\u7684\u9999\u519c\u71b5\u91cf\u5316\uff0c\u65f6\u95f4\u56e0\u679c\u4f9d\u8d56\u901a\u8fc7Granger\u56e0\u679c\u5b50\u56fe\u8bc6\u522b\u3002\u4fe1\u606f\u901a\u8fc7\u89d2\u5ea6\u6d88\u606f\u4f20\u9012\u673a\u5236\u4f20\u64ad\uff0c\u5c0a\u91cd\u4fe1\u5ff5\u5206\u6563\u548c\u65b9\u5411\u6027\u8bed\u4e49\u3002\u5728SNARE\uff08\u79bb\u7ebf\u7f51\u7edc\uff09\u3001PHEME\uff08\u5728\u7ebf\u8ba8\u8bba\uff09\u548cAMIGOS\uff08\u591a\u6a21\u6001\u60c5\u611f\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCausal-SphHN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6821\u51c6\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5f71\u54cd\u6a21\u5f0f\u548c\u793e\u4ea4\u6a21\u7cca\u6027\u5206\u6790\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u52a8\u6001\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56e0\u679c\u51e0\u4f55\u65b9\u6cd5\u3002"}}
{"id": "2506.18862", "pdf": "https://arxiv.org/pdf/2506.18862", "abs": "https://arxiv.org/abs/2506.18862", "authors": ["Zhongbin Guo", "Yuhao Wang", "Ping Jian", "Xinyue Chen", "Wei Peng", "Ertai E"], "title": "TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 33rd ACM International Conference on Multimedia. Our\n  dataset can be found at https://huggingface.co/datasets/IceInPot/TAMMs", "summary": "Satellite image time-series analysis demands fine-grained spatial-temporal\nreasoning, which remains a challenge for existing multimodal large language\nmodels (MLLMs). In this work, we study the capabilities of MLLMs on a novel\ntask that jointly targets temporal change understanding and future scene\ngeneration, aiming to assess their potential for modeling complex multimodal\ndynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for\nsatellite image change understanding and forecasting, which enhances frozen\nMLLMs with lightweight temporal modules for structured sequence encoding and\ncontextual prompting. To guide future image generation, TAMMs introduces a\nSemantic-Fused Control Injection (SFCI) mechanism that adaptively combines\nhigh-level semantic reasoning and structural priors within an enhanced\nControlNet. This dual-path conditioning enables temporally consistent and\nsemantically grounded image synthesis. Experiments demonstrate that TAMMs\noutperforms strong MLLM baselines in both temporal change understanding and\nfuture image forecasting tasks, highlighting how carefully designed temporal\nreasoning and semantic fusion can unlock the full potential of MLLMs for\nspatio-temporal understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTAMMs\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6a21\u5757\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u7528\u4e8e\u536b\u661f\u56fe\u50cf\u65f6\u5e8f\u53d8\u5316\u7406\u89e3\u548c\u672a\u6765\u573a\u666f\u751f\u6210\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u65f6\u7a7a\u7406\u89e3\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u536b\u661f\u56fe\u50cf\u65f6\u5e8f\u5206\u6790\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u63a8\u7406\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22MLLMs\u5728\u8054\u5408\u4efb\u52a1\uff08\u65f6\u5e8f\u53d8\u5316\u7406\u89e3\u548c\u672a\u6765\u573a\u666f\u751f\u6210\uff09\u4e2d\u7684\u6f5c\u529b\u3002", "method": "TAMMs\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6a21\u5757\u589e\u5f3a\u51bb\u7ed3\u7684MLLMs\uff0c\u652f\u6301\u7ed3\u6784\u5316\u5e8f\u5217\u7f16\u7801\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\uff1b\u5f15\u5165\u8bed\u4e49\u878d\u5408\u63a7\u5236\u6ce8\u5165\uff08SFCI\uff09\u673a\u5236\uff0c\u7ed3\u5408\u9ad8\u7ea7\u8bed\u4e49\u63a8\u7406\u548c\u7ed3\u6784\u5148\u9a8c\uff0c\u751f\u6210\u65f6\u5e8f\u4e00\u81f4\u4e14\u8bed\u4e49\u5408\u7406\u7684\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTAMMs\u5728\u65f6\u5e8f\u53d8\u5316\u7406\u89e3\u548c\u672a\u6765\u56fe\u50cf\u9884\u6d4b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebfMLLMs\uff0c\u9a8c\u8bc1\u4e86\u5176\u65f6\u7a7a\u63a8\u7406\u548c\u8bed\u4e49\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "TAMMs\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u8bed\u4e49\u878d\u5408\uff0c\u5145\u5206\u91ca\u653e\u4e86MLLMs\u5728\u65f6\u7a7a\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "TAMMs\uff1a\u9762\u5411\u536b\u661f\u56fe\u50cf\u53d8\u5316\u7406\u89e3\u4e0e\u9884\u6d4b\u7684\u65f6\u5e8f\u611f\u77e5\u591a\u6a21\u6001\u6a21\u578b", "abstract_zh": "\u536b\u661f\u56fe\u50cf\u65f6\u5e8f\u5206\u6790\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u63a8\u7406\uff0c\u8fd9\u5bf9\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u7814\u7a76MLLMs\u5728\u8054\u5408\u4efb\u52a1\uff08\u65f6\u5e8f\u53d8\u5316\u7406\u89e3\u548c\u672a\u6765\u573a\u666f\u751f\u6210\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u65e8\u5728\u8bc4\u4f30\u5176\u5efa\u6a21\u590d\u6742\u591a\u6a21\u6001\u52a8\u6001\u7684\u6f5c\u529b\u3002\u6211\u4eec\u63d0\u51faTAMMs\uff0c\u4e00\u79cd\u65f6\u5e8f\u611f\u77e5\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6a21\u5757\u589e\u5f3a\u51bb\u7ed3\u7684MLLMs\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u5e8f\u5217\u7f16\u7801\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\u3002\u4e3a\u5f15\u5bfc\u672a\u6765\u56fe\u50cf\u751f\u6210\uff0cTAMMs\u5f15\u5165\u8bed\u4e49\u878d\u5408\u63a7\u5236\u6ce8\u5165\uff08SFCI\uff09\u673a\u5236\uff0c\u5728\u589e\u5f3a\u7684ControlNet\u4e2d\u81ea\u9002\u5e94\u7ed3\u5408\u9ad8\u7ea7\u8bed\u4e49\u63a8\u7406\u548c\u7ed3\u6784\u5148\u9a8c\u3002\u8fd9\u79cd\u53cc\u8def\u5f84\u6761\u4ef6\u652f\u6301\u751f\u6210\u65f6\u5e8f\u4e00\u81f4\u4e14\u8bed\u4e49\u5408\u7406\u7684\u56fe\u50cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTAMMs\u5728\u65f6\u5e8f\u53d8\u5316\u7406\u89e3\u548c\u672a\u6765\u56fe\u50cf\u9884\u6d4b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebfMLLMs\uff0c\u7a81\u663e\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u8bed\u4e49\u878d\u5408\u5982\u4f55\u5145\u5206\u91ca\u653eMLLMs\u5728\u65f6\u7a7a\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17842", "pdf": "https://arxiv.org/pdf/2506.17842", "abs": "https://arxiv.org/abs/2506.17842", "authors": ["Al-Harith Farhad", "Khalil Abuibaid", "Christiane Plociennik", "Achim Wagner", "Martin Ruskowski"], "title": "Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria", "categories": ["cs.RO", "cs.AI"], "comment": "RAAD 2025: 34th International Conference on Robotics in\n  Alpe-Adria-Danube Region", "summary": "Neural networks are often regarded as universal equations that can estimate\nany function. This flexibility, however, comes with the drawback of high\ncomplexity, rendering these networks into black box models, which is especially\nrelevant in safety-centric applications. To that end, we propose a pipeline for\na collaborative robot (Cobot) grasping algorithm that detects relevant tools\nand generates the optimal grasp. To increase the transparency and reliability\nof this approach, we integrate an explainable AI method that provides an\nexplanation for the underlying prediction of a model by extracting the learned\nfeatures and correlating them to corresponding classes from the input. These\nconcepts are then used as additional criteria to ensure the safe handling of\nwork tools. In this paper, we show the consistency of this approach and the\ncriterion for improving the handover position. This approach was tested in an\nindustrial environment, where a camera system was set up to enable a robot to\npick up certain tools and objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u5b66\u4e60\u7684\u534f\u4f5c\u673a\u5668\u4eba\u6293\u53d6\u7b97\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\u65b9\u6cd5\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u786e\u4fdd\u5de5\u5177\u7684\u5b89\u5168\u64cd\u4f5c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7684\u590d\u6742\u6027\u4f7f\u5176\u6210\u4e3a\u9ed1\u76d2\u6a21\u578b\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u95ee\u9898\u3002\u4e3a\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u89e3\u91caAI\u7684\u6293\u53d6\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u673a\u5668\u4eba\u6293\u53d6\u7b97\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\u65b9\u6cd5\u63d0\u53d6\u5b66\u4e60\u7279\u5f81\u5e76\u4e0e\u8f93\u5165\u7c7b\u522b\u5173\u8054\uff0c\u4f5c\u4e3a\u5b89\u5168\u64cd\u4f5c\u7684\u9644\u52a0\u6807\u51c6\u3002", "result": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6293\u53d6\u4f4d\u7f6e\u7684\u51c6\u786e\u6027\uff0c\u5e76\u786e\u4fdd\u5de5\u5177\u7684\u5b89\u5168\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6982\u5ff5\u5b66\u4e60\u63d0\u9ad8\u4e86\u6293\u53d6\u7b97\u6cd5\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "paper_title_zh": "\u57fa\u4e8e\u6982\u5ff5\u5b66\u4e60\u7684\u5b89\u5168\u6807\u51c6\u751f\u6210\u6293\u53d6\u68c0\u6d4b\u4e0e\u4f30\u8ba1", "abstract_zh": "\u795e\u7ecf\u7f51\u7edc\u5e38\u88ab\u89c6\u4e3a\u53ef\u4ee5\u4f30\u8ba1\u4efb\u4f55\u51fd\u6570\u7684\u901a\u7528\u65b9\u7a0b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7075\u6d3b\u6027\u4f34\u968f\u7740\u9ad8\u590d\u6742\u6027\u7684\u7f3a\u70b9\uff0c\u4f7f\u8fd9\u4e9b\u7f51\u7edc\u6210\u4e3a\u9ed1\u76d2\u6a21\u578b\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u673a\u5668\u4eba\uff08Cobot\uff09\u6293\u53d6\u7b97\u6cd5\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u68c0\u6d4b\u76f8\u5173\u5de5\u5177\u5e76\u751f\u6210\u6700\u4f73\u6293\u53d6\u3002\u4e3a\u63d0\u9ad8\u8be5\u65b9\u6cd5\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u4e00\u79cd\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u5b66\u4e60\u7279\u5f81\u5e76\u5c06\u5176\u4e0e\u8f93\u5165\u7c7b\u522b\u5173\u8054\uff0c\u4e3a\u6a21\u578b\u7684\u9884\u6d4b\u63d0\u4f9b\u89e3\u91ca\u3002\u8fd9\u4e9b\u6982\u5ff5\u968f\u540e\u88ab\u7528\u4f5c\u9644\u52a0\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd\u5de5\u5177\u7684\u5b89\u5168\u64cd\u4f5c\u3002\u672c\u6587\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u53ca\u5176\u5bf9\u6539\u8fdb\u4ea4\u63a5\u4f4d\u7f6e\u7684\u51c6\u5219\u3002\u8be5\u65b9\u6cd5\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8bbe\u7f6e\u6444\u50cf\u5934\u7cfb\u7edf\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6293\u53d6\u7279\u5b9a\u5de5\u5177\u548c\u7269\u4f53\u3002"}}
{"id": "2506.18866", "pdf": "https://arxiv.org/pdf/2506.18866", "abs": "https://arxiv.org/abs/2506.18866", "authors": ["Qijun Gan", "Ruizi Yang", "Jianke Zhu", "Shaofei Xue", "Steven Hoi"], "title": "OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project page: https://omni-avatar.github.io/", "summary": "Significant progress has been made in audio-driven human animation, while\nmost existing methods focus mainly on facial movements, limiting their ability\nto create full-body animations with natural synchronization and fluidity. They\nalso struggle with precise prompt control for fine-grained generation. To\ntackle these challenges, we introduce OmniAvatar, an innovative audio-driven\nfull-body video generation model that enhances human animation with improved\nlip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise\nmulti-hierarchical audio embedding strategy to better capture audio features in\nthe latent space, enhancing lip-syncing across diverse scenes. To preserve the\ncapability for prompt-driven control of foundation models while effectively\nincorporating audio features, we employ a LoRA-based training approach.\nExtensive experiments show that OmniAvatar surpasses existing models in both\nfacial and semi-body video generation, offering precise text-based control for\ncreating videos in various domains, such as podcasts, human interactions,\ndynamic scenes, and singing. Our project page is\nhttps://omni-avatar.github.io/.", "AI": {"tldr": "OmniAvatar\u662f\u4e00\u79cd\u521b\u65b0\u7684\u97f3\u9891\u9a71\u52a8\u5168\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\u548cLoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5507\u90e8\u540c\u6b65\u548c\u81ea\u7136\u52a8\u4f5c\u7684\u751f\u6210\u6548\u679c\uff0c\u540c\u65f6\u652f\u6301\u7cbe\u786e\u7684\u6587\u672c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u52a8\u753b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u52a8\u4f5c\uff0c\u96be\u4ee5\u751f\u6210\u81ea\u7136\u540c\u6b65\u7684\u5168\u8eab\u52a8\u753b\uff0c\u4e14\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63d0\u793a\u63a7\u5236\u80fd\u529b\u3002OmniAvatar\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u50cf\u7d20\u7ea7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u97f3\u9891\u7279\u5f81\uff0c\u7ed3\u5408LoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u97f3\u9891\u7279\u5f81\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u6709\u6548\u878d\u5408\uff0c\u540c\u65f6\u4fdd\u7559\u6587\u672c\u63d0\u793a\u63a7\u5236\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOmniAvatar\u5728\u9762\u90e8\u548c\u534a\u8eab\u89c6\u9891\u751f\u6210\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u652f\u6301\u64ad\u5ba2\u3001\u4eba\u9645\u4e92\u52a8\u3001\u52a8\u6001\u573a\u666f\u548c\u6b4c\u5531\u7b49\u591a\u79cd\u9886\u57df\u7684\u89c6\u9891\u751f\u6210\u3002", "conclusion": "OmniAvatar\u901a\u8fc7\u521b\u65b0\u7684\u97f3\u9891\u5d4c\u5165\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210\u7684\u81ea\u7136\u6027\u548c\u63a7\u5236\u7cbe\u5ea6\uff0c\u4e3a\u591a\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002", "paper_title_zh": "OmniAvatar\uff1a\u57fa\u4e8e\u81ea\u9002\u5e94\u8eab\u4f53\u52a8\u753b\u7684\u9ad8\u6548\u97f3\u9891\u9a71\u52a8\u865a\u62df\u5f62\u8c61\u89c6\u9891\u751f\u6210", "abstract_zh": "\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u52a8\u753b\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u52a8\u4f5c\uff0c\u96be\u4ee5\u751f\u6210\u81ea\u7136\u540c\u6b65\u7684\u5168\u8eab\u52a8\u753b\uff0c\u4e14\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63d0\u793a\u63a7\u5236\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OmniAvatar\uff0c\u4e00\u79cd\u521b\u65b0\u7684\u97f3\u9891\u9a71\u52a8\u5168\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u5507\u90e8\u540c\u6b65\u548c\u81ea\u7136\u52a8\u4f5c\u63d0\u5347\u4e86\u4eba\u4f53\u52a8\u753b\u6548\u679c\u3002OmniAvatar\u91c7\u7528\u50cf\u7d20\u7ea7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\uff0c\u66f4\u597d\u5730\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u97f3\u9891\u7279\u5f81\uff0c\u4ece\u800c\u589e\u5f3a\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u5507\u90e8\u540c\u6b65\u6548\u679c\u3002\u4e3a\u5728\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u63d0\u793a\u9a71\u52a8\u63a7\u5236\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u878d\u5408\u97f3\u9891\u7279\u5f81\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u57fa\u4e8eLoRA\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOmniAvatar\u5728\u9762\u90e8\u548c\u534a\u8eab\u89c6\u9891\u751f\u6210\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u652f\u6301\u64ad\u5ba2\u3001\u4eba\u9645\u4e92\u52a8\u3001\u52a8\u6001\u573a\u666f\u548c\u6b4c\u5531\u7b49\u591a\u79cd\u9886\u57df\u7684\u7cbe\u786e\u6587\u672c\u63a7\u5236\u89c6\u9891\u751f\u6210\u3002\u9879\u76ee\u9875\u9762\u8bf7\u8bbf\u95ee\uff1ahttps://omni-avatar.github.io/\u3002"}}
{"id": "2506.17847", "pdf": "https://arxiv.org/pdf/2506.17847", "abs": "https://arxiv.org/abs/2506.17847", "authors": ["Cristian Del Gobbo"], "title": "A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity", "categories": ["cs.LG", "cs.AI"], "comment": "23 Pages, 5 figures, and 6 tables", "summary": "High-quality training data is critical to the performance of machine learning\nmodels, particularly Large Language Models (LLMs). However, obtaining real,\nhigh-quality data can be challenging, especially for smaller organizations and\nearly-stage startups. Synthetic data generators provide a promising solution by\nreplicating the statistical and structural properties of real data while\npreserving privacy and scalability. This study evaluates the performance of six\ntabular synthetic data generators from two widely used open-source libraries:\nSDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,\nTVAE). Using a real-world dataset from the UCI Machine Learning Repository,\ncomprising energy consumption and environmental variables from Belgium, we\nsimulate a low-data regime by training models on only 1,000 rows. Each\ngenerator is then tasked with producing synthetic datasets under two\nconditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.\nEvaluation is conducted using two criteria: statistical similarity, measured\nvia classical statistics and distributional metrics; and predictive utility,\nassessed using a \"Train on Synthetic, Test on Real\" approach with four\nregression models. While statistical similarity remained consistent across\nmodels in both scenarios, predictive utility declined notably in the 1:10 case.\nThe Bayesian Network from Synthicity achieved the highest fidelity in both\nscenarios, while TVAE from SDV performed best in predictive tasks under the\n1:10 setting. Although no significant performance gap was found between the two\nlibraries, SDV stands out for its superior documentation and ease of use,\nmaking it more accessible for practitioners.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5f00\u6e90\u5e93\uff08SDV\u548cSynthCity\uff09\u4e2d\u7684\u516d\u79cd\u8868\u683c\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff0c\u8bc4\u4f30\u5176\u5728\u7edf\u8ba1\u76f8\u4f3c\u6027\u548c\u9884\u6d4b\u5b9e\u7528\u6027\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0cSynthCity\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cSDV\u7684TVAE\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002SDV\u56e0\u5176\u6587\u6863\u548c\u6613\u7528\u6027\u66f4\u53d7\u63a8\u8350\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u771f\u5b9e\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\u5c0f\u578b\u7ec4\u7ec7\u548c\u521d\u521b\u516c\u53f8\u5177\u6709\u6311\u6218\u6027\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u901a\u8fc7\u590d\u5236\u771f\u5b9e\u6570\u636e\u7684\u7edf\u8ba1\u548c\u7ed3\u6784\u7279\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u548c\u6269\u5c55\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u6765\u81eaUCI\u673a\u5668\u5b66\u4e60\u5e93\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff08\u6bd4\u5229\u65f6\u80fd\u6e90\u6d88\u8017\u548c\u73af\u5883\u53d8\u91cf\uff09\uff0c\u5728\u4f4e\u6570\u636e\u91cf\uff081,000\u884c\uff09\u4e0b\u8bad\u7ec3\u6a21\u578b\u3002\u8bc4\u4f30\u4e86\u516d\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff08SDV\u4e2d\u7684\u9ad8\u65afCopula\u3001CTGAN\u3001TVAE\uff1bSynthCity\u4e2d\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u3001CTGAN\u3001TVAE\uff09\uff0c\u5206\u522b\u57281:1\uff081,000\u884c\uff09\u548c1:10\uff0810,000\u884c\uff09\u8f93\u5165\u8f93\u51fa\u6bd4\u4f8b\u4e0b\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u8bc4\u4f30\u6807\u51c6\u5305\u62ec\u7edf\u8ba1\u76f8\u4f3c\u6027\uff08\u7ecf\u5178\u7edf\u8ba1\u548c\u5206\u5e03\u5ea6\u91cf\uff09\u548c\u9884\u6d4b\u5b9e\u7528\u6027\uff08\u201c\u5408\u6210\u8bad\u7ec3\uff0c\u771f\u5b9e\u6d4b\u8bd5\u201d\u65b9\u6cd5\uff09\u3002", "result": "\u7edf\u8ba1\u76f8\u4f3c\u6027\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u9884\u6d4b\u5b9e\u7528\u6027\u57281:10\u6bd4\u4f8b\u4e0b\u663e\u8457\u4e0b\u964d\u3002SynthCity\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u4fdd\u771f\u5ea6\u6700\u9ad8\uff0c\u800cSDV\u7684TVAE\u57281:10\u6bd4\u4f8b\u4e0b\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\u6700\u4f73\u3002\u4e24\u79cd\u5e93\u5728\u6027\u80fd\u4e0a\u65e0\u663e\u8457\u5dee\u8ddd\uff0c\u4f46SDV\u56e0\u5176\u6587\u6863\u548c\u6613\u7528\u6027\u66f4\u53d7\u63a8\u8350\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9884\u6d4b\u5b9e\u7528\u6027\u968f\u6570\u636e\u91cf\u589e\u52a0\u800c\u4e0b\u964d\u3002SDV\u548cSynthCity\u5404\u6709\u4f18\u52bf\uff0cSDV\u66f4\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "paper_title_zh": "\u5f00\u6e90\u8868\u683c\u5408\u6210\u6570\u636e\u751f\u6210\u5e93\u7684\u6bd4\u8f83\u7814\u7a76\uff1aSDV vs. SynthCity", "abstract_zh": "\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u771f\u5b9e\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\u5c0f\u578b\u7ec4\u7ec7\u548c\u521d\u521b\u516c\u53f8\u5177\u6709\u6311\u6218\u6027\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u901a\u8fc7\u590d\u5236\u771f\u5b9e\u6570\u636e\u7684\u7edf\u8ba1\u548c\u7ed3\u6784\u7279\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u548c\u6269\u5c55\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90\u5e93\uff08SDV\u548cSynthCity\uff09\u4e2d\u7684\u516d\u79cd\u8868\u683c\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u3002\u4f7f\u7528\u6765\u81eaUCI\u673a\u5668\u5b66\u4e60\u5e93\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff08\u6bd4\u5229\u65f6\u80fd\u6e90\u6d88\u8017\u548c\u73af\u5883\u53d8\u91cf\uff09\uff0c\u5728\u4f4e\u6570\u636e\u91cf\uff081,000\u884c\uff09\u4e0b\u8bad\u7ec3\u6a21\u578b\u3002\u6bcf\u79cd\u751f\u6210\u5668\u5206\u522b\u57281:1\uff081,000\u884c\uff09\u548c1:10\uff0810,000\u884c\uff09\u8f93\u5165\u8f93\u51fa\u6bd4\u4f8b\u4e0b\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u8bc4\u4f30\u6807\u51c6\u5305\u62ec\u7edf\u8ba1\u76f8\u4f3c\u6027\uff08\u7ecf\u5178\u7edf\u8ba1\u548c\u5206\u5e03\u5ea6\u91cf\uff09\u548c\u9884\u6d4b\u5b9e\u7528\u6027\uff08\u201c\u5408\u6210\u8bad\u7ec3\uff0c\u771f\u5b9e\u6d4b\u8bd5\u201d\u65b9\u6cd5\uff09\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7edf\u8ba1\u76f8\u4f3c\u6027\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u9884\u6d4b\u5b9e\u7528\u6027\u57281:10\u6bd4\u4f8b\u4e0b\u663e\u8457\u4e0b\u964d\u3002SynthCity\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u4fdd\u771f\u5ea6\u6700\u9ad8\uff0c\u800cSDV\u7684TVAE\u57281:10\u6bd4\u4f8b\u4e0b\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\u6700\u4f73\u3002\u4e24\u79cd\u5e93\u5728\u6027\u80fd\u4e0a\u65e0\u663e\u8457\u5dee\u8ddd\uff0c\u4f46SDV\u56e0\u5176\u6587\u6863\u548c\u6613\u7528\u6027\u66f4\u53d7\u63a8\u8350\u3002"}}
{"id": "2506.18881", "pdf": "https://arxiv.org/pdf/2506.18881", "abs": "https://arxiv.org/abs/2506.18881", "authors": ["Xinyu Zhang", "Dong Gong", "Zicheng Duan", "Anton van den Hengel", "Lingqiao Liu"], "title": "Let Your Video Listen to Your Music!", "categories": ["cs.CV", "cs.MM"], "comment": "project page: https://zhangxinyu-xyz.github.io/MVAA/", "summary": "Aligning the rhythm of visual motion in a video with a given music track is a\npractical need in multimedia production, yet remains an underexplored task in\nautonomous video editing. Effective alignment between motion and musical beats\nenhances viewer engagement and visual appeal, particularly in music videos,\npromotional content, and cinematic editing. Existing methods typically depend\non labor-intensive manual cutting, speed adjustments, or heuristic-based\nediting techniques to achieve synchronization. While some generative models\nhandle joint video and music generation, they often entangle the two\nmodalities, limiting flexibility in aligning video to music beats while\npreserving the full visual content. In this paper, we propose a novel and\nefficient framework, termed MVAA (Music-Video Auto-Alignment), that\nautomatically edits video to align with the rhythm of a given music track while\npreserving the original visual content. To enhance flexibility, we modularize\nthe task into a two-step process in our MVAA: aligning motion keyframes with\naudio beats, followed by rhythm-aware video inpainting. Specifically, we first\ninsert keyframes at timestamps aligned with musical beats, then use a\nframe-conditioned diffusion model to generate coherent intermediate frames,\npreserving the original video's semantic content. Since comprehensive test-time\ntraining can be time-consuming, we adopt a two-stage strategy: pretraining the\ninpainting module on a small video set to learn general motion priors, followed\nby rapid inference-time fine-tuning for video-specific adaptation. This hybrid\napproach enables adaptation within 10 minutes with one epoch on a single NVIDIA\n4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show\nthat our approach can achieve high-quality beat alignment and visual\nsmoothness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMVAA\uff08\u97f3\u4e50\u89c6\u9891\u81ea\u52a8\u5bf9\u9f50\uff09\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u7f16\u8f91\u89c6\u9891\u4ee5\u4e0e\u97f3\u4e50\u8282\u594f\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u89c6\u89c9\u5185\u5bb9\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u5173\u952e\u5e27\u5bf9\u9f50\u548c\u8282\u594f\u611f\u77e5\u89c6\u9891\u4fee\u590d\uff09\uff0c\u8be5\u65b9\u6cd5\u572810\u5206\u949f\u5185\u5b8c\u6210\u9002\u914d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u548c\u89c6\u89c9\u6d41\u7545\u6027\u3002", "motivation": "\u5728\u591a\u5a92\u4f53\u5236\u4f5c\u4e2d\uff0c\u5c06\u89c6\u9891\u7684\u89c6\u89c9\u8fd0\u52a8\u8282\u594f\u4e0e\u97f3\u4e50\u5bf9\u9f50\u662f\u4e00\u4e2a\u5b9e\u9645\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u7f16\u8f91\u6216\u542f\u53d1\u5f0f\u6280\u672f\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u4e0e\u97f3\u4e50\u5bf9\u9f50\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "MVAA\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5c06\u8fd0\u52a8\u5173\u952e\u5e27\u4e0e\u97f3\u9891\u8282\u62cd\u5bf9\u9f50\uff0c\u7136\u540e\u901a\u8fc7\u5e27\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u7684\u4e2d\u95f4\u5e27\uff0c\u4fdd\u7559\u539f\u59cb\u89c6\u9891\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u91c7\u7528\u9884\u8bad\u7ec3\u548c\u5feb\u901f\u5fae\u8c03\u7b56\u7565\uff0c\u9002\u914d\u65f6\u95f4\u77ed\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMVAA\u80fd\u591f\u572810\u5206\u949f\u5185\u5b8c\u6210\u9002\u914d\uff0c\u5e76\u5728\u5355\u5757NVIDIA 4090 GPU\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\u548c\u89c6\u89c9\u6d41\u7545\u6027\u3002", "conclusion": "MVAA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u81ea\u52a8\u5316\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u9891\u4e0e\u97f3\u4e50\u5bf9\u9f50\u7684\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u97f3\u4e50\u89c6\u9891\u3001\u5ba3\u4f20\u5185\u5bb9\u7b49\u591a\u79cd\u573a\u666f\u3002", "paper_title_zh": "\u8ba9\u4f60\u7684\u89c6\u9891\u8046\u542c\u4f60\u7684\u97f3\u4e50\uff01", "abstract_zh": "\u5c06\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u8282\u594f\u4e0e\u7ed9\u5b9a\u97f3\u4e50\u8f68\u9053\u5bf9\u9f50\u662f\u591a\u5a92\u4f53\u5236\u4f5c\u4e2d\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u4f46\u5728\u81ea\u4e3b\u89c6\u9891\u7f16\u8f91\u4e2d\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u4efb\u52a1\u3002\u6709\u6548\u7684\u8fd0\u52a8\u4e0e\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\u80fd\u591f\u589e\u5f3a\u89c2\u4f17\u7684\u53c2\u4e0e\u611f\u548c\u89c6\u89c9\u5438\u5f15\u529b\uff0c\u5c24\u5176\u662f\u5728\u97f3\u4e50\u89c6\u9891\u3001\u5ba3\u4f20\u5185\u5bb9\u548c\u7535\u5f71\u526a\u8f91\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u624b\u52a8\u526a\u8f91\u3001\u901f\u5ea6\u8c03\u6574\u6216\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u7f16\u8f91\u6280\u672f\u6765\u5b9e\u73b0\u540c\u6b65\u3002\u867d\u7136\u4e00\u4e9b\u751f\u6210\u6a21\u578b\u80fd\u591f\u5904\u7406\u89c6\u9891\u548c\u97f3\u4e50\u7684\u8054\u5408\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5c06\u4e24\u79cd\u6a21\u6001\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u9650\u5236\u4e86\u5728\u4fdd\u7559\u5b8c\u6574\u89c6\u89c9\u5185\u5bb9\u7684\u540c\u65f6\u5bf9\u9f50\u89c6\u9891\u4e0e\u97f3\u4e50\u8282\u62cd\u7684\u7075\u6d3b\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u79f0\u4e3aMVAA\uff08\u97f3\u4e50\u89c6\u9891\u81ea\u52a8\u5bf9\u9f50\uff09\uff0c\u80fd\u591f\u81ea\u52a8\u7f16\u8f91\u89c6\u9891\u4ee5\u4e0e\u7ed9\u5b9a\u97f3\u4e50\u8f68\u9053\u7684\u8282\u594f\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u89c6\u89c9\u5185\u5bb9\u3002\u4e3a\u4e86\u589e\u5f3a\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5c06\u4efb\u52a1\u6a21\u5757\u5316\u4e3a\u4e24\u6b65\u8fc7\u7a0b\uff1a\u9996\u5148\u5c06\u8fd0\u52a8\u5173\u952e\u5e27\u4e0e\u97f3\u9891\u8282\u62cd\u5bf9\u9f50\uff0c\u7136\u540e\u8fdb\u884c\u8282\u594f\u611f\u77e5\u7684\u89c6\u9891\u4fee\u590d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5728\u4e0e\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\u7684\u65f6\u95f4\u6233\u63d2\u5165\u5173\u952e\u5e27\uff0c\u7136\u540e\u4f7f\u7528\u5e27\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u7684\u4e2d\u95f4\u5e27\uff0c\u4fdd\u7559\u539f\u59cb\u89c6\u9891\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u7531\u4e8e\u5168\u9762\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u53ef\u80fd\u8017\u65f6\u8f83\u957f\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u5728\u5c0f\u578b\u89c6\u9891\u96c6\u4e0a\u9884\u8bad\u7ec3\u4fee\u590d\u6a21\u5757\u4ee5\u5b66\u4e60\u4e00\u822c\u8fd0\u52a8\u5148\u9a8c\uff0c\u7136\u540e\u8fdb\u884c\u5feb\u901f\u7684\u63a8\u7406\u65f6\u5fae\u8c03\u4ee5\u5b9e\u73b0\u89c6\u9891\u7279\u5b9a\u9002\u914d\u3002\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u80fd\u591f\u572810\u5206\u949f\u5185\u5b8c\u6210\u9002\u914d\uff0c\u5e76\u5728\u5355\u5757NVIDIA 4090 GPU\u4e0a\u4f7f\u7528CogVideoX-5b-I2V\u4f5c\u4e3a\u4e3b\u5e72\u8fdb\u884c\u4e00\u4e2a\u5468\u671f\u7684\u5fae\u8c03\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8282\u62cd\u5bf9\u9f50\u548c\u89c6\u89c9\u6d41\u7545\u6027\u3002"}}
{"id": "2506.17848", "pdf": "https://arxiv.org/pdf/2506.17848", "abs": "https://arxiv.org/abs/2506.17848", "authors": ["Suyash Gaurav", "Jukka Heikkonen", "Jatin Chaudhary"], "title": "Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continual learning systems face the dual challenge of preventing catastrophic\nforgetting while maintaining energy efficiency, particularly in\nresource-constrained environments. This paper introduces Pathway-based\nProgressive Inference (PaPI), a novel theoretical framework that addresses\nthese challenges through a mathematically rigorous approach to pathway\nselection and adaptation. We formulate continual learning as an\nenergy-constrained optimization problem and provide formal convergence\nguarantees for our pathway routing mechanisms. Our theoretical analysis\ndemonstrates that PaPI achieves an $\\mathcal{O}(K)$ improvement in the\nstability-plasticity trade-off compared to monolithic architectures, where $K$\nis the number of pathways. We derive tight bounds on forgetting rates using\nFisher Information Matrix analysis and prove that PaPI's energy consumption\nscales with the number of active parameters rather than the total model size.\nComparative theoretical analysis shows that PaPI provides stronger guarantees\nagainst catastrophic forgetting than Elastic Weight Consolidation (EWC) while\nmaintaining better energy efficiency than both EWC and Gradient Episodic Memory\n(GEM). Our experimental validation confirms these theoretical advantages across\nmultiple benchmarks, demonstrating PaPI's effectiveness for continual learning\nin energy-constrained settings. Our codes are available at\nhttps://github.com/zser092/PAPI_FILES.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "paper_title_zh": "Error", "abstract_zh": "Error"}}
{"id": "2506.18882", "pdf": "https://arxiv.org/pdf/2506.18882", "abs": "https://arxiv.org/abs/2506.18882", "authors": ["Hong Li", "Houyuan Chen", "Chongjie Ye", "Zhaoxi Chen", "Bohan Li", "Shaocong Xu", "Xianda Guo", "Xuhui Liu", "Yikai Wang", "Baochang Zhang", "Satoshi Ikehata", "Boxin Shi", "Anyi Rao", "Hao Zhao"], "title": "Light of Normals: Unified Feature Representation for Universal Photometric Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u4e2d\u7684\u5149\u7167\u4e0e\u8868\u9762\u6cd5\u7ebf\u8026\u5408\u95ee\u9898\uff0c\u5e76\u4fdd\u7559\u590d\u6742\u8868\u9762\u7684\u9ad8\u9891\u51e0\u4f55\u7ec6\u8282\u3002", "motivation": "\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\uff08PS\uff09\u65e8\u5728\u4ece\u4efb\u610f\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u8868\u9762\u6cd5\u7ebf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5149\u7167\u4e0e\u8868\u9762\u6cd5\u7ebf\u7279\u5f81\u6df1\u5ea6\u8026\u5408\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u590d\u6742\u8868\u9762\u9ad8\u9891\u51e0\u4f55\u7ec6\u8282\u96be\u4ee5\u4fdd\u7559\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u5149\u7167\u53d8\u5316\u4e0e\u8868\u9762\u6cd5\u7ebf\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7684\u7279\u5f81\u5904\u7406\u64cd\u4f5c\uff0c\u4ee5\u51c6\u786e\u6355\u6349\u590d\u6742\u8868\u9762\u7684\u51e0\u4f55\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5206\u79bb\u5149\u7167\u4e0e\u8868\u9762\u6cd5\u7ebf\u7279\u5f81\uff0c\u5e76\u5728\u590d\u6742\u8868\u9762\u4e0a\u4fdd\u7559\u9ad8\u9891\u51e0\u4f55\u7ec6\u8282\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6cd5\u4e3a\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5149\u7167\u4e0e\u6cd5\u7ebf\u8026\u5408\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u590d\u6742\u8868\u9762\u51e0\u4f55\u7ec6\u8282\u7684\u6062\u590d\u80fd\u529b\u3002", "paper_title_zh": "\u6cd5\u7ebf\u4e4b\u5149\uff1a\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u7684\u7edf\u4e00\u7279\u5f81\u8868\u793a", "abstract_zh": "\u901a\u7528\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\uff08PS\uff09\u65e8\u5728\u4ece\u4efb\u610f\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u8868\u9762\u6cd5\u7ebf\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u7279\u5b9a\u7684\u5149\u7167\u6a21\u578b\u3002\u5c3d\u7ba1\u8fd1\u671f\u51fa\u73b0\u4e86SDM-UniPS\u548cUni MS-PS\u7b49\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a1\uff09\u5149\u7167\u53d8\u5316\u4e0e\u8868\u9762\u6cd5\u7ebf\u7279\u5f81\u7684\u6df1\u5ea6\u8026\u5408\uff0c\u89c2\u6d4b\u4eae\u5ea6\u53d8\u5316\u96be\u4ee5\u533a\u5206\u662f\u6e90\u4e8e\u5149\u7167\u53d8\u5316\u8fd8\u662f\u8868\u9762\u65b9\u5411\uff1b2\uff09\u590d\u6742\u8868\u9762\u4e2d\u9ad8\u9891\u51e0\u4f55\u7ec6\u8282\u7684\u4fdd\u7559\uff0c\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u5bfc\u81f4\u81ea\u9634\u5f71\u3001\u76f8\u4e92\u53cd\u5c04\u548c\u7ec6\u5fae\u6cd5\u7ebf\u53d8\u5316\uff0c\u4f20\u7edf\u7279\u5f81\u5904\u7406\u64cd\u4f5c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u3002"}}
{"id": "2506.17859", "pdf": "https://arxiv.org/pdf/2506.17859", "abs": "https://arxiv.org/abs/2506.17859", "authors": ["Daniel Wurgaft", "Ekdeep Singh Lubana", "Core Francisco Park", "Hidenori Tanaka", "Gautam Reddy", "Noah D. Goodman"], "title": "In-Context Learning Strategies Emerge Rationally", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recent work analyzing in-context learning (ICL) has identified a broad set of\nstrategies that describe model behavior in different experimental conditions.\nWe aim to unify these findings by asking why a model learns these disparate\nstrategies in the first place. Specifically, we start with the observation that\nwhen trained to learn a mixture of tasks, as is popular in the literature, the\nstrategies learned by a model for performing ICL can be captured by a family of\nBayesian predictors: a memorizing predictor, which assumes a discrete prior on\nthe set of seen tasks, and a generalizing predictor, wherein the prior matches\nthe underlying task distribution. Adopting the lens of rational analysis from\ncognitive science, where a learner's behavior is explained as an optimal\nadaptation to data given computational constraints, we develop a hierarchical\nBayesian framework that almost perfectly predicts Transformer next token\npredictions throughout training without assuming access to its weights. Under\nthis framework, pretraining is viewed as a process of updating the posterior\nprobability of different strategies, and its inference-time behavior as a\nposterior-weighted average over these strategies' predictions. Our framework\ndraws on common assumptions about neural network learning dynamics, which make\nexplicit a tradeoff between loss and complexity among candidate strategies:\nbeyond how well it explains the data, a model's preference towards implementing\na strategy is dictated by its complexity. This helps explain well-known ICL\nphenomena, while offering novel predictions: e.g., we show a superlinear trend\nin the timescale for transition to memorization as task diversity is increased.\nOverall, our work advances an explanatory and predictive account of ICL\ngrounded in tradeoffs between strategy loss and complexity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8d1d\u53f6\u65af\u6846\u67b6\u89e3\u91ca\u4e86Transformer\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u4e3a\u4f55\u9009\u62e9\u4e0d\u540c\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u7b56\u7565\u9009\u62e9\u662f\u635f\u5931\u4e0e\u590d\u6742\u5ea6\u6743\u8861\u7684\u7ed3\u679c\uff0c\u5e76\u9884\u6d4b\u4e86\u4efb\u52a1\u591a\u6837\u6027\u589e\u52a0\u65f6\u8bb0\u5fc6\u5316\u7b56\u7565\u7684\u8f6c\u53d8\u8d8b\u52bf\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u6a21\u578b\u884c\u4e3a\u7684\u591a\u79cd\u7b56\u7565\uff0c\u4f46\u672a\u89e3\u91ca\u4e3a\u4f55\u6a21\u578b\u4f1a\u5b66\u4e60\u8fd9\u4e9b\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u6027\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u6a21\u578b\u9009\u62e9\u7b56\u7565\u7684\u5185\u5728\u539f\u56e0\uff0c\u5e76\u7edf\u4e00\u73b0\u6709\u53d1\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u89c6\u4e3a\u66f4\u65b0\u7b56\u7565\u540e\u9a8c\u6982\u7387\u7684\u8fc7\u7a0b\uff0c\u63a8\u65ad\u884c\u4e3a\u5219\u662f\u8fd9\u4e9b\u7b56\u7565\u9884\u6d4b\u7684\u540e\u9a8c\u52a0\u6743\u5e73\u5747\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u52a8\u6001\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u660e\u786e\u4e86\u7b56\u7565\u635f\u5931\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u8be5\u6846\u67b6\u51e0\u4e4e\u5b8c\u7f8e\u9884\u6d4b\u4e86Transformer\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u53d1\u73b0\u4efb\u52a1\u591a\u6837\u6027\u589e\u52a0\u65f6\uff0c\u8bb0\u5fc6\u5316\u7b56\u7565\u7684\u8f6c\u53d8\u65f6\u95f4\u5448\u73b0\u8d85\u7ebf\u6027\u8d8b\u52bf\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u635f\u5931\u4e0e\u590d\u6742\u5ea6\u7684\u6743\u8861\uff0c\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7b56\u7565\u9009\u62e9\u7684\u5408\u7406\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u9884\u6d4b\uff0c\u5982\u4efb\u52a1\u591a\u6837\u6027\u5bf9\u8bb0\u5fc6\u5316\u7b56\u7565\u7684\u5f71\u54cd\u3002", "paper_title_zh": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u7684\u7406\u6027\u6d8c\u73b0", "abstract_zh": "\u8fd1\u671f\u5173\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u884c\u4e3a\u7684\u591a\u79cd\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63a2\u7a76\u6a21\u578b\u4e3a\u4f55\u5b66\u4e60\u8fd9\u4e9b\u7b56\u7565\u6765\u7edf\u4e00\u8fd9\u4e9b\u53d1\u73b0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5f53\u6a21\u578b\u88ab\u8bad\u7ec3\u5b66\u4e60\u6df7\u5408\u4efb\u52a1\uff08\u5982\u6587\u732e\u4e2d\u5e38\u89c1\uff09\u65f6\uff0c\u5176\u7528\u4e8eICL\u7684\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u8d1d\u53f6\u65af\u9884\u6d4b\u5668\u5bb6\u65cf\u6355\u6349\uff1a\u8bb0\u5fc6\u5316\u9884\u6d4b\u5668\uff08\u5047\u8bbe\u5bf9\u5df2\u89c1\u4efb\u52a1\u96c6\u91c7\u7528\u79bb\u6563\u5148\u9a8c\uff09\u548c\u6cdb\u5316\u9884\u6d4b\u5668\uff08\u5148\u9a8c\u4e0e\u5e95\u5c42\u4efb\u52a1\u5206\u5e03\u5339\u914d\uff09\u3002\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u7406\u6027\u5206\u6790\u89c6\u89d2\uff08\u5c06\u5b66\u4e60\u8005\u884c\u4e3a\u89e3\u91ca\u4e3a\u5728\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5bf9\u6570\u636e\u7684\u6700\u4f18\u9002\u5e94\uff09\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u51e0\u4e4e\u5b8c\u7f8e\u9884\u6d4b\u4e86Transformer\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5176\u6743\u91cd\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u9884\u8bad\u7ec3\u88ab\u89c6\u4e3a\u66f4\u65b0\u4e0d\u540c\u7b56\u7565\u540e\u9a8c\u6982\u7387\u7684\u8fc7\u7a0b\uff0c\u63a8\u65ad\u884c\u4e3a\u5219\u662f\u8fd9\u4e9b\u7b56\u7565\u9884\u6d4b\u7684\u540e\u9a8c\u52a0\u6743\u5e73\u5747\u3002\u6211\u4eec\u7684\u6846\u67b6\u57fa\u4e8e\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u52a8\u6001\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u660e\u786e\u4e86\u5019\u9009\u7b56\u7565\u5728\u635f\u5931\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u6a21\u578b\u5bf9\u7b56\u7565\u7684\u504f\u597d\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u5176\u5bf9\u6570\u636e\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u8fd8\u53d6\u51b3\u4e8e\u5176\u590d\u6742\u5ea6\u3002\u8fd9\u89e3\u91ca\u4e86\u5df2\u77e5\u7684ICL\u73b0\u8c61\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u65b0\u9884\u6d4b\uff1a\u4f8b\u5982\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4efb\u52a1\u591a\u6837\u6027\u589e\u52a0\u65f6\u8bb0\u5fc6\u5316\u7b56\u7565\u8f6c\u53d8\u65f6\u95f4\u5c3a\u5ea6\u7684\u8d85\u7ebf\u6027\u8d8b\u52bf\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u57fa\u4e8e\u7b56\u7565\u635f\u5931\u4e0e\u590d\u6742\u5ea6\u7684\u6743\u8861\uff0c\u4e3aICL\u63d0\u4f9b\u4e86\u517c\u5177\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u7684\u7406\u8bba\u3002"}}
{"id": "2506.18883", "pdf": "https://arxiv.org/pdf/2506.18883", "abs": "https://arxiv.org/abs/2506.18883", "authors": ["Zeqian Li", "Shangzhe Di", "Zhonghua Zhai", "Weilin Huang", "Yanfeng Wang", "Weidi Xie"], "title": "Universal Video Temporal Grounding with Generative Multi-modal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a computational model for universal video temporal\ngrounding, which accurately localizes temporal moments in videos based on\nnatural language queries (e.g., questions or descriptions). Unlike existing\nmethods that are often limited to specific video domains or durations, we\npropose UniTime, a robust and universal video grounding model leveraging the\nstrong vision-language understanding capabilities of generative Multi-modal\nLarge Language Models (MLLMs). Our model effectively handles videos of diverse\nviews, genres, and lengths while comprehending complex language queries. The\nkey contributions include: (i) We consider steering strong MLLMs for temporal\ngrounding in videos. To enable precise timestamp outputs, we incorporate\ntemporal information by interleaving timestamp tokens with video tokens. (ii)\nBy training the model to handle videos with different input granularities\nthrough adaptive frame scaling, our approach achieves robust temporal grounding\nfor both short and long videos. (iii) Comprehensive experiments show that\nUniTime outperforms state-of-the-art approaches in both zero-shot and\ndataset-specific finetuned settings across five public temporal grounding\nbenchmarks. (iv) When employed as a preliminary moment retriever for long-form\nvideo question-answering (VideoQA), UniTime significantly improves VideoQA\naccuracy, highlighting its value for complex video understanding tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u6a21\u578bUniTime\uff0c\u5229\u7528\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u80fd\u591f\u7cbe\u51c6\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u7247\u6bb5\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u63d2\u5165\u65f6\u95f4\u6233\u6807\u8bb0\u548c\u81ea\u9002\u5e94\u5e27\u7f29\u653e\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u957f\u5ea6\u548c\u7c7b\u578b\u89c6\u9891\u7684\u9c81\u68d2\u6027\u65f6\u95f4\u5b9a\u4f4d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u89c6\u9891\u9886\u57df\u6216\u65f6\u957f\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u6837\u5316\u7684\u89c6\u9891\u5185\u5bb9\u548c\u590d\u6742\u7684\u8bed\u8a00\u67e5\u8be2\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u7684\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u6a21\u578b\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c6\u56fe\u3001\u7c7b\u578b\u548c\u957f\u5ea6\u7684\u89c6\u9891\uff0c\u5e76\u7406\u89e3\u590d\u6742\u7684\u8bed\u8a00\u67e5\u8be2\u3002", "method": "UniTime\u6a21\u578b\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u5b9e\u73b0\u901a\u7528\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\uff1a(i) \u5229\u7528\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u63d2\u5165\u65f6\u95f4\u6233\u6807\u8bb0\u4e0e\u89c6\u9891\u6807\u8bb0\u7ed3\u5408\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u65f6\u95f4\u6233\u8f93\u51fa\uff1b(ii) \u91c7\u7528\u81ea\u9002\u5e94\u5e27\u7f29\u653e\u6280\u672f\uff0c\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u4e0d\u540c\u8f93\u5165\u7c92\u5ea6\u7684\u89c6\u9891\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u77ed\u89c6\u9891\u548c\u957f\u89c6\u9891\u7684\u9c81\u68d2\u6027\u65f6\u95f4\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniTime\u5728\u4e94\u4e2a\u516c\u5f00\u7684\u65f6\u95f4\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u8bba\u662f\u96f6\u6837\u672c\u8fd8\u662f\u6570\u636e\u96c6\u7279\u5b9a\u7684\u5fae\u8c03\u8bbe\u7f6e\u4e0b\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5f53UniTime\u4f5c\u4e3a\u957f\u89c6\u9891\u95ee\u7b54\uff08VideoQA\uff09\u7684\u521d\u6b65\u7247\u6bb5\u68c0\u7d22\u5668\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86VideoQA\u7684\u51c6\u786e\u6027\u3002", "conclusion": "UniTime\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u9002\u5e94\u5e27\u7f29\u653e\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u6837\u5316\u89c6\u9891\u7684\u9c81\u68d2\u6027\u65f6\u95f4\u5b9a\u4f4d\uff0c\u5e76\u5728\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u91cd\u8981\u4ef7\u503c\u3002", "paper_title_zh": "\u57fa\u4e8e\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u8ba1\u7b97\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff08\u5982\u95ee\u9898\u6216\u63cf\u8ff0\uff09\u7cbe\u51c6\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u7247\u6bb5\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u89c6\u9891\u9886\u57df\u6216\u65f6\u957f\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86UniTime\uff0c\u4e00\u79cd\u5229\u7528\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u9c81\u68d2\u901a\u7528\u89c6\u9891\u5b9a\u4f4d\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6837\u5316\u89c6\u56fe\u3001\u7c7b\u578b\u548c\u957f\u5ea6\u7684\u89c6\u9891\uff0c\u540c\u65f6\u7406\u89e3\u590d\u6742\u7684\u8bed\u8a00\u67e5\u8be2\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a(i) \u6211\u4eec\u63a2\u7d22\u4e86\u5982\u4f55\u5229\u7528\u5f3a\u5927\u7684MLLMs\u8fdb\u884c\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u3002\u4e3a\u4e86\u5b9e\u73b0\u7cbe\u786e\u7684\u65f6\u95f4\u6233\u8f93\u51fa\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u65f6\u95f4\u6233\u6807\u8bb0\u4e0e\u89c6\u9891\u6807\u8bb0\u4ea4\u9519\u7ed3\u5408\uff0c\u5f15\u5165\u4e86\u65f6\u95f4\u4fe1\u606f\u3002(ii) \u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u4e0d\u540c\u8f93\u5165\u7c92\u5ea6\u7684\u89c6\u9891\uff08\u91c7\u7528\u81ea\u9002\u5e94\u5e27\u7f29\u653e\u6280\u672f\uff09\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u77ed\u89c6\u9891\u548c\u957f\u89c6\u9891\u7684\u9c81\u68d2\u6027\u65f6\u95f4\u5b9a\u4f4d\u3002(iii) \u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cUniTime\u5728\u4e94\u4e2a\u516c\u5f00\u7684\u65f6\u95f4\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u8bba\u662f\u96f6\u6837\u672c\u8fd8\u662f\u6570\u636e\u96c6\u7279\u5b9a\u7684\u5fae\u8c03\u8bbe\u7f6e\u4e0b\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002(iv) \u5f53UniTime\u4f5c\u4e3a\u957f\u89c6\u9891\u95ee\u7b54\uff08VideoQA\uff09\u7684\u521d\u6b65\u7247\u6bb5\u68c0\u7d22\u5668\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86VideoQA\u7684\u51c6\u786e\u6027\uff0c\u51f8\u663e\u4e86\u5176\u5728\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.17870", "pdf": "https://arxiv.org/pdf/2506.17870", "abs": "https://arxiv.org/abs/2506.17870", "authors": ["Jianhang Xie", "Chuntao Ding", "Xiaqing Li", "Shenyuan Ren", "Yidong Li", "Zhichao Lu"], "title": "NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "IEEE Transactions on Mobile Computing, accepted manuscript, DOI:\n  10.1109/TMC.2025.3582583; Code: https://github.com/jianhayes/NESTQUANT", "summary": "Deploying quantized deep neural network (DNN) models with resource adaptation\ncapabilities on ubiquitous Internet of Things (IoT) devices to provide\nhigh-quality AI services can leverage the benefits of compression and meet\nmulti-scenario resource requirements. However, existing dynamic/mixed precision\nquantization requires retraining or special hardware, whereas post-training\nquantization (PTQ) has two limitations for resource adaptation: (i) The\nstate-of-the-art PTQ methods only provide one fixed bitwidth model, which makes\nit challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying\nmultiple PTQ models with diverse bitwidths consumes large storage resources and\nswitching overheads. To this end, this paper introduces a resource-friendly\npost-training integer-nesting quantization, i.e., NestQuant, for on-device\nquantized model switching on IoT devices. The proposed NestQuant incorporates\nthe integer weight decomposition, which bit-wise splits quantized weights into\nhigher-bit and lower-bit weights of integer data types. It also contains a\ndecomposed weights nesting mechanism to optimize the higher-bit weights by\nadaptive rounding and nest them into the original quantized weights. In\ndeployment, we can send and store only one NestQuant model and switch between\nthe full-bit/part-bit model by paging in/out lower-bit weights to adapt to\nresource changes and reduce consumption. Experimental results on the\nImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve\nhigh performance in top-1 accuracy, and reduce in terms of data transmission,\nstorage consumption, and switching overheads. In particular, the ResNet-101\nwith INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and\npart-bit models, respectively, and reduce switching overheads by approximately\n78.1% compared with diverse bitwidths PTQ models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNestQuant\u7684\u540e\u8bad\u7ec3\u6574\u6570\u5d4c\u5957\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u91cf\u5316\u6a21\u578b\u5207\u6362\u3002\u901a\u8fc7\u6574\u6570\u6743\u91cd\u5206\u89e3\u548c\u5d4c\u5957\u673a\u5236\uff0cNestQuant\u4ec5\u9700\u5b58\u50a8\u4e00\u4e2a\u6a21\u578b\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u8d44\u6e90\u9700\u6c42\uff0c\u663e\u8457\u51cf\u5c11\u4f20\u8f93\u3001\u5b58\u50a8\u548c\u5207\u6362\u5f00\u9500\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u5f53\u524d\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u56fa\u5b9a\u4f4d\u5bbd\u6a21\u578b\uff0c\u96be\u4ee5\u9002\u5e94\u7269\u8054\u7f51\u8bbe\u5907\u7684\u52a8\u6001\u8d44\u6e90\u9700\u6c42\uff1b\u800c\u90e8\u7f72\u591a\u91cf\u5316\u6a21\u578b\u5219\u6d88\u8017\u5927\u91cf\u5b58\u50a8\u548c\u5207\u6362\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u8d44\u6e90\u53cb\u597d\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u65e2\u80fd\u9002\u5e94\u52a8\u6001\u8d44\u6e90\u53d8\u5316\uff0c\u53c8\u80fd\u51cf\u5c11\u5f00\u9500\u3002", "method": "NestQuant\u901a\u8fc7\u6574\u6570\u6743\u91cd\u5206\u89e3\u5c06\u91cf\u5316\u6743\u91cd\u6309\u4f4d\u62c6\u5206\u4e3a\u9ad8\u4f4d\u548c\u4f4e\u4f4d\u6574\u6570\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u5d4c\u5957\u673a\u5236\u4f18\u5316\u9ad8\u4f4d\u6743\u91cd\u3002\u5728\u90e8\u7f72\u65f6\uff0c\u4ec5\u9700\u5b58\u50a8\u4e00\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u8f7d\u4f4e\u4f4d\u6743\u91cd\u5b9e\u73b0\u6a21\u578b\u5207\u6362\uff0c\u4ece\u800c\u9002\u5e94\u8d44\u6e90\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cNestQuant\u5728ImageNet-1K\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982ResNet-101\u7684INT8\u5d4c\u5957INT6\u6a21\u578b\u5728\u5b8c\u6574\u4f4d\u548c\u90e8\u5206\u4f4d\u6a21\u5f0f\u4e0b\u5206\u522b\u8fbe\u523078.1%\u548c77.9%\u7684\u51c6\u786e\u7387\uff0c\u5207\u6362\u5f00\u9500\u51cf\u5c11\u7ea678.1%\u3002", "conclusion": "NestQuant\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u7269\u8054\u7f51\u8bbe\u5907\u7684\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u8d44\u6e90\u73af\u5883\u3002", "paper_title_zh": "NestQuant\uff1a\u4e00\u79cd\u7528\u4e8e\u8bbe\u5907\u7aef\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u540e\u8bad\u7ec3\u6574\u6570\u5d4c\u5957\u91cf\u5316\u65b9\u6cd5", "abstract_zh": "\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u90e8\u7f72\u5177\u6709\u8d44\u6e90\u9002\u5e94\u80fd\u529b\u7684\u91cf\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6a21\u578b\uff0c\u53ef\u4ee5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684AI\u670d\u52a1\uff0c\u540c\u65f6\u5229\u7528\u538b\u7f29\u7684\u4f18\u52bf\u5e76\u6ee1\u8db3\u591a\u573a\u666f\u8d44\u6e90\u9700\u6c42\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u52a8\u6001/\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u7279\u6b8a\u786c\u4ef6\u652f\u6301\uff0c\u800c\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u5728\u8d44\u6e90\u9002\u5e94\u65b9\u9762\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\uff08i\uff09\u73b0\u6709PTQ\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u56fa\u5b9a\u4f4d\u5bbd\u6a21\u578b\uff0c\u96be\u4ee5\u9002\u5e94\u7269\u8054\u7f51\u8bbe\u5907\u7684\u52a8\u6001\u8d44\u6e90\uff1b\uff08ii\uff09\u90e8\u7f72\u591a\u91cf\u5316\u6a21\u578b\u4f1a\u6d88\u8017\u5927\u91cf\u5b58\u50a8\u8d44\u6e90\u5e76\u589e\u52a0\u5207\u6362\u5f00\u9500\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u53cb\u597d\u7684\u540e\u8bad\u7ec3\u6574\u6570\u5d4c\u5957\u91cf\u5316\u65b9\u6cd5NestQuant\uff0c\u7528\u4e8e\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u91cf\u5316\u6a21\u578b\u5207\u6362\u3002NestQuant\u901a\u8fc7\u6574\u6570\u6743\u91cd\u5206\u89e3\u5c06\u91cf\u5316\u6743\u91cd\u6309\u4f4d\u62c6\u5206\u4e3a\u9ad8\u4f4d\u548c\u4f4e\u4f4d\u6574\u6570\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u5d4c\u5957\u673a\u5236\u4f18\u5316\u9ad8\u4f4d\u6743\u91cd\u3002\u5728\u90e8\u7f72\u65f6\uff0c\u4ec5\u9700\u5b58\u50a8\u4e00\u4e2aNestQuant\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u8f7d\u4f4e\u4f4d\u6743\u91cd\u5b9e\u73b0\u6a21\u578b\u5207\u6362\uff0c\u4ece\u800c\u9002\u5e94\u8d44\u6e90\u53d8\u5316\u5e76\u51cf\u5c11\u5f00\u9500\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNestQuant\u5728ImageNet-1K\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982ResNet-101\u7684INT8\u5d4c\u5957INT6\u6a21\u578b\u5728\u5b8c\u6574\u4f4d\u548c\u90e8\u5206\u4f4d\u6a21\u5f0f\u4e0b\u5206\u522b\u8fbe\u523078.1%\u548c77.9%\u7684\u51c6\u786e\u7387\uff0c\u5207\u6362\u5f00\u9500\u51cf\u5c11\u7ea678.1%\u3002"}}
{"id": "2506.18890", "pdf": "https://arxiv.org/pdf/2506.18890", "abs": "https://arxiv.org/abs/2506.18890", "authors": ["Ziqiao Ma", "Xuweiyi Chen", "Shoubin Yu", "Sai Bi", "Kai Zhang", "Chen Ziwen", "Sihan Xu", "Jianing Yang", "Zexiang Xu", "Kalyan Sunkavalli", "Mohit Bansal", "Joyce Chai", "Hao Tan"], "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time", "categories": ["cs.CV"], "comment": "Project page: https://4dlrm.github.io/", "summary": "Can we scale 4D pretraining to learn general space-time representations that\nreconstruct an object from a few views at some times to any view at any time?\nWe provide an affirmative answer with 4D-LRM, the first large-scale 4D\nreconstruction model that takes input from unconstrained views and timestamps\nand renders arbitrary novel view-time combinations. Unlike prior 4D approaches,\ne.g., optimization-based, geometry-based, or generative, that struggle with\nefficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time\nrepresentation and directly predicts per-pixel 4D Gaussian primitives from\nposed image tokens across time, enabling fast, high-quality rendering at, in\nprinciple, infinite frame rate. Our results demonstrate that scaling\nspatiotemporal pretraining enables accurate and efficient 4D reconstruction. We\nshow that 4D-LRM generalizes to novel objects, interpolates across time, and\nhandles diverse camera setups. It reconstructs 24-frame sequences in one\nforward pass with less than 1.5 seconds on a single A100 GPU.", "AI": {"tldr": "4D-LRM\u662f\u9996\u4e2a\u5927\u89c4\u6a214D\u91cd\u5efa\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u70b9\u8f93\u5165\uff0c\u6e32\u67d3\u4efb\u610f\u65b0\u89c6\u89d2\u548c\u65f6\u95f4\u7ec4\u5408\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u76844D\u91cd\u5efa\u3002", "motivation": "\u73b0\u67094D\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u4f18\u5316\u3001\u51e0\u4f55\u6216\u751f\u6210\u7684\u65b9\u6cd5\uff09\u5728\u6548\u7387\u3001\u6cdb\u5316\u6027\u6216\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c4D-LRM\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u7edf\u4e00\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "4D-LRM\u901a\u8fc7\u4ece\u65f6\u95f4\u5e8f\u5217\u7684\u4f4d\u59ff\u56fe\u50cf\u6807\u8bb0\u4e2d\u76f4\u63a5\u9884\u6d4b\u6bcf\u50cf\u7d20\u76844D\u9ad8\u65af\u57fa\u5143\uff0c\u5b66\u4e60\u7edf\u4e00\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4D-LRM\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u3001\u65f6\u95f4\u63d2\u503c\uff0c\u5e76\u5904\u7406\u591a\u6837\u5316\u7684\u76f8\u673a\u8bbe\u7f6e\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u5728A100 GPU\u4e0a1.5\u79d2\u5185\u91cd\u5efa24\u5e27\u5e8f\u5217\u3002", "conclusion": "4D-LRM\u901a\u8fc7\u6269\u5c55\u65f6\u7a7a\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u76844D\u91cd\u5efa\uff0c\u4e3a\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "4D-LRM\uff1a\u4ece\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u70b9\u5230\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u7684\u5927\u89c4\u6a21\u65f6\u7a7a\u91cd\u5efa\u6a21\u578b", "abstract_zh": "\u6211\u4eec\u80fd\u5426\u901a\u8fc74D\u9884\u8bad\u7ec3\u5b66\u4e60\u901a\u7528\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u4ece\u67d0\u4e9b\u65f6\u95f4\u7684\u5c11\u91cf\u89c6\u89d2\u91cd\u5efa\u7269\u4f53\u5230\u4efb\u610f\u65f6\u95f4\u7684\u4efb\u610f\u89c6\u89d2\uff1f\u6211\u4eec\u901a\u8fc74D-LRM\u7ed9\u51fa\u4e86\u80af\u5b9a\u7684\u7b54\u6848\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a214D\u91cd\u5efa\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u4e0d\u53d7\u7ea6\u675f\u7684\u89c6\u89d2\u548c\u65f6\u95f4\u70b9\u8f93\u5165\uff0c\u5e76\u6e32\u67d3\u4efb\u610f\u65b0\u89c6\u89d2\u548c\u65f6\u95f4\u7684\u7ec4\u5408\u3002\u4e0e\u4e4b\u524d\u76844D\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u4f18\u5316\u3001\u51e0\u4f55\u6216\u751f\u6210\u7684\u65b9\u6cd5\uff09\u4e0d\u540c\uff0c4D-LRM\u901a\u8fc7\u5b66\u4e60\u7edf\u4e00\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u76f4\u63a5\u4ece\u65f6\u95f4\u5e8f\u5217\u7684\u4f4d\u59ff\u56fe\u50cf\u6807\u8bb0\u4e2d\u9884\u6d4b\u6bcf\u50cf\u7d20\u76844D\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\uff0c\u7406\u8bba\u4e0a\u652f\u6301\u65e0\u9650\u5e27\u7387\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6269\u5c55\u65f6\u7a7a\u9884\u8bad\u7ec3\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u76844D\u91cd\u5efa\u30024D-LRM\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u3001\u65f6\u95f4\u63d2\u503c\uff0c\u5e76\u5904\u7406\u591a\u6837\u5316\u7684\u76f8\u673a\u8bbe\u7f6e\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u5728A100 GPU\u4e0a1.5\u79d2\u5185\u91cd\u5efa24\u5e27\u5e8f\u5217\u3002"}}
{"id": "2506.18899", "pdf": "https://arxiv.org/pdf/2506.18899", "abs": "https://arxiv.org/abs/2506.18899", "authors": ["Kaiyi Huang", "Yukun Huang", "Xintao Wang", "Zinan Lin", "Xuefei Ning", "Pengfei Wan", "Di Zhang", "Yu Wang", "Xihui Liu"], "title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation", "categories": ["cs.CV"], "comment": "Project Page: https://filmaster-ai.github.io/", "summary": "AI-driven content creation has shown potential in film production. However,\nexisting film generation systems struggle to implement cinematic principles and\nthus fail to generate professional-quality films, particularly lacking diverse\ncamera language and cinematic rhythm. This results in templated visuals and\nunengaging narratives. To address this, we introduce FilMaster, an end-to-end\nAI system that integrates real-world cinematic principles for\nprofessional-grade film generation, yielding editable, industry-standard\noutputs. FilMaster is built on two key principles: (1) learning cinematography\nfrom extensive real-world film data and (2) emulating professional,\naudience-centric post-production workflows. Inspired by these principles,\nFilMaster incorporates two stages: a Reference-Guided Generation Stage which\ntransforms user input to video clips, and a Generative Post-Production Stage\nwhich transforms raw footage into audiovisual outputs by orchestrating visual\nand auditory elements for cinematic rhythm. Our generation stage highlights a\nMulti-shot Synergized RAG Camera Language Design module to guide the AI in\ngenerating professional camera language by retrieving reference clips from a\nvast corpus of 440,000 film clips. Our post-production stage emulates\nprofessional workflows by designing an Audience-Centric Cinematic Rhythm\nControl module, including Rough Cut and Fine Cut processes informed by\nsimulated audience feedback, for effective integration of audiovisual elements\nto achieve engaging content. The system is empowered by generative AI models\nlike (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a\ncomprehensive benchmark for evaluating AI-generated films. Extensive\nexperiments show FilMaster's superior performance in camera language design and\ncinematic rhythm control, advancing generative AI in professional filmmaking.", "AI": {"tldr": "FilMaster\u662f\u4e00\u6b3e\u7aef\u5230\u7aefAI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u7535\u5f71\u539f\u5219\uff0c\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u7535\u5f71\u751f\u6210\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u955c\u5934\u8bed\u8a00\u548c\u8282\u594f\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709AI\u7535\u5f71\u751f\u6210\u7cfb\u7edf\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u955c\u5934\u8bed\u8a00\u548c\u7535\u5f71\u8282\u594f\uff0c\u5bfc\u81f4\u89c6\u89c9\u6548\u679c\u6a21\u677f\u5316\u548c\u53d9\u4e8b\u4e4f\u5473\u3002FilMaster\u65e8\u5728\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u7535\u5f71\u539f\u5219\uff0c\u751f\u6210\u4e13\u4e1a\u7ea7\u7535\u5f71\u3002", "method": "FilMaster\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u53c2\u8003\u5f15\u5bfc\u751f\u6210\u9636\u6bb5\u548c\u591a\u955c\u5934\u534f\u540cRAG\u6a21\u5757\u751f\u6210\u4e13\u4e1a\u955c\u5934\u8bed\u8a00\uff1b\u751f\u6210\u540e\u5236\u4f5c\u9636\u6bb5\u901a\u8fc7\u89c2\u4f17\u4e2d\u5fc3\u8282\u594f\u63a7\u5236\u6a21\u5757\u6a21\u62df\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFilMaster\u5728\u955c\u5934\u8bed\u8a00\u8bbe\u8ba1\u548c\u7535\u5f71\u8282\u594f\u63a7\u5236\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u4e13\u4e1a\u7535\u5f71\u5236\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "FilMaster\u901a\u8fc7\u6574\u5408\u7535\u5f71\u539f\u5219\u548c\u751f\u6210\u5f0fAI\uff0c\u5b9e\u73b0\u4e86\u4e13\u4e1a\u7ea7\u7535\u5f71\u751f\u6210\uff0c\u5e76\u5f15\u5165FilmEval\u57fa\u51c6\u8bc4\u4f30AI\u751f\u6210\u7535\u5f71\u3002", "paper_title_zh": "FilMaster\uff1a\u878d\u5408\u7535\u5f71\u539f\u5219\u4e0e\u751f\u6210\u5f0fAI\u7684\u81ea\u52a8\u5316\u7535\u5f71\u751f\u6210\u7cfb\u7edf", "abstract_zh": "AI\u9a71\u52a8\u7684\u5185\u5bb9\u521b\u4f5c\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7535\u5f71\u751f\u6210\u7cfb\u7edf\u96be\u4ee5\u5b9e\u73b0\u7535\u5f71\u539f\u5219\uff0c\u5bfc\u81f4\u751f\u6210\u6548\u679c\u7f3a\u4e4f\u4e13\u4e1a\u6c34\u51c6\uff0c\u5c24\u5176\u5728\u955c\u5934\u8bed\u8a00\u548c\u7535\u5f71\u8282\u594f\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9020\u6210\u89c6\u89c9\u6548\u679c\u6a21\u677f\u5316\u548c\u53d9\u4e8b\u4e4f\u5473\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faFilMaster\uff0c\u4e00\u6b3e\u7aef\u5230\u7aefAI\u7cfb\u7edf\uff0c\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7535\u5f71\u539f\u5219\uff0c\u751f\u6210\u53ef\u7f16\u8f91\u3001\u7b26\u5408\u884c\u4e1a\u6807\u51c6\u7684\u4e13\u4e1a\u7ea7\u7535\u5f71\u3002FilMaster\u57fa\u4e8e\u4e24\u5927\u539f\u5219\uff1a(1)\u4ece\u6d77\u91cf\u771f\u5b9e\u7535\u5f71\u6570\u636e\u4e2d\u5b66\u4e60\u7535\u5f71\u6444\u5f71\uff1b(2)\u6a21\u62df\u4ee5\u89c2\u4f17\u4e3a\u4e2d\u5fc3\u7684\u4e13\u4e1a\u540e\u671f\u5236\u4f5c\u6d41\u7a0b\u3002\u53d7\u6b64\u542f\u53d1\uff0cFilMaster\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u53c2\u8003\u5f15\u5bfc\u751f\u6210\u9636\u6bb5\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u5316\u4e3a\u89c6\u9891\u7247\u6bb5\uff1b\u751f\u6210\u540e\u5236\u4f5c\u9636\u6bb5\u901a\u8fc7\u534f\u8c03\u89c6\u542c\u5143\u7d20\u5b9e\u73b0\u7535\u5f71\u8282\u594f\uff0c\u5c06\u539f\u59cb\u7d20\u6750\u8f6c\u5316\u4e3a\u89c6\u542c\u8f93\u51fa\u3002\u751f\u6210\u9636\u6bb5\u91c7\u7528\u591a\u955c\u5934\u534f\u540cRAG\u955c\u5934\u8bed\u8a00\u8bbe\u8ba1\u6a21\u5757\uff0c\u901a\u8fc7\u4ece44\u4e07\u7535\u5f71\u7247\u6bb5\u5e93\u4e2d\u68c0\u7d22\u53c2\u8003\u7247\u6bb5\uff0c\u6307\u5bfcAI\u751f\u6210\u4e13\u4e1a\u955c\u5934\u8bed\u8a00\u3002\u540e\u5236\u4f5c\u9636\u6bb5\u901a\u8fc7\u8bbe\u8ba1\u89c2\u4f17\u4e2d\u5fc3\u7535\u5f71\u8282\u594f\u63a7\u5236\u6a21\u5757\uff08\u5305\u62ec\u57fa\u4e8e\u6a21\u62df\u89c2\u4f17\u53cd\u9988\u7684\u7c97\u526a\u548c\u7cbe\u526a\u6d41\u7a0b\uff09\uff0c\u6a21\u62df\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6709\u6548\u6574\u5408\u89c6\u542c\u5143\u7d20\u4ee5\u751f\u6210\u5f15\u4eba\u5165\u80dc\u7684\u5185\u5bb9\u3002\u7cfb\u7edf\u7531(M)LLMs\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u7b49\u751f\u6210\u5f0fAI\u6a21\u578b\u9a71\u52a8\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165FilmEval\uff0c\u4e00\u4e2a\u8bc4\u4f30AI\u751f\u6210\u7535\u5f71\u7684\u5168\u9762\u57fa\u51c6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFilMaster\u5728\u955c\u5934\u8bed\u8a00\u8bbe\u8ba1\u548c\u7535\u5f71\u8282\u594f\u63a7\u5236\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u4e13\u4e1a\u7535\u5f71\u5236\u4f5c\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.18900", "pdf": "https://arxiv.org/pdf/2506.18900", "abs": "https://arxiv.org/abs/2506.18900", "authors": ["Kiymet Akdemir", "Tahira Kazimi", "Pinar Yanardag"], "title": "Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Project webpage: https://auditandrepair.github.io/", "summary": "Story visualization has become a popular task where visual scenes are\ngenerated to depict a narrative across multiple panels. A central challenge in\nthis setting is maintaining visual consistency, particularly in how characters\nand objects persist and evolve throughout the story. Despite recent advances in\ndiffusion models, current approaches often fail to preserve key character\nattributes, leading to incoherent narratives. In this work, we propose a\ncollaborative multi-agent framework that autonomously identifies, corrects, and\nrefines inconsistencies across multi-panel story visualizations. The agents\noperate in an iterative loop, enabling fine-grained, panel-level updates\nwithout re-generating entire sequences. Our framework is model-agnostic and\nflexibly integrates with a variety of diffusion models, including rectified\nflow transformers such as Flux and latent diffusion models such as Stable\nDiffusion. Quantitative and qualitative experiments show that our method\noutperforms prior approaches in terms of multi-panel consistency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u673a\u5236\u63d0\u5347\u591a\u9762\u677f\u53d9\u4e8b\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "motivation": "\u6545\u4e8b\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\uff0c\u4fdd\u6301\u89d2\u8272\u548c\u5bf9\u8c61\u5728\u591a\u9762\u677f\u4e2d\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u6269\u6563\u6a21\u578b\u5e38\u56e0\u65e0\u6cd5\u7ef4\u6301\u5173\u952e\u5c5e\u6027\u800c\u5bfc\u81f4\u53d9\u4e8b\u4e0d\u8fde\u8d2f\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u8bc6\u522b\u3001\u4fee\u6b63\u548c\u4f18\u5316\u591a\u9762\u677f\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u91c7\u7528\u8fed\u4ee3\u5faa\u73af\u673a\u5236\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u9762\u677f\u7ea7\u66f4\u65b0\uff0c\u65e0\u9700\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u5e8f\u5217\uff0c\u5e76\u53ef\u7075\u6d3b\u96c6\u6210\u591a\u79cd\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9762\u677f\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6545\u4e8b\u53ef\u89c6\u5316\u7684\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u4e14\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u7075\u6d3b\u6027\u3002", "paper_title_zh": "\u5ba1\u8ba1\u4e0e\u4fee\u590d\uff1a\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u4e00\u81f4\u6545\u4e8b\u53ef\u89c6\u5316\u7684\u667a\u80fd\u6846\u67b6", "abstract_zh": "\u6545\u4e8b\u53ef\u89c6\u5316\u5df2\u6210\u4e3a\u4e00\u9879\u6d41\u884c\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u4e2a\u9762\u677f\u751f\u6210\u89c6\u89c9\u573a\u666f\u4ee5\u63cf\u7ed8\u53d9\u4e8b\u3002\u8be5\u4efb\u52a1\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u89d2\u8272\u548c\u5bf9\u8c61\u5728\u6545\u4e8b\u4e2d\u7684\u6301\u7eed\u6027\u548c\u6f14\u53d8\u3002\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u8fd1\u671f\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u65e0\u6cd5\u4fdd\u7559\u5173\u952e\u89d2\u8272\u5c5e\u6027\uff0c\u5bfc\u81f4\u53d9\u4e8b\u4e0d\u8fde\u8d2f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u8bc6\u522b\u3001\u4fee\u6b63\u548c\u4f18\u5316\u591a\u9762\u677f\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u667a\u80fd\u4f53\u5728\u8fed\u4ee3\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u9762\u677f\u7ea7\u66f4\u65b0\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u5e8f\u5217\u3002\u8be5\u6846\u67b6\u4e0e\u591a\u79cd\u6269\u6563\u6a21\u578b\u517c\u5bb9\uff0c\u5305\u62ecFlux\u7b49\u6574\u6d41\u6d41\u53d8\u6362\u5668\u548cStable Diffusion\u7b49\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u591a\u9762\u677f\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18901", "pdf": "https://arxiv.org/pdf/2506.18901", "abs": "https://arxiv.org/abs/2506.18901", "authors": ["Wenqiang Sun", "Fangyun Wei", "Jinjing Zhao", "Xi Chen", "Zilong Chen", "Hongyang Zhang", "Jun Zhang", "Yan Lu"], "title": "From Virtual Games to Real-World Play", "categories": ["cs.CV"], "comment": "Project page: https://wenqsun.github.io/RealPlay/", "summary": "We introduce RealPlay, a neural network-based real-world game engine that\nenables interactive video generation from user control signals. Unlike prior\nworks focused on game-style visuals, RealPlay aims to produce photorealistic,\ntemporally consistent video sequences that resemble real-world footage. It\noperates in an interactive loop: users observe a generated scene, issue a\ncontrol command, and receive a short video chunk in response. To enable such\nrealistic and responsive generation, we address key challenges including\niterative chunk-wise prediction for low-latency feedback, temporal consistency\nacross iterations, and accurate control response. RealPlay is trained on a\ncombination of labeled game data and unlabeled real-world videos, without\nrequiring real-world action annotations. Notably, we observe two forms of\ngeneralization: (1) control transfer-RealPlay effectively maps control signals\nfrom virtual to real-world scenarios; and (2) entity transfer-although training\nlabels originate solely from a car racing game, RealPlay generalizes to control\ndiverse real-world entities, including bicycles and pedestrians, beyond\nvehicles. Project page can be found: https://wenqsun.github.io/RealPlay/", "AI": {"tldr": "RealPlay\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u771f\u5b9e\u4e16\u754c\u6e38\u620f\u5f15\u64ce\uff0c\u80fd\u591f\u901a\u8fc7\u7528\u6237\u63a7\u5236\u4fe1\u53f7\u751f\u6210\u4ea4\u4e92\u5f0f\u89c6\u9891\u3002\u5b83\u4e13\u6ce8\u4e8e\u751f\u6210\u903c\u771f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5757\u9884\u6d4b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6e38\u620f\u98ce\u683c\u7684\u89c6\u89c9\u6548\u679c\uff0c\u800cRealPlay\u65e8\u5728\u751f\u6210\u903c\u771f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53cd\u9988\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "RealPlay\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u6807\u8bb0\u7684\u6e38\u620f\u6570\u636e\u548c\u65e0\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u6807\u6ce8\u3002\u901a\u8fc7\u8fed\u4ee3\u5757\u9884\u6d4b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u53cd\u9988\uff0c\u5e76\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u63a7\u5236\u51c6\u786e\u6027\u3002", "result": "RealPlay\u5b9e\u73b0\u4e86\u4e24\u79cd\u6cdb\u5316\u80fd\u529b\uff1a\u63a7\u5236\u4fe1\u53f7\u4ece\u865a\u62df\u573a\u666f\u5230\u771f\u5b9e\u573a\u666f\u7684\u6620\u5c04\uff0c\u4ee5\u53ca\u4ece\u8d5b\u8f66\u6e38\u620f\u6807\u7b7e\u6269\u5c55\u5230\u63a7\u5236\u81ea\u884c\u8f66\u3001\u884c\u4eba\u7b49\u591a\u6837\u5316\u5b9e\u4f53\u3002", "conclusion": "RealPlay\u5c55\u793a\u4e86\u5728\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u903c\u771f\u4ea4\u4e92\u5f0f\u89c6\u9891\u7684\u6f5c\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u63a7\u5236\u4fe1\u53f7\u548c\u5b9e\u4f53\u6cdb\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "paper_title_zh": "\u4ece\u865a\u62df\u6e38\u620f\u5230\u771f\u5b9e\u4e16\u754c\u6e38\u620f", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86RealPlay\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u771f\u5b9e\u4e16\u754c\u6e38\u620f\u5f15\u64ce\uff0c\u80fd\u591f\u901a\u8fc7\u7528\u6237\u63a7\u5236\u4fe1\u53f7\u751f\u6210\u4ea4\u4e92\u5f0f\u89c6\u9891\u3002\u4e0e\u4ee5\u5f80\u4e13\u6ce8\u4e8e\u6e38\u620f\u98ce\u683c\u89c6\u89c9\u6548\u679c\u7684\u7814\u7a76\u4e0d\u540c\uff0cRealPlay\u65e8\u5728\u751f\u6210\u903c\u771f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u573a\u666f\u3002\u5176\u5de5\u4f5c\u6a21\u5f0f\u4e3a\u4ea4\u4e92\u5f0f\u5faa\u73af\uff1a\u7528\u6237\u89c2\u5bdf\u751f\u6210\u7684\u573a\u666f\uff0c\u53d1\u51fa\u63a7\u5236\u547d\u4ee4\uff0c\u5e76\u6536\u5230\u4e00\u6bb5\u77ed\u89c6\u9891\u4f5c\u4e3a\u54cd\u5e94\u3002\u4e3a\u5b9e\u73b0\u8fd9\u79cd\u903c\u771f\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u751f\u6210\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u4f4e\u5ef6\u8fdf\u53cd\u9988\u7684\u8fed\u4ee3\u5757\u9884\u6d4b\u3001\u8de8\u8fed\u4ee3\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u4ee5\u53ca\u51c6\u786e\u7684\u63a7\u5236\u54cd\u5e94\u3002RealPlay\u901a\u8fc7\u7ed3\u5408\u6807\u8bb0\u7684\u6e38\u620f\u6570\u636e\u548c\u65e0\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u6807\u6ce8\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e24\u79cd\u6cdb\u5316\u5f62\u5f0f\uff1a\uff081\uff09\u63a7\u5236\u4fe1\u53f7\u4ece\u865a\u62df\u5230\u771f\u5b9e\u573a\u666f\u7684\u6709\u6548\u6620\u5c04\uff1b\uff082\uff09\u5c3d\u7ba1\u8bad\u7ec3\u6807\u7b7e\u4ec5\u6765\u81ea\u8d5b\u8f66\u6e38\u620f\uff0c\u4f46RealPlay\u80fd\u591f\u6cdb\u5316\u63a7\u5236\u81ea\u884c\u8f66\u3001\u884c\u4eba\u7b49\u591a\u6837\u5316\u5b9e\u4f53\uff0c\u800c\u4e0d\u4ec5\u9650\u4e8e\u8f66\u8f86\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://wenqsun.github.io/RealPlay/"}}
{"id": "2506.18903", "pdf": "https://arxiv.org/pdf/2506.18903", "abs": "https://arxiv.org/abs/2506.18903", "authors": ["Runjia Li", "Philip Torr", "Andrea Vedaldi", "Tomas Jakab"], "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory", "categories": ["cs.CV"], "comment": "Project page: https://v-mem.github.io", "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVMem\u7684\u65b0\u578b\u5185\u5b58\u673a\u5236\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u4ea4\u4e92\u5f0f\u63a2\u7d22\u73af\u5883\u7684\u89c6\u9891\u751f\u6210\u5668\u3002\u901a\u8fc7\u51e0\u4f55\u7d22\u5f15\u8fc7\u53bb\u89c6\u56fe\uff0cVMem\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u573a\u666f\u5408\u6210\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e2D\u89c6\u56fe\u5916\u7ed8\u7684\u65b9\u6cd5\u4f1a\u5feb\u901f\u7d2f\u79ef\u8bef\u5dee\uff0c\u800c\u77ed\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u89c6\u9891\u751f\u6210\u5668\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u573a\u666f\u4e00\u81f4\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "VMem\u901a\u8fc73D\u8868\u9762\u5143\u7d20\uff08surfels\uff09\u51e0\u4f55\u7d22\u5f15\u8fc7\u53bb\u89c6\u56fe\uff0c\u4ec5\u68c0\u7d22\u4e0e\u751f\u6210\u65b0\u89c6\u56fe\u6700\u76f8\u5173\u7684\u5386\u53f2\u89c6\u56fe\uff0c\u4ece\u800c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u3002", "result": "\u5728\u957f\u671f\u573a\u666f\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVMem\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u4e00\u81f4\u6027\u548c\u76f8\u673a\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "VMem\u901a\u8fc7\u9ad8\u6548\u7d22\u5f15\u548c\u68c0\u7d22\u5386\u53f2\u89c6\u56fe\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u573a\u666f\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u5f0f\u63a2\u7d22\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "VMem\uff1a\u57fa\u4e8e\u8868\u9762\u5143\u7d20\u7d22\u5f15\u89c6\u56fe\u5185\u5b58\u7684\u4e00\u81f4\u6027\u4ea4\u4e92\u5f0f\u89c6\u9891\u573a\u666f\u751f\u6210", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5185\u5b58\u673a\u5236\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u4ea4\u4e92\u5f0f\u63a2\u7d22\u73af\u5883\u7684\u89c6\u9891\u751f\u6210\u5668\u3002\u4ee5\u5f80\u65b9\u6cd5\u901a\u8fc7\u5916\u7ed8\u573a\u666f\u76842D\u89c6\u56fe\u5e76\u9010\u6b65\u91cd\u5efa\u51763D\u51e0\u4f55\u7ed3\u6784\uff0c\u4f46\u4f1a\u5feb\u901f\u7d2f\u79ef\u8bef\u5dee\uff1b\u6216\u4f7f\u7528\u77ed\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u89c6\u9891\u751f\u6210\u5668\uff0c\u4f46\u96be\u4ee5\u957f\u671f\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8e3D\u8868\u9762\u5143\u7d20\uff08surfels\uff09\u51e0\u4f55\u7d22\u5f15\u7684\u89c6\u56fe\u5185\u5b58\uff08VMem\uff09\uff0c\u80fd\u591f\u9ad8\u6548\u68c0\u7d22\u4e0e\u751f\u6210\u65b0\u89c6\u56fe\u6700\u76f8\u5173\u7684\u5386\u53f2\u89c6\u56fe\u3002\u901a\u8fc7\u4ec5\u5173\u6ce8\u8fd9\u4e9b\u76f8\u5173\u89c6\u56fe\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ee5\u8fdc\u4f4e\u4e8e\u4f7f\u7528\u6240\u6709\u5386\u53f2\u89c6\u56fe\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u5bf9\u60f3\u8c61\u73af\u5883\u7684\u4e00\u81f4\u6027\u63a2\u7d22\u3002\u6211\u4eec\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u671f\u573a\u666f\u5408\u6210\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u548c\u76f8\u673a\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18904", "pdf": "https://arxiv.org/pdf/2506.18904", "abs": "https://arxiv.org/abs/2506.18904", "authors": ["Yang Liu", "Chuanchen Luo", "Zimo Tang", "Yingyan Li", "Yuran Yang", "Yuanyong Ning", "Lue Fan", "Junran Peng", "Zhaoxiang Zhang"], "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos", "categories": ["cs.CV"], "comment": "Project Page: https://dekuliutesla.github.io/tclight/ Code:\n  https://github.com/Linketic/TC-Light", "summary": "Editing illumination in long videos with complex dynamics has significant\nvalue in various downstream tasks, including visual content creation and\nmanipulation, as well as data scaling up for embodied AI through sim2real and\nreal2real transfer. Nevertheless, existing video relighting techniques are\npredominantly limited to portrait videos or fall into the bottleneck of\ntemporal consistency and computation efficiency. In this paper, we propose\nTC-Light, a novel paradigm characterized by the proposed two-stage post\noptimization mechanism. Starting from the video preliminarily relighted by an\ninflated video relighting model, it optimizes appearance embedding in the first\nstage to align global illumination. Then it optimizes the proposed canonical\nvideo representation, i.e., Unique Video Tensor (UVT), to align fine-grained\ntexture and lighting in the second stage. To comprehensively evaluate\nperformance, we also establish a long and highly dynamic video benchmark.\nExtensive experiments show that our method enables physically plausible\nrelighting results with superior temporal coherence and low computation cost.\nThe code and video demos are available at\nhttps://dekuliutesla.github.io/tclight/.", "AI": {"tldr": "TC-Light\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u91cd\u5149\u7167\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u673a\u5236\u5b9e\u73b0\u52a8\u6001\u957f\u89c6\u9891\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u91cd\u5149\u7167\uff0c\u5177\u6709\u9ad8\u6548\u8ba1\u7b97\u548c\u4f18\u8d8a\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u91cd\u5149\u7167\u6280\u672f\u4e3b\u8981\u5c40\u9650\u4e8e\u8096\u50cf\u89c6\u9891\u6216\u9762\u4e34\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u74f6\u9888\u3002\u52a8\u6001\u957f\u89c6\u9891\u7684\u91cd\u5149\u7167\u5728\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\u548c\u6570\u636e\u6269\u5c55\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u65b9\u6cd5\u3002", "method": "TC-Light\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u673a\u5236\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f18\u5316\u5916\u89c2\u5d4c\u5165\u4ee5\u5bf9\u9f50\u5168\u5c40\u5149\u7167\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u63d0\u51fa\u7684\u89c4\u8303\u89c6\u9891\u8868\u793a\uff08UVT\uff09\u4ee5\u5bf9\u9f50\u7ec6\u7c92\u5ea6\u7eb9\u7406\u548c\u5149\u7167\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTC-Light\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e0a\u5408\u7406\u4e14\u65f6\u95f4\u8fde\u8d2f\u7684\u91cd\u5149\u7167\u7ed3\u679c\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "TC-Light\u4e3a\u52a8\u6001\u957f\u89c6\u9891\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u91cd\u5149\u7167\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "paper_title_zh": "TC-Light\uff1a\u52a8\u6001\u957f\u89c6\u9891\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u91cd\u5149\u7167", "abstract_zh": "\u5728\u52a8\u6001\u957f\u89c6\u9891\u4e2d\u7f16\u8f91\u5149\u7167\u5bf9\u4e8e\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\u548c\u6570\u636e\u6269\u5c55\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u7136\u800c\uff0c\u73b0\u6709\u89c6\u9891\u91cd\u5149\u7167\u6280\u672f\u4e3b\u8981\u5c40\u9650\u4e8e\u8096\u50cf\u89c6\u9891\u6216\u9762\u4e34\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u74f6\u9888\u3002\u672c\u6587\u63d0\u51faTC-Light\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e24\u9636\u6bb5\u4f18\u5316\u673a\u5236\u7684\u65b0\u8303\u5f0f\u3002\u9996\u5148\u901a\u8fc7\u81a8\u80c0\u89c6\u9891\u91cd\u5149\u7167\u6a21\u578b\u521d\u6b65\u91cd\u5149\u7167\u89c6\u9891\uff0c\u7b2c\u4e00\u9636\u6bb5\u4f18\u5316\u5916\u89c2\u5d4c\u5165\u4ee5\u5bf9\u9f50\u5168\u5c40\u5149\u7167\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u63d0\u51fa\u7684\u89c4\u8303\u89c6\u9891\u8868\u793a\uff08Unique Video Tensor, UVT\uff09\u4ee5\u5bf9\u9f50\u7ec6\u7c92\u5ea6\u7eb9\u7406\u548c\u5149\u7167\u3002\u6211\u4eec\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u957f\u4e14\u9ad8\u5ea6\u52a8\u6001\u7684\u89c6\u9891\u57fa\u51c6\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e0a\u5408\u7406\u4e14\u65f6\u95f4\u8fde\u8d2f\u7684\u91cd\u5149\u7167\u7ed3\u679c\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4f4e\u3002\u4ee3\u7801\u548c\u89c6\u9891\u6f14\u793a\u53ef\u5728https://dekuliutesla.github.io/tclight/\u83b7\u53d6\u3002"}}
{"id": "2506.17919", "pdf": "https://arxiv.org/pdf/2506.17919", "abs": "https://arxiv.org/abs/2506.17919", "authors": ["Zhiyu Mou", "Miao Xu", "Wei Chen", "Rongquan Bai", "Chuan Yu", "Jian Xu"], "title": "Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) for auto-bidding has shifted from using\nsimplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL\non fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are\nlimited by the dataset's state space coverage, offering modest gains. While\nSRLB expands state coverage, its simulator-reality gap risks misleading\npolicies. This paper introduces Model-based RL Bidding (MRLB), which learns an\nenvironment model from real data to bridge this gap. MRLB trains policies using\nboth real and model-generated data, expanding state coverage beyond ORLB. To\nensure model reliability, we propose: 1) A permutation equivariant model\narchitecture for better generalization, and 2) A robust offline Q-learning\nmethod that pessimistically penalizes model errors. These form the Permutation\nEquivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments\nshow that PE-MORL outperforms state-of-the-art auto-bidding methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u6362\u7b49\u53d8\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08PE-MORL\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u7ade\u4ef7\uff0c\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u548c\u6a21\u578b\u751f\u6210\u6570\u636e\u6269\u5c55\u72b6\u6001\u8986\u76d6\u8303\u56f4\uff0c\u5e76\u91c7\u7528\u60b2\u89c2\u60e9\u7f5a\u673a\u5236\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u7ade\u4ef7\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5\uff08ORLB\uff09\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u72b6\u6001\u8986\u76d6\u8303\u56f4\uff0c\u800c\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u65b9\u6cd5\uff08SRLB\uff09\u5b58\u5728\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u5b66\u4e60\u7ed3\u5408\u771f\u5b9e\u4e0e\u6a21\u62df\u6570\u636e\uff0c\u63d0\u5347\u81ea\u52a8\u7ade\u4ef7\u6548\u679c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5\uff08MRLB\uff09\uff0c\u5305\u62ec\u7f6e\u6362\u7b49\u53d8\u6a21\u578b\u67b6\u6784\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u60b2\u89c2\u60e9\u7f5a\u7684\u79bb\u7ebfQ\u5b66\u4e60\u65b9\u6cd5\uff08PE-MORL\uff09\uff0c\u7ed3\u5408\u771f\u5b9e\u4e0e\u6a21\u578b\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPE-MORL\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u72b6\u6001\u8986\u76d6\u8303\u56f4\u5e76\u63d0\u9ad8\u4e86\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "PE-MORL\u901a\u8fc7\u6a21\u578b\u5b66\u4e60\u548c\u60b2\u89c2\u60e9\u7f5a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u7ade\u4ef7\u4e2d\u72b6\u6001\u8986\u76d6\u4e0d\u8db3\u548c\u6a21\u62df\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u7ade\u4ef7\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u57fa\u4e8e\u7f6e\u6362\u7b49\u53d8\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5", "abstract_zh": "\u81ea\u52a8\u7ade\u4ef7\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ece\u57fa\u4e8e\u79bb\u7ebf\u6a21\u62df\u5668\uff08SRLB\uff09\u8f6c\u5411\u57fa\u4e8e\u56fa\u5b9a\u771f\u5b9e\u6570\u636e\u96c6\u7684\u79bb\u7ebfRL\uff08ORLB\uff09\u3002\u7136\u800c\uff0cORLB\u7b56\u7565\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u7684\u72b6\u6001\u8986\u76d6\u8303\u56f4\uff0c\u63d0\u5347\u6709\u9650\u3002\u867d\u7136SRLB\u6269\u5c55\u4e86\u72b6\u6001\u8986\u76d6\uff0c\u4f46\u5176\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u8bef\u5bfc\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684RL\u7ade\u4ef7\u65b9\u6cd5\uff08MRLB\uff09\uff0c\u901a\u8fc7\u4ece\u771f\u5b9e\u6570\u636e\u5b66\u4e60\u73af\u5883\u6a21\u578b\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002MRLB\u7ed3\u5408\u771f\u5b9e\u4e0e\u6a21\u578b\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7b56\u7565\uff0c\u6269\u5c55\u4e86\u72b6\u6001\u8986\u76d6\u8303\u56f4\u3002\u4e3a\u786e\u4fdd\u6a21\u578b\u53ef\u9760\u6027\uff0c\u6211\u4eec\u63d0\u51fa\uff1a1\uff09\u7f6e\u6362\u7b49\u53d8\u6a21\u578b\u67b6\u6784\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b2\uff09\u60b2\u89c2\u60e9\u7f5a\u6a21\u578b\u8bef\u5dee\u7684\u9c81\u68d2\u79bb\u7ebfQ\u5b66\u4e60\u65b9\u6cd5\u3002\u4e8c\u8005\u6784\u6210\u7f6e\u6362\u7b49\u53d8\u6a21\u578b\u79bb\u7ebfRL\u7b97\u6cd5\uff08PE-MORL\uff09\u3002\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0cPE-MORL\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5\u3002"}}
{"id": "2506.17307", "pdf": "https://arxiv.org/pdf/2506.17307", "abs": "https://arxiv.org/abs/2506.17307", "authors": ["Zhixiang Chi", "Li Gu", "Huan Liu", "Ziqiang Wang", "Yanan Wu", "Yang Wang", "Konstantinos N Plataniotis"], "title": "Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "ICLR2025,https://github.com/chi-chi-zx/L2C", "summary": "Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time\nto a specific domain using only a few unlabeled examples, addressing domain\nshift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities\nby generating domain-specific prompts to guide its generalized, frozen\nfeatures. However, since downstream datasets are not explicitly seen by CLIP,\nsolely depending on the feature space knowledge is constrained by CLIP's prior\nknowledge. Notably, when using a less robust backbone like ViT-B/16,\nperformance significantly drops on challenging real-world benchmarks. Departing\nfrom the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,\nthis work introduces learning directly on the input space to complement the\ndataset-specific knowledge for frozen CLIP. Specifically, an independent side\nbranch is attached in parallel with CLIP and enforced to learn exclusive\nknowledge via revert attention. To better capture the dataset-specific label\nsemantics for downstream adaptation, we propose to enhance the inter-dispersion\namong text features via greedy text ensemble and refinement. The text and\nvisual features are then progressively fused in a domain-aware manner by a\ngenerated domain prompt to adapt toward a specific domain. Extensive\nexperiments show our method's superiority on 5 large-scale benchmarks (WILDS\nand DomainNet), notably improving over smaller networks like ViT-B/16 with\ngains of \\textbf{+5.1} in F1 for iWildCam and \\textbf{+3.1\\%} in WC Acc for\nFMoW.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u8865\u5145\u51bb\u7ed3CLIP\u7684\u7279\u5b9a\u6570\u636e\u96c6\u77e5\u8bc6\uff0c\u7ed3\u5408\u8d2a\u5a6a\u6587\u672c\u96c6\u6210\u548c\u9886\u57df\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u6d4b\u8bd5\u65f6\u57df\u9002\u5e94\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56CLIP\u7684\u9884\u8bad\u7ec3\u7279\u5f81\u7a7a\u95f4\u77e5\u8bc6\uff0c\u4f46\u9762\u5bf9\u672a\u663e\u5f0f\u89c1\u8fc7\u7684\u4e0b\u6e38\u6570\u636e\u96c6\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5f31\u7684\u4e3b\u5e72\u7f51\u7edc\uff08\u5982ViT-B/16\uff09\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u8f93\u5165\u7a7a\u95f4\u77e5\u8bc6\u6765\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u4e8eCLIP\u7684\u72ec\u7acb\u5206\u652f\uff0c\u901a\u8fc7\u53cd\u5411\u6ce8\u610f\u529b\u5b66\u4e60\u4e13\u5c5e\u77e5\u8bc6\uff1b\u91c7\u7528\u8d2a\u5a6a\u6587\u672c\u96c6\u6210\u589e\u5f3a\u6587\u672c\u7279\u5f81\u7684\u5206\u6563\u6027\uff1b\u901a\u8fc7\u751f\u6210\u7684\u9886\u57df\u63d0\u793a\u9010\u6b65\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5b9e\u73b0\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u3002", "result": "\u57285\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff08WILDS\u548cDomainNet\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86ViT-B/16\u7b49\u5c0f\u578b\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5982iWildCam\u7684F1\u5206\u6570\u63d0\u9ad8\u4e865.1\uff0cFMoW\u7684WC\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.1%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8f93\u5165\u7a7a\u95f4\u5b66\u4e60\u548c\u9886\u57df\u63d0\u793a\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u6d4b\u8bd5\u65f6\u57df\u9002\u5e94\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5f31\u7684\u4e3b\u5e72\u7f51\u7edc\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "paper_title_zh": "\u5b66\u4e60\u9002\u5e94\u51bb\u7ed3CLIP\u7684\u5c0f\u6837\u672c\u6d4b\u8bd5\u65f6\u57df\u9002\u5e94", "abstract_zh": "\u5c0f\u6837\u672c\u6d4b\u8bd5\u65f6\u57df\u9002\u5e94\u65e8\u5728\u5229\u7528\u5c11\u91cf\u672a\u6807\u8bb0\u6837\u672c\u5728\u6d4b\u8bd5\u65f6\u5c06\u6a21\u578b\u9002\u5e94\u5230\u7279\u5b9a\u9886\u57df\uff0c\u4ee5\u89e3\u51b3\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u9886\u57df\u7279\u5b9a\u63d0\u793a\u6765\u5229\u7528CLIP\u7684\u5f3a\u5927\u5206\u5e03\u5916\uff08OOD\uff09\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u4e0b\u6e38\u6570\u636e\u96c6\u672a\u88abCLIP\u663e\u5f0f\u89c1\u8fc7\uff0c\u4ec5\u4f9d\u8d56\u7279\u5f81\u7a7a\u95f4\u77e5\u8bc6\u53d7\u9650\u4e8eCLIP\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u5c24\u5176\u662f\u4f7f\u7528\u8f83\u5f31\u7684\u4e3b\u5e72\u7f51\u7edc\uff08\u5982ViT-B/16\uff09\u65f6\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u4e0d\u540c\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u76f4\u63a5\u5728\u8f93\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u4ee5\u8865\u5145\u51bb\u7ed3CLIP\u7684\u7279\u5b9a\u6570\u636e\u96c6\u77e5\u8bc6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728CLIP\u65c1\u5e76\u884c\u9644\u52a0\u4e00\u4e2a\u72ec\u7acb\u5206\u652f\uff0c\u901a\u8fc7\u53cd\u5411\u6ce8\u610f\u529b\u5f3a\u5236\u5b66\u4e60\u4e13\u5c5e\u77e5\u8bc6\uff1b\u4e3a\u66f4\u597d\u5730\u6355\u6349\u4e0b\u6e38\u9002\u5e94\u7684\u7279\u5b9a\u6807\u7b7e\u8bed\u4e49\uff0c\u63d0\u51fa\u901a\u8fc7\u8d2a\u5a6a\u6587\u672c\u96c6\u6210\u548c\u7ec6\u5316\u589e\u5f3a\u6587\u672c\u7279\u5f81\u7684\u5206\u6563\u6027\uff1b\u968f\u540e\u901a\u8fc7\u751f\u6210\u7684\u9886\u57df\u63d0\u793a\u9010\u6b65\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5b9e\u73b0\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u65b9\u6cd5\u57285\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff08WILDS\u548cDomainNet\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86ViT-B/16\u7b49\u5c0f\u578b\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5982iWildCam\u7684F1\u5206\u6570\u63d0\u9ad8\u4e865.1\uff0cFMoW\u7684WC\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.1%\u3002"}}
{"id": "2506.17929", "pdf": "https://arxiv.org/pdf/2506.17929", "abs": "https://arxiv.org/abs/2506.17929", "authors": ["Shulun Chen", "Wei Shao", "Flora D. Salim", "Hao Xue"], "title": "ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation", "categories": ["cs.LG", "cs.AI"], "comment": "ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic\n  Resource Allocation", "summary": "Supporting decision-making has long been a central vision in the field of\nspatio-temporal intelligence. While prior work has improved the timeliness and\naccuracy of spatio-temporal forecasting, converting these forecasts into\nactionable strategies remains a key challenge. A main limitation is the\ndecoupling of the prediction and the downstream decision phases, which can\nsignificantly degrade the downstream efficiency. For example, in emergency\nresponse, the priority is successful resource allocation and intervention, not\njust incident prediction. To this end, it is essential to propose an Adaptive\nSpatio-Temporal Early Decision model (ASTER) that reforms the forecasting\nparadigm from event anticipation to actionable decision support. This framework\nensures that information is directly used for decision-making, thereby\nmaximizing overall effectiveness. Specifically, ASTER introduces a new\nResource-aware Spatio-Temporal interaction module (RaST) that adaptively\ncaptures long- and short-term dependencies under dynamic resource conditions,\nproducing context-aware spatiotemporal representations. To directly generate\nactionable decisions, we further design a Preference-oriented decision agent\n(Poda) based on multi-objective reinforcement learning, which transforms\npredictive signals into resource-efficient intervention strategies by deriving\noptimal actions under specific preferences and dynamic constraints.\nExperimental results on four benchmark datasets demonstrate the\nstate-of-the-art performance of ASTER in improving both early prediction\naccuracy and resource allocation outcomes across six downstream metrics.", "AI": {"tldr": "ASTER\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65f6\u7a7a\u65e9\u671f\u51b3\u7b56\u6a21\u578b\uff0c\u901a\u8fc7\u8d44\u6e90\u611f\u77e5\u65f6\u7a7a\u4ea4\u4e92\u6a21\u5757\u548c\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5c06\u9884\u6d4b\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u9884\u6d4b\u7814\u7a76\u591a\u5173\u6ce8\u9884\u6d4b\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f46\u5982\u4f55\u5c06\u9884\u6d4b\u7ed3\u679c\u8f6c\u5316\u4e3a\u5b9e\u9645\u51b3\u7b56\uff08\u5982\u5e94\u6025\u8d44\u6e90\u5206\u914d\uff09\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002ASTER\u65e8\u5728\u89e3\u51b3\u9884\u6d4b\u4e0e\u51b3\u7b56\u8131\u8282\u7684\u95ee\u9898\uff0c\u76f4\u63a5\u652f\u6301\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u3002", "method": "ASTER\u8bbe\u8ba1\u4e86\u8d44\u6e90\u611f\u77e5\u65f6\u7a7a\u4ea4\u4e92\u6a21\u5757\uff08RaST\uff09\uff0c\u52a8\u6001\u6355\u6349\u957f\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u4e86\u504f\u597d\u5bfc\u5411\u51b3\u7b56\u4ee3\u7406\uff08Poda\uff09\uff0c\u5c06\u9884\u6d4b\u4fe1\u53f7\u8f6c\u5316\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u5e72\u9884\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cASTER\u5728\u65e9\u671f\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8d44\u6e90\u5206\u914d\u6548\u679c\u4e0a\u5747\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u516d\u9879\u4e0b\u6e38\u6307\u6807\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ASTER\u901a\u8fc7\u6574\u5408\u9884\u6d4b\u4e0e\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u667a\u80fd\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u4e3a\u52a8\u6001\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "ASTER\uff1a\u9762\u5411\u52a8\u6001\u8d44\u6e90\u5206\u914d\u7684\u81ea\u9002\u5e94\u65f6\u7a7a\u65e9\u671f\u51b3\u7b56\u6a21\u578b", "abstract_zh": "\u652f\u6301\u51b3\u7b56\u4e00\u76f4\u662f\u65f6\u7a7a\u667a\u80fd\u9886\u57df\u7684\u6838\u5fc3\u76ee\u6807\u3002\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u63d0\u5347\u4e86\u65f6\u7a7a\u9884\u6d4b\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f46\u5982\u4f55\u5c06\u8fd9\u4e9b\u9884\u6d4b\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u4ecd\u662f\u5173\u952e\u6311\u6218\u3002\u4e3b\u8981\u9650\u5236\u5728\u4e8e\u9884\u6d4b\u4e0e\u4e0b\u6e38\u51b3\u7b56\u9636\u6bb5\u7684\u8131\u8282\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u4e0b\u6e38\u6548\u7387\u3002\u4f8b\u5982\uff0c\u5728\u5e94\u6025\u54cd\u5e94\u4e2d\uff0c\u91cd\u70b9\u662f\u6210\u529f\u7684\u8d44\u6e90\u5206\u914d\u548c\u5e72\u9884\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e8b\u4ef6\u9884\u6d4b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65f6\u7a7a\u65e9\u671f\u51b3\u7b56\u6a21\u578b\uff08ASTER\uff09\uff0c\u5c06\u9884\u6d4b\u8303\u5f0f\u4ece\u4e8b\u4ef6\u9884\u5224\u8f6c\u53d8\u4e3a\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u652f\u6301\u3002\u8be5\u6846\u67b6\u786e\u4fdd\u4fe1\u606f\u76f4\u63a5\u7528\u4e8e\u51b3\u7b56\uff0c\u4ece\u800c\u6700\u5927\u5316\u6574\u4f53\u6548\u679c\u3002\u5177\u4f53\u800c\u8a00\uff0cASTER\u5f15\u5165\u4e86\u8d44\u6e90\u611f\u77e5\u65f6\u7a7a\u4ea4\u4e92\u6a21\u5757\uff08RaST\uff09\uff0c\u52a8\u6001\u6355\u6349\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u957f\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65f6\u7a7a\u8868\u5f81\u3002\u4e3a\u4e86\u76f4\u63a5\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u504f\u597d\u5bfc\u5411\u51b3\u7b56\u4ee3\u7406\uff08Poda\uff09\uff0c\u901a\u8fc7\u7279\u5b9a\u504f\u597d\u548c\u52a8\u6001\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u884c\u52a8\uff0c\u5c06\u9884\u6d4b\u4fe1\u53f7\u8f6c\u5316\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u5e72\u9884\u7b56\u7565\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cASTER\u5728\u65e9\u671f\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8d44\u6e90\u5206\u914d\u6548\u679c\u4e0a\u5747\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u516d\u9879\u4e0b\u6e38\u6307\u6807\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2506.17320", "pdf": "https://arxiv.org/pdf/2506.17320", "abs": "https://arxiv.org/abs/2506.17320", "authors": ["Akash Awasthi", "Brandon V. Chang", "Anh M. Vu", "Ngan Le", "Rishi Agrawal", "Zhigang Deng", "Carol Wu", "Hien Van Nguyen"], "title": "MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant", "categories": ["cs.CY", "cs.CV", "cs.LG"], "comment": "Accepted to MICCAI 2025 (Main Conference)", "summary": "Radiology students often struggle to develop perceptual expertise due to\nlimited expert mentorship time, leading to errors in visual search and\ndiagnostic interpretation. These perceptual errors, such as missed fixations,\nshort dwell times, or misinterpretations, are not adequately addressed by\ncurrent AI systems, which focus on diagnostic accuracy but fail to explain how\nand why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic\nAdaptive Radiology Teaching Assistant), a multi-agent framework that analyzes\ngaze patterns and radiology reports to provide personalized feedback. Unlike\nsingle-agent models, MAARTA dynamically selects agents based on error\ncomplexity, enabling adaptive and efficient reasoning. By comparing expert and\nstudent gaze behavior through structured graphs, the system identifies missed\nfindings and assigns Perceptual Error Teacher agents to analyze discrepancies.\nMAARTA then uses step-by-step prompting to help students understand their\nerrors and improve diagnostic reasoning, advancing AI-driven radiology\neducation.", "AI": {"tldr": "MAARTA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u7ebf\u6a21\u5f0f\u548c\u653e\u5c04\u5b66\u62a5\u544a\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u5e2e\u52a9\u653e\u5c04\u5b66\u5b66\u751f\u51cf\u5c11\u89c6\u89c9\u641c\u7d22\u548c\u8bca\u65ad\u89e3\u91ca\u4e2d\u7684\u9519\u8bef\u3002", "motivation": "\u653e\u5c04\u5b66\u5b66\u751f\u56e0\u7f3a\u4e4f\u4e13\u5bb6\u6307\u5bfc\u65f6\u95f4\uff0c\u5e38\u51fa\u73b0\u89c6\u89c9\u641c\u7d22\u548c\u8bca\u65ad\u89e3\u91ca\u9519\u8bef\uff0c\u73b0\u6709AI\u7cfb\u7edf\u4ec5\u5173\u6ce8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u65e0\u6cd5\u89e3\u91ca\u9519\u8bef\u539f\u56e0\u3002MAARTA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MAARTA\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u667a\u80fd\u4f53\u5206\u6790\u9519\u8bef\u590d\u6742\u6027\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u56fe\u8868\u6bd4\u8f83\u4e13\u5bb6\u4e0e\u5b66\u751f\u89c6\u7ebf\u884c\u4e3a\uff0c\u8bc6\u522b\u9057\u6f0f\u53d1\u73b0\u5e76\u5206\u914d\u611f\u77e5\u9519\u8bef\u6559\u5e08\u667a\u80fd\u4f53\u5206\u6790\u5dee\u5f02\uff0c\u9010\u6b65\u63d0\u793a\u5b66\u751f\u6539\u8fdb\u3002", "result": "MAARTA\u80fd\u591f\u8bc6\u522b\u5b66\u751f\u4e0e\u4e13\u5bb6\u89c6\u7ebf\u884c\u4e3a\u7684\u5dee\u5f02\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u9519\u8bef\u5e76\u63d0\u5347\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "MAARTA\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u653e\u5c04\u5b66\u6559\u80b2\u53d1\u5c55\uff0c\u6709\u6548\u5e2e\u52a9\u5b66\u751f\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u3002", "paper_title_zh": "MAARTA\uff1a\u591a\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u653e\u5c04\u5b66\u6559\u5b66\u52a9\u624b", "abstract_zh": "\u653e\u5c04\u5b66\u5b66\u751f\u5e38\u56e0\u4e13\u5bb6\u6307\u5bfc\u65f6\u95f4\u6709\u9650\u800c\u96be\u4ee5\u53d1\u5c55\u611f\u77e5\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5bfc\u81f4\u89c6\u89c9\u641c\u7d22\u548c\u8bca\u65ad\u89e3\u91ca\u9519\u8bef\u3002\u8fd9\u4e9b\u9519\u8bef\uff08\u5982\u9057\u6f0f\u6ce8\u89c6\u3001\u77ed\u6682\u505c\u7559\u6216\u8bef\u5224\uff09\u672a\u88ab\u5f53\u524dAI\u7cfb\u7edf\u5145\u5206\u89e3\u51b3\uff0c\u540e\u8005\u4ec5\u5173\u6ce8\u8bca\u65ad\u51c6\u786e\u6027\u800c\u672a\u80fd\u89e3\u91ca\u9519\u8bef\u539f\u56e0\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faMAARTA\uff08\u591a\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u653e\u5c04\u5b66\u6559\u5b66\u52a9\u624b\uff09\uff0c\u4e00\u4e2a\u901a\u8fc7\u5206\u6790\u89c6\u7ebf\u6a21\u5f0f\u548c\u653e\u5c04\u5b66\u62a5\u544a\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u4e0e\u5355\u667a\u80fd\u4f53\u6a21\u578b\u4e0d\u540c\uff0cMAARTA\u6839\u636e\u9519\u8bef\u590d\u6742\u6027\u52a8\u6001\u9009\u62e9\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u9ad8\u6548\u63a8\u7406\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u56fe\u8868\u6bd4\u8f83\u4e13\u5bb6\u4e0e\u5b66\u751f\u89c6\u7ebf\u884c\u4e3a\uff0c\u7cfb\u7edf\u8bc6\u522b\u9057\u6f0f\u53d1\u73b0\u5e76\u5206\u914d\u611f\u77e5\u9519\u8bef\u6559\u5e08\u667a\u80fd\u4f53\u5206\u6790\u5dee\u5f02\u3002MAARTA\u901a\u8fc7\u9010\u6b65\u63d0\u793a\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u9519\u8bef\u5e76\u6539\u8fdb\u8bca\u65ad\u63a8\u7406\uff0c\u63a8\u52a8AI\u9a71\u52a8\u7684\u653e\u5c04\u5b66\u6559\u80b2\u53d1\u5c55\u3002"}}
{"id": "2506.17324", "pdf": "https://arxiv.org/pdf/2506.17324", "abs": "https://arxiv.org/abs/2506.17324", "authors": ["Emma Finn", "T. Anderson Keller", "Manos Theodosis", "Demba E. Ba"], "title": "Origins of Creativity in Attention-Based Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "As diffusion models have become the tool of choice for image generation and\nas the quality of the images continues to improve, the question of how\n`creativity' originates in diffusion has become increasingly important. The\nscore matching perspective on diffusion has proven particularly fruitful for\nunderstanding how and why diffusion models generate images that remain\nplausible while differing significantly from their training images. In\nparticular, as explained in (Kamb \\& Ganguli, 2024) and others, e.g.,\n(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we\nwould only be able to recover training samples through our diffusion process.\nHowever, as shown by Kamb \\& Ganguli, (2024), in diffusion models where the\nscore is parametrized by a simple CNN, the inductive biases of the CNN itself\n(translation equivariance and locality) allow the model to generate samples\nthat globally do not match any training samples, but are rather patch-wise\n`mosaics'. Notably, however, this theory does not extend to describe the role\nof self-attention in this process. In this work, we take a preliminary step in\nthis direction to extend this theory to the case of diffusion models whose\nscore is parametrized by a CNN with a final self-attention layer. We show that\nour theory suggests that self-attention will induce a globally image-consistent\narrangement of local features beyond the patch-level in generated samples, and\nwe verify this behavior empirically on a carefully crafted dataset.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578b\u4e2d\u521b\u9020\u529b\u7684\u8d77\u6e90\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7406\u8bba\u4ee5\u89e3\u91ca\u81ea\u6ce8\u610f\u529b\u5728\u751f\u6210\u5168\u5c40\u4e00\u81f4\u56fe\u50cf\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u6210\u4e3a\u56fe\u50cf\u751f\u6210\u7684\u9996\u9009\u5de5\u5177\uff0c\u7406\u89e3\u5176\u521b\u9020\u529b\u6765\u6e90\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7406\u8bba\u672a\u80fd\u89e3\u91ca\u81ea\u6ce8\u610f\u529b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6269\u5c55\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76\u5c06\u73b0\u6709CNN\u6269\u6563\u6a21\u578b\u7406\u8bba\u6269\u5c55\u5230\u5305\u542b\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u60c5\u51b5\uff0c\u5206\u6790\u5176\u5bf9\u56fe\u50cf\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u8868\u660e\u81ea\u6ce8\u610f\u529b\u80fd\u4fc3\u8fdb\u5c40\u90e8\u7279\u5f81\u7684\u5168\u5c40\u4e00\u81f4\u6027\u6392\u5217\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u81ea\u6ce8\u610f\u529b\u5728\u6269\u6563\u6a21\u578b\u4e2d\u8d77\u5230\u5173\u952e\u4f5c\u7528\uff0c\u4f7f\u751f\u6210\u7684\u56fe\u50cf\u5728\u5168\u5c40\u4e0a\u4fdd\u6301\u4e00\u81f4\uff0c\u8d85\u8d8a\u4e86\u5c40\u90e8\u62fc\u8d34\u7684\u9650\u5236\u3002", "paper_title_zh": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578b\u4e2d\u521b\u9020\u529b\u7684\u8d77\u6e90", "abstract_zh": "\u968f\u7740\u6269\u6563\u6a21\u578b\u6210\u4e3a\u56fe\u50cf\u751f\u6210\u7684\u9996\u9009\u5de5\u5177\uff0c\u56fe\u50cf\u8d28\u91cf\u4e0d\u65ad\u63d0\u5347\uff0c\u63a2\u8ba8\u6269\u6563\u6a21\u578b\u4e2d\u2018\u521b\u9020\u529b\u2019\u7684\u8d77\u6e90\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u5206\u6570\u5339\u914d\u89c6\u89d2\u4e3a\u7406\u89e3\u6269\u6563\u6a21\u578b\u5982\u4f55\u751f\u6210\u65e2\u5408\u7406\u53c8\u663e\u8457\u533a\u522b\u4e8e\u8bad\u7ec3\u56fe\u50cf\u7684\u56fe\u50cf\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002\u7136\u800c\uff0c\u73b0\u6709\u7406\u8bba\u4ec5\u9002\u7528\u4e8eCNN\u53c2\u6570\u5316\u7684\u6269\u6563\u6a21\u578b\uff0c\u672a\u80fd\u89e3\u91ca\u81ea\u6ce8\u610f\u529b\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002\u672c\u6587\u521d\u6b65\u5c06\u7406\u8bba\u6269\u5c55\u5230\u5305\u542b\u81ea\u6ce8\u610f\u529b\u5c42\u7684CNN\u6269\u6563\u6a21\u578b\uff0c\u8868\u660e\u81ea\u6ce8\u610f\u529b\u80fd\u4fc3\u8fdb\u751f\u6210\u6837\u672c\u4e2d\u5c40\u90e8\u7279\u5f81\u7684\u5168\u5c40\u4e00\u81f4\u6027\u6392\u5217\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u884c\u4e3a\u3002"}}
{"id": "2506.17934", "pdf": "https://arxiv.org/pdf/2506.17934", "abs": "https://arxiv.org/abs/2506.17934", "authors": ["Syed N. Sakib", "Kallol Naha", "Sajratul Y. Rubaiat", "Hasan M. Jamil"], "title": "A GenAI System for Improved FAIR Independent Biological Database Integration", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Life sciences research increasingly requires identifying, accessing, and\neffectively processing data from an ever-evolving array of information sources\non the Linked Open Data (LOD) network. This dynamic landscape places a\nsignificant burden on researchers, as the quality of query responses depends\nheavily on the selection and semantic integration of data sources --processes\nthat are often labor-intensive, error-prone, and costly. While the adoption of\nFAIR (Findable, Accessible, Interoperable, and Reusable) data principles has\naimed to address these challenges, barriers to efficient and accurate\nscientific data processing persist.\n  In this paper, we introduce FAIRBridge, an experimental natural\nlanguage-based query processing system designed to empower scientists to\ndiscover, access, and query biological databases, even when they are not\nFAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query\nintents, map them to relevant databases described in scientific literature, and\ngenerate executable queries via intelligent resource access plans. The system\nalso includes robust tools for mitigating low-quality query processing,\nensuring high fidelity and responsiveness in the information delivered.\n  FAIRBridge's autonomous query processing framework enables users to explore\nalternative data sources, make informed choices at every step, and leverage\ncommunity-driven crowd curation when needed. By providing a user-friendly,\nautomated hypothesis-testing platform in natural English, FAIRBridge\nsignificantly enhances the integration and processing of scientific data,\noffering researchers a powerful new tool for advancing their inquiries.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aFAIRBridge\u7684\u5b9e\u9a8c\u6027\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5904\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u79d1\u5b66\u5bb6\u53d1\u73b0\u3001\u8bbf\u95ee\u548c\u67e5\u8be2\u751f\u7269\u6570\u636e\u5e93\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6570\u636e\u5e93\u4e0d\u7b26\u5408FAIR\u539f\u5219\u3002\u8be5\u7cfb\u7edf\u5229\u7528AI\u6280\u672f\u89e3\u6790\u67e5\u8be2\u610f\u56fe\uff0c\u751f\u6210\u53ef\u6267\u884c\u67e5\u8be2\uff0c\u5e76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u4fe1\u606f\u5904\u7406\u5de5\u5177\u3002", "motivation": "\u751f\u547d\u79d1\u5b66\u7814\u7a76\u9700\u8981\u4ece\u4e0d\u65ad\u53d8\u5316\u7684Linked Open Data\uff08LOD\uff09\u7f51\u7edc\u4e2d\u8bc6\u522b\u3001\u8bbf\u95ee\u548c\u5904\u7406\u6570\u636e\uff0c\u4f46\u6570\u636e\u6e90\u7684\u9009\u62e9\u548c\u8bed\u4e49\u6574\u5408\u8fc7\u7a0b\u5f80\u5f80\u7e41\u7410\u4e14\u6613\u51fa\u9519\u3002\u5c3d\u7ba1FAIR\u539f\u5219\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u9ad8\u6548\u51c6\u786e\u7684\u6570\u636e\u5904\u7406\u4ecd\u5b58\u5728\u969c\u788d\u3002", "method": "FAIRBridge\u901a\u8fc7AI\u6280\u672f\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u610f\u56fe\uff0c\u5c06\u5176\u6620\u5c04\u5230\u79d1\u5b66\u6587\u732e\u4e2d\u63cf\u8ff0\u7684\u6570\u636e\u5e93\uff0c\u5e76\u751f\u6210\u53ef\u6267\u884c\u67e5\u8be2\u3002\u7cfb\u7edf\u8fd8\u63d0\u4f9b\u5de5\u5177\u4ee5\u5e94\u5bf9\u4f4e\u8d28\u91cf\u67e5\u8be2\u5904\u7406\uff0c\u786e\u4fdd\u4fe1\u606f\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u54cd\u5e94\u6027\u3002", "result": "FAIRBridge\u7684\u81ea\u4e3b\u67e5\u8be2\u5904\u7406\u6846\u67b6\u4f7f\u7528\u6237\u80fd\u591f\u63a2\u7d22\u66ff\u4ee3\u6570\u636e\u6e90\uff0c\u5e76\u5728\u6bcf\u4e00\u6b65\u505a\u51fa\u660e\u667a\u9009\u62e9\uff0c\u540c\u65f6\u652f\u6301\u793e\u533a\u9a71\u52a8\u7684\u4f17\u5305\u7ba1\u7406\u3002\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u6570\u636e\u7684\u6574\u5408\u4e0e\u5904\u7406\u6548\u7387\u3002", "conclusion": "FAIRBridge\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5e73\u53f0\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u663e\u8457\u6539\u5584\u4e86\u79d1\u5b66\u6570\u636e\u7684\u6574\u5408\u4e0e\u5904\u7406\uff0c\u63a8\u52a8\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "paper_title_zh": "\u4e00\u79cd\u6539\u8fdbFAIR\u72ec\u7acb\u751f\u7269\u6570\u636e\u5e93\u6574\u5408\u7684GenAI\u7cfb\u7edf", "abstract_zh": "\u751f\u547d\u79d1\u5b66\u7814\u7a76\u65e5\u76ca\u9700\u8981\u4eceLinked Open Data\uff08LOD\uff09\u7f51\u7edc\u4e2d\u8bc6\u522b\u3001\u8bbf\u95ee\u548c\u5904\u7406\u4e0d\u65ad\u53d8\u5316\u7684\u4fe1\u606f\u6e90\u6570\u636e\u3002\u8fd9\u79cd\u52a8\u6001\u73af\u5883\u7ed9\u7814\u7a76\u4eba\u5458\u5e26\u6765\u4e86\u5de8\u5927\u8d1f\u62c5\uff0c\u56e0\u4e3a\u67e5\u8be2\u54cd\u5e94\u7684\u8d28\u91cf\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6570\u636e\u6e90\u7684\u9009\u62e9\u548c\u8bed\u4e49\u6574\u5408\u2014\u2014\u8fd9\u4e9b\u8fc7\u7a0b\u901a\u5e38\u7e41\u7410\u3001\u6613\u51fa\u9519\u4e14\u6210\u672c\u9ad8\u6602\u3002\u5c3d\u7ba1FAIR\uff08\u53ef\u67e5\u627e\u3001\u53ef\u8bbf\u95ee\u3001\u53ef\u4e92\u64cd\u4f5c\u548c\u53ef\u91cd\u7528\uff09\u6570\u636e\u539f\u5219\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u9ad8\u6548\u51c6\u786e\u7684\u6570\u636e\u5904\u7406\u4ecd\u5b58\u5728\u969c\u788d\u3002\n\n\u672c\u6587\u4ecb\u7ecd\u4e86FAIRBridge\uff0c\u4e00\u79cd\u5b9e\u9a8c\u6027\u7684\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u67e5\u8be2\u5904\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u79d1\u5b66\u5bb6\u53d1\u73b0\u3001\u8bbf\u95ee\u548c\u67e5\u8be2\u751f\u7269\u6570\u636e\u5e93\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6570\u636e\u5e93\u4e0d\u7b26\u5408FAIR\u539f\u5219\u3002FAIRBridge\u5229\u7528AI\u6280\u672f\u89e3\u6790\u67e5\u8be2\u610f\u56fe\uff0c\u5c06\u5176\u6620\u5c04\u5230\u79d1\u5b66\u6587\u732e\u4e2d\u63cf\u8ff0\u7684\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u8d44\u6e90\u8bbf\u95ee\u8ba1\u5212\u751f\u6210\u53ef\u6267\u884c\u67e5\u8be2\u3002\u7cfb\u7edf\u8fd8\u5305\u542b\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5e94\u5bf9\u4f4e\u8d28\u91cf\u67e5\u8be2\u5904\u7406\uff0c\u786e\u4fdd\u6240\u63d0\u4f9b\u4fe1\u606f\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u54cd\u5e94\u6027\u3002\n\nFAIRBridge\u7684\u81ea\u4e3b\u67e5\u8be2\u5904\u7406\u6846\u67b6\u4f7f\u7528\u6237\u80fd\u591f\u63a2\u7d22\u66ff\u4ee3\u6570\u636e\u6e90\uff0c\u5728\u6bcf\u4e00\u6b65\u505a\u51fa\u660e\u667a\u9009\u62e9\uff0c\u5e76\u5728\u9700\u8981\u65f6\u5229\u7528\u793e\u533a\u9a71\u52a8\u7684\u4f17\u5305\u7ba1\u7406\u3002\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u81ea\u7136\u82f1\u8bed\u81ea\u52a8\u5316\u5047\u8bbe\u6d4b\u8bd5\u5e73\u53f0\uff0cFAIRBridge\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u6570\u636e\u7684\u6574\u5408\u4e0e\u5904\u7406\u6548\u7387\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b0\u5de5\u5177\u4ee5\u63a8\u52a8\u5176\u7814\u7a76\u3002"}}
{"id": "2506.17936", "pdf": "https://arxiv.org/pdf/2506.17936", "abs": "https://arxiv.org/abs/2506.17936", "authors": ["Romy M\u00fcller"], "title": "When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Concept-based explainable artificial intelligence (C-XAI) can help reveal the\ninner representations of AI models. Understanding these representations is\nparticularly important in complex tasks like safety evaluation. Such tasks rely\non high-level semantic information (e.g., about actions) to make decisions\nabout abstract categories (e.g., whether a situation is dangerous). In this\ncontext, it may desirable for C-XAI concepts to show some variability,\nsuggesting that the AI is capable of generalising beyond the concrete details\nof a situation. However, it is unclear whether people recognise and appreciate\nsuch generalisations and can distinguish them from other, less desirable forms\nof imprecision. This was investigated in an experimental railway safety\nscenario. Participants evaluated the performance of a simulated AI that\nevaluated whether traffic scenes involving people were dangerous. To explain\nthese decisions, the AI provided concepts in the form of similar image\nsnippets. These concepts differed in their match with the classified image,\neither regarding a highly relevant feature (i.e., relation to tracks) or a less\nrelevant feature (i.e., actions). Contrary to the hypotheses, concepts that\ngeneralised over less relevant features led to ratings that were lower than for\nprecisely matching concepts and comparable to concepts that systematically\nmisrepresented these features. Conversely, participants were highly sensitive\nto imprecisions in relevant features. These findings cast doubts on whether\npeople spontaneously recognise generalisations. Accordingly, they might not be\nable to infer from C-XAI concepts whether AI models have gained a deeper\nunderstanding of complex situations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684XAI\uff08C-XAI\uff09\u4e2d\uff0c\u4eba\u4eec\u662f\u5426\u80fd\u533a\u5206AI\u7684\u6cdb\u5316\u4e0e\u9519\u8bef\u8868\u793a\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u53c2\u4e0e\u8005\u5bf9\u4e0d\u76f8\u5173\u7279\u5f81\u7684\u6cdb\u5316\u8bc4\u4ef7\u8f83\u4f4e\uff0c\u4e14\u4e0e\u9519\u8bef\u8868\u793a\u76f8\u5f53\uff0c\u4f46\u5bf9\u76f8\u5173\u7279\u5f81\u7684\u4e0d\u7cbe\u786e\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "motivation": "\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5b89\u5168\u8bc4\u4f30\uff09\u4e2d\uff0c\u7406\u89e3AI\u7684\u5185\u90e8\u8868\u793a\u81f3\u5173\u91cd\u8981\u3002C-XAI\u901a\u8fc7\u6982\u5ff5\u63ed\u793a\u8fd9\u4e9b\u8868\u793a\uff0c\u4f46\u4eba\u4eec\u662f\u5426\u80fd\u533a\u5206\u6cdb\u5316\u4e0e\u9519\u8bef\u8868\u793a\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u70b9\u3002", "method": "\u5728\u94c1\u8def\u5b89\u5168\u573a\u666f\u4e2d\uff0c\u53c2\u4e0e\u8005\u8bc4\u4f30\u6a21\u62dfAI\u5bf9\u5371\u9669\u4ea4\u901a\u573a\u666f\u7684\u5224\u65ad\u3002AI\u901a\u8fc7\u7c7b\u4f3c\u56fe\u50cf\u7247\u6bb5\u63d0\u4f9b\u89e3\u91ca\uff0c\u8fd9\u4e9b\u7247\u6bb5\u5728\u76f8\u5173\u7279\u5f81\uff08\u5982\u8f68\u9053\u5173\u7cfb\uff09\u6216\u4e0d\u76f8\u5173\u7279\u5f81\uff08\u5982\u52a8\u4f5c\uff09\u4e0a\u4e0e\u5206\u7c7b\u56fe\u50cf\u5339\u914d\u5ea6\u4e0d\u540c\u3002", "result": "\u4e0e\u5047\u8bbe\u76f8\u53cd\uff0c\u5bf9\u4e0d\u76f8\u5173\u7279\u5f81\u7684\u6cdb\u5316\u6982\u5ff5\u8bc4\u5206\u4f4e\u4e8e\u7cbe\u786e\u5339\u914d\u6982\u5ff5\uff0c\u4e14\u4e0e\u9519\u8bef\u8868\u793a\u76f8\u5f53\u3002\u53c2\u4e0e\u8005\u5bf9\u76f8\u5173\u7279\u5f81\u7684\u4e0d\u7cbe\u786e\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u4eba\u4eec\u53ef\u80fd\u65e0\u6cd5\u81ea\u53d1\u8bc6\u522bAI\u7684\u6cdb\u5316\u884c\u4e3a\uff0c\u56e0\u6b64\u96be\u4ee5\u901a\u8fc7C-XAI\u6982\u5ff5\u5224\u65adAI\u662f\u5426\u6df1\u5165\u7406\u89e3\u4e86\u590d\u6742\u60c5\u5883\u3002", "paper_title_zh": "\u5f53\u57fa\u4e8e\u6982\u5ff5\u7684XAI\u4e0d\u7cbe\u786e\u65f6\uff1a\u4eba\u4eec\u80fd\u5426\u533a\u5206\u6cdb\u5316\u4e0e\u9519\u8bef\u8868\u793a\uff1f", "abstract_zh": "\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08C-XAI\uff09\u6709\u52a9\u4e8e\u63ed\u793aAI\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u3002\u7406\u89e3\u8fd9\u4e9b\u8868\u793a\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5b89\u5168\u8bc4\u4f30\uff09\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002\u6b64\u7c7b\u4efb\u52a1\u4f9d\u8d56\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u52a8\u4f5c\uff09\u6765\u5bf9\u62bd\u8c61\u7c7b\u522b\uff08\u5982\u60c5\u5883\u662f\u5426\u5371\u9669\uff09\u505a\u51fa\u51b3\u7b56\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0cC-XAI\u6982\u5ff5\u53ef\u80fd\u9700\u8981\u4e00\u5b9a\u53d8\u5f02\u6027\uff0c\u8868\u660eAI\u80fd\u591f\u8d85\u8d8a\u5177\u4f53\u60c5\u5883\u7ec6\u8282\u8fdb\u884c\u6cdb\u5316\u3002\u7136\u800c\uff0c\u4eba\u4eec\u662f\u5426\u80fd\u8bc6\u522b\u5e76\u6b23\u8d4f\u8fd9\u79cd\u6cdb\u5316\uff0c\u5e76\u5c06\u5176\u4e0e\u5176\u4ed6\u4e0d\u7406\u60f3\u7684\u4e0d\u7cbe\u786e\u5f62\u5f0f\u533a\u5206\u5f00\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u901a\u8fc7\u94c1\u8def\u5b89\u5168\u5b9e\u9a8c\u573a\u666f\u63a2\u8ba8\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u53c2\u4e0e\u8005\u8bc4\u4f30\u4e86\u4e00\u4e2a\u6a21\u62dfAI\u7684\u6027\u80fd\uff0c\u8be5AI\u5224\u65ad\u6d89\u53ca\u4eba\u5458\u7684\u4ea4\u901a\u573a\u666f\u662f\u5426\u5371\u9669\u3002\u4e3a\u89e3\u91ca\u8fd9\u4e9b\u51b3\u7b56\uff0cAI\u63d0\u4f9b\u4e86\u7c7b\u4f3c\u56fe\u50cf\u7247\u6bb5\u4f5c\u4e3a\u6982\u5ff5\u3002\u8fd9\u4e9b\u6982\u5ff5\u5728\u4e0e\u5206\u7c7b\u56fe\u50cf\u7684\u5339\u914d\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u6d89\u53ca\u9ad8\u5ea6\u76f8\u5173\u7279\u5f81\uff08\u5982\u8f68\u9053\u5173\u7cfb\uff09\u6216\u8f83\u4e0d\u76f8\u5173\u7279\u5f81\uff08\u5982\u52a8\u4f5c\uff09\u3002\u4e0e\u5047\u8bbe\u76f8\u53cd\uff0c\u5bf9\u8f83\u4e0d\u76f8\u5173\u7279\u5f81\u7684\u6cdb\u5316\u6982\u5ff5\u8bc4\u5206\u4f4e\u4e8e\u7cbe\u786e\u5339\u914d\u6982\u5ff5\uff0c\u4e14\u4e0e\u7cfb\u7edf\u6027\u9519\u8bef\u8868\u793a\u76f8\u5f53\u3002\u76f8\u53cd\uff0c\u53c2\u4e0e\u8005\u5bf9\u76f8\u5173\u7279\u5f81\u7684\u4e0d\u7cbe\u786e\u6027\u9ad8\u5ea6\u654f\u611f\u3002\u8fd9\u4e9b\u53d1\u73b0\u8d28\u7591\u4eba\u4eec\u662f\u5426\u80fd\u81ea\u53d1\u8bc6\u522b\u6cdb\u5316\u884c\u4e3a\uff0c\u8fdb\u800c\u53ef\u80fd\u65e0\u6cd5\u901a\u8fc7C-XAI\u6982\u5ff5\u63a8\u65adAI\u6a21\u578b\u662f\u5426\u6df1\u5165\u7406\u89e3\u4e86\u590d\u6742\u60c5\u5883\u3002"}}
{"id": "2506.17937", "pdf": "https://arxiv.org/pdf/2506.17937", "abs": "https://arxiv.org/abs/2506.17937", "authors": ["Tommi Mikkonen", "Antero Taivalsaari"], "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u65f6\u4ee3\u4e0b\u7684\u8f6f\u4ef6\u590d\u7528\uff0c\u4ece\u2018\u8d27\u7269\u5d07\u62dc\u2019\u5f00\u53d1\u8f6c\u5411AI\u539f\u751f\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u95ee\u9898\u548c\u7814\u7a76\u8bae\u7a0b\u3002", "motivation": "\u968f\u7740AI\u548c\u751f\u6210\u5f0f\u8f6f\u4ef6\u590d\u7528\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5d1b\u8d77\uff0c\u4f20\u7edf\u590d\u7528\u65b9\u6cd5\u6b63\u88abAI\u8f85\u52a9\u65b9\u5f0f\u53d6\u4ee3\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u65b0\u590d\u7528\u5f62\u5f0f\u53ca\u5176\u95ee\u9898\u7684\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u8f85\u52a9\u751f\u6210\u5f0f\u8f6f\u4ef6\u590d\u7528\u7684\u73b0\u8c61\uff0c\u63d0\u51fa\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u7814\u7a76\u8bae\u7a0b\u4ee5\u89e3\u51b3\u8fd9\u4e00\u65b0\u590d\u7528\u5f62\u5f0f\u7684\u6838\u5fc3\u95ee\u9898\u3002", "result": "\u6587\u7ae0\u63ed\u793a\u4e86AI\u8f85\u52a9\u8f6f\u4ef6\u590d\u7528\u4e0e\u2018\u8d27\u7269\u5d07\u62dc\u2019\u5f00\u53d1\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u5bf9\u8fd9\u4e00\u8d8b\u52bf\u7684\u7814\u7a76\u65b9\u5411\u548c\u884c\u52a8\u547c\u5401\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u65f6\u4ee3\u7684\u8f6f\u4ef6\u590d\u7528\u9700\u8981\u65b0\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u4ee5\u786e\u4fddAI\u539f\u751f\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5065\u5eb7\u53d1\u5c55\u3002", "paper_title_zh": "\u751f\u6210\u5f0fAI\u65f6\u4ee3\u7684\u8f6f\u4ef6\u590d\u7528\uff1a\u4ece\u8d27\u7269\u5d07\u62dc\u5230AI\u539f\u751f\u8f6f\u4ef6\u5de5\u7a0b", "abstract_zh": "\u8f6f\u4ef6\u5f00\u53d1\u6b63\u7ecf\u5386\u4e00\u573a\u8303\u5f0f\u8f6c\u53d8\uff0c\u4eba\u5de5\u667a\u80fd\u548c\u751f\u6210\u5f0f\u8f6f\u4ef6\u590d\u7528\u6210\u4e3a\u8f6f\u4ef6\u521b\u9020\u7684\u6838\u5fc3\u3002\u4f20\u7edf\u7684\u8f6f\u4ef6\u590d\u7528\u5b9e\u8df5\u548c\u65b9\u6cd5\u6b63\u8fc5\u901f\u88abAI\u8f85\u52a9\u65b9\u6cd5\u53d6\u4ee3\uff0c\u5f00\u53d1\u8005\u5f00\u59cb\u4fe1\u4efb\u7531AI\u751f\u6210\u7684\u4ee3\u7801\u3002\u8fd9\u5bfc\u81f4\u4e86\u4e00\u79cd\u65b0\u7684\u8f6f\u4ef6\u590d\u7528\u5f62\u5f0f\uff0c\u6982\u5ff5\u4e0a\u4e0e\u2018\u8d27\u7269\u5d07\u62dc\u2019\u5f00\u53d1\u5e76\u65e0\u592a\u5927\u5dee\u5f02\u3002\u672c\u6587\u8ba8\u8bba\u4e86AI\u8f85\u52a9\u751f\u6210\u5f0f\u8f6f\u4ef6\u590d\u7528\u5728\u2018AI\u539f\u751f\u2019\u8f6f\u4ef6\u5de5\u7a0b\u80cc\u666f\u4e0b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u521d\u6b65\u7684\u7814\u7a76\u8bae\u7a0b\u548c\u884c\u52a8\u547c\u5401\uff0c\u4ee5\u89e3\u51b3\u4e0e\u8fd9\u4e00\u65b9\u6cd5\u76f8\u5173\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2506.17378", "pdf": "https://arxiv.org/pdf/2506.17378", "abs": "https://arxiv.org/abs/2506.17378", "authors": ["Abhishek Phadke", "Shakib Mahmud Dipto", "Pratip Rana"], "title": "A workflow for generating synthetic LiDAR datasets in simulation environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents a simulation workflow for generating synthetic LiDAR\ndatasets to support autonomous vehicle perception, robotics research, and\nsensor security analysis. Leveraging the CoppeliaSim simulation environment and\nits Python API, we integrate time-of-flight LiDAR, image sensors, and two\ndimensional scanners onto a simulated vehicle platform operating within an\nurban scenario. The workflow automates data capture, storage, and annotation\nacross multiple formats (PCD, PLY, CSV), producing synchronized multimodal\ndatasets with ground truth pose information. We validate the pipeline by\ngenerating large-scale point clouds and corresponding RGB and depth imagery.\nThe study examines potential security vulnerabilities in LiDAR data, such as\nadversarial point injection and spoofing attacks, and demonstrates how\nsynthetic datasets can facilitate the evaluation of defense strategies.\nFinally, limitations related to environmental realism, sensor noise modeling,\nand computational scalability are discussed, and future research directions,\nsuch as incorporating weather effects, real-world terrain models, and advanced\nscanner configurations, are proposed. The workflow provides a versatile,\nreproducible framework for generating high-fidelity synthetic LiDAR datasets to\nadvance perception research and strengthen sensor security in autonomous\nsystems. Documentation and examples accompany this framework; samples of\nanimated cloud returns and image sensor data can be found at this Link.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4eff\u771f\u73af\u5883\u4e2d\u751f\u6210\u5408\u6210LiDAR\u6570\u636e\u96c6\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3001\u673a\u5668\u4eba\u7814\u7a76\u548c\u4f20\u611f\u5668\u5b89\u5168\u5206\u6790\u3002\u901a\u8fc7CoppeliaSim\u4eff\u771f\u73af\u5883\u53ca\u5176Python API\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u81ea\u52a8\u5316\u6355\u83b7\u3001\u5b58\u50a8\u548c\u6807\u6ce8\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5927\u89c4\u6a21\u70b9\u4e91\u548cRGB/\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u9886\u57df\u5bf9\u9ad8\u8d28\u91cfLiDAR\u6570\u636e\u7684\u9700\u6c42\uff0c\u540c\u65f6\u63a2\u7d22LiDAR\u6570\u636e\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u9632\u5fa1\u7b56\u7565\u8bc4\u4f30\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u5229\u7528CoppeliaSim\u4eff\u771f\u73af\u5883\u53ca\u5176Python API\uff0c\u5c06LiDAR\u3001\u56fe\u50cf\u4f20\u611f\u5668\u548c\u4e8c\u7ef4\u626b\u63cf\u4eea\u96c6\u6210\u5230\u6a21\u62df\u8f66\u8f86\u5e73\u53f0\u4e0a\uff0c\u81ea\u52a8\u5316\u751f\u6210\u591a\u683c\u5f0f\uff08PCD\u3001PLY\u3001CSV\uff09\u7684\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u5305\u542b\u771f\u5b9e\u59ff\u6001\u4fe1\u606f\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u5927\u89c4\u6a21\u70b9\u4e91\u53ca\u5bf9\u5e94\u7684RGB\u548c\u6df1\u5ea6\u56fe\u50cf\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86LiDAR\u6570\u636e\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff08\u5982\u5bf9\u6297\u70b9\u6ce8\u5165\u548c\u6b3a\u9a97\u653b\u51fb\uff09\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u7a0b\u4e3a\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210LiDAR\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u652f\u6301\u611f\u77e5\u7814\u7a76\u548c\u4f20\u611f\u5668\u5b89\u5168\u3002\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5929\u6c14\u6548\u679c\u3001\u771f\u5b9e\u5730\u5f62\u6a21\u578b\u548c\u9ad8\u7ea7\u626b\u63cf\u914d\u7f6e\u3002", "paper_title_zh": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\u751f\u6210\u5408\u6210LiDAR\u6570\u636e\u96c6\u7684\u5de5\u4f5c\u6d41\u7a0b", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u771f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u5408\u6210LiDAR\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3001\u673a\u5668\u4eba\u7814\u7a76\u548c\u4f20\u611f\u5668\u5b89\u5168\u5206\u6790\u3002\u901a\u8fc7CoppeliaSim\u4eff\u771f\u73af\u5883\u53ca\u5176Python API\uff0c\u6211\u4eec\u5c06\u98de\u884c\u65f6\u95f4LiDAR\u3001\u56fe\u50cf\u4f20\u611f\u5668\u548c\u4e8c\u7ef4\u626b\u63cf\u4eea\u96c6\u6210\u5230\u6a21\u62df\u8f66\u8f86\u5e73\u53f0\u4e0a\uff0c\u5e76\u5728\u57ce\u5e02\u573a\u666f\u4e2d\u8fd0\u884c\u3002\u8be5\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5316\u4e86\u6570\u636e\u6355\u83b7\u3001\u5b58\u50a8\u548c\u6807\u6ce8\uff0c\u652f\u6301\u591a\u79cd\u683c\u5f0f\uff08PCD\u3001PLY\u3001CSV\uff09\uff0c\u5e76\u751f\u6210\u5e26\u6709\u771f\u5b9e\u59ff\u6001\u4fe1\u606f\u7684\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u6211\u4eec\u901a\u8fc7\u751f\u6210\u5927\u89c4\u6a21\u70b9\u4e91\u53ca\u5bf9\u5e94\u7684RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u9a8c\u8bc1\u4e86\u8be5\u6d41\u7a0b\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86LiDAR\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e\uff08\u5982\u5bf9\u6297\u70b9\u6ce8\u5165\u548c\u6b3a\u9a97\u653b\u51fb\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u96c6\u5982\u4f55\u4fc3\u8fdb\u9632\u5fa1\u7b56\u7565\u7684\u8bc4\u4f30\u3002\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u73af\u5883\u771f\u5b9e\u6027\u3001\u4f20\u611f\u5668\u566a\u58f0\u5efa\u6a21\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7b49\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u52a0\u5165\u5929\u6c14\u6548\u679c\u3001\u771f\u5b9e\u5730\u5f62\u6a21\u578b\u548c\u9ad8\u7ea7\u626b\u63cf\u914d\u7f6e\u3002\u8be5\u5de5\u4f5c\u6d41\u7a0b\u4e3a\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210LiDAR\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u611f\u77e5\u7814\u7a76\u5e76\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u4f20\u611f\u5668\u5b89\u5168\u6027\u3002\u6587\u6863\u548c\u793a\u4f8b\u968f\u6846\u67b6\u63d0\u4f9b\uff0c\u52a8\u753b\u4e91\u8fd4\u56de\u548c\u56fe\u50cf\u4f20\u611f\u5668\u6570\u636e\u7684\u6837\u672c\u53ef\u901a\u8fc7\u94fe\u63a5\u67e5\u770b\u3002"}}
{"id": "2506.17940", "pdf": "https://arxiv.org/pdf/2506.17940", "abs": "https://arxiv.org/abs/2506.17940", "authors": ["Davide Bassetti", "Luk\u00e1\u0161 Posp\u00ed\u0161il", "Michael Groom", "Terence J. O'Kane", "Illia Horenko"], "title": "An entropy-optimal path to humble AI", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "30 pages, 4 figures", "summary": "Progress of AI has led to a creation of very successful, but by no means\nhumble models and tools, especially regarding (i) the huge and further\nexploding costs and resources they demand, and (ii) the over-confidence of\nthese tools with the answers they provide. Here we introduce a novel\nmathematical framework for a non-equilibrium entropy-optimizing reformulation\nof Boltzmann machines based on the exact law of total probability. It results\nin the highly-performant, but much cheaper, gradient-descent-free learning\nframework with mathematically-justified existence and uniqueness criteria, and\nanswer confidence/reliability measures. Comparisons to state-of-the-art AI\ntools in terms of performance, cost and the model descriptor lengths on a set\nof synthetic problems with varying complexity reveal that the proposed method\nresults in more performant and slim models, with the descriptor lengths being\nvery close to the intrinsic complexity scaling bounds for the underlying\nproblems. Applying this framework to historical climate data results in models\nwith systematically higher prediction skills for the onsets of La Ni\\~na and El\nNi\\~no climate phenomena, requiring just few years of climate data for training\n- a small fraction of what is necessary for contemporary climate prediction\ntools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u5e73\u8861\u71b5\u4f18\u5316\u7684\u65b0\u578b\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u6784\u73bb\u5c14\u5179\u66fc\u673a\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dAI\u6a21\u578b\u7684\u9ad8\u6210\u672c\u548c\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u68af\u5ea6\u4e0b\u964d\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u95ee\u9898\u548c\u6c14\u5019\u9884\u6d4b\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5b58\u5728\u9ad8\u6210\u672c\u548c\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u6d88\u8017\u548c\u7b54\u6848\u53ef\u9760\u6027\u65b9\u9762\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u5b66\u4f18\u5316\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7ecf\u6d4e\u7684AI\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u603b\u6982\u7387\u5b9a\u5f8b\u7684\u975e\u5e73\u8861\u71b5\u4f18\u5316\u6846\u67b6\uff0c\u91cd\u6784\u73bb\u5c14\u5179\u66fc\u673a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u68af\u5ea6\u4e0b\u964d\uff0c\u5177\u6709\u6570\u5b66\u4e0a\u53ef\u8bc1\u660e\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u6807\u51c6\uff0c\u5e76\u63d0\u4f9b\u7b54\u6848\u7f6e\u4fe1\u5ea6\u8861\u91cf\u3002", "result": "\u5728\u5408\u6210\u95ee\u9898\u548c\u5386\u53f2\u6c14\u5019\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u3001\u66f4\u7b80\u6d01\uff0c\u63cf\u8ff0\u957f\u5ea6\u63a5\u8fd1\u95ee\u9898\u7684\u5185\u5728\u590d\u6742\u5ea6\u8fb9\u754c\u3002\u6c14\u5019\u9884\u6d4b\u4e2d\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "conclusion": "\u8be5\u71b5\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u590d\u6742\u95ee\u9898\u548c\u6c14\u5019\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u901a\u5f80\u8c26\u900aAI\u7684\u71b5\u6700\u4f18\u8def\u5f84", "abstract_zh": "AI\u7684\u8fdb\u6b65\u50ac\u751f\u4e86\u8bb8\u591a\u6210\u529f\u4f46\u8fdc\u975e\u8c26\u900a\u7684\u6a21\u578b\u548c\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\uff08i\uff09\u5b83\u4eec\u6240\u9700\u7684\u9ad8\u6602\u4e14\u4e0d\u65ad\u98d9\u5347\u7684\u6210\u672c\u548c\u8d44\u6e90\uff0c\u4ee5\u53ca\uff08ii\uff09\u8fd9\u4e9b\u5de5\u5177\u5bf9\u7b54\u6848\u7684\u8fc7\u5ea6\u81ea\u4fe1\u65b9\u9762\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u603b\u6982\u7387\u5b9a\u5f8b\u7684\u975e\u5e73\u8861\u71b5\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u6784\u73bb\u5c14\u5179\u66fc\u673a\u3002\u8be5\u65b9\u6cd5\u5f62\u6210\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u4f46\u6210\u672c\u66f4\u4f4e\u3001\u65e0\u9700\u68af\u5ea6\u4e0b\u964d\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u6570\u5b66\u4e0a\u53ef\u8bc1\u660e\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u6807\u51c6\uff0c\u5e76\u63d0\u4f9b\u7b54\u6848\u7f6e\u4fe1\u5ea6\u8861\u91cf\u3002\u4e0e\u6700\u5148\u8fdb\u7684AI\u5de5\u5177\u5728\u6027\u80fd\u3001\u6210\u672c\u548c\u6a21\u578b\u63cf\u8ff0\u957f\u5ea6\u4e0a\u7684\u6bd4\u8f83\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u4e14\u66f4\u7b80\u6d01\uff0c\u63cf\u8ff0\u957f\u5ea6\u975e\u5e38\u63a5\u8fd1\u95ee\u9898\u7684\u5185\u5728\u590d\u6742\u5ea6\u8fb9\u754c\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u5386\u53f2\u6c14\u5019\u6570\u636e\uff0c\u751f\u6210\u7684\u6a21\u578b\u5bf9\u62c9\u5c3c\u5a1c\u548c\u5384\u5c14\u5c3c\u8bfa\u6c14\u5019\u73b0\u8c61\u7684\u9884\u6d4b\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4ec5\u9700\u5c11\u91cf\u6c14\u5019\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u2014\u2014\u8fdc\u5c11\u4e8e\u5f53\u4ee3\u6c14\u5019\u9884\u6d4b\u5de5\u5177\u6240\u9700\u7684\u6570\u636e\u91cf\u3002"}}
{"id": "2506.17450", "pdf": "https://arxiv.org/pdf/2506.17450", "abs": "https://arxiv.org/abs/2506.17450", "authors": ["Jiacheng Chen", "Ramin Mehran", "Xuhui Jia", "Saining Xie", "Sanghyun Woo"], "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://blenderfusion.github.io", "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.", "AI": {"tldr": "BlenderFusion\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u89c6\u89c9\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc73D\u5b9e\u4f53\u7f16\u8f91\u548c\u751f\u6210\u5f0f\u5408\u6210\u6280\u672f\u5b9e\u73b0\u590d\u6742\u573a\u666f\u7684\u7f16\u8f91\u4e0e\u5408\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0cBlenderFusion\u65e8\u5728\u901a\u8fc73D\u5b9e\u4f53\u7f16\u8f91\u548c\u751f\u6210\u5f0f\u5408\u6210\u6280\u672f\u63d0\u5347\u573a\u666f\u7f16\u8f91\u7684\u7075\u6d3b\u6027\u548c\u6548\u679c\u3002", "method": "BlenderFusion\u91c7\u7528\u5206\u5c42-\u7f16\u8f91-\u5408\u6210\u6d41\u7a0b\uff1a(1)\u5c06\u89c6\u89c9\u8f93\u5165\u5206\u5272\u5e76\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u76843D\u5b9e\u4f53\uff1b(2)\u5728Blender\u4e2d\u8fdb\u884c3D\u63a7\u5236\u7f16\u8f91\uff1b(3)\u901a\u8fc7\u751f\u6210\u5f0f\u5408\u6210\u5668\u5c06\u7f16\u8f91\u540e\u7684\u573a\u666f\u878d\u5408\u4e3a\u8fde\u8d2f\u753b\u9762\u3002\u5408\u6210\u5668\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u6e90\u573a\u666f\u548c\u76ee\u6807\u573a\u666f\u5e76\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u6e90\u63a9\u7801\u548c\u6a21\u62df\u5bf9\u8c61\u6296\u52a8\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u6548\u679c\u3002", "result": "BlenderFusion\u5728\u590d\u6742\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u80cc\u666f\u66ff\u6362\u548c\u5bf9\u8c61\u4e0e\u76f8\u673a\u7684\u89e3\u8026\u63a7\u5236\u3002", "conclusion": "BlenderFusion\u901a\u8fc73D\u5b9e\u4f53\u7f16\u8f91\u548c\u751f\u6210\u5f0f\u5408\u6210\u6280\u672f\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "BlenderFusion\uff1a\u57fa\u4e8e3D\u7684\u89c6\u89c9\u7f16\u8f91\u4e0e\u751f\u6210\u5f0f\u5408\u6210", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86BlenderFusion\uff0c\u4e00\u79cd\u751f\u6210\u5f0f\u89c6\u89c9\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u7ec4\u5408\u5bf9\u8c61\u3001\u76f8\u673a\u548c\u80cc\u666f\u6765\u5408\u6210\u65b0\u573a\u666f\u3002\u5176\u6d41\u7a0b\u5206\u4e3a\u4e09\u6b65\uff1a(1)\u5c06\u89c6\u89c9\u8f93\u5165\u5206\u5272\u5e76\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u76843D\u5b9e\u4f53\uff08\u5206\u5c42\uff09\uff1b(2)\u5728Blender\u4e2d\u8fdb\u884c\u57fa\u4e8e3D\u63a7\u5236\u7684\u7f16\u8f91\uff1b(3)\u4f7f\u7528\u751f\u6210\u5f0f\u5408\u6210\u5668\u5c06\u7f16\u8f91\u540e\u7684\u573a\u666f\u878d\u5408\u4e3a\u8fde\u8d2f\u753b\u9762\u3002\u6211\u4eec\u7684\u751f\u6210\u5f0f\u5408\u6210\u5668\u6269\u5c55\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u6e90\u573a\u666f\u548c\u76ee\u6807\u573a\u666f\u7684\u5e76\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u5173\u952e\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\uff1a(1)\u6e90\u63a9\u7801\uff0c\u652f\u6301\u80cc\u666f\u66ff\u6362\u7b49\u7075\u6d3b\u4fee\u6539\uff1b(2)\u6a21\u62df\u5bf9\u8c61\u6296\u52a8\uff0c\u4fbf\u4e8e\u5bf9\u8c61\u4e0e\u76f8\u673a\u7684\u89e3\u8026\u63a7\u5236\u3002BlenderFusion\u5728\u590d\u6742\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.17941", "pdf": "https://arxiv.org/pdf/2506.17941", "abs": "https://arxiv.org/abs/2506.17941", "authors": ["Huitao Yang"], "title": "Greedy Selection under Independent Increments: A Toy Model Analysis", "categories": ["math.PR", "cs.AI", "stat.ML"], "comment": null, "summary": "We study an iterative selection problem over N i.i.d. discrete-time\nstochastic processes with independent increments. At each stage, a fixed number\nof processes are retained based on their observed values. Under this simple\nmodel, we prove that the optimal strategy for selecting the final maximum-value\nprocess is to apply greedy selection at each stage. While the result relies on\nstrong independence assumptions, it offers a clean justification for greedy\nheuristics in multi-stage elimination settings and may serve as a toy example\nfor understanding related algorithms in high-dimensional applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u57fa\u4e8e\u72ec\u7acb\u589e\u91cf\u7684\u8fed\u4ee3\u9009\u62e9\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u591a\u9636\u6bb5\u6dd8\u6c70\u8bbe\u7f6e\u4e2d\uff0c\u8d2a\u5fc3\u9009\u62e9\u7b56\u7565\u662f\u9009\u62e9\u6700\u7ec8\u6700\u5927\u503c\u8fc7\u7a0b\u7684\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5728\u591a\u9636\u6bb5\u6dd8\u6c70\u8fc7\u7a0b\u4e2d\uff0c\u8d2a\u5fc3\u9009\u62e9\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u72ec\u7acb\u589e\u91cf\u5047\u8bbe\u4e0b\uff0c\u4e3a\u9ad8\u7ef4\u5e94\u7528\u4e2d\u7684\u76f8\u5173\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5206\u6790N\u4e2a\u72ec\u7acb\u540c\u5206\u5e03\u7684\u79bb\u6563\u65f6\u95f4\u968f\u673a\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u9636\u6bb5\u57fa\u4e8e\u89c2\u6d4b\u503c\u4fdd\u7559\u56fa\u5b9a\u6570\u91cf\u7684\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u8d2a\u5fc3\u7b56\u7565\u7684\u6700\u4f18\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u72ec\u7acb\u589e\u91cf\u5047\u8bbe\u4e0b\uff0c\u8d2a\u5fc3\u9009\u62e9\u7b56\u7565\u662f\u9009\u62e9\u6700\u7ec8\u6700\u5927\u503c\u8fc7\u7a0b\u7684\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8d2a\u5fc3\u7b56\u7565\u5728\u591a\u9636\u6bb5\u6dd8\u6c70\u8bbe\u7f6e\u4e2d\u5177\u6709\u7406\u8bba\u4e0a\u7684\u6700\u4f18\u6027\uff0c\u4e3a\u76f8\u5173\u7b97\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u7406\u8bba\u652f\u6301\u3002", "paper_title_zh": "\u72ec\u7acb\u589e\u91cf\u4e0b\u7684\u8d2a\u5fc3\u9009\u62e9\uff1a\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\u5206\u6790", "abstract_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u57fa\u4e8eN\u4e2a\u72ec\u7acb\u540c\u5206\u5e03\u7684\u79bb\u6563\u65f6\u95f4\u968f\u673a\u8fc7\u7a0b\u7684\u8fed\u4ee3\u9009\u62e9\u95ee\u9898\uff0c\u8fd9\u4e9b\u8fc7\u7a0b\u5177\u6709\u72ec\u7acb\u589e\u91cf\u3002\u5728\u6bcf\u4e2a\u9636\u6bb5\uff0c\u6839\u636e\u89c2\u6d4b\u503c\u4fdd\u7559\u56fa\u5b9a\u6570\u91cf\u7684\u8fc7\u7a0b\u3002\u5728\u8fd9\u4e00\u7b80\u5355\u6a21\u578b\u4e0b\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u9009\u62e9\u6700\u7ec8\u6700\u5927\u503c\u8fc7\u7a0b\u7684\u6700\u4f18\u7b56\u7565\u662f\u5728\u6bcf\u4e2a\u9636\u6bb5\u5e94\u7528\u8d2a\u5fc3\u9009\u62e9\u3002\u5c3d\u7ba1\u7ed3\u679c\u4f9d\u8d56\u4e8e\u5f3a\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u4f46\u5b83\u4e3a\u591a\u9636\u6bb5\u6dd8\u6c70\u8bbe\u7f6e\u4e2d\u7684\u8d2a\u5fc3\u542f\u53d1\u5f0f\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u7406\u8bba\u652f\u6301\uff0c\u5e76\u53ef\u4f5c\u4e3a\u7406\u89e3\u9ad8\u7ef4\u5e94\u7528\u4e2d\u76f8\u5173\u7b97\u6cd5\u7684\u73a9\u5177\u793a\u4f8b\u3002"}}
{"id": "2506.17501", "pdf": "https://arxiv.org/pdf/2506.17501", "abs": "https://arxiv.org/abs/2506.17501", "authors": ["Shreeram Athreya", "Carlos Olivares", "Ameera Ismail", "Kambiz Nael", "William Speier", "Corey Arnold"], "title": "DSA-NRP: No-Reflow Prediction from Angiographic Perfusion Dynamics in Stroke EVT", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 4 figures", "summary": "Following successful large-vessel recanalization via endovascular\nthrombectomy (EVT) for acute ischemic stroke (AIS), some patients experience a\ncomplication known as no-reflow, defined by persistent microvascular\nhypoperfusion that undermines tissue recovery and worsens clinical outcomes.\nAlthough prompt identification is crucial, standard clinical practice relies on\nperfusion magnetic resonance imaging (MRI) within 24 hours post-procedure,\ndelaying intervention. In this work, we introduce the first-ever machine\nlearning (ML) framework to predict no-reflow immediately after EVT by\nleveraging previously unexplored intra-procedural digital subtraction\nangiography (DSA) sequences and clinical variables. Our retrospective analysis\nincluded AIS patients treated at UCLA Medical Center (2011-2024) who achieved\nfavorable mTICI scores (2b-3) and underwent pre- and post-procedure MRI.\nNo-reflow was defined as persistent hypoperfusion (Tmax > 6 s) on\npost-procedural imaging. From DSA sequences (AP and lateral views), we\nextracted statistical and temporal perfusion features from the target\ndownstream territory to train ML classifiers for predicting no-reflow. Our\nnovel method significantly outperformed a clinical-features baseline(AUC:\n0.7703 $\\pm$ 0.12 vs. 0.5728 $\\pm$ 0.12; accuracy: 0.8125 $\\pm$ 0.10 vs. 0.6331\n$\\pm$ 0.09), demonstrating that real-time DSA perfusion dynamics encode\ncritical insights into microvascular integrity. This approach establishes a\nfoundation for immediate, accurate no-reflow prediction, enabling clinicians to\nproactively manage high-risk patients without reliance on delayed imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSA-NRP\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6025\u6027\u7f3a\u8840\u6027\u5352\u4e2d\u8840\u7ba1\u5185\u53d6\u6813\u672f\uff08EVT\uff09\u540e\u7acb\u5373\u9884\u6d4b\u65e0\u590d\u6d41\u73b0\u8c61\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6570\u5b57\u51cf\u5f71\u8840\u7ba1\u9020\u5f71\uff08DSA\uff09\u5e8f\u5217\u548c\u4e34\u5e8a\u53d8\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4e34\u5e8a\u7279\u5f81\u57fa\u7ebf\uff0c\u4e3a\u5b9e\u65f6\u9884\u6d4b\u65e0\u590d\u6d41\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u6025\u6027\u7f3a\u8840\u6027\u5352\u4e2d\u60a3\u8005\u5728\u6210\u529f\u8fdb\u884c\u8840\u7ba1\u5185\u53d6\u6813\u672f\u540e\uff0c\u90e8\u5206\u60a3\u8005\u4f1a\u51fa\u73b0\u65e0\u590d\u6d41\u73b0\u8c61\uff0c\u5bfc\u81f4\u5fae\u8840\u7ba1\u4f4e\u704c\u6ce8\uff0c\u5f71\u54cd\u7ec4\u7ec7\u6062\u590d\u548c\u4e34\u5e8a\u7ed3\u679c\u3002\u76ee\u524d\u4e34\u5e8a\u4f9d\u8d56\u672f\u540e24\u5c0f\u65f6\u5185\u7684\u704c\u6ce8MRI\u8fdb\u884c\u8bc6\u522b\uff0c\u5ef6\u8fdf\u4e86\u5e72\u9884\u65f6\u673a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u65f6\u9884\u6d4b\u65e0\u590d\u6d41\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u56de\u987e\u6027\u5206\u6790\u4e862011-2024\u5e74UCLA\u533b\u5b66\u4e2d\u5fc3\u63a5\u53d7EVT\u6cbb\u7597\u5e76\u53d6\u5f97\u826f\u597dmTICI\u8bc4\u5206\uff082b-3\uff09\u7684\u60a3\u8005\u3002\u901a\u8fc7\u4eceDSA\u5e8f\u5217\uff08\u524d\u540e\u4f4d\u548c\u4fa7\u4f4d\u89c6\u56fe\uff09\u4e2d\u63d0\u53d6\u76ee\u6807\u4e0b\u6e38\u533a\u57df\u7684\u7edf\u8ba1\u548c\u65f6\u95f4\u704c\u6ce8\u7279\u5f81\uff0c\u7ed3\u5408\u4e34\u5e8a\u53d8\u91cf\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u9884\u6d4b\u65e0\u590d\u6d41\u73b0\u8c61\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4e34\u5e8a\u7279\u5f81\u7684\u57fa\u7ebf\u6a21\u578b\uff08AUC\uff1a0.7703 \u00b1 0.12 vs. 0.5728 \u00b1 0.12\uff1b\u51c6\u786e\u7387\uff1a0.8125 \u00b1 0.10 vs. 0.6331 \u00b1 0.09\uff09\uff0c\u8868\u660eDSA\u704c\u6ce8\u52a8\u6001\u80fd\u591f\u5b9e\u65f6\u53cd\u6620\u5fae\u8840\u7ba1\u5b8c\u6574\u6027\u3002", "conclusion": "DSA-NRP\u4e3a\u65e0\u590d\u6d41\u73b0\u8c61\u7684\u5b9e\u65f6\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u53ca\u65f6\u5e72\u9884\u9ad8\u98ce\u9669\u60a3\u8005\uff0c\u65e0\u9700\u4f9d\u8d56\u5ef6\u8fdf\u7684\u5f71\u50cf\u5b66\u68c0\u67e5\u3002", "paper_title_zh": "DSA-NRP\uff1a\u57fa\u4e8e\u8840\u7ba1\u9020\u5f71\u704c\u6ce8\u52a8\u6001\u7684\u5352\u4e2dEVT\u65e0\u590d\u6d41\u9884\u6d4b", "abstract_zh": "\u5728\u6025\u6027\u7f3a\u8840\u6027\u5352\u4e2d\uff08AIS\uff09\u60a3\u8005\u901a\u8fc7\u8840\u7ba1\u5185\u53d6\u6813\u672f\uff08EVT\uff09\u6210\u529f\u5b9e\u73b0\u5927\u8840\u7ba1\u518d\u901a\u540e\uff0c\u90e8\u5206\u60a3\u8005\u4f1a\u51fa\u73b0\u65e0\u590d\u6d41\u73b0\u8c61\uff0c\u8868\u73b0\u4e3a\u6301\u7eed\u7684\u5fae\u8840\u7ba1\u4f4e\u704c\u6ce8\uff0c\u5f71\u54cd\u7ec4\u7ec7\u6062\u590d\u5e76\u6076\u5316\u4e34\u5e8a\u7ed3\u679c\u3002\u5c3d\u7ba1\u53ca\u65f6\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u51c6\u4e34\u5e8a\u5b9e\u8df5\u4f9d\u8d56\u4e8e\u672f\u540e24\u5c0f\u65f6\u5185\u7684\u704c\u6ce8\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\uff0c\u5bfc\u81f4\u5e72\u9884\u5ef6\u8fdf\u3002\u672c\u7814\u7a76\u9996\u6b21\u5f15\u5165\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u672f\u4e2d\u6570\u5b57\u51cf\u5f71\u8840\u7ba1\u9020\u5f71\uff08DSA\uff09\u5e8f\u5217\u548c\u4e34\u5e8a\u53d8\u91cf\uff0c\u5728EVT\u540e\u7acb\u5373\u9884\u6d4b\u65e0\u590d\u6d41\u73b0\u8c61\u3002\u6211\u4eec\u7684\u56de\u987e\u6027\u5206\u6790\u5305\u62ec2011-2024\u5e74\u5728UCLA\u533b\u5b66\u4e2d\u5fc3\u63a5\u53d7\u6cbb\u7597\u5e76\u53d6\u5f97\u826f\u597dmTICI\u8bc4\u5206\uff082b-3\uff09\u7684AIS\u60a3\u8005\uff0c\u8fd9\u4e9b\u60a3\u8005\u63a5\u53d7\u4e86\u672f\u524d\u548c\u672f\u540eMRI\u68c0\u67e5\u3002\u65e0\u590d\u6d41\u5b9a\u4e49\u4e3a\u672f\u540e\u5f71\u50cf\u4e2d\u6301\u7eed\u7684Tmax > 6\u79d2\u7684\u4f4e\u704c\u6ce8\u3002\u6211\u4eec\u4eceDSA\u5e8f\u5217\uff08\u524d\u540e\u4f4d\u548c\u4fa7\u4f4d\u89c6\u56fe\uff09\u4e2d\u63d0\u53d6\u76ee\u6807\u4e0b\u6e38\u533a\u57df\u7684\u7edf\u8ba1\u548c\u65f6\u95f4\u704c\u6ce8\u7279\u5f81\uff0c\u8bad\u7ec3ML\u5206\u7c7b\u5668\u9884\u6d4b\u65e0\u590d\u6d41\u3002\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4e34\u5e8a\u7279\u5f81\u7684\u57fa\u7ebf\u6a21\u578b\uff08AUC\uff1a0.7703 \u00b1 0.12 vs. 0.5728 \u00b1 0.12\uff1b\u51c6\u786e\u7387\uff1a0.8125 \u00b1 0.10 vs. 0.6331 \u00b1 0.09\uff09\uff0c\u8868\u660e\u5b9e\u65f6DSA\u704c\u6ce8\u52a8\u6001\u80fd\u591f\u53cd\u6620\u5fae\u8840\u7ba1\u5b8c\u6574\u6027\u7684\u5173\u952e\u4fe1\u606f\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u3001\u51c6\u786e\u7684\u65e0\u590d\u6d41\u9884\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u4e3b\u52a8\u7ba1\u7406\u9ad8\u98ce\u9669\u60a3\u8005\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5ef6\u8fdf\u7684\u5f71\u50cf\u5b66\u68c0\u67e5\u3002"}}
{"id": "2506.17960", "pdf": "https://arxiv.org/pdf/2506.17960", "abs": "https://arxiv.org/abs/2506.17960", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Jiaxuan Da", "Nuowen Qian", "Tram Minh Man", "Harold Soh"], "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 5 figures. Jiaming Wang, Diwen Liu, and Jizhuo Chen\n  contributed equally", "summary": "Reliable navigation in unstructured, real-world environments remains a\nsignificant challenge for embodied agents, especially when operating across\ndiverse terrains, weather conditions, and sensor configurations. In this paper,\nwe introduce GeNIE (Generalizable Navigation System for In-the-Wild\nEnvironments), a robust navigation framework designed for global deployment.\nGeNIE integrates a generalizable traversability prediction model built on SAM2\nwith a novel path fusion strategy that enhances planning stability in noisy and\nambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at\nICRA 2025, where it was evaluated across six countries spanning three\ncontinents. GeNIE took first place and achieved 79% of the maximum possible\nscore, outperforming the second-best team by 17%, and completed the entire\ncompetition without a single human intervention. These results set a new\nbenchmark for robust, generalizable outdoor robot navigation. We will release\nthe codebase, pretrained model weights, and newly curated datasets to support\nfuture research in real-world navigation.", "AI": {"tldr": "GeNIE\u662f\u4e00\u79cd\u901a\u7528\u5bfc\u822a\u7cfb\u7edf\uff0c\u4e13\u4e3a\u590d\u6742\u81ea\u7136\u73af\u5883\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u53ef\u6cdb\u5316\u7684\u901a\u884c\u6027\u9884\u6d4b\u6a21\u578b\u548c\u8def\u5f84\u878d\u5408\u7b56\u7565\uff0c\u5728ICRA 2025\u5730\u7403\u6f2b\u6e38\u8005\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5b8c\u6210\u5168\u90e8\u4efb\u52a1\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u3001\u591a\u6837\u5316\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u5bfc\u822a\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u5730\u5f62\u3001\u5929\u6c14\u548c\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u3002GeNIE\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u590d\u6742\u573a\u666f\u3002", "method": "GeNIE\u6574\u5408\u4e86\u57fa\u4e8eSAM2\u7684\u53ef\u6cdb\u5316\u901a\u884c\u6027\u9884\u6d4b\u6a21\u578b\u548c\u4e00\u79cd\u65b0\u578b\u8def\u5f84\u878d\u5408\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5728\u566a\u58f0\u548c\u6a21\u7cca\u73af\u5883\u4e2d\u7684\u89c4\u5212\u7a33\u5b9a\u6027\u3002", "result": "GeNIE\u5728ICRA 2025\u5730\u7403\u6f2b\u6e38\u8005\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5b8c\u6210\u5168\u90e8\u4efb\u52a1\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5f97\u5206\u8d85\u8fc7\u7b2c\u4e8c\u540d17%\u3002", "conclusion": "GeNIE\u4e3a\u6237\u5916\u673a\u5668\u4eba\u5bfc\u822a\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5176\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u4ee3\u7801\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u65b0\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "paper_title_zh": "GeNIE\uff1a\u4e00\u79cd\u9002\u7528\u4e8e\u91ce\u5916\u73af\u5883\u7684\u901a\u7528\u5bfc\u822a\u7cfb\u7edf", "abstract_zh": "\u5728\u975e\u7ed3\u6784\u5316\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u5bfc\u822a\u5bf9\u673a\u5668\u4eba\u6765\u8bf4\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u591a\u6837\u5316\u7684\u5730\u5f62\u3001\u5929\u6c14\u548c\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86GeNIE\uff08\u9002\u7528\u4e8e\u91ce\u5916\u73af\u5883\u7684\u901a\u7528\u5bfc\u822a\u7cfb\u7edf\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e3a\u5168\u7403\u90e8\u7f72\u8bbe\u8ba1\u7684\u9c81\u68d2\u5bfc\u822a\u6846\u67b6\u3002GeNIE\u6574\u5408\u4e86\u57fa\u4e8eSAM2\u7684\u53ef\u6cdb\u5316\u901a\u884c\u6027\u9884\u6d4b\u6a21\u578b\u548c\u4e00\u79cd\u65b0\u578b\u8def\u5f84\u878d\u5408\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5728\u566a\u58f0\u548c\u6a21\u7cca\u73af\u5883\u4e2d\u7684\u89c4\u5212\u7a33\u5b9a\u6027\u3002GeNIE\u5728ICRA 2025\u5730\u7403\u6f2b\u6e38\u8005\u6311\u6218\u8d5b\uff08ERC\uff09\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8de8\u8d8a\u4e09\u5927\u6d32\u7684\u516d\u4e2a\u56fd\u5bb6\u3002GeNIE\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u8fbe\u5230\u4e86\u6700\u9ad8\u5206\u768479%\uff0c\u9886\u5148\u7b2c\u4e8c\u540d17%\uff0c\u5e76\u5728\u6574\u4e2a\u6bd4\u8d5b\u4e2d\u672a\u9700\u4efb\u4f55\u4eba\u5de5\u5e72\u9884\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u6237\u5916\u673a\u5668\u4eba\u5bfc\u822a\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u4ee3\u7801\u5e93\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u548c\u65b0\u6574\u7406\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u5728\u771f\u5b9e\u4e16\u754c\u5bfc\u822a\u4e2d\u7684\u7814\u7a76\u3002"}}
{"id": "2506.17516", "pdf": "https://arxiv.org/pdf/2506.17516", "abs": "https://arxiv.org/abs/2506.17516", "authors": ["Zhou Chen", "Sanjoy Kundu", "Harsimran S. Baweja", "Sathyanarayanan N. Aakur"], "title": "EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to IEEE Robotics and Automation Letters, 2025", "summary": "Active event perception, the ability to dynamically detect, track, and\nsummarize events in real time, is essential for embodied intelligence in tasks\nsuch as human-AI collaboration, assistive robotics, and autonomous navigation.\nHowever, existing approaches often depend on predefined action spaces,\nannotated datasets, and extrinsic rewards, limiting their adaptability and\nscalability in dynamic, real-world scenarios. Inspired by cognitive theories of\nevent perception and predictive coding, we propose EASE, a self-supervised\nframework that unifies spatiotemporal representation learning and embodied\ncontrol through free energy minimization. EASE leverages prediction errors and\nentropy as intrinsic signals to segment events, summarize observations, and\nactively track salient actors, operating without explicit annotations or\nexternal rewards. By coupling a generative perception model with an\naction-driven control policy, EASE dynamically aligns predictions with\nobservations, enabling emergent behaviors such as implicit memory, target\ncontinuity, and adaptability to novel environments. Extensive evaluations in\nsimulation and real-world settings demonstrate EASE's ability to achieve\nprivacy-preserving and scalable event perception, providing a robust foundation\nfor embodied systems in unscripted, dynamic tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEASE\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7531\u80fd\u91cf\u6700\u5c0f\u5316\u5b9e\u73b0\u52a8\u6001\u4e8b\u4ef6\u611f\u77e5\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\u6216\u6807\u6ce8\u6570\u636e\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u611f\u77e5\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\u3001\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5916\u90e8\u5956\u52b1\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u3002\u53d7\u4e8b\u4ef6\u611f\u77e5\u548c\u9884\u6d4b\u7f16\u7801\u7684\u8ba4\u77e5\u7406\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u4e3b\u52a8\u4e8b\u4ef6\u611f\u77e5\u3002", "method": "EASE\u6846\u67b6\u901a\u8fc7\u81ea\u7531\u80fd\u91cf\u6700\u5c0f\u5316\u7edf\u4e00\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\u548c\u5177\u8eab\u63a7\u5236\uff0c\u5229\u7528\u9884\u6d4b\u8bef\u5dee\u548c\u71b5\u4f5c\u4e3a\u5185\u5728\u4fe1\u53f7\u5206\u5272\u4e8b\u4ef6\u3001\u603b\u7ed3\u89c2\u5bdf\u5e76\u4e3b\u52a8\u8ddf\u8e2a\u663e\u8457\u76ee\u6807\uff0c\u65e0\u9700\u6807\u6ce8\u6216\u5916\u90e8\u5956\u52b1\u3002\u7ed3\u5408\u751f\u6210\u611f\u77e5\u6a21\u578b\u548c\u52a8\u4f5c\u9a71\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u52a8\u6001\u5bf9\u9f50\u9884\u6d4b\u4e0e\u89c2\u5bdf\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cEASE\u80fd\u591f\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u6269\u5c55\u7684\u4e8b\u4ef6\u611f\u77e5\uff0c\u652f\u6301\u9690\u5f0f\u8bb0\u5fc6\u3001\u76ee\u6807\u8fde\u7eed\u6027\u548c\u5bf9\u65b0\u73af\u5883\u7684\u9002\u5e94\u6027\u7b49\u884c\u4e3a\u3002", "conclusion": "EASE\u4e3a\u52a8\u6001\u3001\u975e\u811a\u672c\u4efb\u52a1\u4e2d\u7684\u5177\u8eab\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u6846\u67b6\u5728\u4e3b\u52a8\u4e8b\u4ef6\u611f\u77e5\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "EASE\uff1a\u901a\u8fc7\u81ea\u76d1\u7763\u80fd\u91cf\u6700\u5c0f\u5316\u5b9e\u73b0\u5177\u8eab\u4e3b\u52a8\u4e8b\u4ef6\u611f\u77e5", "abstract_zh": "\u4e3b\u52a8\u4e8b\u4ef6\u611f\u77e5\u662f\u5177\u8eab\u667a\u80fd\u5728\u4eba\u7c7b-AI\u534f\u4f5c\u3001\u8f85\u52a9\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\u3001\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5916\u90e8\u5956\u52b1\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u3002\u53d7\u4e8b\u4ef6\u611f\u77e5\u548c\u9884\u6d4b\u7f16\u7801\u7684\u8ba4\u77e5\u7406\u8bba\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86EASE\uff0c\u4e00\u79cd\u901a\u8fc7\u81ea\u7531\u80fd\u91cf\u6700\u5c0f\u5316\u7edf\u4e00\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\u548c\u5177\u8eab\u63a7\u5236\u7684\u81ea\u76d1\u7763\u6846\u67b6\u3002EASE\u5229\u7528\u9884\u6d4b\u8bef\u5dee\u548c\u71b5\u4f5c\u4e3a\u5185\u5728\u4fe1\u53f7\u5206\u5272\u4e8b\u4ef6\u3001\u603b\u7ed3\u89c2\u5bdf\u5e76\u4e3b\u52a8\u8ddf\u8e2a\u663e\u8457\u76ee\u6807\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u6216\u5916\u90e8\u5956\u52b1\u3002\u901a\u8fc7\u5c06\u751f\u6210\u611f\u77e5\u6a21\u578b\u4e0e\u52a8\u4f5c\u9a71\u52a8\u63a7\u5236\u7b56\u7565\u7ed3\u5408\uff0cEASE\u52a8\u6001\u5bf9\u9f50\u9884\u6d4b\u4e0e\u89c2\u5bdf\uff0c\u5b9e\u73b0\u4e86\u9690\u5f0f\u8bb0\u5fc6\u3001\u76ee\u6807\u8fde\u7eed\u6027\u548c\u5bf9\u65b0\u73af\u5883\u7684\u9002\u5e94\u6027\u7b49\u884c\u4e3a\u3002\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cEASE\u80fd\u591f\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u6269\u5c55\u7684\u4e8b\u4ef6\u611f\u77e5\uff0c\u4e3a\u52a8\u6001\u3001\u975e\u811a\u672c\u4efb\u52a1\u4e2d\u7684\u5177\u8eab\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u3002"}}
{"id": "2506.17963", "pdf": "https://arxiv.org/pdf/2506.17963", "abs": "https://arxiv.org/abs/2506.17963", "authors": ["Zhiwei Nie", "Hongyu Zhang", "Hao Jiang", "Yutian Liu", "Xiansong Huang", "Fan Xu", "Jie Fu", "Zhixiang Ren", "Yonghong Tian", "Wen-Bin Zhang", "Jie Chen"], "title": "OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Understanding and modeling enzyme-substrate interactions is crucial for\ncatalytic mechanism research, enzyme engineering, and metabolic engineering.\nAlthough a large number of predictive methods have emerged, they do not\nincorporate prior knowledge of enzyme catalysis to rationally modulate general\nprotein-molecule features that are misaligned with catalytic patterns. To\naddress this issue, we introduce a two-stage progressive framework, OmniESI,\nfor enzyme-substrate interaction prediction through conditional deep learning.\nBy decomposing the modeling of enzyme-substrate interactions into a two-stage\nprogressive process, OmniESI incorporates two conditional networks that\nrespectively emphasize enzymatic reaction specificity and crucial\ncatalysis-related interactions, facilitating a gradual feature modulation in\nthe latent space from general protein-molecule domain to catalysis-aware\ndomain. On top of this unified architecture, OmniESI can adapt to a variety of\ndownstream tasks, including enzyme kinetic parameter prediction,\nenzyme-substrate pairing prediction, enzyme mutational effect prediction, and\nenzymatic active site annotation. Under the multi-perspective performance\nevaluation of in-distribution and out-of-distribution settings, OmniESI\nconsistently delivered superior performance than state-of-the-art specialized\nmethods across seven benchmarks. More importantly, the proposed conditional\nnetworks were shown to internalize the fundamental patterns of catalytic\nefficiency while significantly improving prediction performance, with only\nnegligible parameter increases (0.16%), as demonstrated by ablation studies on\nkey components. Overall, OmniESI represents a unified predictive approach for\nenzyme-substrate interactions, providing an effective tool for catalytic\nmechanism cracking and enzyme engineering with strong generalization and broad\napplicability.", "AI": {"tldr": "OmniESI\u662f\u4e00\u4e2a\u57fa\u4e8e\u6e10\u8fdb\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6e10\u8fdb\u8fc7\u7a0b\u7ed3\u5408\u50ac\u5316\u7279\u5f02\u6027\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u9176\u50ac\u5316\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u8c03\u6574\u4e0e\u50ac\u5316\u6a21\u5f0f\u4e0d\u5339\u914d\u7684\u86cb\u767d\u8d28-\u5206\u5b50\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u50ac\u5316\u7279\u5f02\u6027\u5e76\u9010\u6b65\u4f18\u5316\u7279\u5f81\u7684\u9884\u6d4b\u6846\u67b6\u3002", "method": "OmniESI\u91c7\u7528\u4e24\u9636\u6bb5\u6e10\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u6761\u4ef6\u7f51\u7edc\u5206\u522b\u5f3a\u8c03\u9176\u53cd\u5e94\u7279\u5f02\u6027\u548c\u5173\u952e\u50ac\u5316\u76f8\u5173\u76f8\u4e92\u4f5c\u7528\uff0c\u9010\u6b65\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5c06\u7279\u5f81\u4ece\u901a\u7528\u86cb\u767d\u8d28-\u5206\u5b50\u9886\u57df\u8c03\u6574\u5230\u50ac\u5316\u611f\u77e5\u9886\u57df\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniESI\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u901a\u8fc7\u5173\u952e\u7ec4\u4ef6\u7684\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u5176\u4ec5\u589e\u52a00.16%\u53c2\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "OmniESI\u4e3a\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u662f\u50ac\u5316\u673a\u5236\u7814\u7a76\u548c\u9176\u5de5\u7a0b\u7684\u6709\u6548\u5de5\u5177\u3002", "paper_title_zh": "OmniESI\uff1a\u57fa\u4e8e\u6e10\u8fdb\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u7684\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7edf\u4e00\u6846\u67b6", "abstract_zh": "\u7406\u89e3\u548c\u5efa\u6a21\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\u5bf9\u4e8e\u50ac\u5316\u673a\u5236\u7814\u7a76\u3001\u9176\u5de5\u7a0b\u548c\u4ee3\u8c22\u5de5\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u672a\u80fd\u7ed3\u5408\u9176\u50ac\u5316\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u5408\u7406\u8c03\u6574\u4e0e\u50ac\u5316\u6a21\u5f0f\u4e0d\u5339\u914d\u7684\u901a\u7528\u86cb\u767d\u8d28-\u5206\u5b50\u7279\u5f81\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OmniESI\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u6e10\u8fdb\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\u3002\u901a\u8fc7\u5c06\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\u7684\u5efa\u6a21\u5206\u89e3\u4e3a\u4e24\u9636\u6bb5\u6e10\u8fdb\u8fc7\u7a0b\uff0cOmniESI\u7ed3\u5408\u4e86\u4e24\u4e2a\u6761\u4ef6\u7f51\u7edc\uff0c\u5206\u522b\u5f3a\u8c03\u9176\u53cd\u5e94\u7279\u5f02\u6027\u548c\u5173\u952e\u50ac\u5316\u76f8\u5173\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9010\u6b65\u5c06\u7279\u5f81\u4ece\u901a\u7528\u86cb\u767d\u8d28-\u5206\u5b50\u9886\u57df\u8c03\u6574\u5230\u50ac\u5316\u611f\u77e5\u9886\u57df\u3002\u5728\u8fd9\u4e00\u7edf\u4e00\u67b6\u6784\u7684\u57fa\u7840\u4e0a\uff0cOmniESI\u53ef\u9002\u5e94\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u9176\u52a8\u529b\u5b66\u53c2\u6570\u9884\u6d4b\u3001\u9176-\u5e95\u7269\u914d\u5bf9\u9884\u6d4b\u3001\u9176\u7a81\u53d8\u6548\u5e94\u9884\u6d4b\u548c\u9176\u6d3b\u6027\u4f4d\u70b9\u6ce8\u91ca\u3002\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u7684\u591a\u89c6\u89d2\u6027\u80fd\u8bc4\u4f30\u4e2d\uff0cOmniESI\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u4e13\u7528\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6240\u63d0\u51fa\u7684\u6761\u4ef6\u7f51\u7edc\u80fd\u591f\u5185\u5316\u50ac\u5316\u6548\u7387\u7684\u57fa\u672c\u6a21\u5f0f\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u800c\u53c2\u6570\u4ec5\u589e\u52a00.16%\uff08\u5173\u952e\u7ec4\u4ef6\u7684\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u70b9\uff09\u3002\u603b\u4f53\u800c\u8a00\uff0cOmniESI\u4e3a\u9176-\u5e95\u7269\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u662f\u50ac\u5316\u673a\u5236\u7834\u89e3\u548c\u9176\u5de5\u7a0b\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.17540", "pdf": "https://arxiv.org/pdf/2506.17540", "abs": "https://arxiv.org/abs/2506.17540", "authors": ["Tingting Liu", "Yuan Liu", "Jinhui Tang", "Liyin Yuan", "Chengyu Liu", "Chunlai Li", "Xiubao Sui", "Qian Chen"], "title": "MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Thermal infrared (TIR) images, acquired through thermal radiation imaging,\nare unaffected by variations in lighting conditions and atmospheric haze.\nHowever, TIR images inherently lack color and texture information, limiting\ndownstream tasks and potentially causing visual fatigue. Existing colorization\nmethods primarily rely on single-band images with limited spectral information\nand insufficient feature extraction capabilities, which often result in image\ndistortion and semantic ambiguity. In contrast, multiband infrared imagery\nprovides richer spectral data, facilitating the preservation of finer details\nand enhancing semantic accuracy. In this paper, we propose a generative\nadversarial network (GAN)-based framework designed to integrate spectral\ninformation to enhance the colorization of infrared images. The framework\nemploys a multi-stage spectral self-attention Transformer network (MTSIC) as\nthe generator. Each spectral feature is treated as a token for self-attention\ncomputation, and a multi-head self-attention mechanism forms a spatial-spectral\nattention residual block (SARB), achieving multi-band feature mapping and\nreducing semantic confusion. Multiple SARB units are integrated into a\nTransformer-based single-stage network (STformer), which uses a U-shaped\narchitecture to extract contextual information, combined with multi-scale\nwavelet blocks (MSWB) to align semantic information in the spatial-frequency\ndual domain. Multiple STformer modules are cascaded to form MTSIC,\nprogressively optimizing the reconstruction quality. Experimental results\ndemonstrate that the proposed method significantly outperforms traditional\ntechniques and effectively enhances the visual quality of infrared images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u591a\u9636\u6bb5Transformer\u6846\u67b6\uff08MTSIC\uff09\uff0c\u7528\u4e8e\u7ea2\u5916\u56fe\u50cf\u7684\u989c\u8272\u5316\u3002\u901a\u8fc7\u591a\u6ce2\u6bb5\u5149\u8c31\u4fe1\u606f\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7684\u989c\u8272\u5316\u8d28\u91cf\u548c\u8bed\u4e49\u51c6\u786e\u6027\u3002", "motivation": "\u70ed\u7ea2\u5916\uff08TIR\uff09\u56fe\u50cf\u4e0d\u53d7\u5149\u7167\u548c\u5927\u6c14\u96fe\u973e\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u989c\u8272\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u5e76\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u75b2\u52b3\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u6ce2\u6bb5\u56fe\u50cf\uff0c\u5149\u8c31\u4fe1\u606f\u6709\u9650\u4e14\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0d\u8db3\uff0c\u6613\u5bfc\u81f4\u56fe\u50cf\u5931\u771f\u548c\u8bed\u4e49\u6a21\u7cca\u3002\u591a\u6ce2\u6bb5\u7ea2\u5916\u56fe\u50cf\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u5149\u8c31\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u4fdd\u7559\u7ec6\u8282\u548c\u63d0\u5347\u8bed\u4e49\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faMTSIC\u6846\u67b6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u5149\u8c31\u81ea\u6ce8\u610f\u529bTransformer\u7f51\u7edc\u4f5c\u4e3a\u751f\u6210\u5668\u3002\u5c06\u6bcf\u4e2a\u5149\u8c31\u7279\u5f81\u89c6\u4e3a\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u7684token\uff0c\u5e76\u901a\u8fc7\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f62\u6210\u7a7a\u95f4-\u5149\u8c31\u6ce8\u610f\u529b\u6b8b\u5dee\u5757\uff08SARB\uff09\uff0c\u5b9e\u73b0\u591a\u6ce2\u6bb5\u7279\u5f81\u6620\u5c04\u5e76\u51cf\u5c11\u8bed\u4e49\u6df7\u6dc6\u3002\u591a\u4e2aSARB\u5355\u5143\u96c6\u6210\u5230\u57fa\u4e8eTransformer\u7684\u5355\u9636\u6bb5\u7f51\u7edc\uff08STformer\uff09\u4e2d\uff0c\u7ed3\u5408U\u5f62\u67b6\u6784\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5757\uff08MSWB\uff09\u5728\u7a7a\u95f4-\u9891\u7387\u53cc\u57df\u5bf9\u9f50\u8bed\u4e49\u4fe1\u606f\u3002\u591a\u4e2aSTformer\u6a21\u5757\u7ea7\u8054\u5f62\u6210MTSIC\uff0c\u9010\u6b65\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ea2\u5916\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "MTSIC\u901a\u8fc7\u591a\u6ce2\u6bb5\u5149\u8c31\u4fe1\u606f\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7ea2\u5916\u56fe\u50cf\u989c\u8272\u5316\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u548c\u5931\u771f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u8bed\u4e49\u51c6\u786e\u6027\u3002", "paper_title_zh": "MTSIC\uff1a\u57fa\u4e8e\u591a\u9636\u6bb5Transformer\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7528\u4e8e\u5149\u8c31\u7ea2\u5916\u56fe\u50cf\u989c\u8272\u5316", "abstract_zh": "\u70ed\u7ea2\u5916\uff08TIR\uff09\u56fe\u50cf\u901a\u8fc7\u70ed\u8f90\u5c04\u6210\u50cf\u83b7\u53d6\uff0c\u4e0d\u53d7\u5149\u7167\u6761\u4ef6\u548c\u5927\u6c14\u96fe\u973e\u53d8\u5316\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0cTIR\u56fe\u50cf\u672c\u8eab\u7f3a\u4e4f\u989c\u8272\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u4e0b\u6e38\u4efb\u52a1\u5e76\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u75b2\u52b3\u3002\u73b0\u6709\u7684\u989c\u8272\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u6ce2\u6bb5\u56fe\u50cf\uff0c\u5149\u8c31\u4fe1\u606f\u6709\u9650\u4e14\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0d\u8db3\uff0c\u5f80\u5f80\u5bfc\u81f4\u56fe\u50cf\u5931\u771f\u548c\u8bed\u4e49\u6a21\u7cca\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u591a\u6ce2\u6bb5\u7ea2\u5916\u56fe\u50cf\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u5149\u8c31\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u4fdd\u7559\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\u5e76\u63d0\u5347\u8bed\u4e49\u51c6\u786e\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u6574\u5408\u5149\u8c31\u4fe1\u606f\u4ee5\u589e\u5f3a\u7ea2\u5916\u56fe\u50cf\u7684\u989c\u8272\u5316\u3002\u8be5\u6846\u67b6\u91c7\u7528\u591a\u9636\u6bb5\u5149\u8c31\u81ea\u6ce8\u610f\u529bTransformer\u7f51\u7edc\uff08MTSIC\uff09\u4f5c\u4e3a\u751f\u6210\u5668\u3002\u6bcf\u4e2a\u5149\u8c31\u7279\u5f81\u88ab\u89c6\u4e3a\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u7684token\uff0c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f62\u6210\u7a7a\u95f4-\u5149\u8c31\u6ce8\u610f\u529b\u6b8b\u5dee\u5757\uff08SARB\uff09\uff0c\u5b9e\u73b0\u591a\u6ce2\u6bb5\u7279\u5f81\u6620\u5c04\u5e76\u51cf\u5c11\u8bed\u4e49\u6df7\u6dc6\u3002\u591a\u4e2aSARB\u5355\u5143\u96c6\u6210\u5230\u57fa\u4e8eTransformer\u7684\u5355\u9636\u6bb5\u7f51\u7edc\uff08STformer\uff09\u4e2d\uff0c\u5229\u7528U\u5f62\u67b6\u6784\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5757\uff08MSWB\uff09\u5728\u7a7a\u95f4-\u9891\u7387\u53cc\u57df\u5bf9\u9f50\u8bed\u4e49\u4fe1\u606f\u3002\u591a\u4e2aSTformer\u6a21\u5757\u7ea7\u8054\u5f62\u6210MTSIC\uff0c\u9010\u6b65\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ea2\u5916\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2506.17967", "pdf": "https://arxiv.org/pdf/2506.17967", "abs": "https://arxiv.org/abs/2506.17967", "authors": ["Mariya Hendriksen", "Tabish Rashid", "David Bignell", "Raluca Georgescu", "Abdelhak Lemkhenter", "Katja Hofmann", "Sam Devlin", "Sarah Parisot"], "title": "Adapting Vision-Language Models for Evaluating World Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "World models -- generative models that simulate environment dynamics\nconditioned on past observations and actions -- are gaining prominence in\nplanning, simulation, and embodied AI. However, evaluating their rollouts\nremains a fundamental challenge, requiring fine-grained, temporally grounded\nassessment of action alignment and semantic consistency -- capabilities not\ncaptured by existing metrics. Vision-Language Models (VLMs) have shown promise\nas automatic evaluators of generative content due to their strong multimodal\nreasoning abilities. Yet, their use in fine-grained, temporally sensitive\nevaluation tasks remains limited and requires targeted adaptation. We introduce\na evaluation protocol targeting two recognition tasks -- action recognition and\ncharacter recognition -- each assessed across binary, multiple-choice, and\nopen-ended formats. To support this, we present UNIVERSE (UNIfied\nVision-language Evaluator for Rollouts in Simulated Environments), a method for\nadapting VLMs to rollout evaluation under data and compute constraints. We\nconduct a large-scale study comparing full, partial, and parameter-efficient\nfinetuning across task formats, context lengths, sampling strategies, and data\ncompositions. The resulting unified evaluator matches the performance of\ntask-specific baselines using a single checkpoint. Human studies confirm strong\nalignment with human judgments, establishing UNIVERSE as a scalable,\nsemantics-aware evaluator for world models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u65b9\u6cd5UNIVERSE\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u7684\u52a8\u6001\u6a21\u62df\u6548\u679c\uff0c\u901a\u8fc7\u52a8\u4f5c\u8bc6\u522b\u548c\u89d2\u8272\u8bc6\u522b\u4efb\u52a1\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u5e76\u5728\u8ba1\u7b97\u548c\u6570\u636e\u9650\u5236\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u5728\u89c4\u5212\u3001\u6a21\u62df\u548c\u5177\u8eabAI\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u52a8\u6001\u6a21\u62df\u6548\u679c\u7684\u8bc4\u4f30\u4ecd\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u548c\u65f6\u95f4\u654f\u611f\u7684\u6307\u6807\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u6709\u671b\u6210\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u9700\u9488\u5bf9\u6027\u9002\u914d\u3002", "method": "\u63d0\u51faUNIVERSE\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u4f5c\u8bc6\u522b\u548c\u89d2\u8272\u8bc6\u522b\u4efb\u52a1\uff08\u5305\u62ec\u4e8c\u9009\u4e00\u3001\u591a\u9009\u548c\u5f00\u653e\u5f0f\u683c\u5f0f\uff09\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u5b8c\u6574\u3001\u90e8\u5206\u548c\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u53ca\u4e0d\u540c\u4efb\u52a1\u683c\u5f0f\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u91c7\u6837\u7b56\u7565\u548c\u6570\u636e\u7ec4\u5408\u3002", "result": "UNIVERSE\u5728\u5355\u4e00\u68c0\u67e5\u70b9\u4e0b\u8fbe\u5230\u4efb\u52a1\u4e13\u7528\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u6210\u4e3a\u53ef\u6269\u5c55\u4e14\u8bed\u4e49\u611f\u77e5\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "UNIVERSE\u4e3a\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8bed\u4e49\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u8ba1\u7b97\u548c\u6570\u636e\u9650\u5236\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "paper_title_zh": "\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee5\u8bc4\u4f30\u4e16\u754c\u6a21\u578b", "abstract_zh": "\u4e16\u754c\u6a21\u578b\u2014\u2014\u57fa\u4e8e\u8fc7\u53bb\u89c2\u5bdf\u548c\u52a8\u4f5c\u751f\u6210\u73af\u5883\u52a8\u6001\u7684\u751f\u6210\u6a21\u578b\u2014\u2014\u5728\u89c4\u5212\u3001\u6a21\u62df\u548c\u5177\u8eabAI\u4e2d\u65e5\u76ca\u91cd\u8981\u3002\u7136\u800c\uff0c\u8bc4\u4f30\u5176\u52a8\u6001\u6a21\u62df\u6548\u679c\u4ecd\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u3001\u65f6\u95f4\u654f\u611f\u7684\u8bc4\u4f30\u52a8\u4f5c\u5bf9\u9f50\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u800c\u73b0\u6709\u6307\u6807\u65e0\u6cd5\u6ee1\u8db3\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u56e0\u5176\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u6709\u671b\u6210\u4e3a\u751f\u6210\u5185\u5bb9\u7684\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u3001\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u9650\u4e14\u9700\u9488\u5bf9\u6027\u9002\u914d\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u534f\u8bae\uff0c\u9488\u5bf9\u52a8\u4f5c\u8bc6\u522b\u548c\u89d2\u8272\u8bc6\u522b\u4e24\u9879\u4efb\u52a1\uff0c\u5206\u522b\u4ee5\u4e8c\u9009\u4e00\u3001\u591a\u9009\u548c\u5f00\u653e\u5f0f\u683c\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86UNIVERSE\uff08\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u8bc4\u4f30\u5668\uff09\uff0c\u4e00\u79cd\u5728\u8ba1\u7b97\u548c\u6570\u636e\u9650\u5236\u4e0b\u9002\u914dVLMs\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u5b8c\u6574\u3001\u90e8\u5206\u548c\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u53ca\u4e0d\u540c\u4efb\u52a1\u683c\u5f0f\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u91c7\u6837\u7b56\u7565\u548c\u6570\u636e\u7ec4\u5408\u3002\u6700\u7ec8\u7684\u7edf\u4e00\u8bc4\u4f30\u5668\u5728\u5355\u4e00\u68c0\u67e5\u70b9\u4e0b\u8fbe\u5230\u4efb\u52a1\u4e13\u7528\u57fa\u7ebf\u7684\u6027\u80fd\u3002\u4eba\u7c7b\u7814\u7a76\u8bc1\u5b9e\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u786e\u7acb\u4e86UNIVERSE\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u53ef\u6269\u5c55\u3001\u8bed\u4e49\u611f\u77e5\u8bc4\u4f30\u5de5\u5177\u7684\u5730\u4f4d\u3002"}}
{"id": "2506.17552", "pdf": "https://arxiv.org/pdf/2506.17552", "abs": "https://arxiv.org/abs/2506.17552", "authors": ["Wei Zhang", "Zi Wang", "Hanwen Zhou", "Zhaohong Deng", "Weiping Ding", "Yuxi Ge", "Te Zhang", "Yuanpeng Zhang", "Kup-Sze Choi", "Shitong Wang", "Shudong Hu"], "title": "DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "A reliable evaluation of surgical difficulty can improve the success of the\ntreatment for rectal cancer and the current evaluation method is based on\nclinical data. However, more data about rectal cancer can be collected with the\ndevelopment of technology. Meanwhile, with the development of artificial\nintelligence, its application in rectal cancer treatment is becoming possible.\nIn this paper, a multi-view rectal cancer dataset is first constructed to give\na more comprehensive view of patients, including the high-resolution MRI image\nview, pressed-fat MRI image view, and clinical data view. Then, an\ninterpretable incomplete multi-view surgical evaluation model is proposed,\nconsidering that it is hard to obtain extensive and complete patient data in\nreal application scenarios. Specifically, a dual representation incomplete\nmulti-view learning model is first proposed to extract the common information\nbetween views and specific information in each view. In this model, the missing\nview imputation is integrated into representation learning, and second-order\nsimilarity constraint is also introduced to improve the cooperative learning\nbetween these two parts. Then, based on the imputed multi-view data and the\nlearned dual representation, a multi-view surgical evaluation model with the\nTSK fuzzy system is proposed. In the proposed model, a cooperative learning\nmechanism is constructed to explore the consistent information between views,\nand Shannon entropy is also introduced to adapt the view weight. On the MVRC\ndataset, we compared it with several advanced algorithms and DRIMV_TSK obtained\nthe best results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u4e0d\u5b8c\u6574\u591a\u89c6\u89d2\u76f4\u80a0\u764c\u624b\u672f\u8bc4\u4f30\u6a21\u578bDRIMV_TSK\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u89c6\u89d2\u6570\u636e\u96c6\uff08\u5305\u62ec\u9ad8\u5206\u8fa8\u7387MRI\u3001\u538b\u8102MRI\u548c\u4e34\u5e8a\u6570\u636e\uff09\u5e76\u6574\u5408\u53cc\u8868\u793a\u5b66\u4e60\u548cTSK\u6a21\u7cca\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u96be\u5ea6\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u76f4\u80a0\u764c\u624b\u672f\u96be\u5ea6\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e34\u5e8a\u6570\u636e\uff0c\u4f46\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u66f4\u591a\u6570\u636e\uff08\u5982MRI\u56fe\u50cf\uff09\u53ef\u7528\u4e8e\u8bc4\u4f30\u3002\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u4e3a\u76f4\u80a0\u764c\u6cbb\u7597\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u89c6\u89d2\u6570\u636e\u6784\u5efa\u548c\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u63d0\u5347\u624b\u672f\u8bc4\u4f30\u7684\u5168\u9762\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u9996\u5148\u6784\u5efa\u591a\u89c6\u89d2\u76f4\u80a0\u764c\u6570\u636e\u96c6\uff0c\u5305\u62ec\u9ad8\u5206\u8fa8\u7387MRI\u3001\u538b\u8102MRI\u548c\u4e34\u5e8a\u6570\u636e\u3002\u63d0\u51fa\u53cc\u8868\u793a\u4e0d\u5b8c\u6574\u591a\u89c6\u89d2\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u53d6\u89c6\u56fe\u95f4\u5171\u540c\u4fe1\u606f\u548c\u5404\u89c6\u56fe\u7279\u6709\u4fe1\u606f\uff0c\u5e76\u6574\u5408\u7f3a\u5931\u89c6\u56fe\u586b\u8865\u4e0e\u8868\u793a\u5b66\u4e60\u3002\u5f15\u5165\u4e8c\u9636\u76f8\u4f3c\u6027\u7ea6\u675f\u4f18\u5316\u534f\u4f5c\u5b66\u4e60\u3002\u57fa\u4e8e\u586b\u8865\u540e\u7684\u591a\u89c6\u89d2\u6570\u636e\u548c\u53cc\u8868\u793a\uff0c\u63d0\u51fa\u7ed3\u5408TSK\u6a21\u7cca\u7cfb\u7edf\u7684\u591a\u89c6\u89d2\u624b\u672f\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u4f5c\u5b66\u4e60\u673a\u5236\u63a2\u7d22\u89c6\u56fe\u95f4\u4e00\u81f4\u6027\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u9999\u519c\u71b5\u81ea\u9002\u5e94\u8c03\u6574\u89c6\u56fe\u6743\u91cd\u3002", "result": "\u5728MVRC\u6570\u636e\u96c6\u4e0a\uff0cDRIMV_TSK\u6a21\u578b\u4e0e\u591a\u79cd\u5148\u8fdb\u7b97\u6cd5\u5bf9\u6bd4\uff0c\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u624b\u672f\u96be\u5ea6\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DRIMV_TSK\u6a21\u578b\u901a\u8fc7\u591a\u89c6\u89d2\u6570\u636e\u6574\u5408\u548c\u53ef\u89e3\u91ca\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f4\u80a0\u764c\u624b\u672f\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "paper_title_zh": "DRIMV_TSK\uff1a\u4e00\u79cd\u9488\u5bf9\u4e0d\u5b8c\u6574\u591a\u89c6\u89d2\u76f4\u80a0\u764c\u6570\u636e\u7684\u53ef\u89e3\u91ca\u624b\u672f\u8bc4\u4f30\u6a21\u578b", "abstract_zh": "\u53ef\u9760\u7684\u624b\u672f\u96be\u5ea6\u8bc4\u4f30\u53ef\u63d0\u9ad8\u76f4\u80a0\u764c\u6cbb\u7597\u7684\u6210\u529f\u7387\uff0c\u800c\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u4e34\u5e8a\u6570\u636e\u3002\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u66f4\u591a\u5173\u4e8e\u76f4\u80a0\u764c\u7684\u6570\u636e\uff08\u5982MRI\u56fe\u50cf\uff09\u53ef\u88ab\u91c7\u96c6\uff0c\u540c\u65f6\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u4f7f\u5176\u5728\u76f4\u80a0\u764c\u6cbb\u7597\u4e2d\u7684\u5e94\u7528\u6210\u4e3a\u53ef\u80fd\u3002\u672c\u6587\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u76f4\u80a0\u764c\u6570\u636e\u96c6\uff0c\u5305\u62ec\u9ad8\u5206\u8fa8\u7387MRI\u56fe\u50cf\u3001\u538b\u8102MRI\u56fe\u50cf\u548c\u4e34\u5e8a\u6570\u636e\u89c6\u56fe\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u60a3\u8005\u4fe1\u606f\u3002\u968f\u540e\uff0c\u9488\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u83b7\u53d6\u5b8c\u6574\u60a3\u8005\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u4e0d\u5b8c\u6574\u591a\u89c6\u89d2\u624b\u672f\u8bc4\u4f30\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8868\u793a\u4e0d\u5b8c\u6574\u591a\u89c6\u89d2\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u53d6\u89c6\u56fe\u95f4\u7684\u5171\u540c\u4fe1\u606f\u548c\u5404\u89c6\u56fe\u7684\u7279\u6709\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u5c06\u7f3a\u5931\u89c6\u56fe\u586b\u8865\u6574\u5408\u5230\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u5e76\u5f15\u5165\u4e8c\u9636\u76f8\u4f3c\u6027\u7ea6\u675f\u4ee5\u4f18\u5316\u4e24\u90e8\u5206\u534f\u4f5c\u5b66\u4e60\u3002\u7136\u540e\uff0c\u57fa\u4e8e\u586b\u8865\u540e\u7684\u591a\u89c6\u89d2\u6570\u636e\u548c\u5b66\u4e60\u7684\u53cc\u8868\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TSK\u6a21\u7cca\u7cfb\u7edf\u7684\u591a\u89c6\u89d2\u624b\u672f\u8bc4\u4f30\u6a21\u578b\u3002\u5728\u8be5\u6a21\u578b\u4e2d\uff0c\u6784\u5efa\u4e86\u534f\u4f5c\u5b66\u4e60\u673a\u5236\u4ee5\u63a2\u7d22\u89c6\u56fe\u95f4\u7684\u4e00\u81f4\u6027\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u9999\u519c\u71b5\u81ea\u9002\u5e94\u8c03\u6574\u89c6\u56fe\u6743\u91cd\u3002\u5728MVRC\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u591a\u79cd\u5148\u8fdb\u7b97\u6cd5\u5bf9\u6bd4\uff0cDRIMV_TSK\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\u3002"}}
{"id": "2506.17968", "pdf": "https://arxiv.org/pdf/2506.17968", "abs": "https://arxiv.org/abs/2506.17968", "authors": ["Wenjian Huang", "Guiping Cao", "Jiahao Xia", "Jingkun Chen", "Hao Wang", "Jianguo Zhang"], "title": "h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.PR", "stat.ML"], "comment": null, "summary": "Deep neural networks have demonstrated remarkable performance across numerous\nlearning tasks but often suffer from miscalibration, resulting in unreliable\nprobability outputs. This has inspired many recent works on mitigating\nmiscalibration, particularly through post-hoc recalibration methods that aim to\nobtain calibrated probabilities without sacrificing the classification\nperformance of pre-trained models. In this study, we summarize and categorize\nprevious works into three general strategies: intuitively designed methods,\nbinning-based methods, and methods based on formulations of ideal calibration.\nThrough theoretical and practical analysis, we highlight ten common limitations\nin previous approaches. To address these limitations, we propose a\nprobabilistic learning framework for calibration called h-calibration, which\ntheoretically constructs an equivalent learning formulation for canonical\ncalibration with boundedness. On this basis, we design a simple yet effective\npost-hoc calibration algorithm. Our method not only overcomes the ten\nidentified limitations but also achieves markedly better performance than\ntraditional methods, as validated by extensive experiments. We further analyze,\nboth theoretically and experimentally, the relationship and advantages of our\nlearning objective compared to traditional proper scoring rule. In summary, our\nprobabilistic framework derives an approximately equivalent differentiable\nobjective for learning error-bounded calibrated probabilities, elucidating the\ncorrespondence and convergence properties of computational statistics with\nrespect to theoretical bounds in canonical calibration. The theoretical\neffectiveness is verified on standard post-hoc calibration benchmarks by\nachieving state-of-the-art performance. This research offers valuable reference\nfor learning reliable likelihood in related fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ah-calibration\u7684\u6982\u7387\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u6982\u7387\u7684\u6821\u51c6\u95ee\u9898\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u7684\u5341\u5927\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u6982\u7387\u5f80\u5f80\u5b58\u5728\u6821\u51c6\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u6982\u7387\u9884\u6d4b\u3002\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u540e\u6821\u51c6\u65b9\u6cd5\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u591a\u79cd\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u65b0\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86h-calibration\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u6709\u754c\u6027\u7684\u7b49\u6548\u5b66\u4e60\u76ee\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u540e\u6821\u51c6\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5341\u5927\u5c40\u9650\u6027\uff0c\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0ch-calibration\u5728\u6807\u51c6\u540e\u6821\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u7406\u8bba\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u5176\u4e0e\u4f20\u7edf\u8bc4\u5206\u89c4\u5219\u7684\u5173\u7cfb\u548c\u4f18\u52bf\u3002", "conclusion": "h-calibration\u6846\u67b6\u4e3a\u5b66\u4e60\u6709\u754c\u6821\u51c6\u6982\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u8fd1\u4f3c\u53ef\u5fae\u7684\u76ee\u6807\uff0c\u9610\u660e\u4e86\u8ba1\u7b97\u7edf\u8ba1\u4e0e\u7406\u8bba\u754c\u9650\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u5176\u7406\u8bba\u6709\u6548\u6027\u901a\u8fc7\u5b9e\u9a8c\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "paper_title_zh": "h-calibration\uff1a\u91cd\u65b0\u601d\u8003\u57fa\u4e8e\u6982\u7387\u8bef\u5dee\u6709\u754c\u76ee\u6807\u7684\u5206\u7c7b\u5668\u6821\u51c6", "abstract_zh": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bb8\u591a\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u6982\u7387\u5f80\u5f80\u5b58\u5728\u6821\u51c6\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u6982\u7387\u9884\u6d4b\u3002\u8fd9\u6fc0\u53d1\u4e86\u8fd1\u5e74\u6765\u8bb8\u591a\u5173\u4e8e\u7f13\u89e3\u6821\u51c6\u95ee\u9898\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u901a\u8fc7\u540e\u6821\u51c6\u65b9\u6cd5\uff0c\u65e8\u5728\u4e0d\u727a\u7272\u9884\u8bad\u7ec3\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u6821\u51c6\u6982\u7387\u3002\u672c\u7814\u7a76\u5c06\u524d\u4eba\u5de5\u4f5c\u603b\u7ed3\u4e3a\u4e09\u79cd\u7b56\u7565\uff1a\u76f4\u89c2\u8bbe\u8ba1\u65b9\u6cd5\u3001\u57fa\u4e8e\u5206\u7bb1\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u7406\u60f3\u6821\u51c6\u516c\u5f0f\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8df5\u5206\u6790\uff0c\u6211\u4eec\u6307\u51fa\u4e86\u524d\u4eba\u65b9\u6cd5\u7684\u5341\u5927\u5e38\u89c1\u5c40\u9650\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ah-calibration\u7684\u6982\u7387\u5b66\u4e60\u6846\u67b6\uff0c\u7406\u8bba\u4e0a\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u6709\u754c\u6027\u7684\u7b49\u6548\u5b66\u4e60\u76ee\u6807\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u540e\u6821\u51c6\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u514b\u670d\u4e86\u5341\u5927\u5c40\u9650\u6027\uff0c\u8fd8\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u4ece\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5206\u6790\u4e86\u6211\u4eec\u7684\u5b66\u4e60\u76ee\u6807\u4e0e\u4f20\u7edf\u8bc4\u5206\u89c4\u5219\u7684\u5173\u7cfb\u548c\u4f18\u52bf\u3002\u603b\u4e4b\uff0c\u6211\u4eec\u7684\u6982\u7387\u6846\u67b6\u4e3a\u5b66\u4e60\u8bef\u5dee\u6709\u754c\u7684\u6821\u51c6\u6982\u7387\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8fd1\u4f3c\u53ef\u5fae\u7684\u76ee\u6807\uff0c\u9610\u660e\u4e86\u8ba1\u7b97\u7edf\u8ba1\u4e0e\u7406\u8bba\u754c\u9650\u7684\u5bf9\u5e94\u5173\u7cfb\u548c\u6536\u655b\u6027\u8d28\u3002\u7406\u8bba\u6709\u6548\u6027\u5728\u6807\u51c6\u540e\u6821\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u8fc7\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u5b66\u4e60\u76f8\u5173\u9886\u57df\u7684\u53ef\u9760\u4f3c\u7136\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2506.17623", "pdf": "https://arxiv.org/pdf/2506.17623", "abs": "https://arxiv.org/abs/2506.17623", "authors": ["Yuesheng Huang", "Peng Zhang", "Riliang Liu", "Jiaqi Liang"], "title": "Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?", "categories": ["cs.MM", "cs.CV"], "comment": "4 figures,7 tables", "summary": "A significant ``modality gap\" exists between the abundance of text-only data\nand the increasing power of multimodal models. This work systematically\ninvestigates whether images generated on-the-fly by Text-to-Image (T2I) models\ncan serve as a valuable complementary modality for text-centric tasks. Through\na comprehensive evaluation framework on text classification, we analyze the\nimpact of critical variables, including T2I model quality, prompt engineering\nstrategies, and multimodal fusion architectures. Our findings demonstrate that\nthis``synthetic perception\" can yield significant performance gains, even when\naugmenting strong large language model baselines. However, we find the\neffectiveness of this approach is highly conditional, depending critically on\nthe semantic alignment between text and the generated image, the inherent\n``visual groundability\" of the task, and the generative fidelity of the T2I\nmodel. Our work establishes the first rigorous benchmark for this paradigm,\nproviding a clear analysis of its potential and current limitations, and\ndemonstrating its viability as a pathway to enrich language understanding in\ntraditionally unimodal scenarios.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u53ef\u4f5c\u4e3a\u6587\u672c\u4e2d\u5fc3\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6709\u6548\u8865\u5145\u6a21\u6001\u3002\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u7efc\u5408\u8bc4\u4f30\uff0c\u53d1\u73b0\u5408\u6210\u56fe\u50cf\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5176\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u4efb\u52a1\u7684\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u53caT2I\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u6570\u636e\u4e30\u5bcc\uff0c\u4f46\u591a\u6a21\u6001\u6a21\u578b\u80fd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6a21\u6001\u9e3f\u6c9f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22T2I\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u80fd\u586b\u8865\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u4e3a\u6587\u672c\u4e2d\u5fc3\u4efb\u52a1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8865\u5145\u6a21\u6001\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u4e86T2I\u6a21\u578b\u8d28\u91cf\u3001\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u7b49\u5173\u952e\u53d8\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u56fe\u50cf\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5176\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u4efb\u52a1\u7684\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u53caT2I\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002\u7814\u7a76\u8fd8\u5efa\u7acb\u4e86\u8be5\u8303\u5f0f\u7684\u9996\u4e2a\u4e25\u683c\u57fa\u51c6\u3002", "conclusion": "\u5408\u6210\u611f\u77e5\u4e3a\u4f20\u7edf\u5355\u6a21\u6001\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u5176\u5e94\u7528\u9700\u8c28\u614e\u8003\u8651\u8bed\u4e49\u5bf9\u9f50\u548c\u751f\u6210\u8d28\u91cf\u7b49\u6761\u4ef6\u3002", "paper_title_zh": "\u751f\u6210\u7684\u56fe\u50cf\u80fd\u5426\u4f5c\u4e3a\u6587\u672c\u4e2d\u5fc3\u591a\u6a21\u6001\u5b66\u4e60\u7684\u53ef\u884c\u6a21\u6001\uff1f", "abstract_zh": "\u6587\u672c\u6570\u636e\u4e30\u5bcc\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u201c\u6a21\u6001\u9e3f\u6c9f\u201d\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u63a2\u8ba8\u4e86\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5b9e\u65f6\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u80fd\u4f5c\u4e3a\u6587\u672c\u4e2d\u5fc3\u4efb\u52a1\u7684\u6709\u4ef7\u503c\u8865\u5145\u6a21\u6001\u3002\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u4e86T2I\u6a21\u578b\u8d28\u91cf\u3001\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u7b49\u5173\u952e\u53d8\u91cf\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u79cd\u201c\u5408\u6210\u611f\u77e5\u201d\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u751a\u81f3\u80fd\u589e\u5f3a\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\u3002\u7136\u800c\uff0c\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u6587\u672c\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u4efb\u52a1\u7684\u201c\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u201d\u53caT2I\u6a21\u578b\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u3002\u672c\u7814\u7a76\u4e3a\u8be5\u8303\u5f0f\u5efa\u7acb\u4e86\u9996\u4e2a\u4e25\u683c\u57fa\u51c6\uff0c\u6e05\u6670\u5206\u6790\u4e86\u5176\u6f5c\u529b\u4e0e\u5f53\u524d\u5c40\u9650\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u4f20\u7edf\u5355\u6a21\u6001\u573a\u666f\u4e0b\u4e30\u5bcc\u8bed\u8a00\u7406\u89e3\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.18011", "pdf": "https://arxiv.org/pdf/2506.18011", "abs": "https://arxiv.org/abs/2506.18011", "authors": ["Eddie Conti", "Alejandro Astruc", "Alvaro Parafita", "Axel Brando"], "title": "Probing the Embedding Space of Transformers via Minimal Token Perturbations", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "IJCAI 2025 Workshop on Explainable Artificial Intelligence", "summary": "Understanding how information propagates through Transformer models is a key\nchallenge for interpretability. In this work, we study the effects of minimal\ntoken perturbations on the embedding space. In our experiments, we analyze the\nfrequency of which tokens yield to minimal shifts, highlighting that rare\ntokens usually lead to larger shifts. Moreover, we study how perturbations\npropagate across layers, demonstrating that input information is increasingly\nintermixed in deeper layers. Our findings validate the common assumption that\nthe first layers of a model can be used as proxies for model explanations.\nOverall, this work introduces the combination of token perturbations and shifts\non the embedding space as a powerful tool for model interpretability.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6700\u5c0f\u5316\u4ee4\u724c\u6270\u52a8\u7814\u7a76Transformer\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u53d1\u73b0\u7f55\u89c1\u4ee4\u724c\u901a\u5e38\u5bfc\u81f4\u66f4\u5927\u7684\u5d4c\u5165\u504f\u79fb\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6d45\u5c42\u53ef\u4f5c\u4e3a\u6a21\u578b\u89e3\u91ca\u7684\u4ee3\u7406\u3002", "motivation": "\u7406\u89e3Transformer\u6a21\u578b\u4e2d\u4fe1\u606f\u4f20\u64ad\u7684\u65b9\u5f0f\u662f\u89e3\u91ca\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6700\u5c0f\u4ee4\u724c\u6270\u52a8\u7814\u7a76\u5d4c\u5165\u7a7a\u95f4\u7684\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4ee4\u724c\u6270\u52a8\u5bf9\u5d4c\u5165\u7a7a\u95f4\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u6270\u52a8\u5728\u4e0d\u540c\u5c42\u95f4\u7684\u4f20\u64ad\uff0c\u5e76\u6bd4\u8f83\u7f55\u89c1\u4e0e\u5e38\u89c1\u4ee4\u724c\u7684\u5d4c\u5165\u504f\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7f55\u89c1\u4ee4\u724c\u901a\u5e38\u5bfc\u81f4\u66f4\u5927\u7684\u5d4c\u5165\u504f\u79fb\uff0c\u4e14\u8f93\u5165\u4fe1\u606f\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u9010\u6e10\u6df7\u5408\uff0c\u9a8c\u8bc1\u4e86\u6d45\u5c42\u53ef\u4f5c\u4e3a\u89e3\u91ca\u6a21\u578b\u7684\u4ee3\u7406\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4ee4\u724c\u6270\u52a8\u4e0e\u5d4c\u5165\u7a7a\u95f4\u504f\u79fb\u7684\u7ed3\u5408\u662f\u6a21\u578b\u89e3\u91ca\u6027\u7684\u6709\u529b\u5de5\u5177\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6d45\u5c42\u7f51\u7edc\u5728\u89e3\u91ca\u4e2d\u7684\u91cd\u8981\u6027\u3002", "paper_title_zh": "\u901a\u8fc7\u6700\u5c0f\u4ee4\u724c\u6270\u52a8\u63a2\u7a76Transformer\u7684\u5d4c\u5165\u7a7a\u95f4", "abstract_zh": "\u7406\u89e3Transformer\u6a21\u578b\u4e2d\u4fe1\u606f\u7684\u4f20\u64ad\u662f\u89e3\u91ca\u6027\u7684\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u901a\u8fc7\u7814\u7a76\u6700\u5c0f\u4ee4\u724c\u6270\u52a8\u5bf9\u5d4c\u5165\u7a7a\u95f4\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u54ea\u4e9b\u4ee4\u724c\u66f4\u5bb9\u6613\u5bfc\u81f4\u5fae\u5c0f\u504f\u79fb\uff0c\u53d1\u73b0\u7f55\u89c1\u4ee4\u724c\u901a\u5e38\u5f15\u53d1\u66f4\u5927\u7684\u504f\u79fb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6270\u52a8\u5728\u4e0d\u540c\u5c42\u95f4\u7684\u4f20\u64ad\uff0c\u8868\u660e\u8f93\u5165\u4fe1\u606f\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u9010\u6e10\u6df7\u5408\u3002\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6d45\u5c42\u7f51\u7edc\u53ef\u4f5c\u4e3a\u6a21\u578b\u89e3\u91ca\u7684\u4ee3\u7406\u8fd9\u4e00\u5e38\u89c1\u5047\u8bbe\u3002\u603b\u4f53\u800c\u8a00\uff0c\u672c\u6587\u63d0\u51fa\u4ee4\u724c\u6270\u52a8\u4e0e\u5d4c\u5165\u7a7a\u95f4\u504f\u79fb\u7684\u7ed3\u5408\u662f\u6a21\u578b\u89e3\u91ca\u6027\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2506.17636", "pdf": "https://arxiv.org/pdf/2506.17636", "abs": "https://arxiv.org/abs/2506.17636", "authors": ["Shihan Chen", "Zhaojin Li", "Zeyu Chen", "Qingsong Yan", "Gaoyang Shen", "Ran Duan"], "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": "IROS 2025", "summary": "Recent developments in 3D Gaussian Splatting have made significant advances\nin surface reconstruction. However, scaling these methods to large-scale scenes\nremains challenging due to high computational demands and the complex dynamic\nappearances typical of outdoor environments. These challenges hinder the\napplication in aerial surveying and autonomous driving. This paper proposes a\nnovel solution to reconstruct large-scale surfaces with fine details,\nsupervised by full-sized images. Firstly, we introduce a coarse-to-fine\nstrategy to reconstruct a coarse model efficiently, followed by adaptive scene\npartitioning and sub-scene refining from image segments. Additionally, we\nintegrate a decoupling appearance model to capture global appearance variations\nand a transient mask model to mitigate interference from moving objects.\nFinally, we expand the multi-view constraint and introduce a single-view\nregularization for texture-less areas. Our experiments were conducted on the\npublicly available dataset GauU-Scene V2, which was captured using unmanned\naerial vehicles. To the best of our knowledge, our method outperforms existing\nNeRF-based and Gaussian-based methods, achieving high-fidelity visual results\nand accurate surface from full-size image optimization. Open-source code will\nbe available on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5927\u89c4\u6a21\u573a\u666f\u7cbe\u7ec6\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7b56\u7565\u3001\u81ea\u9002\u5e94\u573a\u666f\u5206\u533a\u548c\u5916\u89c2\u89e3\u8026\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba13D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u8868\u9762\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u573a\u666f\uff08\u5982\u822a\u6d4b\u548c\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u4ecd\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u52a8\u6001\u5916\u89c2\u5e72\u6270\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u8868\u9762\u91cd\u5efa\u3002", "method": "1. \u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\uff0c\u5148\u5feb\u901f\u6784\u5efa\u7c97\u7cd9\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u81ea\u9002\u5e94\u573a\u666f\u5206\u533a\u548c\u5b50\u573a\u666f\u7ec6\u5316\u4f18\u5316\u7ec6\u8282\u30022. \u5f15\u5165\u89e3\u8026\u5916\u89c2\u6a21\u578b\u6355\u6349\u5168\u5c40\u5916\u89c2\u53d8\u5316\uff0c\u5e76\u4f7f\u7528\u77ac\u6001\u63a9\u6a21\u6a21\u578b\u51cf\u5c11\u79fb\u52a8\u7269\u4f53\u7684\u5e72\u6270\u30023. \u6269\u5c55\u591a\u89c6\u89d2\u7ea6\u675f\uff0c\u5e76\u4e3a\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u5f15\u5165\u5355\u89c6\u89d2\u6b63\u5219\u5316\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6GauU-Scene V2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8868\u9762\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eNeRF\u548c\u9ad8\u65af\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7b56\u7565\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u8868\u9762\u91cd\u5efa\uff0c\u4e3a\u822a\u6d4b\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5927\u89c4\u6a21\u573a\u666f\u7cbe\u7ec6\u8868\u9762\u91cd\u5efa", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u8868\u9762\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8ba1\u7b97\u9700\u6c42\u9ad8\u548c\u6237\u5916\u73af\u5883\u590d\u6742\u7684\u52a8\u6001\u5916\u89c2\uff0c\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\u4ecd\u5177\u6311\u6218\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u822a\u6d4b\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c3a\u5bf8\u56fe\u50cf\u7684\u76d1\u7763\u91cd\u5efa\u5177\u6709\u7cbe\u7ec6\u7ec6\u8282\u7684\u5927\u89c4\u6a21\u8868\u9762\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7c97\u5230\u7ec6\u7b56\u7565\uff0c\u9ad8\u6548\u91cd\u5efa\u7c97\u7cd9\u6a21\u578b\uff0c\u968f\u540e\u901a\u8fc7\u81ea\u9002\u5e94\u573a\u666f\u5206\u533a\u548c\u56fe\u50cf\u7247\u6bb5\u7ec6\u5316\u5b50\u573a\u666f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u4e00\u4e2a\u89e3\u8026\u5916\u89c2\u6a21\u578b\u4ee5\u6355\u6349\u5168\u5c40\u5916\u89c2\u53d8\u5316\uff0c\u4ee5\u53ca\u4e00\u4e2a\u77ac\u6001\u63a9\u6a21\u6a21\u578b\u4ee5\u51cf\u5c11\u79fb\u52a8\u7269\u4f53\u7684\u5e72\u6270\u3002\u6700\u540e\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u591a\u89c6\u89d2\u7ea6\u675f\uff0c\u5e76\u4e3a\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u5f15\u5165\u4e86\u5355\u89c6\u89d2\u6b63\u5219\u5316\u3002\u5b9e\u9a8c\u5728\u516c\u5f00\u6570\u636e\u96c6GauU-Scene V2\u4e0a\u8fdb\u884c\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u65e0\u4eba\u673a\u62cd\u6444\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8868\u9762\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eNeRF\u548c\u9ad8\u65af\u7684\u65b9\u6cd5\u3002\u5f00\u6e90\u4ee3\u7801\u5c06\u5728GitHub\u4e0a\u63d0\u4f9b\u3002"}}
{"id": "2506.18016", "pdf": "https://arxiv.org/pdf/2506.18016", "abs": "https://arxiv.org/abs/2506.18016", "authors": ["Yongxin Shao", "Binrui Wang", "Aihong Tan"], "title": "ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "LiDAR SLAM has demonstrated significant application value in various fields,\nincluding mobile robot navigation and high-precision map construction. However,\nexisting methods often need to make a trade-off between positioning accuracy\nand system robustness when faced with dynamic object interference, point cloud\nnoise, and unstructured environments. To address this challenge, we propose an\nadaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference\nin both aspects. We design the Dynamic Segmentation Head to predict the\ncategory of feature points belonging to dynamic points, to eliminate dynamic\nfeature points; design the Global Importance Scoring Head to adaptively select\nfeature points with higher contribution and features while suppressing noise\ninterference; and construct the Cross Layer Intra-Graph Convolution Module\n(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the\ndiscriminative ability of overlapping features. Finally, to further validate\nthe effectiveness of our method, we tested it on several publicly available\ndatasets and achieved outstanding results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u63cf\u8ff0\u7b26\u7684\u81ea\u9002\u5e94\u566a\u58f0\u70b9\u8fc7\u6ee4\u7b56\u7565ADA-DPM\uff0c\u7528\u4e8e\u63d0\u5347SLAM\u7cfb\u7edf\u5728\u52a8\u6001\u7269\u4f53\u5e72\u6270\u548c\u566a\u58f0\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u52a8\u6001\u5206\u5272\u5934\u548c\u5168\u5c40\u91cd\u8981\u6027\u8bc4\u5206\u5934\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u90bb\u57df\u7ed3\u6784\u7684\u56fe\u5377\u79ef\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u70b9\u7684\u5224\u522b\u80fd\u529b\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u5e72\u6270\u3001\u70b9\u4e91\u566a\u58f0\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\uff0c\u5f80\u5f80\u9700\u8981\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u4e4b\u95f4\u505a\u51fa\u59a5\u534f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u566a\u58f0\u8fc7\u6ee4\u7b56\u7565\uff0c\u65e8\u5728\u540c\u65f6\u63d0\u5347\u8fd9\u4e24\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u52a8\u6001\u5206\u5272\u5934\uff08Dynamic Segmentation Head\uff09\u7528\u4e8e\u9884\u6d4b\u52a8\u6001\u7279\u5f81\u70b9\u7c7b\u522b\u4ee5\u6d88\u9664\u52a8\u6001\u70b9\uff1b\u5168\u5c40\u91cd\u8981\u6027\u8bc4\u5206\u5934\uff08Global Importance Scoring Head\uff09\u7528\u4e8e\u81ea\u9002\u5e94\u9009\u62e9\u8d21\u732e\u66f4\u9ad8\u7684\u7279\u5f81\u70b9\u5e76\u6291\u5236\u566a\u58f0\u5e72\u6270\uff1b\u6784\u5efa\u4e86\u8de8\u5c42\u56fe\u5185\u5377\u79ef\u6a21\u5757\uff08GLI-GCN\uff09\u4ee5\u878d\u5408\u591a\u5c3a\u5ea6\u90bb\u57df\u7ed3\u6784\uff0c\u589e\u5f3a\u91cd\u53e0\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cADA-DPM\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ADA-DPM\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u8fc7\u6ee4\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86SLAM\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u548c\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002", "paper_title_zh": "ADA-DPM\uff1a\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u63cf\u8ff0\u7b26\u7684\u81ea\u9002\u5e94\u566a\u58f0\u70b9\u8fc7\u6ee4\u7b56\u7565\u7528\u4e8eSLAM", "abstract_zh": "\u6fc0\u5149\u96f7\u8fbeSLAM\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u548c\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6784\u5efa\u7b49\u9886\u57df\u5c55\u73b0\u4e86\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u52a8\u6001\u7269\u4f53\u5e72\u6270\u3001\u70b9\u4e91\u566a\u58f0\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u4e4b\u95f4\u505a\u51fa\u59a5\u534f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u566a\u58f0\u8fc7\u6ee4SLAM\u7b56\u7565\u2014\u2014ADA-DPM\uff0c\u5728\u4e24\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u52a8\u6001\u5206\u5272\u5934\u4ee5\u9884\u6d4b\u52a8\u6001\u7279\u5f81\u70b9\u7c7b\u522b\uff0c\u6d88\u9664\u52a8\u6001\u7279\u5f81\u70b9\uff1b\u8bbe\u8ba1\u4e86\u5168\u5c40\u91cd\u8981\u6027\u8bc4\u5206\u5934\u4ee5\u81ea\u9002\u5e94\u9009\u62e9\u8d21\u732e\u66f4\u9ad8\u7684\u7279\u5f81\u70b9\u5e76\u6291\u5236\u566a\u58f0\u5e72\u6270\uff1b\u6784\u5efa\u4e86\u8de8\u5c42\u56fe\u5185\u5377\u79ef\u6a21\u5757\uff08GLI-GCN\uff09\u4ee5\u878d\u5408\u591a\u5c3a\u5ea6\u90bb\u57df\u7ed3\u6784\uff0c\u4ece\u800c\u589e\u5f3a\u91cd\u53e0\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002\u6700\u540e\uff0c\u4e3a\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.17747", "pdf": "https://arxiv.org/pdf/2506.17747", "abs": "https://arxiv.org/abs/2506.17747", "authors": ["Abdulrahman Al-Fakih", "Ardiansyah Koeshidayatullah", "Nabil A. Saraih", "Tapan Mukerji", "Rayan Kanfar", "Abdulmohsen Alali", "SanLinn I. Kaka"], "title": "Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation", "categories": ["physics.geo-ph", "cs.CE", "cs.CV", "cs.LG", "cs.NE"], "comment": "34 pages, 13 figures", "summary": "Accurate geological modeling is critical for reservoir characterization, yet\ntraditional methods struggle with complex subsurface heterogeneity, and they\nhave problems with conditioning to observed data. This study introduces\nPix2Geomodel, a novel conditional generative adversarial network (cGAN)\nframework based on Pix2Pix, designed to predict reservoir properties (facies,\nporosity, permeability, and water saturation) from the Rotliegend reservoir of\nthe Groningen gas field. Utilizing a 7.6 million-cell dataset from the\nNederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology\nincluded data preprocessing, augmentation to generate 2,350 images per\nproperty, and training with a U-Net generator and PatchGAN discriminator over\n19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection\nover union (mIoU), frequency weighted intersection over union (FWIoU), and\nvisualizations assessed performance in masked property prediction and\nproperty-to-property translation tasks. Results demonstrated high accuracy for\nfacies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with\nmoderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,\nFWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA\n0.98, FWIoU 0.97). The framework captured spatial variability and geological\nrealism, as validated by variogram analysis, and calculated the training loss\ncurves for the generator and discriminator for each property. Compared to\ntraditional methods, Pix2Geomodel offers enhanced fidelity in direct property\nmapping. Limitations include challenges with microstructural variability and 2D\nconstraints, suggesting future integration of multi-modal data and 3D modeling\n(Pix2Geomodel v2.0). This study advances the application of generative AI in\ngeoscience, supporting improved reservoir management and open science\ninitiatives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPix2Geomodel\uff0c\u4e00\u79cd\u57fa\u4e8ePix2Pix\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8377\u5170Groningen\u6c14\u7530\u7684Rotliegend\u50a8\u5c42\u9884\u6d4b\u50a8\u5c42\u5c5e\u6027\uff08\u5ca9\u76f8\u3001\u5b54\u9699\u5ea6\u3001\u6e17\u900f\u7387\u548c\u542b\u6c34\u9971\u548c\u5ea6\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u9884\u5904\u7406\u548c\u589e\u5f3a\u540e\uff0c\u901a\u8fc7U-Net\u751f\u6210\u5668\u548cPatchGAN\u5224\u522b\u5668\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u5730\u8d28\u771f\u5b9e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5730\u8d28\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u5730\u4e0b\u5f02\u8d28\u6027\u548c\u89c2\u6d4b\u6570\u636e\u6761\u4ef6\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u50a8\u5c42\u5c5e\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528Pix2Pix\u6846\u67b6\u7684cGAN\u6a21\u578b\uff0c\u5229\u7528760\u4e07\u5355\u5143\u6570\u636e\u96c6\u8fdb\u884c\u9884\u5904\u7406\u548c\u589e\u5f3a\uff0c\u751f\u6210\u6bcf\u79cd\u5c5e\u6027\u76842350\u5f20\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7U-Net\u751f\u6210\u5668\u548cPatchGAN\u5224\u522b\u5668\u8fdb\u884c\u8bad\u7ec3\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u50cf\u7d20\u51c6\u786e\u7387\uff08PA\uff09\u3001\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u548c\u9891\u7387\u52a0\u6743\u4ea4\u5e76\u6bd4\uff08FWIoU\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5ca9\u76f8\uff08PA 0.88\uff0cFWIoU 0.85\uff09\u548c\u542b\u6c34\u9971\u548c\u5ea6\uff08PA 0.96\uff0cFWIoU 0.95\uff09\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff0c\u5b54\u9699\u5ea6\uff08PA 0.70\uff0cFWIoU 0.55\uff09\u548c\u6e17\u900f\u7387\uff08PA 0.74\uff0cFWIoU 0.60\uff09\u8868\u73b0\u4e2d\u7b49\uff0c\u5c5e\u6027\u95f4\u8f6c\u6362\u6027\u80fd\u7a33\u5065\uff08\u5982\u5ca9\u76f8\u5230\u5ca9\u76f8PA 0.98\uff0cFWIoU 0.97\uff09\u3002", "conclusion": "Pix2Geomodel\u5728\u50a8\u5c42\u5c5e\u6027\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5730\u8d28\u771f\u5b9e\u6027\u548c\u7a7a\u95f4\u53d8\u5f02\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u672a\u6765\u9700\u89e3\u51b3\u5fae\u89c2\u7ed3\u6784\u53d8\u5f02\u6027\u548c2D\u9650\u5236\uff0c\u5e76\u63a2\u7d22\u591a\u6a21\u6001\u6570\u636e\u548c3D\u5efa\u6a21\uff08Pix2Geomodel v2.0\uff09\u3002", "paper_title_zh": "Pix2Geomodel\uff1a\u4e0b\u4e00\u4ee3\u50a8\u5c42\u5730\u8d28\u5efa\u6a21\u4e0e\u5c5e\u6027\u95f4\u8f6c\u6362", "abstract_zh": "\u7cbe\u786e\u7684\u5730\u8d28\u5efa\u6a21\u5bf9\u50a8\u5c42\u8868\u5f81\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u5730\u4e0b\u5f02\u8d28\u6027\u548c\u89c2\u6d4b\u6570\u636e\u6761\u4ef6\u9650\u5236\u3002\u672c\u7814\u7a76\u63d0\u51faPix2Geomodel\uff0c\u4e00\u79cd\u57fa\u4e8ePix2Pix\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8377\u5170Groningen\u6c14\u7530\u7684Rotliegend\u50a8\u5c42\u9884\u6d4b\u50a8\u5c42\u5c5e\u6027\uff08\u5ca9\u76f8\u3001\u5b54\u9699\u5ea6\u3001\u6e17\u900f\u7387\u548c\u542b\u6c34\u9971\u548c\u5ea6\uff09\u3002\u901a\u8fc7EPOS-NL\u83b7\u53d6\u7684760\u4e07\u5355\u5143\u6570\u636e\u96c6\uff0c\u65b9\u6cd5\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u589e\u5f3a\uff08\u6bcf\u79cd\u5c5e\u6027\u751f\u62102350\u5f20\u56fe\u50cf\uff09\uff0c\u4ee5\u53ca\u4f7f\u7528U-Net\u751f\u6210\u5668\u548cPatchGAN\u5224\u522b\u5668\u8fdb\u884c19000\u6b65\u8bad\u7ec3\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u50cf\u7d20\u51c6\u786e\u7387\uff08PA\uff09\u3001\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u3001\u9891\u7387\u52a0\u6743\u4ea4\u5e76\u6bd4\uff08FWIoU\uff09\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u8bc4\u4f30\u63a9\u6a21\u5c5e\u6027\u9884\u6d4b\u548c\u5c5e\u6027\u95f4\u8f6c\u6362\u4efb\u52a1\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\u5ca9\u76f8\uff08PA 0.88\uff0cFWIoU 0.85\uff09\u548c\u542b\u6c34\u9971\u548c\u5ea6\uff08PA 0.96\uff0cFWIoU 0.95\uff09\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff0c\u5b54\u9699\u5ea6\uff08PA 0.70\uff0cFWIoU 0.55\uff09\u548c\u6e17\u900f\u7387\uff08PA 0.74\uff0cFWIoU 0.60\uff09\u8868\u73b0\u4e2d\u7b49\uff0c\u5c5e\u6027\u95f4\u8f6c\u6362\u6027\u80fd\u7a33\u5065\uff08\u5982\u5ca9\u76f8\u5230\u5ca9\u76f8PA 0.98\uff0cFWIoU 0.97\uff09\u3002\u8be5\u6846\u67b6\u6355\u6349\u4e86\u7a7a\u95f4\u53d8\u5f02\u6027\u548c\u5730\u8d28\u771f\u5b9e\u6027\uff0c\u5e76\u901a\u8fc7\u53d8\u5dee\u51fd\u6570\u5206\u6790\u9a8c\u8bc1\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cPix2Geomodel\u5728\u76f4\u63a5\u5c5e\u6027\u6620\u5c04\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3002\u5c40\u9650\u6027\u5305\u62ec\u5fae\u89c2\u7ed3\u6784\u53d8\u5f02\u6027\u548c2D\u9650\u5236\uff0c\u672a\u6765\u9700\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u548c3D\u5efa\u6a21\uff08Pix2Geomodel v2.0\uff09\u3002\u672c\u7814\u7a76\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u5730\u7403\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u652f\u6301\u6539\u8fdb\u50a8\u5c42\u7ba1\u7406\u548c\u5f00\u653e\u79d1\u5b66\u5021\u8bae\u3002"}}
{"id": "2506.18017", "pdf": "https://arxiv.org/pdf/2506.18017", "abs": "https://arxiv.org/abs/2506.18017", "authors": ["Yang Li", "Victor Cheung", "Xinhai Liu", "Yuguang Chen", "Zhongjin Luo", "Biwen Lei", "Haohan Weng", "Zibo Zhao", "Jingwei Huang", "Zhuo Chen", "Chunchao Guo"], "title": "Auto-Regressive Surface Cutting", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Tech. report. https://victorcheung12.github.io/seamgpt", "summary": "Surface cutting is a fundamental task in computer graphics, with applications\nin UV parameterization, texture mapping, and mesh decomposition. However,\nexisting methods often produce technically valid but overly fragmented atlases\nthat lack semantic coherence. We introduce SeamGPT, an auto-regressive model\nthat generates cutting seams by mimicking professional workflows. Our key\ntechnical innovation lies in formulating surface cutting as a next token\nprediction task: sample point clouds on mesh vertices and edges, encode them as\nshape conditions, and employ a GPT-style transformer to sequentially predict\nseam segments with quantized 3D coordinates. Our approach achieves exceptional\nperformance on UV unwrapping benchmarks containing both manifold and\nnon-manifold meshes, including artist-created, and 3D-scanned models. In\naddition, it enhances existing 3D segmentation tools by providing clean\nboundaries for part decomposition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeamGPT\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u8868\u9762\u5207\u5272\u7f1d\uff0c\u6a21\u4eff\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86UV\u5c55\u5f00\u548c\u7f51\u683c\u5206\u89e3\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u9762\u5207\u5272\u65b9\u6cd5\u867d\u7136\u6280\u672f\u6709\u6548\uff0c\u4f46\u751f\u6210\u7684\u56fe\u8c31\u5f80\u5f80\u8fc7\u4e8e\u788e\u7247\u5316\u4e14\u7f3a\u4e4f\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u8868\u9762\u5207\u5272\u4efb\u52a1\u8f6c\u5316\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\uff1a\u5728\u7f51\u683c\u9876\u70b9\u548c\u8fb9\u7f18\u4e0a\u91c7\u6837\u70b9\u4e91\uff0c\u5c06\u5176\u7f16\u7801\u4e3a\u5f62\u72b6\u6761\u4ef6\uff0c\u5e76\u4f7f\u7528GPT\u98ce\u683c\u7684\u53d8\u6362\u5668\u6a21\u578b\u9010\u6b65\u9884\u6d4b\u5e26\u6709\u91cf\u53163D\u5750\u6807\u7684\u5207\u5272\u7f1d\u6bb5\u3002", "result": "\u5728\u5305\u542b\u6d41\u5f62\u548c\u975e\u6d41\u5f62\u7f51\u683c\u7684UV\u5c55\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u827a\u672f\u5bb6\u521b\u4f5c\u548c3D\u626b\u63cf\u6a21\u578b\u3002\u540c\u65f6\uff0c\u4e3a\u73b0\u67093D\u5206\u5272\u5de5\u5177\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8fb9\u754c\u652f\u6301\u3002", "conclusion": "SeamGPT\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u8868\u9762\u5207\u5272\u4e2d\u7684\u788e\u7247\u5316\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u81ea\u56de\u5f52\u8868\u9762\u5207\u5272", "abstract_zh": "\u8868\u9762\u5207\u5272\u662f\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u4e00\u9879\u57fa\u7840\u4efb\u52a1\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8eUV\u53c2\u6570\u5316\u3001\u7eb9\u7406\u6620\u5c04\u548c\u7f51\u683c\u5206\u89e3\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u751f\u6210\u6280\u672f\u4e0a\u6709\u6548\u4f46\u8fc7\u4e8e\u788e\u7247\u5316\u7684\u56fe\u8c31\uff0c\u7f3a\u4e4f\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86SeamGPT\uff0c\u4e00\u79cd\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u4eff\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u751f\u6210\u5207\u5272\u7f1d\u3002\u6211\u4eec\u7684\u5173\u952e\u6280\u672f\u521b\u65b0\u5728\u4e8e\u5c06\u8868\u9762\u5207\u5272\u4efb\u52a1\u8f6c\u5316\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\uff1a\u5728\u7f51\u683c\u9876\u70b9\u548c\u8fb9\u7f18\u4e0a\u91c7\u6837\u70b9\u4e91\uff0c\u5c06\u5176\u7f16\u7801\u4e3a\u5f62\u72b6\u6761\u4ef6\uff0c\u5e76\u91c7\u7528GPT\u98ce\u683c\u7684\u53d8\u6362\u5668\u9010\u6b65\u9884\u6d4b\u5e26\u6709\u91cf\u53163D\u5750\u6807\u7684\u5207\u5272\u7f1d\u6bb5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5305\u542b\u6d41\u5f62\u548c\u975e\u6d41\u5f62\u7f51\u683c\u7684UV\u5c55\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u827a\u672f\u5bb6\u521b\u4f5c\u548c3D\u626b\u63cf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3a\u90e8\u4ef6\u5206\u89e3\u63d0\u4f9b\u6e05\u6670\u8fb9\u754c\uff0c\u589e\u5f3a\u4e86\u73b0\u67093D\u5206\u5272\u5de5\u5177\u7684\u529f\u80fd\u3002"}}
{"id": "2506.17770", "pdf": "https://arxiv.org/pdf/2506.17770", "abs": "https://arxiv.org/abs/2506.17770", "authors": ["Tomas Akenine-M\u00f6ller", "Pontus Ebelin", "Matt Pharr", "Bartlomiej Wronski"], "title": "Collaborative Texture Filtering", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ACM/EG Symposium on High Performance Graphics (HPG), 2025", "summary": "Recent advances in texture compression provide major improvements in\ncompression ratios, but cannot use the GPU's texture units for decompression\nand filtering. This has led to the development of stochastic texture filtering\n(STF) techniques to avoid the high cost of multiple texel evaluations with such\nformats. Unfortunately, those methods can give undesirable visual appearance\nchanges under magnification and may contain visible noise and flicker despite\nthe use of spatiotemporal denoisers. Recent work substantially improves the\nquality of magnification filtering with STF by sharing decoded texel values\nbetween nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,\nthis sharing can be performed inside actively executing shaders without memory\ntraffic overhead. We take this idea further and present novel algorithms that\nuse wave communication between lanes to avoid repeated texel decompression\nprior to filtering. By distributing unique work across lanes, we can achieve\nzero-error filtering using <=1 texel evaluations per pixel given a sufficiently\nlarge magnification factor. For the remaining cases, we propose novel filtering\nfallback methods that also achieve higher quality than prior approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u6ce2\u901a\u4fe1\u7684\u534f\u4f5c\u7eb9\u7406\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u89e3\u538b\u7f29\u7684\u7eb9\u7406\u503c\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7eb9\u7406\u653e\u5927\u548c\u964d\u566a\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u7eb9\u7406\u8fc7\u6ee4\uff08STF\uff09\u6280\u672f\u5728\u7eb9\u7406\u653e\u5927\u65f6\u53ef\u80fd\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\u548c\u566a\u58f0\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u65f6\u7a7a\u53bb\u566a\u5668\uff0c\u6548\u679c\u4ecd\u4e0d\u7406\u60f3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u8fc7\u6ee4\u65b9\u6cd5\u3002", "method": "\u5229\u7528GPU\u6ce2\u901a\u4fe1\u7279\u6027\uff0c\u5728\u7740\u8272\u5668\u6267\u884c\u8fc7\u7a0b\u4e2d\u5171\u4eab\u89e3\u538b\u7f29\u7684\u7eb9\u7406\u503c\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u3002\u5bf9\u4e8e\u9ad8\u653e\u5927\u500d\u6570\uff0c\u5b9e\u73b0\u4e86\u96f6\u8bef\u5dee\u8fc7\u6ee4\uff1b\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u9ad8\u8d28\u91cf\u8fc7\u6ee4\u56de\u9000\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9ad8\u653e\u5927\u500d\u6570\u4e0b\u5b9e\u73b0\u4e86\u96f6\u8bef\u5dee\u8fc7\u6ee4\uff0c\u4e14\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u6ee4\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7GPU\u6ce2\u901a\u4fe1\u534f\u4f5c\u8fc7\u6ee4\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eb9\u7406\u8fc7\u6ee4\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "paper_title_zh": "\u534f\u4f5c\u7eb9\u7406\u8fc7\u6ee4", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u7eb9\u7406\u538b\u7f29\u6280\u672f\u7684\u8fdb\u6b65\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u6bd4\uff0c\u4f46\u65e0\u6cd5\u5229\u7528GPU\u7684\u7eb9\u7406\u5355\u5143\u8fdb\u884c\u89e3\u538b\u7f29\u548c\u8fc7\u6ee4\u3002\u8fd9\u4fc3\u4f7f\u4e86\u968f\u673a\u7eb9\u7406\u8fc7\u6ee4\uff08STF\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ee5\u907f\u514d\u6b64\u7c7b\u683c\u5f0f\u4e0b\u591a\u6b21\u7eb9\u7406\u8bc4\u4f30\u7684\u9ad8\u6210\u672c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u653e\u5927\u65f6\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7406\u60f3\u7684\u89c6\u89c9\u53d8\u5316\uff0c\u5e76\u53ef\u80fd\u5305\u542b\u53ef\u89c1\u7684\u566a\u58f0\u548c\u95ea\u70c1\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u65f6\u7a7a\u53bb\u566a\u5668\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u8fc7\u5171\u4eab\u9644\u8fd1\u50cf\u7d20\u7684\u89e3\u538b\u7f29\u7eb9\u7406\u503c\uff08Wronski 2025\uff09\uff0c\u663e\u8457\u6539\u5584\u4e86STF\u7684\u653e\u5927\u8fc7\u6ee4\u8d28\u91cf\u3002\u5229\u7528GPU\u6ce2\u901a\u4fe1\u7279\u6027\uff0c\u8fd9\u79cd\u5171\u4eab\u53ef\u4ee5\u5728\u6d3b\u8dc3\u6267\u884c\u7684\u7740\u8272\u5668\u5185\u90e8\u5b8c\u6210\uff0c\u65e0\u9700\u989d\u5916\u7684\u5185\u5b58\u5f00\u9500\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u8fd9\u4e00\u601d\u8def\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u6ce2\u901a\u4fe1\u907f\u514d\u8fc7\u6ee4\u524d\u7684\u91cd\u590d\u7eb9\u7406\u89e3\u538b\u7f29\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u901a\u9053\u95f4\u5206\u914d\u552f\u4e00\u4efb\u52a1\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u8db3\u591f\u5927\u7684\u653e\u5927\u500d\u6570\u4e0b\uff0c\u5b9e\u73b0\u6bcf\u50cf\u7d20\u22641\u6b21\u7eb9\u7406\u8bc4\u4f30\u7684\u96f6\u8bef\u5dee\u8fc7\u6ee4\u3002\u5bf9\u4e8e\u5176\u4ed6\u60c5\u51b5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u9ad8\u8d28\u91cf\u8fc7\u6ee4\u56de\u9000\u65b9\u6cd5\uff0c\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.17872", "pdf": "https://arxiv.org/pdf/2506.17872", "abs": "https://arxiv.org/abs/2506.17872", "authors": ["Sree Bhargavi Balija", "Amitash Nanda", "Debashis Sahoo"], "title": "Decoding Federated Learning: The FedNAM+ Conformal Revolution", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated learning has significantly advanced distributed training of machine\nlearning models across decentralized data sources. However, existing frameworks\noften lack comprehensive solutions that combine uncertainty quantification,\ninterpretability, and robustness. To address this, we propose FedNAM+, a\nfederated learning framework that integrates Neural Additive Models (NAMs) with\na novel conformal prediction method to enable interpretable and reliable\nuncertainty estimation. Our method introduces a dynamic level adjustment\ntechnique that utilizes gradient-based sensitivity maps to identify key input\nfeatures influencing predictions. This facilitates both interpretability and\npixel-wise uncertainty estimates. Unlike traditional interpretability methods\nsuch as LIME and SHAP, which do not provide confidence intervals, FedNAM+\noffers visual insights into prediction reliability. We validate our approach\nthrough experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high\nprediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with\ntransparent uncertainty measures. Visual analysis highlights variable\nuncertainty intervals, revealing low-confidence regions where model performance\ncan be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+\ndelivers efficient and global uncertainty estimates with reduced computational\noverhead, making it particularly suitable for federated learning scenarios.\nOverall, FedNAM+ provides a robust, interpretable, and computationally\nefficient framework that enhances trust and transparency in decentralized\npredictive modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFedNAM+\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\u4e0e\u65b0\u578b\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u548c\u900f\u660e\u6027\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7f3a\u4e4f\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0cFedNAM+\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "FedNAM+\u901a\u8fc7\u52a8\u6001\u5c42\u7ea7\u8c03\u6574\u6280\u672f\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u654f\u611f\u5ea6\u6620\u5c04\uff0c\u8bc6\u522b\u5173\u952e\u8f93\u5165\u7279\u5f81\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u53ef\u89c6\u5316\u9884\u6d4b\u53ef\u9760\u6027\u3002", "result": "\u5728CT\u626b\u63cf\u3001MNIST\u548cCIFAR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedNAM+\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff08\u5982MNIST\u4ec5\u635f\u59310.1%\uff09\uff0c\u4e14\u63d0\u4f9b\u900f\u660e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e\u8499\u7279\u5361\u6d1bDropout\u3002", "conclusion": "FedNAM+\u4e3a\u53bb\u4e2d\u5fc3\u5316\u9884\u6d4b\u5efa\u6a21\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u3002", "paper_title_zh": "\u89e3\u7801\u8054\u90a6\u5b66\u4e60\uff1aFedNAM+\u5171\u5f62\u9769\u547d", "abstract_zh": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u901a\u5e38\u7f3a\u4e4f\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faFedNAM+\uff0c\u4e00\u79cd\u5c06\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff08NAMs\uff09\u4e0e\u65b0\u578b\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u7ed3\u5408\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u5c42\u7ea7\u8c03\u6574\u6280\u672f\uff0c\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u654f\u611f\u5ea6\u6620\u5c04\u8bc6\u522b\u5f71\u54cd\u9884\u6d4b\u7684\u5173\u952e\u8f93\u5165\u7279\u5f81\uff0c\u4ece\u800c\u540c\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u548c\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u4e0eLIME\u548cSHAP\u7b49\u4f20\u7edf\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u4e0d\u540c\uff0cFedNAM+\u63d0\u4f9b\u4e86\u9884\u6d4b\u53ef\u9760\u6027\u7684\u53ef\u89c6\u5316\u6d1e\u5bdf\u3002\u6211\u4eec\u5728CT\u626b\u63cf\u3001MNIST\u548cCIFAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff08\u5982MNIST\u4ec5\u635f\u59310.1%\uff09\u548c\u900f\u660e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u3002\u53ef\u89c6\u5316\u5206\u6790\u63ed\u793a\u4e86\u53ef\u53d8\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\uff0c\u6807\u51fa\u4e86\u53ef\u901a\u8fc7\u8865\u5145\u6570\u636e\u6539\u8fdb\u7684\u4f4e\u7f6e\u4fe1\u533a\u57df\u3002\u4e0e\u8499\u7279\u5361\u6d1bDropout\u76f8\u6bd4\uff0cFedNAM+\u4ee5\u66f4\u4f4e\u8ba1\u7b97\u5f00\u9500\u63d0\u4f9b\u9ad8\u6548\u4e14\u5168\u5c40\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5c24\u5176\u9002\u5408\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002\u603b\u4f53\u800c\u8a00\uff0cFedNAM+\u4e3a\u53bb\u4e2d\u5fc3\u5316\u9884\u6d4b\u5efa\u6a21\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2506.17874", "pdf": "https://arxiv.org/pdf/2506.17874", "abs": "https://arxiv.org/abs/2506.17874", "authors": ["Jiaming Hu", "Debarghya Mukherjee", "Ioannis Ch. Paschalidis"], "title": "DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": "26 pages,3 figures", "summary": "In many real-world applications, ensuring the robustness and stability of\ndeep neural networks (DNNs) is crucial, particularly for image classification\ntasks that encounter various input perturbations. While data augmentation\ntechniques have been widely adopted to enhance the resilience of a trained\nmodel against such perturbations, there remains significant room for\nimprovement in robustness against corrupted data and adversarial attacks\nsimultaneously. To address this challenge, we introduce DRO-Augment, a novel\nframework that integrates Wasserstein Distributionally Robust Optimization\n(W-DRO) with various data augmentation strategies to improve the robustness of\nthe models significantly across a broad spectrum of corruptions. Our method\noutperforms existing augmentation methods under severe data perturbations and\nadversarial attack scenarios while maintaining the accuracy on the clean\ndatasets on a range of benchmark datasets, including but not limited to\nCIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we\nestablish novel generalization error bounds for neural networks trained using a\ncomputationally efficient, variation-regularized loss function closely related\nto the W-DRO problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDRO-Augment\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08W-DRO\uff09\u4e0e\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u79cd\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u56fe\u50cf\u5206\u7c7b\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9700\u5e94\u5bf9\u591a\u79cd\u8f93\u5165\u6270\u52a8\uff0c\u73b0\u6709\u6570\u636e\u589e\u5f3a\u6280\u672f\u867d\u80fd\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u6570\u636e\u635f\u574f\u548c\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "DRO-Augment\u6846\u67b6\u5c06Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08W-DRO\uff09\u4e0e\u591a\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7ed3\u5408\uff0c\u901a\u8fc7\u8ba1\u7b97\u9ad8\u6548\u7684\u53d8\u5206\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRO-Augment\u5728CIFAR-10-C\u3001CIFAR-100-C\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e25\u91cd\u6570\u636e\u6270\u52a8\u548c\u5bf9\u6297\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DRO-Augment\u901a\u8fc7\u534f\u540c\u4f18\u5316W-DRO\u4e0e\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u7406\u8bba\u5206\u6790\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u3002", "paper_title_zh": "DRO-Augment\u6846\u67b6\uff1a\u901a\u8fc7Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u4e0e\u6570\u636e\u589e\u5f3a\u7684\u534f\u540c\u63d0\u5347\u9c81\u68d2\u6027", "abstract_zh": "\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u786e\u4fdd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u9762\u4e34\u591a\u79cd\u8f93\u5165\u6270\u52a8\u65f6\u3002\u5c3d\u7ba1\u6570\u636e\u589e\u5f3a\u6280\u672f\u88ab\u5e7f\u6cdb\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u5bf9\u8fd9\u4e9b\u6270\u52a8\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4f46\u5728\u540c\u65f6\u5e94\u5bf9\u6570\u636e\u635f\u574f\u548c\u5bf9\u6297\u653b\u51fb\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faDRO-Augment\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08W-DRO\uff09\u4e0e\u591a\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5e7f\u6cdb\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728CIFAR-10-C\u3001CIFAR-100-C\u3001MNIST\u548cFashion-MNIST\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u589e\u5f3a\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e25\u91cd\u6570\u636e\u6270\u52a8\u548c\u5bf9\u6297\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u3002\u5728\u7406\u8bba\u65b9\u9762\uff0c\u6211\u4eec\u4e3a\u4f7f\u7528\u4e0eW-DRO\u95ee\u9898\u5bc6\u5207\u76f8\u5173\u7684\u8ba1\u7b97\u9ad8\u6548\u3001\u53d8\u5206\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5efa\u7acb\u4e86\u65b0\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u3002"}}
{"id": "2506.18037", "pdf": "https://arxiv.org/pdf/2506.18037", "abs": "https://arxiv.org/abs/2506.18037", "authors": ["Seongwoo Lim", "Won Jo", "Joohyung Lee", "Jaesik Choi"], "title": "Pathwise Explanation of ReLU Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "In Proceedings of The 27th International Conference on Artificial\n  Intelligence and Statistics, PMLR 238:4645-4653, 2024", "summary": "Neural networks have demonstrated a wide range of successes, but their\n``black box\" nature raises concerns about transparency and reliability.\nPrevious research on ReLU networks has sought to unwrap these networks into\nlinear models based on activation states of all hidden units. In this paper, we\nintroduce a novel approach that considers subsets of the hidden units involved\nin the decision making path. This pathwise explanation provides a clearer and\nmore consistent understanding of the relationship between the input and the\ndecision-making process. Our method also offers flexibility in adjusting the\nrange of explanations within the input, i.e., from an overall attribution input\nto particular components within the input. Furthermore, it allows for the\ndecomposition of explanations for a given input for more detailed explanations.\nExperiments demonstrate that our method outperforms others both quantitatively\nand qualitatively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8def\u5f84\u5f0f\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u4e8e\u63ed\u793aReLU\u795e\u7ecf\u7f51\u7edc\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5173\u6ce8\u9690\u85cf\u5355\u5143\u7684\u5b50\u96c6\u800c\u975e\u5168\u90e8\uff0c\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u548c\u4e00\u81f4\u7684\u8f93\u5165\u4e0e\u51b3\u7b56\u5173\u7cfb\u7406\u89e3\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7684\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u5f15\u53d1\u4e86\u5bf9\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002\u4ee5\u5f80\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u6240\u6709\u9690\u85cf\u5355\u5143\u7684\u6fc0\u6d3b\u72b6\u6001\u5c06ReLU\u7f51\u7edc\u89e3\u6784\u4e3a\u7ebf\u6027\u6a21\u578b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u7ec6\u8282\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8def\u5f84\u5f0f\u89e3\u91ca\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u6e05\u6670\u3001\u7075\u6d3b\u7684\u51b3\u7b56\u8fc7\u7a0b\u89e3\u91ca\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u5f84\u5f0f\u89e3\u91ca\u65b9\u6cd5\uff0c\u5173\u6ce8\u51b3\u7b56\u8def\u5f84\u4e2d\u6d89\u53ca\u7684\u9690\u85cf\u5355\u5143\u5b50\u96c6\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u8c03\u6574\u8f93\u5165\u8303\u56f4\u5185\u7684\u89e3\u91ca\u8303\u56f4\uff08\u4ece\u6574\u4f53\u5f52\u56e0\u5230\u7279\u5b9a\u7ec4\u6210\u90e8\u5206\uff09\uff0c\u5e76\u652f\u6301\u5bf9\u8f93\u5165\u8fdb\u884c\u5206\u89e3\u4ee5\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6e05\u6670\u5730\u63ed\u793a\u8f93\u5165\u4e0e\u51b3\u7b56\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u8def\u5f84\u5f0f\u89e3\u91ca\u65b9\u6cd5\u4e3aReLU\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u548c\u53ef\u9760\u7684\u51b3\u7b56\u8fc7\u7a0b\u89e3\u91ca\uff0c\u5177\u6709\u7075\u6d3b\u6027\u548c\u7ec6\u8282\u6027\u4f18\u52bf\u3002", "paper_title_zh": "ReLU\u795e\u7ecf\u7f51\u7edc\u7684\u8def\u5f84\u5f0f\u89e3\u91ca", "abstract_zh": "\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u5f15\u53d1\u4e86\u5bf9\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002\u4ee5\u5f80\u5173\u4e8eReLU\u7f51\u7edc\u7684\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u6240\u6709\u9690\u85cf\u5355\u5143\u7684\u6fc0\u6d3b\u72b6\u6001\u5c06\u5176\u89e3\u6784\u4e3a\u7ebf\u6027\u6a21\u578b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5173\u6ce8\u51b3\u7b56\u8def\u5f84\u4e2d\u6d89\u53ca\u7684\u9690\u85cf\u5355\u5143\u5b50\u96c6\u3002\u8fd9\u79cd\u8def\u5f84\u5f0f\u89e3\u91ca\u63d0\u4f9b\u4e86\u5bf9\u8f93\u5165\u4e0e\u51b3\u7b56\u8fc7\u7a0b\u5173\u7cfb\u7684\u66f4\u6e05\u6670\u548c\u4e00\u81f4\u7684\u7406\u89e3\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5141\u8bb8\u8c03\u6574\u8f93\u5165\u8303\u56f4\u5185\u7684\u89e3\u91ca\u8303\u56f4\uff08\u4ece\u6574\u4f53\u5f52\u56e0\u5230\u7279\u5b9a\u7ec4\u6210\u90e8\u5206\uff09\uff0c\u5e76\u652f\u6301\u5bf9\u8f93\u5165\u8fdb\u884c\u5206\u89e3\u4ee5\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u89e3\u91ca\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002"}}
{"id": "2506.17966", "pdf": "https://arxiv.org/pdf/2506.17966", "abs": "https://arxiv.org/abs/2506.17966", "authors": ["Wangyu Wu", "Zhenhong Chen", "Xianglin Qiu", "Siqi Song", "Xiaowei Huang", "Fei Ma", "Jimin Xiao"], "title": "LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation", "categories": ["cs.IR", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15085", "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences and capturing both intra- and inter-sequence\nitem relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain\nSequential Recommendation (LLM-EMF), a novel and advanced approach that\nenhances textual information with Large Language Models (LLM) knowledge and\nsignificantly improves recommendation performance through the fusion of visual\nand textual data. Using the frozen CLIP model, we generate image and text\nembeddings, thereby enriching item representations with multimodal data. A\nmultiple attention mechanism jointly learns both single-domain and cross-domain\npreferences, effectively capturing and understanding complex user interests\nacross diverse domains. Evaluations conducted on four e-commerce datasets\ndemonstrate that LLM-EMF consistently outperforms existing methods in modeling\ncross-domain user preferences, thereby highlighting the effectiveness of\nmultimodal data integration and its advantages in enhancing sequential\nrecommendation systems. Our source code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff08LLM-EMF\uff09\uff0c\u7528\u4e8e\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff08CDSR\uff09\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff08CDSR\uff09\u65e8\u5728\u901a\u8fc7\u5229\u7528\u7528\u6237\u5728\u591a\u9886\u57df\u7684\u4ea4\u4e92\u5386\u53f2\u6765\u9884\u6d4b\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u8de8\u57df\u504f\u597d\u548c\u6355\u6349\u590d\u6742\u7528\u6237\u5174\u8da3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faLLM-EMF\u65b9\u6cd5\uff0c\u5229\u7528\u51bb\u7ed3\u7684CLIP\u6a21\u578b\u751f\u6210\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4e30\u5bcc\u9879\u76ee\u8868\u793a\uff1b\u901a\u8fc7\u591a\u91cd\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u5b66\u4e60\u5355\u57df\u548c\u8de8\u57df\u504f\u597d\uff0c\u4ee5\u6355\u6349\u590d\u6742\u7528\u6237\u5174\u8da3\u3002", "result": "\u5728\u56db\u4e2a\u7535\u5546\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM-EMF\u5728\u5efa\u6a21\u8de8\u57df\u7528\u6237\u504f\u597d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLM-EMF\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u7684\u6027\u80fd\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "paper_title_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u591a\u6a21\u6001\u878d\u5408\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5", "abstract_zh": "\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff08CDSR\uff09\u901a\u8fc7\u5229\u7528\u7528\u6237\u5728\u591a\u9886\u57df\u7684\u5386\u53f2\u4ea4\u4e92\u884c\u4e3a\u9884\u6d4b\u5176\u884c\u4e3a\uff0c\u91cd\u70b9\u5173\u6ce8\u8de8\u57df\u504f\u597d\u7684\u5efa\u6a21\u4ee5\u53ca\u5e8f\u5217\u5185\u548c\u5e8f\u5217\u95f4\u9879\u76ee\u5173\u7cfb\u7684\u6355\u6349\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u5148\u8fdb\u7684\u65b9\u6cd5\u2014\u2014\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u591a\u6a21\u6001\u878d\u5408\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff08LLM-EMF\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u77e5\u8bc6\u589e\u5f3a\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002\u5229\u7528\u51bb\u7ed3\u7684CLIP\u6a21\u578b\uff0c\u6211\u4eec\u751f\u6210\u4e86\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u4e30\u5bcc\u4e86\u9879\u76ee\u7684\u591a\u6a21\u6001\u8868\u793a\u3002\u901a\u8fc7\u591a\u91cd\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u5b66\u4e60\u5355\u57df\u548c\u8de8\u57df\u504f\u597d\uff0c\u6709\u6548\u6355\u6349\u548c\u7406\u89e3\u7528\u6237\u5728\u4e0d\u540c\u9886\u57df\u7684\u590d\u6742\u5174\u8da3\u3002\u5728\u56db\u4e2a\u7535\u5546\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cLLM-EMF\u5728\u5efa\u6a21\u8de8\u57df\u7528\u6237\u504f\u597d\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51f8\u663e\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u589e\u5f3a\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4f18\u52bf\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.18053", "pdf": "https://arxiv.org/pdf/2506.18053", "abs": "https://arxiv.org/abs/2506.18053", "authors": ["Marcos Florencio", "Thomas Barton"], "title": "Mechanistic Interpretability in the Presence of Architectural Obfuscation", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Architectural obfuscation - e.g., permuting hidden-state tensors, linearly\ntransforming embedding tables, or remapping tokens - has recently gained\ntraction as a lightweight substitute for heavyweight cryptography in\nprivacy-preserving large-language-model (LLM) inference. While recent work has\nshown that these techniques can be broken under dedicated reconstruction\nattacks, their impact on mechanistic interpretability has not been\nsystematically studied. In particular, it remains unclear whether scrambling a\nnetwork's internal representations truly thwarts efforts to understand how the\nmodel works, or simply relocates the same circuits to an unfamiliar coordinate\nsystem. We address this gap by analyzing a GPT-2-small model trained from\nscratch with a representative obfuscation map. Assuming the obfuscation map is\nprivate and the original basis is hidden (mirroring an honest-but-curious\nserver), we apply logit-lens attribution, causal path-patching, and\nattention-head ablation to locate and manipulate known circuits. Our findings\nreveal that obfuscation dramatically alters activation patterns within\nattention heads yet preserves the layer-wise computational graph. This\ndisconnect hampers reverse-engineering of user prompts: causal traces lose\ntheir alignment with baseline semantics, and token-level logit attributions\nbecome too noisy to reconstruct. At the same time, feed-forward and residual\npathways remain functionally intact, suggesting that obfuscation degrades\nfine-grained interpretability without compromising top-level task performance.\nThese results establish quantitative evidence that architectural obfuscation\ncan simultaneously (i) retain global model behaviour and (ii) impede\nmechanistic analyses of user-specific content. By mapping where\ninterpretability breaks down, our study provides guidance for future privacy\ndefences and for robustness-aware interpretability tooling.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u67b6\u6784\u6df7\u6dc6\u5bf9\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6df7\u6dc6\u867d\u6539\u53d8\u6fc0\u6d3b\u6a21\u5f0f\u4f46\u4fdd\u7559\u8ba1\u7b97\u56fe\uff0c\u963b\u788d\u7ec6\u7c92\u5ea6\u89e3\u91ca\u4f46\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u67b6\u6784\u6df7\u6dc6\uff08\u5982\u9690\u85cf\u72b6\u6001\u5f20\u91cf\u7f6e\u6362\u6216\u5d4c\u5165\u8868\u7ebf\u6027\u53d8\u6362\uff09\u88ab\u7528\u4f5c\u8f7b\u91cf\u7ea7\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff0c\u4f46\u5176\u5bf9\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e00\u4e2a\u4ece\u5934\u8bad\u7ec3\u7684GPT-2-small\u6a21\u578b\uff0c\u5e94\u7528logit-lens\u5f52\u56e0\u3001\u56e0\u679c\u8def\u5f84\u4fee\u8865\u548c\u6ce8\u610f\u529b\u5934\u6d88\u878d\u7b49\u65b9\u6cd5\uff0c\u7814\u7a76\u6df7\u6dc6\u5bf9\u5df2\u77e5\u7535\u8def\u7684\u5f71\u54cd\u3002", "result": "\u6df7\u6dc6\u663e\u8457\u6539\u53d8\u6ce8\u610f\u529b\u5934\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4f46\u4fdd\u7559\u5c42\u95f4\u8ba1\u7b97\u56fe\u3002\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027\u53d7\u635f\uff0c\u4f46\u4efb\u52a1\u6027\u80fd\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u67b6\u6784\u6df7\u6dc6\u80fd\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u6a21\u578b\u884c\u4e3a\u5e76\u963b\u788d\u7528\u6237\u7279\u5b9a\u5185\u5bb9\u7684\u673a\u5236\u5206\u6790\uff0c\u4e3a\u672a\u6765\u9690\u79c1\u9632\u5fa1\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u63d0\u4f9b\u6307\u5bfc\u3002", "paper_title_zh": "\u67b6\u6784\u6df7\u6dc6\u4e0b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76", "abstract_zh": "\u67b6\u6784\u6df7\u6dc6\uff08\u5982\u9690\u85cf\u72b6\u6001\u5f20\u91cf\u7f6e\u6362\u3001\u5d4c\u5165\u8868\u7ebf\u6027\u53d8\u6362\u6216\u4ee4\u724c\u91cd\u6620\u5c04\uff09\u8fd1\u5e74\u6765\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u5bc6\u7801\u5b66\u7684\u65b9\u6cd5\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u53d7\u5230\u5173\u6ce8\u3002\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6280\u672f\u53ef\u901a\u8fc7\u4e13\u7528\u91cd\u6784\u653b\u51fb\u7834\u89e3\uff0c\u4f46\u5176\u5bf9\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\u3002\u7279\u522b\u662f\uff0c\u5c1a\u4e0d\u6e05\u695a\u6df7\u6dc6\u7f51\u7edc\u7684\u5185\u90e8\u8868\u793a\u662f\u5426\u4f1a\u771f\u6b63\u963b\u788d\u5bf9\u6a21\u578b\u5de5\u4f5c\u539f\u7406\u7684\u7406\u89e3\uff0c\u8fd8\u662f\u4ec5\u5c06\u76f8\u540c\u7535\u8def\u8f6c\u79fb\u5230\u4e0d\u719f\u6089\u7684\u5750\u6807\u7cfb\u4e2d\u3002\u672c\u6587\u901a\u8fc7\u5206\u6790\u4e00\u4e2a\u4ece\u5934\u8bad\u7ec3\u7684GPT-2-small\u6a21\u578b\uff08\u91c7\u7528\u4ee3\u8868\u6027\u6df7\u6dc6\u6620\u5c04\uff09\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u5047\u8bbe\u6df7\u6dc6\u6620\u5c04\u4e3a\u79c1\u6709\u4e14\u539f\u59cb\u57fa\u9690\u85cf\uff08\u6a21\u62df\u8bda\u5b9e\u4f46\u597d\u5947\u7684\u670d\u52a1\u5668\uff09\uff0c\u6211\u4eec\u5e94\u7528logit-lens\u5f52\u56e0\u3001\u56e0\u679c\u8def\u5f84\u4fee\u8865\u548c\u6ce8\u610f\u529b\u5934\u6d88\u878d\u6765\u5b9a\u4f4d\u548c\u64cd\u4f5c\u5df2\u77e5\u7535\u8def\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6df7\u6dc6\u663e\u8457\u6539\u53d8\u6ce8\u610f\u529b\u5934\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4f46\u4fdd\u7559\u5c42\u95f4\u8ba1\u7b97\u56fe\u3002\u8fd9\u79cd\u8131\u8282\u963b\u788d\u4e86\u7528\u6237\u63d0\u793a\u7684\u53cd\u5411\u5de5\u7a0b\uff1a\u56e0\u679c\u8f68\u8ff9\u4e0e\u57fa\u7ebf\u8bed\u4e49\u5931\u53bb\u5bf9\u9f50\uff0c\u4ee4\u724c\u7ea7logit\u5f52\u56e0\u566a\u58f0\u8fc7\u5927\u65e0\u6cd5\u91cd\u6784\u3002\u540c\u65f6\uff0c\u524d\u9988\u548c\u6b8b\u5dee\u8def\u5f84\u529f\u80fd\u5b8c\u597d\uff0c\u8868\u660e\u6df7\u6dc6\u867d\u635f\u5bb3\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027\u4f46\u4e0d\u5f71\u54cd\u9876\u5c42\u4efb\u52a1\u6027\u80fd\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u67b6\u6784\u6df7\u6dc6\u80fd\u540c\u65f6\uff08i\uff09\u4fdd\u7559\u5168\u5c40\u6a21\u578b\u884c\u4e3a\u548c\uff08ii\uff09\u963b\u788d\u7528\u6237\u7279\u5b9a\u5185\u5bb9\u7684\u673a\u5236\u5206\u6790\u63d0\u4f9b\u4e86\u91cf\u5316\u8bc1\u636e\u3002\u901a\u8fc7\u6620\u5c04\u53ef\u89e3\u91ca\u6027\u5931\u6548\u70b9\uff0c\u672c\u7814\u7a76\u4e3a\u672a\u6765\u9690\u79c1\u9632\u5fa1\u548c\u9c81\u68d2\u6027\u611f\u77e5\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.17983", "pdf": "https://arxiv.org/pdf/2506.17983", "abs": "https://arxiv.org/abs/2506.17983", "authors": ["Chenyue Song", "Chen Hui", "Qing Lin", "Wei Zhang", "Siqiao Li", "Shengping Zhang", "Haiqi Zhu", "Zhixuan Li", "Shaohui Liu", "Feng Jiang", "Xiang Li"], "title": "LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "Autoregressive Initial Bits is a framework that integrates sub-image\nautoregression and latent variable modeling, demonstrating its advantages in\nlossless medical image compression. However, in existing methods, the image\nsegmentation process leads to an even distribution of latent variable\ninformation across each sub-image, which in turn causes posterior collapse and\ninefficient utilization of latent variables. To deal with these issues, we\npropose a prediction-based end-to-end lossless medical image compression method\nnamed LVPNet, leveraging global latent variables to predict pixel values and\nencoding predicted probabilities for lossless compression. Specifically, we\nintroduce the Global Multi-scale Sensing Module (GMSM), which extracts compact\nand informative latent representations from the entire image, effectively\ncapturing spatial dependencies within the latent space. Furthermore, to\nmitigate the information loss introduced during quantization, we propose the\nQuantization Compensation Module (QCM), which learns the distribution of\nquantization errors and refines the quantized features to compensate for\nquantization loss. Extensive experiments on challenging benchmarks demonstrate\nthat our method achieves superior compression efficiency compared to\nstate-of-the-art lossless image compression approaches, while maintaining\ncompetitive inference speed. The code is at\nhttps://github.com/Anonymity00000/Anonymity-repository/.", "AI": {"tldr": "LVPNet\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u53d8\u91cf\u7684\u9884\u6d4b\u9a71\u52a8\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u65e0\u635f\u538b\u7f29\uff0c\u901a\u8fc7\u5168\u5c40\u591a\u5c3a\u5ea6\u611f\u77e5\u6a21\u5757\u548c\u91cf\u5316\u8865\u507f\u6a21\u5757\u63d0\u5347\u538b\u7f29\u6548\u7387\u548c\u6f5c\u5728\u53d8\u91cf\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u5bfc\u81f4\u6f5c\u5728\u53d8\u91cf\u4fe1\u606f\u5747\u5300\u5206\u5e03\uff0c\u5f15\u53d1\u540e\u9a8c\u5d29\u6e83\u548c\u6f5c\u5728\u53d8\u91cf\u5229\u7528\u4e0d\u8db3\u3002LVPNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u65e0\u635f\u538b\u7f29\u6548\u7387\u3002", "method": "\u63d0\u51faLVPNet\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u6f5c\u5728\u53d8\u91cf\u9884\u6d4b\u50cf\u7d20\u503c\u5e76\u7f16\u7801\u9884\u6d4b\u6982\u7387\uff1b\u5f15\u5165\u5168\u5c40\u591a\u5c3a\u5ea6\u611f\u77e5\u6a21\u5757\uff08GMSM\uff09\u63d0\u53d6\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u91cf\u5316\u8865\u507f\u6a21\u5757\uff08QCM\uff09\u8865\u507f\u91cf\u5316\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLVPNet\u538b\u7f29\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65e0\u635f\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u901f\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "LVPNet\u901a\u8fc7\u5168\u5c40\u6f5c\u5728\u53d8\u91cf\u548c\u91cf\u5316\u8865\u507f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u65e0\u635f\u538b\u7f29\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "paper_title_zh": "LVPNet\uff1a\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u53d8\u91cf\u7684\u9884\u6d4b\u9a71\u52a8\u7aef\u5230\u7aef\u6846\u67b6\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u65e0\u635f\u538b\u7f29", "abstract_zh": "\u81ea\u56de\u5f52\u521d\u59cb\u6bd4\u7279\u662f\u4e00\u79cd\u7ed3\u5408\u5b50\u56fe\u50cf\u81ea\u56de\u5f52\u548c\u6f5c\u5728\u53d8\u91cf\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u5728\u65e0\u635f\u533b\u5b66\u56fe\u50cf\u538b\u7f29\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e2d\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u5bfc\u81f4\u6f5c\u5728\u53d8\u91cf\u4fe1\u606f\u5747\u5300\u5206\u5e03\u4e8e\u5404\u5b50\u56fe\u50cf\uff0c\u5f15\u53d1\u540e\u9a8c\u5d29\u6e83\u548c\u6f5c\u5728\u53d8\u91cf\u5229\u7528\u4e0d\u8db3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u57fa\u4e8e\u9884\u6d4b\u7684\u7aef\u5230\u7aef\u65e0\u635f\u533b\u5b66\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5LVPNet\uff0c\u5229\u7528\u5168\u5c40\u6f5c\u5728\u53d8\u91cf\u9884\u6d4b\u50cf\u7d20\u503c\u5e76\u7f16\u7801\u9884\u6d4b\u6982\u7387\u4ee5\u5b9e\u73b0\u65e0\u635f\u538b\u7f29\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5f15\u5165\u5168\u5c40\u591a\u5c3a\u5ea6\u611f\u77e5\u6a21\u5757\uff08GMSM\uff09\uff0c\u4ece\u6574\u5e45\u56fe\u50cf\u4e2d\u63d0\u53d6\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\uff0c\u6709\u6548\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u5185\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u7f13\u89e3\u91cf\u5316\u5f15\u5165\u7684\u4fe1\u606f\u635f\u5931\uff0c\u6211\u4eec\u63d0\u51fa\u91cf\u5316\u8865\u507f\u6a21\u5757\uff08QCM\uff09\uff0c\u5b66\u4e60\u91cf\u5316\u8bef\u5dee\u5206\u5e03\u5e76\u4f18\u5316\u91cf\u5316\u7279\u5f81\u4ee5\u8865\u507f\u91cf\u5316\u635f\u5931\u3002\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u538b\u7f29\u6548\u7387\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65e0\u635f\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u63a8\u7406\u901f\u5ea6\u3002\u4ee3\u7801\u8be6\u89c1https://github.com/Anonymity00000/Anonymity-repository/\u3002"}}
{"id": "2506.18072", "pdf": "https://arxiv.org/pdf/2506.18072", "abs": "https://arxiv.org/abs/2506.18072", "authors": ["Yunhao Liu", "Suyang Xi", "Shiqi Liu", "Hong Ding", "Chicheng Jin", "Chenxi Yang", "Junjun He", "Yiqing Shen"], "title": "Multimodal Medical Image Binding via Shared Text Embeddings", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "Medical image analysis increasingly relies on the integration of multiple\nimaging modalities to capture complementary anatomical and functional\ninformation, enabling more accurate diagnosis and treatment planning. Achieving\naligned feature representations across these diverse modalities is therefore\nimportant for effective multimodal analysis. While contrastive language-image\npre-training (CLIP) and its variant have enabled image-text alignments, they\nrequire explicitly paired data between arbitrary two modalities, which is\ndifficult to acquire in medical contexts. To address the gap, we present\nMultimodal Medical Image Binding with Text (M\\textsuperscript{3}Bind), a novel\npre-training framework that enables seamless alignment of multiple medical\nimaging modalities through a shared text representation space without requiring\nexplicit paired data between any two medical image modalities. Specifically,\nbased on the insight that different images can naturally bind with text,\nM\\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text\nmodels to align their modality-specific text embedding space while preserving\ntheir original image-text alignments. Subsequently, we distill these\nmodality-specific text encoders into a unified model, creating a shared text\nembedding space. Experiments on X-ray, CT, retina, ECG, and pathological images\non multiple downstream tasks demonstrate that M\\textsuperscript{3}Bind achieves\nstate-of-the-art performance in zero-shot, few-shot classification and\ncross-modal retrieval tasks compared to its CLIP-like counterparts. These\nresults validate M\\textsuperscript{3}Bind's effectiveness in achieving\ncross-image-modal alignment for medical analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM\u00b3Bind\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6587\u672c\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u7684\u65e0\u7f1d\u5bf9\u9f50\uff0c\u65e0\u9700\u663e\u5f0f\u914d\u5bf9\u6570\u636e\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u5206\u6790\u9700\u8981\u6574\u5408\u591a\u79cd\u6210\u50cf\u6a21\u6001\u4ee5\u83b7\u53d6\u4e92\u8865\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982CLIP\u9700\u8981\u663e\u5f0f\u914d\u5bf9\u6570\u636e\uff0c\u8fd9\u5728\u533b\u5b66\u9886\u57df\u96be\u4ee5\u83b7\u53d6\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "M\u00b3Bind\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684CLIP\u7c7b\u56fe\u50cf-\u6587\u672c\u6a21\u578b\uff0c\u5bf9\u9f50\u6a21\u6001\u7279\u5b9a\u7684\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u968f\u540e\u5c06\u8fd9\u4e9b\u6a21\u6001\u7279\u5b9a\u7684\u6587\u672c\u7f16\u7801\u5668\u84b8\u998f\u4e3a\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u521b\u5efa\u5171\u4eab\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728X\u5149\u3001CT\u3001\u89c6\u7f51\u819c\u3001\u5fc3\u7535\u56fe\u548c\u75c5\u7406\u56fe\u50cf\u7b49\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cM\u00b3Bind\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709CLIP\u7c7b\u65b9\u6cd5\u3002", "conclusion": "M\u00b3Bind\u901a\u8fc7\u5171\u4eab\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u6709\u6548\u5b9e\u73b0\u4e86\u533b\u5b66\u5f71\u50cf\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4e3a\u533b\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "paper_title_zh": "\u901a\u8fc7\u5171\u4eab\u6587\u672c\u5d4c\u5165\u5b9e\u73b0\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u7ed1\u5b9a", "abstract_zh": "\u533b\u5b66\u5f71\u50cf\u5206\u6790\u8d8a\u6765\u8d8a\u4f9d\u8d56\u591a\u79cd\u6210\u50cf\u6a21\u6001\u7684\u6574\u5408\uff0c\u4ee5\u6355\u6349\u4e92\u8865\u7684\u89e3\u5256\u548c\u529f\u80fd\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u3002\u56e0\u6b64\uff0c\u5728\u8fd9\u4e9b\u591a\u6837\u5316\u6a21\u6001\u4e4b\u95f4\u5b9e\u73b0\u5bf9\u9f50\u7684\u7279\u5f81\u8868\u793a\u5bf9\u4e8e\u6709\u6548\u7684\u591a\u6a21\u6001\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u53ca\u5176\u53d8\u4f53\u5b9e\u73b0\u4e86\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u4efb\u610f\u4e24\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u663e\u5f0f\u914d\u5bf9\u6570\u636e\uff0c\u8fd9\u5728\u533b\u5b66\u80cc\u666f\u4e0b\u96be\u4ee5\u83b7\u53d6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u4e0e\u6587\u672c\u7ed1\u5b9a\uff08M\u00b3Bind\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6587\u672c\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u7684\u65e0\u7f1d\u5bf9\u9f50\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u4e24\u79cd\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u4e4b\u95f4\u7684\u663e\u5f0f\u914d\u5bf9\u6570\u636e\u3002\u5177\u4f53\u800c\u8a00\uff0c\u57fa\u4e8e\u4e0d\u540c\u56fe\u50cf\u53ef\u4ee5\u81ea\u7136\u4e0e\u6587\u672c\u7ed1\u5b9a\u7684\u6d1e\u5bdf\uff0cM\u00b3Bind\u9996\u5148\u5fae\u8c03\u9884\u8bad\u7ec3\u7684CLIP\u7c7b\u56fe\u50cf-\u6587\u672c\u6a21\u578b\uff0c\u4ee5\u5bf9\u9f50\u5176\u6a21\u6001\u7279\u5b9a\u7684\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u6a21\u6001\u7279\u5b9a\u7684\u6587\u672c\u7f16\u7801\u5668\u84b8\u998f\u4e3a\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u521b\u5efa\u5171\u4eab\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u3002\u5728X\u5149\u3001CT\u3001\u89c6\u7f51\u819c\u3001\u5fc3\u7535\u56fe\u548c\u75c5\u7406\u56fe\u50cf\u4e0a\u7684\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684CLIP\u7c7b\u65b9\u6cd5\u76f8\u6bd4\uff0cM\u00b3Bind\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86M\u00b3Bind\u5728\u533b\u5b66\u5206\u6790\u4e2d\u5b9e\u73b0\u8de8\u56fe\u50cf\u6a21\u6001\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18074", "pdf": "https://arxiv.org/pdf/2506.18074", "abs": "https://arxiv.org/abs/2506.18074", "authors": ["Matteo Rufolo", "Dario Piga", "Marco Forgione"], "title": "Distributionally robust minimization in meta-learning for system identification", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Meta learning aims at learning how to solve tasks, and thus it allows to\nestimate models that can be quickly adapted to new scenarios. This work\nexplores distributionally robust minimization in meta learning for system\nidentification. Standard meta learning approaches optimize the expected loss,\noverlooking task variability. We use an alternative approach, adopting a\ndistributionally robust optimization paradigm that prioritizes high-loss tasks,\nenhancing performance in worst-case scenarios. Evaluated on a meta model\ntrained on a class of synthetic dynamical systems and tested in both\nin-distribution and out-of-distribution settings, the proposed approach allows\nto reduce failures in safety-critical applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5143\u5b66\u4e60\u4e2d\u91c7\u7528\u5206\u5e03\u9c81\u68d2\u6700\u5c0f\u5316\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8fa8\u8bc6\uff0c\u901a\u8fc7\u4f18\u5316\u9ad8\u635f\u5931\u4efb\u52a1\u63d0\u5347\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u671f\u671b\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u95f4\u7684\u53d8\u5f02\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u8303\u5f0f\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u635f\u5931\u4efb\u52a1\uff0c\u4ee5\u63d0\u5347\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u5143\u5b66\u4e60\u4e2d\u4f18\u5148\u4f18\u5316\u9ad8\u635f\u5931\u4efb\u52a1\uff0c\u5e76\u5728\u5408\u6210\u52a8\u6001\u7cfb\u7edf\u7c7b\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5305\u62ec\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5931\u8d25\u60c5\u51b5\uff0c\u63d0\u5347\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "\u5206\u5e03\u9c81\u68d2\u6700\u5c0f\u5316\u65b9\u6cd5\u5728\u5143\u5b66\u4e60\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u8fa8\u8bc6\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u5143\u5b66\u4e60\u4e2d\u7528\u4e8e\u7cfb\u7edf\u8fa8\u8bc6\u7684\u5206\u5e03\u9c81\u68d2\u6700\u5c0f\u5316", "abstract_zh": "\u5143\u5b66\u4e60\u65e8\u5728\u5b66\u4e60\u5982\u4f55\u89e3\u51b3\u4efb\u52a1\uff0c\u4ece\u800c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u573a\u666f\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5143\u5b66\u4e60\u4e2d\u91c7\u7528\u5206\u5e03\u9c81\u68d2\u6700\u5c0f\u5316\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8fa8\u8bc6\u3002\u4f20\u7edf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u671f\u671b\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u95f4\u7684\u53d8\u5f02\u6027\u3002\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5373\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u8303\u5f0f\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u635f\u5931\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u5347\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5728\u5408\u6210\u52a8\u6001\u7cfb\u7edf\u7c7b\u4e0a\u8bad\u7ec3\u7684\u5143\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e0b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5931\u8d25\u60c5\u51b5\u3002"}}
{"id": "2506.18069", "pdf": "https://arxiv.org/pdf/2506.18069", "abs": "https://arxiv.org/abs/2506.18069", "authors": ["Klaudia Ropel", "Krzysztof Kutt", "Luiz do Valle Miranda", "Grzegorz J. Nalepa"], "title": "Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages", "categories": ["cs.DL", "cs.CV"], "comment": "10 pages, 8 figures; submitted to TPDL 2025", "summary": "We developed a proof-of-concept method for the automatic analysis of the\nstructure and content of incunabula pages. A custom dataset comprising 500\nannotated pages from five different incunabula was created using resources from\nthe Jagiellonian Digital Library. Each page was manually labeled with five\npredefined classes: Text, Title, Picture, Table, and Handwriting. Additionally,\nthe publicly available DocLayNet dataset was utilized as supplementary training\ndata. To perform object detection, YOLO11n and YOLO11s models were employed and\ntrained using two strategies: a combined dataset (DocLayNet and the custom\ndataset) and the custom dataset alone. The highest performance (F1 = 0.94) was\nachieved by the YOLO11n model trained exclusively on the custom data. Optical\ncharacter recognition was then conducted on regions classified as Text, using\nboth Tesseract and Kraken OCR, with Tesseract demonstrating superior results.\nSubsequently, image classification was applied to the Picture class using a\nResNet18 model, achieving an accuracy of 98.7% across five subclasses:\nDecorative_letter, Illustration, Other, Stamp, and Wrong_detection.\nFurthermore, the CLIP model was utilized to generate semantic descriptions of\nillustrations. The results confirm the potential of machine learning in the\nanalysis of early printed books, while emphasizing the need for further\nadvancements in OCR performance and visual content interpretation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u65e9\u671f\u5370\u5237\u4e66\u7c4d\uff08incunabula\uff09\u9875\u9762\u7684\u7ed3\u6784\u548c\u5185\u5bb9\uff0c\u7ed3\u5408YOLO\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0cTesseract\u548cKraken\u8fdb\u884cOCR\uff0c\u4ee5\u53caResNet18\u548cCLIP\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u4e49\u63cf\u8ff0\u751f\u6210\uff0c\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u6027\u80fd\u3002", "motivation": "\u65e9\u671f\u5370\u5237\u4e66\u7c4d\uff08incunabula\uff09\u7684\u5206\u6790\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5206\u6790\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "1. \u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b500\u9875\u6807\u6ce8\u6570\u636e\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u7c7b\u522b\u5305\u62ec\u6587\u672c\u3001\u6807\u9898\u3001\u56fe\u7247\u3001\u8868\u683c\u548c\u624b\u5199\u5185\u5bb9\u30022. \u4f7f\u7528YOLO11n\u548cYOLO11s\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u8bad\u7ec3\u7b56\u7565\u5305\u62ec\u7ed3\u5408\u516c\u5f00\u6570\u636e\u96c6DocLayNet\u548c\u4ec5\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u30023. \u5bf9\u6587\u672c\u533a\u57df\u4f7f\u7528Tesseract\u548cKraken\u8fdb\u884cOCR\u30024. \u5bf9\u56fe\u7247\u7c7b\u522b\u4f7f\u7528ResNet18\u8fdb\u884c\u5b50\u7c7b\u5206\u7c7b\u30025. \u4f7f\u7528CLIP\u6a21\u578b\u751f\u6210\u63d2\u56fe\u7684\u8bed\u4e49\u63cf\u8ff0\u3002", "result": "1. YOLO11n\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u65f6\u8868\u73b0\u6700\u4f73\uff08F1=0.94\uff09\u30022. Tesseract\u5728OCR\u4efb\u52a1\u4e2d\u4f18\u4e8eKraken\u30023. ResNet18\u5728\u56fe\u7247\u5b50\u7c7b\u5206\u7c7b\u4e2d\u8fbe\u523098.7%\u7684\u51c6\u786e\u7387\u30024. CLIP\u6a21\u578b\u6210\u529f\u751f\u6210\u4e86\u63d2\u56fe\u7684\u8bed\u4e49\u63cf\u8ff0\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5728\u65e9\u671f\u5370\u5237\u4e66\u7c4d\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46OCR\u6027\u80fd\u548c\u89c6\u89c9\u5185\u5bb9\u7406\u89e3\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "paper_title_zh": "\u63ed\u5f00\u5386\u53f2\uff1a\u4e00\u79cd\u5168\u9762\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u65e9\u671f\u5370\u5237\u4e66\u7c4d\u9875\u9762", "abstract_zh": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6982\u5ff5\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u6790\u65e9\u671f\u5370\u5237\u4e66\u7c4d\u9875\u9762\u7684\u7ed3\u6784\u548c\u5185\u5bb9\u3002\u901a\u8fc7\u4f7f\u7528\u96c5\u76d6\u9686\u6570\u5b57\u56fe\u4e66\u9986\u7684\u8d44\u6e90\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b500\u9875\u6807\u6ce8\u6570\u636e\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u6bcf\u9875\u6807\u6ce8\u4e86\u4e94\u79cd\u9884\u5b9a\u4e49\u7c7b\u522b\uff1a\u6587\u672c\u3001\u6807\u9898\u3001\u56fe\u7247\u3001\u8868\u683c\u548c\u624b\u5199\u5185\u5bb9\u3002\u6b64\u5916\uff0c\u8fd8\u5229\u7528\u4e86\u516c\u5f00\u7684DocLayNet\u6570\u636e\u96c6\u4f5c\u4e3a\u8865\u5145\u8bad\u7ec3\u6570\u636e\u3002\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u4e86YOLO11n\u548cYOLO11s\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff1a\u7ed3\u5408DocLayNet\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4ec5\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002YOLO11n\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u65f6\u8868\u73b0\u6700\u4f73\uff08F1=0.94\uff09\u3002\u968f\u540e\uff0c\u5bf9\u5206\u7c7b\u4e3a\u6587\u672c\u7684\u533a\u57df\u8fdb\u884c\u4e86\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\uff0c\u4f7f\u7528\u4e86Tesseract\u548cKraken\uff0c\u5176\u4e2dTesseract\u8868\u73b0\u66f4\u4f18\u3002\u63a5\u7740\uff0c\u5bf9\u56fe\u7247\u7c7b\u522b\u4f7f\u7528ResNet18\u6a21\u578b\u8fdb\u884c\u4e86\u56fe\u50cf\u5206\u7c7b\uff0c\u5728\u4e94\u4e2a\u5b50\u7c7b\uff08\u88c5\u9970\u5b57\u6bcd\u3001\u63d2\u56fe\u3001\u5176\u4ed6\u3001\u5370\u7ae0\u548c\u9519\u8bef\u68c0\u6d4b\uff09\u4e2d\u8fbe\u5230\u4e8698.7%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528CLIP\u6a21\u578b\u751f\u6210\u4e86\u63d2\u56fe\u7684\u8bed\u4e49\u63cf\u8ff0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u673a\u5668\u5b66\u4e60\u5728\u65e9\u671f\u5370\u5237\u4e66\u7c4d\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46OCR\u6027\u80fd\u548c\u89c6\u89c9\u5185\u5bb9\u7406\u89e3\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2506.18087", "pdf": "https://arxiv.org/pdf/2506.18087", "abs": "https://arxiv.org/abs/2506.18087", "authors": ["Huaiying Luo", "Cheng Ji"], "title": "Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by the 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the widespread application of edge computing and cloud systems in\nAI-driven applications, how to maintain efficient performance while ensuring\ndata privacy has become an urgent security issue. This paper proposes a\nfederated learning-based data collaboration method to improve the security of\nedge cloud AI systems, and use large-scale language models (LLMs) to enhance\ndata privacy protection and system robustness. Based on the existing federated\nlearning framework, this method introduces a secure multi-party computation\nprotocol, which optimizes the data aggregation and encryption process between\ndistributed nodes by using LLM to ensure data privacy and improve system\nefficiency. By combining advanced adversarial training techniques, the model\nenhances the resistance of edge cloud AI systems to security threats such as\ndata leakage and model poisoning. Experimental results show that the proposed\nmethod is 15% better than the traditional federated learning method in terms of\ndata protection and model robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u6570\u636e\u534f\u4f5c\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u5347\u8fb9\u7f18\u4e91AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\u534f\u8bae\u548c\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\u4f18\u5316\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18\u8ba1\u7b97\u548c\u4e91\u7cfb\u7edf\u5728AI\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u9ad8\u6548\u6027\u80fd\u6210\u4e3a\u4e9f\u5f85\u89e3\u51b3\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5728\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\u534f\u8bae\uff0c\u5229\u7528LLM\u4f18\u5316\u5206\u5e03\u5f0f\u8282\u70b9\u95f4\u7684\u6570\u636e\u805a\u5408\u4e0e\u52a0\u5bc6\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\u589e\u5f3a\u7cfb\u7edf\u5bf9\u6570\u636e\u6cc4\u9732\u548c\u6a21\u578b\u6295\u6bd2\u7b49\u5a01\u80c1\u7684\u62b5\u6297\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u4fdd\u62a4\u548c\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u6bd4\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u4e8615%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8fb9\u7f18\u4e91AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u6570\u636e\u534f\u4f5c\u65b9\u6cd5\uff1a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u8fb9\u7f18\u4e91AI\u7cfb\u7edf\u5b89\u5168\u6027", "abstract_zh": "\u968f\u7740\u8fb9\u7f18\u8ba1\u7b97\u548c\u4e91\u7cfb\u7edf\u5728AI\u9a71\u52a8\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u5728\u786e\u4fdd\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u80fd\u6210\u4e3a\u4e00\u4e2a\u7d27\u8feb\u7684\u5b89\u5168\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u6570\u636e\u534f\u4f5c\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8fb9\u7f18\u4e91AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\u534f\u8bae\uff0c\u901a\u8fc7\u4f7f\u7528LLM\u4f18\u5316\u5206\u5e03\u5f0f\u8282\u70b9\u95f4\u7684\u6570\u636e\u805a\u5408\u548c\u52a0\u5bc6\u8fc7\u7a0b\uff0c\u786e\u4fdd\u6570\u636e\u9690\u79c1\u5e76\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u3002\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u7684\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\uff0c\u6a21\u578b\u589e\u5f3a\u4e86\u8fb9\u7f18\u4e91AI\u7cfb\u7edf\u5bf9\u6570\u636e\u6cc4\u9732\u548c\u6a21\u578b\u6295\u6bd2\u7b49\u5b89\u5168\u5a01\u80c1\u7684\u62b5\u6297\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u636e\u4fdd\u62a4\u548c\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u6bd4\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u4e8615%\u3002"}}
{"id": "2506.18110", "pdf": "https://arxiv.org/pdf/2506.18110", "abs": "https://arxiv.org/abs/2506.18110", "authors": ["Mohammad Hossein Amani", "Aryo Lotfi", "Nicolas Mario Baldwin", "Samy Bengio", "Mehrdad Farajtabar", "Emmanuel Abbe", "Robert West"], "title": "RL for Reasoning by Adaptively Revealing Rationales", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 8 figures", "summary": "We propose that reinforcement learning (RL) from partial expert\ndemonstrations is not merely a training heuristic, but a promising framework\nfor solving complex sequence generation tasks. Supervised fine-tuning (SFT)\nrelies on dense ground-truth labels, which become increasingly costly as\nsequence length grows. RL, on the other hand, struggles with sparse rewards and\na combinatorially large output space. We address this by introducing adaptive\nbacktracking (AdaBack), a per-sample curriculum learning algorithm that reveals\nonly a partial prefix of the target output during training. The supervision\nlength is adjusted dynamically for each sample based on the model's past reward\nsignal, allowing it to incrementally learn to complete reasoning chains by\nconditioning on correct partial solutions. We investigate this intermediate\nregime between SFT and RL and argue that per-sample curriculum learning is more\nthan a trade-off between efficiency and generality, it can succeed in tasks\nwith long sequences of latent dependencies where SFT and RL both fail to\ngeneralize. Using a synthetic task with latent parity constraints, we show that\nour adaptive curriculum over partial answers reliably solves problems that are\notherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we\nfind that curriculum learning enables models to solve problems that RL alone\ncannot, acquiring new reasoning capabilities through incremental exposure to\npartial solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u81ea\u9002\u5e94\u56de\u6eaf\u65b9\u6cd5\uff08AdaBack\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u65f6\u7684\u90e8\u5206\u76ee\u6807\u8f93\u51fa\u524d\u7f00\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u4e2d\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548cRL\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u96be\u9898\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4f9d\u8d56\u5bc6\u96c6\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u6210\u672c\u9ad8\u6602\uff1b\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5219\u56e0\u7a00\u758f\u5956\u52b1\u548c\u5de8\u5927\u7684\u8f93\u51fa\u7a7a\u95f4\u800c\u96be\u4ee5\u5e94\u5bf9\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u4ecb\u4e8eSFT\u548cRL\u4e4b\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u90e8\u5206\u76d1\u7763\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u56de\u6eaf\uff08AdaBack\uff09\u7b97\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u6837\u672c\u7684\u8bad\u7ec3\u76d1\u7763\u957f\u5ea6\uff0c\u4ec5\u63ed\u793a\u90e8\u5206\u76ee\u6807\u8f93\u51fa\u524d\u7f00\u3002\u6a21\u578b\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u9010\u6b65\u5b66\u4e60\u57fa\u4e8e\u6b63\u786e\u90e8\u5206\u89e3\u5b8c\u6210\u63a8\u7406\u94fe\u7684\u80fd\u529b\u3002", "result": "\u5728\u5177\u6709\u6f5c\u5728\u5947\u5076\u7ea6\u675f\u7684\u5408\u6210\u4efb\u52a1\u4e2d\uff0cAdaBack\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u95ee\u9898\u3002\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff08MATH\u3001GSM8k\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u83b7\u5f97\u65b0\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4ec5\u9760RL\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\u3002", "conclusion": "\u81ea\u9002\u5e94\u90e8\u5206\u76d1\u7763\u7684\u8bfe\u7a0b\u5b66\u4e60\u4e0d\u4ec5\u662f\u4e00\u79cd\u6548\u7387\u4e0e\u901a\u7528\u6027\u7684\u6743\u8861\uff0c\u8fd8\u80fd\u5728SFT\u548cRL\u5747\u5931\u6548\u7684\u957f\u5e8f\u5217\u6f5c\u5728\u4f9d\u8d56\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\u3002AdaBack\u4e3a\u590d\u6742\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u901a\u8fc7\u81ea\u9002\u5e94\u63ed\u793a\u7406\u6027\u5b9e\u73b0\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\uff0c\u57fa\u4e8e\u90e8\u5206\u4e13\u5bb6\u6f14\u793a\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0d\u4ec5\u662f\u4e00\u79cd\u8bad\u7ec3\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8fd8\u662f\u89e3\u51b3\u590d\u6742\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u7684\u6709\u524d\u666f\u6846\u67b6\u3002\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4f9d\u8d56\u5bc6\u96c6\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u6210\u672c\u6025\u5267\u4e0a\u5347\uff1b\u800cRL\u5219\u56e0\u7a00\u758f\u5956\u52b1\u548c\u5de8\u5927\u7684\u8f93\u51fa\u7a7a\u95f4\u800c\u96be\u4ee5\u5e94\u5bf9\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u56de\u6eaf\uff08AdaBack\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bad\u7ec3\u65f6\u4ec5\u63ed\u793a\u76ee\u6807\u8f93\u51fa\u7684\u90e8\u5206\u524d\u7f00\u3002\u76d1\u7763\u957f\u5ea6\u6839\u636e\u6a21\u578b\u8fc7\u53bb\u7684\u5956\u52b1\u4fe1\u53f7\u52a8\u6001\u8c03\u6574\uff0c\u4f7f\u5176\u80fd\u591f\u9010\u6b65\u5b66\u4e60\u57fa\u4e8e\u6b63\u786e\u90e8\u5206\u89e3\u5b8c\u6210\u63a8\u7406\u94fe\u7684\u80fd\u529b\u3002\u6211\u4eec\u7814\u7a76\u4e86SFT\u4e0eRL\u4e4b\u95f4\u7684\u8fd9\u79cd\u4e2d\u95f4\u72b6\u6001\uff0c\u5e76\u8ba4\u4e3a\u57fa\u4e8e\u6837\u672c\u7684\u8bfe\u7a0b\u5b66\u4e60\u4e0d\u4ec5\u662f\u6548\u7387\u4e0e\u901a\u7528\u6027\u7684\u6743\u8861\uff0c\u8fd8\u80fd\u5728\u957f\u5e8f\u5217\u6f5c\u5728\u4f9d\u8d56\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u800cSFT\u548cRL\u5747\u65e0\u6cd5\u6cdb\u5316\u3002\u901a\u8fc7\u5177\u6709\u6f5c\u5728\u5947\u5076\u7ea6\u675f\u7684\u5408\u6210\u4efb\u52a1\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u8bfe\u7a0b\u5bf9\u90e8\u5206\u7b54\u6848\u7684\u53ef\u9760\u89e3\u51b3\u80fd\u529b\u3002\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff08MATH\u3001GSM8k\uff09\u4e0a\uff0c\u6211\u4eec\u53d1\u73b0\u8bfe\u7a0b\u5b66\u4e60\u4f7f\u6a21\u578b\u80fd\u591f\u89e3\u51b3\u4ec5\u9760RL\u65e0\u6cd5\u5b8c\u6210\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u9010\u6b65\u63a5\u89e6\u90e8\u5206\u89e3\u83b7\u5f97\u65b0\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.18162", "pdf": "https://arxiv.org/pdf/2506.18162", "abs": "https://arxiv.org/abs/2506.18162", "authors": ["Hendrik Mehrtens", "Tabea Bucher", "Titus J. Brinker"], "title": "Pitfalls of Conformal Predictions for Medical Image Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Reliable uncertainty estimation is one of the major challenges for medical\nclassification tasks. While many approaches have been proposed, recently the\nstatistical framework of conformal predictions has gained a lot of attention,\ndue to its ability to provide provable calibration guarantees. Nonetheless, the\napplication of conformal predictions in safety-critical areas such as medicine\ncomes with pitfalls, limitations and assumptions that practitioners need to be\naware of. We demonstrate through examples from dermatology and histopathology\nthat conformal predictions are unreliable under distributional shifts in input\nand label variables. Additionally, conformal predictions should not be used for\nselecting predictions to improve accuracy and are not reliable for subsets of\nthe data, such as individual classes or patient attributes. Moreover, in\nclassification settings with a small number of classes, which are common in\nmedical image classification tasks, conformal predictions have limited\npractical value.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\uff0c\u5c3d\u7ba1\u4fdd\u5f62\u9884\u6d4b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u56e0\u5176\u53ef\u8bc1\u660e\u7684\u6821\u51c6\u4fdd\u8bc1\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u8f93\u5165\u548c\u6807\u7b7e\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u65f6\u4e0d\u53ef\u9760\uff0c\u4e14\u4e0d\u9002\u7528\u4e8e\u63d0\u9ad8\u51c6\u786e\u6027\u6216\u7279\u5b9a\u6570\u636e\u5b50\u96c6\uff08\u5982\u5355\u4e2a\u7c7b\u522b\u6216\u60a3\u8005\u5c5e\u6027\uff09\u3002\u6b64\u5916\uff0c\u5728\u7c7b\u522b\u8f83\u5c11\u7684\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5176\u5b9e\u7528\u4ef7\u503c\u6709\u9650\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u4fdd\u5f62\u9884\u6d4b\u56e0\u5176\u53ef\u8bc1\u660e\u7684\u6821\u51c6\u4fdd\u8bc1\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u5b58\u5728\u6f5c\u5728\u95ee\u9898\u548c\u9650\u5236\uff0c\u9700\u8981\u5f15\u8d77\u5b9e\u8df5\u8005\u7684\u91cd\u89c6\u3002", "method": "\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u7684\u5b9e\u4f8b\uff0c\u5206\u6790\u4e86\u4fdd\u5f62\u9884\u6d4b\u5728\u8f93\u5165\u548c\u6807\u7b7e\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u6216\u7279\u5b9a\u6570\u636e\u5b50\u96c6\uff08\u5982\u5355\u4e2a\u7c7b\u522b\u6216\u60a3\u8005\u5c5e\u6027\uff09\u4e2d\u7684\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4fdd\u5f62\u9884\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u65f6\u4e0d\u53ef\u9760\uff0c\u4e0d\u9002\u7528\u4e8e\u9009\u62e9\u9884\u6d4b\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4e14\u5728\u6570\u636e\u5b50\u96c6\uff08\u5982\u5355\u4e2a\u7c7b\u522b\u6216\u60a3\u8005\u5c5e\u6027\uff09\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002\u6b64\u5916\uff0c\u5728\u7c7b\u522b\u8f83\u5c11\u7684\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5176\u5b9e\u7528\u4ef7\u503c\u6709\u9650\u3002", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5c40\u9650\u6027\uff08\u5982\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u654f\u611f\u6027\u548c\u5728\u7279\u5b9a\u6570\u636e\u5b50\u96c6\u4e2d\u7684\u4e0d\u53ef\u9760\u6027\uff09\u9700\u5f15\u8d77\u91cd\u89c6\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u7684\u533b\u5b66\u9886\u57df\u3002", "paper_title_zh": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u4fdd\u5f62\u9884\u6d4b\u7684\u9677\u9631", "abstract_zh": "\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u662f\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u8fd1\u5e74\u6765\uff0c\u4fdd\u5f62\u9884\u6d4b\u7684\u7edf\u8ba1\u6846\u67b6\u56e0\u5176\u80fd\u591f\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6821\u51c6\u4fdd\u8bc1\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5728\u533b\u5b66\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u4e2d\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u65f6\uff0c\u5b58\u5728\u4e00\u4e9b\u9677\u9631\u3001\u9650\u5236\u548c\u5047\u8bbe\uff0c\u5b9e\u8df5\u8005\u9700\u8981\u610f\u8bc6\u5230\u8fd9\u4e9b\u95ee\u9898\u3002\u6211\u4eec\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u7684\u5b9e\u4f8b\u8868\u660e\uff0c\u4fdd\u5f62\u9884\u6d4b\u5728\u8f93\u5165\u548c\u6807\u7b7e\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u65f6\u4e0d\u53ef\u9760\u3002\u6b64\u5916\uff0c\u4fdd\u5f62\u9884\u6d4b\u4e0d\u5e94\u7528\u4e8e\u9009\u62e9\u9884\u6d4b\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4e14\u5728\u6570\u636e\u5b50\u96c6\uff08\u5982\u5355\u4e2a\u7c7b\u522b\u6216\u60a3\u8005\u5c5e\u6027\uff09\u4e2d\u4e0d\u53ef\u9760\u3002\u53e6\u5916\uff0c\u5728\u7c7b\u522b\u8f83\u5c11\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4fdd\u5f62\u9884\u6d4b\u7684\u5b9e\u7528\u4ef7\u503c\u6709\u9650\u3002"}}
{"id": "2506.18119", "pdf": "https://arxiv.org/pdf/2506.18119", "abs": "https://arxiv.org/abs/2506.18119", "authors": ["Jaime Banks", "Zhixin Li"], "title": "Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The notion of machine companions has long been embedded in\nsocial-technological imaginaries. Recent advances in AI have moved those media\nmusings into believable sociality manifested in interfaces, robotic bodies, and\ndevices. Those machines are often referred to colloquially as \"companions\" yet\nthere is little careful engagement of machine companionship (MC) as a formal\nconcept or measured variable. This PRISMA-guided scoping review systematically\nsamples, surveys, and synthesizes current scholarly works on MC (N = 71;\n2017-2025), to that end. Works varied widely in considerations of MC according\nto guiding theories, dimensions of a-priori specified properties (subjectively\npositive, sustained over time, co-active, autotelic), and in measured concepts\n(with more than 50 distinct measured variables). WE ultimately offer a\nliterature-guided definition of MC as an autotelic, coordinated connection\nbetween human and machine that unfolds over time and is subjectively positive.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PRISMA\u6307\u5bfc\u7684\u8303\u56f4\u7efc\u8ff0\uff0c\u7cfb\u7edf\u68b3\u7406\u4e862017-2025\u5e74\u95f471\u7bc7\u5173\u4e8e\u673a\u5668\u4f34\u4fa3\uff08MC\uff09\u7684\u5b66\u672f\u7814\u7a76\uff0c\u63d0\u51fa\u4e86MC\u7684\u5b9a\u4e49\uff1a\u4e00\u79cd\u81ea\u4e3b\u3001\u534f\u8c03\u4e14\u968f\u65f6\u95f4\u53d1\u5c55\u7684\u4eba\u7c7b\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\u4e3b\u89c2\u79ef\u6781\u8fde\u63a5\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u4f34\u4fa3\u5728\u793e\u4ea4\u6280\u672f\u60f3\u8c61\u4e2d\u7531\u6765\u5df2\u4e45\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u4f5c\u4e3a\u6b63\u5f0f\u6982\u5ff5\u6216\u6d4b\u91cf\u53d8\u91cf\u7684\u6df1\u5165\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u6587\u732e\uff0c\u660e\u786eMC\u7684\u5b9a\u4e49\u548c\u6d4b\u91cf\u7ef4\u5ea6\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5bfc\u7684\u8303\u56f4\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7b5b\u9009\u3001\u8c03\u67e5\u5e76\u7efc\u5408\u4e862017-2025\u5e74\u95f471\u7bc7\u5173\u4e8eMC\u7684\u5b66\u672f\u7814\u7a76\uff0c\u5206\u6790\u4e86\u5176\u7406\u8bba\u6846\u67b6\u3001\u5148\u9a8c\u5c5e\u6027\u7ef4\u5ea6\uff08\u4e3b\u89c2\u79ef\u6781\u3001\u6301\u7eed\u3001\u534f\u540c\u3001\u81ea\u4e3b\uff09\u53ca\u6d4b\u91cf\u53d8\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cMC\u7684\u7814\u7a76\u5728\u7406\u8bba\u6846\u67b6\u3001\u5c5e\u6027\u7ef4\u5ea6\u548c\u6d4b\u91cf\u53d8\u91cf\u4e0a\u5dee\u5f02\u663e\u8457\uff08\u8d85\u8fc750\u79cd\u4e0d\u540c\u7684\u6d4b\u91cf\u53d8\u91cf\uff09\u3002\u6700\u7ec8\u63d0\u51fa\u4e86MC\u7684\u6587\u732e\u6307\u5bfc\u5b9a\u4e49\uff1a\u4e00\u79cd\u81ea\u4e3b\u3001\u534f\u8c03\u4e14\u968f\u65f6\u95f4\u53d1\u5c55\u7684\u4eba\u7c7b\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\u4e3b\u89c2\u79ef\u6781\u8fde\u63a5\u3002", "conclusion": "\u672c\u6587\u4e3a\u673a\u5668\u4f34\u4fa3\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u7efc\u8ff0\u548c\u5b9a\u4e49\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "paper_title_zh": "\u673a\u5668\u4f34\u4fa3\u7684\u6982\u5ff5\u5316\u3001\u64cd\u4f5c\u5316\u4e0e\u6d4b\u91cf\uff1a\u8303\u56f4\u7efc\u8ff0", "abstract_zh": "\u673a\u5668\u4f34\u4fa3\u7684\u6982\u5ff5\u957f\u671f\u6839\u690d\u4e8e\u793e\u4f1a\u6280\u672f\u60f3\u8c61\u4e2d\u3002\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u5c06\u8fd9\u4e9b\u5a92\u4f53\u8bbe\u60f3\u8f6c\u5316\u4e3a\u53ef\u611f\u77e5\u7684\u793e\u4ea4\u6027\uff0c\u4f53\u73b0\u5728\u754c\u9762\u3001\u673a\u5668\u4eba\u8eab\u4f53\u548c\u8bbe\u5907\u4e2d\u3002\u8fd9\u4e9b\u673a\u5668\u5e38\u88ab\u901a\u4fd7\u5730\u79f0\u4e3a\u201c\u4f34\u4fa3\u201d\uff0c\u4f46\u9c9c\u6709\u7814\u7a76\u5c06\u673a\u5668\u4f34\u4fa3\uff08MC\uff09\u4f5c\u4e3a\u6b63\u5f0f\u6982\u5ff5\u6216\u6d4b\u91cf\u53d8\u91cf\u8fdb\u884c\u6df1\u5165\u63a2\u8ba8\u3002\u672c\u6587\u901a\u8fc7PRISMA\u6307\u5bfc\u7684\u8303\u56f4\u7efc\u8ff0\uff0c\u7cfb\u7edf\u7b5b\u9009\u3001\u8c03\u67e5\u5e76\u7efc\u5408\u4e862017-2025\u5e74\u95f4\u5173\u4e8eMC\u7684\u5b66\u672f\u7814\u7a76\uff08N=71\uff09\u3002\u7814\u7a76\u53d1\u73b0\uff0cMC\u7684\u7814\u7a76\u5728\u7406\u8bba\u6846\u67b6\u3001\u5148\u9a8c\u5c5e\u6027\u7ef4\u5ea6\uff08\u4e3b\u89c2\u79ef\u6781\u3001\u6301\u7eed\u3001\u534f\u540c\u3001\u81ea\u4e3b\uff09\u53ca\u6d4b\u91cf\u53d8\u91cf\u4e0a\u5dee\u5f02\u663e\u8457\uff08\u8d85\u8fc750\u79cd\u4e0d\u540c\u7684\u6d4b\u91cf\u53d8\u91cf\uff09\u3002\u6700\u7ec8\u63d0\u51fa\u4e86MC\u7684\u6587\u732e\u6307\u5bfc\u5b9a\u4e49\uff1a\u4e00\u79cd\u81ea\u4e3b\u3001\u534f\u8c03\u4e14\u968f\u65f6\u95f4\u53d1\u5c55\u7684\u4eba\u7c7b\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\u4e3b\u89c2\u79ef\u6781\u8fde\u63a5\u3002"}}
{"id": "2506.18251", "pdf": "https://arxiv.org/pdf/2506.18251", "abs": "https://arxiv.org/abs/2506.18251", "authors": ["Chao Li", "Jiawei Fan", "Anbang Yao"], "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "This work is accepted to ICML 2025. The project page:\n  https://github.com/deep-optimization/Morse", "summary": "In this paper, we present Morse, a simple dual-sampling framework for\naccelerating diffusion models losslessly. The key insight of Morse is to\nreformulate the iterative generation (from noise to data) process via taking\nadvantage of fast jump sampling and adaptive residual feedback strategies.\nSpecifically, Morse involves two models called Dash and Dot that interact with\neach other. The Dash model is just the pre-trained diffusion model of any type,\nbut operates in a jump sampling regime, creating sufficient space for sampling\nefficiency improvement. The Dot model is significantly faster than the Dash\nmodel, which is learnt to generate residual feedback conditioned on the\nobservations at the current jump sampling point on the trajectory of the Dash\nmodel, lifting the noise estimate to easily match the next-step estimate of the\nDash model without jump sampling. By chaining the outputs of the Dash and Dot\nmodels run in a time-interleaved fashion, Morse exhibits the merit of flexibly\nattaining desired image generation performance while improving overall runtime\nefficiency. With our proposed weight sharing strategy between the Dash and Dot\nmodels, Morse is efficient for training and inference. Our method shows a\nlossless speedup of 1.78X to 3.31X on average over a wide range of sampling\nstep budgets relative to 9 baseline diffusion models on 6 image generation\ntasks. Furthermore, we show that our method can be also generalized to improve\nthe Latent Consistency Model (LCM-SDXL, which is already accelerated with\nconsistency distillation technique) tailored for few-step text-to-image\nsynthesis. The code and models are available at\nhttps://github.com/deep-optimization/Morse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMorse\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91c7\u6837\u7b56\u7565\uff08Dash\u548cDot\u6a21\u578b\u4ea4\u4e92\uff09\u65e0\u635f\u52a0\u901f\u6269\u6563\u6a21\u578b\uff0c\u5e73\u5747\u63d0\u901f1.78X\u81f33.31X\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u901f\u5ea6\u6162\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u53ef\u80fd\u635f\u5931\u6027\u80fd\u3002Morse\u65e8\u5728\u901a\u8fc7\u53cc\u91c7\u6837\u6846\u67b6\u5b9e\u73b0\u65e0\u635f\u52a0\u901f\uff0c\u63d0\u5347\u6548\u7387\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "Morse\u5305\u542bDash\u548cDot\u6a21\u578b\uff1aDash\u4e3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u8df3\u8dc3\u91c7\u6837\uff1bDot\u751f\u6210\u6b8b\u5dee\u53cd\u9988\uff0c\u63d0\u5347\u566a\u58f0\u4f30\u8ba1\u6548\u7387\u3002\u4e24\u8005\u901a\u8fc7\u65f6\u95f4\u4ea4\u9519\u8fd0\u884c\u548c\u6743\u91cd\u5171\u4eab\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u4e0e\u63a8\u7406\u3002", "result": "\u57286\u9879\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cMorse\u5e73\u5747\u63d0\u901f1.78X\u81f33.31X\uff0c\u4f18\u4e8e9\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3Latent Consistency Model\uff08LCM-SDXL\uff09\u3002", "conclusion": "Morse\u901a\u8fc7\u53cc\u91c7\u6837\u6846\u67b6\u548c\u6b8b\u5dee\u53cd\u9988\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u7684\u65e0\u635f\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e14\u4fdd\u6301\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u4efb\u52a1\u3002", "paper_title_zh": "Morse\uff1a\u53cc\u91c7\u6837\u6846\u67b6\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u7684\u65e0\u635f\u52a0\u901f", "abstract_zh": "\u672c\u6587\u63d0\u51faMorse\uff0c\u4e00\u79cd\u7b80\u5355\u7684\u53cc\u91c7\u6837\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u635f\u52a0\u901f\u6269\u6563\u6a21\u578b\u3002Morse\u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5feb\u901f\u8df3\u8dc3\u91c7\u6837\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u53cd\u9988\u7b56\u7565\u91cd\u6784\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u3002\u5177\u4f53\u800c\u8a00\uff0cMorse\u5305\u542bDash\u548cDot\u4e24\u4e2a\u4ea4\u4e92\u6a21\u578b\uff1aDash\u4e3a\u4efb\u610f\u7c7b\u578b\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u8df3\u8dc3\u91c7\u6837\u4ee5\u63d0\u5347\u6548\u7387\uff1bDot\u6a21\u578b\u901f\u5ea6\u66f4\u5feb\uff0c\u901a\u8fc7\u5b66\u4e60\u751f\u6210\u6b8b\u5dee\u53cd\u9988\uff0c\u5c06\u566a\u58f0\u4f30\u8ba1\u63d0\u5347\u81f3\u4e0eDash\u6a21\u578b\u7684\u4e0b\u4e00\u6b65\u4f30\u8ba1\u5339\u914d\uff0c\u65e0\u9700\u8df3\u8dc3\u91c7\u6837\u3002\u901a\u8fc7\u65f6\u95f4\u4ea4\u9519\u8fd0\u884cDash\u548cDot\u6a21\u578b\uff0cMorse\u7075\u6d3b\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u751f\u6210\u3002\u63d0\u51fa\u7684\u6743\u91cd\u5171\u4eab\u7b56\u7565\u4f7f\u8bad\u7ec3\u548c\u63a8\u7406\u66f4\u9ad8\u6548\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMorse\u57286\u9879\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u901f1.78X\u81f33.31X\uff0c\u4f18\u4e8e9\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u9002\u7528\u4e8e\u5c11\u6b65\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7684Latent Consistency Model\uff08LCM-SDXL\uff09\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.18371", "pdf": "https://arxiv.org/pdf/2506.18371", "abs": "https://arxiv.org/abs/2506.18371", "authors": ["Sara Rehmat", "Hafeez Ur Rehman"], "title": "Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The overexpression of the human epidermal growth factor receptor 2 (HER2) in\nbreast cells is a key driver of HER2-positive breast cancer, a highly\naggressive subtype requiring precise diagnosis and targeted therapy.\nImmunohistochemistry (IHC) is the standard technique for HER2 assessment but is\ncostly, labor-intensive, and highly dependent on antibody selection. In\ncontrast, hematoxylin and eosin (H&E) staining, a routine histopathological\nprocedure, offers broader accessibility but lacks HER2 specificity. This study\nproposes an advanced deep learning-based image translation framework to\ngenerate highfidelity IHC images from H&E-stained tissue samples, enabling\ncost-effective and scalable HER2 assessment. By modifying the loss function of\npyramid pix2pix, we mitigate mode collapse, a fundamental limitation in\ngenerative adversarial networks (GANs), and introduce a novel variance-based\npenalty that enforces structural diversity in generated images. Our model\nparticularly excels in translating HER2-positive (IHC 3+) images, which have\nremained challenging for existing methods due to their complex morphological\nvariations. Extensive evaluations on the BCI histopathological dataset\ndemonstrate that our model surpasses state-of-the-art methods in terms of peak\nsignal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet\nInception Distance (FID), particularly in accurately translating HER2-positive\n(IHC 3+) images. Beyond medical imaging, our model exhibits superior\nperformance in general image-to-image translation tasks, showcasing its\npotential across multiple domains. This work marks a significant step toward\nAI-driven precision oncology, offering a reliable and efficient alternative to\ntraditional HER2 diagnostics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684GAN\u6a21\u578b\u5c06H&E\u67d3\u8272\u56fe\u50cf\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684IHC\u56fe\u50cf\uff0c\u7528\u4e8e\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u7684HER2\u8bc4\u4f30\u3002", "motivation": "HER2\u9633\u6027\u4e73\u817a\u764c\u7684\u8bca\u65ad\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684IHC\u6280\u672f\uff0c\u800c\u5e38\u89c4\u7684H&E\u67d3\u8272\u867d\u666e\u53ca\u4f46\u7f3a\u4e4f\u7279\u5f02\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5c06H&E\u56fe\u50cf\u8f6c\u6362\u4e3aIHC\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u6539\u8fdb\u4e86\u91d1\u5b57\u5854pix2pix\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5165\u65b9\u5dee\u60e9\u7f5a\u9879\u4ee5\u907f\u514d\u6a21\u5f0f\u5d29\u6e83\uff0c\u5e76\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u7ed3\u6784\u591a\u6837\u6027\u3002\u6a21\u578b\u7279\u522b\u9488\u5bf9HER2\u9633\u6027\uff08IHC 3+\uff09\u56fe\u50cf\u7684\u590d\u6742\u5f62\u6001\u53d8\u5316\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u5728BCI\u75c5\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728PSNR\u3001SSIM\u548cFID\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728HER2\u9633\u6027\u56fe\u50cf\u7684\u8f6c\u6362\u4e0a\u8868\u73b0\u7a81\u51fa\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u901a\u7528\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e2d\u4e5f\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684HER2\u8bca\u65ad\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u6a21\u578b\u5728\u8de8\u9886\u57df\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "paper_title_zh": "\u5c06H&E\u56fe\u50cf\u8f6c\u6362\u4e3aIHC\uff1a\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u60e9\u7f5a\u7684GAN\u7528\u4e8e\u7cbe\u51c6\u80bf\u7624\u5b66", "abstract_zh": "\u4eba\u7c7b\u8868\u76ae\u751f\u957f\u56e0\u5b50\u53d7\u4f532\uff08HER2\uff09\u5728\u4e73\u817a\u7ec6\u80de\u4e2d\u7684\u8fc7\u8868\u8fbe\u662fHER2\u9633\u6027\u4e73\u817a\u764c\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u8fd9\u79cd\u9ad8\u5ea6\u4fb5\u88ad\u6027\u7684\u4e9a\u578b\u9700\u8981\u7cbe\u786e\u8bca\u65ad\u548c\u9776\u5411\u6cbb\u7597\u3002\u514d\u75ab\u7ec4\u5316\uff08IHC\uff09\u662fHER2\u8bc4\u4f30\u7684\u6807\u51c6\u6280\u672f\uff0c\u4f46\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u9ad8\u5ea6\u4f9d\u8d56\u6297\u4f53\u9009\u62e9\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5e38\u89c4\u7684\u82cf\u6728\u7cbe\u548c\u4f0a\u7ea2\uff08H&E\uff09\u67d3\u8272\u867d\u7136\u666e\u53ca\uff0c\u4f46\u7f3a\u4e4fHER2\u7279\u5f02\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u8f6c\u6362\u6846\u67b6\uff0c\u80fd\u591f\u4eceH&E\u67d3\u8272\u7684\u7ec4\u7ec7\u6837\u672c\u4e2d\u751f\u6210\u9ad8\u4fdd\u771f\u7684IHC\u56fe\u50cf\uff0c\u4ece\u800c\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684HER2\u8bc4\u4f30\u3002\u901a\u8fc7\u6539\u8fdb\u91d1\u5b57\u5854pix2pix\u7684\u635f\u5931\u51fd\u6570\uff0c\u6211\u4eec\u7f13\u89e3\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u7684\u65b0\u578b\u60e9\u7f5a\u9879\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u7ed3\u6784\u591a\u6837\u6027\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u8f6c\u6362HER2\u9633\u6027\uff08IHC 3+\uff09\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u8fd9\u4e9b\u56fe\u50cf\u56e0\u5176\u590d\u6742\u7684\u5f62\u6001\u53d8\u5316\u800c\u6210\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u6311\u6218\u3002\u5728BCI\u75c5\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\uff08SSIM\uff09\u548c\u5f17\u96f7\u6b47\u8d77\u59cb\u8ddd\u79bb\uff08FID\uff09\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u662f\u5728\u51c6\u786e\u8f6c\u6362HER2\u9633\u6027\uff08IHC 3+\uff09\u56fe\u50cf\u65b9\u9762\u3002\u9664\u4e86\u533b\u5b66\u5f71\u50cf\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u901a\u7528\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u6f5c\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740AI\u9a71\u52a8\u7684\u7cbe\u51c6\u80bf\u7624\u5b66\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u4f20\u7edf\u7684HER2\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.18378", "pdf": "https://arxiv.org/pdf/2506.18378", "abs": "https://arxiv.org/abs/2506.18378", "authors": ["Haoneng Lin", "Cheng Xu", "Jing Qin"], "title": "Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review", "categories": ["eess.IV", "cs.CV"], "comment": "34 pages", "summary": "Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in\ncross-modal semantic understanding between visual and textual modalities. Given\nthe intrinsic need for multi-modal integration in clinical applications, VLMs\nhave emerged as a promising solution for a wide range of medical image analysis\ntasks. However, adapting general-purpose VLMs to medical domain poses numerous\nchallenges, such as large domain gaps, complicated pathological variations, and\ndiversity and uniqueness of different tasks. The central purpose of this review\nis to systematically summarize recent advances in adapting VLMs for medical\nimage analysis, analyzing current challenges, and recommending promising yet\nurgent directions for further investigations. We begin by introducing core\nlearning strategies for medical VLMs, including pretraining, fine-tuning, and\nprompt learning. We then categorize five major VLM adaptation strategies for\nmedical image analysis. These strategies are further analyzed across eleven\nmedical imaging tasks to illustrate their current practical implementations.\nFurthermore, we analyze key challenges that impede the effective adaptation of\nVLMs to clinical applications and discuss potential directions for future\nresearch. We also provide an open-access repository of related literature to\nfacilitate further research, available at\nhttps://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this\narticle can help researchers who are interested in harnessing VLMs in medical\nimage analysis tasks have a better understanding on their capabilities and\nlimitations, as well as current technical barriers, to promote their\ninnovative, robust, and safe application in clinical practice.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u9002\u5e94\u7b56\u7565\uff0c\u603b\u7ed3\u4e86\u6838\u5fc3\u5b66\u4e60\u65b9\u6cd5\u3001\u4e94\u5927\u9002\u5e94\u7b56\u7565\u53ca\u5176\u572811\u9879\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9700\u8981\u591a\u6a21\u6001\u6574\u5408\uff0c\u800c\u901a\u7528VLMs\u5728\u533b\u5b66\u9886\u57df\u7684\u9002\u5e94\u9762\u4e34\u9886\u57df\u5dee\u5f02\u5927\u3001\u75c5\u7406\u53d8\u5316\u590d\u6742\u7b49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u603b\u7ed3\u533b\u5b66VLMs\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u95ee\u9898\u5e76\u63a8\u8350\u7814\u7a76\u65b9\u5411\u3002", "method": "\u4ecb\u7ecd\u4e86\u533b\u5b66VLMs\u7684\u6838\u5fc3\u5b66\u4e60\u7b56\u7565\uff08\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u3001\u63d0\u793a\u5b66\u4e60\uff09\uff0c\u5206\u7c7b\u4e86\u4e94\u5927\u9002\u5e94\u7b56\u7565\uff0c\u5e76\u572811\u9879\u533b\u5b66\u4efb\u52a1\u4e2d\u5206\u6790\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u533b\u5b66VLMs\u7684\u9002\u5e94\u7b56\u7565\u53ca\u5176\u5e94\u7528\uff0c\u5206\u6790\u4e86\u963b\u788d\u4e34\u5e8a\u9002\u5e94\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5bf9VLMs\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u6280\u672f\u969c\u788d\u7684\u5168\u9762\u7406\u89e3\uff0c\u4ee5\u4fc3\u8fdb\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u521b\u65b0\u3001\u7a33\u5065\u548c\u5b89\u5168\u5e94\u7528\u3002", "paper_title_zh": "\u9a6f\u670d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a\u5168\u9762\u7efc\u8ff0", "abstract_zh": "\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u95f4\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\u3002\u9274\u4e8e\u4e34\u5e8a\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u6574\u5408\u7684\u5185\u5728\u9700\u6c42\uff0cVLMs\u5df2\u6210\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u5e7f\u6cdb\u4efb\u52a1\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5c06\u901a\u7528VLMs\u9002\u5e94\u533b\u5b66\u9886\u57df\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u9886\u57df\u5dee\u5f02\u5927\u3001\u75c5\u7406\u53d8\u5316\u590d\u6742\u4ee5\u53ca\u4efb\u52a1\u591a\u6837\u6027\u548c\u72ec\u7279\u6027\u3002\u672c\u7efc\u8ff0\u7684\u6838\u5fc3\u76ee\u7684\u662f\u7cfb\u7edf\u603b\u7ed3\u533b\u5b66VLMs\u9002\u5e94\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u5f53\u524d\u6311\u6218\uff0c\u5e76\u63a8\u8350\u672a\u6765\u7814\u7a76\u7684\u7d27\u8feb\u65b9\u5411\u3002\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u533b\u5b66VLMs\u7684\u6838\u5fc3\u5b66\u4e60\u7b56\u7565\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u63d0\u793a\u5b66\u4e60\u3002\u968f\u540e\u5206\u7c7b\u4e86\u4e94\u5927\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684VLM\u9002\u5e94\u7b56\u7565\uff0c\u5e76\u572811\u9879\u533b\u5b66\u4efb\u52a1\u4e2d\u5206\u6790\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5206\u6790\u4e86\u963b\u788dVLMs\u6709\u6548\u9002\u5e94\u4e34\u5e8a\u5e94\u7528\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u76f8\u5173\u6587\u732e\u5e93\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttps://github.com/haonenglin/Awesome-VLM-for-MIA\u3002\u672c\u6587\u65e8\u5728\u5e2e\u52a9\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5229\u7528VLMs\u611f\u5174\u8da3\u7684\u7814\u7a76\u8005\u66f4\u597d\u5730\u7406\u89e3\u5176\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u5f53\u524d\u6280\u672f\u969c\u788d\uff0c\u4ee5\u63a8\u52a8\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u521b\u65b0\u3001\u7a33\u5065\u548c\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2506.18143", "pdf": "https://arxiv.org/pdf/2506.18143", "abs": "https://arxiv.org/abs/2506.18143", "authors": ["Lancelot Blanchard", "Cameron Holt", "Joseph A. Paradiso"], "title": "AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "comment": "4 pages, 3 figures", "summary": "Vocals harmonizers are powerful tools to help solo vocalists enrich their\nmelodies with harmonically supportive voices. These tools exist in various\nforms, from commercially available pedals and software to custom-built systems,\neach employing different methods to generate harmonies. Traditional harmonizers\noften require users to manually specify a key or tonal center, while others\nallow pitch selection via an external keyboard-both approaches demanding some\ndegree of musical expertise. The AI Harmonizer introduces a novel approach by\nautonomously generating musically coherent four-part harmonies without\nrequiring prior harmonic input from the user. By integrating state-of-the-art\ngenerative AI techniques for pitch detection and voice modeling with\ncustom-trained symbolic music models, our system arranges any vocal melody into\nrich choral textures. In this paper, we present our methods, explore potential\napplications in performance and composition, and discuss future directions for\nreal-time implementations. While our system currently operates offline, we\nbelieve it represents a significant step toward AI-assisted vocal performance\nand expressive musical augmentation. We release our implementation on GitHub.", "AI": {"tldr": "AI Harmonizer\u662f\u4e00\u79cd\u521b\u65b0\u7684\u751f\u6210\u5f0f\u795e\u7ecf\u7b26\u53f7\u97f3\u4e50AI\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u4e3a\u72ec\u5531\u65cb\u5f8b\u751f\u6210\u56db\u90e8\u548c\u58f0\uff0c\u65e0\u9700\u7528\u6237\u9884\u5148\u8f93\u5165\u548c\u58f0\u4fe1\u606f\uff0c\u4ece\u800c\u6269\u5c55\u4e86\u4eba\u58f0\u8868\u8fbe\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u548c\u58f0\u751f\u6210\u5de5\u5177\u901a\u5e38\u9700\u8981\u7528\u6237\u624b\u52a8\u6307\u5b9a\u8c03\u6027\u6216\u901a\u8fc7\u5916\u90e8\u952e\u76d8\u9009\u62e9\u97f3\u9ad8\uff0c\u8fd9\u5bf9\u97f3\u4e50\u4e13\u4e1a\u77e5\u8bc6\u6709\u4e00\u5b9a\u8981\u6c42\u3002AI Harmonizer\u65e8\u5728\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u97f3\u4e50\u4e0a\u8fde\u8d2f\u7684\u548c\u58f0\uff0c\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u540c\u65f6\u4e30\u5bcc\u72ec\u5531\u7684\u8868\u73b0\u529b\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0fAI\u6280\u672f\uff08\u7528\u4e8e\u97f3\u9ad8\u68c0\u6d4b\u548c\u58f0\u97f3\u5efa\u6a21\uff09\u4e0e\u5b9a\u5236\u7684\u7b26\u53f7\u97f3\u4e50\u6a21\u578b\uff0c\u80fd\u591f\u5c06\u4efb\u4f55\u72ec\u5531\u65cb\u5f8b\u8f6c\u5316\u4e3a\u4e30\u5bcc\u7684\u5408\u5531\u7eb9\u7406\u3002", "result": "AI Harmonizer\u6210\u529f\u751f\u6210\u4e86\u97f3\u4e50\u4e0a\u8fde\u8d2f\u7684\u56db\u90e8\u548c\u58f0\uff0c\u5c55\u793a\u4e86\u5728\u8868\u6f14\u548c\u4f5c\u66f2\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002\u5c3d\u7ba1\u76ee\u524d\u7cfb\u7edf\u4ec5\u652f\u6301\u79bb\u7ebf\u64cd\u4f5c\uff0c\u4f46\u4e3a\u5b9e\u65f6AI\u8f85\u52a9\u4eba\u58f0\u8868\u6f14\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "AI Harmonizer\u4ee3\u8868\u4e86AI\u8f85\u52a9\u4eba\u58f0\u8868\u6f14\u548c\u97f3\u4e50\u8868\u8fbe\u589e\u5f3a\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u672a\u6765\u6709\u671b\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002\u7cfb\u7edf\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002", "paper_title_zh": "AI Harmonizer\uff1a\u5229\u7528\u751f\u6210\u5f0f\u795e\u7ecf\u7b26\u53f7\u97f3\u4e50AI\u7cfb\u7edf\u6269\u5c55\u4eba\u58f0\u8868\u8fbe", "abstract_zh": "\u4eba\u58f0\u548c\u58f0\u5668\u662f\u5e2e\u52a9\u72ec\u5531\u6b4c\u624b\u901a\u8fc7\u548c\u58f0\u4e30\u5bcc\u65cb\u5f8b\u7684\u5f3a\u5927\u5de5\u5177\u3002\u8fd9\u4e9b\u5de5\u5177\u5f62\u5f0f\u591a\u6837\uff0c\u4ece\u5546\u7528\u8e0f\u677f\u548c\u8f6f\u4ef6\u5230\u5b9a\u5236\u7cfb\u7edf\uff0c\u6bcf\u79cd\u90fd\u91c7\u7528\u4e0d\u540c\u7684\u548c\u58f0\u751f\u6210\u65b9\u6cd5\u3002\u4f20\u7edf\u548c\u58f0\u5668\u901a\u5e38\u9700\u8981\u7528\u6237\u624b\u52a8\u6307\u5b9a\u8c03\u6027\u6216\u901a\u8fc7\u5916\u90e8\u952e\u76d8\u9009\u62e9\u97f3\u9ad8\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5747\u9700\u4e00\u5b9a\u7684\u97f3\u4e50\u4e13\u4e1a\u77e5\u8bc6\u3002AI Harmonizer\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u97f3\u4e50\u4e0a\u8fde\u8d2f\u7684\u56db\u90e8\u548c\u58f0\uff0c\u65e0\u9700\u7528\u6237\u9884\u5148\u8f93\u5165\u548c\u58f0\u4fe1\u606f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0fAI\u6280\u672f\uff08\u7528\u4e8e\u97f3\u9ad8\u68c0\u6d4b\u548c\u58f0\u97f3\u5efa\u6a21\uff09\u4e0e\u5b9a\u5236\u7684\u7b26\u53f7\u97f3\u4e50\u6a21\u578b\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u80fd\u5c06\u4efb\u4f55\u72ec\u5531\u65cb\u5f8b\u8f6c\u5316\u4e3a\u4e30\u5bcc\u7684\u5408\u5531\u7eb9\u7406\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5728\u8868\u6f14\u548c\u4f5c\u66f2\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u65f6\u5b9e\u73b0\u7684\u672a\u6765\u65b9\u5411\u3002\u5c3d\u7ba1\u76ee\u524d\u7cfb\u7edf\u4ec5\u652f\u6301\u79bb\u7ebf\u64cd\u4f5c\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u8fc8\u5411AI\u8f85\u52a9\u4eba\u58f0\u8868\u6f14\u548c\u97f3\u4e50\u8868\u8fbe\u589e\u5f3a\u7684\u91cd\u8981\u4e00\u6b65\u3002\u6211\u4eec\u7684\u5b9e\u73b0\u5df2\u5728GitHub\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2506.18407", "pdf": "https://arxiv.org/pdf/2506.18407", "abs": "https://arxiv.org/abs/2506.18407", "authors": ["Yiyao Wang", "Bo Pan", "Ke Wang", "Han Liu", "Jinyuan Mao", "Yuxin Liu", "Minfeng Zhu", "Bo Zhang", "Weifeng Chen", "Xiuqi Huang", "Wei Chen"], "title": "What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Direct volume rendering (DVR) is a fundamental technique for visualizing\nvolumetric data, with transfer functions (TFs) playing a crucial role in\nextracting meaningful structures. However, designing effective TFs remains\nunintuitive due to the semantic gap between user intent and TF parameter space.\nResearchers have developed numerous TF optimization methods to bridge this gap.\nHowever, existing methods still face two challenges: large exploration space\nand weak generalizability. To address these issues, we propose What You Think\nis What You Get (WYTWYG) framework, which leveraging Multi-model Large Language\nModels (MLLMs) to guide the TF optimization based on user intent. Specifically,\nwe first introduce a novel TF optimization approach comprising two core\ncomponents: (1) an evolution-based explorer for effective exploration of the TF\nspace, and (2) a volume rendering quality evaluator based on MLLMs to provide\ngeneralizable visual guidance. We further propose a TF interactive design\nsystem based on this approach. We demonstrate the general applicability of our\nframework through three case studies, and validate the effectiveness of each\ncomponent through extensive experiments. Our code is available at:\nhttps://github.com/wyysteelhead/TFevolve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWYTWYG\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6307\u5bfc\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u7684\u4f20\u9012\u51fd\u6570\uff08TF\uff09\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTF\u8bbe\u8ba1\u4e2d\u7684\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u76f4\u63a5\u4f53\u79ef\u6e32\u67d3\uff08DVR\uff09\u4e2d\uff0c\u4f20\u9012\u51fd\u6570\uff08TF\uff09\u7684\u8bbe\u8ba1\u7531\u4e8e\u7528\u6237\u610f\u56fe\u4e0eTF\u53c2\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u800c\u663e\u5f97\u4e0d\u76f4\u89c2\u3002\u73b0\u6709TF\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "WYTWYG\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u57fa\u4e8e\u8fdb\u5316\u7684\u63a2\u7d22\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u63a2\u7d22TF\u7a7a\u95f4\uff1b(2) \u57fa\u4e8eMLLMs\u7684\u4f53\u79ef\u6e32\u67d3\u8d28\u91cf\u8bc4\u4f30\u5668\uff0c\u63d0\u4f9b\u901a\u7528\u89c6\u89c9\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6b64\u65b9\u6cd5\u7684TF\u4ea4\u4e92\u8bbe\u8ba1\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "WYTWYG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86TF\u8bbe\u8ba1\u4e2d\u7684\u63a2\u7d22\u7a7a\u95f4\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u7684TF\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u6240\u60f3\u5373\u6240\u5f97\uff1a\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6865\u63a5\u7528\u6237\u610f\u56fe\u4e0e\u4f20\u9012\u51fd\u6570\u8bbe\u8ba1", "abstract_zh": "\u76f4\u63a5\u4f53\u79ef\u6e32\u67d3\uff08DVR\uff09\u662f\u53ef\u89c6\u5316\u4f53\u79ef\u6570\u636e\u7684\u57fa\u672c\u6280\u672f\uff0c\u4f20\u9012\u51fd\u6570\uff08TFs\uff09\u5728\u63d0\u53d6\u6709\u610f\u4e49\u7ed3\u6784\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7528\u6237\u610f\u56fe\u4e0eTF\u53c2\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u8bbe\u8ba1\u6709\u6548\u7684TFs\u4ecd\u4e0d\u76f4\u89c2\u3002\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u8bb8\u591aTF\u4f18\u5316\u65b9\u6cd5\u4ee5\u5f25\u5408\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u6240\u60f3\u5373\u6240\u5f97\u201d\uff08WYTWYG\uff09\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u6307\u5bfcTF\u4f18\u5316\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TF\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u57fa\u4e8e\u8fdb\u5316\u7684\u63a2\u7d22\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u63a2\u7d22TF\u7a7a\u95f4\uff1b(2) \u57fa\u4e8eMLLMs\u7684\u4f53\u79ef\u6e32\u67d3\u8d28\u91cf\u8bc4\u4f30\u5668\uff0c\u63d0\u4f9b\u901a\u7528\u89c6\u89c9\u6307\u5bfc\u3002\u6211\u4eec\u8fd8\u57fa\u4e8e\u6b64\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2aTF\u4ea4\u4e92\u8bbe\u8ba1\u7cfb\u7edf\u3002\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728https://github.com/wyysteelhead/TFevolve\u83b7\u53d6\u3002"}}
{"id": "2506.18145", "pdf": "https://arxiv.org/pdf/2506.18145", "abs": "https://arxiv.org/abs/2506.18145", "authors": ["Zheng Zhan", "Liliang Ren", "Shuohang Wang", "Liyuan Liu", "Yang Liu", "Yeyun Gong", "Yanzhi Wang", "Yelong Shen"], "title": "Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Linear State Space Models (SSMs) offer remarkable performance gains in\nefficient sequence modeling, with constant inference-time computation and\nmemory complexity. Recent advances, such as Mamba, further enhance SSMs with\ninput-dependent gating and hardware-aware implementations, positioning them as\nstrong alternatives to Transformers for long sequence modeling. However,\nefficiently scaling the expressive power of SSMs, particularly with Mixture of\nExperts (MoE), remains challenging, as naive integration attempts often falter\nor degrade performance. In this work, we introduce Routing Mamba (RoM), a novel\napproach that scales SSM parameters using sparse mixtures of linear projection\nexperts. By sharing routing decisions between projection layers and lightweight\nsub-modules within Mamba across experts, RoM leverages synergies among linear\nprojection experts for effective and efficient sparse scaling of Mamba layers.\nAt a scale of 1.3B active parameters (10B total) and 16K training sequence\nlength, RoM achieves language modeling performance equivalent to a dense Mamba\nmodel requiring over 2.3x more active parameters, and demonstrates consistent\nperplexity across context lengths. Experimental results further show RoM\neffectively scales hybrid language models, yielding a 23% FLOPS saving compared\nto dense Mamba scaling for similar performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRouting Mamba\uff08RoM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u6df7\u5408\u7ebf\u6027\u6295\u5f71\u4e13\u5bb6\uff08MoE\uff09\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86Mamba\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRoM\u57281.3B\u6d3b\u8dc3\u53c2\u6570\uff0810B\u603b\u53c2\u6570\uff09\u548c16K\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0c\u6027\u80fd\u4e0e\u9700\u89812.3\u500d\u6d3b\u8dc3\u53c2\u6570\u7684\u5bc6\u96c6Mamba\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u8282\u7701\u4e8623%\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5728\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6709\u6548\u6269\u5c55\u5176\u8868\u8fbe\u80fd\u529b\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u6269\u5c55Mamba\u6a21\u578b\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "method": "RoM\u901a\u8fc7\u7a00\u758f\u6df7\u5408\u7ebf\u6027\u6295\u5f71\u4e13\u5bb6\uff08MoE\uff09\u6269\u5c55Mamba\u6a21\u578b\u7684\u53c2\u6570\uff0c\u5171\u4eab\u6295\u5f71\u5c42\u548c\u8f7b\u91cf\u5b50\u6a21\u5757\u7684\u8def\u7531\u51b3\u7b56\uff0c\u4ece\u800c\u5229\u7528\u7ebf\u6027\u6295\u5f71\u4e13\u5bb6\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a00\u758f\u6269\u5c55\u3002", "result": "\u57281.3B\u6d3b\u8dc3\u53c2\u6570\uff0810B\u603b\u53c2\u6570\uff09\u548c16K\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0cRoM\u7684\u6027\u80fd\u4e0e\u9700\u89812.3\u500d\u6d3b\u8dc3\u53c2\u6570\u7684\u5bc6\u96c6Mamba\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u8868\u73b0\u4e00\u81f4\uff0c\u5e76\u8282\u7701\u4e8623%\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "RoM\u6210\u529f\u6269\u5c55\u4e86Mamba\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\uff0c\u4e3a\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u6709\u529b\u4e14\u8d44\u6e90\u53cb\u597d\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "paper_title_zh": "\u8def\u7531Mamba\uff1a\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u6295\u5f71\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6269\u5c55", "abstract_zh": "\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5728\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u6052\u5b9a\u7684\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u3002\u6700\u8fd1\u7684Mamba\u7b49\u8fdb\u5c55\u901a\u8fc7\u8f93\u5165\u4f9d\u8d56\u7684\u95e8\u63a7\u548c\u786c\u4ef6\u611f\u77e5\u5b9e\u73b0\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86SSM\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u6210\u4e3a\u957f\u5e8f\u5217\u5efa\u6a21\u4e2dTransformer\u7684\u6709\u529b\u66ff\u4ee3\u8005\u3002\u7136\u800c\uff0c\u5982\u4f55\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u9ad8\u6548\u6269\u5c55SSM\u7684\u8868\u8fbe\u80fd\u529b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u7b80\u5355\u7684\u96c6\u6210\u5c1d\u8bd5\u5f80\u5f80\u5931\u8d25\u6216\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51faRouting Mamba\uff08RoM\uff09\uff0c\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u6df7\u5408\u7ebf\u6027\u6295\u5f71\u4e13\u5bb6\u6269\u5c55SSM\u53c2\u6570\u7684\u65b0\u65b9\u6cd5\u3002RoM\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u5c42\u548cMamba\u5185\u90e8\u8f7b\u91cf\u5b50\u6a21\u5757\u7684\u8def\u7531\u51b3\u7b56\uff0c\u5229\u7528\u7ebf\u6027\u6295\u5f71\u4e13\u5bb6\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u5b9e\u73b0\u4e86Mamba\u5c42\u7684\u9ad8\u6548\u7a00\u758f\u6269\u5c55\u3002\u57281.3B\u6d3b\u8dc3\u53c2\u6570\uff0810B\u603b\u53c2\u6570\uff09\u548c16K\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0cRoM\u7684\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u4e0e\u9700\u89812.3\u500d\u6d3b\u8dc3\u53c2\u6570\u7684\u5bc6\u96c6Mamba\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u8868\u73b0\u4e00\u81f4\u3002\u5b9e\u9a8c\u8fd8\u8868\u660e\uff0cRoM\u80fd\u6709\u6548\u6269\u5c55\u6df7\u5408\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u76f8\u4f3c\u6027\u80fd\u4e0b\u6bd4\u5bc6\u96c6Mamba\u6269\u5c55\u8282\u770123%\u7684\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2506.18443", "pdf": "https://arxiv.org/pdf/2506.18443", "abs": "https://arxiv.org/abs/2506.18443", "authors": ["Yang Lyu", "Zhenghao Zou", "Yanfeng Li", "Chunhui Zhao", "Quan Pan"], "title": "Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Achieving reliable ego motion estimation for agile robots, e.g., aerobatic\naircraft, remains challenging because most robot sensors fail to respond timely\nand clearly to highly dynamic robot motions, often resulting in measurement\nblurring, distortion, and delays. In this paper, we propose an IMU-free and\nfeature-association-free framework to achieve aggressive ego-motion velocity\nestimation of a robot platform in highly dynamic scenarios by combining two\ntypes of exteroceptive sensors, an event camera and a millimeter wave radar,\nFirst, we used instantaneous raw events and Doppler measurements to derive\nrotational and translational velocities directly. Without a sophisticated\nassociation process between measurement frames, the proposed method is more\nrobust in texture-less and structureless environments and is more\ncomputationally efficient for edge computing devices. Then, in the back-end, we\npropose a continuous-time state-space model to fuse the hybrid time-based and\nevent-based measurements to estimate the ego-motion velocity in a fixed-lagged\nsmoother fashion. In the end, we validate our velometer framework extensively\nin self-collected experiment datasets. The results indicate that our IMU-free\nand association-free ego motion estimation framework can achieve reliable and\nefficient velocity output in challenging environments. The source code,\nillustrative video and dataset are available at\nhttps://github.com/ZzhYgwh/TwistEstimator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u65e0IMU\u548c\u65e0\u7279\u5f81\u5173\u8054\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u52a8\u6001\u573a\u666f\u4e0b\u673a\u5668\u4eba\u5e73\u53f0\u7684\u654f\u6377\u81ea\u8fd0\u52a8\u901f\u5ea6\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u539f\u59cb\u4e8b\u4ef6\u548c\u591a\u666e\u52d2\u6d4b\u91cf\u8ba1\u7b97\u901f\u5ea6\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u5e27\u95f4\u5173\u8054\uff0c\u63d0\u9ad8\u4e86\u5728\u65e0\u7eb9\u7406\u548c\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u540e\u7aef\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u878d\u5408\u6d4b\u91cf\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u9ad8\u52a8\u6001\u673a\u5668\u4eba\uff08\u5982\u7279\u6280\u98de\u884c\u5668\uff09\u7684\u81ea\u8fd0\u52a8\u4f30\u8ba1\u9762\u4e34\u4f20\u611f\u5668\u54cd\u5e94\u4e0d\u53ca\u65f6\u6216\u6a21\u7cca\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u6d4b\u91cf\u6a21\u7cca\u3001\u5931\u771f\u548c\u5ef6\u8fdf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700IMU\u548c\u7279\u5f81\u5173\u8054\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u9ad8\u52a8\u6001\u573a\u666f\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u76f4\u63a5\u5229\u7528\u539f\u59cb\u4e8b\u4ef6\u548c\u591a\u666e\u52d2\u6d4b\u91cf\u8ba1\u7b97\u65cb\u8f6c\u548c\u5e73\u79fb\u901f\u5ea6\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u5e27\u95f4\u5173\u8054\u3002\u540e\u7aef\u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4ee5\u56fa\u5b9a\u6ede\u540e\u5e73\u6ed1\u65b9\u5f0f\u878d\u5408\u57fa\u4e8e\u65f6\u95f4\u548c\u4e8b\u4ef6\u7684\u6d4b\u91cf\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u901f\u5ea6\u8f93\u51fa\uff0c\u9002\u7528\u4e8e\u65e0\u7eb9\u7406\u548c\u65e0\u7ed3\u6784\u7684\u73af\u5883\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u5408\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65e0IMU\u548c\u65e0\u7279\u5f81\u5173\u8054\u7684\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6846\u67b6\u5728\u9ad8\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u654f\u6377\u673a\u5668\u4eba\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u96f7\u8fbe\u4e0e\u4e8b\u4ef6\u76f8\u673a\u878d\u5408\u7528\u4e8e\u654f\u6377\u673a\u5668\u4eba\u81ea\u8fd0\u52a8\u4f30\u8ba1", "abstract_zh": "\u4e3a\u9ad8\u52a8\u6001\u673a\u5668\u4eba\uff08\u5982\u7279\u6280\u98de\u884c\u5668\uff09\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u8fd0\u52a8\u4f30\u8ba1\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u4f20\u611f\u5668\u65e0\u6cd5\u53ca\u65f6\u6e05\u6670\u5730\u54cd\u5e94\u9ad8\u5ea6\u52a8\u6001\u7684\u8fd0\u52a8\uff0c\u5e38\u5bfc\u81f4\u6d4b\u91cf\u6a21\u7cca\u3001\u5931\u771f\u548c\u5ef6\u8fdf\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0IMU\u548c\u65e0\u7279\u5f81\u5173\u8054\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e24\u79cd\u5916\u90e8\u611f\u77e5\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u9ad8\u52a8\u6001\u573a\u666f\u4e0b\u673a\u5668\u4eba\u5e73\u53f0\u7684\u654f\u6377\u81ea\u8fd0\u52a8\u901f\u5ea6\u4f30\u8ba1\u3002\u9996\u5148\uff0c\u6211\u4eec\u5229\u7528\u77ac\u65f6\u539f\u59cb\u4e8b\u4ef6\u548c\u591a\u666e\u52d2\u6d4b\u91cf\u76f4\u63a5\u8ba1\u7b97\u65cb\u8f6c\u548c\u5e73\u79fb\u901f\u5ea6\u3002\u65e0\u9700\u590d\u6742\u7684\u6d4b\u91cf\u5e27\u95f4\u5173\u8054\u8fc7\u7a0b\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u7eb9\u7406\u548c\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4e14\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u66f4\u9ad8\u6548\u3002\u540e\u7aef\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4ee5\u56fa\u5b9a\u6ede\u540e\u5e73\u6ed1\u65b9\u5f0f\u878d\u5408\u57fa\u4e8e\u65f6\u95f4\u548c\u4e8b\u4ef6\u7684\u6d4b\u91cf\u6570\u636e\uff0c\u4f30\u8ba1\u81ea\u8fd0\u52a8\u901f\u5ea6\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u81ea\u6536\u96c6\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\u4e2d\u5e7f\u6cdb\u9a8c\u8bc1\u4e86\u901f\u5ea6\u8ba1\u6846\u67b6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65e0IMU\u548c\u65e0\u5173\u8054\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6846\u67b6\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u901f\u5ea6\u8f93\u51fa\u3002\u6e90\u4ee3\u7801\u3001\u6f14\u793a\u89c6\u9891\u548c\u6570\u636e\u96c6\u53ef\u5728https://github.com/ZzhYgwh/TwistEstimator\u83b7\u53d6\u3002"}}
{"id": "2506.18474", "pdf": "https://arxiv.org/pdf/2506.18474", "abs": "https://arxiv.org/abs/2506.18474", "authors": ["Atifa Kalsoom", "M. A. Iftikhar", "Amjad Ali", "Zubair Shah", "Shidin Balakrishnan", "Hazrat Ali"], "title": "A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "This is preprint of the paper submitted to Scientific Reports journal", "summary": "Retinal fundus images provide valuable insights into the human eye's interior\nstructure and crucial features, such as blood vessels, optic disk, macula, and\nfovea. However, accurate segmentation of retinal blood vessels can be\nchallenging due to imbalanced data distribution and varying vessel thickness.\nIn this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and\nbi-level class balancing scheme to achieve vessel segmentation in retinal\nfundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)\narchitecture and an empirical approach to balance the distribution of pixels\nacross vessel and non-vessel classes and within thin and thick vessels. Level-I\nis used for vessel/non-vessel balancing and Level-II is used for thick/thin\nvessel balancing. Additionally, pre-processing of the input retinal fundus\nimage is performed by Global Contrast Normalization (GCN), Contrast Limited\nAdaptive Histogram Equalization (CLAHE), and gamma corrections to increase\nintensity uniformity as well as to enhance the contrast between vessels and\nbackground pixels. The resulting balanced dataset is used for\nclassification-based segmentation of the retinal vascular tree. We evaluate the\nproposed scheme on standard retinal fundus images and achieve superior\nperformance measures, including an area under the ROC curve of 98.23%, Accuracy\nof 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also\ndemonstrate the method's efficacy through external cross-validation on STARE\nimages, confirming its generalization ability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u53cc\u5c42\u7c7b\u522b\u5e73\u8861\u65b9\u6848\u7684\u65b0\u578b\u7ba1\u9053BLCB-CNN\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e2d\u7684\u8840\u7ba1\u5206\u5272\uff0c\u901a\u8fc7\u5e73\u8861\u8840\u7ba1\u4e0e\u975e\u8840\u7ba1\u4ee5\u53ca\u539a\u8584\u8840\u7ba1\u7684\u50cf\u7d20\u5206\u5e03\uff0c\u7ed3\u5408\u9884\u5904\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e2d\u7684\u8840\u7ba1\u5206\u5272\u7531\u4e8e\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u548c\u8840\u7ba1\u539a\u5ea6\u53d8\u5316\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e73\u8861\u7c7b\u522b\u5206\u5e03\u5e76\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "BLCB-CNN\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u53cc\u5c42\u7c7b\u522b\u5e73\u8861\u65b9\u6848\uff08Level-I\u7528\u4e8e\u8840\u7ba1/\u975e\u8840\u7ba1\u5e73\u8861\uff0cLevel-II\u7528\u4e8e\u539a/\u8584\u8840\u7ba1\u5e73\u8861\uff09\uff0c\u5e76\u7ed3\u5408\u5168\u5c40\u5bf9\u6bd4\u5ea6\u5f52\u4e00\u5316\uff08GCN\uff09\u3001\u9650\u5236\u5bf9\u6bd4\u5ea6\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff08CLAHE\uff09\u548c\u4f3d\u9a6c\u6821\u6b63\u7b49\u9884\u5904\u7406\u6280\u672f\u3002", "result": "\u5728\u6807\u51c6\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e0a\uff0cBLCB-CNN\u5b9e\u73b0\u4e86ROC\u66f2\u7ebf\u4e0b\u9762\u79ef98.23%\u3001\u51c6\u786e\u738796.22%\u3001\u7075\u654f\u5ea681.57%\u548c\u7279\u5f02\u602797.65%\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u5728STARE\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BLCB-CNN\u901a\u8fc7\u53cc\u5c42\u7c7b\u522b\u5e73\u8861\u548c\u9884\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u578b\u7c7b\u522b\u5e73\u8861\u65b9\u6cd5\u7528\u4e8e\u4e0d\u5e73\u8861\u6570\u636e\u5206\u5272", "abstract_zh": "\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u63d0\u4f9b\u4e86\u4eba\u773c\u5185\u90e8\u7ed3\u6784\u548c\u5173\u952e\u7279\u5f81\uff08\u5982\u8840\u7ba1\u3001\u89c6\u76d8\u3001\u9ec4\u6591\u548c\u4e2d\u592e\u51f9\uff09\u7684\u5b9d\u8d35\u4fe1\u606f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u548c\u8840\u7ba1\u539a\u5ea6\u53d8\u5316\uff0c\u89c6\u7f51\u819c\u8840\u7ba1\u7684\u7cbe\u786e\u5206\u5272\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86BLCB-CNN\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u53cc\u5c42\u7c7b\u522b\u5e73\u8861\u65b9\u6848\u7684\u65b0\u578b\u7ba1\u9053\uff0c\u7528\u4e8e\u5b9e\u73b0\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e2d\u7684\u8840\u7ba1\u5206\u5272\u3002BLCB-CNN\u65b9\u6848\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\u548c\u4e00\u79cd\u7ecf\u9a8c\u65b9\u6cd5\uff0c\u5e73\u8861\u8840\u7ba1\u4e0e\u975e\u8840\u7ba1\u7c7b\u522b\u4ee5\u53ca\u539a\u8584\u8840\u7ba1\u4e4b\u95f4\u7684\u50cf\u7d20\u5206\u5e03\u3002Level-I\u7528\u4e8e\u8840\u7ba1/\u975e\u8840\u7ba1\u5e73\u8861\uff0cLevel-II\u7528\u4e8e\u539a/\u8584\u8840\u7ba1\u5e73\u8861\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5168\u5c40\u5bf9\u6bd4\u5ea6\u5f52\u4e00\u5316\uff08GCN\uff09\u3001\u9650\u5236\u5bf9\u6bd4\u5ea6\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff08CLAHE\uff09\u548c\u4f3d\u9a6c\u6821\u6b63\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4ee5\u589e\u5f3a\u5f3a\u5ea6\u5747\u5300\u6027\u548c\u8840\u7ba1\u4e0e\u80cc\u666f\u50cf\u7d20\u7684\u5bf9\u6bd4\u5ea6\u3002\u751f\u6210\u7684\u5e73\u8861\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u4e8e\u5206\u7c7b\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u6811\u5206\u5272\u3002\u6211\u4eec\u5728\u6807\u51c6\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6848\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u6307\u6807\uff0c\u5305\u62ecROC\u66f2\u7ebf\u4e0b\u9762\u79ef98.23%\u3001\u51c6\u786e\u738796.22%\u3001\u7075\u654f\u5ea681.57%\u548c\u7279\u5f02\u602797.65%\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5728STARE\u56fe\u50cf\u4e0a\u7684\u5916\u90e8\u4ea4\u53c9\u9a8c\u8bc1\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u8ba4\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.18165", "pdf": "https://arxiv.org/pdf/2506.18165", "abs": "https://arxiv.org/abs/2506.18165", "authors": ["Jaemoo Choi", "Yongxin Chen", "Molei Tao", "Guan-Horng Liu"], "title": "Non-equilibrium Annealed Adjoint Sampler", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 7 figures", "summary": "Recently, there has been significant progress in learning-based diffusion\nsamplers, which aim to sample from a given unnormalized density. These methods\ntypically follow one of two paradigms: (i) formulating sampling as an unbiased\nstochastic optimal control (SOC) problem using a canonical reference process,\nor (ii) refining annealed path measures through importance-weighted sampling.\nAlthough annealing approaches have advantages in guiding samples toward\nhigh-density regions, reliance on importance sampling leads to high variance\nand limited scalability in practice. In this paper, we introduce the\n\\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based\ndiffusion sampler that leverages annealed reference dynamics without resorting\nto importance sampling. NAAS employs a lean adjoint system inspired by adjoint\nmatching, enabling efficient and scalable training. We demonstrate the\neffectiveness of our approach across a range of tasks, including sampling from\nclassical energy landscapes and molecular Boltzmann distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6269\u6563\u91c7\u6837\u5668NAAS\uff0c\u901a\u8fc7\u7ed3\u5408\u9000\u706b\u53c2\u8003\u52a8\u529b\u5b66\u548c\u4f34\u968f\u5339\u914d\u6280\u672f\uff0c\u907f\u514d\u4e86\u91cd\u8981\u6027\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u91c7\u6837\u5668\u901a\u5e38\u4f9d\u8d56\u91cd\u8981\u6027\u91c7\u6837\uff0c\u5bfc\u81f4\u9ad8\u65b9\u5dee\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u9000\u706b\u53c2\u8003\u52a8\u529b\u5b66\u548c\u4f34\u968f\u5339\u914d\u6280\u672f\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u975e\u5e73\u8861\u9000\u706b\u4f34\u968f\u91c7\u6837\u5668\uff08NAAS\uff09\uff0c\u5229\u7528\u9000\u706b\u53c2\u8003\u52a8\u529b\u5b66\u548c\u4f34\u968f\u5339\u914d\u6280\u672f\uff0c\u907f\u514d\u91cd\u8981\u6027\u91c7\u6837\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNAAS\u5728\u7ecf\u5178\u80fd\u91cf\u666f\u89c2\u548c\u5206\u5b50\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "NAAS\u901a\u8fc7\u7ed3\u5408\u9000\u706b\u53c2\u8003\u52a8\u529b\u5b66\u548c\u4f34\u968f\u5339\u914d\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6269\u6563\u91c7\u6837\u65b9\u6cd5\u3002", "paper_title_zh": "\u975e\u5e73\u8861\u9000\u706b\u4f34\u968f\u91c7\u6837\u5668", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u6269\u6563\u91c7\u6837\u5668\u5728\u4ece\u672a\u5f52\u4e00\u5316\u5bc6\u5ea6\u4e2d\u91c7\u6837\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9075\u5faa\u4e24\u79cd\u8303\u5f0f\u4e4b\u4e00\uff1a(i) \u4f7f\u7528\u89c4\u8303\u53c2\u8003\u8fc7\u7a0b\u5c06\u91c7\u6837\u8868\u8ff0\u4e3a\u65e0\u504f\u968f\u673a\u6700\u4f18\u63a7\u5236\uff08SOC\uff09\u95ee\u9898\uff0c\u6216 (ii) \u901a\u8fc7\u91cd\u8981\u6027\u52a0\u6743\u91c7\u6837\u6539\u8fdb\u9000\u706b\u8def\u5f84\u6d4b\u5ea6\u3002\u5c3d\u7ba1\u9000\u706b\u65b9\u6cd5\u5728\u5f15\u5bfc\u6837\u672c\u5411\u9ad8\u5bc6\u5ea6\u533a\u57df\u79fb\u52a8\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5bf9\u91cd\u8981\u6027\u91c7\u6837\u7684\u4f9d\u8d56\u5bfc\u81f4\u5b9e\u8df5\u4e2d\u65b9\u5dee\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bSOC\u6269\u6563\u91c7\u6837\u5668\u2014\u2014\u975e\u5e73\u8861\u9000\u706b\u4f34\u968f\u91c7\u6837\u5668\uff08NAAS\uff09\uff0c\u5b83\u5229\u7528\u9000\u706b\u53c2\u8003\u52a8\u529b\u5b66\u800c\u65e0\u9700\u4f9d\u8d56\u91cd\u8981\u6027\u91c7\u6837\u3002NAAS\u91c7\u7528\u53d7\u4f34\u968f\u5339\u914d\u542f\u53d1\u7684\u8f7b\u91cf\u4f34\u968f\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u3002\u6211\u4eec\u5728\u5305\u62ec\u7ecf\u5178\u80fd\u91cf\u666f\u89c2\u548c\u5206\u5b50\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u91c7\u6837\u5728\u5185\u7684\u591a\u79cd\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18167", "pdf": "https://arxiv.org/pdf/2506.18167", "abs": "https://arxiv.org/abs/2506.18167", "authors": ["Constantin Venhoff", "Iv\u00e1n Arcuschin", "Philip Torr", "Arthur Conmy", "Neel Nanda"], "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to the development\nof thinking language models that generate extensive internal reasoning chains\nbefore producing responses. While these models achieve improved performance,\ncontrolling their reasoning processes remains challenging. This work presents a\nsteering approach for thinking LLMs by analyzing and manipulating specific\nreasoning behaviors in DeepSeek-R1-Distill models. Through a systematic\nexperiment on 500 tasks across 10 diverse categories, we identify several\nreasoning behaviors exhibited by thinking models, including expressing\nuncertainty, generating examples for hypothesis validation, and backtracking in\nreasoning chains. We demonstrate that these behaviors are mediated by linear\ndirections in the model's activation space and can be controlled using steering\nvectors. By extracting and applying these vectors, we provide a method to\nmodulate specific aspects of the model's reasoning process, such as its\ntendency to backtrack or express uncertainty. Our approach offers practical\ntools for steering reasoning processes in thinking models in a controlled and\ninterpretable manner. We validate our steering method using two\nDeepSeek-R1-Distill models, demonstrating consistent control across different\nmodel architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u64cd\u7eb5\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\uff08\u8f6c\u5411\u5411\u91cf\uff09\u6765\u63a7\u5236\u601d\u8003\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u5e76\u5728DeepSeek-R1-Distill\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u601d\u8003\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u5185\u90e8\u63a8\u7406\u94fe\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u63a7\u5236\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5e76\u64cd\u7eb5\u6a21\u578b\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7279\u5b9a\u65b9\u5411\uff0c\u5b9e\u73b0\u5bf9\u63a8\u7406\u884c\u4e3a\u7684\u53ef\u63a7\u8c03\u8282\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u5206\u6790\u4e86500\u4e2a\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u8bc6\u522b\u51fa\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u3001\u751f\u6210\u5047\u8bbe\u9a8c\u8bc1\u793a\u4f8b\u548c\u56de\u6eaf\u63a8\u7406\u94fe\u7b49\u884c\u4e3a\u3002\u8fd9\u4e9b\u884c\u4e3a\u7531\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\u4ecb\u5bfc\uff0c\u53ef\u901a\u8fc7\u8f6c\u5411\u5411\u91cf\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8f6c\u5411\u5411\u91cf\u80fd\u6709\u6548\u8c03\u8282\u6a21\u578b\u7684\u7279\u5b9a\u63a8\u7406\u884c\u4e3a\uff08\u5982\u56de\u6eaf\u6216\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u67b6\u6784\u7684DeepSeek-R1-Distill\u6a21\u578b\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63a7\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u8c03\u8282\u601d\u8003\u578b\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "paper_title_zh": "\u901a\u8fc7\u8f6c\u5411\u5411\u91cf\u7406\u89e3\u601d\u8003\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\u63a8\u52a8\u4e86\u601d\u8003\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u54cd\u5e94\u524d\u4f1a\u751f\u6210\u590d\u6742\u7684\u5185\u90e8\u63a8\u7406\u94fe\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u63a7\u5236\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u548c\u64cd\u7eb5DeepSeek-R1-Distill\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u63a8\u7406\u884c\u4e3a\u6765\u5b9e\u73b0\u8f6c\u5411\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf910\u4e2a\u4e0d\u540c\u7c7b\u522b\u7684500\u9879\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u601d\u8003\u578b\u6a21\u578b\u8868\u73b0\u51fa\u7684\u591a\u79cd\u63a8\u7406\u884c\u4e3a\uff0c\u5305\u62ec\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u3001\u751f\u6210\u5047\u8bbe\u9a8c\u8bc1\u793a\u4f8b\u4ee5\u53ca\u56de\u6eaf\u63a8\u7406\u94fe\u3002\u6211\u4eec\u8bc1\u660e\u8fd9\u4e9b\u884c\u4e3a\u7531\u6a21\u578b\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\u4ecb\u5bfc\uff0c\u5e76\u53ef\u901a\u8fc7\u8f6c\u5411\u5411\u91cf\u63a7\u5236\u3002\u901a\u8fc7\u63d0\u53d6\u548c\u5e94\u7528\u8fd9\u4e9b\u5411\u91cf\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u8c03\u8282\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u7279\u5b9a\u65b9\u9762\uff08\u5982\u56de\u6eaf\u503e\u5411\u6216\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\uff09\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u53ef\u63a7\u4e14\u53ef\u89e3\u91ca\u5730\u8c03\u8282\u601d\u8003\u578b\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002\u6211\u4eec\u4f7f\u7528\u4e24\u4e2aDeepSeek-R1-Distill\u6a21\u578b\u9a8c\u8bc1\u4e86\u8f6c\u5411\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u4e00\u81f4\u6027\u63a7\u5236\u3002"}}
{"id": "2506.18601", "pdf": "https://arxiv.org/pdf/2506.18601", "abs": "https://arxiv.org/abs/2506.18601", "authors": ["Denys Rozumnyi", "Jonathon Luiten", "Numair Khan", "Johannes Sch\u00f6nberger", "Peter Kontschieder"], "title": "BulletGen: Improving 4D Reconstruction with Bullet-Time Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Transforming casually captured, monocular videos into fully immersive dynamic\nexperiences is a highly ill-posed task, and comes with significant challenges,\ne.g., reconstructing unseen regions, and dealing with the ambiguity in\nmonocular depth estimation. In this work we introduce BulletGen, an approach\nthat takes advantage of generative models to correct errors and complete\nmissing information in a Gaussian-based dynamic scene representation. This is\ndone by aligning the output of a diffusion-based video generation model with\nthe 4D reconstruction at a single frozen \"bullet-time\" step. The generated\nframes are then used to supervise the optimization of the 4D Gaussian model.\nOur method seamlessly blends generative content with both static and dynamic\nscene components, achieving state-of-the-art results on both novel-view\nsynthesis, and 2D/3D tracking tasks.", "AI": {"tldr": "BulletGen\u5229\u7528\u751f\u6210\u6a21\u578b\u4fee\u6b63\u9ad8\u65af\u52a8\u6001\u573a\u666f\u8868\u793a\u4e2d\u7684\u9519\u8bef\u5e76\u8865\u5168\u7f3a\u5931\u4fe1\u606f\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5e27\u76d1\u77634D\u9ad8\u65af\u6a21\u578b\u4f18\u5316\uff0c\u5b9e\u73b0\u6c89\u6d78\u5f0f\u52a8\u6001\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u5c06\u5355\u76ee\u89c6\u9891\u8f6c\u5316\u4e3a\u6c89\u6d78\u5f0f\u52a8\u6001\u573a\u666f\u5b58\u5728\u4e25\u91cd\u6311\u6218\uff0c\u5982\u91cd\u5efa\u672a\u89c2\u6d4b\u533a\u57df\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u7cca\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faBulletGen\u65b9\u6cd5\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u5e27\u4e0e4D\u91cd\u5efa\u5bf9\u9f50\uff0c\u751f\u6210\u5185\u5bb9\u7528\u4e8e\u76d1\u77634D\u9ad8\u65af\u6a21\u578b\u4f18\u5316\uff0c\u878d\u5408\u9759\u6001\u4e0e\u52a8\u6001\u573a\u666f\u7ec4\u4ef6\u3002", "result": "\u5728\u65b0\u578b\u89c6\u56fe\u5408\u6210\u548c2D/3D\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6210\u679c\uff0c\u751f\u6210\u5185\u5bb9\u4e0e\u573a\u666f\u65e0\u7f1d\u878d\u5408\u3002", "conclusion": "BulletGen\u901a\u8fc7\u751f\u6210\u6a21\u578b\u4e0e4D\u91cd\u5efa\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u6c89\u6d78\u611f\u3002", "paper_title_zh": "BulletGen\uff1a\u901a\u8fc7\u5b50\u5f39\u65f6\u95f4\u751f\u6210\u6539\u8fdb4D\u91cd\u5efa", "abstract_zh": "\u5c06\u968f\u610f\u6355\u6349\u7684\u5355\u76ee\u89c6\u9891\u8f6c\u5316\u4e3a\u5b8c\u5168\u6c89\u6d78\u5f0f\u7684\u52a8\u6001\u4f53\u9a8c\u662f\u4e00\u9879\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u7684\u4efb\u52a1\uff0c\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u91cd\u5efa\u672a\u89c2\u6d4b\u533a\u57df\u548c\u5904\u7406\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6a21\u7cca\u6027\u3002\u672c\u6587\u63d0\u51faBulletGen\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u4fee\u6b63\u9ad8\u65af\u52a8\u6001\u573a\u666f\u8868\u793a\u4e2d\u7684\u9519\u8bef\u5e76\u8865\u5168\u7f3a\u5931\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f93\u51fa\u4e0e\u5355\u6b21\u51bb\u7ed3\u7684\u201c\u5b50\u5f39\u65f6\u95f4\u201d\u6b65\u9aa4\u76844D\u91cd\u5efa\u5bf9\u9f50\uff0c\u751f\u6210\u7684\u5e27\u7528\u4e8e\u76d1\u77634D\u9ad8\u65af\u6a21\u578b\u7684\u4f18\u5316\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u65e0\u7f1d\u878d\u5408\u4e86\u751f\u6210\u5185\u5bb9\u4e0e\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u7ec4\u4ef6\uff0c\u5728\u65b0\u578b\u89c6\u56fe\u5408\u6210\u548c2D/3D\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.18671", "pdf": "https://arxiv.org/pdf/2506.18671", "abs": "https://arxiv.org/abs/2506.18671", "authors": ["Yuqin Dai", "Wanlu Zhu", "Ronghui Li", "Xiu Li", "Zhenyu Zhang", "Jun Li", "Jian Yang"], "title": "TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": null, "summary": "Music-driven dance generation has garnered significant attention due to its\nwide range of industrial applications, particularly in the creation of group\nchoreography. During the group dance generation process, however, most existing\nmethods still face three primary issues: multi-dancer collisions, single-dancer\nfoot sliding and abrupt swapping in the generation of long group dance. In this\npaper, we propose TCDiff++, a music-driven end-to-end framework designed to\ngenerate harmonious group dance. Specifically, to mitigate multi-dancer\ncollisions, we utilize a dancer positioning embedding to better maintain the\nrelative positioning among dancers. Additionally, we incorporate a\ndistance-consistency loss to ensure that inter-dancer distances remain within\nplausible ranges. To address the issue of single-dancer foot sliding, we\nintroduce a swap mode embedding to indicate dancer swapping patterns and design\na Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For\nlong group dance generation, we present a long group diffusion sampling\nstrategy that reduces abrupt position shifts by injecting positional\ninformation into the noisy input. Furthermore, we integrate a Sequence Decoder\nlayer to enhance the model's ability to selectively process long sequences.\nExtensive experiments demonstrate that our TCDiff++ achieves state-of-the-art\nperformance, particularly in long-duration scenarios, ensuring high-quality and\ncoherent group dance generation.", "AI": {"tldr": "TCDiff++\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u8f68\u8ff9\u53ef\u63a7\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u548c\u8c10\u7684\u97f3\u4e50\u9a71\u52a8\u7fa4\u821e\uff0c\u89e3\u51b3\u4e86\u591a\u821e\u8005\u78b0\u649e\u3001\u5355\u821e\u8005\u811a\u6ed1\u548c\u957f\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u7a81\u7136\u4ea4\u6362\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u65b9\u6cd5\u5728\u7fa4\u821e\u751f\u6210\u4e2d\u5b58\u5728\u591a\u821e\u8005\u78b0\u649e\u3001\u5355\u821e\u8005\u811a\u6ed1\u548c\u957f\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u7a81\u7136\u4ea4\u6362\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u548c\u8c10\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "TCDiff++\u91c7\u7528\u821e\u8005\u5b9a\u4f4d\u5d4c\u5165\u548c\u8ddd\u79bb\u4e00\u81f4\u6027\u635f\u5931\u907f\u514d\u78b0\u649e\uff0c\u5f15\u5165\u4ea4\u6362\u6a21\u5f0f\u5d4c\u5165\u548c\u811a\u6b65\u9002\u914d\u5668\u51cf\u5c11\u811a\u6ed1\uff0c\u5e76\u901a\u8fc7\u957f\u7fa4\u6269\u6563\u91c7\u6837\u7b56\u7565\u548c\u5e8f\u5217\u89e3\u7801\u5c42\u4f18\u5316\u957f\u7fa4\u821e\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTCDiff++\u5728\u957f\u65f6\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u8fde\u8d2f\u7684\u7fa4\u821e\u3002", "conclusion": "TCDiff++\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u548c\u8c10\u4e14\u53ef\u63a7\u7684\u7fa4\u821e\u751f\u6210\u3002", "paper_title_zh": "TCDiff++\uff1a\u4e00\u79cd\u7aef\u5230\u7aef\u8f68\u8ff9\u53ef\u63a7\u6269\u6563\u6a21\u578b\u7528\u4e8e\u548c\u8c10\u97f3\u4e50\u9a71\u52a8\u7fa4\u821e\u751f\u6210", "abstract_zh": "\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u56e0\u5176\u5e7f\u6cdb\u7684\u5de5\u4e1a\u5e94\u7528\uff08\u5c24\u5176\u662f\u7fa4\u821e\u521b\u4f5c\uff09\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5728\u7fa4\u821e\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u591a\u821e\u8005\u78b0\u649e\u3001\u5355\u821e\u8005\u811a\u6ed1\u548c\u957f\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u7a81\u7136\u4ea4\u6362\u3002\u672c\u6587\u63d0\u51faTCDiff++\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u65e8\u5728\u751f\u6210\u548c\u8c10\u7684\u7fa4\u821e\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e3a\u907f\u514d\u591a\u821e\u8005\u78b0\u649e\uff0c\u6211\u4eec\u91c7\u7528\u821e\u8005\u5b9a\u4f4d\u5d4c\u5165\u4ee5\u66f4\u597d\u5730\u7ef4\u6301\u821e\u8005\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u5e76\u5f15\u5165\u8ddd\u79bb\u4e00\u81f4\u6027\u635f\u5931\u786e\u4fdd\u821e\u8005\u95f4\u8ddd\u5408\u7406\u3002\u4e3a\u89e3\u51b3\u5355\u821e\u8005\u811a\u6ed1\u95ee\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4ea4\u6362\u6a21\u5f0f\u5d4c\u5165\u6307\u793a\u821e\u8005\u4ea4\u6362\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u811a\u6b65\u9002\u914d\u5668\u4f18\u5316\u539f\u59cb\u52a8\u4f5c\u4ee5\u51cf\u5c11\u811a\u6ed1\u3002\u9488\u5bf9\u957f\u7fa4\u821e\u751f\u6210\uff0c\u6211\u4eec\u63d0\u51fa\u957f\u7fa4\u6269\u6563\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5411\u566a\u58f0\u8f93\u5165\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\u51cf\u5c11\u7a81\u7136\u4f4d\u7f6e\u53d8\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6574\u5408\u5e8f\u5217\u89e3\u7801\u5c42\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u957f\u5e8f\u5217\u7684\u9009\u62e9\u6027\u5904\u7406\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTCDiff++\u5728\u957f\u65f6\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u786e\u4fdd\u4e86\u9ad8\u8d28\u91cf\u4e14\u8fde\u8d2f\u7684\u7fa4\u821e\u751f\u6210\u3002"}}
{"id": "2506.18680", "pdf": "https://arxiv.org/pdf/2506.18680", "abs": "https://arxiv.org/abs/2506.18680", "authors": ["Anindita Ghosh", "Bing Zhou", "Rishabh Dabral", "Jian Wang", "Vladislav Golyanik", "Christian Theobalt", "Philipp Slusallek", "Chuan Guo"], "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 7 figures, 2 tables, accepted in ACM Siggraph 2025\n  conference track", "summary": "We present DuetGen, a novel framework for generating interactive two-person\ndances from music. The key challenge of this task lies in the inherent\ncomplexities of two-person dance interactions, where the partners need to\nsynchronize both with each other and with the music. Inspired by the recent\nadvances in motion synthesis, we propose a two-stage solution: encoding\ntwo-person motions into discrete tokens and then generating these tokens from\nmusic. To effectively capture intricate interactions, we represent both\ndancers' motions as a unified whole to learn the necessary motion tokens, and\nadopt a coarse-to-fine learning strategy in both the stages. Our first stage\nutilizes a VQ-VAE that hierarchically separates high-level semantic features at\na coarse temporal resolution from low-level details at a finer resolution,\nproducing two discrete token sequences at different abstraction levels.\nSubsequently, in the second stage, two generative masked transformers learn to\nmap music signals to these dance tokens: the first producing high-level\nsemantic tokens, and the second, conditioned on music and these semantic\ntokens, producing the low-level tokens. We train both transformers to learn to\npredict randomly masked tokens within the sequence, enabling them to\niteratively generate motion tokens by filling an empty token sequence during\ninference. Through the hierarchical masked modeling and dedicated interaction\nrepresentation, DuetGen achieves the generation of synchronized and interactive\ntwo-person dances across various genres. Extensive experiments and user studies\non a benchmark duet dance dataset demonstrate state-of-the-art performance of\nDuetGen in motion realism, music-dance alignment, and partner coordination.", "AI": {"tldr": "DuetGen\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63a9\u7801\u5efa\u6a21\u4ece\u97f3\u4e50\u751f\u6210\u4ea4\u4e92\u5f0f\u53cc\u4eba\u821e\u8e48\uff0c\u89e3\u51b3\u4e86\u53cc\u4eba\u821e\u8e48\u540c\u6b65\u548c\u97f3\u4e50\u5bf9\u9f50\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u53cc\u4eba\u821e\u8e48\u751f\u6210\u4efb\u52a1\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u821e\u4f34\u4e4b\u95f4\u4ee5\u53ca\u4e0e\u97f3\u4e50\u7684\u540c\u6b65\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u4ea4\u4e92\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u5c42\u5efa\u6a21\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u53cc\u4eba\u821e\u8e48\u7684\u6846\u67b6\u3002", "method": "DuetGen\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528VQ-VAE\u5c06\u53cc\u4eba\u52a8\u4f5c\u7f16\u7801\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u5206\u5c42\u5206\u79bb\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u548c\u4f4e\u7ea7\u7ec6\u8282\uff1b2) \u4f7f\u7528\u4e24\u4e2a\u751f\u6210\u5f0f\u63a9\u7801\u53d8\u6362\u5668\uff0c\u5206\u522b\u5c06\u97f3\u4e50\u4fe1\u53f7\u6620\u5c04\u5230\u9ad8\u7ea7\u548c\u4f4e\u7ea7\u821e\u8e48\u4ee4\u724c\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u9010\u6b65\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u57fa\u51c6\u53cc\u4eba\u821e\u8e48\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cDuetGen\u5728\u52a8\u4f5c\u771f\u5b9e\u6027\u3001\u97f3\u4e50\u821e\u8e48\u5bf9\u9f50\u548c\u821e\u4f34\u534f\u8c03\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DuetGen\u901a\u8fc7\u5206\u5c42\u63a9\u7801\u5efa\u6a21\u548c\u4e13\u7528\u4ea4\u4e92\u8868\u793a\uff0c\u6210\u529f\u751f\u6210\u4e86\u8de8\u591a\u79cd\u98ce\u683c\u7684\u540c\u6b65\u4e14\u4ea4\u4e92\u6027\u5f3a\u7684\u53cc\u4eba\u821e\u8e48\uff0c\u4e3a\u97f3\u4e50\u9a71\u52a8\u7684\u821e\u8e48\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "DuetGen\uff1a\u57fa\u4e8e\u5206\u5c42\u63a9\u7801\u5efa\u6a21\u7684\u97f3\u4e50\u9a71\u52a8\u53cc\u4eba\u821e\u8e48\u751f\u6210", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86DuetGen\uff0c\u4e00\u79cd\u4ece\u97f3\u4e50\u751f\u6210\u4ea4\u4e92\u5f0f\u53cc\u4eba\u821e\u8e48\u7684\u65b0\u9896\u6846\u67b6\u3002\u8be5\u4efb\u52a1\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u53cc\u4eba\u821e\u8e48\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u821e\u4f34\u9700\u8981\u5f7c\u6b64\u540c\u6b65\u5e76\u4e0e\u97f3\u4e50\u540c\u6b65\u3002\u53d7\u8fd0\u52a8\u5408\u6210\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff1a\u5c06\u53cc\u4eba\u52a8\u4f5c\u7f16\u7801\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u7136\u540e\u4ece\u97f3\u4e50\u751f\u6210\u8fd9\u4e9b\u4ee4\u724c\u3002\u4e3a\u4e86\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u4ea4\u4e92\uff0c\u6211\u4eec\u5c06\u4e24\u4f4d\u821e\u8005\u7684\u52a8\u4f5c\u8868\u793a\u4e3a\u4e00\u4e2a\u6574\u4f53\u6765\u5b66\u4e60\u5fc5\u8981\u7684\u52a8\u4f5c\u4ee4\u724c\uff0c\u5e76\u5728\u4e24\u4e2a\u9636\u6bb5\u4e2d\u91c7\u7528\u7531\u7c97\u5230\u7ec6\u7684\u5b66\u4e60\u7b56\u7565\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528VQ-VAE\uff0c\u5728\u7c97\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u5206\u5c42\u5206\u79bb\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u7ec6\u5206\u8fa8\u7387\u4e0b\u5206\u79bb\u4f4e\u7ea7\u7ec6\u8282\uff0c\u751f\u6210\u4e24\u4e2a\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u79bb\u6563\u4ee4\u724c\u5e8f\u5217\u3002\u968f\u540e\uff0c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u4e24\u4e2a\u751f\u6210\u5f0f\u63a9\u7801\u53d8\u6362\u5668\u5b66\u4e60\u5c06\u97f3\u4e50\u4fe1\u53f7\u6620\u5c04\u5230\u8fd9\u4e9b\u821e\u8e48\u4ee4\u724c\uff1a\u7b2c\u4e00\u4e2a\u751f\u6210\u9ad8\u7ea7\u8bed\u4e49\u4ee4\u724c\uff0c\u7b2c\u4e8c\u4e2a\u5728\u97f3\u4e50\u548c\u8fd9\u4e9b\u8bed\u4e49\u4ee4\u724c\u7684\u6761\u4ef6\u4e0b\u751f\u6210\u4f4e\u7ea7\u4ee4\u724c\u3002\u6211\u4eec\u8bad\u7ec3\u8fd9\u4e24\u4e2a\u53d8\u6362\u5668\u9884\u6d4b\u5e8f\u5217\u4e2d\u968f\u673a\u63a9\u7801\u7684\u4ee4\u724c\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u586b\u5145\u7a7a\u4ee4\u724c\u5e8f\u5217\u9010\u6b65\u751f\u6210\u52a8\u4f5c\u4ee4\u724c\u3002\u901a\u8fc7\u5206\u5c42\u63a9\u7801\u5efa\u6a21\u548c\u4e13\u7528\u4ea4\u4e92\u8868\u793a\uff0cDuetGen\u5b9e\u73b0\u4e86\u8de8\u591a\u79cd\u98ce\u683c\u7684\u540c\u6b65\u4e14\u4ea4\u4e92\u6027\u5f3a\u7684\u53cc\u4eba\u821e\u8e48\u751f\u6210\u3002\u5728\u57fa\u51c6\u53cc\u4eba\u821e\u8e48\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cDuetGen\u5728\u52a8\u4f5c\u771f\u5b9e\u6027\u3001\u97f3\u4e50\u821e\u8e48\u5bf9\u9f50\u548c\u821e\u4f34\u534f\u8c03\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.18191", "pdf": "https://arxiv.org/pdf/2506.18191", "abs": "https://arxiv.org/abs/2506.18191", "authors": ["Masudul Hasan Masud Bhuiyan", "Gianluca De Stefano", "Giancarlo Pellegrino", "Cristian-Alexandru Staicu"], "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGRAPHIA\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6539\u8fdbJavaScript\u8c03\u7528\u56fe\u7684\u6784\u5efa\uff0c\u901a\u8fc7\u94fe\u63a5\u9884\u6d4b\u586b\u8865\u73b0\u6709\u5de5\u5177\u7684\u9057\u6f0f\u8fb9\uff0c\u663e\u8457\u63d0\u5347\u53ec\u56de\u7387\u3002", "motivation": "JavaScript\u8c03\u7528\u56fe\u6784\u5efa\u56e0\u8bed\u8a00\u7279\u6027\u590d\u6742\uff0c\u73b0\u6709\u5de5\u5177\u65e2\u4e0d\u5065\u5168\u4e5f\u4e0d\u5b8c\u6574\uff0c\u5e38\u4ea7\u751f\u5047\u8fb9\u6216\u9057\u6f0f\u6709\u6548\u8fb9\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8f85\u52a9\u5de5\u5177\u8bc6\u522b\u9057\u6f0f\u7684\u8c03\u7528\u8fb9\u3002", "method": "GRAPHIA\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5168\u7a0b\u5e8f\u56fe\u4e0a\u7684\u94fe\u63a5\u9884\u6d4b\uff0c\u7ed3\u5408\u53e5\u6cd5\u548c\u8bed\u4e49\u8fb9\u8868\u793a\u4ee3\u7801\u5143\u7d20\u5173\u7cfb\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u975e\u5c40\u90e8\u5173\u7cfb\uff0c\u652f\u6301\u4ece\u9759\u6001\u5de5\u5177\u548c\u52a8\u6001\u6d4b\u8bd5\u4e2d\u5b66\u4e60\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u3002", "result": "\u572850\u4e2a\u6d41\u884cJavaScript\u5e93\u7684163K\u8c03\u7528\u8fb9\uff08150K\u9759\u6001\u548c13K\u52a8\u6001\uff09\u4e0a\u8bc4\u4f30\uff0cGRAPHIA\u572842%\u672a\u89e3\u6790\u8c03\u7528\u4e2d\u5c06\u6b63\u786e\u76ee\u6807\u6392\u540d\u7b2c\u4e00\uff0c72%\u6392\u540d\u524d\u4e94\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5206\u6790\u9700\u6c42\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u63d0\u5347JavaScript\u8c03\u7528\u56fe\u6784\u5efa\u7684\u53ec\u56de\u7387\uff0cGRAPHIA\u662f\u9996\u4e2a\u5c06GNN\u94fe\u63a5\u9884\u6d4b\u5e94\u7528\u4e8e\u591a\u6587\u4ef6\u7a0b\u5e8f\u56fe\u8fdb\u884c\u8fc7\u7a0b\u95f4\u5206\u6790\u7684\u5de5\u4f5c\u3002", "paper_title_zh": "\u6216\u8bb8\u53ef\u4ee5\u53eb\u6211\uff1a\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u589e\u5f3aJavaScript\u8c03\u7528\u56fe\u6784\u5efa", "abstract_zh": "\u9759\u6001\u5206\u6790\u5728\u53d1\u73b0\u6f0f\u6d1e\uff08\u5305\u62ec\u5b89\u5168\u95ee\u9898\uff09\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u800c\u6784\u5efa\u51c6\u786e\u7684\u8c03\u7528\u56fe\u662f\u9759\u6001\u5206\u6790\u7684\u6838\u5fc3\u6b65\u9aa4\u3002\u7136\u800c\uff0c\u7531\u4e8e\u96be\u4ee5\u5206\u6790\u7684\u8bed\u8a00\u7279\u6027\uff0c\u73b0\u6709JavaScript\u8c03\u7528\u56fe\u6784\u5efa\u7b97\u6cd5\u65e2\u4e0d\u5065\u5168\u4e5f\u4e0d\u5b8c\u6574\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u9ad8\u7ea7\u89e3\u51b3\u65b9\u6848\u4e5f\u4f1a\u4ea7\u751f\u5047\u8fb9\u6216\u9057\u6f0f\u6709\u6548\u8fb9\u3002\u672c\u6587\u901a\u8fc7\u8bc6\u522b\u9057\u6f0f\u7684\u8c03\u7528\u8fb9\u8f85\u52a9\u8fd9\u4e9b\u5de5\u5177\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5168\u7a0b\u5e8f\u56fe\u4e0a\u7684\u94fe\u63a5\u9884\u6d4b\uff0c\u91c7\u7528\u591a\u8fb9\u7c7b\u578b\u7684\u4e30\u5bcc\u8868\u793a\u3002\u6211\u4eec\u7684\u65b9\u6cd5GRAPHIA\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u65b0\u8fdb\u5c55\u5efa\u6a21\u4ee3\u7801\u5143\u7d20\u95f4\u7684\u975e\u5c40\u90e8\u5173\u7cfb\uff0c\u5177\u4f53\u63d0\u51fa\u7ed3\u5408\u53e5\u6cd5\u548c\u8bed\u4e49\u8fb9\u8868\u793aJavaScript\u7a0b\u5e8f\u3002GRAPHIA\u53ef\u4ece\u9759\u6001\u5de5\u5177\u548c\u52a8\u6001\u6d4b\u8bd5\u7684\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u4e2d\u5b66\u4e60\uff0c\u5305\u62ec\u6765\u81ea\u540c\u4e00\u6216\u4e0d\u540c\u9879\u76ee\u7684\u8c03\u7528\u8fb9\u3002\u7531\u4e8e\u8c03\u7528\u56fe\u7a00\u758f\uff0c\u6807\u51c6\u673a\u5668\u5b66\u4e60\u6307\u6807\uff08\u5982ROC\uff09\u4e0d\u9002\u7528\uff0c\u6211\u4eec\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u672a\u89e3\u6790\u8c03\u7528\u70b9\u6392\u540d\u51fd\u6570\u5b9a\u4e49\u6765\u8bc4\u4f30GRAPHIA\u3002\u572850\u4e2a\u6d41\u884cJavaScript\u5e93\u7684163K\u8c03\u7528\u8fb9\uff08150K\u9759\u6001\u548c13K\u52a8\u6001\uff09\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0cGRAPHIA\u6784\u5efa\u4e86\u5305\u542b6.6M\u7ed3\u6784\u8fb9\u548c386K\u8bed\u4e49\u8fb9\u7684\u7a0b\u5e8f\u56fe\u3002\u572842%\u672a\u89e3\u6790\u6848\u4f8b\u4e2d\uff0c\u5b83\u5c06\u6b63\u786e\u76ee\u6807\u6392\u540d\u7b2c\u4e00\uff0c72%\u6392\u540d\u524d\u4e94\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5206\u6790\u9700\u6c42\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u63d0\u5347JavaScript\u8c03\u7528\u56fe\u6784\u5efa\u7684\u53ec\u56de\u7387\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u5c06GNN\u94fe\u63a5\u9884\u6d4b\u5e94\u7528\u4e8e\u591a\u6587\u4ef6\u7a0b\u5e8f\u56fe\u8fdb\u884c\u8fc7\u7a0b\u95f4\u5206\u6790\u7684\u5de5\u4f5c\u3002"}}
{"id": "2506.18720", "pdf": "https://arxiv.org/pdf/2506.18720", "abs": "https://arxiv.org/abs/2506.18720", "authors": ["Daniel M. Lang", "Richard Osuala", "Veronika Spieker", "Karim Lekadir", "Rickmer Braren", "Julia A. Schnabel"], "title": "Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025", "summary": "Synthetic contrast enhancement offers fast image acquisition and eliminates\nthe need for intravenous injection of contrast agent. This is particularly\nbeneficial for breast imaging, where long acquisition times and high cost are\nsignificantly limiting the applicability of magnetic resonance imaging (MRI) as\na widespread screening modality. Recent studies have demonstrated the\nfeasibility of synthetic contrast generation. However, current state-of-the-art\n(SOTA) methods lack sufficient measures for consistent temporal evolution.\nNeural cellular automata (NCA) offer a robust and lightweight architecture to\nmodel evolving patterns between neighboring cells or pixels. In this work we\nintroduce TeNCA (Temporal Neural Cellular Automata), which extends and further\nrefines NCAs to effectively model temporally sparse, non-uniformly sampled\nimaging data. To achieve this, we advance the training strategy by enabling\nadaptive loss computation and define the iterative nature of the method to\nresemble a physical progression in time. This conditions the model to learn a\nphysiologically plausible evolution of contrast enhancement. We rigorously\ntrain and test TeNCA on a diverse breast MRI dataset and demonstrate its\neffectiveness, surpassing the performance of existing methods in generation of\nimages that align with ground truth post-contrast sequences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTeNCA\uff08\u65f6\u95f4\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u4e73\u817aMRI\u4e2d\u7684\u5bf9\u6bd4\u589e\u5f3a\uff0c\u901a\u8fc7\u6539\u8fdb\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u751f\u6210\u4e0e\u771f\u5b9e\u5bf9\u6bd4\u589e\u5f3a\u5e8f\u5217\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e73\u817aMRI\u7684\u5408\u6210\u5bf9\u6bd4\u589e\u5f3a\u6280\u672f\u53ef\u4ee5\u7f29\u77ed\u6210\u50cf\u65f6\u95f4\u5e76\u907f\u514d\u6ce8\u5c04\u5bf9\u6bd4\u5242\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u6f14\u5316\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdbNCA\u67b6\u6784\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faTeNCA\u65b9\u6cd5\uff0c\u6269\u5c55\u5e76\u4f18\u5316\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u5931\u8ba1\u7b97\u548c\u6a21\u62df\u7269\u7406\u65f6\u95f4\u6f14\u8fdb\u7684\u8fed\u4ee3\u8bad\u7ec3\u7b56\u7565\uff0c\u5efa\u6a21\u7a00\u758f\u4e14\u975e\u5747\u5300\u91c7\u6837\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u4e73\u817aMRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86TeNCA\u7684\u6709\u6548\u6027\uff0c\u5176\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u771f\u5b9e\u5bf9\u6bd4\u589e\u5f3a\u5e8f\u5217\u9ad8\u5ea6\u4e00\u81f4\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TeNCA\u4e3a\u4e73\u817aMRI\u7684\u5408\u6210\u5bf9\u6bd4\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u751f\u7406\u5b66\u5408\u7406\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "\u65f6\u95f4\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff1a\u5728\u4e73\u817aMRI\u5bf9\u6bd4\u589e\u5f3a\u5efa\u6a21\u4e2d\u7684\u5e94\u7528", "abstract_zh": "\u5408\u6210\u5bf9\u6bd4\u589e\u5f3a\u6280\u672f\u80fd\u591f\u5feb\u901f\u83b7\u53d6\u56fe\u50cf\u5e76\u907f\u514d\u9759\u8109\u6ce8\u5c04\u5bf9\u6bd4\u5242\uff0c\u8fd9\u5bf9\u4e73\u817a\u6210\u50cf\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u957f\u6210\u50cf\u65f6\u95f4\u548c\u9ad8\u6210\u672c\u9650\u5236\u4e86MRI\u4f5c\u4e3a\u5e7f\u6cdb\u7b5b\u67e5\u624b\u6bb5\u7684\u9002\u7528\u6027\u3002\u8fd1\u671f\u7814\u7a76\u8bc1\u660e\u4e86\u5408\u6210\u5bf9\u6bd4\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u6f14\u5316\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u4e14\u9c81\u68d2\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5efa\u6a21\u76f8\u90bb\u7ec6\u80de\u6216\u50cf\u7d20\u95f4\u7684\u6f14\u5316\u6a21\u5f0f\u3002\u672c\u6587\u63d0\u51faTeNCA\uff08\u65f6\u95f4\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff09\uff0c\u6269\u5c55\u5e76\u4f18\u5316\u4e86NCA\uff0c\u4ee5\u6709\u6548\u5efa\u6a21\u7a00\u758f\u4e14\u975e\u5747\u5300\u91c7\u6837\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6539\u8fdb\u4e86\u8bad\u7ec3\u7b56\u7565\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u635f\u5931\u8ba1\u7b97\uff0c\u5e76\u5b9a\u4e49\u65b9\u6cd5\u7684\u8fed\u4ee3\u7279\u6027\u4ee5\u6a21\u62df\u7269\u7406\u65f6\u95f4\u6f14\u8fdb\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u5b66\u4e60\u751f\u7406\u5b66\u5408\u7406\u7684\u5bf9\u6bd4\u589e\u5f3a\u6f14\u5316\u3002\u6211\u4eec\u5728\u591a\u6837\u5316\u7684\u4e73\u817aMRI\u6570\u636e\u96c6\u4e0a\u4e25\u683c\u8bad\u7ec3\u548c\u6d4b\u8bd5TeNCA\uff0c\u8bc1\u660e\u5176\u5728\u751f\u6210\u4e0e\u771f\u5b9e\u5bf9\u6bd4\u589e\u5f3a\u5e8f\u5217\u4e00\u81f4\u7684\u56fe\u50cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18193", "pdf": "https://arxiv.org/pdf/2506.18193", "abs": "https://arxiv.org/abs/2506.18193", "authors": ["Zih-Hao Huang", "You-Teng Lin", "Hung-Hsuan Chen"], "title": "DeInfoReg: A Decoupled Learning Framework for Better Training Throughput", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "This paper introduces Decoupled Supervised Learning with Information\nRegularization (DeInfoReg), a novel approach that transforms a long gradient\nflow into multiple shorter ones, thereby mitigating the vanishing gradient\nproblem. Integrating a pipeline strategy, DeInfoReg enables model\nparallelization across multiple GPUs, significantly improving training\nthroughput. We compare our proposed method with standard backpropagation and\nother gradient flow decomposition techniques. Extensive experiments on diverse\ntasks and datasets demonstrate that DeInfoReg achieves superior performance and\nbetter noise resistance than traditional BP models and efficiently utilizes\nparallel computing resources. The code for reproducibility is available at:\nhttps://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.", "AI": {"tldr": "DeInfoReg\u662f\u4e00\u79cd\u65b0\u578b\u89e3\u8026\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u957f\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u591a\u4e2a\u77ed\u68af\u5ea6\u6d41\uff0c\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u6d41\u6c34\u7ebf\u7b56\u7565\u5b9e\u73b0\u591aGPU\u5e76\u884c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u5728\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u65f6\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u5145\u5206\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u8d44\u6e90\u3002DeInfoReg\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u548c\u4fe1\u606f\u6b63\u5219\u5316\uff0c\u4f18\u5316\u68af\u5ea6\u6d41\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "method": "DeInfoReg\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u957f\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u591a\u4e2a\u77ed\u68af\u5ea6\u6d41\uff0c\u5e76\u7ed3\u5408\u4fe1\u606f\u6b63\u5219\u5316\u6280\u672f\u3002\u901a\u8fc7\u6d41\u6c34\u7ebf\u7b56\u7565\uff0c\u5b9e\u73b0\u6a21\u578b\u5728\u591aGPU\u4e0a\u7684\u5e76\u884c\u5316\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeInfoReg\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfBP\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u6297\u566a\u80fd\u529b\u548c\u66f4\u9ad8\u7684\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "conclusion": "DeInfoReg\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u548c\u4fe1\u606f\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u5145\u5206\u5229\u7528\u4e86\u5e76\u884c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "DeInfoReg\uff1a\u4e00\u79cd\u89e3\u8026\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u6b63\u5219\u5316\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u89e3\u8026\u76d1\u7763\u5b66\u4e60\u4e0e\u4fe1\u606f\u6b63\u5219\u5316\u201d\uff08DeInfoReg\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u957f\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u591a\u4e2a\u77ed\u68af\u5ea6\u6d41\uff0c\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002\u7ed3\u5408\u6d41\u6c34\u7ebf\u7b56\u7565\uff0cDeInfoReg\u5b9e\u73b0\u4e86\u6a21\u578b\u5728\u591aGPU\u4e0a\u7684\u5e76\u884c\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u541e\u5410\u91cf\u3002\u6211\u4eec\u5bf9\u6bd4\u4e86\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u548c\u5176\u4ed6\u68af\u5ea6\u6d41\u5206\u89e3\u6280\u672f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeInfoReg\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfBP\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u6297\u566a\u80fd\u529b\uff0c\u5e76\u80fd\u9ad8\u6548\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u8d44\u6e90\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/\u3002"}}
{"id": "2506.18725", "pdf": "https://arxiv.org/pdf/2506.18725", "abs": "https://arxiv.org/abs/2506.18725", "authors": ["Anirban Ghosh", "Ian Dahlin", "Ayan Dutta"], "title": "TDACloud: Point Cloud Recognition Using Topological Data Analysis", "categories": ["cs.RO", "cs.CG", "cs.CV"], "comment": null, "summary": "Point cloud-based object/place recognition remains a problem of interest in\napplications such as autonomous driving, scene reconstruction, and\nlocalization. Extracting meaningful local descriptors from a query point cloud\nthat can be matched with the descriptors of the collected point clouds is a\nchallenging problem. Furthermore, when the query point cloud is noisy or has\nbeen transformed (e.g., rotated), it adds to the complexity. To this end, we\npropose a novel methodology, named TDACloud, using Topological Data Analysis\n(TDA) for local descriptor extraction from a point cloud, which does not need\nresource-intensive GPU-based machine learning training. More specifically, we\nused the ATOL vectorization method to generate vectors for point clouds. Unlike\nvoxelization, our proposed technique can take raw point clouds as inputs and\noutputs a fixed-size TDA-descriptor vector. To test the quality of the proposed\nTDACloud technique, we have implemented it on multiple real-world (e.g., Oxford\nRobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for\nobject and place recognition. We have also tested TDACloud on noisy and\ntransformed test cases where the query point cloud has been scaled, translated,\nor rotated. Our results demonstrate high recognition accuracies in noisy\nconditions and large-scale real-world place recognition while outperforming the\nbaselines by up to approximately 14%.", "AI": {"tldr": "TDACloud\u662f\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u7684\u70b9\u4e91\u8bc6\u522b\u65b9\u6cd5\uff0c\u65e0\u9700GPU\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u901a\u8fc7ATOL\u5411\u91cf\u5316\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u63cf\u8ff0\u7b26\u5411\u91cf\uff0c\u5728\u566a\u58f0\u548c\u53d8\u6362\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u70b9\u4e91\u8bc6\u522b\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u573a\u666f\u91cd\u5efa\u548c\u5b9a\u4f4d\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u548c\u53d8\u6362\uff08\u5982\u65cb\u8f6c\uff09\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u70b9\u4e91\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTDACloud\u65b9\u6cd5\uff0c\u5229\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u63d0\u53d6\u70b9\u4e91\u7684\u5c40\u90e8\u63cf\u8ff0\u7b26\uff0c\u91c7\u7528ATOL\u5411\u91cf\u5316\u6280\u672f\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u63cf\u8ff0\u7b26\u5411\u91cf\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u70b9\u4e91\u8f93\u5165\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\uff08\u5982Oxford RobotCar\u3001KITTI-360\uff09\u548c\u4eff\u771f\uff08\u5982ShapeNet\uff09\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cTDACloud\u5728\u566a\u58f0\u548c\u53d8\u6362\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9ad8\u8bc6\u522b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7ea614%\u3002", "conclusion": "TDACloud\u901a\u8fc7TDA\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u70b9\u4e91\u8bc6\u522b\u65b9\u6cd5\uff0c\u65e0\u9700GPU\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u7684\u5bf9\u8c61\u548c\u573a\u666f\u8bc6\u522b\u3002", "paper_title_zh": "TDACloud\uff1a\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\u7684\u70b9\u4e91\u8bc6\u522b", "abstract_zh": "\u70b9\u4e91\u8bc6\u522b\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u573a\u666f\u91cd\u5efa\u548c\u5b9a\u4f4d\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4ece\u67e5\u8be2\u70b9\u4e91\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u5c40\u90e8\u63cf\u8ff0\u7b26\u4ee5\u4e0e\u6536\u96c6\u7684\u70b9\u4e91\u63cf\u8ff0\u7b26\u5339\u914d\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u70b9\u4e91\u5b58\u5728\u566a\u58f0\u6216\u53d8\u6362\uff08\u5982\u65cb\u8f6c\uff09\u65f6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTDACloud\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u4ece\u70b9\u4e91\u4e2d\u63d0\u53d6\u5c40\u90e8\u63cf\u8ff0\u7b26\uff0c\u65e0\u9700\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u7684GPU\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4f7f\u7528ATOL\u5411\u91cf\u5316\u65b9\u6cd5\u4e3a\u70b9\u4e91\u751f\u6210\u5411\u91cf\u3002\u4e0e\u4f53\u7d20\u5316\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u6280\u672f\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u70b9\u4e91\u8f93\u5165\uff0c\u5e76\u8f93\u51fa\u56fa\u5b9a\u5927\u5c0f\u7684TDA\u63cf\u8ff0\u7b26\u5411\u91cf\u3002\u4e3a\u9a8c\u8bc1TDACloud\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\uff08\u5982Oxford RobotCar\u3001KITTI-360\uff09\u548c\u4eff\u771f\uff08\u5982ShapeNet\uff09\u70b9\u4e91\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5bf9\u8c61\u548c\u573a\u666f\u8bc6\u522b\u6d4b\u8bd5\uff0c\u5e76\u5728\u566a\u58f0\u548c\u53d8\u6362\uff08\u5982\u7f29\u653e\u3001\u5e73\u79fb\u6216\u65cb\u8f6c\uff09\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\uff0cTDACloud\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u548c\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u573a\u666f\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7ea614%\u3002"}}
{"id": "2506.18195", "pdf": "https://arxiv.org/pdf/2506.18195", "abs": "https://arxiv.org/abs/2506.18195", "authors": ["Giacomo Como", "Fabio Fagnani", "Anton Proskurnikov"], "title": "Wisdom of Crowds Through Myopic Self-Confidence Adaptation", "categories": ["math.OC", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "physics.soc-ph"], "comment": null, "summary": "The wisdom of crowds is an umbrella term for phenomena suggesting that the\ncollective judgment or decision of a large group can be more accurate than the\nindividual judgments or decisions of the group members. A well-known example\nillustrating this concept is the competition at a country fair described by\nGalton, where the median value of the individual guesses about the weight of an\nox resulted in an astonishingly accurate estimate of the actual weight. This\nphenomenon resembles classical results in probability theory and relies on\nindependent decision-making. The accuracy of the group's final decision can be\nsignificantly reduced if the final agents' opinions are driven by a few\ninfluential agents.\n  In this paper, we consider a group of agents who initially possess\nuncorrelated and unbiased noisy measurements of a common state of the world.\nAssume these agents iteratively update their estimates according to a simple\nnon-Bayesian learning rule, commonly known in mathematical sociology as the\nFrench-DeGroot dynamics or iterative opinion pooling. As a result of this\niterative distributed averaging process, each agent arrives at an asymptotic\nestimate of the state of the world, with the variance of this estimate\ndetermined by the matrix of weights the agents assign to each other. Every\nagent aims at minimizing the variance of her asymptotic estimate of the state\nof the world; however, such variance is also influenced by the weights\nallocated by other agents. To achieve the best possible estimate, the agents\nmust then solve a game-theoretic, multi-objective optimization problem defined\nby the available sets of influence weights. We characterize both the Pareto\nfrontier and the set of Nash equilibria in the resulting game. Additionally, we\nexamine asynchronous best-response dynamics for the group of agents and prove\ntheir convergence to the set of strict Nash equilibria.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7fa4\u4f53\u667a\u6167\u73b0\u8c61\uff0c\u63a2\u8ba8\u4e86\u5728\u8fed\u4ee3\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u4e2a\u4f53\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u81ea\u4fe1\u6743\u91cd\u6765\u4f18\u5316\u96c6\u4f53\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u6700\u4f18\u6743\u91cd\u5206\u914d\uff0c\u5e76\u8bc1\u660e\u4e86\u5f02\u6b65\u6700\u4f73\u54cd\u5e94\u52a8\u6001\u4f1a\u6536\u655b\u5230\u4e25\u683c\u7eb3\u4ec0\u5747\u8861\u3002", "motivation": "\u7fa4\u4f53\u667a\u6167\u73b0\u8c61\u8868\u660e\uff0c\u96c6\u4f53\u51b3\u7b56\u53ef\u80fd\u6bd4\u4e2a\u4f53\u51b3\u7b56\u66f4\u51c6\u786e\uff0c\u4f46\u82e5\u4e2a\u4f53\u610f\u89c1\u53d7\u5c11\u6570\u5f71\u54cd\u8005\u4e3b\u5bfc\uff0c\u51c6\u786e\u6027\u4f1a\u964d\u4f4e\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4e2a\u4f53\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u81ea\u4fe1\u6743\u91cd\u6765\u4f18\u5316\u96c6\u4f53\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u5047\u8bbe\u4e2a\u4f53\u521d\u59cb\u62e5\u6709\u5bf9\u4e16\u754c\u72b6\u6001\u7684\u72ec\u7acb\u566a\u58f0\u6d4b\u91cf\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u89c4\u5219\uff08\u5982\u6cd5\u56fd-\u5fb7\u683c\u9c81\u7279\u52a8\u6001\uff09\u66f4\u65b0\u4f30\u8ba1\u503c\u3002\u4e2a\u4f53\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u4f18\u5316\u6743\u91cd\u5206\u914d\uff0c\u4ee5\u6700\u5c0f\u5316\u4f30\u8ba1\u65b9\u5dee\u3002", "result": "\u7814\u7a76\u523b\u753b\u4e86\u535a\u5f08\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u548c\u7eb3\u4ec0\u5747\u8861\u96c6\uff0c\u5e76\u8bc1\u660e\u5f02\u6b65\u6700\u4f73\u54cd\u5e94\u52a8\u6001\u4f1a\u6536\u655b\u5230\u4e25\u683c\u7eb3\u4ec0\u5747\u8861\u3002", "conclusion": "\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u4e2a\u4f53\u53ef\u4ee5\u4f18\u5316\u81ea\u4fe1\u6743\u91cd\u5206\u914d\uff0c\u4ece\u800c\u63d0\u9ad8\u96c6\u4f53\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff0c\u5f02\u6b65\u52a8\u6001\u6536\u655b\u6027\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "paper_title_zh": "\u901a\u8fc7\u77ed\u89c6\u81ea\u4fe1\u8c03\u6574\u5b9e\u73b0\u7684\u7fa4\u4f53\u667a\u6167", "abstract_zh": "\u7fa4\u4f53\u667a\u6167\u662f\u4e00\u4e2a\u6cdb\u6307\u73b0\u8c61\uff0c\u8868\u660e\u5927\u89c4\u6a21\u7fa4\u4f53\u7684\u96c6\u4f53\u5224\u65ad\u6216\u51b3\u7b56\u53ef\u80fd\u6bd4\u4e2a\u4f53\u5224\u65ad\u6216\u51b3\u7b56\u66f4\u51c6\u786e\u3002\u4e00\u4e2a\u8457\u540d\u7684\u4f8b\u5b50\u662fGalton\u63cf\u8ff0\u7684\u4e61\u6751\u96c6\u5e02\u6bd4\u8d5b\uff0c\u5176\u4e2d\u4e2a\u4f53\u5bf9\u725b\u91cd\u91cf\u7684\u731c\u6d4b\u4e2d\u4f4d\u6570\u60ca\u4eba\u5730\u63a5\u8fd1\u5b9e\u9645\u91cd\u91cf\u3002\u8fd9\u79cd\u73b0\u8c61\u7c7b\u4f3c\u4e8e\u6982\u7387\u8bba\u4e2d\u7684\u7ecf\u5178\u7ed3\u679c\uff0c\u4f9d\u8d56\u4e8e\u72ec\u7acb\u51b3\u7b56\u3002\u82e5\u6700\u7ec8\u4ee3\u7406\u610f\u89c1\u53d7\u5c11\u6570\u5f71\u54cd\u8005\u4e3b\u5bfc\uff0c\u7fa4\u4f53\u51b3\u7b56\u7684\u51c6\u786e\u6027\u4f1a\u663e\u8457\u964d\u4f4e\u3002\n  \u672c\u6587\u7814\u7a76\u4e86\u4e00\u7ec4\u4ee3\u7406\uff0c\u4ed6\u4eec\u6700\u521d\u62e5\u6709\u5bf9\u4e16\u754c\u72b6\u6001\u7684\u72ec\u7acb\u4e14\u65e0\u504f\u7684\u566a\u58f0\u6d4b\u91cf\u3002\u5047\u8bbe\u8fd9\u4e9b\u4ee3\u7406\u6839\u636e\u7b80\u5355\u7684\u975e\u8d1d\u53f6\u65af\u5b66\u4e60\u89c4\u5219\uff08\u6570\u5b66\u793e\u4f1a\u5b66\u4e2d\u79f0\u4e3a\u6cd5\u56fd-\u5fb7\u683c\u9c81\u7279\u52a8\u6001\u6216\u8fed\u4ee3\u610f\u89c1\u6c60\uff09\u8fed\u4ee3\u66f4\u65b0\u5176\u4f30\u8ba1\u3002\u901a\u8fc7\u8fd9\u79cd\u8fed\u4ee3\u5206\u5e03\u5f0f\u5e73\u5747\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u4ee3\u7406\u6700\u7ec8\u5f97\u5230\u4e16\u754c\u72b6\u6001\u7684\u6e10\u8fd1\u4f30\u8ba1\uff0c\u5176\u65b9\u5dee\u7531\u4ee3\u7406\u95f4\u5206\u914d\u7684\u6743\u91cd\u77e9\u9635\u51b3\u5b9a\u3002\u6bcf\u4e2a\u4ee3\u7406\u65e8\u5728\u6700\u5c0f\u5316\u5176\u6e10\u8fd1\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u4f46\u8be5\u65b9\u5dee\u4e5f\u53d7\u5176\u4ed6\u4ee3\u7406\u5206\u914d\u6743\u91cd\u7684\u5f71\u54cd\u3002\u4e3a\u5b9e\u73b0\u6700\u4f73\u4f30\u8ba1\uff0c\u4ee3\u7406\u9700\u89e3\u51b3\u7531\u53ef\u7528\u5f71\u54cd\u6743\u91cd\u96c6\u5b9a\u4e49\u7684\u535a\u5f08\u8bba\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002\u6211\u4eec\u523b\u753b\u4e86\u535a\u5f08\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u548c\u7eb3\u4ec0\u5747\u8861\u96c6\uff0c\u5e76\u7814\u7a76\u4e86\u4ee3\u7406\u7ec4\u7684\u5f02\u6b65\u6700\u4f73\u54cd\u5e94\u52a8\u6001\uff0c\u8bc1\u660e\u5176\u6536\u655b\u5230\u4e25\u683c\u7eb3\u4ec0\u5747\u8861\u96c6\u3002"}}
{"id": "2506.18196", "pdf": "https://arxiv.org/pdf/2506.18196", "abs": "https://arxiv.org/abs/2506.18196", "authors": ["Fangzheng Liu", "Lancelot Blanchard", "Don D. Haddad", "Joseph A. Paradiso"], "title": "Two Sonification Methods for the MindCube", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "comment": "5 pages, 5 figures", "summary": "In this work, we explore the musical interface potential of the MindCube, an\ninteractive device designed to study emotions. Embedding diverse sensors and\ninput devices, this interface resembles a fidget cube toy commonly used to help\nusers relieve their stress and anxiety. As such, it is a particularly\nwell-suited controller for musical systems that aim to help with emotion\nregulation. In this regard, we present two different mappings for the MindCube,\nwith and without AI. With our generative AI mapping, we propose a way to infuse\nmeaning within a latent space and techniques to navigate through it with an\nexternal controller. We discuss our results and propose directions for future\nwork.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86MindCube\u4f5c\u4e3a\u97f3\u4e50\u754c\u9762\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6620\u5c04\u65b9\u6cd5\uff08\u5e26AI\u548c\u4e0d\u5e26AI\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u97f3\u4e50\u7cfb\u7edf\u5e2e\u52a9\u60c5\u7eea\u8c03\u8282\u3002", "motivation": "MindCube\u662f\u4e00\u79cd\u7528\u4e8e\u7814\u7a76\u60c5\u7eea\u7684\u4ea4\u4e92\u8bbe\u5907\uff0c\u5177\u6709\u591a\u79cd\u4f20\u611f\u5668\u548c\u8f93\u5165\u8bbe\u5907\uff0c\u9002\u5408\u4f5c\u4e3a\u97f3\u4e50\u7cfb\u7edf\u7684\u63a7\u5236\u5668\uff0c\u5e2e\u52a9\u7528\u6237\u8c03\u8282\u60c5\u7eea\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u4f5c\u4e3a\u97f3\u4e50\u754c\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cdMindCube\u7684\u6620\u5c04\u65b9\u6cd5\uff1a\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6ce8\u5165\u610f\u4e49\u5e76\u5229\u7528\u5916\u90e8\u63a7\u5236\u5668\u5bfc\u822a\uff1b\u53e6\u4e00\u79cd\u4e0d\u4f9d\u8d56AI\u3002", "result": "\u8ba8\u8bba\u4e86\u4e24\u79cd\u6620\u5c04\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86MindCube\u5728\u60c5\u7eea\u8c03\u8282\u97f3\u4e50\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "MindCube\u4f5c\u4e3a\u97f3\u4e50\u754c\u9762\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u60c5\u7eea\u8c03\u8282\u4e2d\u7684\u5e94\u7528\u3002", "paper_title_zh": "MindCube\u7684\u4e24\u79cd\u58f0\u97f3\u5316\u65b9\u6cd5", "abstract_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86MindCube\u4f5c\u4e3a\u97f3\u4e50\u754c\u9762\u7684\u6f5c\u529b\u3002MindCube\u662f\u4e00\u79cd\u7528\u4e8e\u7814\u7a76\u60c5\u7eea\u7684\u4ea4\u4e92\u8bbe\u5907\uff0c\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u548c\u8f93\u5165\u8bbe\u5907\uff0c\u7c7b\u4f3c\u4e8e\u5e38\u7528\u4e8e\u7f13\u89e3\u538b\u529b\u548c\u7126\u8651\u7684\u6307\u5c16\u9b54\u65b9\u73a9\u5177\u3002\u56e0\u6b64\uff0c\u5b83\u7279\u522b\u9002\u5408\u4f5c\u4e3a\u65e8\u5728\u5e2e\u52a9\u60c5\u7eea\u8c03\u8282\u7684\u97f3\u4e50\u7cfb\u7edf\u7684\u63a7\u5236\u5668\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u7684MindCube\u6620\u5c04\u65b9\u6cd5\uff0c\u5206\u522b\u5305\u542b\u548c\u4e0d\u5305\u542bAI\u3002\u901a\u8fc7\u751f\u6210\u5f0fAI\u6620\u5c04\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6ce8\u5165\u610f\u4e49\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u5916\u90e8\u63a7\u5236\u5668\u5bfc\u822a\u7684\u6280\u672f\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u7ed3\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u65b9\u5411\u3002"}}
{"id": "2506.18842", "pdf": "https://arxiv.org/pdf/2506.18842", "abs": "https://arxiv.org/abs/2506.18842", "authors": ["Patrick Beukema", "Henry Herzog", "Yawen Zhang", "Hunter Pitelka", "Favyen Bastani"], "title": "LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth", "categories": ["cs.DB", "cs.CV", "cs.LG"], "comment": "8 pages, 7 figures, 1 table, ICML 2025 ML4RS", "summary": "We introduce a new dataset and algorithm for fast and efficient coastal\ndistance calculations from Anywhere on Earth (AoE). Existing global coastal\ndatasets are only available at coarse resolution (e.g. 1-4 km) which limits\ntheir utility. Publicly available satellite imagery combined with computer\nvision enable much higher precision. We provide a global coastline dataset at\n10 meter resolution, a 100+ fold improvement in precision over existing data.\nTo handle the computational challenge of querying at such an increased scale,\nwe introduce a new library: Layered Iterative Geospatial Hierarchical\nTerrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both\nexceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM\nto achieve millisecond online inference, making it well suited for real-time\napplications in resource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5730\u7403\u4e0a\u4efb\u4f55\u4f4d\u7f6e\u5feb\u901f\u9ad8\u6548\u5730\u8ba1\u7b97\u6d77\u5cb8\u7ebf\u8ddd\u79bb\u3002\u73b0\u6709\u5168\u7403\u6d77\u5cb8\u6570\u636e\u96c6\u5206\u8fa8\u7387\u8f83\u4f4e\uff081-4\u516c\u91cc\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u901a\u8fc7\u7ed3\u5408\u516c\u5f00\u536b\u661f\u56fe\u50cf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u672c\u6587\u63d0\u4f9b\u4e8610\u7c73\u5206\u8fa8\u7387\u7684\u5168\u7403\u6d77\u5cb8\u7ebf\u6570\u636e\u96c6\uff0c\u7cbe\u5ea6\u63d0\u5347\u4e86100\u500d\u4ee5\u4e0a\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLighthouse\u7684\u9ad8\u6548\u8ba1\u7b97\u5e93\uff0c\u4ec5\u97001\u4e2aCPU\u548c2GB\u5185\u5b58\u5373\u53ef\u5b9e\u73b0\u6beb\u79d2\u7ea7\u5728\u7ebf\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u5168\u7403\u6d77\u5cb8\u6570\u636e\u96c6\u5206\u8fa8\u7387\u8f83\u4f4e\uff081-4\u516c\u91cc\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002\u516c\u5f00\u536b\u661f\u56fe\u50cf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u53d1\u5c55\u4e3a\u66f4\u9ad8\u7cbe\u5ea6\u7684\u6d77\u5cb8\u7ebf\u8ddd\u79bb\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "1. \u5229\u7528\u516c\u5f00\u536b\u661f\u56fe\u50cf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u751f\u621010\u7c73\u5206\u8fa8\u7387\u7684\u5168\u7403\u6d77\u5cb8\u7ebf\u6570\u636e\u96c6\u30022. \u5f00\u53d1Lighthouse\u5e93\uff0c\u91c7\u7528\u5206\u5c42\u8fed\u4ee3\u5730\u7406\u7a7a\u95f4\u5c42\u6b21\u5316\u5730\u5f62\u5bfc\u5411\u7edf\u4e00\u641c\u7d22\u7b97\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u30023. \u4f18\u5316\u8d44\u6e90\u5360\u7528\uff0c\u4ec5\u97001\u4e2aCPU\u548c2GB\u5185\u5b58\u5373\u53ef\u5b8c\u6210\u6beb\u79d2\u7ea7\u5728\u7ebf\u63a8\u7406\u3002", "result": "1. \u63d0\u4f9b\u4e8610\u7c73\u5206\u8fa8\u7387\u7684\u5168\u7403\u6d77\u5cb8\u7ebf\u6570\u636e\u96c6\uff0c\u7cbe\u5ea6\u6bd4\u73b0\u6709\u6570\u636e\u63d0\u5347100\u500d\u4ee5\u4e0a\u30022. Lighthouse\u5e93\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u97001\u4e2aCPU\u548c2GB\u5185\u5b58\u5373\u53ef\u5b9e\u73b0\u6beb\u79d2\u7ea7\u54cd\u5e94\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u5206\u8fa8\u7387\u5168\u7403\u6d77\u5cb8\u7ebf\u6570\u636e\u96c6\u548c\u9ad8\u6548\u8ba1\u7b97\u5e93Lighthouse\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d77\u5cb8\u7ebf\u8ddd\u79bb\u8ba1\u7b97\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "paper_title_zh": "\u706f\u5854\uff1a\u5730\u7403\u4e0a\u4efb\u610f\u4f4d\u7f6e\u5feb\u901f\u7cbe\u786e\u7684\u6d77\u5cb8\u7ebf\u8ddd\u79bb\u8ba1\u7b97", "abstract_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5730\u7403\u4e0a\u4efb\u4f55\u4f4d\u7f6e\uff08AoE\uff09\u5feb\u901f\u9ad8\u6548\u5730\u8ba1\u7b97\u6d77\u5cb8\u7ebf\u8ddd\u79bb\u3002\u73b0\u6709\u7684\u5168\u7403\u6d77\u5cb8\u6570\u636e\u96c6\u5206\u8fa8\u7387\u8f83\u4f4e\uff08\u4f8b\u59821-4\u516c\u91cc\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u901a\u8fc7\u7ed3\u5408\u516c\u5f00\u536b\u661f\u56fe\u50cf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u6211\u4eec\u63d0\u4f9b\u4e8610\u7c73\u5206\u8fa8\u7387\u7684\u5168\u7403\u6d77\u5cb8\u7ebf\u6570\u636e\u96c6\uff0c\u7cbe\u5ea6\u6bd4\u73b0\u6709\u6570\u636e\u63d0\u9ad8\u4e86100\u500d\u4ee5\u4e0a\u3002\u4e3a\u4e86\u5e94\u5bf9\u5982\u6b64\u5927\u89c4\u6a21\u67e5\u8be2\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u5e93\uff1a\u5206\u5c42\u8fed\u4ee3\u5730\u7406\u7a7a\u95f4\u5c42\u6b21\u5316\u5730\u5f62\u5bfc\u5411\u7edf\u4e00\u641c\u7d22\u5f15\u64ce\uff08Lighthouse\uff09\u3002Lighthouse\u65e2\u5f02\u5e38\u5feb\u901f\u53c8\u8d44\u6e90\u9ad8\u6548\uff0c\u4ec5\u97001\u4e2aCPU\u548c2GB\u5185\u5b58\u5373\u53ef\u5b9e\u73b0\u6beb\u79d2\u7ea7\u5728\u7ebf\u63a8\u7406\uff0c\u975e\u5e38\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2506.18844", "pdf": "https://arxiv.org/pdf/2506.18844", "abs": "https://arxiv.org/abs/2506.18844", "authors": ["Olivier Gamache", "Jean-Michel Fortin", "Mat\u011bj Boxan", "Fran\u00e7ois Pomerleau", "Philippe Gigu\u00e8re"], "title": "Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned", "categories": ["cs.RO", "cs.CV"], "comment": "19 pages, 11 figures, pre-print version of the accepted paper for\n  IEEE Transactions on Field Robotics (T-FR)", "summary": "Standard datasets often present limitations, particularly due to the fixed\nnature of input data sensors, which makes it difficult to compare methods that\nactively adjust sensor parameters to suit environmental conditions. This is the\ncase with Automatic-Exposure (AE) methods, which rely on environmental factors\nto influence the image acquisition process. As a result, AE methods have\ntraditionally been benchmarked in an online manner, rendering experiments\nnon-reproducible. Building on our prior work, we propose a methodology that\nutilizes an emulator capable of generating images at any exposure time. This\napproach leverages BorealHDR, a unique multi-exposure stereo dataset, along\nwith its new extension, in which data was acquired along a repeated trajectory\nat different times of the day to assess the impact of changing illumination. In\ntotal, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting\nconditions. The dataset also includes lidar-inertial-odometry-based maps with\npose estimation for each image frame, as well as Global Navigation Satellite\nSystem (GNSS) data for comparison. We demonstrate that by using images acquired\nat various exposure times, we can emulate realistic images with a\nRoot-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.\nUsing this offline approach, we benchmarked eight AE methods, concluding that\nthe classical AE method remains the field's best performer. To further support\nreproducibility, we provide in-depth details on the development of our backpack\nacquisition platform, including hardware, electrical components, and\nperformance specifications. Additionally, we share valuable lessons learned\nfrom deploying the backpack over more than 25 km across various environments.\nOur code and dataset are available online at this link:\nhttps://github.com/norlab-ulaval/TFR24 BorealHDR", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6a21\u62df\u5668\u751f\u6210\u4efb\u610f\u66dd\u5149\u65f6\u95f4\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8eBorealHDR\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u516b\u79cd\u81ea\u52a8\u66dd\u5149\uff08AE\uff09\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f20\u7edfAE\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5206\u4eab\u4e86\u786c\u4ef6\u5e73\u53f0\u5f00\u53d1\u7ecf\u9a8c\u3002", "motivation": "\u6807\u51c6\u6570\u636e\u96c6\u7684\u56fa\u5b9a\u8f93\u5165\u4f20\u611f\u5668\u53c2\u6570\u9650\u5236\u4e86\u81ea\u52a8\u66dd\u5149\uff08AE\uff09\u65b9\u6cd5\u7684\u6bd4\u8f83\uff0c\u4f20\u7edf\u5728\u7ebf\u5b9e\u9a8c\u96be\u4ee5\u590d\u73b0\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u5668\u548c\u591a\u66dd\u5149\u6570\u636e\u96c6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528BorealHDR\u591a\u66dd\u5149\u7acb\u4f53\u6570\u636e\u96c6\u53ca\u5176\u6269\u5c55\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6a21\u62df\u5668\u751f\u6210\u4efb\u610f\u66dd\u5149\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7RMSE\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002\u8bc4\u4f30\u4e86\u516b\u79cdAE\u65b9\u6cd5\u3002", "result": "\u6a21\u62df\u5668\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684RMSE\u4f4e\u4e8e1.78%\uff0c\u4f20\u7edfAE\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002\u6570\u636e\u96c6\u5305\u542b13.4 km\u8f68\u8ff9\u548c\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u3002", "conclusion": "\u4f20\u7edfAE\u65b9\u6cd5\u4ecd\u4e3a\u6700\u4f18\uff0c\u6a21\u62df\u5668\u548c\u6570\u636e\u96c6\u652f\u6301\u53ef\u590d\u73b0\u6027\u7814\u7a76\u3002\u786c\u4ef6\u5e73\u53f0\u5f00\u53d1\u7ecf\u9a8c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "paper_title_zh": "\u91ce\u5916\u76f8\u673a\u81ea\u52a8\u66dd\u5149\u65b9\u6cd5\u7684\u53ef\u590d\u73b0\u8bc4\u4f30\uff1a\u5e73\u53f0\u3001\u57fa\u51c6\u4e0e\u7ecf\u9a8c\u6559\u8bad", "abstract_zh": "\u6807\u51c6\u6570\u636e\u96c6\u5e38\u56e0\u8f93\u5165\u4f20\u611f\u5668\u6570\u636e\u7684\u56fa\u5b9a\u6027\u800c\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u4f9d\u8d56\u73af\u5883\u56e0\u7d20\u8c03\u6574\u4f20\u611f\u5668\u53c2\u6570\u7684\u81ea\u52a8\u66dd\u5149\uff08AE\uff09\u65b9\u6cd5\u3002\u4f20\u7edfAE\u65b9\u6cd5\u7684\u5728\u7ebf\u5b9e\u9a8c\u96be\u4ee5\u590d\u73b0\u3002\u57fa\u4e8e\u524d\u671f\u5de5\u4f5c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6a21\u62df\u5668\u751f\u6210\u4efb\u610f\u66dd\u5149\u65f6\u95f4\u56fe\u50cf\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u72ec\u7279\u7684BorealHDR\u591a\u66dd\u5149\u7acb\u4f53\u6570\u636e\u96c6\u53ca\u5176\u6269\u5c55\uff0c\u6570\u636e\u91c7\u96c6\u4e8e\u4e00\u5929\u4e2d\u4e0d\u540c\u65f6\u6bb5\u4ee5\u8bc4\u4f30\u5149\u7167\u53d8\u5316\u7684\u5f71\u54cd\u3002BorealHDR\u5171\u8986\u76d613.4\u516c\u91cc\u300159\u6761\u8f68\u8ff9\uff0c\u6db5\u76d6\u6311\u6218\u6027\u5149\u7167\u6761\u4ef6\u3002\u6570\u636e\u96c6\u8fd8\u5305\u62ec\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u5730\u56fe\u3001\u6bcf\u5e27\u56fe\u50cf\u7684\u59ff\u6001\u4f30\u8ba1\u53caGNSS\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4e0d\u540c\u66dd\u5149\u65f6\u95f4\u56fe\u50cf\u751f\u6210\u7684\u6a21\u62df\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684RMSE\u4f4e\u4e8e1.78%\u3002\u5229\u7528\u8fd9\u4e00\u79bb\u7ebf\u65b9\u6cd5\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u516b\u79cdAE\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f20\u7edfAE\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002\u4e3a\u652f\u6301\u590d\u73b0\u6027\uff0c\u6211\u4eec\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u80cc\u5305\u91c7\u96c6\u5e73\u53f0\u7684\u786c\u4ef6\u3001\u7535\u6c14\u7ec4\u4ef6\u53ca\u6027\u80fd\u89c4\u683c\uff0c\u5e76\u5206\u4eab\u4e86\u572825\u516c\u91cc\u4ee5\u4e0a\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u7ecf\u9a8c\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u89c1\uff1ahttps://github.com/norlab-ulaval/TFR24 BorealHDR"}}
{"id": "2506.18885", "pdf": "https://arxiv.org/pdf/2506.18885", "abs": "https://arxiv.org/abs/2506.18885", "authors": ["Annika Thomas", "Aneesa Sonawalla", "Alex Rose", "Jonathan P. How"], "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D Gaussian splatting has emerged as an expressive scene representation for\nRGB-D visual SLAM, but its application to large-scale, multi-agent outdoor\nenvironments remains unexplored. Multi-agent Gaussian SLAM is a promising\napproach to rapid exploration and reconstruction of environments, offering\nscalable environment representations, but existing approaches are limited to\nsmall-scale, indoor environments. To that end, we propose Gaussian\nReconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative\nGaussian splatting SLAM method that integrates i) an implicit tracking module\nbased on local optimization over submaps and ii) an approach to inter- and\nintra-robot loop closure integrated into a pose-graph optimization framework.\nExperiments show that GRAND-SLAM provides state-of-the-art tracking performance\nand 28% higher PSNR than existing methods on the Replica indoor dataset, as\nwell as 91% lower multi-agent tracking error and improved rendering over\nexisting multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.", "AI": {"tldr": "GRAND-SLAM\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u9ad8\u65afSLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u548c\u95ed\u73af\u68c0\u6d4b\u63d0\u5347\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e0b\u7684\u8ddf\u8e2a\u4e0e\u91cd\u5efa\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u5ba4\u5185\u548c\u6237\u5916\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u9ad8\u65afSLAM\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u5ba4\u5185\u73af\u5883\uff0c\u800c\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002GRAND-SLAM\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5cSLAM\u89e3\u51b3\u65b9\u6848\u3002", "method": "GRAND-SLAM\u7ed3\u5408\u4e86\u57fa\u4e8e\u5b50\u56fe\u7684\u5c40\u90e8\u4f18\u5316\u9690\u5f0f\u8ddf\u8e2a\u6a21\u5757\u548c\u96c6\u6210\u4e8e\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u7684\u673a\u5668\u4eba\u95f4\u4e0e\u673a\u5668\u4eba\u5185\u95ed\u73af\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5728Replica\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\uff0cGRAND-SLAM\u7684\u8ddf\u8e2a\u6027\u80fd\u9886\u5148\uff0cPSNR\u63d0\u534728%\uff1b\u5728Kimera-Multi\u6237\u5916\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e91%\uff0c\u6e32\u67d3\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "GRAND-SLAM\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u548c\u95ed\u73af\u68c0\u6d4b\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u9ad8\u65afSLAM\u7684\u9ad8\u6548\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u548c\u91cd\u5efa\u6027\u80fd\u3002", "paper_title_zh": "GRAND-SLAM\uff1a\u9762\u5411\u5168\u5c40\u4e00\u81f4\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u9ad8\u65afSLAM\u7684\u5c40\u90e8\u4f18\u5316\u65b9\u6cd5", "abstract_zh": "3D\u9ad8\u65af\u55b7\u7ed8\u5df2\u6210\u4e3aRGB-D\u89c6\u89c9SLAM\u7684\u4e00\u79cd\u9ad8\u6548\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u6237\u5916\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002\u591a\u667a\u80fd\u4f53\u9ad8\u65afSLAM\u662f\u4e00\u79cd\u5feb\u901f\u63a2\u7d22\u548c\u91cd\u5efa\u73af\u5883\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u73af\u5883\u8868\u793a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5c0f\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GRAND-SLAM\uff0c\u4e00\u79cd\u534f\u4f5c\u5f0f\u9ad8\u65af\u55b7\u7ed8SLAM\u65b9\u6cd5\uff0c\u96c6\u6210\u4e86i\uff09\u57fa\u4e8e\u5b50\u56fe\u5c40\u90e8\u4f18\u5316\u7684\u9690\u5f0f\u8ddf\u8e2a\u6a21\u5757\u548cii\uff09\u96c6\u6210\u4e8e\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u7684\u673a\u5668\u4eba\u95f4\u4e0e\u673a\u5668\u4eba\u5185\u95ed\u73af\u68c0\u6d4b\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGRAND-SLAM\u5728Replica\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u6027\u80fd\uff0cPSNR\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad828%\uff1b\u5728Kimera-Multi\u5927\u89c4\u6a21\u6237\u5916\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e91%\uff0c\u6e32\u67d3\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002"}}
{"id": "2506.18221", "pdf": "https://arxiv.org/pdf/2506.18221", "abs": "https://arxiv.org/abs/2506.18221", "authors": ["Xingyu Alice Yang", "Jianyu Zhang", "L\u00e9on Bottou"], "title": "These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 7 figures, Preprint. Under review", "summary": "Transfer learning is a cornerstone of modern machine learning, promising a\nway to adapt models pretrained on a broad mix of data to new tasks with minimal\nnew data. However, a significant challenge remains in ensuring that transferred\nfeatures are sufficient to handle unseen datasets, amplified by the difficulty\nof quantifying whether two tasks are \"related\". To address these challenges, we\nevaluate model transfer from a pretraining mixture to each of its component\ntasks, assessing whether pretrained features can match the performance of\ntask-specific direct training. We identify a fundamental limitation in deep\nlearning models -- an \"information saturation bottleneck\" -- where networks\nfail to learn new features once they encode similar competing features during\ntraining. When restricted to learning only a subset of key features during\npretraining, models will permanently lose critical features for transfer and\nperform inconsistently on data distributions, even components of the training\nmixture. Empirical evidence from published studies suggests that this\nphenomenon is pervasive in deep learning architectures -- factors such as data\ndistribution or ordering affect the features that current representation\nlearning methods can learn over time. This study suggests that relying solely\non large-scale networks may not be as effective as focusing on task-specific\ntraining, when available. We propose richer feature representations as a\npotential solution to better generalize across new datasets and, specifically,\npresent existing methods alongside a novel approach, the initial steps towards\naddressing this challenge.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u5b58\u5728\u7684\u4fe1\u606f\u9971\u548c\u74f6\u9888\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u65e0\u6cd5\u5b66\u4e60\u65b0\u7279\u5f81\uff0c\u5bfc\u81f4\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\u3002\u4f5c\u8005\u63d0\u51fa\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u8fc1\u79fb\u5b66\u4e60\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\uff0c\u4f46\u5982\u4f55\u786e\u4fdd\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7279\u5f81\u80fd\u591f\u9002\u5e94\u65b0\u4efb\u52a1\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u4ece\u9884\u8bad\u7ec3\u6df7\u5408\u6570\u636e\u8fc1\u79fb\u5230\u5176\u7ec4\u6210\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5206\u6790\u9884\u8bad\u7ec3\u7279\u5f81\u662f\u5426\u80fd\u8fbe\u5230\u4efb\u52a1\u7279\u5b9a\u76f4\u63a5\u8bad\u7ec3\u7684\u6548\u679c\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4fe1\u606f\u9971\u548c\u74f6\u9888\u7684\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u5206\u5e03\u548c\u987a\u5e8f\u5bf9\u7279\u5f81\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u4fe1\u606f\u9971\u548c\u74f6\u9888\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u4e2d\u65e0\u6cd5\u5b66\u4e60\u65b0\u7279\u5f81\uff0c\u4ece\u800c\u5f71\u54cd\u8fc1\u79fb\u6548\u679c\u3002\u5b9e\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728\u73b0\u6709\u67b6\u6784\u4e2d\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u5355\u7eaf\u4f9d\u8d56\u5927\u89c4\u6a21\u7f51\u7edc\u53ef\u80fd\u4e0d\u5982\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6709\u6548\u3002\u4f5c\u8005\u5efa\u8bae\u91c7\u7528\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "\u8fd9\u4e9b\u5e76\u975e\u4f60\u6240\u9700\u7684\u5168\u90e8\u7279\u5f81\uff1a\u76d1\u7763\u9884\u8bad\u7ec3\u4e2d\u7684\u6839\u672c\u74f6\u9888", "abstract_zh": "\u8fc1\u79fb\u5b66\u4e60\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7684\u57fa\u77f3\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u9884\u8bad\u7ec3\u5728\u5e7f\u6cdb\u6570\u636e\u4e0a\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u800c\u53ea\u9700\u5c11\u91cf\u65b0\u6570\u636e\u3002\u7136\u800c\uff0c\u786e\u4fdd\u8fc1\u79fb\u7279\u5f81\u8db3\u4ee5\u5904\u7406\u672a\u89c1\u6570\u636e\u96c6\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u96be\u4ee5\u91cf\u5316\u4e24\u4e2a\u4efb\u52a1\u662f\u5426\u201c\u76f8\u5173\u201d\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u4ece\u9884\u8bad\u7ec3\u6df7\u5408\u6570\u636e\u8fc1\u79fb\u5230\u5176\u7ec4\u6210\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u68c0\u9a8c\u9884\u8bad\u7ec3\u7279\u5f81\u662f\u5426\u80fd\u5339\u914d\u4efb\u52a1\u7279\u5b9a\u76f4\u63a5\u8bad\u7ec3\u7684\u6548\u679c\u3002\u6211\u4eec\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u6839\u672c\u9650\u5236\u2014\u2014\"\u4fe1\u606f\u9971\u548c\u74f6\u9888\"\uff0c\u5373\u7f51\u7edc\u5728\u8bad\u7ec3\u4e2d\u4e00\u65e6\u7f16\u7801\u4e86\u76f8\u4f3c\u7ade\u4e89\u7279\u5f81\uff0c\u5c31\u65e0\u6cd5\u5b66\u4e60\u65b0\u7279\u5f81\u3002\u5f53\u9884\u8bad\u7ec3\u4e2d\u4ec5\u5b66\u4e60\u90e8\u5206\u5173\u952e\u7279\u5f81\u65f6\uff0c\u6a21\u578b\u5c06\u6c38\u4e45\u4e22\u5931\u8fc1\u79fb\u6240\u9700\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5728\u6570\u636e\u5206\u5e03\u4e0a\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5373\u4f7f\u662f\u8bad\u7ec3\u6df7\u5408\u6570\u636e\u7684\u7ec4\u6210\u90e8\u5206\u3002\u5df2\u6709\u7814\u7a76\u7684\u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u666e\u904d\u5b58\u5728\u2014\u2014\u6570\u636e\u5206\u5e03\u6216\u987a\u5e8f\u7b49\u56e0\u7d20\u4f1a\u5f71\u54cd\u5f53\u524d\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u968f\u65f6\u95f4\u5b66\u4e60\u7684\u7279\u5f81\u3002\u672c\u7814\u7a76\u6307\u51fa\uff0c\u5f53\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u53ef\u7528\u65f6\uff0c\u4ec5\u4f9d\u8d56\u5927\u89c4\u6a21\u7f51\u7edc\u53ef\u80fd\u4e0d\u5982\u4e13\u6ce8\u4e8e\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6709\u6548\u3002\u6211\u4eec\u63d0\u51fa\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\u4f5c\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u66f4\u597d\u5730\u6cdb\u5316\u5230\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5177\u4f53\u4ecb\u7ecd\u4e86\u73b0\u6709\u65b9\u6cd5\u53ca\u4e00\u79cd\u65b0\u65b9\u6cd5\u7684\u521d\u6b65\u5c1d\u8bd5\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2506.18240", "pdf": "https://arxiv.org/pdf/2506.18240", "abs": "https://arxiv.org/abs/2506.18240", "authors": ["Wenxin Li", "Chuan Wang", "Hongdong Zhu", "Qi Gao", "Yin Ma", "Hai Wei", "Kai Wen"], "title": "Quantum-Classical Hybrid Quantized Neural Network", "categories": ["cs.LG", "cs.AI", "physics.optics"], "comment": "30 pages, 5 figures, comments are welcome", "summary": "Here in this work, we present a novel Quadratic Binary Optimization (QBO)\nmodel for quantized neural network training, enabling the use of arbitrary\nactivation and loss functions through spline interpolation. We introduce\nForward Interval Propagation (FIP), a method designed to tackle the challenges\nof non-linearity and the multi-layer composite structure in neural networks by\ndiscretizing activation functions into linear subintervals. This approach\npreserves the universal approximation properties of neural networks while\nallowing complex nonlinear functions to be optimized using quantum computers,\nthus broadening their applicability in artificial intelligence. We provide\ntheoretical upper bounds on the approximation error and the number of Ising\nspins required, by deriving the sample complexity of the empirical risk\nminimization problem, from an optimization perspective. A significant challenge\nin solving the associated Quadratic Constrained Binary Optimization (QCBO)\nmodel on a large scale is the presence of numerous constraints. When employing\nthe penalty method to handle these constraints, tuning a large number of\npenalty coefficients becomes a critical hyperparameter optimization problem,\nincreasing computational complexity and potentially affecting solution quality.\nTo address this, we employ the Quantum Conditional Gradient Descent (QCGD)\nalgorithm, which leverages quantum computing to directly solve the QCBO\nproblem. We prove the convergence of QCGD under a quantum oracle with\nrandomness and bounded variance in objective value, as well as under limited\nprecision constraints in the coefficient matrix. Additionally, we provide an\nupper bound on the Time-To-Solution for the QCBO solving process. Experimental\nresults using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on\nthe Fashion MNIST classification task, with only 1.1-bit precision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6837\u6761\u63d2\u503c\u652f\u6301\u4efb\u610f\u6fc0\u6d3b\u548c\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u4f18\u5316\u975e\u7ebf\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5728\u5904\u7406\u975e\u7ebf\u6027\u95ee\u9898\u548c\u591a\u5c42\u590d\u5408\u7ed3\u6784\u65f6\u9762\u4e34\u6311\u6218\uff0c\u91cf\u5b50\u8ba1\u7b97\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u524d\u5411\u533a\u95f4\u4f20\u64ad\uff08FIP\uff09\u65b9\u6cd5\uff0c\u5c06\u6fc0\u6d3b\u51fd\u6570\u79bb\u6563\u5316\u4e3a\u7ebf\u6027\u5b50\u533a\u95f4\uff0c\u5e76\u7ed3\u5408\u91cf\u5b50\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\uff08QCGD\uff09\u7b97\u6cd5\u76f4\u63a5\u6c42\u89e3\u4e8c\u6b21\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff08QCBO\uff09\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Fashion MNIST\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8694.95%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u97001.1\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u91cf\u5b50\u8ba1\u7b97\u548c\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4e3a\u590d\u6742\u975e\u7ebf\u6027\u95ee\u9898\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u91cf\u5316\u795e\u7ecf\u7f51\u7edc", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e8c\u6b21\u4e8c\u8fdb\u5236\u4f18\u5316\uff08QBO\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u901a\u8fc7\u6837\u6761\u63d2\u503c\u652f\u6301\u4efb\u610f\u6fc0\u6d3b\u548c\u635f\u5931\u51fd\u6570\u3002\u6211\u4eec\u5f15\u5165\u4e86\u524d\u5411\u533a\u95f4\u4f20\u64ad\uff08FIP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6fc0\u6d3b\u51fd\u6570\u79bb\u6563\u5316\u4e3a\u7ebf\u6027\u5b50\u533a\u95f4\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u975e\u7ebf\u6027\u548c\u591a\u5c42\u590d\u5408\u7ed3\u6784\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u903c\u8fd1\u7279\u6027\uff0c\u540c\u65f6\u5141\u8bb8\u4f7f\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u4f18\u5316\u590d\u6742\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u4ece\u800c\u6269\u5c55\u4e86\u5176\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u4ece\u4f18\u5316\u89d2\u5ea6\u63a8\u5bfc\u4e86\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u63d0\u4f9b\u4e86\u903c\u8fd1\u8bef\u5dee\u548c\u6240\u9700\u4f0a\u8f9b\u81ea\u65cb\u6570\u91cf\u7684\u7406\u8bba\u4e0a\u754c\u3002\u5927\u89c4\u6a21\u6c42\u89e3\u76f8\u5173\u4e8c\u6b21\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff08QCBO\uff09\u6a21\u578b\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u662f\u5b58\u5728\u5927\u91cf\u7ea6\u675f\u6761\u4ef6\u3002\u4f7f\u7528\u60e9\u7f5a\u65b9\u6cd5\u5904\u7406\u8fd9\u4e9b\u7ea6\u675f\u65f6\uff0c\u8c03\u6574\u5927\u91cf\u60e9\u7f5a\u7cfb\u6570\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u7684\u8d85\u53c2\u6570\u4f18\u5316\u95ee\u9898\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u53ef\u80fd\u5f71\u54cd\u89e3\u7684\u8d28\u91cf\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u91cf\u5b50\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\uff08QCGD\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u76f4\u63a5\u6c42\u89e3QCBO\u95ee\u9898\u3002\u6211\u4eec\u8bc1\u660e\u4e86QCGD\u5728\u91cf\u5b50\u968f\u673a\u9884\u8a00\u673a\u4e0b\u76ee\u6807\u503c\u5177\u6709\u968f\u673a\u6027\u548c\u6709\u754c\u65b9\u5dee\u65f6\u7684\u6536\u655b\u6027\uff0c\u4ee5\u53ca\u5728\u7cfb\u6570\u77e9\u9635\u7cbe\u5ea6\u53d7\u9650\u65f6\u7684\u6536\u655b\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86QCBO\u6c42\u89e3\u8fc7\u7a0b\u7684\u65f6\u95f4\u89e3\u4e0a\u754c\u3002\u4f7f\u7528\u76f8\u5e72\u4f0a\u8f9b\u673a\uff08CIM\uff09\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728Fashion MNIST\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8694.95%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u97001.1\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2506.18245", "pdf": "https://arxiv.org/pdf/2506.18245", "abs": "https://arxiv.org/abs/2506.18245", "authors": ["Lei Yu", "Zhirong Huang", "Hang Yuan", "Shiqi Cheng", "Li Yang", "Fengjun Zhang", "Chenjie Shen", "Jiajia Ma", "Jingyuan Zhang", "Junyi Lu", "Chun Zuo"], "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "Accepted to ISSTA 2025", "summary": "Smart contract vulnerability detection remains a major challenge in\nblockchain security. Existing vulnerability detection methods face two main\nissues: (1) Existing datasets lack comprehensive coverage and high-quality\nexplanations for preference learning. (2) Large language models (LLMs) often\nstruggle with accurately interpreting specific concepts in smart contract\nsecurity. Empirical analysis shows that even after continual pre-training (CPT)\nand supervised fine-tuning (SFT), LLMs may misinterpret the execution order of\nstate changes, resulting in incorrect explanations despite making correct\ndetection decisions. To address these challenges, we propose Smart-LLaMA-DPO\nbased on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major\nvulnerability types and machine-unauditable vulnerabilities, including precise\nlabels, explanations, and locations for SFT, as well as high-quality and\nlow-quality output pairs for Direct Preference Optimization (DPO). Second, we\nperform CPT using large-scale smart contract to enhance the LLM's understanding\nof specific security practices in smart contracts. Futhermore, we conduct SFT\nwith our comprehensive dataset. Finally, we apply DPO, leveraging human\nfeedback and a specially designed loss function that increases the probability\nof preferred explanations while reducing the likelihood of non-preferred\noutputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:\nreentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,\nas well as machine-unauditable vulnerabilities. Our method significantly\noutperforms state-of-the-art baselines, with average improvements of 10.43% in\nF1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human\nevaluation confirm that our method generates more correct, thorough, and clear\nexplanations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSmart-LLaMA-DPO\uff0c\u57fa\u4e8eLLaMA-3.1-8B\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u9762\u6570\u636e\u96c6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u662f\u533a\u5757\u94fe\u5b89\u5168\u7684\u4e3b\u8981\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u5168\u548c\u89e3\u91ca\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u4e14\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u6982\u5ff5\u3002", "method": "1. \u6784\u5efa\u6db5\u76d6\u56db\u5927\u6f0f\u6d1e\u7c7b\u578b\u548c\u673a\u5668\u4e0d\u53ef\u5ba1\u8ba1\u6f0f\u6d1e\u7684\u5168\u9762\u6570\u636e\u96c6\uff1b2. \u901a\u8fc7\u5927\u89c4\u6a21\u667a\u80fd\u5408\u7ea6\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\uff1b3. \u4f7f\u7528\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff1b4. \u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u7279\u6b8a\u635f\u5931\u51fd\u6570\u3002", "result": "Smart-LLaMA-DPO\u5728\u56db\u5927\u6f0f\u6d1e\u7c7b\u578b\u548c\u673a\u5668\u4e0d\u53ef\u5ba1\u8ba1\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cF1\u5206\u6570\u548c\u51c6\u786e\u7387\u5206\u522b\u5e73\u5747\u63d0\u534710.43%\u548c7.87%\uff0c\u4e14\u751f\u6210\u66f4\u6b63\u786e\u3001\u5168\u9762\u548c\u6e05\u6670\u7684\u89e3\u91ca\u3002", "conclusion": "Smart-LLaMA-DPO\u901a\u8fc7\u7efc\u5408\u6570\u636e\u96c6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u89e3\u91ca\u8d28\u91cf\uff0c\u4e3a\u533a\u5757\u94fe\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "Smart-LLaMA-DPO\uff1a\u57fa\u4e8e\u5f3a\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b", "abstract_zh": "\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u4ecd\u662f\u533a\u5757\u94fe\u5b89\u5168\u7684\u4e3b\u8981\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\uff081\uff09\u6570\u636e\u96c6\u7f3a\u4e4f\u5168\u9762\u8986\u76d6\u548c\u9ad8\u8d28\u91cf\u89e3\u91ca\uff1b\uff082\uff09\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u6982\u5ff5\u3002\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u5373\u4f7f\u7ecf\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u548c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0cLLM\u4ecd\u53ef\u80fd\u8bef\u89e3\u72b6\u6001\u53d8\u5316\u7684\u6267\u884c\u987a\u5e8f\uff0c\u5bfc\u81f4\u89e3\u91ca\u9519\u8bef\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u57fa\u4e8eLLaMA-3.1-8B\u63d0\u51faSmart-LLaMA-DPO\u3002\u9996\u5148\uff0c\u6784\u5efa\u6db5\u76d6\u56db\u5927\u6f0f\u6d1e\u7c7b\u578b\u548c\u673a\u5668\u4e0d\u53ef\u5ba1\u8ba1\u6f0f\u6d1e\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5305\u62ec\u7cbe\u786e\u6807\u7b7e\u3001\u89e3\u91ca\u548c\u4f4d\u7f6e\u7528\u4e8eSFT\uff0c\u4ee5\u53ca\u9ad8\u8d28\u91cf\u548c\u4f4e\u8d28\u91cf\u8f93\u51fa\u5bf9\u7528\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u667a\u80fd\u5408\u7ea6\u8fdb\u884cCPT\u4ee5\u589e\u5f3aLLM\u5bf9\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u5b9e\u8df5\u7684\u7406\u89e3\u3002\u63a5\u7740\uff0c\u4f7f\u7528\u6570\u636e\u96c6\u8fdb\u884cSFT\u3002\u6700\u540e\uff0c\u5e94\u7528DPO\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u7279\u6b8a\u635f\u5931\u51fd\u6570\uff0c\u63d0\u9ad8\u504f\u597d\u89e3\u91ca\u7684\u6982\u7387\u5e76\u964d\u4f4e\u975e\u504f\u597d\u8f93\u51fa\u7684\u53ef\u80fd\u6027\u3002\u6211\u4eec\u5728\u56db\u5927\u6f0f\u6d1e\u7c7b\u578b\uff08\u91cd\u5165\u3001\u65f6\u95f4\u6233\u4f9d\u8d56\u3001\u6574\u6570\u6ea2\u51fa/\u4e0b\u6ea2\u548c\u59d4\u6258\u8c03\u7528\uff09\u53ca\u673a\u5668\u4e0d\u53ef\u5ba1\u8ba1\u6f0f\u6d1e\u4e0a\u8bc4\u4f30Smart-LLaMA-DPO\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0cF1\u5206\u6570\u548c\u51c6\u786e\u7387\u5206\u522b\u5e73\u5747\u63d0\u534710.43%\u548c7.87%\u3002\u6b64\u5916\uff0cLLM\u8bc4\u4f30\u548c\u4eba\u7c7b\u8bc4\u4f30\u5747\u8bc1\u5b9e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u66f4\u6b63\u786e\u3001\u5168\u9762\u548c\u6e05\u6670\u7684\u89e3\u91ca\u3002"}}
{"id": "2506.18267", "pdf": "https://arxiv.org/pdf/2506.18267", "abs": "https://arxiv.org/abs/2506.18267", "authors": ["Haseeb Ullah Khan Shinwari", "Muhammad Usama"], "title": "ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing\nuniform adaptation across transformer layers and attention heads despite their\nheterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic\nLoRA (ARD-LoRA), a novel framework that automates rank allocation through\nlearnable scaling factors. These factors are optimized via a meta-objective\nbalancing task performance and parameter efficiency, incorporating $\\ell_1$\nsparsity for minimal rank and Total Variation regularization for stable rank\ntransitions. ARD-LoRA enables continuous, differentiable, per-head rank\nadaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's\nefficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%\ntrainable parameters, outperforming strong baselines like DoRA and AdaLoRA.\nFurthermore, it reduces multimodal adaptation memory by 41%. These results\nestablish dynamic, fine-grained rank allocation as a critical paradigm for\nefficient foundation model adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faARD-LoRA\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u79e9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56fa\u5b9a\u79e9\u5bfc\u81f4\u7684\u5f02\u6784\u5b66\u4e60\u9700\u6c42\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u79e9\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540cTransformer\u5c42\u548c\u6ce8\u610f\u529b\u5934\u7684\u5f02\u6784\u5b66\u4e60\u52a8\u6001\uff0c\u9650\u5236\u4e86\u53c2\u6570\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u7684\u4f18\u5316\u3002", "method": "ARD-LoRA\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u56e0\u5b50\u81ea\u52a8\u5206\u914d\u79e9\uff0c\u7ed3\u5408\u21131\u7a00\u758f\u6027\u548c\u603b\u53d8\u5dee\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u8fde\u7eed\u3001\u53ef\u5fae\u7684\u9010\u5934\u79e9\u9002\u5e94\uff0c\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\u3002", "result": "\u5728LLAMA-3.1-70B\u548cPaliGemma-2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cARD-LoRA\u4ec5\u97000.32%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u8fbe\u5230\u5168\u5fae\u8c03\u6027\u80fd\u768499.3%\uff0c\u5e76\u51cf\u5c11\u591a\u6a21\u6001\u9002\u5e94\u5185\u5b5841%\uff0c\u4f18\u4e8eDoRA\u548cAdaLoRA\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u3001\u7ec6\u7c92\u5ea6\u7684\u79e9\u5206\u914d\u662f\u9ad8\u6548\u9002\u5e94\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8303\u5f0f\uff0cARD-LoRA\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "ARD-LoRA\uff1a\u52a8\u6001\u79e9\u5206\u914d\u7528\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u4e0e\u5f02\u6784\u9002\u5e94\u9700\u6c42", "abstract_zh": "\u4f20\u7edf\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u79e9\uff0c\u5bfc\u81f4\u5bf9\u4e0d\u540cTransformer\u5c42\u548c\u6ce8\u610f\u529b\u5934\u7684\u5f02\u6784\u5b66\u4e60\u52a8\u6001\u65bd\u52a0\u4e86\u7edf\u4e00\u7684\u9002\u5e94\u65b9\u5f0f\u3002\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u79e9\u52a8\u6001LoRA\uff08ARD-LoRA\uff09\uff0c\u4e00\u79cd\u901a\u8fc7\u53ef\u5b66\u4e60\u7f29\u653e\u56e0\u5b50\u81ea\u52a8\u5206\u914d\u79e9\u7684\u65b0\u6846\u67b6\u3002\u8fd9\u4e9b\u56e0\u5b50\u901a\u8fc7\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\u7684\u5143\u76ee\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u7ed3\u5408\u21131\u7a00\u758f\u6027\u4ee5\u5b9e\u73b0\u6700\u5c0f\u79e9\u548c\u603b\u53d8\u5dee\u6b63\u5219\u5316\u4ee5\u7a33\u5b9a\u79e9\u7684\u8fc7\u6e21\u3002ARD-LoRA\u5b9e\u73b0\u4e86\u8fde\u7eed\u3001\u53ef\u5fae\u7684\u9010\u5934\u79e9\u9002\u5e94\u3002\u5728LLAMA-3.1-70B\u548cPaliGemma-2\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86ARD-LoRA\u7684\u6709\u6548\u6027\uff0c\u4ec5\u4f7f\u75280.32%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u8fbe\u5230\u5168\u5fae\u8c03\u6027\u80fd\u768499.3%\uff0c\u4f18\u4e8eDoRA\u548cAdaLoRA\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5c06\u591a\u6a21\u6001\u9002\u5e94\u7684\u5185\u5b58\u51cf\u5c11\u4e8641%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u52a8\u6001\u3001\u7ec6\u7c92\u5ea6\u7684\u79e9\u5206\u914d\u662f\u9ad8\u6548\u9002\u5e94\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8303\u5f0f\u3002"}}
{"id": "2506.18285", "pdf": "https://arxiv.org/pdf/2506.18285", "abs": "https://arxiv.org/abs/2506.18285", "authors": ["Naiyu Yin", "Tian Gao", "Yue Yu"], "title": "Learning Causal Graphs at Scale: A Foundation Model Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to its human-interpretability and invariance properties, Directed Acyclic\nGraph (DAG) has been a foundational tool across various areas of AI research,\nleading to significant advancements. However, DAG learning remains highly\nchallenging, due to its super-exponential growth in computational cost and\nidentifiability issues, particularly in small-sample regimes. To address these\ntwo challenges, in this work we leverage the recent success of linear\ntransformers and develop a foundation model approach for discovering multiple\norder-consistent DAGs across tasks. In particular, we propose Attention-DAG\n(ADAG), a novel attention-mechanism-based architecture for learning multiple\nlinear Structural Equation Models (SEMs). ADAG learns the mapping from observed\ndata to both graph structure and parameters via a nonlinear attention-based\nkernel, enabling efficient multi-task estimation of the underlying linear SEMs.\nBy formulating the learning process across multiple tasks as a continuous\noptimization problem, the pre-trained ADAG model captures the common structural\nproperties as a shared low-dimensional prior, thereby reducing the\nill-posedness of downstream DAG learning tasks in small-sample regimes. We\nevaluate our proposed approach on benchmark synthetic datasets and find that\nADAG achieves substantial improvements in both DAG learning accuracy and\nzero-shot inference efficiency. To the best of our knowledge, this is the first\npractical approach for pre-training a foundation model specifically designed\nfor DAG learning, representing a step toward more efficient and generalizable\ndown-stream applications in causal discovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u65b0\u65b9\u6cd5ADAG\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u591a\u4efb\u52a1\u4e0b\u7684\u7ebf\u6027\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7684\u5b66\u4e60\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u7531\u4e8eDAG\u5728AI\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53ca\u5176\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u53d8\u6027\uff0c\u4f46\u5176\u5b66\u4e60\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684DAG\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faADAG\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u7684DAG\u5b66\u4e60\u67b6\u6784\uff09\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6ce8\u610f\u529b\u6838\u5b66\u4e60\u89c2\u6d4b\u6570\u636e\u5230\u56fe\u7ed3\u6784\u548c\u53c2\u6570\u7684\u6620\u5c04\uff0c\u5c06\u591a\u4efb\u52a1\u5b66\u4e60\u5efa\u6a21\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6355\u6349\u5171\u4eab\u7684\u4f4e\u7ef4\u5148\u9a8c\uff0c\u51cf\u5c11\u4e0b\u6e38\u4efb\u52a1\u7684\u75c5\u6001\u6027\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cADAG\u5728DAG\u5b66\u4e60\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ADAG\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aDAG\u5b66\u4e60\u8bbe\u8ba1\u7684\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u7684\u9ad8\u6548\u548c\u901a\u7528\u4e0b\u6e38\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "paper_title_zh": "\u5927\u89c4\u6a21\u5b66\u4e60\u56e0\u679c\u56fe\uff1a\u4e00\u79cd\u57fa\u7840\u6a21\u578b\u65b9\u6cd5", "abstract_zh": "\u7531\u4e8e\u5176\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u53d8\u6027\u7279\u6027\uff0c\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u5df2\u6210\u4e3aAI\u7814\u7a76\u4e2d\u591a\u4e2a\u9886\u57df\u7684\u57fa\u7840\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0cDAG\u5b66\u4e60\u4ecd\u7136\u6781\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u6e90\u4e8e\u5176\u8ba1\u7b97\u6210\u672c\u7684\u8d85\u6307\u6570\u589e\u957f\u548c\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e24\u5927\u6311\u6218\uff0c\u672c\u6587\u5229\u7528\u7ebf\u6027\u53d8\u6362\u5668\u7684\u6700\u65b0\u6210\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u591a\u4efb\u52a1\u4e2d\u987a\u5e8f\u4e00\u81f4\u7684DAG\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ADAG\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u7684DAG\u5b66\u4e60\u67b6\u6784\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60\u591a\u4e2a\u7ebf\u6027\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff08SEM\uff09\u3002ADAG\u901a\u8fc7\u975e\u7ebf\u6027\u6ce8\u610f\u529b\u6838\u5b66\u4e60\u4ece\u89c2\u6d4b\u6570\u636e\u5230\u56fe\u7ed3\u6784\u548c\u53c2\u6570\u7684\u6620\u5c04\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5e95\u5c42\u7ebf\u6027SEM\u7684\u9ad8\u6548\u591a\u4efb\u52a1\u4f30\u8ba1\u3002\u901a\u8fc7\u5c06\u591a\u4efb\u52a1\u5b66\u4e60\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u9884\u8bad\u7ec3\u7684ADAG\u6a21\u578b\u5c06\u5171\u540c\u7ed3\u6784\u7279\u6027\u6355\u6349\u4e3a\u5171\u4eab\u7684\u4f4e\u7ef4\u5148\u9a8c\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u4e0b\u6e38DAG\u5b66\u4e60\u4efb\u52a1\u7684\u75c5\u6001\u6027\u3002\u6211\u4eec\u5728\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u53d1\u73b0ADAG\u5728DAG\u5b66\u4e60\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aDAG\u5b66\u4e60\u8bbe\u8ba1\u7684\u5b9e\u7528\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u7684\u9ad8\u6548\u548c\u901a\u7528\u4e0b\u6e38\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2506.18289", "pdf": "https://arxiv.org/pdf/2506.18289", "abs": "https://arxiv.org/abs/2506.18289", "authors": ["Saurabhsingh Rajput", "Mootez Saad", "Tushar Sharma"], "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations", "categories": ["cs.SE", "cs.AI"], "comment": "In review", "summary": "AI's exponential growth intensifies computational demands and energy\nchallenges. While practitioners employ various optimization techniques, that we\nrefer as \"knobs\" in this paper, to tune model efficiency, these are typically\nafterthoughts and reactive ad-hoc changes applied in isolation without\nunderstanding their combinatorial effects on energy efficiency. This paper\nemphasizes on treating energy efficiency as the first-class citizen and as a\nfundamental design consideration for a compute-intensive pipeline. We show that\nstrategic selection across five AI pipeline phases (data, model, training,\nsystem, inference) creates cascading efficiency. Experimental validation shows\northogonal combinations reduce energy consumption by up to $94.6$% while\npreserving $95.95$% of the original F1 score of non-optimized pipelines. This\ncurated approach provides actionable frameworks for informed sustainable AI\nthat balance efficiency, performance, and environmental responsibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u80fd\u6e90\u6548\u7387\u4f5c\u4e3aAI\u8ba1\u7b97\u5bc6\u96c6\u578b\u6d41\u7a0b\u7684\u6838\u5fc3\u8bbe\u8ba1\u8003\u91cf\uff0c\u901a\u8fc7\u6b63\u4ea4\u4f18\u5316\u7ec4\u5408\u5728\u4e94\u4e2aAI\u6d41\u7a0b\u9636\u6bb5\uff08\u6570\u636e\u3001\u6a21\u578b\u3001\u8bad\u7ec3\u3001\u7cfb\u7edf\u3001\u63a8\u7406\uff09\u4e2d\u5b9e\u73b0\u80fd\u6e90\u6548\u7387\u7684\u7ea7\u8054\u63d0\u5347\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u51cf\u5c11\u9ad8\u8fbe94.6%\u7684\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u630195.95%\u7684\u539f\u59cbF1\u5206\u6570\u3002", "motivation": "AI\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u80fd\u6e90\u6311\u6218\u3002\u76ee\u524d\uff0c\u4f18\u5316\u6280\u672f\uff08\u5982\u201c\u65cb\u94ae\u201d\uff09\u901a\u5e38\u662f\u88ab\u52a8\u3001\u5b64\u7acb\u5730\u5e94\u7528\uff0c\u7f3a\u4e4f\u5bf9\u5176\u7ec4\u5408\u6548\u5e94\u5bf9\u80fd\u6e90\u6548\u7387\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u5c06\u80fd\u6e90\u6548\u7387\u63d0\u5347\u4e3a\u8bbe\u8ba1AI\u6d41\u7a0b\u65f6\u7684\u9996\u8981\u8003\u8651\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u4ea4\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6218\u7565\u6027\u5730\u9009\u62e9\u5e76\u7ec4\u5408\u4e94\u4e2aAI\u6d41\u7a0b\u9636\u6bb5\uff08\u6570\u636e\u3001\u6a21\u578b\u3001\u8bad\u7ec3\u3001\u7cfb\u7edf\u3001\u63a8\u7406\uff09\u4e2d\u7684\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u80fd\u6e90\u6548\u7387\u7684\u7ea7\u8054\u63d0\u5347\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6b63\u4ea4\u7ec4\u5408\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6b63\u4ea4\u4f18\u5316\u7ec4\u5408\u53ef\u51cf\u5c11\u9ad8\u8fbe94.6%\u7684\u80fd\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u630195.95%\u7684\u539f\u59cbF1\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6e90\u6548\u7387\u800c\u4e0d\u663e\u8457\u727a\u7272\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u5e2e\u52a9\u5b9e\u73b0\u53ef\u6301\u7eed\u7684AI\u8bbe\u8ba1\uff0c\u5e73\u8861\u6548\u7387\u3001\u6027\u80fd\u548c\u73af\u4fdd\u8d23\u4efb\u3002\u80fd\u6e90\u6548\u7387\u5e94\u6210\u4e3aAI\u6d41\u7a0b\u8bbe\u8ba1\u7684\u6838\u5fc3\u8003\u91cf\u3002", "paper_title_zh": "\u5c06AI\u201c\u8c03\u7eff\u201d\uff1a\u901a\u8fc7\u6b63\u4ea4\u4f18\u5316\u63a2\u7d22\u80fd\u6e90\u6548\u7387\u7684\u7ea7\u8054\u6548\u5e94", "abstract_zh": "AI\u7684\u6307\u6570\u7ea7\u589e\u957f\u52a0\u5267\u4e86\u8ba1\u7b97\u9700\u6c42\u548c\u80fd\u6e90\u6311\u6218\u3002\u5c3d\u7ba1\u4ece\u4e1a\u8005\u91c7\u7528\u4e86\u591a\u79cd\u4f18\u5316\u6280\u672f\uff08\u672c\u6587\u79f0\u4e4b\u4e3a\u201c\u65cb\u94ae\u201d\uff09\u6765\u8c03\u6574\u6a21\u578b\u6548\u7387\uff0c\u4f46\u8fd9\u4e9b\u901a\u5e38\u662f\u4e8b\u540e\u88ab\u52a8\u3001\u5b64\u7acb\u7684\u4e34\u65f6\u8c03\u6574\uff0c\u7f3a\u4e4f\u5bf9\u5176\u7ec4\u5408\u6548\u5e94\u5bf9\u80fd\u6e90\u6548\u7387\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002\u672c\u6587\u5f3a\u8c03\u5c06\u80fd\u6e90\u6548\u7387\u4f5c\u4e3a\u8ba1\u7b97\u5bc6\u96c6\u578b\u6d41\u7a0b\u7684\u4e00\u7b49\u516c\u6c11\u548c\u57fa\u672c\u8bbe\u8ba1\u8003\u91cf\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e94\u4e2aAI\u6d41\u7a0b\u9636\u6bb5\uff08\u6570\u636e\u3001\u6a21\u578b\u3001\u8bad\u7ec3\u3001\u7cfb\u7edf\u3001\u63a8\u7406\uff09\u4e2d\u6218\u7565\u6027\u5730\u9009\u62e9\u4f18\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u80fd\u6e90\u6548\u7387\u7684\u7ea7\u8054\u63d0\u5347\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u6b63\u4ea4\u7ec4\u5408\u53ef\u5c06\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe94.6%\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u4f18\u5316\u6d41\u7a0b\u539f\u59cbF1\u5206\u6570\u768495.95%\u3002\u8fd9\u79cd\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65b9\u6cd5\u4e3a\u5e73\u8861\u6548\u7387\u3001\u6027\u80fd\u548c\u73af\u4fdd\u8d23\u4efb\u7684\u53ef\u6301\u7eedAI\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6846\u67b6\u3002"}}
{"id": "2506.18295", "pdf": "https://arxiv.org/pdf/2506.18295", "abs": "https://arxiv.org/abs/2506.18295", "authors": ["Kejia Bian", "Meixia Tao", "Shu Sun", "Jun Yu"], "title": "GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural ray tracing (RT) has emerged as a promising paradigm for channel\nmodeling by combining physical propagation principles with neural networks. It\nenables high modeling accuracy and efficiency. However, current neural RT\nmethods face two key limitations: constrained generalization capability due to\nstrong spatial dependence, and weak adherence to electromagnetic laws. In this\npaper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced\ngeneralization, accuracy and efficiency. GeNeRT supports both intra-scenario\nspatial transferability and inter-scenario zero-shot generalization. By\nincorporating Fresnel-inspired neural network design, it also achieves higher\naccuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized\nacceleration strategy is introduced to improve runtime efficiency. Extensive\nexperiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes\nwell across untrained regions within a scenario and entirely unseen\nenvironments, and achieves superior accuracy in MPC prediction compared to\nbaselines. Moreover, it outperforms Wireless Insite in runtime efficiency,\nparticularly in multi-transmitter settings. Ablation experiments validate the\neffectiveness of the network architecture and training strategy in capturing\nphysical principles of ray-surface interactions.", "AI": {"tldr": "GeNeRT\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u667a\u80fd\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u6cdb\u5316\u7684\u795e\u7ecf\u5c04\u7ebf\u8ffd\u8e2a\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u548c\u7535\u78c1\u5b9a\u5f8b\u9075\u5faa\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5efa\u6a21\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u5c04\u7ebf\u8ffd\u8e2a\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4f9d\u8d56\u6027\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u548c\u5bf9\u7535\u78c1\u5b9a\u5f8b\u9075\u5faa\u6027\u5f31\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u901a\u7528\u3001\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u795e\u7ecf\u5c04\u7ebf\u8ffd\u8e2a\u6846\u67b6\u3002", "method": "GeNeRT\u7ed3\u5408\u4e86\u83f2\u6d85\u5c14\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\uff0c\u652f\u6301\u573a\u666f\u5185\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u548c\u573a\u666f\u95f4\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u91c7\u7528GPU\u5f20\u91cf\u5316\u52a0\u901f\u7b56\u7565\u63d0\u5347\u8fd0\u884c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeNeRT\u5728\u672a\u8bad\u7ec3\u533a\u57df\u548c\u5168\u65b0\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u591a\u5f84\u5206\u91cf\u9884\u6d4b\u7cbe\u5ea6\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u591a\u53d1\u5c04\u5668\u8bbe\u7f6e\u4e0b\u8fd0\u884c\u6548\u7387\u8d85\u8fc7Wireless Insite\u3002", "conclusion": "GeNeRT\u901a\u8fc7\u521b\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u6355\u6349\u4e86\u5c04\u7ebf\u4e0e\u8868\u9762\u76f8\u4e92\u4f5c\u7528\u7684\u7269\u7406\u539f\u7406\uff0c\u4e3a\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "GeNeRT\uff1a\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u901a\u7528\u795e\u7ecf\u5c04\u7ebf\u8ffd\u8e2a\u667a\u80fd\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5", "abstract_zh": "\u795e\u7ecf\u5c04\u7ebf\u8ffd\u8e2a\uff08RT\uff09\u901a\u8fc7\u5c06\u7269\u7406\u4f20\u64ad\u539f\u7406\u4e0e\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\uff0c\u6210\u4e3a\u4fe1\u9053\u5efa\u6a21\u7684\u4e00\u79cd\u6709\u524d\u666f\u8303\u5f0f\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u3002\u7136\u800c\uff0c\u73b0\u6709\u795e\u7ecfRT\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u56e0\u5f3a\u7a7a\u95f4\u4f9d\u8d56\u6027\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u4ee5\u53ca\u5bf9\u7535\u78c1\u5b9a\u5f8b\u7684\u9075\u5faa\u6027\u8f83\u5f31\u3002\u672c\u6587\u63d0\u51faGeNeRT\uff0c\u4e00\u79cd\u5177\u6709\u589e\u5f3a\u6cdb\u5316\u6027\u3001\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u901a\u7528\u795e\u7ecfRT\u6846\u67b6\u3002GeNeRT\u652f\u6301\u573a\u666f\u5185\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u548c\u573a\u666f\u95f4\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7\u83f2\u6d85\u5c14\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u5728\u591a\u5f84\u5206\u91cf\uff08MPC\uff09\u9884\u6d4b\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u5f15\u5165GPU\u5f20\u91cf\u5316\u52a0\u901f\u7b56\u7565\u4ee5\u63d0\u5347\u8fd0\u884c\u6548\u7387\u3002\u5728\u6237\u5916\u573a\u666f\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeNeRT\u5728\u672a\u8bad\u7ec3\u533a\u57df\u548c\u5168\u65b0\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5728MPC\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5176\u8fd0\u884c\u6548\u7387\u5728\u591a\u53d1\u5c04\u5668\u8bbe\u7f6e\u4e0b\u8d85\u8fc7Wireless Insite\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u5728\u6355\u6349\u5c04\u7ebf-\u8868\u9762\u76f8\u4e92\u4f5c\u7528\u7269\u7406\u539f\u7406\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18304", "pdf": "https://arxiv.org/pdf/2506.18304", "abs": "https://arxiv.org/abs/2506.18304", "authors": ["Junchao Fan", "Xuyang Lei", "Xiaolin Chang"], "title": "Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 3 figures, 2 tables", "summary": "Deep reinforcement learning (DRL) has emerged as a promising paradigm for\nautonomous driving. However, despite their advanced capabilities, DRL-based\npolicies remain highly vulnerable to adversarial attacks, posing serious safety\nrisks in real-world deployments. Investigating such attacks is crucial for\nrevealing policy vulnerabilities and guiding the development of more robust\nautonomous systems. While prior attack methods have made notable progress, they\nstill face several challenges: 1) they often rely on high-frequency attacks,\nyet critical attack opportunities are typically context-dependent and\ntemporally sparse, resulting in inefficient attack patterns; 2) restricting\nattack frequency can improve efficiency but often results in unstable training\ndue to the adversary's limited exploration. To address these challenges, we\npropose an adaptive expert-guided adversarial attack method that enhances both\nthe stability and efficiency of attack policy training. Our method first\nderives an expert policy from successful attack demonstrations using imitation\nlearning, strengthened by an ensemble Mixture-of-Experts architecture for\nrobust generalization across scenarios. This expert policy then guides a\nDRL-based adversary through a KL-divergence regularization term. Due to the\ndiversity of scenarios, expert policies may be imperfect. To address this, we\nfurther introduce a performance-aware annealing strategy that gradually reduces\nreliance on the expert as the adversary improves. Extensive experiments\ndemonstrate that our method achieves outperforms existing approaches in terms\nof collision rate, attack efficiency, and training stability, especially in\ncases where the expert policy is sub-optimal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e13\u5bb6\u5f15\u5bfc\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u9488\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u653b\u51fb\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u548c\u4e13\u5bb6\u96c6\u6210\u67b6\u6784\uff0c\u7ed3\u5408KL\u6563\u5ea6\u6b63\u5219\u5316\u548c\u6027\u80fd\u611f\u77e5\u9000\u706b\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57fa\u4e8eDRL\u7684\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u9891\u653b\u51fb\u6216\u9650\u5236\u653b\u51fb\u9891\u7387\uff0c\u5bfc\u81f4\u653b\u51fb\u6548\u7387\u4f4e\u6216\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4ece\u6210\u529f\u653b\u51fb\u6f14\u793a\u4e2d\u63d0\u53d6\u4e13\u5bb6\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u96c6\u6210\u4e13\u5bb6\u67b6\u6784\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b2) \u4f7f\u7528KL\u6563\u5ea6\u6b63\u5219\u5316\u5f15\u5bfcDRL\u5bf9\u6297\u5668\uff1b3) \u5f15\u5165\u6027\u80fd\u611f\u77e5\u9000\u706b\u7b56\u7565\uff0c\u9010\u6b65\u51cf\u5c11\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u78b0\u649e\u7387\u3001\u653b\u51fb\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e13\u5bb6\u7b56\u7565\u4e0d\u7406\u60f3\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u5f15\u5bfc\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e3a\u63ed\u793a\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u8106\u5f31\u6027\u548c\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "paper_title_zh": "\u78e8\u783a\u957f\u77db\uff1a\u9488\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u5f15\u5bfc\u5bf9\u6297\u653b\u51fb", "abstract_zh": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5df2\u6210\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u8303\u5f0f\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5176\u80fd\u529b\u5148\u8fdb\uff0c\u57fa\u4e8eDRL\u7684\u7b56\u7565\u4ecd\u6781\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u7ed9\u5b9e\u9645\u90e8\u7f72\u5e26\u6765\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002\u7814\u7a76\u6b64\u7c7b\u653b\u51fb\u5bf9\u63ed\u793a\u7b56\u7565\u8106\u5f31\u6027\u548c\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u4ee5\u4e0b\u6311\u6218\uff1a1) \u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u9ad8\u9891\u653b\u51fb\uff0c\u4f46\u5173\u952e\u653b\u51fb\u673a\u4f1a\u901a\u5e38\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u4e14\u65f6\u95f4\u7a00\u758f\u7684\uff0c\u5bfc\u81f4\u653b\u51fb\u6a21\u5f0f\u6548\u7387\u4f4e\u4e0b\uff1b2) \u9650\u5236\u653b\u51fb\u9891\u7387\u53ef\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5e38\u56e0\u5bf9\u6297\u5668\u63a2\u7d22\u6709\u9650\u800c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e13\u5bb6\u5f15\u5bfc\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u63d0\u5347\u653b\u51fb\u7b56\u7565\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4ece\u6210\u529f\u653b\u51fb\u6f14\u793a\u4e2d\u63d0\u53d6\u4e13\u5bb6\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u4e13\u5bb6\u67b6\u6784\u589e\u5f3a\u8de8\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002\u968f\u540e\uff0c\u8be5\u4e13\u5bb6\u7b56\u7565\u901a\u8fc7KL\u6563\u5ea6\u6b63\u5219\u5316\u5f15\u5bfcDRL\u5bf9\u6297\u5668\u3002\u7531\u4e8e\u573a\u666f\u591a\u6837\u6027\uff0c\u4e13\u5bb6\u7b56\u7565\u53ef\u80fd\u4e0d\u5b8c\u7f8e\uff0c\u4e3a\u6b64\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u6027\u80fd\u611f\u77e5\u9000\u706b\u7b56\u7565\uff0c\u968f\u7740\u5bf9\u6297\u5668\u6027\u80fd\u63d0\u5347\u9010\u6b65\u51cf\u5c11\u5bf9\u4e13\u5bb6\u7684\u4f9d\u8d56\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u78b0\u649e\u7387\u3001\u653b\u51fb\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e13\u5bb6\u7b56\u7565\u4e0d\u7406\u60f3\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.18306", "pdf": "https://arxiv.org/pdf/2506.18306", "abs": "https://arxiv.org/abs/2506.18306", "authors": ["Andrey Derzhavin", "Denis Larionov"], "title": "Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi", "categories": ["cs.NE", "cs.AI"], "comment": "7 pages, 3 figures", "summary": "This paper presents a lightweight software-based approach for running spiking\nneural networks (SNNs) without relying on specialized neuromorphic hardware or\nframeworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust\nand optimize it for common computing platforms. As a case study, we demonstrate\nour implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset.\nSpiffy achieves 92% accuracy with low latency - just 0.9 ms per training step\nand 0.45 ms per inference step. The code is open-source.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8f6f\u4ef6\u65b9\u6cd5Spiffy\uff0c\u7528\u4e8e\u5728\u666e\u901a\u8ba1\u7b97\u5e73\u53f0\u4e0a\u9ad8\u6548\u8fd0\u884c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09CoLaNET\uff0c\u65e0\u9700\u4f9d\u8d56\u4e13\u7528\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u3002\u4ee5Raspberry Pi\u4e3a\u4f8b\uff0cSpiffy\u5728MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8692%\u7684\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u5ef6\u8fdf\u5206\u522b\u4ec5\u4e3a0.9\u6beb\u79d2\u548c0.45\u6beb\u79d2\u3002", "motivation": "\u5f53\u524d\u8fd0\u884c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u9700\u8981\u4e13\u7528\u786c\u4ef6\u6216\u6846\u67b6\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8f6f\u4ef6\u4f18\u5316\uff0c\u5728\u666e\u901a\u8ba1\u7b97\u5e73\u53f0\u4e0a\u9ad8\u6548\u5b9e\u73b0SNN\uff0c\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\u3002", "method": "\u91c7\u7528Rust\u8bed\u8a00\u5b9e\u73b0\u7279\u5b9aSNN\u67b6\u6784CoLaNET\uff0c\u5e76\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u666e\u901a\u8ba1\u7b97\u5e73\u53f0\u3002\u4ee5Raspberry Pi\u4e3a\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u5176\u5728MNIST\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "Spiffy\u5728Raspberry Pi\u4e0a\u5b9e\u73b0\u4e8692%\u7684\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u5ef6\u8fdf\u5206\u522b\u4e3a0.9\u6beb\u79d2\u548c0.45\u6beb\u79d2\uff0c\u4ee3\u7801\u5f00\u6e90\u3002", "conclusion": "Spiffy\u8bc1\u660e\u4e86\u901a\u8fc7\u8f6f\u4ef6\u4f18\u5316\u53ef\u5728\u666e\u901a\u8ba1\u7b97\u5e73\u53f0\u4e0a\u9ad8\u6548\u8fd0\u884cSNN\uff0c\u4e3a\u5e7f\u6cdb\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "paper_title_zh": "Spiffy\uff1a\u4e3a\u6811\u8393\u6d3e\u9ad8\u6548\u5b9e\u73b0CoLaNET", "abstract_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8f6f\u4ef6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u4f9d\u8d56\u4e13\u7528\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u6216\u6846\u67b6\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u3002\u6211\u4eec\u91c7\u7528Rust\u8bed\u8a00\u5b9e\u73b0\u4e86\u4e00\u79cd\u7279\u5b9a\u7684SNN\u67b6\u6784\uff08CoLaNET\uff09\uff0c\u5e76\u9488\u5bf9\u666e\u901a\u8ba1\u7b97\u5e73\u53f0\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u5728\u6811\u8393\u6d3e\u4e0a\u4f7f\u7528MNIST\u6570\u636e\u96c6\u5c55\u793a\u4e86\u6211\u4eec\u7684\u5b9e\u73b0\uff08\u79f0\u4e3aSpiffy\uff09\u3002Spiffy\u5b9e\u73b0\u4e8692%\u7684\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u5ef6\u8fdf\u5206\u522b\u4ec5\u4e3a0.9\u6beb\u79d2\u548c0.45\u6beb\u79d2\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.18309", "pdf": "https://arxiv.org/pdf/2506.18309", "abs": "https://arxiv.org/abs/2506.18309", "authors": ["Lu Wang", "Di Zhang", "Fangkai Yang", "Pu Zhao", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Weiwei Deng", "Dongmei Zhang", "Feng Sun", "Qi Zhang"], "title": "LettinGo: Explore User Profile Generation for Recommendation System", "categories": ["cs.IR", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLettinGo\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u591a\u6837\u5316\u548c\u81ea\u9002\u5e94\u7684\u7528\u6237\u753b\u50cf\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5d4c\u5165\u7684\u7528\u6237\u753b\u50cf\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\uff0c\u800c\u73b0\u6709\u6587\u672c\u753b\u50cf\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u683c\u5f0f\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u591a\u6837\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u753b\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "LettinGo\u6846\u67b6\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u901a\u8fc7\u591a\u4e2aLLM\u63a2\u7d22\u591a\u6837\u5316\u7684\u7528\u6237\u753b\u50cf\uff1b2\uff09\u57fa\u4e8e\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u8bc4\u4f30\u753b\u50cf\u8d28\u91cf\uff1b3\uff09\u5229\u7528\u4efb\u52a1\u6027\u80fd\u751f\u6210\u7684\u6210\u5bf9\u504f\u597d\u6570\u636e\u5bf9\u9f50\u753b\u50cf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLettinGo\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3001\u7075\u6d3b\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "LettinGo\u901a\u8fc7\u7ed3\u5408LLM\u7684\u8868\u8fbe\u80fd\u529b\u548c\u4efb\u52a1\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7528\u6237\u753b\u50cf\u751f\u6210\u65b9\u6848\u3002", "paper_title_zh": "LettinGo\uff1a\u63a2\u7d22\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u753b\u50cf\u751f\u6210", "abstract_zh": "\u7528\u6237\u753b\u50cf\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u5c06\u539f\u59cb\u7528\u6237\u4ea4\u4e92\u6570\u636e\u8f6c\u5316\u4e3a\u7b80\u6d01\u4e14\u7ed3\u6784\u5316\u7684\u8868\u793a\uff0c\u4ece\u800c\u9a71\u52a8\u4e2a\u6027\u5316\u63a8\u8350\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u753b\u50cf\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\uff0c\u800c\u8fd1\u671f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u5c55\u4f7f\u5f97\u57fa\u4e8e\u6587\u672c\u7684\u753b\u50cf\u66f4\u5177\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u900f\u660e\u5ea6\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u683c\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u7528\u6237\u884c\u4e3a\u591a\u6837\u6027\u7684\u80fd\u529b\u3002\u672c\u6587\u63d0\u51faLettinGo\uff0c\u4e00\u79cd\u751f\u6210\u591a\u6837\u5316\u548c\u81ea\u9002\u5e94\u7528\u6237\u753b\u50cf\u7684\u65b0\u6846\u67b6\u3002\u901a\u8fc7\u5229\u7528LLM\u7684\u8868\u8fbe\u80fd\u529b\u5e76\u7ed3\u5408\u4e0b\u6e38\u63a8\u8350\u4efb\u52a1\u7684\u76f4\u63a5\u53cd\u9988\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u907f\u514d\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u521a\u6027\u7ea6\u675f\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u91c7\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5c06\u753b\u50cf\u751f\u6210\u5668\u4e0e\u4efb\u52a1\u6027\u80fd\u5bf9\u9f50\uff0c\u786e\u4fdd\u753b\u50cf\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002LettinGo\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u901a\u8fc7\u591a\u4e2aLLM\u63a2\u7d22\u591a\u6837\u5316\u7528\u6237\u753b\u50cf\uff1b2\uff09\u57fa\u4e8e\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u8bc4\u4f30\u753b\u50cf\u8d28\u91cf\uff1b3\uff09\u5229\u7528\u4efb\u52a1\u6027\u80fd\u751f\u6210\u7684\u6210\u5bf9\u504f\u597d\u6570\u636e\u5bf9\u9f50\u753b\u50cf\u751f\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3001\u7075\u6d3b\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u5c06\u753b\u50cf\u751f\u6210\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u63a8\u8350\u7cfb\u7edf\u7684\u5173\u952e\u521b\u65b0\u3002"}}
{"id": "2506.18315", "pdf": "https://arxiv.org/pdf/2506.18315", "abs": "https://arxiv.org/abs/2506.18315", "authors": ["Lehan He", "Zeren Chen", "Zhe Zhang", "Jing Shao", "Xiang Gao", "Lu Sheng"], "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u6d4b\u8bd5\uff08PBT\uff09\u7684\u65b0\u6846\u67b6Property-Generated Solver\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u4ee3\u7801\u529f\u80fd\u6b63\u786e\u6027\u3002\u901a\u8fc7\u4e24\u4e2a\u534f\u4f5c\u7684LLM\u4ee3\u7406\uff08\u751f\u6210\u5668\u548c\u6d4b\u8bd5\u5668\uff09\uff0c\u8be5\u6846\u67b6\u907f\u514d\u4e86\u4f20\u7edf\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u5728\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6b63\u786e\u6027\u96be\u4ee5\u4fdd\u8bc1\u3002\u4f20\u7edf\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u56e0\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u7a00\u7f3a\u6216\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u7684\u7f3a\u9677\uff08\u5982\u504f\u5dee\u6d4b\u8bd5\u6216\u4e0d\u51c6\u786e\u7684\u8f93\u51fa\u9884\u6d4b\uff09\u800c\u6548\u679c\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faProperty-Generated Solver\u6846\u67b6\uff0c\u5229\u7528\u5c5e\u6027\u6d4b\u8bd5\uff08PBT\uff09\u9a8c\u8bc1\u7a0b\u5e8f\u7684\u9ad8\u5c42\u5c5e\u6027\u6216\u4e0d\u53d8\u5f0f\uff0c\u800c\u975e\u4f9d\u8d56\u5177\u4f53\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\u3002\u6846\u67b6\u5305\u542b\u4e24\u4e2aLLM\u4ee3\u7406\uff1a\u751f\u6210\u5668\u8d1f\u8d23\u4ee3\u7801\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\uff0c\u6d4b\u8bd5\u5668\u7ba1\u7406PBT\u751f\u547d\u5468\u671f\u5e76\u4ece\u5c5e\u6027\u8fdd\u89c4\u4e2d\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProperty-Generated Solver\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfTDD\u65b9\u6cd5\uff0cpass@1\u6307\u6807\u76f8\u5bf9\u63d0\u5347\u4e8623.1%\u81f337.3%\u3002", "conclusion": "\u901a\u8fc7\u5c06PBT\u4f5c\u4e3a\u6838\u5fc3\u9a8c\u8bc1\u5f15\u64ce\uff0cProperty-Generated Solver\u4e3aLLM\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "paper_title_zh": "\u5229\u7528\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\u6865\u63a5LLM\u4ee3\u7801\u751f\u6210\u4e0e\u9a8c\u8bc1", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u64c5\u957f\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u786e\u4fdd\u5176\u8f93\u51fa\u5728\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6b63\u786e\u6027\u4ecd\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218\u3002\u4f20\u7edf\u7684\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u867d\u4e3a\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u4f46\u5176\u5728LLM\u4e2d\u7684\u5e94\u7528\u5e38\u56e0\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u7a00\u7f3a\u6216\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u7684\u7f3a\u9677\uff08\u5982\u504f\u5dee\u6d4b\u8bd5\u6216\u4e0d\u51c6\u786e\u7684\u8f93\u51fa\u9884\u6d4b\uff09\u800c\u53d7\u9650\u3002\u672c\u6587\u63d0\u51faProperty-Generated Solver\uff0c\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\uff08PBT\uff09\u9a8c\u8bc1\u7a0b\u5e8f\u7684\u9ad8\u5c42\u5c5e\u6027\u6216\u4e0d\u53d8\u5f0f\uff0c\u800c\u975e\u4f9d\u8d56\u5177\u4f53\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\u3002\u8fd9\u4e9b\u5c5e\u6027\u901a\u5e38\u6bd4\u76f4\u63a5\u9884\u6d4b\u8be6\u5c3d\u6d4b\u8bd5\u9884\u8a00\u66f4\u6613\u5b9a\u4e49\u548c\u9a8c\u8bc1\uff0c\u4ece\u800c\u6253\u7834\u4e86\u6d4b\u8bd5\u53ef\u80fd\u4e0e\u5f85\u9a8c\u8bc1\u4ee3\u7801\u5171\u4eab\u7f3a\u9677\u7684\u201c\u81ea\u6211\u6b3a\u9a97\u5faa\u73af\u201d\u3002Property-Generated Solver\u91c7\u7528\u4e24\u4e2a\u534f\u4f5c\u7684LLM\u4ee3\u7406\uff1a\u751f\u6210\u5668\u4e13\u6ce8\u4e8e\u4ee3\u7801\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\uff0c\u6d4b\u8bd5\u5668\u5219\u7ba1\u7406PBT\u751f\u547d\u5468\u671f\u5e76\u4ece\u5c5e\u6027\u8fdd\u89c4\u4e2d\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u53cd\u9988\u3002\u7531\u6b64\u4ea7\u751f\u7684\u5168\u9762\u4e14\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u6307\u5bfc\u751f\u6210\u5668\u8fdb\u884c\u4f18\u5316\u3002\u901a\u8fc7\u5c06PBT\u4f5c\u4e3a\u8fd9\u4e00\u8fed\u4ee3\u95ed\u73af\u8303\u5f0f\u4e2d\u7684\u6838\u5fc3\u9a8c\u8bc1\u5f15\u64ce\uff0cProperty-Generated Solver\u4e3a\u5f15\u5bfcLLM\u751f\u6210\u66f4\u6b63\u786e\u4e14\u6cdb\u5316\u7684\u4ee3\u7801\u63d0\u4f9b\u4e86\u7a33\u5065\u673a\u5236\u3002\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProperty-Generated Solver\u76f8\u5bf9\u4e8e\u4f20\u7edfTDD\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684pass@1\u63d0\u5347\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe23.1%\u81f337.3%\u3002"}}
{"id": "2506.18327", "pdf": "https://arxiv.org/pdf/2506.18327", "abs": "https://arxiv.org/abs/2506.18327", "authors": ["Tahsin Alamgir Kheya", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommendation systems play a crucial role in our daily lives by impacting\nuser experience across various domains, including e-commerce, job\nadvertisements, entertainment, etc. Given the vital role of such systems in our\nlives, practitioners must ensure they do not produce unfair and imbalanced\nrecommendations. Previous work addressing bias in recommendations overlooked\nbias in certain item categories, potentially leaving some biases unaddressed.\nAdditionally, most previous work on fair re-ranking focused on binary-sensitive\nattributes. In this paper, we address these issues by proposing a\nfairness-aware re-ranking approach that helps mitigate bias in different\ncategories of items. This re-ranking approach leverages existing biases to\ncorrect disparities in recommendations across various demographic groups. We\nshow how our approach can mitigate bias on multiple sensitive attributes,\nincluding gender, age, and occupation. We experimented on three real-world\ndatasets to evaluate the effectiveness of our re-ranking scheme in mitigating\nbias in recommendations. Our results show how this approach helps mitigate\nsocial bias with little to no degradation in performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e0d\u540c\u7c7b\u522b\u9879\u76ee\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u51cf\u5c11\u6027\u522b\u3001\u5e74\u9f84\u548c\u804c\u4e1a\u7b49\u591a\u91cd\u654f\u611f\u5c5e\u6027\u504f\u89c1\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5728\u7535\u5b50\u5546\u52a1\u3001\u62db\u8058\u5e7f\u544a\u548c\u5a31\u4e50\u7b49\u9886\u57df\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5168\u9762\u89e3\u51b3\u4e0d\u540c\u7c7b\u522b\u9879\u76ee\u7684\u504f\u89c1\u95ee\u9898\uff0c\u4e14\u591a\u6570\u516c\u5e73\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4e8c\u5143\u654f\u611f\u5c5e\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u504f\u89c1\u7ea0\u6b63\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u63a8\u8350\u5dee\u5f02\uff0c\u5e76\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6027\u522b\u3001\u5e74\u9f84\u548c\u804c\u4e1a\u7b49\u591a\u91cd\u654f\u611f\u5c5e\u6027\u7684\u504f\u89c1\uff0c\u4e14\u5bf9\u63a8\u8350\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u516c\u5e73\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u8350\u6027\u80fd\u3002", "paper_title_zh": "\u504f\u89c1\u5bf9\u504f\u89c1\u2014\u2014\u6b63\u4e49\u7684\u66d9\u5149\uff1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u5bf9\u6297", "abstract_zh": "\u63a8\u8350\u7cfb\u7edf\u901a\u8fc7\u5f71\u54cd\u7535\u5b50\u5546\u52a1\u3001\u62db\u8058\u5e7f\u544a\u548c\u5a31\u4e50\u7b49\u9886\u57df\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u5728\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u9274\u4e8e\u6b64\u7c7b\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u4ece\u4e1a\u8005\u5fc5\u987b\u786e\u4fdd\u5176\u4e0d\u4f1a\u4ea7\u751f\u4e0d\u516c\u5e73\u548c\u4e0d\u5e73\u8861\u7684\u63a8\u8350\u3002\u4ee5\u5f80\u5173\u4e8e\u63a8\u8350\u504f\u89c1\u7684\u7814\u7a76\u5ffd\u7565\u4e86\u67d0\u4e9b\u9879\u76ee\u7c7b\u522b\u7684\u504f\u89c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u90e8\u5206\u504f\u89c1\u672a\u88ab\u89e3\u51b3\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u5173\u4e8e\u516c\u5e73\u91cd\u65b0\u6392\u5e8f\u7684\u7814\u7a76\u4ec5\u5173\u6ce8\u4e8c\u5143\u654f\u611f\u5c5e\u6027\u3002\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u7f13\u89e3\u4e0d\u540c\u7c7b\u522b\u9879\u76ee\u7684\u504f\u89c1\u3002\u8fd9\u79cd\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u5229\u7528\u73b0\u6709\u504f\u89c1\u6765\u7ea0\u6b63\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u63a8\u8350\u5dee\u5f02\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5982\u4f55\u5728\u6027\u522b\u3001\u5e74\u9f84\u548c\u804c\u4e1a\u7b49\u591a\u91cd\u654f\u611f\u5c5e\u6027\u4e0a\u51cf\u5c11\u504f\u89c1\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u91cd\u65b0\u6392\u5e8f\u65b9\u6848\u5728\u51cf\u5c11\u63a8\u8350\u504f\u89c1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\uff0c\u540c\u65f6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\u3002"}}
{"id": "2506.18339", "pdf": "https://arxiv.org/pdf/2506.18339", "abs": "https://arxiv.org/abs/2506.18339", "authors": ["Wei Liu", "Kiran Bacsa", "Loon Ching Tang", "Eleni Chatzi"], "title": "Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics", "categories": ["cs.LG", "cs.AI", "cs.SC", "nlin.CD", "physics.data-an"], "comment": null, "summary": "Understanding and modeling nonlinear dynamical systems is a fundamental\nproblem across scientific and engineering domains. While deep learning has\ndemonstrated remarkable potential for learning complex system behavior,\nachieving models that are both highly accurate and physically interpretable\nremains a major challenge. To address this, we propose Structured\nKolmogorov-Arnold Neural ODEs (SKANODEs), a novel framework that integrates\nstructured state-space modeling with the Kolmogorov-Arnold Network (KAN).\nSKANODE first employs a fully trainable KAN as a universal function\napproximator within a structured Neural ODE framework to perform virtual\nsensing, recovering latent states that correspond to physically interpretable\nquantities such as positions and velocities. Once this structured latent\nrepresentation is established, we exploit the symbolic regression capability of\nKAN to extract compact and interpretable expressions for the system's governing\ndynamics. The resulting symbolic expression is then substituted back into the\nNeural ODE framework and further calibrated through continued training to\nrefine its coefficients, enhancing both the precision of the discovered\nequations and the predictive accuracy of system responses. Extensive\nexperiments on both simulated and real-world systems demonstrate that SKANODE\nachieves superior performance while offering interpretable, physics-consistent\nmodels that uncover the underlying mechanisms of nonlinear dynamical systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316Kolmogorov-Arnold\u795e\u7ecfODE\uff08SKANODE\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u4e0eKolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\uff0c\u7528\u4e8e\u5b66\u4e60\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u865a\u62df\u611f\u77e5\u6062\u590d\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u5e76\u5229\u7528KAN\u7684\u7b26\u53f7\u56de\u5f52\u80fd\u529b\u63d0\u53d6\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u7d27\u51d1\u8868\u8fbe\u5f0f\uff0c\u6700\u7ec8\u901a\u8fc7\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSKANODE\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7cfb\u7edf\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u7269\u7406\u4e00\u81f4\u7684\u6a21\u578b\u3002", "motivation": "\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u7406\u89e3\u662f\u79d1\u5b66\u4e0e\u5de5\u7a0b\u9886\u57df\u7684\u6838\u5fc3\u95ee\u9898\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u590d\u6742\u7cfb\u7edf\u884c\u4e3a\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u5efa\u6a21\u4e0e\u7b26\u53f7\u56de\u5f52\u6280\u672f\uff0c\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u51c6\u786e\u9884\u6d4b\u53c8\u80fd\u63ed\u793a\u7cfb\u7edf\u5185\u5728\u673a\u5236\u7684\u6a21\u578b\u3002", "method": "SKANODE\u6846\u67b6\u9996\u5148\u5229\u7528\u5b8c\u5168\u53ef\u8bad\u7ec3\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u795e\u7ecfODE\u4e2d\u7684\u901a\u7528\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u901a\u8fc7\u865a\u62df\u611f\u77e5\u6062\u590d\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u72b6\u6001\uff08\u5982\u4f4d\u7f6e\u548c\u901f\u5ea6\uff09\u3002\u968f\u540e\uff0c\u5229\u7528KAN\u7684\u7b26\u53f7\u56de\u5f52\u80fd\u529b\u63d0\u53d6\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u7d27\u51d1\u8868\u8fbe\u5f0f\uff0c\u5e76\u5c06\u8be5\u8868\u8fbe\u5f0f\u91cd\u65b0\u5d4c\u5165\u795e\u7ecfODE\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6301\u7eed\u8bad\u7ec3\u4f18\u5316\u7cfb\u6570\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u548c\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cSKANODE\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u5185\u5728\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SKANODE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u5efa\u6a21\u4e0e\u7b26\u53f7\u56de\u5f52\u6280\u672f\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\uff0c\u8fd8\u63d0\u4f9b\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u7406\u89e3\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "paper_title_zh": "\u7ed3\u6784\u5316Kolmogorov-Arnold\u795e\u7ecfODE\uff1a\u7528\u4e8e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u53ef\u89e3\u91ca\u5b66\u4e60\u4e0e\u7b26\u53f7\u53d1\u73b0", "abstract_zh": "\u7406\u89e3\u548c\u5efa\u6a21\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u662f\u79d1\u5b66\u4e0e\u5de5\u7a0b\u9886\u57df\u7684\u57fa\u7840\u95ee\u9898\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u5b66\u4e60\u590d\u6742\u7cfb\u7edf\u884c\u4e3a\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316Kolmogorov-Arnold\u795e\u7ecfODE\uff08SKANODE\uff09\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u4e0eKolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u76f8\u7ed3\u5408\u3002SKANODE\u9996\u5148\u5229\u7528\u5b8c\u5168\u53ef\u8bad\u7ec3\u7684KAN\u4f5c\u4e3a\u7ed3\u6784\u5316\u795e\u7ecfODE\u4e2d\u7684\u901a\u7528\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u901a\u8fc7\u865a\u62df\u611f\u77e5\u6062\u590d\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u72b6\u6001\uff08\u5982\u4f4d\u7f6e\u548c\u901f\u5ea6\uff09\u3002\u968f\u540e\uff0c\u5229\u7528KAN\u7684\u7b26\u53f7\u56de\u5f52\u80fd\u529b\u63d0\u53d6\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u7d27\u51d1\u8868\u8fbe\u5f0f\uff0c\u5e76\u5c06\u8be5\u8868\u8fbe\u5f0f\u91cd\u65b0\u5d4c\u5165\u795e\u7ecfODE\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6301\u7eed\u8bad\u7ec3\u4f18\u5316\u7cfb\u6570\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u548c\u9884\u6d4b\u80fd\u529b\u3002\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cSKANODE\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u7269\u7406\u4e00\u81f4\u7684\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u5185\u5728\u673a\u5236\u3002"}}
{"id": "2506.18340", "pdf": "https://arxiv.org/pdf/2506.18340", "abs": "https://arxiv.org/abs/2506.18340", "authors": ["Floor Eijkelboom", "Heiko Zimmermann", "Sharvaree Vadgama", "Erik J Bekkers", "Max Welling", "Christian A. Naesseth", "Jan-Willem van de Meent"], "title": "Controlled Generation with Equivariant Variational Flow Matching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We derive a controlled generation objective within the framework of\nVariational Flow Matching (VFM), which casts flow matching as a variational\ninference problem. We demonstrate that controlled generation can be implemented\ntwo ways: (1) by way of end-to-end training of conditional generative models,\nor (2) as a Bayesian inference problem, enabling post hoc control of\nunconditional models without retraining. Furthermore, we establish the\nconditions required for equivariant generation and provide an equivariant\nformulation of VFM tailored for molecular generation, ensuring invariance to\nrotations, translations, and permutations. We evaluate our approach on both\nuncontrolled and controlled molecular generation, achieving state-of-the-art\nperformance on uncontrolled generation and outperforming state-of-the-art\nmodels in controlled generation, both with end-to-end training and in the\nBayesian inference setting. This work strengthens the connection between\nflow-based generative modeling and Bayesian inference, offering a scalable and\nprincipled framework for constraint-driven and symmetry-aware generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u6d41\u5339\u914d\uff08VFM\uff09\u7684\u53d7\u63a7\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5e76\u5728\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53d8\u5206\u6d41\u5339\u914d\u6846\u67b6\u5b9e\u73b0\u53d7\u63a7\u751f\u6210\uff0c\u540c\u65f6\u89e3\u51b3\u5206\u5b50\u751f\u6210\u4e2d\u7684\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u4e3a\u7ea6\u675f\u9a71\u52a8\u548c\u5bf9\u79f0\u6027\u611f\u77e5\u7684\u751f\u6210\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u79cd\u53d7\u63a7\u751f\u6210\u5b9e\u73b0\uff1a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\u548c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u5206\u5b50\u751f\u6210\u7684\u7b49\u53d8VFM\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u53d7\u63a7\u548c\u53d7\u63a7\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u8d1d\u53f6\u65af\u63a8\u7406\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u5316\u4e86\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u4e0e\u8d1d\u53f6\u65af\u63a8\u7406\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3a\u7ea6\u675f\u9a71\u52a8\u548c\u5bf9\u79f0\u6027\u611f\u77e5\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7406\u8bba\u6846\u67b6\u3002", "paper_title_zh": "\u57fa\u4e8e\u7b49\u53d8\u53d8\u5206\u6d41\u5339\u914d\u7684\u53d7\u63a7\u751f\u6210", "abstract_zh": "\u6211\u4eec\u5728\u53d8\u5206\u6d41\u5339\u914d\uff08VFM\uff09\u6846\u67b6\u4e0b\u63a8\u5bfc\u4e86\u4e00\u79cd\u53d7\u63a7\u751f\u6210\u76ee\u6807\uff0c\u5c06\u6d41\u5339\u914d\u95ee\u9898\u8f6c\u5316\u4e3a\u53d8\u5206\u63a8\u65ad\u95ee\u9898\u3002\u6211\u4eec\u5c55\u793a\u4e86\u53d7\u63a7\u751f\u6210\u7684\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a\uff081\uff09\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff1b\uff082\uff09\u4f5c\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5bf9\u65e0\u6761\u4ef6\u6a21\u578b\u8fdb\u884c\u540e\u9a8c\u63a7\u5236\u3002\u6b64\u5916\uff0c\u6211\u4eec\u786e\u7acb\u4e86\u7b49\u53d8\u751f\u6210\u7684\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u5206\u5b50\u751f\u6210\u7684\u7b49\u53d8VFM\u6846\u67b6\uff0c\u786e\u4fdd\u5bf9\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u7f6e\u6362\u7684\u4e0d\u53d8\u6027\u3002\u6211\u4eec\u5728\u975e\u53d7\u63a7\u548c\u53d7\u63a7\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5728\u975e\u53d7\u63a7\u751f\u6210\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u8bbe\u7f6e\u4e0b\u7684\u53d7\u63a7\u751f\u6210\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u672c\u7814\u7a76\u5f3a\u5316\u4e86\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u4e0e\u8d1d\u53f6\u65af\u63a8\u7406\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3a\u7ea6\u675f\u9a71\u52a8\u548c\u5bf9\u79f0\u6027\u611f\u77e5\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2506.18365", "pdf": "https://arxiv.org/pdf/2506.18365", "abs": "https://arxiv.org/abs/2506.18365", "authors": ["Imene Tarakli", "Samuele Vinanzi", "Richard Moore", "Alessandro Di Nuovo"], "title": "Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Despite growing interest in Learning-by-Teaching (LbT), few studies have\nexplored how this paradigm can be implemented with autonomous, peer-like social\nrobots in real classrooms. Most prior work has relied on scripted or\nWizard-of-Oz behaviors, limiting our understanding of how real-time,\ninteractive learning can be supported by artificial agents. This study\naddresses this gap by introducing Interactive Reinforcement Learning (RL) as a\ncognitive model for teachable social robots. We conducted two between-subject\nexperiments with 58 primary school children, who either taught a robot or\npracticed independently on a tablet while learning French vocabulary\n(memorization) and grammatical rules (inference). The robot, powered by\nInteractive RL, learned from the child's evaluative feedback. Children in the\nLbT condition achieved significantly higher retention gains compared to those\nin the self-practice condition, especially on the grammar task. Learners with\nlower prior knowledge benefited most from teaching the robot. Behavioural\nmetrics revealed that children adapted their teaching strategies over time and\nengaged more deeply during inference tasks. This work makes two contributions:\n(1) it introduces Interactive RL as a pedagogically effective and scalable\nmodel for peer-robot learning, and (2) it demonstrates, for the first time, the\nfeasibility of deploying multiple autonomous robots simultaneously in real\nclassrooms. These findings extend theoretical understanding of LbT by showing\nthat social robots can function not only as passive tutees but as adaptive\npartners that enhance meta-cognitive engagement and long-term learning\noutcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u513f\u7ae5\u4e0e\u81ea\u4e3b\u793e\u4ea4\u673a\u5668\u4eba\u901a\u8fc7\u4e92\u52a8\u5f3a\u5316\u5b66\u4e60\uff08Interactive RL\uff09\u5171\u540c\u5b66\u4e60\u7684\u6548\u679c\uff0c\u53d1\u73b0\u513f\u7ae5\u5728\u6559\u6388\u673a\u5668\u4eba\u6cd5\u8bed\u8bcd\u6c47\u548c\u8bed\u6cd5\u65f6\uff0c\u77e5\u8bc6\u4fdd\u7559\u7387\u663e\u8457\u9ad8\u4e8e\u81ea\u4e3b\u7ec3\u4e60\uff0c\u5c24\u5176\u662f\u8bed\u6cd5\u4efb\u52a1\u548c\u4f4e\u57fa\u7840\u5b66\u4e60\u8005\u3002", "motivation": "\u5c3d\u7ba1\u201c\u901a\u8fc7\u6559\u5b66\u5b66\u4e60\u201d\uff08LbT\uff09\u6a21\u5f0f\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u591a\u4f9d\u8d56\u811a\u672c\u6216\u201c\u7eff\u91ce\u4ed9\u8e2a\u201d\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u81ea\u4e3b\u793e\u4ea4\u673a\u5668\u4eba\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u5b9e\u65f6\u4e92\u52a8\u5b66\u4e60\u6548\u679c\u7684\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e92\u52a8\u5f3a\u5316\u5b66\u4e60\uff08Interactive RL\uff09\u4f5c\u4e3a\u673a\u5668\u4eba\u7684\u8ba4\u77e5\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\uff0858\u540d\u5c0f\u5b66\u751f\u53c2\u4e0e\uff09\u6bd4\u8f83\u513f\u7ae5\u6559\u6388\u673a\u5668\u4eba\u4e0e\u81ea\u4e3b\u7ec3\u4e60\u7684\u6548\u679c\uff0c\u4efb\u52a1\u5305\u62ec\u6cd5\u8bed\u8bcd\u6c47\u8bb0\u5fc6\u548c\u8bed\u6cd5\u89c4\u5219\u63a8\u7406\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLbT\u7ec4\u7684\u513f\u7ae5\u5728\u77e5\u8bc6\u4fdd\u7559\u4e0a\u663e\u8457\u4f18\u4e8e\u81ea\u4e3b\u7ec3\u4e60\u7ec4\uff0c\u5c24\u5176\u662f\u8bed\u6cd5\u4efb\u52a1\uff1b\u4f4e\u57fa\u7840\u5b66\u4e60\u8005\u53d7\u76ca\u6700\u5927\u3002\u884c\u4e3a\u6307\u6807\u8868\u660e\u513f\u7ae5\u9010\u6e10\u8c03\u6574\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u66f4\u6295\u5165\u3002", "conclusion": "\u7814\u7a76\u8d21\u732e\u5728\u4e8e\uff1a\uff081\uff09\u63d0\u51faInteractive RL\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u540c\u4f34\u673a\u5668\u4eba\u5b66\u4e60\u6a21\u578b\uff1b\uff082\uff09\u9996\u6b21\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u540c\u65f6\u90e8\u7f72\u591a\u4e2a\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u8bc1\u660e\u5176\u53ef\u884c\u6027\u3002\u793e\u4ea4\u673a\u5668\u4eba\u53ef\u4f5c\u4e3a\u9002\u5e94\u6027\u4f19\u4f34\u63d0\u5347\u5143\u8ba4\u77e5\u53c2\u4e0e\u548c\u957f\u671f\u5b66\u4e60\u6548\u679c\u3002", "paper_title_zh": "\u513f\u7ae5\u4e0e\u673a\u5668\u4eba\u5171\u540c\u5b66\u4e60\uff1a\u901a\u8fc7\u6559\u6388\u540c\u4f34\u5f0f\u4e92\u52a8\u673a\u5668\u4eba\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\u7387", "abstract_zh": "\u5c3d\u7ba1\u201c\u901a\u8fc7\u6559\u5b66\u5b66\u4e60\u201d\uff08LbT\uff09\u6a21\u5f0f\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5c11\u6709\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u5c06\u5176\u5e94\u7528\u4e8e\u771f\u5b9e\u8bfe\u5802\u4e2d\u7684\u81ea\u4e3b\u3001\u540c\u4f34\u5f0f\u793e\u4ea4\u673a\u5668\u4eba\u3002\u6b64\u524d\u7814\u7a76\u591a\u4f9d\u8d56\u811a\u672c\u6216\u201c\u7eff\u91ce\u4ed9\u8e2a\u201d\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u4eba\u5de5\u4ee3\u7406\u5982\u4f55\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\u5b66\u4e60\u7684\u7406\u89e3\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4e92\u52a8\u5f3a\u5316\u5b66\u4e60\uff08Interactive RL\uff09\u4f5c\u4e3a\u53ef\u6559\u5b66\u673a\u5668\u4eba\u7684\u8ba4\u77e5\u6a21\u578b\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4e24\u9879\u5b9e\u9a8c\uff0c58\u540d\u5c0f\u5b66\u751f\u5206\u522b\u6559\u6388\u673a\u5668\u4eba\u6216\u81ea\u4e3b\u7ec3\u4e60\u5e73\u677f\uff0c\u5b66\u4e60\u6cd5\u8bed\u8bcd\u6c47\uff08\u8bb0\u5fc6\uff09\u548c\u8bed\u6cd5\u89c4\u5219\uff08\u63a8\u7406\uff09\u3002\u673a\u5668\u4eba\u901a\u8fc7Interactive RL\u4ece\u513f\u7ae5\u7684\u8bc4\u4ef7\u53cd\u9988\u4e2d\u5b66\u4e60\u3002\u7ed3\u679c\u663e\u793a\uff0cLbT\u7ec4\u7684\u513f\u7ae5\u5728\u77e5\u8bc6\u4fdd\u7559\u4e0a\u663e\u8457\u4f18\u4e8e\u81ea\u4e3b\u7ec3\u4e60\u7ec4\uff0c\u5c24\u5176\u662f\u8bed\u6cd5\u4efb\u52a1\uff1b\u4f4e\u57fa\u7840\u5b66\u4e60\u8005\u53d7\u76ca\u6700\u5927\u3002\u884c\u4e3a\u6307\u6807\u8868\u660e\u513f\u7ae5\u9010\u6e10\u8c03\u6574\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u66f4\u6295\u5165\u3002\u672c\u7814\u7a76\u8d21\u732e\u5728\u4e8e\uff1a\uff081\uff09\u63d0\u51faInteractive RL\u4f5c\u4e3a\u4e00\u79cd\u6559\u80b2\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u540c\u4f34\u673a\u5668\u4eba\u5b66\u4e60\u6a21\u578b\uff1b\uff082\uff09\u9996\u6b21\u8bc1\u660e\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u540c\u65f6\u90e8\u7f72\u591a\u4e2a\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u6269\u5c55\u4e86LbT\u7684\u7406\u8bba\u7406\u89e3\uff0c\u8868\u660e\u793e\u4ea4\u673a\u5668\u4eba\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u88ab\u52a8\u5b66\u4e60\u8005\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u9002\u5e94\u6027\u4f19\u4f34\uff0c\u63d0\u5347\u5143\u8ba4\u77e5\u53c2\u4e0e\u548c\u957f\u671f\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2506.18382", "pdf": "https://arxiv.org/pdf/2506.18382", "abs": "https://arxiv.org/abs/2506.18382", "authors": ["Haotong Du", "Yaqing Wang", "Fei Xiong", "Lei Shao", "Ming Liu", "Hao Gu", "Quanming Yao", "Zhen Wang"], "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "Accepted by KDD 2025", "summary": "With the expansion of business scales and scopes on online platforms,\nmulti-scenario matching has become a mainstream solution to reduce maintenance\ncosts and alleviate data sparsity. The key to effective multi-scenario\nrecommendation lies in capturing both user preferences shared across all\nscenarios and scenario-aware preferences specific to each scenario. However,\nexisting methods often overlook user-specific modeling, limiting the generation\nof personalized user representations. To address this, we propose PERSCEN, an\ninnovative approach that incorporates user-specific modeling into\nmulti-scenario matching. PERSCEN constructs a user-specific feature graph based\non user characteristics and employs a lightweight graph neural network to\ncapture higher-order interaction patterns, enabling personalized extraction of\npreferences shared across scenarios. Additionally, we leverage vector\nquantization techniques to distil scenario-aware preferences from users'\nbehavior sequence within individual scenarios, facilitating user-specific and\nscenario-aware preference modeling. To enhance efficient and flexible\ninformation transfer, we introduce a progressive scenario-aware gated linear\nunit that allows fine-grained, low-latency fusion. Extensive experiments\ndemonstrate that PERSCEN outperforms existing methods. Further efficiency\nanalysis confirms that PERSCEN effectively balances performance with\ncomputational cost, ensuring its practicality for real-world industrial\nsystems.", "AI": {"tldr": "PERSCEN\u662f\u4e00\u79cd\u521b\u65b0\u7684\u591a\u573a\u666f\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u7279\u5b9a\u5efa\u6a21\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u8de8\u573a\u666f\u5171\u4eab\u504f\u597d\uff0c\u7ed3\u5408\u5411\u91cf\u91cf\u5316\u6280\u672f\u63d0\u53d6\u573a\u666f\u611f\u77e5\u504f\u597d\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u591a\u573a\u666f\u63a8\u8350\u65b9\u6cd5\u5ffd\u7565\u7528\u6237\u7279\u5b9a\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u751f\u6210\u4e2a\u6027\u5316\u7528\u6237\u8868\u793a\u3002PERSCEN\u65e8\u5728\u901a\u8fc7\u7528\u6237\u7279\u5b9a\u5efa\u6a21\u548c\u573a\u666f\u611f\u77e5\u504f\u597d\u63d0\u53d6\uff0c\u63d0\u5347\u591a\u573a\u666f\u5339\u914d\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "method": "PERSCEN\u6784\u5efa\u7528\u6237\u7279\u5b9a\u7279\u5f81\u56fe\uff0c\u5229\u7528\u8f7b\u91cf\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u9ad8\u9636\u4ea4\u4e92\u6a21\u5f0f\uff0c\u63d0\u53d6\u8de8\u573a\u666f\u5171\u4eab\u504f\u597d\uff1b\u7ed3\u5408\u5411\u91cf\u91cf\u5316\u6280\u672f\u4ece\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e2d\u63d0\u53d6\u573a\u666f\u611f\u77e5\u504f\u597d\uff1b\u5f15\u5165\u6e10\u8fdb\u5f0f\u573a\u666f\u611f\u77e5\u95e8\u63a7\u7ebf\u6027\u5355\u5143\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4f4e\u5ef6\u8fdf\u4fe1\u606f\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePERSCEN\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u7cfb\u7edf\u3002", "conclusion": "PERSCEN\u901a\u8fc7\u7528\u6237\u7279\u5b9a\u5efa\u6a21\u548c\u573a\u666f\u611f\u77e5\u504f\u597d\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u573a\u666f\u63a8\u8350\u7684\u4e2a\u6027\u5316\u6548\u679c\u548c\u5b9e\u7528\u6027\u3002", "paper_title_zh": "PERSCEN\uff1a\u5b66\u4e60\u4e2a\u6027\u5316\u4ea4\u4e92\u6a21\u5f0f\u548c\u573a\u666f\u504f\u597d\u4ee5\u5b9e\u73b0\u591a\u573a\u666f\u5339\u914d", "abstract_zh": "\u968f\u7740\u5728\u7ebf\u5e73\u53f0\u4e1a\u52a1\u89c4\u6a21\u548c\u8303\u56f4\u7684\u6269\u5927\uff0c\u591a\u573a\u666f\u5339\u914d\u6210\u4e3a\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u548c\u7f13\u89e3\u6570\u636e\u7a00\u758f\u6027\u7684\u4e3b\u6d41\u89e3\u51b3\u65b9\u6848\u3002\u6709\u6548\u7684\u591a\u573a\u666f\u63a8\u8350\u5173\u952e\u5728\u4e8e\u6355\u6349\u7528\u6237\u5728\u6240\u6709\u573a\u666f\u4e2d\u5171\u4eab\u7684\u504f\u597d\u548c\u6bcf\u4e2a\u573a\u666f\u7279\u6709\u7684\u573a\u666f\u611f\u77e5\u504f\u597d\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u7528\u6237\u7279\u5b9a\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u751f\u6210\u4e2a\u6027\u5316\u7528\u6237\u8868\u793a\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faPERSCEN\uff0c\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u5c06\u7528\u6237\u7279\u5b9a\u5efa\u6a21\u878d\u5165\u591a\u573a\u666f\u5339\u914d\u3002PERSCEN\u57fa\u4e8e\u7528\u6237\u7279\u5f81\u6784\u5efa\u7528\u6237\u7279\u5b9a\u7279\u5f81\u56fe\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u9ad8\u9636\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8de8\u573a\u666f\u5171\u4eab\u504f\u597d\u7684\u4e2a\u6027\u5316\u63d0\u53d6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u5411\u91cf\u91cf\u5316\u6280\u672f\u4ece\u7528\u6237\u5728\u5404\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u5e8f\u5217\u4e2d\u63d0\u53d6\u573a\u666f\u611f\u77e5\u504f\u597d\uff0c\u652f\u6301\u7528\u6237\u7279\u5b9a\u548c\u573a\u666f\u611f\u77e5\u504f\u597d\u7684\u5efa\u6a21\u3002\u4e3a\u63d0\u5347\u4fe1\u606f\u4f20\u9012\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5f15\u5165\u6e10\u8fdb\u5f0f\u573a\u666f\u611f\u77e5\u95e8\u63a7\u7ebf\u6027\u5355\u5143\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u7684\u878d\u5408\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660ePERSCEN\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u7684\u6548\u7387\u5206\u6790\u8bc1\u5b9ePERSCEN\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861\uff0c\u786e\u4fdd\u4e86\u5176\u5728\u5b9e\u9645\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.18383", "pdf": "https://arxiv.org/pdf/2506.18383", "abs": "https://arxiv.org/abs/2506.18383", "authors": ["Koushik Viswanadha", "Deepanway Ghosal", "Somak Aditya"], "title": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Logical reasoning is a key task for artificial intelligence due to it's role\nin major downstream tasks such as Question Answering, Summarization. Recent\nmethods in improving the reasoning ability of LLMs fall short in correctly\nconverting a natural language reasoning problem to an equivalent logical\nformulation, which hinders the framework's overall ability to reason. Towards\nthis, we propose to use finetuning on a preference optimization dataset to\nlearn to parse and represent a natural language problem as a whole to a\nconsistent logical program by 1) introducing a new supervised and preference\noptimization dataset LogicPO, and 2) adopting popular techniques such as Direct\nPreference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune\nopen-source LLMs. Our best model with Phi-3.5 consistently outperforms\nGPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%\nless syntax errors. Through the framework and our improved evaluation metrics,\nwe offer a promising direction in improving the logical reasoning of LLMs by\nbetter representing them in their logical formulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u4f18\u5316\u7684\u65b9\u6cd5LogicPO\uff0c\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90LLMs\uff08\u5982Phi-3.5\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u95ee\u9898\u9ad8\u6548\u8f6c\u6362\u4e3aFOL\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u4e8eGPT-3.5-turbo\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u95ee\u9898\u8f6c\u6362\u4e3a\u903b\u8f91\u8868\u793a\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6\u548c\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u5347LLMs\u7684\u903b\u8f91\u8868\u793a\u80fd\u529b\u3002", "method": "1) \u5f15\u5165\u65b0\u7684\u76d1\u7763\u548c\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6LogicPO\uff1b2) \u91c7\u7528DPO\u548cKTO\u7b49\u6280\u672f\u5fae\u8c03\u5f00\u6e90LLMs\uff08\u5982Phi-3.5\uff09\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u903b\u8f91\u7a0b\u5e8f\u3002", "result": "\u6700\u4f73\u6a21\u578bPhi-3.5\u5728\u903b\u8f91\u6b63\u786e\u6027\u4e0a\u6bd4GPT-3.5-turbo\uff088-shot\uff09\u63d0\u534710%\uff0c\u8bed\u6cd5\u9519\u8bef\u51cf\u5c1114%\u3002", "conclusion": "LogicPO\u6846\u67b6\u548c\u8bc4\u4f30\u6307\u6807\u4e3a\u63d0\u5347LLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u5411\uff0c\u901a\u8fc7\u6539\u8fdb\u903b\u8f91\u8868\u793a\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "paper_title_zh": "LOGICPO\uff1a\u57fa\u4e8eLLMs\u548c\u504f\u597d\u4f18\u5316\u7684\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u95ee\u9898\u9ad8\u6548\u7ffb\u8bd1\u4e3a\u4e00\u9636\u903b\u8f91", "abstract_zh": "\u903b\u8f91\u63a8\u7406\u662f\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u4efb\u52a1\uff0c\u56e0\u5176\u5728\u95ee\u7b54\u3001\u6458\u8981\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u95ee\u9898\u8f6c\u6362\u4e3a\u7b49\u6548\u903b\u8f91\u8868\u793a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6\u5fae\u8c03LLMs\uff0c\u4ee5\u5b66\u4e60\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u6574\u4f53\u89e3\u6790\u4e3a\u4e00\u81f4\u7684\u903b\u8f91\u7a0b\u5e8f\uff1a1) \u5f15\u5165\u65b0\u7684\u76d1\u7763\u548c\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6LogicPO\uff1b2) \u91c7\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548cKahneman-Tversky\u4f18\u5316\uff08KTO\uff09\u7b49\u6280\u672f\u5fae\u8c03\u5f00\u6e90LLMs\u3002\u6211\u4eec\u7684\u6700\u4f73\u6a21\u578bPhi-3.5\u5728\u903b\u8f91\u6b63\u786e\u6027\u4e0a\u6bd4GPT-3.5-turbo\uff088-shot\uff09\u63d0\u534710%\uff0c\u8bed\u6cd5\u9519\u8bef\u51cf\u5c1114%\u3002\u901a\u8fc7\u8be5\u6846\u67b6\u548c\u6539\u8fdb\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u6211\u4eec\u4e3a\u63d0\u5347LLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5373\u901a\u8fc7\u4f18\u5316\u903b\u8f91\u8868\u793a\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002"}}
{"id": "2506.18396", "pdf": "https://arxiv.org/pdf/2506.18396", "abs": "https://arxiv.org/abs/2506.18396", "authors": ["Marco Aruta", "Ciro Listone", "Giuseppe Murano", "Aniello Murano"], "title": "ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 1 figure, under review", "summary": "Leukemia diagnosis and monitoring rely increasingly on high-throughput image\ndata, yet conventional clustering methods lack the flexibility to accommodate\nevolving cellular patterns and quantify uncertainty in real time. We introduce\nAdaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable\nframework that combines Convolutional Neural Network-based feature extraction\nwith an online fuzzy clustering engine. ADNF initializes soft partitions via\nFuzzy C-Means, then continuously updates micro-cluster centers, densities, and\nfuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy\nevolution. A topology refinement stage performs density-weighted merging and\nentropy-guided splitting to guard against over- and under-segmentation. On the\nC-NMC leukemia microscopy dataset, our tool achieves a silhouette score of\n0.51, demonstrating superior cohesion and separation over static baselines. The\nmethod's adaptive uncertainty modeling and label-free operation hold immediate\npotential for integration within the INFANT pediatric oncology network,\nenabling scalable, up-to-date support for personalized leukemia management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADNF-Clustering\u7684\u81ea\u9002\u5e94\u52a8\u6001\u795e\u7ecf\u6a21\u7cca\u805a\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u767d\u8840\u75c5\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u63d0\u53d6\u548c\u5728\u7ebf\u6a21\u7cca\u805a\u7c7b\u5f15\u64ce\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u805a\u7c7b\u53c2\u6570\u548c\u62d3\u6251\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6548\u679c\u3002", "motivation": "\u767d\u8840\u75c5\u8bca\u65ad\u548c\u76d1\u6d4b\u4f9d\u8d56\u9ad8\u901a\u91cf\u56fe\u50cf\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7ec6\u80de\u6a21\u5f0f\u5e76\u5b9e\u65f6\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u805a\u7c7b\u53c2\u6570\u5e76\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "ADNF-Clustering\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u63d0\u53d6\u548c\u5728\u7ebf\u6a21\u7cca\u805a\u7c7b\u5f15\u64ce\uff0c\u901a\u8fc7\u6a21\u7ccaC\u5747\u503c\u521d\u59cb\u5316\u8f6f\u5206\u533a\uff0c\u5e76\u4f7f\u7528\u6a21\u7cca\u65f6\u95f4\u6307\u6570\uff08FTI\uff09\u52a8\u6001\u66f4\u65b0\u5fae\u805a\u7c7b\u4e2d\u5fc3\u3001\u5bc6\u5ea6\u548c\u6a21\u7cca\u53c2\u6570\u3002\u62d3\u6251\u4f18\u5316\u9636\u6bb5\u901a\u8fc7\u5bc6\u5ea6\u52a0\u6743\u5408\u5e76\u548c\u71b5\u5f15\u5bfc\u5206\u88c2\u9632\u6b62\u8fc7\u5206\u5272\u548c\u6b20\u5206\u5272\u3002", "result": "\u5728C-NMC\u767d\u8840\u75c5\u663e\u5fae\u955c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8f6e\u5ed3\u5206\u6570\u8fbe\u52300.51\uff0c\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u597d\u7684\u805a\u7c7b\u5185\u805a\u6027\u548c\u5206\u79bb\u6027\u3002", "conclusion": "ADNF-Clustering\u7684\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u65e0\u6807\u7b7e\u64cd\u4f5c\u4f7f\u5176\u5728\u4e2a\u6027\u5316\u767d\u8840\u75c5\u7ba1\u7406\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u513f\u79d1\u80bf\u7624\u7f51\u7edcINFANT\u3002", "paper_title_zh": "ADNF-Clustering\uff1a\u4e00\u79cd\u7528\u4e8e\u767d\u8840\u75c5\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u52a8\u6001\u795e\u7ecf\u6a21\u7cca\u805a\u7c7b\u65b9\u6cd5", "abstract_zh": "\u767d\u8840\u75c5\u7684\u8bca\u65ad\u548c\u76d1\u6d4b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u9ad8\u901a\u91cf\u56fe\u50cf\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u7684\u805a\u7c7b\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7ec6\u80de\u6a21\u5f0f\u5e76\u5b9e\u65f6\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u9002\u5e94\u52a8\u6001\u795e\u7ecf\u6a21\u7cca\u805a\u7c7b\uff08ADNF-Clustering\uff09\u7684\u65b0\u578b\u6d41\u5f0f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u63d0\u53d6\u548c\u5728\u7ebf\u6a21\u7cca\u805a\u7c7b\u5f15\u64ce\u3002ADNF\u901a\u8fc7\u6a21\u7ccaC\u5747\u503c\u521d\u59cb\u5316\u8f6f\u5206\u533a\uff0c\u968f\u540e\u4f7f\u7528\u6a21\u7cca\u65f6\u95f4\u6307\u6570\uff08FTI\uff09\u52a8\u6001\u66f4\u65b0\u5fae\u805a\u7c7b\u4e2d\u5fc3\u3001\u5bc6\u5ea6\u548c\u6a21\u7cca\u53c2\u6570\uff0c\u4ee5\u6d4b\u91cf\u71b5\u7684\u6f14\u53d8\u3002\u62d3\u6251\u4f18\u5316\u9636\u6bb5\u901a\u8fc7\u5bc6\u5ea6\u52a0\u6743\u5408\u5e76\u548c\u71b5\u5f15\u5bfc\u5206\u88c2\u9632\u6b62\u8fc7\u5206\u5272\u548c\u6b20\u5206\u5272\u3002\u5728C-NMC\u767d\u8840\u75c5\u663e\u5fae\u955c\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u5de5\u5177\u8f6e\u5ed3\u5206\u6570\u8fbe\u52300.51\uff0c\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7684\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u65e0\u6807\u7b7e\u64cd\u4f5c\u4e3a\u5176\u5728INFANT\u513f\u79d1\u80bf\u7624\u7f51\u7edc\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6f5c\u5728\u53ef\u80fd\uff0c\u4ece\u800c\u4e3a\u4e2a\u6027\u5316\u767d\u8840\u75c5\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6700\u65b0\u652f\u6301\u3002"}}
{"id": "2506.18403", "pdf": "https://arxiv.org/pdf/2506.18403", "abs": "https://arxiv.org/abs/2506.18403", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u8c03\u8bd5\u8870\u51cf\u6307\u6570\uff08DDI\uff09\u201d\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316AI\u8c03\u8bd5\u80fd\u529b\u7684\u8870\u51cf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u6027\u91cd\u542f\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u8c03\u8bd5\u6548\u679c\u3002", "motivation": "\u5f53\u524dAI\u8c03\u8bd5\u80fd\u529b\u57282-3\u6b21\u5c1d\u8bd5\u540e\u4f1a\u663e\u8457\u8870\u51cf60-80%\uff0c\u800c\u8fed\u4ee3\u8c03\u8bd5\u662f\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u5173\u952e\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u91cf\u5316\u5de5\u5177\u548c\u4f18\u5316\u7b56\u7565\u3002", "method": "\u5f15\u5165\u8c03\u8bd5\u8870\u51cf\u6307\u6570\uff08DDI\uff09\u4f5c\u4e3a\u91cf\u5316\u5de5\u5177\uff0c\u9884\u6d4b\u8c03\u8bd5\u5931\u6548\u70b9\uff0c\u5e76\u63d0\u51fa\u7b56\u7565\u6027\u91cd\u542f\u65b9\u6cd5\uff0c\u4ece\u5229\u7528\u8f6c\u5411\u63a2\u7d22\uff0c\u4ee5\u63d0\u5347\u8c03\u8bd5\u6548\u679c\u3002", "result": "DDI\u63ed\u793a\u4e86\u5f53\u524dAI\u8c03\u8bd5\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u6027\u91cd\u542f\u663e\u8457\u63d0\u5347\u4e86\u8c03\u8bd5\u6548\u679c\uff0c\u4e3a\u4f18\u5316\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u7b56\u7565\u63d0\u4f9b\u4e86\u9996\u4e2a\u91cf\u5316\u6846\u67b6\u3002", "conclusion": "DDI\u4e3aAI\u8c03\u8bd5\u63d0\u4f9b\u4e86\u91cf\u5316\u5206\u6790\u5de5\u5177\uff0c\u7b56\u7565\u6027\u91cd\u542f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8c03\u8bd5\u80fd\u529b\u8870\u51cf\u95ee\u9898\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "\u8c03\u8bd5\u8870\u51cf\u6307\u6570\uff1a\u91cd\u65b0\u601d\u8003\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c03\u8bd5\u7b56\u7565", "abstract_zh": "AI\u8c03\u8bd5\u7684\u6548\u679c\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u6307\u6570\u8870\u51cf\u6a21\u5f0f\uff1b\u5927\u591a\u6570\u6a21\u578b\u5728\u4ec52-3\u6b21\u5c1d\u8bd5\u540e\u5c31\u4f1a\u4e27\u593160-80%\u7684\u8c03\u8bd5\u80fd\u529b\uff0c\u800c\u8fed\u4ee3\u8c03\u8bd5\u662f\u5b9e\u9645\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u5173\u952e\u80fd\u529b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u8c03\u8bd5\u8870\u51cf\u6307\u6570\uff08DDI\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u8c03\u8bd5\u4f55\u65f6\u5931\u6548\u5e76\u9884\u6d4b\u5e72\u9884\u70b9\u3002\u6211\u4eec\u7684\u7b56\u7565\u6027\u91cd\u542f\u65b9\u6cd5\u5728\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u4ece\u5229\u7528\u8f6c\u5411\u63a2\u7d22\uff0c\u8868\u660e\u9002\u65f6\u5e72\u9884\u53ef\u4ee5\u633d\u6551\u8c03\u8bd5\u6548\u679c\u3002DDI\u63ed\u793a\u4e86\u5f53\u524dAI\u8c03\u8bd5\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u4f18\u5316\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u7b56\u7565\u63d0\u4f9b\u4e86\u9996\u4e2a\u91cf\u5316\u6846\u67b6\u3002"}}
{"id": "2506.18499", "pdf": "https://arxiv.org/pdf/2506.18499", "abs": "https://arxiv.org/abs/2506.18499", "authors": ["Alessandra Agostini", "Andrea Maurino", "Blerina Spahiu"], "title": "PuckTrick: A Library for Making Synthetic Data More Realistic", "categories": ["cs.LG", "cs.AI", "cs.DB", "H.4.1; I.2.1"], "comment": "17 pages, 3 figures", "summary": "The increasing reliance on machine learning (ML) models for decision-making\nrequires high-quality training data. However, access to real-world datasets is\noften restricted due to privacy concerns, proprietary restrictions, and\nincomplete data availability. As a result, synthetic data generation (SDG) has\nemerged as a viable alternative, enabling the creation of artificial datasets\nthat preserve the statistical properties of real data while ensuring privacy\ncompliance. Despite its advantages, synthetic data is often overly clean and\nlacks real-world imperfections, such as missing values, noise, outliers, and\nmisclassified labels, which can significantly impact model generalization and\nrobustness. To address this limitation, we introduce Pucktrick, a Python\nlibrary designed to systematically contaminate synthetic datasets by\nintroducing controlled errors. The library supports multiple error types,\nincluding missing data, noisy values, outliers, label misclassification,\nduplication, and class imbalance, offering a structured approach to evaluating\nML model resilience under real-world data imperfections. Pucktrick provides two\ncontamination modes: one for injecting errors into clean datasets and another\nfor further corrupting already contaminated datasets. Through extensive\nexperiments on real-world financial datasets, we evaluate the impact of\nsystematic data contamination on model performance. Our findings demonstrate\nthat ML models trained on contaminated synthetic data outperform those trained\non purely synthetic, error-free data, particularly for tree-based and linear\nmodels such as SVMs and Extra Trees.", "AI": {"tldr": "PuckTrick\u662f\u4e00\u4e2aPython\u5e93\uff0c\u7528\u4e8e\u5728\u5408\u6210\u6570\u636e\u4e2d\u5f15\u5165\u53d7\u63a7\u9519\u8bef\uff08\u5982\u7f3a\u5931\u503c\u3001\u566a\u58f0\u3001\u5f02\u5e38\u503c\u7b49\uff09\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u6c61\u67d3\u540e\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u7eaf\u51c0\u5408\u6210\u6570\u636e\u7684\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u4e13\u6709\u9650\u5236\uff0c\u771f\u5b9e\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5408\u6210\u6570\u636e\u8fc7\u4e8e\u5e72\u51c0\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u4e0d\u5b8c\u7f8e\uff08\u5982\u566a\u58f0\u3001\u7f3a\u5931\u503c\u7b49\uff09\uff0c\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002PuckTrick\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PuckTrick\u63d0\u4f9b\u4e24\u79cd\u6c61\u67d3\u6a21\u5f0f\uff1a\u4e00\u79cd\u5728\u5e72\u51c0\u6570\u636e\u4e2d\u6ce8\u5165\u9519\u8bef\uff0c\u53e6\u4e00\u79cd\u8fdb\u4e00\u6b65\u6c61\u67d3\u5df2\u6c61\u67d3\u7684\u6570\u636e\u3002\u652f\u6301\u591a\u79cd\u9519\u8bef\u7c7b\u578b\uff08\u7f3a\u5931\u6570\u636e\u3001\u566a\u58f0\u3001\u5f02\u5e38\u503c\u3001\u6807\u7b7e\u9519\u8bef\u7b49\uff09\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u7f3a\u9677\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528PuckTrick\u6c61\u67d3\u540e\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u6811\u548c\u7ebf\u6027\u6a21\u578b\u5982SVM\u548cExtra Trees\uff09\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u7eaf\u51c0\u5408\u6210\u6570\u636e\u7684\u6a21\u578b\u3002", "conclusion": "PuckTrick\u901a\u8fc7\u5f15\u5165\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u4e0d\u5b8c\u7f8e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5408\u6210\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "paper_title_zh": "PuckTrick\uff1a\u4e00\u4e2a\u4f7f\u5408\u6210\u6570\u636e\u66f4\u771f\u5b9e\u7684\u5e93", "abstract_zh": "\u968f\u7740\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\u5728\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u3001\u4e13\u6709\u9650\u5236\u548c\u6570\u636e\u4e0d\u5b8c\u6574\uff0c\u771f\u5b9e\u6570\u636e\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002\u56e0\u6b64\uff0c\u5408\u6210\u6570\u636e\u751f\u6210\uff08SDG\uff09\u6210\u4e3a\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u4fdd\u7559\u771f\u5b9e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u4e14\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u4eba\u5de5\u6570\u636e\u96c6\u3002\u5c3d\u7ba1\u5408\u6210\u6570\u636e\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u901a\u5e38\u8fc7\u4e8e\u5e72\u51c0\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u4e0d\u5b8c\u7f8e\uff08\u5982\u7f3a\u5931\u503c\u3001\u566a\u58f0\u3001\u5f02\u5e38\u503c\u548c\u9519\u8bef\u6807\u7b7e\uff09\uff0c\u8fd9\u4e9b\u7f3a\u9677\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86PuckTrick\uff0c\u8fd9\u662f\u4e00\u4e2aPython\u5e93\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u9519\u8bef\u7cfb\u7edf\u6027\u5730\u6c61\u67d3\u5408\u6210\u6570\u636e\u96c6\u3002\u8be5\u5e93\u652f\u6301\u591a\u79cd\u9519\u8bef\u7c7b\u578b\uff0c\u5305\u62ec\u7f3a\u5931\u6570\u636e\u3001\u566a\u58f0\u503c\u3001\u5f02\u5e38\u503c\u3001\u6807\u7b7e\u9519\u8bef\u3001\u91cd\u590d\u6570\u636e\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4e3a\u8bc4\u4f30ML\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u7f3a\u9677\u4e0b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\u3002PuckTrick\u63d0\u4f9b\u4e24\u79cd\u6c61\u67d3\u6a21\u5f0f\uff1a\u4e00\u79cd\u7528\u4e8e\u5728\u5e72\u51c0\u6570\u636e\u4e2d\u6ce8\u5165\u9519\u8bef\uff0c\u53e6\u4e00\u79cd\u7528\u4e8e\u8fdb\u4e00\u6b65\u6c61\u67d3\u5df2\u6c61\u67d3\u7684\u6570\u636e\u3002\u901a\u8fc7\u5bf9\u771f\u5b9e\u91d1\u878d\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u7cfb\u7edf\u6027\u6570\u636e\u6c61\u67d3\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u6c61\u67d3\u540e\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684ML\u6a21\u578b\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u6811\u548c\u7ebf\u6027\u6a21\u578b\u5982SVM\u548cExtra Trees\uff09\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u7eaf\u51c0\u5408\u6210\u6570\u636e\u7684\u6a21\u578b\u3002"}}
{"id": "2506.18530", "pdf": "https://arxiv.org/pdf/2506.18530", "abs": "https://arxiv.org/abs/2506.18530", "authors": ["Muhammad Ihsan Al Hafiz", "Naresh Ravichandran", "Anders Lansner", "Pawel Herman", "Artur Podobas"], "title": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Edge AI applications increasingly require models that can learn and adapt\non-device with minimal energy budget. Traditional deep learning models, while\npowerful, are often overparameterized, energy-hungry, and dependent on cloud\nconnectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian\nConfidence Propagation Neural Network (BCPNN), propose a neuromorphic\nalternative by mimicking cortical architecture and biologically-constrained\nlearning. They offer sparse architectures with local learning rules and\nunsupervised/semi-supervised learning, making them well-suited for low-power\nedge intelligence. However, existing BCPNN implementations rely on GPUs or\ndatacenter FPGAs, limiting their applicability to embedded systems. This work\npresents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+\nSoC using High-Level Synthesis. We implement both online learning and\ninference-only kernels with support for variable and mixed precision. Evaluated\non MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to\n17.5x latency and 94% energy savings over ARM baselines, without sacrificing\naccuracy. This work enables practical neuromorphic computing on edge devices,\nbridging the gap between brain-like learning and real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u5f0fFPGA\u7684\u8111\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\uff0c\u9996\u6b21\u5728Zynq UltraScale+ SoC\u4e0a\u5b9e\u73b0\u4e86BCPNN\u7684\u5728\u7ebf\u5b66\u4e60\u548c\u63a8\u7406\u529f\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u8fb9\u7f18AI\u5e94\u7528\u9700\u8981\u4f4e\u529f\u8017\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u8017\u9ad8\u4e14\u4f9d\u8d56\u4e91\u7aef\u3002\u8111\u795e\u7ecf\u7f51\u7edc\uff08\u5982BCPNN\uff09\u5177\u6709\u7a00\u758f\u67b6\u6784\u548c\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u4f9d\u8d56GPU\u6216\u6570\u636e\u4e2d\u5fc3FPGA\uff0c\u9650\u5236\u4e86\u5d4c\u5165\u5f0f\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u9ad8\u5c42\u6b21\u7efc\u5408\uff08HLS\uff09\u5728Zynq UltraScale+ SoC\u4e0a\u5b9e\u73b0\u5d4c\u5165\u5f0fFPGA\u52a0\u901f\u5668\uff0c\u652f\u6301\u53ef\u53d8\u548c\u6df7\u5408\u7cbe\u5ea6\u7684\u5728\u7ebf\u5b66\u4e60\u548c\u63a8\u7406\u529f\u80fd\u3002", "result": "\u5728MNIST\u3001\u80ba\u708e\u548c\u4e73\u817a\u764c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u52a0\u901f\u5668\u6bd4ARM\u57fa\u7ebf\u964d\u4f4e\u4e8617.5\u500d\u7684\u5ef6\u8fdf\u548c94%\u7684\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\uff0c\u586b\u8865\u4e86\u8111\u7c7b\u4f3c\u5b66\u4e60\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "paper_title_zh": "\u5d4c\u5165\u5f0fFPGA\u52a0\u901f\u8111\u7c7b\u4f3c\u795e\u7ecf\u7f51\u7edc\uff1a\u4ece\u5728\u7ebf\u5b66\u4e60\u5230\u53ef\u6269\u5c55\u63a8\u7406", "abstract_zh": "\u8fb9\u7f18AI\u5e94\u7528\u8d8a\u6765\u8d8a\u9700\u8981\u80fd\u591f\u5728\u8bbe\u5907\u4e0a\u4ee5\u6700\u4f4e\u80fd\u8017\u5b66\u4e60\u548c\u9002\u5e94\u7684\u6a21\u578b\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u901a\u5e38\u53c2\u6570\u8fc7\u591a\u3001\u80fd\u8017\u9ad8\u4e14\u4f9d\u8d56\u4e91\u7aef\u8fde\u63a5\u3002\u8111\u7c7b\u4f3c\u795e\u7ecf\u7f51\u7edc\uff08\u5982\u8d1d\u53f6\u65af\u7f6e\u4fe1\u4f20\u64ad\u795e\u7ecf\u7f51\u7edc\uff0cBCPNN\uff09\u901a\u8fc7\u6a21\u4eff\u76ae\u5c42\u67b6\u6784\u548c\u751f\u7269\u7ea6\u675f\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u66ff\u4ee3\u65b9\u6848\u3002\u5b83\u4eec\u5177\u6709\u7a00\u758f\u67b6\u6784\u548c\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff0c\u652f\u6301\u65e0\u76d1\u7763/\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u975e\u5e38\u9002\u5408\u4f4e\u529f\u8017\u8fb9\u7f18\u667a\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684BCPNN\u5b9e\u73b0\u4f9d\u8d56\u4e8eGPU\u6216\u6570\u636e\u4e2d\u5fc3FPGA\uff0c\u9650\u5236\u4e86\u5176\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u9996\u6b21\u5728Zynq UltraScale+ SoC\u4e0a\u4f7f\u7528\u9ad8\u5c42\u6b21\u7efc\u5408\u5b9e\u73b0\u4e86\u5d4c\u5165\u5f0fFPGA\u52a0\u901f\u5668\uff0c\u652f\u6301\u53ef\u53d8\u548c\u6df7\u5408\u7cbe\u5ea6\u7684\u5728\u7ebf\u5b66\u4e60\u548c\u63a8\u7406\u529f\u80fd\u3002\u5728MNIST\u3001\u80ba\u708e\u548c\u4e73\u817a\u764c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u52a0\u901f\u5668\u6bd4ARM\u57fa\u7ebf\u964d\u4f4e\u4e8617.5\u500d\u7684\u5ef6\u8fdf\u548c94%\u7684\u80fd\u8017\uff0c\u540c\u65f6\u672a\u727a\u7272\u51c6\u786e\u6027\u3002\u8fd9\u9879\u7814\u7a76\u5b9e\u73b0\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\uff0c\u586b\u8865\u4e86\u8111\u7c7b\u4f3c\u5b66\u4e60\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.18543", "pdf": "https://arxiv.org/pdf/2506.18543", "abs": "https://arxiv.org/abs/2506.18543", "authors": ["Xiaodong Wu", "Xiangman Li", "Jianbing Ni"], "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86DeepSeek\u7cfb\u5217\u6a21\u578b\u4e0eGPT-3.5\u548cGPT-4\u5728\u8d8a\u72f1\u653b\u51fb\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0DeepSeek\u7684MoE\u67b6\u6784\u5bf9\u4f18\u5316\u653b\u51fb\u6709\u9009\u62e9\u6027\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u63d0\u793a\u653b\u51fb\u66f4\u8106\u5f31\uff0c\u800cGPT-4 Turbo\u8868\u73b0\u66f4\u4e00\u81f4\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u5f15\u53d1\u5173\u6ce8\u3002\u5c3d\u7ba1GPT-4\u7b49\u4e13\u6709\u6a21\u578b\u5df2\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u5982DeepSeek\u7684\u5b89\u5168\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528HarmBench\u57fa\u51c6\u8bc4\u4f30DeepSeek\u7cfb\u5217\u6a21\u578b\u4e0eGPT-3.5\u548cGPT-4\uff0c\u6db5\u76d67\u79cd\u4ee3\u8868\u6027\u653b\u51fb\u7b56\u7565\u548c510\u79cd\u6709\u5bb3\u884c\u4e3a\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u653b\u51fb\u4e0b\u7684\u8868\u73b0\u3002", "result": "DeepSeek\u7684MoE\u67b6\u6784\u5bf9\u4f18\u5316\u653b\u51fb\uff08\u5982TAP-T\uff09\u6709\u9009\u62e9\u6027\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u63d0\u793a\u653b\u51fb\u548c\u4eba\u5de5\u8bbe\u8ba1\u653b\u51fb\u66f4\u8106\u5f31\uff1bGPT-4 Turbo\u8868\u73b0\u66f4\u4e00\u81f4\uff0c\u5b89\u5168\u6027\u66f4\u5f3a\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u67b6\u6784\u6548\u7387\u4e0e\u5bf9\u9f50\u6cdb\u5316\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5f00\u6e90LLMs\u9700\u9488\u5bf9\u6027\u5b89\u5168\u8c03\u4f18\u548c\u6a21\u5757\u5316\u5bf9\u9f50\u7b56\u7565\u4ee5\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u3002", "paper_title_zh": "DeepSeek\u4e0eGPT\u7cfb\u5217\u6a21\u578b\u5728\u8d8a\u72f1\u653b\u51fb\u4e0b\u7684\u5b89\u5168\u6027\u8bc4\u4f30", "abstract_zh": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u90e8\u7f72\u5f15\u53d1\u4e86\u5bf9\u5176\u8d8a\u72f1\u653b\u51fb\u8106\u5f31\u6027\u7684\u91cd\u8981\u5173\u6ce8\uff0c\u5373\u901a\u8fc7\u5bf9\u6297\u6027\u63d0\u793a\u7ed5\u8fc7\u5bf9\u9f50\u673a\u5236\uff0c\u5f15\u53d1\u6709\u5bb3\u6216\u8fdd\u53cd\u653f\u7b56\u7684\u8f93\u51fa\u3002\u5c3d\u7ba1\u50cfGPT-4\u8fd9\u6837\u7684\u4e13\u6709\u6a21\u578b\u5df2\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f46\u65b0\u5174\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\uff08\u5982DeepSeek\uff09\u7684\u9c81\u68d2\u6027\u4ecd\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5c3d\u7ba1\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u65e5\u76ca\u589e\u957f\u3002\u672c\u6587\u9996\u6b21\u5bf9DeepSeek\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u8d8a\u72f1\u8bc4\u4f30\uff0c\u5e76\u4e0eGPT-3.5\u548cGPT-4\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4f7f\u7528\u4e86HarmBench\u57fa\u51c6\u3002\u6211\u4eec\u8bc4\u4f30\u4e867\u79cd\u4ee3\u8868\u6027\u653b\u51fb\u7b56\u7565\uff0c\u6db5\u76d6\u6309\u529f\u80fd\u548c\u8bed\u4e49\u9886\u57df\u5206\u7c7b\u7684510\u79cd\u6709\u5bb3\u884c\u4e3a\u3002\u5206\u6790\u8868\u660e\uff0cDeepSeek\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u901a\u8fc7\u8def\u7531\u7a00\u758f\u6027\u5bf9\u57fa\u4e8e\u4f18\u5316\u7684\u653b\u51fb\uff08\u5982TAP-T\uff09\u63d0\u4f9b\u4e86\u9009\u62e9\u6027\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u63d0\u793a\u653b\u51fb\u548c\u4eba\u5de5\u8bbe\u8ba1\u653b\u51fb\u4e0b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8106\u5f31\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cGPT-4 Turbo\u7531\u4e8e\u5176\u5bc6\u96c6\u7684Transformer\u8bbe\u8ba1\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4e0d\u540c\u884c\u4e3a\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u4e14\u66f4\u4e00\u81f4\u7684\u5b89\u5168\u5bf9\u9f50\u3002\u7ec6\u7c92\u5ea6\u884c\u4e3a\u5206\u6790\u548c\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u663e\u793a\uff0cDeepSeek\u5e38\u5c06\u5bf9\u6297\u6027\u63d0\u793a\u8def\u7531\u81f3\u672a\u5145\u5206\u5bf9\u9f50\u7684\u4e13\u5bb6\u6a21\u5757\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u7684\u62d2\u7edd\u884c\u4e3a\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u67b6\u6784\u6548\u7387\u4e0e\u5bf9\u9f50\u6cdb\u5316\u4e4b\u95f4\u7684\u6839\u672c\u6743\u8861\uff0c\u5f3a\u8c03\u4e86\u9488\u5bf9\u6027\u5b89\u5168\u8c03\u4f18\u548c\u6a21\u5757\u5316\u5bf9\u9f50\u7b56\u7565\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u5f00\u6e90LLMs\u7684\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2506.18588", "pdf": "https://arxiv.org/pdf/2506.18588", "abs": "https://arxiv.org/abs/2506.18588", "authors": ["R\u00f3is\u00edn Luo", "James McDermott", "Christian Gagn\u00e9", "Qiang Sun", "Colm O'Riordan"], "title": "Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Lipschitz continuity characterizes the worst-case sensitivity of neural\nnetworks to small input perturbations; yet its dynamics (i.e. temporal\nevolution) during training remains under-explored. We present a rigorous\nmathematical framework to model the temporal evolution of Lipschitz continuity\nduring training with stochastic gradient descent (SGD). This framework\nleverages a system of stochastic differential equations (SDEs) to capture both\ndeterministic and stochastic forces. Our theoretical analysis identifies three\nprincipal factors driving the evolution: (i) the projection of gradient flows,\ninduced by the optimization dynamics, onto the operator-norm Jacobian of\nparameter matrices; (ii) the projection of gradient noise, arising from the\nrandomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)\nthe projection of the gradient noise onto the operator-norm Hessian of\nparameter matrices. Furthermore, our theoretical framework sheds light on such\nas how noisy supervision, parameter initialization, batch size, and mini-batch\nsampling trajectories, among other factors, shape the evolution of the\nLipschitz continuity of neural networks. Our experimental results demonstrate\nstrong agreement between the theoretical implications and the observed\nbehaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u8bad\u7ec3\u8fc7\u7a0b\u4e2dLipschitz\u8fde\u7eed\u6027\u7684\u52a8\u6001\u6f14\u5316\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u6d41\u3001\u68af\u5ea6\u566a\u58f0\u53caHessian\u77e9\u9635\u6295\u5f71\u5bf9\u5176\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\u3002", "motivation": "Lipschitz\u8fde\u7eed\u6027\u53cd\u6620\u4e86\u795e\u7ecf\u7f51\u7edc\u5bf9\u5c0f\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u4f46\u5176\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u6f14\u5316\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63ed\u793a\u4f18\u5316\u8fc7\u7a0b\u4e2dLipschitz\u8fde\u7eed\u6027\u7684\u53d8\u5316\u673a\u5236\u3002", "method": "\u4f5c\u8005\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDEs\uff09\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u529b\u91cf\u3002\u8be5\u6846\u67b6\u5206\u6790\u4e86\u68af\u5ea6\u6d41\u3001\u68af\u5ea6\u566a\u58f0\u53ca\u5176\u5728\u7b97\u5b50\u8303\u6570Jacobian\u548cHessian\u77e9\u9635\u4e0a\u7684\u6295\u5f71\u5bf9Lipschitz\u8fde\u7eed\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u68af\u5ea6\u6d41\u3001\u68af\u5ea6\u566a\u58f0\u53caHessian\u77e9\u9635\u6295\u5f71\u662f\u9a71\u52a8Lipschitz\u8fde\u7eed\u6027\u6f14\u5316\u7684\u4e09\u5927\u56e0\u7d20\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9645\u89c2\u6d4b\u884c\u4e3a\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u6210\u529f\u89e3\u91ca\u4e86Lipschitz\u8fde\u7eed\u6027\u5728\u8bad\u7ec3\u4e2d\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u566a\u58f0\u76d1\u7763\u3001\u53c2\u6570\u521d\u59cb\u5316\u3001\u6279\u91cf\u5927\u5c0f\u7b49\u56e0\u7d20\u5bf9\u5176\u6f14\u5316\u7684\u5f71\u54cd\u3002", "paper_title_zh": "\u795e\u7ecf\u7f51\u7edc\u4e2dLipschitz\u8fde\u7eed\u6027\u7684\u4f18\u5316\u8bf1\u5bfc\u52a8\u529b\u5b66", "abstract_zh": "Lipschitz\u8fde\u7eed\u6027\u8868\u5f81\u4e86\u795e\u7ecf\u7f51\u7edc\u5bf9\u5c0f\u8f93\u5165\u6270\u52a8\u7684\u6700\u574f\u654f\u611f\u6027\uff0c\u4f46\u5176\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u6f14\u5316\uff08\u5373\u65f6\u95f4\u6f14\u5316\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u8bad\u7ec3\u8fc7\u7a0b\u4e2dLipschitz\u8fde\u7eed\u6027\u7684\u65f6\u95f4\u6f14\u5316\u3002\u8be5\u6846\u67b6\u5229\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDEs\uff09\u7cfb\u7edf\u6765\u6355\u6349\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u529b\u91cf\u3002\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u786e\u5b9a\u4e86\u9a71\u52a8\u6f14\u5316\u7684\u4e09\u4e2a\u4e3b\u8981\u56e0\u7d20\uff1a\uff08i\uff09\u7531\u4f18\u5316\u52a8\u529b\u5b66\u5f15\u8d77\u7684\u68af\u5ea6\u6d41\u5728\u53c2\u6570\u77e9\u9635\u7684\u7b97\u5b50\u8303\u6570Jacobian\u4e0a\u7684\u6295\u5f71\uff1b\uff08ii\uff09\u7531\u5c0f\u6279\u91cf\u91c7\u6837\u7684\u968f\u673a\u6027\u5f15\u8d77\u7684\u68af\u5ea6\u566a\u58f0\u5728\u7b97\u5b50\u8303\u6570Jacobian\u4e0a\u7684\u6295\u5f71\uff1b\u4ee5\u53ca\uff08iii\uff09\u68af\u5ea6\u566a\u58f0\u5728\u53c2\u6570\u77e9\u9635\u7684\u7b97\u5b50\u8303\u6570Hessian\u4e0a\u7684\u6295\u5f71\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7406\u8bba\u6846\u67b6\u63ed\u793a\u4e86\u566a\u58f0\u76d1\u7763\u3001\u53c2\u6570\u521d\u59cb\u5316\u3001\u6279\u91cf\u5927\u5c0f\u548c\u5c0f\u6279\u91cf\u91c7\u6837\u8f68\u8ff9\u7b49\u56e0\u7d20\u5982\u4f55\u5851\u9020\u795e\u7ecf\u7f51\u7edcLipschitz\u8fde\u7eed\u6027\u7684\u6f14\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7406\u8bba\u9884\u6d4b\u4e0e\u89c2\u6d4b\u884c\u4e3a\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2506.18604", "pdf": "https://arxiv.org/pdf/2506.18604", "abs": "https://arxiv.org/abs/2506.18604", "authors": ["Mengjian Hua", "Eric Vanden-Eijnden", "Ricky T. Q. Chen"], "title": "Simulation-Free Differential Dynamics through Neural Conservation Laws", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present a novel simulation-free framework for training continuous-time\ndiffusion processes over very general objective functions. Existing methods\ntypically involve either prescribing the optimal diffusion process -- which\nonly works for heavily restricted problem formulations -- or require expensive\nsimulation to numerically obtain the time-dependent densities and sample from\nthe diffusion process. In contrast, we propose a coupled parameterization which\njointly models a time-dependent density function, or probability path, and the\ndynamics of a diffusion process that generates this probability path. To\naccomplish this, our approach directly bakes in the Fokker-Planck equation and\ndensity function requirements as hard constraints, by extending and greatly\nsimplifying the construction of Neural Conservation Laws. This enables\nsimulation-free training for a large variety of problem formulations, from\ndata-driven objectives as in generative modeling and dynamical optimal\ntransport, to optimality-based objectives as in stochastic optimal control,\nwith straightforward extensions to mean-field objectives due to the ease of\naccessing exact density functions. We validate our method in a diverse range of\napplication domains from modeling spatio-temporal events to learning optimal\ndynamics from population data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6a21\u62df\u7684\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u5b88\u6052\u5b9a\u5f8b\u76f4\u63a5\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5bc6\u5ea6\u51fd\u6570\u548c\u6269\u6563\u8fc7\u7a0b\u52a8\u529b\u5b66\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u95ee\u9898\uff0c\u5982\u751f\u6210\u5efa\u6a21\u548c\u968f\u673a\u6700\u4f18\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u89c4\u5b9a\u6700\u4f18\u6269\u6563\u8fc7\u7a0b\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u6a21\u62df\uff0c\u9650\u5236\u4e86\u95ee\u9898\u7684\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u6a21\u62df\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u95ee\u9898\u5f62\u5f0f\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u548c\u7b80\u5316\u795e\u7ecf\u5b88\u6052\u5b9a\u5f8b\u7684\u6784\u5efa\uff0c\u5c06Fokker-Planck\u65b9\u7a0b\u548c\u5bc6\u5ea6\u51fd\u6570\u8981\u6c42\u4f5c\u4e3a\u786c\u7ea6\u675f\uff0c\u8054\u5408\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5bc6\u5ea6\u51fd\u6570\u548c\u6269\u6563\u8fc7\u7a0b\u52a8\u529b\u5b66\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u5efa\u6a21\u3001\u65f6\u7a7a\u4e8b\u4ef6\u5efa\u6a21\u548c\u4ece\u4eba\u53e3\u6570\u636e\u5b66\u4e60\u6700\u4f18\u52a8\u529b\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u591a\u79cd\u95ee\u9898\u63d0\u4f9b\u4e86\u65e0\u6a21\u62df\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u795e\u7ecf\u5b88\u6052\u5b9a\u5f8b\u7684\u5e94\u7528\u8303\u56f4\u3002", "paper_title_zh": "\u901a\u8fc7\u795e\u7ecf\u5b88\u6052\u5b9a\u5f8b\u5b9e\u73b0\u65e0\u6a21\u62df\u7684\u5fae\u5206\u52a8\u529b\u5b66", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u975e\u5e38\u901a\u7528\u7684\u76ee\u6807\u51fd\u6570\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u89c4\u5b9a\u6700\u4f18\u6269\u6563\u8fc7\u7a0b\uff08\u4ec5\u9002\u7528\u4e8e\u9ad8\u5ea6\u53d7\u9650\u7684\u95ee\u9898\u5f62\u5f0f\uff09\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u6a21\u62df\u6765\u6570\u503c\u83b7\u53d6\u65f6\u95f4\u4f9d\u8d56\u5bc6\u5ea6\u5e76\u4ece\u6269\u6563\u8fc7\u7a0b\u4e2d\u91c7\u6837\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8026\u5408\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u8054\u5408\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5bc6\u5ea6\u51fd\u6570\uff08\u6216\u6982\u7387\u8def\u5f84\uff09\u548c\u751f\u6210\u8be5\u6982\u7387\u8def\u5f84\u7684\u6269\u6563\u8fc7\u7a0b\u52a8\u529b\u5b66\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u5e76\u5927\u5e45\u7b80\u5316\u795e\u7ecf\u5b88\u6052\u5b9a\u5f8b\u7684\u6784\u5efa\uff0c\u76f4\u63a5\u5c06Fokker-Planck\u65b9\u7a0b\u548c\u5bc6\u5ea6\u51fd\u6570\u8981\u6c42\u4f5c\u4e3a\u786c\u7ea6\u675f\u3002\u8fd9\u4f7f\u5f97\u65e0\u6a21\u62df\u8bad\u7ec3\u9002\u7528\u4e8e\u591a\u79cd\u95ee\u9898\u5f62\u5f0f\uff0c\u4ece\u751f\u6210\u5efa\u6a21\u548c\u52a8\u6001\u6700\u4f18\u4f20\u8f93\u7b49\u6570\u636e\u9a71\u52a8\u76ee\u6807\uff0c\u5230\u968f\u673a\u6700\u4f18\u63a7\u5236\u7b49\u57fa\u4e8e\u6700\u4f18\u6027\u7684\u76ee\u6807\uff0c\u5e76\u53ef\u8f7b\u677e\u6269\u5c55\u5230\u5747\u503c\u573a\u76ee\u6807\uff0c\u56e0\u4e3a\u53ef\u4ee5\u8f7b\u677e\u8bbf\u95ee\u7cbe\u786e\u5bc6\u5ea6\u51fd\u6570\u3002\u6211\u4eec\u5728\u4ece\u65f6\u7a7a\u4e8b\u4ef6\u5efa\u6a21\u5230\u4ece\u4eba\u53e3\u6570\u636e\u5b66\u4e60\u6700\u4f18\u52a8\u529b\u5b66\u7b49\u591a\u4e2a\u5e94\u7528\u9886\u57df\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.18611", "pdf": "https://arxiv.org/pdf/2506.18611", "abs": "https://arxiv.org/abs/2506.18611", "authors": ["Waleed Breesam", "Rezvan Alamian", "Nima Tashakor", "Brahim Elkhalil Youcefa", "Stefan M. Goetz"], "title": "Frequency Control in Microgrids: An Adaptive Fuzzy-Neural-Network Virtual Synchronous Generator", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "11 pages, 17 figures", "summary": "The reliance on distributed renewable energy has increased recently. As a\nresult, power electronic-based distributed generators replaced synchronous\ngenerators which led to a change in the dynamic characteristics of the\nmicrogrid. Most critically, they reduced system inertia and damping. Virtual\nsynchronous generators emulated in power electronics, which mimic the dynamic\nbehaviour of synchronous generators, are meant to fix this problem. However,\nfixed virtual synchronous generator parameters cannot guarantee a frequency\nregulation within the acceptable tolerance range. Conversely, a dynamic\nadjustment of these virtual parameters promises robust solution with stable\nfrequency. This paper proposes a method to adapt the inertia, damping, and\ndroop parameters dynamically through a fuzzy neural network controller. This\ncontroller trains itself online to choose appropriate values for these virtual\nparameters. The proposed method can be applied to a typical AC microgrid by\nconsidering the penetration and impact of renewable energy sources. We study\nthe system in a MATLAB/Simulink model and validate it experimentally in real\ntime using hardware-in-the-loop based on an embedded ARM system (SAM3X8E,\nCortex-M3). Compared to traditional and fuzzy logic controller methods, the\nresults demonstrate that the proposed method significantly reduces the\nfrequency deviation to less than 0.03 Hz and shortens the stabilizing/recovery\ntime.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u7684\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u5fae\u7535\u7f51\u4e2d\u7684\u60ef\u6027\u3001\u963b\u5c3c\u548c\u4e0b\u5782\u53c2\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9891\u7387\u504f\u5dee\u81f30.03 Hz\u4ee5\u4e0b\uff0c\u5e76\u7f29\u77ed\u4e86\u7a33\u5b9a\u65f6\u95f4\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u53ef\u518d\u751f\u80fd\u6e90\u7684\u666e\u53ca\uff0c\u7535\u529b\u7535\u5b50\u8bbe\u5907\u53d6\u4ee3\u4e86\u540c\u6b65\u53d1\u7535\u673a\uff0c\u5bfc\u81f4\u5fae\u7535\u7f51\u52a8\u6001\u7279\u6027\u53d8\u5316\uff0c\u7cfb\u7edf\u60ef\u6027\u548c\u963b\u5c3c\u964d\u4f4e\u3002\u56fa\u5b9a\u53c2\u6570\u7684\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u65e0\u6cd5\u4fdd\u8bc1\u9891\u7387\u8c03\u8282\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u8c03\u6574\u53c2\u6570\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u9891\u7387\u3002", "method": "\u901a\u8fc7\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u7684\u60ef\u6027\u3001\u963b\u5c3c\u548c\u4e0b\u5782\u53c2\u6570\u3002\u8be5\u63a7\u5236\u5668\u5728\u7ebf\u8bad\u7ec3\uff0c\u9009\u62e9\u9002\u5f53\u7684\u865a\u62df\u53c2\u6570\u503c\uff0c\u5e76\u5e94\u7528\u4e8e\u5178\u578b\u4ea4\u6d41\u5fae\u7535\u7f51\u4e2d\uff0c\u8003\u8651\u53ef\u518d\u751f\u80fd\u6e90\u7684\u6e17\u900f\u548c\u5f71\u54cd\u3002", "result": "\u5728MATLAB/Simulink\u6a21\u578b\u4e2d\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5d4c\u5165\u5f0fARM\u7cfb\u7edf\u7684\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u5b9e\u65f6\u9a8c\u8bc1\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u548c\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5c06\u9891\u7387\u504f\u5dee\u663e\u8457\u964d\u4f4e\u81f30.03 Hz\u4ee5\u4e0b\uff0c\u5e76\u7f29\u77ed\u4e86\u7a33\u5b9a\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5fae\u7535\u7f51\u9891\u7387\u63a7\u5236\u95ee\u9898\uff0c\u52a8\u6001\u8c03\u6574\u53c2\u6570\u663e\u8457\u63d0\u5347\u4e86\u9891\u7387\u7a33\u5b9a\u6027\u548c\u6062\u590d\u901f\u5ea6\u3002", "paper_title_zh": "\u5fae\u7535\u7f51\u9891\u7387\u63a7\u5236\uff1a\u4e00\u79cd\u81ea\u9002\u5e94\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u65b9\u6cd5", "abstract_zh": "\u8fd1\u5e74\u6765\uff0c\u5206\u5e03\u5f0f\u53ef\u518d\u751f\u80fd\u6e90\u7684\u4f9d\u8d56\u5ea6\u589e\u52a0\uff0c\u7535\u529b\u7535\u5b50\u8bbe\u5907\u53d6\u4ee3\u4e86\u540c\u6b65\u53d1\u7535\u673a\uff0c\u5bfc\u81f4\u5fae\u7535\u7f51\u52a8\u6001\u7279\u6027\u53d8\u5316\uff0c\u7cfb\u7edf\u60ef\u6027\u548c\u963b\u5c3c\u964d\u4f4e\u3002\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u901a\u8fc7\u6a21\u62df\u540c\u6b65\u53d1\u7535\u673a\u7684\u52a8\u6001\u884c\u4e3a\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u56fa\u5b9a\u53c2\u6570\u65e0\u6cd5\u4fdd\u8bc1\u9891\u7387\u8c03\u8282\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002\u52a8\u6001\u8c03\u6574\u8fd9\u4e9b\u865a\u62df\u53c2\u6570\u6709\u671b\u63d0\u4f9b\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u60ef\u6027\u3001\u963b\u5c3c\u548c\u4e0b\u5782\u53c2\u6570\u7684\u65b9\u6cd5\u3002\u8be5\u63a7\u5236\u5668\u5728\u7ebf\u8bad\u7ec3\u4ee5\u9009\u62e9\u9002\u5f53\u7684\u865a\u62df\u53c2\u6570\u503c\uff0c\u5e76\u5e94\u7528\u4e8e\u5178\u578b\u4ea4\u6d41\u5fae\u7535\u7f51\u4e2d\uff0c\u8003\u8651\u53ef\u518d\u751f\u80fd\u6e90\u7684\u6e17\u900f\u548c\u5f71\u54cd\u3002\u6211\u4eec\u5728MATLAB/Simulink\u6a21\u578b\u4e2d\u7814\u7a76\u4e86\u8be5\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5d4c\u5165\u5f0fARM\u7cfb\u7edf\uff08SAM3X8E\uff0cCortex-M3\uff09\u7684\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u5b9e\u65f6\u9a8c\u8bc1\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u548c\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5c06\u9891\u7387\u504f\u5dee\u663e\u8457\u964d\u4f4e\u81f30.03 Hz\u4ee5\u4e0b\uff0c\u5e76\u7f29\u77ed\u4e86\u7a33\u5b9a/\u6062\u590d\u65f6\u95f4\u3002"}}
{"id": "2506.18627", "pdf": "https://arxiv.org/pdf/2506.18627", "abs": "https://arxiv.org/abs/2506.18627", "authors": ["Yannik Mahlau", "Maximilian Schier", "Christoph Reinders", "Frederik Schubert", "Marco B\u00fcgling", "Bodo Rosenhahn"], "title": "Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inverse design of photonic integrated circuits (PICs) has traditionally\nrelied on gradientbased optimization. However, this approach is prone to end up\nin local minima, which results in suboptimal design functionality. As interest\nin PICs increases due to their potential for addressing modern hardware demands\nthrough optical computing, more adaptive optimization algorithms are needed. We\npresent a reinforcement learning (RL) environment as well as multi-agent RL\nalgorithms for the design of PICs. By discretizing the design space into a\ngrid, we formulate the design task as an optimization problem with thousands of\nbinary variables. We consider multiple two- and three-dimensional design tasks\nthat represent PIC components for an optical computing system. By decomposing\nthe design space into thousands of individual agents, our algorithms are able\nto optimize designs with only a few thousand environment samples. They\noutperform previous state-of-the-art gradient-based optimization in both twoand\nthree-dimensional design tasks. Our work may also serve as a benchmark for\nfurther exploration of sample-efficient RL for inverse design in photonics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5149\u5b50\u96c6\u6210\u7535\u8def\uff08PIC\uff09\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8bbe\u8ba1\u7a7a\u95f4\u5e76\u5206\u89e3\u4e3a\u6570\u5343\u4e2a\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\uff0c\u5e76\u5728\u4e8c\u7ef4\u548c\u4e09\u7ef4\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u5149\u5b50\u96c6\u6210\u7535\u8def\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u529f\u80fd\u4e0d\u7406\u60f3\u3002\u968f\u7740\u5149\u5b66\u8ba1\u7b97\u5bf9PIC\u9700\u6c42\u7684\u589e\u52a0\uff0c\u4e9f\u9700\u66f4\u81ea\u9002\u5e94\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "method": "\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u7f51\u683c\uff0c\u5c06\u8bbe\u8ba1\u4efb\u52a1\u5efa\u6a21\u4e3a\u5305\u542b\u6570\u5343\u4e2a\u4e8c\u5143\u53d8\u91cf\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53RL\u7b97\u6cd5\u5206\u89e3\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u8d1f\u8d23\u5c40\u90e8\u4f18\u5316\uff0c\u4ec5\u9700\u6570\u5343\u6b21\u73af\u5883\u91c7\u6837\u5373\u53ef\u5b8c\u6210\u8bbe\u8ba1\u3002", "result": "\u5728\u4e8c\u7ef4\u548c\u4e09\u7ef4PIC\u7ec4\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u4e2d\uff0c\u591a\u667a\u80fd\u4f53RL\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u4e14\u6837\u672c\u6548\u7387\u9ad8\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53RL\u4e3a\u5149\u5b50\u96c6\u6210\u7535\u8def\u9006\u5411\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u5149\u5b50\u5b66\u4e2d\u6837\u672c\u9ad8\u6548RL\u7684\u63a2\u7d22\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "paper_title_zh": "\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5149\u5b50\u96c6\u6210\u7535\u8def\u9006\u5411\u8bbe\u8ba1", "abstract_zh": "\u5149\u5b50\u96c6\u6210\u7535\u8def\uff08PIC\uff09\u7684\u9006\u5411\u8bbe\u8ba1\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u529f\u80fd\u4e0d\u7406\u60f3\u3002\u968f\u7740\u5149\u5b66\u8ba1\u7b97\u5bf9PIC\u9700\u6c42\u7684\u589e\u52a0\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u7684\u4f18\u5316\u7b97\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u73af\u5883\u53ca\u591a\u667a\u80fd\u4f53RL\u7b97\u6cd5\u7528\u4e8ePIC\u8bbe\u8ba1\u3002\u901a\u8fc7\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u7f51\u683c\uff0c\u6211\u4eec\u5c06\u8bbe\u8ba1\u4efb\u52a1\u5efa\u6a21\u4e3a\u5305\u542b\u6570\u5343\u4e2a\u4e8c\u5143\u53d8\u91cf\u7684\u4f18\u5316\u95ee\u9898\u3002\u6211\u4eec\u7814\u7a76\u4e86\u591a\u4e2a\u4ee3\u8868\u5149\u5b66\u8ba1\u7b97\u7cfb\u7edfPIC\u7ec4\u4ef6\u7684\u4e8c\u7ef4\u548c\u4e09\u7ef4\u8bbe\u8ba1\u4efb\u52a1\u3002\u901a\u8fc7\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u5206\u89e3\u4e3a\u6570\u5343\u4e2a\u72ec\u7acb\u667a\u80fd\u4f53\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u4ec5\u9700\u6570\u5343\u6b21\u73af\u5883\u91c7\u6837\u5373\u53ef\u4f18\u5316\u8bbe\u8ba1\uff0c\u5e76\u5728\u4e8c\u7ef4\u548c\u4e09\u7ef4\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u8fd8\u53ef\u4f5c\u4e3a\u672a\u6765\u5149\u5b50\u5b66\u4e2d\u6837\u672c\u9ad8\u6548RL\u9006\u5411\u8bbe\u8ba1\u7684\u57fa\u51c6\u3002"}}
{"id": "2506.18637", "pdf": "https://arxiv.org/pdf/2506.18637", "abs": "https://arxiv.org/abs/2506.18637", "authors": ["Shuyin Xia", "Yifan Wang", "Lifeng Shen", "Guoyin Wang"], "title": "Granular-Ball-Induced Multiple Kernel K-Means", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Most existing multi-kernel clustering algorithms, such as multi-kernel\nK-means, often struggle with computational efficiency and robustness when faced\nwith complex data distributions. These challenges stem from their dependence on\npoint-to-point relationships for optimization, which can lead to difficulty in\naccurately capturing data sets' inherent structure and diversity. Additionally,\nthe intricate interplay between multiple kernels in such algorithms can further\nexacerbate these issues, effectively impacting their ability to cluster data\npoints in high-dimensional spaces. In this paper, we leverage granular-ball\ncomputing to improve the multi-kernel clustering framework. The core of\ngranular-ball computing is to adaptively fit data distribution by balls from\ncoarse to acceptable levels. Each ball can enclose data points based on a\ndensity consistency measurement. Such ball-based data description thus improves\nthe computational efficiency and the robustness to unknown noises.\nSpecifically, based on granular-ball representations, we introduce the\ngranular-ball kernel (GBK) and its corresponding granular-ball multi-kernel\nK-means framework (GB-MKKM) for efficient clustering. Using granular-ball\nrelationships in multiple kernel spaces, the proposed GB-MKKM framework shows\nits superiority in efficiency and clustering performance in the empirical\nevaluation of various clustering tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u7403\u8ba1\u7b97\u7684\u591a\u6838K\u5747\u503c\u805a\u7c7b\u6846\u67b6\uff08GB-MKKM\uff09\uff0c\u901a\u8fc7\u7c92\u7403\u6838\uff08GBK\uff09\u548c\u7c92\u7403\u5173\u7cfb\u63d0\u5347\u805a\u7c7b\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6838\u805a\u7c7b\u7b97\u6cd5\uff08\u5982\u591a\u6838K\u5747\u503c\uff09\u5728\u590d\u6742\u6570\u636e\u5206\u5e03\u4e0b\u5e38\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u4f9d\u8d56\u70b9\u5bf9\u70b9\u5173\u7cfb\u4f18\u5316\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u6570\u636e\u5185\u5728\u7ed3\u6784\u548c\u591a\u6837\u6027\u3002", "method": "\u5229\u7528\u7c92\u7403\u8ba1\u7b97\u81ea\u9002\u5e94\u62df\u5408\u6570\u636e\u5206\u5e03\uff0c\u63d0\u51fa\u7c92\u7403\u6838\uff08GBK\uff09\u548c\u7c92\u7403\u591a\u6838K\u5747\u503c\u6846\u67b6\uff08GB-MKKM\uff09\uff0c\u901a\u8fc7\u7c92\u7403\u5173\u7cfb\u5728\u591a\u6838\u7a7a\u95f4\u4e2d\u63d0\u5347\u805a\u7c7b\u6548\u7387\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGB-MKKM\u6846\u67b6\u5728\u591a\u79cd\u805a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u805a\u7c7b\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u7c92\u7403\u8ba1\u7b97\u7684\u591a\u6838\u805a\u7c7b\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6570\u636e\u5206\u5e03\u4e0b\u7684\u805a\u7c7b\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u6570\u636e\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u57fa\u4e8e\u7c92\u7403\u7684\u591a\u6838K\u5747\u503c\u805a\u7c7b", "abstract_zh": "\u73b0\u6709\u7684\u591a\u6838\u805a\u7c7b\u7b97\u6cd5\uff08\u5982\u591a\u6838K\u5747\u503c\uff09\u5728\u9762\u5bf9\u590d\u6742\u6570\u636e\u5206\u5e03\u65f6\uff0c\u5e38\u56e0\u4f9d\u8d56\u70b9\u5bf9\u70b9\u5173\u7cfb\u4f18\u5316\u800c\u96be\u4ee5\u9ad8\u6548\u4e14\u9c81\u68d2\u5730\u6355\u6349\u6570\u636e\u5185\u5728\u7ed3\u6784\u548c\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u591a\u6838\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u6570\u636e\u805a\u7c7b\u80fd\u529b\u3002\u672c\u6587\u5229\u7528\u7c92\u7403\u8ba1\u7b97\u6539\u8fdb\u591a\u6838\u805a\u7c7b\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u7c92\u7403\u81ea\u9002\u5e94\u62df\u5408\u6570\u636e\u5206\u5e03\uff0c\u6bcf\u4e2a\u7c92\u7403\u57fa\u4e8e\u5bc6\u5ea6\u4e00\u81f4\u6027\u5ea6\u91cf\u5305\u542b\u6570\u636e\u70b9\u3002\u8fd9\u79cd\u57fa\u4e8e\u7c92\u7403\u7684\u6570\u636e\u63cf\u8ff0\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5bf9\u672a\u77e5\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u57fa\u4e8e\u7c92\u7403\u8868\u793a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7c92\u7403\u6838\uff08GBK\uff09\u53ca\u5176\u5bf9\u5e94\u7684\u7c92\u7403\u591a\u6838K\u5747\u503c\u6846\u67b6\uff08GB-MKKM\uff09\u4ee5\u5b9e\u73b0\u9ad8\u6548\u805a\u7c7b\u3002\u901a\u8fc7\u591a\u6838\u7a7a\u95f4\u4e2d\u7684\u7c92\u7403\u5173\u7cfb\uff0cGB-MKKM\u6846\u67b6\u5728\u591a\u79cd\u805a\u7c7b\u4efb\u52a1\u7684\u5b9e\u8bc1\u8bc4\u4f30\u4e2d\u5c55\u73b0\u4e86\u5176\u5728\u6548\u7387\u548c\u805a\u7c7b\u6027\u80fd\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.18640", "pdf": "https://arxiv.org/pdf/2506.18640", "abs": "https://arxiv.org/abs/2506.18640", "authors": ["Christian Intern\u00f2", "Markus Olhofer", "Yaochu Jin", "Barbara Hammer"], "title": "Federated Loss Exploration for Improved Convergence on Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) has emerged as a groundbreaking paradigm in machine\nlearning (ML), offering privacy-preserving collaborative model training across\ndiverse datasets. Despite its promise, FL faces significant hurdles in\nnon-identically and independently distributed (non-IID) data scenarios, where\nmost existing methods often struggle with data heterogeneity and lack\nrobustness in performance. This paper introduces Federated Loss Exploration\n(FedLEx), an innovative approach specifically designed to tackle these\nchallenges. FedLEx distinctively addresses the shortcomings of existing FL\nmethods in non-IID settings by optimizing its learning behavior for scenarios\nin which assumptions about data heterogeneity are impractical or unknown. It\nemploys a federated loss exploration technique, where clients contribute to a\nglobal guidance matrix by calculating gradient deviations for model parameters.\nThis matrix serves as a strategic compass to guide clients' gradient updates in\nsubsequent FL rounds, thereby fostering optimal parameter updates for the\nglobal model. FedLEx effectively navigates the complex loss surfaces inherent\nin non-IID data, enhancing knowledge transfer in an efficient manner, since\nonly a small number of epochs and small amount of data are required to build a\nstrong global guidance matrix that can achieve model convergence without the\nneed for additional data sharing or data distribution statics in a large client\nscenario. Our extensive experiments with state-of-the art FL algorithms\ndemonstrate significant improvements in performance, particularly under\nrealistic non-IID conditions, thus highlighting FedLEx's potential to overcome\ncritical barriers in diverse FL applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedLEx\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002\u901a\u8fc7\u4f18\u5316\u5b66\u4e60\u884c\u4e3a\u5e76\u5229\u7528\u68af\u5ea6\u504f\u5dee\u6784\u5efa\u5168\u5c40\u6307\u5bfc\u77e9\u9635\uff0cFedLEx\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u975eIID\u6570\u636e\u4e0b\u7684\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u573a\u666f\u4e2d\u9762\u4e34\u6027\u80fd\u4e0d\u8db3\u548c\u9c81\u68d2\u6027\u5dee\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u6570\u636e\u5171\u4eab\u6216\u6570\u636e\u5206\u5e03\u7edf\u8ba1\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FedLEx\u91c7\u7528\u8054\u90a6\u635f\u5931\u63a2\u7d22\u6280\u672f\uff0c\u5ba2\u6237\u7aef\u901a\u8fc7\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u504f\u5dee\u8d21\u732e\u5230\u5168\u5c40\u6307\u5bfc\u77e9\u9635\u3002\u8be5\u77e9\u9635\u4f5c\u4e3a\u7b56\u7565\u6307\u5357\uff0c\u6307\u5bfc\u540e\u7eed\u8054\u90a6\u5b66\u4e60\u8f6e\u6b21\u4e2d\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u4ece\u800c\u4f18\u5316\u5168\u5c40\u6a21\u578b\u7684\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedLEx\u5728\u975eIID\u6570\u636e\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u548c\u8bad\u7ec3\u8f6e\u6b21\u5373\u53ef\u6784\u5efa\u9ad8\u6548\u7684\u5168\u5c40\u6307\u5bfc\u77e9\u9635\uff0c\u5b9e\u73b0\u6a21\u578b\u5feb\u901f\u6536\u655b\u3002", "conclusion": "FedLEx\u901a\u8fc7\u521b\u65b0\u7684\u635f\u5931\u63a2\u7d22\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u975eIID\u6570\u636e\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u591a\u6837\u5316\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u5728\u7a81\u7834\u3002", "paper_title_zh": "\u8054\u90a6\u635f\u5931\u63a2\u7d22\uff1a\u63d0\u5347\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u7684\u6536\u655b\u6027\u80fd", "abstract_zh": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f5c\u4e3a\u4e00\u79cd\u7a81\u7834\u6027\u7684\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u573a\u666f\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u5f02\u8d28\u6027\u548c\u6027\u80fd\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u663e\u8457\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8054\u90a6\u635f\u5931\u63a2\u7d22\uff08FedLEx\uff09\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u8bbe\u8ba1\u3002FedLEx\u901a\u8fc7\u4f18\u5316\u5b66\u4e60\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u975eIID\u6570\u636e\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\u6216\u672a\u77e5\u65f6\u7684\u4e0d\u8db3\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8054\u90a6\u635f\u5931\u63a2\u7d22\u6280\u672f\uff0c\u5ba2\u6237\u7aef\u901a\u8fc7\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u504f\u5dee\u8d21\u732e\u5230\u5168\u5c40\u6307\u5bfc\u77e9\u9635\u3002\u8be5\u77e9\u9635\u4f5c\u4e3a\u7b56\u7565\u6307\u5357\uff0c\u6307\u5bfc\u540e\u7eed\u8054\u90a6\u5b66\u4e60\u8f6e\u6b21\u4e2d\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u4ece\u800c\u4f18\u5316\u5168\u5c40\u6a21\u578b\u7684\u53c2\u6570\u66f4\u65b0\u3002FedLEx\u80fd\u591f\u9ad8\u6548\u5e94\u5bf9\u975eIID\u6570\u636e\u4e2d\u590d\u6742\u7684\u635f\u5931\u66f2\u9762\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u548c\u8bad\u7ec3\u8f6e\u6b21\u5373\u53ef\u6784\u5efa\u5f3a\u5927\u7684\u5168\u5c40\u6307\u5bfc\u77e9\u9635\uff0c\u5b9e\u73b0\u6a21\u578b\u6536\u655b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u5171\u4eab\u6216\u5927\u89c4\u6a21\u5ba2\u6237\u7aef\u573a\u666f\u4e2d\u7684\u6570\u636e\u5206\u5e03\u7edf\u8ba1\u3002\u6211\u4eec\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFedLEx\u5728\u975eIID\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u591a\u6837\u5316\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u4e2d\u7a81\u7834\u5173\u952e\u969c\u788d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.18689", "pdf": "https://arxiv.org/pdf/2506.18689", "abs": "https://arxiv.org/abs/2506.18689", "authors": ["Alessandro Saviolo", "Giuseppe Loianno"], "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.", "AI": {"tldr": "NOVA\u662f\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u673a\u8f7d\u8bbe\u5907\u7684\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89c9\u81ea\u4e3b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u65e0GPS\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u901f\u76ee\u6807\u8ddf\u8e2a\u548c\u907f\u969c\u5bfc\u822a\uff0c\u4ec5\u9700\u7acb\u4f53\u76f8\u673a\u548cIMU\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u7a7a\u4e2d\u76ee\u6807\u8ddf\u8e2a\u5728\u975e\u7ed3\u6784\u5316\u548c\u65e0GPS\u73af\u5883\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u3001\u9884\u5efa\u5730\u56fe\u6216\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4f4d\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002NOVA\u65e8\u5728\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "NOVA\u91c7\u7528\u8f7b\u91cf\u7ea7\u76ee\u6807\u68c0\u6d4b\u5668\u548c\u7acb\u4f53\u6df1\u5ea6\u8865\u5168\u6280\u672f\uff0c\u7ed3\u5408\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u6ee4\u6ce2\u63a8\u65ad\u76ee\u6807\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u5668\u6062\u590d\u673a\u5668\u4eba\u76f8\u5bf9\u4e8e\u76ee\u6807\u76846\u81ea\u7531\u5ea6\u4f4d\u59ff\u3002\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08NMPC\uff09\u5728\u76ee\u6807\u5750\u6807\u7cfb\u4e2d\u89c4\u5212\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff0c\u540c\u65f6\u5229\u7528\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5b9e\u73b0\u5b9e\u65f6\u907f\u969c\u3002", "result": "NOVA\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\uff08\u5982\u57ce\u5e02\u8ff7\u5bab\u3001\u68ee\u6797\u5c0f\u5f84\u548c\u5efa\u7b51\u8fc7\u6e21\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u80fd\u591f\u5728\u95f4\u6b47\u6027GPS\u4e22\u5931\u548c\u5149\u7167\u53d8\u5316\u4e0b\u5b9e\u73b0\u8d85\u8fc750\u516c\u91cc/\u5c0f\u65f6\u7684\u9ad8\u901f\u76ee\u6807\u8ddf\u8e2a\u3002", "conclusion": "NOVA\u5c55\u793a\u4e86\u4ec5\u4f9d\u8d56\u673a\u8f7d\u4f20\u611f\u5668\u7684\u9ad8\u901f\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u6216\u73af\u5883\u5047\u8bbe\u3002", "paper_title_zh": "NOVA\uff1a\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89c9\u81ea\u4e3b\u7684\u9ad8\u901f\u76ee\u6807\u8ddf\u8e2a\u5728\u65e0GPS\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5bfc\u822a", "abstract_zh": "\u5728\u975e\u7ed3\u6784\u5316\u548c\u65e0GPS\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u7a7a\u4e2d\u76ee\u6807\u8ddf\u8e2a\u4ecd\u7136\u662f\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4e00\u9879\u57fa\u672c\u6311\u6218\u3002\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u3001\u9884\u5efa\u573a\u666f\u6216\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4f4d\u6765\u786e\u4fdd\u5b89\u5168\u548c\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u90e8\u7f72\u3002\u6211\u4eec\u63d0\u51fa\u4e86NOVA\uff0c\u4e00\u79cd\u5b8c\u5168\u673a\u8f7d\u7684\u5bf9\u8c61\u4e2d\u5fc3\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u7acb\u4f53\u76f8\u673a\u548cIMU\u5b9e\u73b0\u9c81\u68d2\u7684\u76ee\u6807\u8ddf\u8e2a\u548c\u907f\u969c\u5bfc\u822a\u3002NOVA\u4e0d\u6784\u5efa\u5168\u5c40\u5730\u56fe\u6216\u4f9d\u8d56\u7edd\u5bf9\u5b9a\u4f4d\uff0c\u800c\u662f\u5b8c\u5168\u5728\u76ee\u6807\u53c2\u8003\u7cfb\u4e2d\u5b9e\u73b0\u611f\u77e5\u3001\u4f30\u8ba1\u548c\u63a7\u5236\u3002\u4e00\u4e2a\u7d27\u5bc6\u96c6\u6210\u7684\u5806\u6808\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u76ee\u6807\u68c0\u6d4b\u5668\u4e0e\u7acb\u4f53\u6df1\u5ea6\u8865\u5168\uff0c\u968f\u540e\u901a\u8fc7\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u6ee4\u6ce2\u63a8\u65ad\u906e\u6321\u548c\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u76ee\u6807\u8ddd\u79bb\u3002\u8fd9\u4e9b\u6d4b\u91cf\u7ed3\u679c\u8f93\u5165\u89c6\u89c9\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u6062\u590d\u673a\u5668\u4eba\u76f8\u5bf9\u4e8e\u76ee\u6807\u7684\u5b8c\u65746\u81ea\u7531\u5ea6\u4f4d\u59ff\u3002\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08NMPC\uff09\u5728\u76ee\u6807\u5750\u6807\u7cfb\u4e2d\u89c4\u5212\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\u3002\u4e3a\u786e\u4fdd\u5b89\u5168\uff0c\u4ece\u6df1\u5ea6\u63d0\u53d6\u7684\u9ad8\u98ce\u9669\u78b0\u649e\u70b9\u6784\u5efa\u5728\u7ebf\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u5b9e\u73b0\u65e0\u9700\u5730\u56fe\u6216\u5bc6\u96c6\u8868\u793a\u7684\u5b9e\u65f6\u907f\u969c\u3002\u6211\u4eec\u5728\u591a\u79cd\u6311\u6218\u6027\u73b0\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86NOVA\uff0c\u5305\u62ec\u57ce\u5e02\u8ff7\u5bab\u3001\u68ee\u6797\u5c0f\u5f84\u548c\u5efa\u7b51\u8fc7\u6e21\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2dGPS\u95f4\u6b47\u6027\u4e22\u5931\u548c\u4e25\u91cd\u5149\u7167\u53d8\u5316\u4f1a\u5e72\u6270\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4f4d\u3002\u6bcf\u4e2a\u5b9e\u9a8c\u5728\u76f8\u4f3c\u6761\u4ef6\u4e0b\u91cd\u590d\u591a\u6b21\u4ee5\u8bc4\u4f30\u9c81\u68d2\u6027\uff0c\u7ed3\u679c\u663e\u793aNOVA\u5177\u6709\u4e00\u81f4\u4e14\u53ef\u9760\u7684\u6027\u80fd\u3002NOVA\u5b9e\u73b0\u4e86\u8d85\u8fc750\u516c\u91cc/\u5c0f\u65f6\u7684\u654f\u6377\u76ee\u6807\u8ddf\u8e2a\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f9d\u8d56\u673a\u8f7d\u4f20\u611f\u7684\u9ad8\u901f\u89c6\u89c9\u8ddf\u8e2a\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u6216\u73af\u5883\u5047\u8bbe\u3002"}}
{"id": "2506.18714", "pdf": "https://arxiv.org/pdf/2506.18714", "abs": "https://arxiv.org/abs/2506.18714", "authors": ["Nasser-Eddine Monir", "Paul Magron", "Romain Serizel"], "title": "Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "This is the preprint of the paper submitted to the 26th IEEE\n  International Workshop on Multimedia Signal Processing (MMSP)", "summary": "Recent advances in deep learning have significantly improved multichannel\nspeech enhancement algorithms, yet conventional training loss functions such as\nthe scale-invariant signal-to-distortion ratio (SDR) may fail to preserve\nfine-grained spectral cues essential for phoneme intelligibility. In this work,\nwe propose perceptually-informed variants of the SDR loss, formulated in the\ntime-frequency domain and modulated by frequency-dependent weighting schemes.\nThese weights are designed to emphasize time-frequency regions where speech is\nprominent or where the interfering noise is particularly strong. We investigate\nboth fixed and adaptive strategies, including ANSI band-importance weights,\nspectral magnitude-based weighting, and dynamic weighting based on the relative\namount of speech and noise. We train the FaSNet multichannel speech enhancement\nmodel using these various losses. Experimental results show that while standard\nmetrics such as the SDR are only marginally improved, their perceptual\nfrequency-weighted counterparts exhibit a more substantial improvement.\nBesides, spectral and phoneme-level analysis indicates better consonant\nreconstruction, which points to a better preservation of certain acoustic cues.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u52a0\u6743\u7684\u8bad\u7ec3\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u5bf9\u97f3\u7d20\u7ea7\u7ec6\u8282\u7684\u4fdd\u7559\u80fd\u529b\u3002\u901a\u8fc7\u8bbe\u8ba1\u56fa\u5b9a\u548c\u81ea\u9002\u5e94\u7684\u9891\u7387\u6743\u91cd\u7b56\u7565\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u611f\u77e5\u6307\u6807\u548c\u97f3\u7d20\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u4f20\u7edf\u8bad\u7ec3\u635f\u5931\u51fd\u6570\uff08\u5982SDR\uff09\u5728\u591a\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u4e2d\u53ef\u80fd\u65e0\u6cd5\u4fdd\u7559\u5bf9\u97f3\u7d20\u53ef\u61c2\u6027\u81f3\u5173\u91cd\u8981\u7684\u9891\u8c31\u7ec6\u8282\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u611f\u77e5\u9a71\u52a8\u7684\u9891\u7387\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u66f4\u597d\u5730\u63d0\u5347\u8bed\u97f3\u589e\u5f3a\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u65f6\u9891\u57df\u7684\u9891\u7387\u52a0\u6743SDR\u635f\u5931\u51fd\u6570\u53d8\u4f53\uff0c\u5305\u62ec\u56fa\u5b9a\u6743\u91cd\uff08\u5982ANSI\u9891\u5e26\u91cd\u8981\u6027\u6743\u91cd\uff09\u548c\u81ea\u9002\u5e94\u6743\u91cd\uff08\u5982\u57fa\u4e8e\u8bed\u97f3\u548c\u566a\u58f0\u76f8\u5bf9\u91cf\u7684\u52a8\u6001\u6743\u91cd\uff09\u3002\u4f7f\u7528FaSNet\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u6743\u91cd\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u6807\u51c6\u6307\u6807\uff08\u5982SDR\uff09\u63d0\u5347\u6709\u9650\uff0c\u4f46\u611f\u77e5\u9891\u7387\u52a0\u6743\u6307\u6807\u663e\u8457\u6539\u5584\u3002\u9891\u8c31\u548c\u97f3\u7d20\u7ea7\u5206\u6790\u8868\u660e\uff0c\u8f85\u97f3\u91cd\u5efa\u6548\u679c\u66f4\u597d\uff0c\u8bf4\u660e\u67d0\u4e9b\u58f0\u5b66\u7ebf\u7d22\u5f97\u5230\u4e86\u66f4\u597d\u7684\u4fdd\u7559\u3002", "conclusion": "\u9891\u7387\u52a0\u6743\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u5728\u591a\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u4e2d\u80fd\u591f\u66f4\u6709\u6548\u5730\u4fdd\u7559\u97f3\u7d20\u7ea7\u7ec6\u8282\uff0c\u5c24\u5176\u662f\u5728\u611f\u77e5\u6307\u6807\u548c\u8f85\u97f3\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "paper_title_zh": "\u57fa\u4e8e\u9891\u7387\u52a0\u6743\u7684\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7528\u4e8e\u97f3\u7d20\u7ea7DNN\u8bed\u97f3\u589e\u5f3a", "abstract_zh": "\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u63d0\u5347\u4e86\u591a\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4f46\u4f20\u7edf\u8bad\u7ec3\u635f\u5931\u51fd\u6570\uff08\u5982\u5c3a\u5ea6\u4e0d\u53d8\u4fe1\u566a\u6bd4SDR\uff09\u53ef\u80fd\u65e0\u6cd5\u4fdd\u7559\u5bf9\u97f3\u7d20\u53ef\u61c2\u6027\u81f3\u5173\u91cd\u8981\u7684\u7cbe\u7ec6\u9891\u8c31\u7ebf\u7d22\u3002\u672c\u6587\u63d0\u51fa\u4e86\u611f\u77e5\u9a71\u52a8\u7684SDR\u635f\u5931\u51fd\u6570\u53d8\u4f53\uff0c\u5728\u65f6\u9891\u57df\u4e2d\u901a\u8fc7\u9891\u7387\u76f8\u5173\u6743\u91cd\u8fdb\u884c\u8c03\u5236\u3002\u8fd9\u4e9b\u6743\u91cd\u65e8\u5728\u5f3a\u8c03\u8bed\u97f3\u663e\u8457\u6216\u566a\u58f0\u8f83\u5f3a\u7684\u65f6\u9891\u533a\u57df\u3002\u6211\u4eec\u7814\u7a76\u4e86\u56fa\u5b9a\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ecANSI\u9891\u5e26\u91cd\u8981\u6027\u6743\u91cd\u3001\u57fa\u4e8e\u9891\u8c31\u5e45\u5ea6\u7684\u52a0\u6743\u4ee5\u53ca\u57fa\u4e8e\u8bed\u97f3\u548c\u566a\u58f0\u76f8\u5bf9\u91cf\u7684\u52a8\u6001\u52a0\u6743\u3002\u4f7f\u7528FaSNet\u591a\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u6807\u51c6\u6307\u6807\uff08\u5982SDR\uff09\u4ec5\u7565\u6709\u63d0\u5347\uff0c\u4f46\u5176\u611f\u77e5\u9891\u7387\u52a0\u6743\u7248\u672c\u8868\u73b0\u51fa\u66f4\u663e\u8457\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u9891\u8c31\u548c\u97f3\u7d20\u7ea7\u5206\u6790\u8868\u660e\u8f85\u97f3\u91cd\u5efa\u6548\u679c\u66f4\u597d\uff0c\u8bf4\u660e\u67d0\u4e9b\u58f0\u5b66\u7ebf\u7d22\u5f97\u5230\u4e86\u66f4\u597d\u7684\u4fdd\u7559\u3002"}}
{"id": "2506.18717", "pdf": "https://arxiv.org/pdf/2506.18717", "abs": "https://arxiv.org/abs/2506.18717", "authors": ["Linyue Hu", "Qi Wang"], "title": "A Study of Dynamic Stock Relationship Modeling and S&P500 Price Forecasting Based on Differential Graph Transformer", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Stock price prediction is vital for investment decisions and risk management,\nyet remains challenging due to markets' nonlinear dynamics and time-varying\ninter-stock correlations. Traditional static-correlation models fail to capture\nevolving stock relationships. To address this, we propose a Differential Graph\nTransformer (DGT) framework for dynamic relationship modeling and price\nprediction. Our DGT integrates sequential graph structure changes into\nmulti-head self-attention via a differential graph mechanism, adaptively\npreserving high-value connections while suppressing noise. Causal temporal\nattention captures global/local dependencies in price sequences. We further\nevaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's\nTau) across global/local/dual scopes as spatial-attention priors. Using 10\nyears of S&P 500 closing prices (z-score normalized; 64-day sliding windows),\nDGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87).\nKendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means\nclustering revealed \"high-volatility growth\" and \"defensive blue-chip\" stocks,\nwith the latter showing lower errors (RMSE: 0.13) due to stable correlations.\nKendall's Tau and Mutual Information excelled in volatile sectors. This study\ninnovatively combines differential graph structures with Transformers,\nvalidating dynamic relationship modeling and identifying optimal correlation\nmetrics/scopes. Clustering analysis supports tailored quantitative strategies.\nOur framework advances financial time-series prediction through dynamic\nmodeling and cross-asset interaction analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u56feTransformer\uff08DGT\uff09\u7684\u52a8\u6001\u80a1\u7968\u5173\u7cfb\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4bS&P500\u4ef7\u683c\u3002\u901a\u8fc7\u6574\u5408\u52a8\u6001\u56fe\u7ed3\u6784\u548c\u591a\u5934\u90e8\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0cDGT\u80fd\u591f\u81ea\u9002\u5e94\u6355\u6349\u80a1\u7968\u95f4\u7684\u65f6\u53d8\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u5bf9\u6295\u8d44\u51b3\u7b56\u548c\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5e02\u573a\u7684\u975e\u7ebf\u6027\u52a8\u6001\u548c\u65f6\u53d8\u7684\u80a1\u7968\u95f4\u76f8\u5173\u6027\uff0c\u4f20\u7edf\u9759\u6001\u76f8\u5173\u6027\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8fd9\u4e9b\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5efa\u6a21\u80a1\u7968\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5dee\u5206\u56feTransformer\uff08DGT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5dee\u5206\u56fe\u673a\u5236\u5c06\u52a8\u6001\u56fe\u7ed3\u6784\u53d8\u5316\u6574\u5408\u5230\u591a\u5934\u90e8\u81ea\u6ce8\u610f\u529b\u4e2d\uff0c\u81ea\u9002\u5e94\u4fdd\u7559\u9ad8\u4ef7\u503c\u8fde\u63a5\u5e76\u6291\u5236\u566a\u58f0\u3002\u540c\u65f6\uff0c\u91c7\u7528\u56e0\u679c\u65f6\u95f4\u6ce8\u610f\u529b\u6355\u6349\u4ef7\u683c\u5e8f\u5217\u7684\u5168\u5c40/\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u8bc4\u4f30\u591a\u79cd\u76f8\u5173\u6027\u6307\u6807\uff08\u5982Pearson\u3001\u4e92\u4fe1\u606f\u7b49\uff09\u4f5c\u4e3a\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u5148\u9a8c\u3002", "result": "\u572810\u5e74S&P500\u6536\u76d8\u4ef7\u6570\u636e\u4e0a\uff0cDGT\u7ed3\u5408\u7a7a\u95f4\u5148\u9a8c\u663e\u8457\u4f18\u4e8eGRU\u57fa\u7ebf\uff08RMSE\uff1a0.24 vs. 0.87\uff09\u3002Kendall's Tau\u5168\u5c40\u77e9\u9635\u8868\u73b0\u6700\u4f18\uff08MAE\uff1a0.11\uff09\u3002\u805a\u7c7b\u5206\u6790\u53d1\u73b0\u201c\u9ad8\u6ce2\u52a8\u589e\u957f\u201d\u548c\u201c\u9632\u5fa1\u6027\u84dd\u7b79\u80a1\u201d\u4e24\u7c7b\u80a1\u7968\uff0c\u540e\u8005\u56e0\u7a33\u5b9a\u76f8\u5173\u6027\u8bef\u5dee\u66f4\u4f4e\uff08RMSE\uff1a0.13\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u521b\u65b0\u6027\u5730\u5c06\u5dee\u5206\u56fe\u7ed3\u6784\u4e0eTransformer\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4e86\u52a8\u6001\u5173\u7cfb\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u6700\u4f18\u76f8\u5173\u6027\u6307\u6807\u548c\u8303\u56f4\u3002\u805a\u7c7b\u5206\u6790\u652f\u6301\u5b9a\u5236\u5316\u91cf\u5316\u7b56\u7565\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "paper_title_zh": "\u57fa\u4e8e\u5dee\u5206\u56feTransformer\u7684\u52a8\u6001\u80a1\u7968\u5173\u7cfb\u5efa\u6a21\u4e0eS&P500\u4ef7\u683c\u9884\u6d4b\u7814\u7a76", "abstract_zh": "\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u5bf9\u6295\u8d44\u51b3\u7b56\u548c\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5e02\u573a\u7684\u975e\u7ebf\u6027\u52a8\u6001\u548c\u65f6\u53d8\u7684\u80a1\u7968\u95f4\u76f8\u5173\u6027\uff0c\u4f20\u7edf\u9759\u6001\u76f8\u5173\u6027\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8fd9\u4e9b\u53d8\u5316\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5206\u56feTransformer\uff08DGT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u5173\u7cfb\u5efa\u6a21\u548c\u4ef7\u683c\u9884\u6d4b\u3002DGT\u901a\u8fc7\u5dee\u5206\u56fe\u673a\u5236\u5c06\u52a8\u6001\u56fe\u7ed3\u6784\u53d8\u5316\u6574\u5408\u5230\u591a\u5934\u90e8\u81ea\u6ce8\u610f\u529b\u4e2d\uff0c\u81ea\u9002\u5e94\u4fdd\u7559\u9ad8\u4ef7\u503c\u8fde\u63a5\u5e76\u6291\u5236\u566a\u58f0\u3002\u56e0\u679c\u65f6\u95f4\u6ce8\u610f\u529b\u6355\u6349\u4ef7\u683c\u5e8f\u5217\u7684\u5168\u5c40/\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e86\u591a\u79cd\u76f8\u5173\u6027\u6307\u6807\uff08Pearson\u3001\u4e92\u4fe1\u606f\u3001Spearman\u3001Kendall's Tau\uff09\u4f5c\u4e3a\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u5148\u9a8c\u3002\u4f7f\u752810\u5e74S&P500\u6536\u76d8\u4ef7\u6570\u636e\uff08z-score\u6807\u51c6\u5316\uff1b64\u5929\u6ed1\u52a8\u7a97\u53e3\uff09\uff0cDGT\u7ed3\u5408\u7a7a\u95f4\u5148\u9a8c\u663e\u8457\u4f18\u4e8eGRU\u57fa\u7ebf\uff08RMSE\uff1a0.24 vs. 0.87\uff09\u3002Kendall's Tau\u5168\u5c40\u77e9\u9635\u8868\u73b0\u6700\u4f18\uff08MAE\uff1a0.11\uff09\u3002K-means\u805a\u7c7b\u63ed\u793a\u4e86\u201c\u9ad8\u6ce2\u52a8\u589e\u957f\u201d\u548c\u201c\u9632\u5fa1\u6027\u84dd\u7b79\u80a1\u201d\u4e24\u7c7b\u80a1\u7968\uff0c\u540e\u8005\u56e0\u7a33\u5b9a\u76f8\u5173\u6027\u8bef\u5dee\u66f4\u4f4e\uff08RMSE\uff1a0.13\uff09\u3002Kendall's Tau\u548c\u4e92\u4fe1\u606f\u5728\u6ce2\u52a8\u6027\u884c\u4e1a\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u672c\u7814\u7a76\u521b\u65b0\u6027\u5730\u5c06\u5dee\u5206\u56fe\u7ed3\u6784\u4e0eTransformer\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4e86\u52a8\u6001\u5173\u7cfb\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u6700\u4f18\u76f8\u5173\u6027\u6307\u6807\u548c\u8303\u56f4\u3002\u805a\u7c7b\u5206\u6790\u652f\u6301\u5b9a\u5236\u5316\u91cf\u5316\u7b56\u7565\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.18729", "pdf": "https://arxiv.org/pdf/2506.18729", "abs": "https://arxiv.org/abs/2506.18729", "authors": ["Fang-Duo Tsai", "Shih-Lun Wu", "Weijaw Lee", "Sheng-Ping Yang", "Bo-Rui Chen", "Hao-Chung Cheng", "Yi-Hsuan Yang"], "title": "MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune\ntext-to-music generation models for precise conditioning using various\ntime-varying musical attributes and reference audio signals. The key finding is\nthat positional embeddings, which have been seldom used by text-to-music\ngeneration models in the conditioner for text conditions, are critical when the\ncondition of interest is a function of time. Using melody control as an\nexample, our experiments show that simply adding rotary positional embeddings\nto the decoupled cross-attention layers increases control accuracy from 56.6%\nto 61.1%, while requiring 6.75 times fewer trainable parameters than\nstate-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion\nTransformer model of Stable Audio Open. We evaluate various forms of musical\nattribute control, audio inpainting, and audio outpainting, demonstrating\nimproved controllability over MusicGen-Large and Stable Audio Open ControlNet\nat a significantly lower fine-tuning cost, with only 85M trainble parameters.\nSource code, model checkpoints, and demo examples are available at: https:\n//MuseControlLite.github.io/web/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMuseControlLite\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u673a\u5236\uff0c\u7528\u4e8e\u901a\u8fc7\u65f6\u95f4\u53d8\u5316\u7684\u97f3\u4e50\u5c5e\u6027\u548c\u53c2\u8003\u97f3\u9891\u4fe1\u53f7\u5bf9\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u8c03\u8282\u3002\u5173\u952e\u53d1\u73b0\u662f\u4f4d\u7f6e\u5d4c\u5165\u5728\u65f6\u95f4\u76f8\u5173\u6761\u4ef6\u4e0b\u81f3\u5173\u91cd\u8981\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u63d0\u5347\u63a7\u5236\u7cbe\u5ea6\u5e76\u5927\u5e45\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u5728\u65f6\u95f4\u76f8\u5173\u6761\u4ef6\u4e0b\u7684\u63a7\u5236\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u4e14\u8c03\u8282\u673a\u5236\u901a\u5e38\u9700\u8981\u5927\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8c03\u8282\u673a\u5236\uff0c\u63d0\u5347\u63a7\u5236\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faMuseControlLite\uff0c\u901a\u8fc7\u5411\u89e3\u8026\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6dfb\u52a0\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4f18\u5316\u65f6\u95f4\u76f8\u5173\u6761\u4ef6\u7684\u63a7\u5236\u3002\u5b9e\u9a8c\u4ee5\u65cb\u5f8b\u63a7\u5236\u4e3a\u4f8b\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMuseControlLite\u5c06\u63a7\u5236\u7cbe\u5ea6\u4ece56.6%\u63d0\u5347\u81f361.1%\uff0c\u540c\u65f6\u4ec5\u970085M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c116.75\u500d\u3002\u5728\u97f3\u4e50\u5c5e\u6027\u63a7\u5236\u3001\u97f3\u9891\u4fee\u590d\u548c\u6269\u5c55\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eMusicGen-Large\u548cStable Audio Open ControlNet\u3002", "conclusion": "MuseControlLite\u5728\u663e\u8457\u964d\u4f4e\u5fae\u8c03\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u591a\u529f\u80fd\u6027\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u8c03\u8282\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "MuseControlLite\uff1a\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u8c03\u8282\u5668\u7684\u591a\u529f\u80fd\u97f3\u4e50\u751f\u6210", "abstract_zh": "\u6211\u4eec\u63d0\u51faMuseControlLite\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u673a\u5236\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u79cd\u65f6\u95f4\u53d8\u5316\u7684\u97f3\u4e50\u5c5e\u6027\u548c\u53c2\u8003\u97f3\u9891\u4fe1\u53f7\u5bf9\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u8c03\u8282\u3002\u5173\u952e\u53d1\u73b0\u662f\uff0c\u4f4d\u7f6e\u5d4c\u5165\u5728\u6587\u672c\u6761\u4ef6\u8c03\u8282\u5668\u4e2d\u5f88\u5c11\u88ab\u4f7f\u7528\uff0c\u4f46\u5bf9\u4e8e\u65f6\u95f4\u76f8\u5173\u6761\u4ef6\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u65cb\u5f8b\u63a7\u5236\u4e3a\u4f8b\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u9700\u5728\u89e3\u8026\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u4e2d\u6dfb\u52a0\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5373\u53ef\u5c06\u63a7\u5236\u7cbe\u5ea6\u4ece56.6%\u63d0\u5347\u81f361.1%\uff0c\u540c\u65f6\u6240\u9700\u53ef\u8bad\u7ec3\u53c2\u6570\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5fae\u8c03\u673a\u5236\u51cf\u5c116.75\u500d\uff08\u57fa\u4e8e\u76f8\u540c\u7684\u9884\u8bad\u7ec3\u6269\u6563Transformer\u6a21\u578bStable Audio Open\uff09\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u97f3\u4e50\u5c5e\u6027\u63a7\u5236\u3001\u97f3\u9891\u4fee\u590d\u548c\u6269\u5c55\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u5176\u63a7\u5236\u6027\u80fd\u4f18\u4e8eMusicGen-Large\u548cStable Audio Open ControlNet\uff0c\u4e14\u5fae\u8c03\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0c\u4ec5\u970085M\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u6e90\u4ee3\u7801\u3001\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u6f14\u793a\u793a\u4f8b\u8be6\u89c1\uff1ahttps://MuseControlLite.github.io/web/\u3002"}}
{"id": "2506.18739", "pdf": "https://arxiv.org/pdf/2506.18739", "abs": "https://arxiv.org/abs/2506.18739", "authors": ["Debanjan Dutta", "Faizanuddin Ansari", "Anish Chakrabarty", "Swagatam Das"], "title": "On the Existence of Universal Simulators of Attention", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prior work on the learnability of transformers has established its capacity\nto approximate specific algorithmic patterns through training under restrictive\narchitectural assumptions. Fundamentally, these arguments remain data-driven\nand therefore can only provide a probabilistic guarantee. Expressivity, on the\ncontrary, has theoretically been explored to address the problems\n\\emph{computable} by such architecture. These results proved the\nTuring-completeness of transformers, investigated bounds focused on circuit\ncomplexity, and formal logic. Being at the crossroad between learnability and\nexpressivity, the question remains: \\emph{can transformer architectures exactly\nsimulate an arbitrary attention mechanism, or in particular, the underlying\noperations?} In this study, we investigate the transformer encoder's ability to\nsimulate a vanilla attention mechanism. By constructing a universal simulator\n$\\mathcal{U}$ composed of transformer encoders, we present algorithmic\nsolutions to identically replicate attention outputs and the underlying\nelementary matrix and activation operations via RASP, a formal framework for\ntransformer computation. Our proofs, for the first time, show the existence of\nan algorithmically achievable data-agnostic solution, previously known to be\napproximated only by learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u7f16\u7801\u5668\u662f\u5426\u80fd\u591f\u7cbe\u786e\u6a21\u62df\u4efb\u610f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7279\u522b\u662f\u5176\u5e95\u5c42\u64cd\u4f5c\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u7531Transformer\u7f16\u7801\u5668\u7ec4\u6210\u7684\u901a\u7528\u6a21\u62df\u5668\uff0c\u4f5c\u8005\u9996\u6b21\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u79cd\u7b97\u6cd5\u53ef\u5b9e\u73b0\u7684\u6570\u636e\u65e0\u5173\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728Transformer\u7684\u53ef\u5b66\u4e60\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e0a\uff0c\u4f46\u7f3a\u4e4f\u5bf9Transformer\u662f\u5426\u80fd\u7cbe\u786e\u6a21\u62df\u6ce8\u610f\u529b\u673a\u5236\u7684\u7406\u8bba\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22Transformer\u7f16\u7801\u5668\u662f\u5426\u80fd\u591f\u5b8c\u5168\u6a21\u62df\u6ce8\u610f\u529b\u673a\u5236\u53ca\u5176\u5e95\u5c42\u64cd\u4f5c\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a$\\mathcal{U}$\u7684\u901a\u7528\u6a21\u62df\u5668\uff0c\u7531Transformer\u7f16\u7801\u5668\u7ec4\u6210\uff0c\u5e76\u901a\u8fc7RASP\uff08\u4e00\u79cdTransformer\u8ba1\u7b97\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff09\u63d0\u4f9b\u4e86\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5b8c\u5168\u590d\u5236\u6ce8\u610f\u529b\u8f93\u51fa\u53ca\u5176\u5e95\u5c42\u77e9\u9635\u548c\u6fc0\u6d3b\u64cd\u4f5c\u3002", "result": "\u7814\u7a76\u9996\u6b21\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u79cd\u7b97\u6cd5\u53ef\u5b9e\u73b0\u7684\u6570\u636e\u65e0\u5173\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7cbe\u786e\u6a21\u62df\u6ce8\u610f\u529b\u673a\u5236\u53ca\u5176\u5e95\u5c42\u64cd\u4f5c\uff0c\u800c\u6b64\u524d\u4ec5\u80fd\u901a\u8fc7\u5b66\u4e60\u8fd1\u4f3c\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u7b97\u6cd5\u6784\u5efa\uff0c\u5c55\u793a\u4e86Transformer\u7f16\u7801\u5668\u80fd\u591f\u5b8c\u5168\u6a21\u62df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3aTransformer\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u3002", "paper_title_zh": "\u5173\u4e8e\u6ce8\u610f\u529b\u901a\u7528\u6a21\u62df\u5668\u7684\u5b58\u5728\u6027", "abstract_zh": "\u5148\u524d\u5173\u4e8eTransformer\u53ef\u5b66\u4e60\u6027\u7684\u7814\u7a76\u5728\u53d7\u9650\u67b6\u6784\u5047\u8bbe\u4e0b\u901a\u8fc7\u8bad\u7ec3\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u7279\u5b9a\u7b97\u6cd5\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u8fd9\u4e9b\u8bba\u8bc1\u672c\u8d28\u4e0a\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u56e0\u6b64\u53ea\u80fd\u63d0\u4f9b\u6982\u7387\u6027\u4fdd\u8bc1\u3002\u76f8\u53cd\uff0c\u8868\u8fbe\u80fd\u529b\u4ece\u7406\u8bba\u4e0a\u63a2\u8ba8\u4e86\u6b64\u7c7b\u67b6\u6784\u53ef\u8ba1\u7b97\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86Transformer\u7684\u56fe\u7075\u5b8c\u5907\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u7535\u8def\u590d\u6742\u6027\u548c\u5f62\u5f0f\u903b\u8f91\u7684\u754c\u9650\u3002\u5904\u4e8e\u53ef\u5b66\u4e60\u6027\u548c\u8868\u8fbe\u80fd\u529b\u7684\u4ea4\u53c9\u70b9\uff0c\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff1aTransformer\u67b6\u6784\u80fd\u5426\u7cbe\u786e\u6a21\u62df\u4efb\u610f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7279\u522b\u662f\u5176\u5e95\u5c42\u64cd\u4f5c\uff1f\u672c\u7814\u7a76\u63a2\u8ba8\u4e86Transformer\u7f16\u7801\u5668\u6a21\u62df\u666e\u901a\u6ce8\u610f\u529b\u673a\u5236\u7684\u80fd\u529b\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u7531Transformer\u7f16\u7801\u5668\u7ec4\u6210\u7684\u901a\u7528\u6a21\u62df\u5668$\\mathcal{U}$\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u901a\u8fc7RASP\uff08\u4e00\u79cdTransformer\u8ba1\u7b97\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff09\u5b8c\u5168\u590d\u5236\u6ce8\u610f\u529b\u8f93\u51fa\u53ca\u5176\u5e95\u5c42\u77e9\u9635\u548c\u6fc0\u6d3b\u64cd\u4f5c\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u8bc1\u660e\u9996\u6b21\u5c55\u793a\u4e86\u5b58\u5728\u4e00\u79cd\u7b97\u6cd5\u53ef\u5b9e\u73b0\u7684\u6570\u636e\u65e0\u5173\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u6b64\u524d\u4ec5\u80fd\u901a\u8fc7\u5b66\u4e60\u8fd1\u4f3c\u5b9e\u73b0\u3002"}}
{"id": "2506.18747", "pdf": "https://arxiv.org/pdf/2506.18747", "abs": "https://arxiv.org/abs/2506.18747", "authors": ["Lorenzo Simone", "Davide Bacciu", "Shuangge Ma"], "title": "ContinualFlow: Learning and Unlearning with Neural Flow Matching", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the ICML 2025 Workshop on Machine Unlearning for\n  Generative AI (MUGen @ ICML25, Vancouver, July 2025)", "summary": "We introduce ContinualFlow, a principled framework for targeted unlearning in\ngenerative models via Flow Matching. Our method leverages an energy-based\nreweighting loss to softly subtract undesired regions of the data distribution\nwithout retraining from scratch or requiring direct access to the samples to be\nunlearned. Instead, it relies on energy-based proxies to guide the unlearning\nprocess. We prove that this induces gradients equivalent to Flow Matching\ntoward a soft mass-subtracted target, and validate the framework through\nexperiments on 2D and image domains, supported by interpretable visualizations\nand quantitative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faContinualFlow\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u5b9e\u73b0\u751f\u6210\u6a21\u578b\u4e2d\u7684\u76ee\u6807\u9057\u5fd8\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u6216\u76f4\u63a5\u8bbf\u95ee\u9700\u9057\u5fd8\u6837\u672c\uff0c\u5229\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u91cd\u52a0\u6743\u635f\u5931\u8f6f\u6027\u53bb\u9664\u6570\u636e\u5206\u5e03\u4e2d\u7684\u4e0d\u671f\u671b\u533a\u57df\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u9700\u9057\u5fd8\u67d0\u4e9b\u6570\u636e\u533a\u57df\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u4ece\u5934\u8bad\u7ec3\u6216\u76f4\u63a5\u8bbf\u95ee\u9057\u5fd8\u6837\u672c\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u5b9e\u7528\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8fd9\u4e9b\u6761\u4ef6\u7684\u9ad8\u6548\u9057\u5fd8\u65b9\u6cd5\u3002", "method": "ContinualFlow\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u91cd\u52a0\u6743\u635f\u5931\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u6280\u672f\u8f6f\u6027\u53bb\u9664\u6570\u636e\u5206\u5e03\u4e2d\u7684\u4e0d\u671f\u671b\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u80fd\u91cf\u4ee3\u7406\u6307\u5bfc\u9057\u5fd8\u8fc7\u7a0b\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u9057\u5fd8\u6837\u672c\u6216\u4ece\u5934\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u57282D\u548c\u56fe\u50cf\u9886\u57df\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u89e3\u91ca\u548c\u5b9a\u91cf\u8bc4\u4f30\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "ContinualFlow\u4e3a\u751f\u6210\u6a21\u578b\u4e2d\u7684\u76ee\u6807\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u6216\u76f4\u63a5\u8bbf\u95ee\u9057\u5fd8\u6837\u672c\u3002", "paper_title_zh": "ContinualFlow\uff1a\u57fa\u4e8e\u795e\u7ecf\u6d41\u5339\u914d\u7684\u5b66\u4e60\u4e0e\u9057\u5fd8", "abstract_zh": "\u6211\u4eec\u63d0\u51fa\u4e86ContinualFlow\uff0c\u4e00\u79cd\u901a\u8fc7\u6d41\u5339\u914d\u5b9e\u73b0\u751f\u6210\u6a21\u578b\u4e2d\u76ee\u6807\u9057\u5fd8\u7684\u539f\u5219\u6027\u6846\u67b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u91cd\u52a0\u6743\u635f\u5931\uff0c\u8f6f\u6027\u53bb\u9664\u6570\u636e\u5206\u5e03\u4e2d\u7684\u4e0d\u671f\u671b\u533a\u57df\uff0c\u800c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u6216\u76f4\u63a5\u8bbf\u95ee\u9700\u9057\u5fd8\u7684\u6837\u672c\u3002\u76f8\u53cd\uff0c\u5b83\u4f9d\u8d56\u57fa\u4e8e\u80fd\u91cf\u7684\u4ee3\u7406\u6765\u6307\u5bfc\u9057\u5fd8\u8fc7\u7a0b\u3002\u6211\u4eec\u8bc1\u660e\u8fd9\u80fd\u8bf1\u5bfc\u4e0e\u6d41\u5339\u914d\u7b49\u6548\u7684\u68af\u5ea6\uff0c\u671d\u5411\u8f6f\u6027\u8d28\u91cf\u51cf\u9664\u7684\u76ee\u6807\uff0c\u5e76\u901a\u8fc72D\u548c\u56fe\u50cf\u9886\u57df\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u8f85\u4ee5\u53ef\u89e3\u91ca\u7684\u53ef\u89c6\u5316\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2506.18749", "pdf": "https://arxiv.org/pdf/2506.18749", "abs": "https://arxiv.org/abs/2506.18749", "authors": ["Abdul Basit", "Maha Nawaz", "Muhammad Shafique"], "title": "BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility", "categories": ["cs.HC", "cs.AI", "cs.RO", "I.2.9; I.2.7"], "comment": "9 pages, 12 figures, Accepted at IJCNN 2025", "summary": "Non-invasive brain-computer interfaces (BCIs) have the potential to enable\nintuitive control of prosthetic limbs for individuals with upper limb\namputations. However, existing EEG-based control systems face challenges\nrelated to signal noise, classification accuracy, and real-time adaptability.\nIn this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic\nsystem that integrates ensemble learning-based EEG classification with a\nhuman-in-the-loop (HITL) correction framework for enhanced responsiveness.\nUnlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims\nto interpret EEG-driven motor intent, enabling movement control without\nreliance on residual muscle activity. To improve classification robustness,\nBRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,\nachieving a classification accuracy of 96% across test subjects. EEG signals\nare preprocessed using a bandpass filter (0.5-45 Hz), Independent Component\nAnalysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature\nextraction to minimize contamination from electromyographic (EMG) and\nelectrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic\nspeech recognition (ASR) to facilitate intuitive mode switching between\ndifferent degrees of freedom (DOF) in the prosthetic arm. The system operates\nin real time, with a response latency of 150 ms, leveraging Lab Streaming Layer\n(LSL) networking for synchronized data acquisition. The system is evaluated on\nan in-house fabricated prosthetic arm and on multiple participants highlighting\nthe generalizability across users. The system is optimized for low-power\nembedded deployment, ensuring practical real-world application beyond\nhigh-performance computing environments. Our results indicate that BRAVE offers\na promising step towards robust, real-time, non-invasive prosthetic control.", "AI": {"tldr": "BRAVE\u662f\u4e00\u79cd\u7ed3\u5408\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u8bed\u97f3\u63a7\u5236\u7684\u5047\u80a2\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u7b97\u6cd5\u548c\u5b9e\u65f6\u6821\u6b63\u6846\u67b6\uff0c\u5b9e\u73b0\u4e8696%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c150\u6beb\u79d2\u7684\u54cd\u5e94\u5ef6\u8fdf\uff0c\u4e3a\u65e0\u521b\u5047\u80a2\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eEEG\u7684\u5047\u80a2\u63a7\u5236\u7cfb\u7edf\u5b58\u5728\u4fe1\u53f7\u566a\u58f0\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u7b49\u95ee\u9898\uff0cBRAVE\u65e8\u5728\u901a\u8fc7\u7ed3\u5408EEG\u548c\u8bed\u97f3\u63a7\u5236\uff0c\u63d0\u4f9b\u66f4\u76f4\u89c2\u3001\u9c81\u68d2\u7684\u65e0\u521b\u5047\u80a2\u63a7\u5236\u65b9\u6848\u3002", "method": "BRAVE\u91c7\u7528LSTM\u3001CNN\u548c\u968f\u673a\u68ee\u6797\u7684\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u5904\u7406EEG\u4fe1\u53f7\uff0c\u7ed3\u5408\u5e26\u901a\u6ee4\u6ce2\u3001\u72ec\u7acb\u6210\u5206\u5206\u6790\uff08ICA\uff09\u548c\u5171\u7a7a\u95f4\u6a21\u5f0f\uff08CSP\uff09\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5e76\u96c6\u6210\u8bed\u97f3\u8bc6\u522b\u4ee5\u5b9e\u73b0\u81ea\u7531\u5ea6\u5207\u6362\u3002\u7cfb\u7edf\u901a\u8fc7Lab Streaming Layer\uff08LSL\uff09\u5b9e\u73b0\u5b9e\u65f6\u540c\u6b65\u6570\u636e\u91c7\u96c6\u3002", "result": "BRAVE\u5728\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8696%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u54cd\u5e94\u5ef6\u8fdf\u4e3a150\u6beb\u79d2\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7528\u6237\uff0c\u5e76\u5728\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "BRAVE\u4e3a\u975e\u4fb5\u5165\u5f0f\u5047\u80a2\u63a7\u5236\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u5b9e\u65f6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "paper_title_zh": "BRAVE\uff1a\u7ed3\u5408\u8bed\u97f3\u96c6\u6210\u4e0e\u5177\u8eab\u5b66\u4e60\u7684\u8111\u63a7\u5047\u80a2\u81c2\u4ee5\u589e\u5f3a\u79fb\u52a8\u6027", "abstract_zh": "\u975e\u4fb5\u5165\u5f0f\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u6709\u671b\u4e3a\u4e0a\u80a2\u622a\u80a2\u8005\u63d0\u4f9b\u76f4\u89c2\u7684\u5047\u80a2\u63a7\u5236\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u4e8eEEG\u7684\u63a7\u5236\u7cfb\u7edf\u9762\u4e34\u4fe1\u53f7\u566a\u58f0\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51faBRAVE\uff0c\u4e00\u79cd\u7ed3\u5408EEG\u548c\u8bed\u97f3\u63a7\u5236\u7684\u6df7\u5408\u5047\u80a2\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u548c\u4eba\u5728\u56de\u8def\uff08HITL\uff09\u6821\u6b63\u673a\u5236\u63d0\u5347\u54cd\u5e94\u6027\u3002\u4e0e\u4f20\u7edf\u57fa\u4e8e\u808c\u7535\u56fe\uff08EMG\uff09\u7684\u63a7\u5236\u4e0d\u540c\uff0cBRAVE\u901a\u8fc7\u89e3\u8bfbEEG\u9a71\u52a8\u7684\u8fd0\u52a8\u610f\u56fe\u5b9e\u73b0\u63a7\u5236\uff0c\u65e0\u9700\u4f9d\u8d56\u6b8b\u4f59\u808c\u8089\u6d3b\u52a8\u3002\u4e3a\u63d0\u9ad8\u5206\u7c7b\u9c81\u68d2\u6027\uff0cBRAVE\u7ed3\u5408\u4e86LSTM\u3001CNN\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u523096%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002EEG\u4fe1\u53f7\u901a\u8fc7\u5e26\u901a\u6ee4\u6ce2\uff080.5-45 Hz\uff09\u3001\u72ec\u7acb\u6210\u5206\u5206\u6790\uff08ICA\uff09\u548c\u5171\u7a7a\u95f4\u6a21\u5f0f\uff08CSP\uff09\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4ee5\u51cf\u5c11EMG\u548c\u773c\u7535\uff08EOG\uff09\u4fe1\u53f7\u7684\u5e72\u6270\u3002\u6b64\u5916\uff0cBRAVE\u96c6\u6210\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4ee5\u5b9e\u73b0\u5047\u80a2\u81c2\u4e0d\u540c\u81ea\u7531\u5ea6\uff08DOF\uff09\u7684\u76f4\u89c2\u5207\u6362\u3002\u7cfb\u7edf\u5b9e\u65f6\u8fd0\u884c\uff0c\u54cd\u5e94\u5ef6\u8fdf\u4e3a150\u6beb\u79d2\uff0c\u5e76\u5229\u7528Lab Streaming Layer\uff08LSL\uff09\u5b9e\u73b0\u540c\u6b65\u6570\u636e\u91c7\u96c6\u3002\u7cfb\u7edf\u5728\u81ea\u5236\u5047\u80a2\u81c2\u548c\u591a\u540d\u53c2\u4e0e\u8005\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u8de8\u7528\u6237\u7684\u901a\u7528\u6027\u3002\u7cfb\u7edf\u9488\u5bf9\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u90e8\u7f72\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u786e\u4fdd\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u5916\u7684\u5b9e\u9645\u5e94\u7528\u3002\u7ed3\u679c\u8868\u660e\uff0cBRAVE\u4e3a\u9c81\u68d2\u3001\u5b9e\u65f6\u7684\u975e\u4fb5\u5165\u5f0f\u5047\u80a2\u63a7\u5236\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2506.18751", "pdf": "https://arxiv.org/pdf/2506.18751", "abs": "https://arxiv.org/abs/2506.18751", "authors": ["Lukas Bahr", "Lucas Po\u00dfner", "Konstantin Weise", "Sophie Gr\u00f6ger", "R\u00fcdiger Daub"], "title": "Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Integrating advanced communication protocols in production has accelerated\nthe adoption of data-driven predictive quality methods, notably machine\nlearning (ML) models. However, ML models in image classification often face\nsignificant uncertainties arising from model, data, and domain shifts. These\nuncertainties lead to overconfidence in the classification model's output. To\nbetter understand these models, sensitivity analysis can help to analyze the\nrelative influence of input parameters on the output. This work investigates\nthe sensitivity of image classification models used for predictive quality. We\npropose modeling the distributional domain shifts of inputs with random\nvariables and quantifying their impact on the model's outputs using Sobol\nindices computed via generalized polynomial chaos (GPC). This approach is\nvalidated through a case study involving a welding defect classification\nproblem, utilizing a fine-tuned ResNet18 model and an emblem classification\nmodel used in BMW Group production facilities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u5728\u9884\u6d4b\u8d28\u91cf\u4e2d\u7684\u654f\u611f\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u591a\u9879\u5f0f\u6df7\u6c8c\uff08GPC\uff09\u548cSobol\u6307\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u8f93\u5165\u53c2\u6570\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u710a\u63a5\u7f3a\u9677\u5206\u7c7b\u548c\u5b9d\u9a6c\u6807\u5fd7\u5206\u7c7b\u6848\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u8d28\u91cf\u65b9\u6cd5\uff08\u5982\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff09\u5728\u751f\u4ea7\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u9762\u4e34\u6a21\u578b\u3001\u6570\u636e\u548c\u9886\u57df\u504f\u79fb\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5206\u7c7b\u7ed3\u679c\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4e3a\u4e86\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\uff0c\u654f\u611f\u6027\u5206\u6790\u6210\u4e3a\u5fc5\u8981\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cf\u5316\u8f93\u5165\u53c2\u6570\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u591a\u9879\u5f0f\u6df7\u6c8c\uff08GPC\uff09\u7684\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u53c2\u6570\u7684\u5206\u5e03\u57df\u504f\u79fb\u5efa\u6a21\u4e3a\u968f\u673a\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7Sobol\u6307\u6570\u91cf\u5316\u5176\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\u3002\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u7684ResNet18\u6a21\u578b\u548c\u5b9d\u9a6c\u6807\u5fd7\u5206\u7c7b\u6a21\u578b\uff0c\u5728\u710a\u63a5\u7f3a\u9677\u5206\u7c7b\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u91cf\u5316\u8f93\u5165\u53c2\u6570\u5bf9\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u710a\u63a5\u7f3a\u9677\u5206\u7c7b\u548c\u5b9d\u9a6c\u6807\u5fd7\u5206\u7c7b\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eGPC\u548cSobol\u6307\u6570\u7684\u654f\u611f\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u9884\u6d4b\u8d28\u91cf\u95ee\u9898\u3002", "paper_title_zh": "\u57fa\u4e8e\u5e7f\u4e49\u591a\u9879\u5f0f\u6df7\u6c8c\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u654f\u611f\u6027\u5206\u6790", "abstract_zh": "\u968f\u7740\u5148\u8fdb\u901a\u4fe1\u534f\u8bae\u5728\u751f\u4ea7\u4e2d\u7684\u96c6\u6210\uff0c\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u8d28\u91cf\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff09\u7684\u91c7\u7528\u52a0\u901f\u3002\u7136\u800c\uff0c\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e38\u9762\u4e34\u6a21\u578b\u3001\u6570\u636e\u548c\u9886\u57df\u504f\u79fb\u5e26\u6765\u7684\u663e\u8457\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5206\u7c7b\u6a21\u578b\u8f93\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\uff0c\u654f\u611f\u6027\u5206\u6790\u6709\u52a9\u4e8e\u5206\u6790\u8f93\u5165\u53c2\u6570\u5bf9\u8f93\u51fa\u7684\u76f8\u5bf9\u5f71\u54cd\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u4e8e\u9884\u6d4b\u8d28\u91cf\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u654f\u611f\u6027\u3002\u6211\u4eec\u63d0\u51fa\u5c06\u8f93\u5165\u7684\u5206\u5e03\u57df\u504f\u79fb\u5efa\u6a21\u4e3a\u968f\u673a\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u5e7f\u4e49\u591a\u9879\u5f0f\u6df7\u6c8c\uff08GPC\uff09\u8ba1\u7b97\u7684Sobol\u6307\u6570\u91cf\u5316\u5176\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u6d89\u53ca\u710a\u63a5\u7f3a\u9677\u5206\u7c7b\u95ee\u9898\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4f7f\u7528\u4e86\u5fae\u8c03\u7684ResNet18\u6a21\u578b\u548c\u5b9d\u9a6c\u96c6\u56e2\u751f\u4ea7\u8bbe\u65bd\u4e2d\u4f7f\u7528\u7684\u6807\u5fd7\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2506.18789", "pdf": "https://arxiv.org/pdf/2506.18789", "abs": "https://arxiv.org/abs/2506.18789", "authors": ["Rahul Atul Bhope", "K. R. Jayaram", "Praveen Venkateswaran", "Nalini Venkatasubramanian"], "title": "Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients without sharing raw data, yet faces significant\nchallenges in real-world settings where client data distributions evolve\ndynamically over time. This paper tackles the critical problem of covariate and\nlabel shifts in streaming FL environments, where non-stationary data\ndistributions degrade model performance and require adaptive middleware\nsolutions. We introduce ShiftEx, a shift-aware mixture of experts framework\nthat dynamically creates and trains specialized global models in response to\ndetected distribution shifts using Maximum Mean Discrepancy for covariate\nshifts. The framework employs a latent memory mechanism for expert reuse and\nimplements facility location-based optimization to jointly minimize covariate\nmismatch, expert creation costs, and label imbalance. Through theoretical\nanalysis and comprehensive experiments on benchmark datasets, we demonstrate\n5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation\ncompared to state-of-the-art FL baselines across diverse shift scenarios. The\nproposed approach offers a scalable, privacy-preserving middleware solution for\nFL systems operating in non-stationary, real-world conditions while minimizing\ncommunication and computational overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faShiftEx\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u52a8\u6001\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u534f\u53d8\u91cf\u548c\u6807\u7b7e\u504f\u79fb\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u9002\u5e94\u65f6\u95f4\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u52a8\u6001\u53d8\u5316\u7684\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u89e3\u51b3\u534f\u53d8\u91cf\u548c\u6807\u7b7e\u504f\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faShiftEx\u6846\u67b6\uff0c\u5229\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\u68c0\u6d4b\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u52a8\u6001\u521b\u5efa\u548c\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b\uff0c\u7ed3\u5408\u6f5c\u5728\u8bb0\u5fc6\u673a\u5236\u548c\u8bbe\u65bd\u9009\u5740\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cShiftEx\u5728\u591a\u79cd\u504f\u79fb\u573a\u666f\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53475.5-12.9%\uff0c\u9002\u5e94\u901f\u5ea6\u52a0\u5feb22-95%\u3002", "conclusion": "ShiftEx\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u4e2d\u95f4\u4ef6\u89e3\u51b3\u65b9\u6848\u3002", "paper_title_zh": "\u504f\u79fb\u53d1\u751f\uff1a\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60\u6301\u7eed\u9002\u5e94\u65b9\u6cd5", "abstract_zh": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u652f\u6301\u8de8\u5206\u6563\u5ba2\u6237\u7aef\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u800c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0c\u4f46\u5728\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u52a8\u6001\u6f14\u53d8\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u89e3\u51b3\u4e86\u6d41\u5f0fFL\u73af\u5883\u4e2d\u534f\u53d8\u91cf\u548c\u6807\u7b7e\u504f\u79fb\u7684\u5173\u952e\u95ee\u9898\uff0c\u5176\u4e2d\u975e\u5e73\u7a33\u6570\u636e\u5206\u5e03\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u81ea\u9002\u5e94\u4e2d\u95f4\u4ef6\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u63d0\u51fa\u4e86ShiftEx\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7684\u504f\u79fb\u611f\u77e5\u6846\u67b6\uff0c\u5229\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\u68c0\u6d4b\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u52a8\u6001\u521b\u5efa\u548c\u8bad\u7ec3\u4e13\u7528\u5168\u5c40\u6a21\u578b\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6f5c\u5728\u8bb0\u5fc6\u673a\u5236\u5b9e\u73b0\u4e13\u5bb6\u91cd\u7528\uff0c\u5e76\u901a\u8fc7\u8bbe\u65bd\u9009\u5740\u4f18\u5316\u8054\u5408\u6700\u5c0f\u5316\u534f\u53d8\u91cf\u4e0d\u5339\u914d\u3001\u4e13\u5bb6\u521b\u5efa\u6210\u672c\u548c\u6807\u7b7e\u4e0d\u5e73\u8861\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u591a\u79cd\u504f\u79fb\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684FL\u57fa\u7ebf\u65b9\u6cd5\uff0cShiftEx\u5b9e\u73b0\u4e865.5-12.9\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c22-95%\u7684\u66f4\u5feb\u9002\u5e94\u901f\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5728\u975e\u5e73\u7a33\u5b9e\u9645\u6761\u4ef6\u4e0b\u8fd0\u884c\u7684FL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u4e2d\u95f4\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2506.18824", "pdf": "https://arxiv.org/pdf/2506.18824", "abs": "https://arxiv.org/abs/2506.18824", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e09\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff08RepairAgent\u3001AutoCodeRover\u548cOpenHands\uff09\u7684\u601d\u7ef4-\u884c\u52a8-\u7ed3\u679c\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u53cd\u6a21\u5f0f\uff0c\u4e3a\u6539\u8fdb\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u5982\u7a0b\u5e8f\u4fee\u590d\u548c\u95ee\u9898\u89e3\u51b3\uff09\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u4ecd\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u64cd\u4f5c\u52a8\u6001\u548c\u5931\u8d25\u6a21\u5f0f\u7684\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u7edf\u4e00\u4e86\u4e09\u79cd\u4ee3\u7406\u7684\u4ea4\u4e92\u65e5\u5fd7\uff08\u5171120\u6761\u8f68\u8ff9\u548c2822\u6b21LLM\u4ea4\u4e92\uff09\uff0c\u7ed3\u5408\u5b9a\u91cf\u5206\u6790\uff08\u7ed3\u6784\u7279\u6027\u3001\u884c\u52a8\u6a21\u5f0f\u548c\u4ee4\u724c\u4f7f\u7528\uff09\u548c\u5b9a\u6027\u8bc4\u4f30\uff08\u63a8\u7406\u4e00\u81f4\u6027\u548c\u53cd\u9988\u6574\u5408\uff09\uff0c\u63ed\u793a\u4e86\u8f68\u8ff9\u7279\u5f81\u548c\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6210\u529f\u4e0e\u5931\u8d25\u6267\u884c\u7684\u5173\u952e\u533a\u522b\u5728\u4e8e\u8fed\u4ee3\u6b21\u6570\u3001\u4ee4\u724c\u6d88\u8017\u3001\u91cd\u590d\u884c\u52a8\u5e8f\u5217\u4ee5\u53ca\u601d\u7ef4\u3001\u884c\u52a8\u4e0e\u7ed3\u679c\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u8bc6\u522b\u4e86\u884c\u4e3a\u6a21\u5f0f\u548c\u53cd\u6a21\u5f0f\uff0c\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u6539\u8fdb\u63d0\u4f9b\u4e86\u5177\u4f53\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u4ee3\u7406\u7684\u884c\u4e3a\u7279\u5f81\u548c\u5931\u8d25\u539f\u56e0\uff0c\u4e3a\u4f18\u5316\u4ee3\u7406\u8bbe\u8ba1\uff08\u5982\u63d0\u793a\u7b56\u7565\u3001\u5931\u8d25\u8bca\u65ad\u548c\u53cd\u6a21\u5f0f\u68c0\u6d4b\uff09\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u6807\u6ce8\u6846\u67b6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "paper_title_zh": "\u7406\u89e3\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff1a\u601d\u7ef4-\u884c\u52a8-\u7ed3\u679c\u8f68\u8ff9\u7814\u7a76", "abstract_zh": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u81ea\u52a8\u5316\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u5982\u7a0b\u5e8f\u4fee\u590d\u548c\u95ee\u9898\u89e3\u51b3\uff09\u3002\u8fd9\u4e9b\u4ee3\u7406\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u601d\u7ef4\u3001\u8c03\u7528\u5916\u90e8\u5de5\u5177\u5e76\u8fed\u4ee3\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u6765\u8fd0\u4f5c\u3002\u5c3d\u7ba1\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u8fd9\u4e9b\u4ee3\u7406\u7684\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u64cd\u4f5c\u52a8\u6001\u548c\u5931\u8d25\u6a21\u5f0f\u7684\u7406\u89e3\u3002\u672c\u6587\u5bf9\u4e09\u79cd\u5148\u8fdbLLM\u4ee3\u7406\uff08RepairAgent\u3001AutoCodeRover\u548cOpenHands\uff09\u7684\u601d\u7ef4-\u884c\u52a8-\u7ed3\u679c\u8f68\u8ff9\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u3002\u6211\u4eec\u5c06\u5176\u4ea4\u4e92\u65e5\u5fd7\u7edf\u4e00\u4e3a\u901a\u7528\u683c\u5f0f\uff0c\u6355\u83b7\u4e86120\u6761\u8f68\u8ff9\u548c2822\u6b21\u4e13\u6ce8\u4e8e\u7a0b\u5e8f\u4fee\u590d\u548c\u95ee\u9898\u89e3\u51b3\u7684LLM\u4ea4\u4e92\u3002\u7814\u7a76\u7ed3\u5408\u4e86\u7ed3\u6784\u7279\u6027\u3001\u884c\u52a8\u6a21\u5f0f\u548c\u4ee4\u724c\u4f7f\u7528\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u4ee5\u53ca\u63a8\u7406\u4e00\u81f4\u6027\u548c\u53cd\u9988\u6574\u5408\u7684\u5b9a\u6027\u8bc4\u4f30\u3002\u6211\u4eec\u8bc6\u522b\u4e86\u5173\u952e\u8f68\u8ff9\u7279\u5f81\uff08\u5982\u8fed\u4ee3\u6b21\u6570\u548c\u4ee4\u724c\u6d88\u8017\uff09\u3001\u91cd\u590d\u884c\u52a8\u5e8f\u5217\u4ee5\u53ca\u601d\u7ef4\u3001\u884c\u52a8\u4e0e\u7ed3\u679c\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u7814\u7a76\u53d1\u73b0\u63ed\u793a\u4e86\u533a\u5206\u6210\u529f\u4e0e\u5931\u8d25\u6267\u884c\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u53cd\u6a21\u5f0f\uff0c\u4e3a\u6539\u8fdb\u4ee3\u7406\u8bbe\u8ba1\uff08\u5305\u62ec\u63d0\u793a\u7b56\u7565\u3001\u5931\u8d25\u8bca\u65ad\u548c\u53cd\u6a21\u5f0f\u68c0\u6d4b\uff09\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002\u6211\u4eec\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u6807\u6ce8\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u5bf9\u900f\u660e\u4e14\u9c81\u68d2\u7684\u81ea\u6d3d\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u7814\u7a76\u3002"}}
{"id": "2506.18897", "pdf": "https://arxiv.org/pdf/2506.18897", "abs": "https://arxiv.org/abs/2506.18897", "authors": ["Xiaowei Chi", "Kuangzhi Ge", "Jiaming Liu", "Siyuan Zhou", "Peidong Jia", "Zichen He", "Yuzhen Liu", "Tingguang Li", "Lei Han", "Sirui Han", "Shanghang Zhang", "Yike Guo"], "title": "MinD: Unified Visual Imagination and Control via Hierarchical World Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Video generation models (VGMs) offer a promising pathway for unified world\nmodeling in robotics by integrating simulation, prediction, and manipulation.\nHowever, their practical application remains limited due to (1) slowgeneration\nspeed, which limits real-time interaction, and (2) poor consistency between\nimagined videos and executable actions. To address these challenges, we propose\nManipulate in Dream (MinD), a hierarchical diffusion-based world model\nframework that employs a dual-system design for vision-language manipulation.\nMinD executes VGM at low frequencies to extract video prediction features,\nwhile leveraging a high-frequency diffusion policy for real-time interaction.\nThis architecture enables low-latency, closed-loop control in manipulation with\ncoherent visual guidance. To better coordinate the two systems, we introduce a\nvideo-action diffusion matching module (DiffMatcher), with a novel co-training\nstrategy that uses separate schedulers for each diffusion model. Specifically,\nwe introduce a diffusion-forcing mechanism to DiffMatcher that aligns their\nintermediate representations during training, helping the fast action model\nbetter understand video-based predictions. Beyond manipulation, MinD also\nfunctions as a world simulator, reliably predicting task success or failure in\nlatent space before execution. Trustworthy analysis further shows that VGMs can\npreemptively evaluate task feasibility and mitigate risks. Extensive\nexperiments across multiple benchmarks demonstrate that MinD achieves\nstate-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of\nunified world modeling in robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMinD\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6269\u6563\u6a21\u578b\u89e3\u51b3\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u751f\u6210\u901f\u5ea6\u6162\u548c\u89c6\u9891\u4e0e\u52a8\u4f5c\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u4e0e\u89c6\u89c9\u5f15\u5bfc\u7684\u95ed\u73af\u63a7\u5236\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08VGMs\uff09\u5728\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u7684\u6f5c\u529b\uff0c\u4f46\u56e0\u751f\u6210\u901f\u5ea6\u6162\u548c\u89c6\u9891\u4e0e\u52a8\u4f5c\u4e00\u81f4\u6027\u5dee\uff0c\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u3002MinD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u65f6\u4ea4\u4e92\u4e0e\u4efb\u52a1\u9884\u6d4b\u80fd\u529b\u3002", "method": "MinD\u91c7\u7528\u5206\u5c42\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4f4e\u9891\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u53d6\u9884\u6d4b\u7279\u5f81\uff0c\u9ad8\u9891\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u3002\u5f15\u5165DiffMatcher\u6a21\u5757\u548c\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u6563\u5f3a\u5236\u673a\u5236\u5bf9\u9f50\u4e2d\u95f4\u8868\u793a\uff0c\u4f18\u5316\u52a8\u4f5c\u6a21\u578b\u5bf9\u89c6\u9891\u9884\u6d4b\u7684\u7406\u89e3\u3002", "result": "MinD\u5728RL-Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4efb\u52a1\u6210\u529f\u7387\u8d85\u8fc763%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u4e2d\u7684\u5148\u8fdb\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "MinD\u4e0d\u4ec5\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b9e\u65f6\u6027\u548c\u4e00\u81f4\u6027\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u9884\u6d4b\u4efb\u52a1\u53ef\u884c\u6027\uff0c\u4e3a\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "paper_title_zh": "MinD\uff1a\u57fa\u4e8e\u5206\u5c42\u4e16\u754c\u6a21\u578b\u7684\u7edf\u4e00\u89c6\u89c9\u60f3\u8c61\u4e0e\u63a7\u5236", "abstract_zh": "\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08VGMs\uff09\u4e3a\u673a\u5668\u4eba\u9886\u57df\u7684\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u4e8e\uff081\uff09\u751f\u6210\u901f\u5ea6\u6162\uff0c\u5f71\u54cd\u5b9e\u65f6\u4ea4\u4e92\uff1b\uff082\uff09\u60f3\u8c61\u89c6\u9891\u4e0e\u53ef\u6267\u884c\u52a8\u4f5c\u7684\u4e00\u81f4\u6027\u5dee\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51faManipulate in Dream\uff08MinD\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u7cfb\u7edf\u8bbe\u8ba1\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u64cd\u4f5c\u3002MinD\u901a\u8fc7\u4f4e\u9891\u6267\u884cVGM\u63d0\u53d6\u89c6\u9891\u9884\u6d4b\u7279\u5f81\uff0c\u540c\u65f6\u5229\u7528\u9ad8\u9891\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u3002\u8be5\u67b6\u6784\u652f\u6301\u4f4e\u5ef6\u8fdf\u7684\u95ed\u73af\u63a7\u5236\u4e0e\u8fde\u8d2f\u7684\u89c6\u89c9\u5f15\u5bfc\u3002\u4e3a\u534f\u8c03\u4e24\u7cfb\u7edf\uff0c\u6211\u4eec\u5f15\u5165\u89c6\u9891-\u52a8\u4f5c\u6269\u6563\u5339\u914d\u6a21\u5757\uff08DiffMatcher\uff09\uff0c\u91c7\u7528\u65b0\u9896\u7684\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a\u6bcf\u4e2a\u6269\u6563\u6a21\u578b\u5206\u914d\u72ec\u7acb\u8c03\u5ea6\u5668\u3002\u5177\u4f53\u800c\u8a00\uff0cDiffMatcher\u901a\u8fc7\u6269\u6563\u5f3a\u5236\u673a\u5236\u5728\u8bad\u7ec3\u4e2d\u5bf9\u9f50\u4e2d\u95f4\u8868\u793a\uff0c\u5e2e\u52a9\u5feb\u901f\u52a8\u4f5c\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u89c6\u9891\u9884\u6d4b\u3002MinD\u8fd8\u53ef\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\uff0c\u5728\u6267\u884c\u524d\u4e8e\u6f5c\u5728\u7a7a\u95f4\u53ef\u9760\u9884\u6d4b\u4efb\u52a1\u6210\u8d25\u3002\u53ef\u4fe1\u5206\u6790\u8fdb\u4e00\u6b65\u8868\u660e\uff0cVGMs\u80fd\u9884\u5148\u8bc4\u4f30\u4efb\u52a1\u53ef\u884c\u6027\u5e76\u964d\u4f4e\u98ce\u9669\u3002\u591a\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cMinD\u5728RL-Bench\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u8d85\u8fc763%\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u7684\u524d\u6cbf\u3002"}}
