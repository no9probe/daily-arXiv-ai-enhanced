{"id": "2506.08120", "pdf": "https://arxiv.org/pdf/2506.08120", "abs": "https://arxiv.org/abs/2506.08120", "authors": ["Toyin Aguda", "Erik Wilson", "Allan Anzagira", "Simerjot Kaur", "Charese Smiley"], "title": "Conservative Bias in Large Language Models: Measuring Relation Predictions", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Large language models (LLMs) exhibit pronounced conservative bias in relation\nextraction tasks, frequently defaulting to No_Relation label when an\nappropriate option is unavailable. While this behavior helps prevent incorrect\nrelation assignments, our analysis reveals that it also leads to significant\ninformation loss when reasoning is not explicitly included in the output. We\nsystematically evaluate this trade-off across multiple prompts, datasets, and\nrelation types, introducing the concept of Hobson's choice to capture scenarios\nwhere models opt for safe but uninformative labels over hallucinated ones. Our\nfindings suggest that conservative bias occurs twice as often as hallucination.\nTo quantify this effect, we use SBERT and LLM prompts to capture the semantic\nsimilarity between conservative bias behaviors in constrained prompts and\nlabels generated from semi-constrained and open-ended prompts.", "AI": {"tldr": "LLMs\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4fdd\u5b88\u504f\u5dee\uff0c\u503e\u5411\u4e8e\u9009\u62e9\u65e0\u5173\u7cfb\u6807\u7b7e\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002\u7814\u7a76\u53d1\u73b0\u4fdd\u5b88\u504f\u5dee\u6bd4\u5e7b\u89c9\u66f4\u5e38\u89c1\uff0c\u5e76\u901a\u8fc7SBERT\u548cLLM\u63d0\u793a\u91cf\u5316\u5176\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u5173\u7cfb\u62bd\u53d6\u4e2d\u7684\u4fdd\u5b88\u504f\u5dee\u884c\u4e3a\u53ca\u5176\u5bf9\u4fe1\u606f\u63d0\u53d6\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u63d0\u793a\u3001\u6570\u636e\u96c6\u548c\u5173\u7cfb\u7c7b\u578b\u7cfb\u7edf\u8bc4\u4f30\u4fdd\u5b88\u504f\u5dee\uff0c\u5f15\u5165Hobson's choice\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7SBERT\u548cLLM\u63d0\u793a\u91cf\u5316\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u4fdd\u5b88\u504f\u5dee\u7684\u53d1\u751f\u9891\u7387\u662f\u5e7b\u89c9\u7684\u4e24\u500d\uff0c\u4e14\u4fdd\u5b88\u884c\u4e3a\u5728\u7ea6\u675f\u63d0\u793a\u4e0b\u4e0e\u534a\u7ea6\u675f\u548c\u5f00\u653e\u5f0f\u63d0\u793a\u751f\u6210\u7684\u6807\u7b7e\u6709\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "conclusion": "LLMs\u7684\u4fdd\u5b88\u504f\u5dee\u867d\u907f\u514d\u9519\u8bef\u5173\u7cfb\u5206\u914d\uff0c\u4f46\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u884c\u4e3a\u3002"}}
{"id": "2506.08123", "pdf": "https://arxiv.org/pdf/2506.08123", "abs": "https://arxiv.org/abs/2506.08123", "authors": ["Jacob Dineen", "Aswin RRV", "Qin Liu", "Zhikun Xu", "Xiao Ye", "Ming Shen", "Zhaonan Li", "Shijie Lu", "Chitta Baral", "Muhao Chen", "Ben Zhou"], "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA", "categories": ["cs.CL"], "comment": null, "summary": "Alignment of large language models with explicit principles (such as\nhelpfulness, honesty, and harmlessness) is crucial for ensuring safe and\nreliable AI systems. However, standard reward-based alignment methods typically\ncollapse diverse feedback into a single scalar reward, entangling multiple\nobjectives into one opaque training signal, which hinders interpretability. In\nthis work, we introduce QA-LIGN, an automatic symbolic reward decomposition\napproach that preserves the structure of each constitutional principle within\nthe reward mechanism. Instead of training a black-box reward model that outputs\na monolithic score, QA-LIGN formulates principle-specific evaluation questions\nand derives separate reward components for each principle, making it a drop-in\nreward model replacement. Experiments aligning an uncensored large language\nmodel with a set of constitutional principles demonstrate that QA-LIGN offers\ngreater transparency and adaptability in the alignment process. At the same\ntime, our approach achieves performance on par with or better than a DPO\nbaseline. Overall, these results represent a step toward more interpretable and\ncontrollable alignment of language models, achieved without sacrificing\nend-task performance.", "AI": {"tldr": "QA-LIGN\u662f\u4e00\u79cd\u81ea\u52a8\u7b26\u53f7\u5956\u52b1\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5956\u52b1\u673a\u5236\u4e2d\u7684\u539f\u5219\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u900f\u660e\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u6807\u51c6\u5956\u52b1\u5bf9\u9f50\u65b9\u6cd5\u5c06\u591a\u6837\u53cd\u9988\u538b\u7f29\u4e3a\u5355\u4e00\u6807\u91cf\u5956\u52b1\uff0c\u5bfc\u81f4\u76ee\u6807\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u89e3\u91ca\u3002", "method": "QA-LIGN\u4e3a\u6bcf\u4e2a\u539f\u5219\u8bbe\u8ba1\u7279\u5b9a\u8bc4\u4f30\u95ee\u9898\uff0c\u751f\u6210\u72ec\u7acb\u5956\u52b1\u7ec4\u4ef6\uff0c\u66ff\u4ee3\u4f20\u7edf\u9ed1\u76d2\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aQA-LIGN\u5728\u900f\u660e\u5ea6\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6027\u80fd\u4e0eDPO\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "QA-LIGN\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u65b9\u6cd5\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.08136", "pdf": "https://arxiv.org/pdf/2506.08136", "abs": "https://arxiv.org/abs/2506.08136", "authors": ["Zefang Liu", "Yinzhu Quan"], "title": "EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments", "categories": ["cs.CL"], "comment": null, "summary": "We introduce EconWebArena, a benchmark for evaluating autonomous agents on\ncomplex, multimodal economic tasks in realistic web environments. The benchmark\ncomprises 360 curated tasks from 82 authoritative websites spanning domains\nsuch as macroeconomics, labor, finance, trade, and public policy. Each task\nchallenges agents to navigate live websites, interpret structured and visual\ncontent, interact with real interfaces, and extract precise, time-sensitive\ndata through multi-step workflows. We construct the benchmark by prompting\nmultiple large language models (LLMs) to generate candidate tasks, followed by\nrigorous human curation to ensure clarity, feasibility, and source reliability.\nUnlike prior work, EconWebArena emphasizes fidelity to authoritative data\nsources and the need for grounded web-based economic reasoning. We evaluate a\ndiverse set of state-of-the-art multimodal LLMs as web agents, analyze failure\ncases, and conduct ablation studies to assess the impact of visual grounding,\nplan-based reasoning, and interaction design. Our results reveal substantial\nperformance gaps and highlight persistent challenges in grounding, navigation,\nand multimodal understanding, positioning EconWebArena as a rigorous testbed\nfor economic web intelligence.", "AI": {"tldr": "EconWebArena\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u4e3b\u4ee3\u7406\u5728\u590d\u6742\u3001\u591a\u6a21\u6001\u7ecf\u6d4e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b360\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6\u591a\u4e2a\u7ecf\u6d4e\u9886\u57df\uff0c\u5f3a\u8c03\u6743\u5a01\u6570\u636e\u6e90\u548c\u57fa\u4e8e\u7f51\u7edc\u7684\u7ecf\u6d4e\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u6743\u5a01\u6570\u636e\u6e90\u548c\u591a\u6a21\u6001\u7ecf\u6d4e\u4efb\u52a1\u7684\u5173\u6ce8\uff0cEconWebArena\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5019\u9009\u4efb\u52a1\uff0c\u7ecf\u4eba\u5de5\u7b5b\u9009\u786e\u4fdd\u6e05\u6670\u6027\u3001\u53ef\u884c\u6027\u548c\u6570\u636e\u53ef\u9760\u6027\uff0c\u8bc4\u4f30\u591a\u6a21\u6001LLM\u5728\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001LLM\u5728\u6570\u636e\u57fa\u7840\u3001\u5bfc\u822a\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "EconWebArena\u4e3a\u7ecf\u6d4e\u7f51\u7edc\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.08147", "pdf": "https://arxiv.org/pdf/2506.08147", "abs": "https://arxiv.org/abs/2506.08147", "authors": ["Muhammad Usman", "Muhammad Ahmad", "M. Shahiki Tash", "Irina Gelbukh", "Rolando Quintero Tellez", "Grigori Sidorov"], "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Social media platforms are critical spaces for public discourse, shaping\nopinions and community dynamics, yet their widespread use has amplified harmful\ncontent, particularly hate speech, threatening online safety and inclusivity.\nWhile hate speech detection has been extensively studied in languages like\nEnglish and Spanish, Urdu remains underexplored, especially using\ntranslation-based approaches. To address this gap, we introduce a trilingual\ndataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and\nSpanish (3,162 samples), collected via keyword filtering, with a balanced\ndistribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology\nleverages attention layers as a precursor to transformer-based models and large\nlanguage models (LLMs), enhancing feature extraction for multilingual hate\nspeech detection. For non-transformer models, we use TF-IDF for feature\nextraction. The dataset is benchmarked using state-of-the-art models, including\nGPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models\nlike SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,\nfollowing rigorous guidelines, ensured high dataset quality, achieving a\nFleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5\nTurbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of\n0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for\nUrdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).\nThese results reflect improvements of 8.75% in English (over SVM baseline\n0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM\nbaseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline\n0.82). Our framework offers a robust solution for multilingual hate speech\ndetection, fostering safer digital communities worldwide.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u5c42\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9488\u5bf9\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u4ec7\u6068\u8a00\u8bba\u5a01\u80c1\u5728\u7ebf\u5b89\u5168\u548c\u5305\u5bb9\u6027\uff0c\u4f46\u4e4c\u5c14\u90fd\u8bed\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7814\u7a76\u8f83\u5c11\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u7ffb\u8bd1\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u5c42\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408GPT-3.5 Turbo\u548cQwen 2.5 72B\u7b49\u6a21\u578b\uff0c\u540c\u65f6\u91c7\u7528TF-IDF\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u4e0a\u5206\u522b\u8fbe\u52300.87\u30010.85\u548c0.81\u7684\u5b8fF1\u5206\u6570\uff0c\u591a\u8bed\u8a00\u6a21\u578b\u8fbe\u52300.88\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u7684\u6570\u5b57\u793e\u533a\u3002"}}
{"id": "2506.08048", "pdf": "https://arxiv.org/pdf/2506.08048", "abs": "https://arxiv.org/abs/2506.08048", "authors": ["Zheng Han", "Jun Zhou", "Jialun Pei", "Jing Qin", "Yingfang Fan", "Qi Dou"], "title": "Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "In augmented reality (AR)-guided surgical navigation, preoperative organ\nmodels are superimposed onto the patient's intraoperative anatomy to visualize\ncritical structures such as vessels and tumors. Accurate deformation modeling\nis essential to maintain the reliability of AR overlays by ensuring alignment\nbetween preoperative models and the dynamically changing anatomy. Although the\nfinite element method (FEM) offers physically plausible modeling, its high\ncomputational cost limits intraoperative applicability. Moreover, existing\nalgorithms often fail to handle large anatomical changes, such as those induced\nby pneumoperitoneum or ligament dissection, leading to inaccurate anatomical\ncorrespondences and compromised AR guidance. To address these challenges, we\npropose a data-driven biomechanics algorithm that preserves FEM-level accuracy\nwhile improving computational efficiency. In addition, we introduce a novel\nhuman-in-the-loop mechanism into the deformation modeling process. This enables\nsurgeons to interactively provide prompts to correct anatomical misalignments,\nthereby incorporating clinical expertise and allowing the model to adapt\ndynamically to complex surgical scenarios. Experiments on a publicly available\ndataset demonstrate that our algorithm achieves a mean target registration\nerror of 3.42 mm. Incorporating surgeon prompts through the interactive\nframework further reduces the error to 2.78 mm, surpassing state-of-the-art\nmethods in volumetric accuracy. These results highlight the ability of our\nframework to deliver efficient and accurate deformation modeling while\nenhancing surgeon-algorithm collaboration, paving the way for safer and more\nreliable computer-assisted surgeries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u751f\u7269\u529b\u5b66\u7b97\u6cd5\uff0c\u7ed3\u5408\u4eba\u673a\u4ea4\u4e92\u673a\u5236\uff0c\u63d0\u9ad8\u4e86AR\u624b\u672f\u5bfc\u822a\u4e2d\u53d8\u5f62\u5efa\u6a21\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5e94\u5bf9\u5927\u89e3\u5256\u53d8\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cdAR\u624b\u672f\u5bfc\u822a\u7684\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u751f\u7269\u529b\u5b66\u7b97\u6cd5\u548c\u5916\u79d1\u533b\u751f\u4ea4\u4e92\u63d0\u793a\uff0c\u52a8\u6001\u8c03\u6574\u6a21\u578b\u3002", "result": "\u7b97\u6cd5\u5e73\u5747\u76ee\u6807\u914d\u51c6\u8bef\u5dee\u4e3a3.42 mm\uff0c\u4ea4\u4e92\u63d0\u793a\u540e\u964d\u81f32.78 mm\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u53d8\u5f62\u5efa\u6a21\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u4e86\u5916\u79d1\u533b\u751f\u4e0e\u7b97\u6cd5\u7684\u534f\u4f5c\uff0c\u4e3a\u66f4\u5b89\u5168\u53ef\u9760\u7684\u624b\u672f\u5bfc\u822a\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.08026", "pdf": "https://arxiv.org/pdf/2506.08026", "abs": "https://arxiv.org/abs/2506.08026", "authors": ["Xibai Wang"], "title": "TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-fin.CP"], "comment": null, "summary": "This paper proposes TIP-Search, a time-predictable inference scheduling\nframework for real-time market prediction under uncertain workloads. Motivated\nby the strict latency demands in high-frequency financial systems, TIP-Search\ndynamically selects a deep learning model from a heterogeneous pool, aiming to\nmaximize predictive accuracy while satisfying per-task deadline constraints.\nOur approach profiles latency and generalization performance offline, then\nperforms online task-aware selection without relying on explicit input domain\nlabels. We evaluate TIP-Search on three real-world limit order book datasets\n(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms\nstatic baselines with up to 8.5% improvement in accuracy and 100% deadline\nsatisfaction. Our results highlight the effectiveness of TIP-Search in robust\nlow-latency financial inference under uncertainty.", "AI": {"tldr": "TIP-Search\u662f\u4e00\u79cd\u5b9e\u65f6\u5e02\u573a\u9884\u6d4b\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u6ee1\u8db3\u4e25\u683c\u5ef6\u8fdf\u9700\u6c42\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u9ad8\u9891\u91d1\u878d\u7cfb\u7edf\u5bf9\u5ef6\u8fdf\u8981\u6c42\u4e25\u683c\uff0c\u9700\u5728\u4e0d\u786e\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9884\u6d4b\u3002", "method": "\u79bb\u7ebf\u5206\u6790\u5ef6\u8fdf\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u7ebf\u52a8\u6001\u9009\u62e9\u6a21\u578b\uff0c\u65e0\u9700\u663e\u5f0f\u8f93\u5165\u6807\u7b7e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\uff0c\u7cbe\u5ea6\u63d0\u53478.5%\uff0c100%\u6ee1\u8db3\u622a\u6b62\u65f6\u95f4\u3002", "conclusion": "TIP-Search\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u91d1\u878d\u9884\u6d4b\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2506.08158", "pdf": "https://arxiv.org/pdf/2506.08158", "abs": "https://arxiv.org/abs/2506.08158", "authors": ["Lijing Zhu", "Qizhen Lan", "Qing Tian", "Wenbo Sun", "Li Yang", "Lu Xia", "Yixin Xie", "Xi Xiao", "Tiehang Duan", "Cui Tao", "Shuteng Niu"], "title": "ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding", "categories": ["cs.CL"], "comment": null, "summary": "Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge\nwhile preserving past information. However, existing methods struggle with\nefficiency and scalability due to two key limitations: (1) suboptimal knowledge\npreservation between snapshots caused by manually designed node/relation\nimportance scores that ignore graph dependencies relevant to the downstream\ntask, and (2) computationally expensive graph traversal for node/relation\nimportance calculation, leading to slow training and high memory overhead. To\naddress these limitations, we introduce ETT-CKGE (Efficient, Task-driven,\nTokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE\nmethod that leverages efficient task-driven tokens for efficient and effective\nknowledge transfer between snapshots. Our method introduces a set of learnable\ntokens that directly capture task-relevant signals, eliminating the need for\nexplicit node scoring or traversal. These tokens serve as consistent and\nreusable guidance across snapshots, enabling efficient token-masked embedding\nalignment between snapshots. Importantly, knowledge transfer is achieved\nthrough simple matrix operations, significantly reducing training time and\nmemory usage. Extensive experiments across six benchmark datasets demonstrate\nthat ETT-CKGE consistently achieves superior or competitive predictive\nperformance, while substantially improving training efficiency and scalability\ncompared to state-of-the-art CKGE methods. The code is available at:\nhttps://github.com/lijingzhu1/ETT-CKGE/tree/main", "AI": {"tldr": "ETT-CKGE\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4efb\u52a1\u9a71\u52a8\u7684\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u76f8\u5173\u4ee4\u724c\u53d6\u4ee3\u4f20\u7edf\u8282\u70b9\u8bc4\u5206\u548c\u56fe\u904d\u5386\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u4fdd\u7559\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u7684\u6301\u7eed\u66f4\u65b0\u3002", "method": "ETT-CKGE\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u9a71\u52a8\u4ee4\u724c\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u77e9\u9635\u64cd\u4f5c\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u8282\u70b9\u8bc4\u5206\u548c\u56fe\u904d\u5386\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cETT-CKGE\u8868\u73b0\u51fa\u4f18\u8d8a\u6216\u7ade\u4e89\u6027\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ETT-CKGE\u4e3a\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u7684\u52a8\u6001\u66f4\u65b0\u3002"}}
{"id": "2506.08052", "pdf": "https://arxiv.org/pdf/2506.08052", "abs": "https://arxiv.org/abs/2506.08052", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS.", "AI": {"tldr": "ReCogDrive\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u89c4\u5212\u5668\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u65b0SOTA\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u548c\u957f\u5c3e\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a1) \u4f7f\u7528\u9a7e\u9a76\u95ee\u7b54\u6570\u636e\u96c6\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff1b2) \u6269\u6563\u89c4\u5212\u5668\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff1b3) \u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6269\u6563\u89c4\u5212\u5668\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523089.6 PDMS\uff0c\u8d85\u8d8a\u4e4b\u524d\u89c6\u89c9SOTA 5.6 PDMS\u3002", "conclusion": "ReCogDrive\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.08098", "pdf": "https://arxiv.org/pdf/2506.08098", "abs": "https://arxiv.org/abs/2506.08098", "authors": ["Akash Vishwakarma", "Hojin Lee", "Mohith Suresh", "Priyam Shankar Sharma", "Rahul Vishwakarma", "Sparsh Gupta", "Yuvraj Anupam Chauhan"], "title": "Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of capable large language model (LLM) based agents necessitates\nmemory architectures that transcend mere data storage, enabling continuous\nlearning, nuanced reasoning, and dynamic adaptation. Current memory systems\noften grapple with fundamental limitations in structural flexibility, temporal\nawareness, and the ability to synthesize higher-level insights from raw\ninteraction data. This paper introduces Cognitive Weave, a novel memory\nframework centered around a multi-layered spatio-temporal resonance graph\n(STRG). This graph manages information as semantically rich insight particles\n(IPs), which are dynamically enriched with resonance keys, signifiers, and\nsituational imprints via a dedicated semantic oracle interface (SOI). These IPs\nare interconnected through typed relational strands, forming an evolving\nknowledge tapestry. A key component of Cognitive Weave is the cognitive\nrefinement process, an autonomous mechanism that includes the synthesis of\ninsight aggregates (IAs) condensed, higher-level knowledge structures derived\nfrom identified clusters of related IPs. We present comprehensive experimental\nresults demonstrating Cognitive Weave's marked enhancement over existing\napproaches in long-horizon planning tasks, evolving question-answering\nscenarios, and multi-session dialogue coherence. The system achieves a notable\n34% average improvement in task completion rates and a 42% reduction in mean\nquery latency when compared to state-of-the-art baselines. Furthermore, this\npaper explores the ethical considerations inherent in such advanced memory\nsystems, discusses the implications for long-term memory in LLMs, and outlines\npromising future research trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCognitive Weave\u7684\u65b0\u578b\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u65f6\u7a7a\u5171\u632f\u56fe\uff08STRG\uff09\u7ba1\u7406\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u67e5\u8be2\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ed3\u6784\u7075\u6d3b\u6027\u3001\u65f6\u95f4\u611f\u77e5\u548c\u4ece\u539f\u59cb\u6570\u636e\u4e2d\u63d0\u53d6\u9ad8\u5c42\u6b21\u6d1e\u5bdf\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u8bb0\u5fc6\u67b6\u6784\u6765\u652f\u6301\u6301\u7eed\u5b66\u4e60\u548c\u52a8\u6001\u9002\u5e94\u3002", "method": "\u91c7\u7528\u591a\u5c42\u6b21\u7684\u65f6\u7a7a\u5171\u632f\u56fe\uff08STRG\uff09\u7ba1\u7406\u8bed\u4e49\u4e30\u5bcc\u7684\u6d1e\u5bdf\u7c92\u5b50\uff08IPs\uff09\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u9884\u8a00\u63a5\u53e3\uff08SOI\uff09\u52a8\u6001\u4e30\u5bccIPs\u3002\u901a\u8fc7\u8ba4\u77e5\u7cbe\u70bc\u8fc7\u7a0b\u5408\u6210\u66f4\u9ad8\u5c42\u6b21\u7684\u77e5\u8bc6\u7ed3\u6784\uff08IAs\uff09\u3002", "result": "Cognitive Weave\u5728\u957f\u671f\u89c4\u5212\u4efb\u52a1\u3001\u52a8\u6001\u95ee\u7b54\u573a\u666f\u548c\u591a\u4f1a\u8bdd\u5bf9\u8bdd\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u5e73\u5747\u63d0\u534734%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u964d\u4f4e42%\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86Cognitive Weave\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4f26\u7406\u610f\u4e49\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.08172", "pdf": "https://arxiv.org/pdf/2506.08172", "abs": "https://arxiv.org/abs/2506.08172", "authors": ["Gerardo Aleman Manzanarez", "Nora de la Cruz Arana", "Jorge Garcia Flores", "Yobany Garcia Medina", "Raul Monroy", "Nathalie Pernelle"], "title": "Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction", "categories": ["cs.CL"], "comment": "28 pages, 16 figures. Submitted to Applied Sciences", "summary": "Automated story writing has been a subject of study for over 60 years. Large\nlanguage models can generate narratively consistent and linguistically coherent\nshort fiction texts. Despite these advancements, rigorous assessment of such\noutputs for literary merit - especially concerning aesthetic qualities - has\nreceived scant attention. In this paper, we address the challenge of evaluating\nAI-generated microfictions and argue that this task requires consideration of\nliterary criteria across various aspects of the text, such as thematic\ncoherence, textual clarity, interpretive depth, and aesthetic quality. To\nfacilitate this, we present GrAImes: an evaluation protocol grounded in\nliterary theory, specifically drawing from a literary perspective, to offer an\nobjective framework for assessing AI-generated microfiction. Furthermore, we\nreport the results of our validation of the evaluation protocol, as answered by\nboth literature experts and literary enthusiasts. This protocol will serve as a\nfoundation for evaluating automatically generated microfictions and assessing\ntheir literary value.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u5b66\u7406\u8bba\u7684\u8bc4\u4f30\u534f\u8baeGrAImes\uff0c\u7528\u4e8e\u5ba2\u89c2\u8bc4\u4f30AI\u751f\u6210\u7684\u5fae\u578b\u5c0f\u8bf4\u7684\u6587\u5b66\u4ef7\u503c\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1AI\u80fd\u751f\u6210\u53d9\u4e8b\u4e00\u81f4\u4e14\u8bed\u8a00\u8fde\u8d2f\u7684\u77ed\u7bc7\u5c0f\u8bf4\uff0c\u4f46\u5bf9\u5176\u6587\u5b66\u4ef7\u503c\u7684\u4e25\u683c\u8bc4\u4f30\uff08\u5c24\u5176\u662f\u7f8e\u5b66\u54c1\u8d28\uff09\u7f3a\u4e4f\u5173\u6ce8\u3002", "method": "\u63d0\u51faGrAImes\u534f\u8bae\uff0c\u57fa\u4e8e\u6587\u5b66\u7406\u8bba\uff0c\u4ece\u4e3b\u9898\u8fde\u8d2f\u6027\u3001\u6587\u672c\u6e05\u6670\u5ea6\u3001\u89e3\u91ca\u6df1\u5ea6\u548c\u7f8e\u5b66\u54c1\u8d28\u7b49\u591a\u65b9\u9762\u8bc4\u4f30AI\u751f\u6210\u7684\u5fae\u578b\u5c0f\u8bf4\u3002", "result": "\u9a8c\u8bc1\u4e86GrAImes\u534f\u8bae\u7684\u6709\u6548\u6027\uff0c\u5f97\u5230\u4e86\u6587\u5b66\u4e13\u5bb6\u548c\u6587\u5b66\u7231\u597d\u8005\u7684\u8ba4\u53ef\u3002", "conclusion": "GrAImes\u534f\u8bae\u4e3a\u8bc4\u4f30AI\u751f\u6210\u7684\u5fae\u578b\u5c0f\u8bf4\u7684\u6587\u5b66\u4ef7\u503c\u63d0\u4f9b\u4e86\u5ba2\u89c2\u6846\u67b6\u3002"}}
{"id": "2506.08071", "pdf": "https://arxiv.org/pdf/2506.08071", "abs": "https://arxiv.org/abs/2506.08071", "authors": ["Aniket Rege", "Zinnia Nie", "Mahesh Ramesh", "Unmesh Raskar", "Zhuoran Yu", "Aditya Kusupati", "Yong Jae Lee", "Ramya Korlakai Vinayak"], "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems", "categories": ["cs.CV"], "comment": "41 pages, 22 figures, 17 tables", "summary": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is\nheavily Amero and Euro-centric, underrepresenting the cultures of the Global\nSouth. To analyze these biases, we introduce CuRe, a novel and scalable\nbenchmarking and scoring suite for cultural representativeness that leverages\nthe marginal utility of attribute specification to T2I systems as a proxy for\nhuman judgments. Our CuRe benchmark dataset has a novel categorical hierarchy\nbuilt from the crowdsourced Wikimedia knowledge graph, with 300 cultural\nartifacts across 32 cultural subcategories grouped into six broad cultural axes\n(food, art, fashion, architecture, celebrations, and people). Our dataset's\ncategorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing\ntheir response to increasing the informativeness of text conditioning, enabling\nfine-grained cultural comparisons. We empirically observe much stronger\ncorrelations of our class of scorers to human judgments of perceptual\nsimilarity, image-text alignment, and cultural diversity across image encoders\n(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,\nGemini 2.0 Flash) and state-of-the-art text-to-image systems, including three\nvariants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,\nand DALL-E 3. The code and dataset is open-sourced and available at\nhttps://aniketrege.github.io/cure/.", "AI": {"tldr": "CuRe\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u6587\u5316\u4ee3\u8868\u6027\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u5c5e\u6027\u89c4\u8303\u5bf9\u7cfb\u7edf\u7684\u8fb9\u9645\u6548\u7528\u8fdb\u884c\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u8bad\u7ec3\u6570\u636e\u504f\u5411\u6b27\u7f8e\u6587\u5316\uff0c\u5ffd\u89c6\u5168\u7403\u5357\u65b9\u6587\u5316\uff0c\u9700\u91cf\u5316\u8bc4\u4f30\u5176\u6587\u5316\u4ee3\u8868\u6027\u3002", "method": "\u5229\u7528Wikimedia\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5c42\u6b21\u5316\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6587\u672c\u6761\u4ef6\u7684\u4fe1\u606f\u589e\u76ca\u8bc4\u4f30\u7cfb\u7edf\u54cd\u5e94\u3002", "result": "CuRe\u8bc4\u5206\u4e0e\u4eba\u7c7b\u611f\u77e5\u76f8\u4f3c\u6027\u3001\u56fe\u6587\u5bf9\u9f50\u548c\u6587\u5316\u591a\u6837\u6027\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "CuRe\u4e3a\u6587\u5316\u4ee3\u8868\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u7f16\u7801\u5668\u548c\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u3002"}}
{"id": "2506.08119", "pdf": "https://arxiv.org/pdf/2506.08119", "abs": "https://arxiv.org/abs/2506.08119", "authors": ["Subhrangshu Nandi", "Arghya Datta", "Nikhil Vichare", "Indranil Bhattacharya", "Huzefa Raja", "Jing Xu", "Shayan Ray", "Giuseppe Carenini", "Abhi Srivastava", "Aaron Chan", "Man Ho Woo", "Amar Kandola", "Brandon Theresa", "Francesco Carbone"], "title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents", "categories": ["cs.AI"], "comment": "Under review", "summary": "Large Language Models (LLMs) demonstrate impressive general-purpose reasoning\nand problem-solving abilities. However, they struggle with executing complex,\nlong-horizon workflows that demand strict adherence to Standard Operating\nProcedures (SOPs), a critical requirement for real-world industrial automation.\nDespite this need, there is a lack of public benchmarks that reflect the\ncomplexity, structure, and domain-specific nuances of SOPs. To address this, we\npresent three main contributions. First, we introduce a synthetic data\ngeneration framework to create realistic, industry-grade SOPs that rigorously\ntest the planning, reasoning, and tool-use capabilities of LLM-based agents.\nSecond, using this framework, we develop SOP-Bench, a benchmark of over 1,800\ntasks across 10 industrial domains, each with APIs, tool interfaces, and\nhuman-validated test cases. Third, we evaluate two prominent agent\narchitectures: Function-Calling and ReAct Agents, on SOP-Bench, observing\naverage success rates of only 27% and 48%, respectively. Remarkably, when the\ntool registry is much larger than necessary, agents invoke incorrect tools\nnearly 100% of the time. These findings underscore a substantial gap between\ncurrent agentic capabilities of LLMs and the demands of automating real-world\nSOPs. Performance varies significantly by task and domain, highlighting the\nneed for domain-specific benchmarking and architectural choices before\ndeployment. SOP-Bench is publicly available at\nhttp://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the\nprompts underpinning the data generation framework to support new\ndomain-specific SOP benchmarks. We invite the community to extend SOP-Bench\nwith SOPs from their industrial domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SOP-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u590d\u6742\u5de5\u4e1aSOP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524dLLM\u4ee3\u7406\u80fd\u529b\u4e0e\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709LLM\u5728\u590d\u6742\u5de5\u4e1aSOP\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u516c\u5f00\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u521b\u5efaSOP-Bench\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e24\u79cd\u4ee3\u7406\u67b6\u6784\u3002", "result": "Function-Calling\u548cReAct\u4ee3\u7406\u7684\u5e73\u5747\u6210\u529f\u7387\u5206\u522b\u4e3a27%\u548c48%\uff0c\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7387\u9ad8\u3002", "conclusion": "LLM\u4ee3\u7406\u80fd\u529b\u4e0e\u5b9e\u9645\u5de5\u4e1aSOP\u9700\u6c42\u5dee\u8ddd\u5927\uff0c\u9700\u9886\u57df\u7279\u5b9a\u4f18\u5316\u3002SOP-Bench\u5f00\u6e90\u4ee5\u63a8\u52a8\u793e\u533a\u6269\u5c55\u3002"}}
{"id": "2506.08174", "pdf": "https://arxiv.org/pdf/2506.08174", "abs": "https://arxiv.org/abs/2506.08174", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "title": "LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding", "categories": ["cs.CL"], "comment": "23 pages", "summary": "The rapid growth of English technical terms challenges traditional\nexpert-driven standardization, especially in fast-evolving fields like AI and\nquantum computing. Manual methods struggle to ensure multilingual consistency.\nWe propose \\textbf{LLM-BT}, a back-translation framework powered by large\nlanguage models (LLMs) to automate terminology verification and standardization\nvia cross-lingual semantic alignment. Our contributions are: \\textbf{(1)\nTerm-Level Consistency Validation:} Using English $\\rightarrow$ intermediate\nlanguage $\\rightarrow$ English back-translation, LLM-BT achieves high term\nconsistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies\nshowing over 90\\% exact or semantic matches. \\textbf{(2) Multi-Path\nVerification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''\npipeline integrates serial (e.g., EN $\\rightarrow$ ZHcn $\\rightarrow$ ZHtw\n$\\rightarrow$ EN) and parallel (e.g., EN $\\rightarrow$ Chinese/Portuguese\n$\\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong\ncross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\\%).\n\\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as\ndynamic semantic embedding, revealing latent meaning trajectories. Unlike\nstatic embeddings, LLM-BT provides transparent path-based embeddings shaped by\nmodel evolution. LLM-BT transforms back-translation into an active engine for\nmultilingual terminology standardization, enabling human--AI collaboration:\nmachines ensure semantic fidelity, humans guide cultural interpretation. This\ninfrastructure supports terminology governance across scientific and\ntechnological fields worldwide.", "AI": {"tldr": "LLM-BT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u56de\u8bd1\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u672f\u8bed\u9a8c\u8bc1\u548c\u6807\u51c6\u5316\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u4e13\u5bb6\u9a71\u52a8\u7684\u6807\u51c6\u5316\u65b9\u6cd5\u5728\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff08\u5982AI\u548c\u91cf\u5b50\u8ba1\u7b97\uff09\u4e2d\u96be\u4ee5\u5e94\u5bf9\u591a\u8bed\u8a00\u4e00\u81f4\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faLLM-BT\u6846\u67b6\uff0c\u7ed3\u5408\u672f\u8bed\u7ea7\u4e00\u81f4\u6027\u9a8c\u8bc1\u3001\u591a\u8def\u5f84\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u548c\u56de\u8bd1\u4f5c\u4e3a\u8bed\u4e49\u5d4c\u5165\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u672f\u8bed\u4e00\u81f4\u6027\u8d85\u8fc790%\uff0c\u8de8\u8bed\u8a00\u9c81\u68d2\u6027\u5f3a\uff08BLEU>0.45\uff0c\u8461\u8404\u7259\u8bed\u51c6\u786e\u7387100%\uff09\u3002", "conclusion": "LLM-BT\u5c06\u56de\u8bd1\u8f6c\u5316\u4e3a\u591a\u8bed\u8a00\u672f\u8bed\u6807\u51c6\u5316\u7684\u4e3b\u52a8\u5f15\u64ce\uff0c\u652f\u6301\u4eba\u673a\u534f\u4f5c\uff0c\u9002\u7528\u4e8e\u5168\u7403\u79d1\u6280\u9886\u57df\u7684\u672f\u8bed\u6cbb\u7406\u3002"}}
{"id": "2506.08137", "pdf": "https://arxiv.org/pdf/2506.08137", "abs": "https://arxiv.org/abs/2506.08137", "authors": ["Oishee Bintey Hoque", "Abhijin Adiga", "Aniruddha Adiga", "Siddharth Chaudhary", "Madhav V. Marathe", "S. S. Ravi", "Kirti Rajagopalan", "Amanda Wilson", "Samarth Swarup"], "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate canal network mapping is essential for water management, including\nirrigation planning and infrastructure maintenance. State-of-the-art semantic\nsegmentation models for infrastructure mapping, such as roads, rely on large,\nwell-annotated remote sensing datasets. However, incomplete or inadequate\nground truth can hinder these learning approaches. Many infrastructure networks\nhave graph-level properties such as reachability to a source (like canals) or\nconnectivity (roads) that can be leveraged to improve these existing ground\ntruth. This paper develops a novel iterative framework IGraSS, combining a\nsemantic segmentation module-incorporating RGB and additional modalities (NDWI,\nDEM)-with a graph-based ground-truth refinement module. The segmentation module\nprocesses satellite imagery patches, while the refinement module operates on\nthe entire data viewing the infrastructure network as a graph. Experiments show\nthat IGraSS reduces unreachable canal segments from around 18% to 3%, and\ntraining with refined ground truth significantly improves canal identification.\nIGraSS serves as a robust framework for both refining noisy ground truth and\nmapping canal networks from remote sensing imagery. We also demonstrate the\neffectiveness and generalizability of IGraSS using road networks as an example,\napplying a different graph-theoretic constraint to complete road networks.", "AI": {"tldr": "IGraSS\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u56fe\u4f18\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u4e0d\u5b8c\u6574\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u6cb3\u548c\u9053\u8def\u7f51\u7edc\u7684\u6620\u5c04\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u4e0d\u5b8c\u6574\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\uff08\u5982\u8fd0\u6cb3\u548c\u9053\u8def\uff09\u5177\u6709\u56fe\u7ea7\u7279\u6027\uff08\u5982\u53ef\u8fbe\u6027\u6216\u8fde\u901a\u6027\uff09\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u5730\u9762\u771f\u5b9e\u6570\u636e\u3002", "method": "\u63d0\u51faIGraSS\u6846\u67b6\uff0c\u7ed3\u5408RGB\u548c\u591a\u6a21\u6001\u6570\u636e\uff08NDWI\u3001DEM\uff09\u7684\u8bed\u4e49\u5206\u5272\u6a21\u5757\u4e0e\u57fa\u4e8e\u56fe\u7684\u4f18\u5316\u6a21\u5757\uff0c\u8fed\u4ee3\u4f18\u5316\u5730\u9762\u771f\u5b9e\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cIGraSS\u5c06\u4e0d\u53ef\u8fbe\u8fd0\u6cb3\u6bb5\u4ece18%\u964d\u81f33%\uff0c\u5e76\u663e\u8457\u63d0\u5347\u8fd0\u6cb3\u8bc6\u522b\u7cbe\u5ea6\u3002\u6846\u67b6\u8fd8\u6210\u529f\u5e94\u7528\u4e8e\u9053\u8def\u7f51\u7edc\u3002", "conclusion": "IGraSS\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u566a\u58f0\u5730\u9762\u771f\u5b9e\u6570\u636e\u5e76\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\u7684\u9065\u611f\u6620\u5c04\u6548\u679c\u3002"}}
{"id": "2506.08134", "pdf": "https://arxiv.org/pdf/2506.08134", "abs": "https://arxiv.org/abs/2506.08134", "authors": ["Qiyao Wei", "Samuel Holt", "Jing Yang", "Markus Wulfmeier", "Mihaela van der Schaar"], "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning", "categories": ["cs.AI", "cs.CY", "68T50, 68T07", "I.2.7; H.5.3"], "comment": "18 pages, 3 figures. Position paper", "summary": "Peer review, the bedrock of scientific advancement in machine learning (ML),\nis strained by a crisis of scale. Exponential growth in manuscript submissions\nto premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite\ncapacity of qualified reviewers, leading to concerns about review quality,\nconsistency, and reviewer fatigue. This position paper argues that AI-assisted\npeer review must become an urgent research and infrastructure priority. We\nadvocate for a comprehensive AI-augmented ecosystem, leveraging Large Language\nModels (LLMs) not as replacements for human judgment, but as sophisticated\ncollaborators for authors, reviewers, and Area Chairs (ACs). We propose\nspecific roles for AI in enhancing factual verification, guiding reviewer\nperformance, assisting authors in quality improvement, and supporting ACs in\ndecision-making. Crucially, we contend that the development of such systems\nhinges on access to more granular, structured, and ethically-sourced peer\nreview process data. We outline a research agenda, including illustrative\nexperiments, to develop and validate these AI assistants, and discuss\nsignificant technical and ethical challenges. We call upon the ML community to\nproactively build this AI-assisted future, ensuring the continued integrity and\nscalability of scientific validation, while maintaining high standards of peer\nreview.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20AI\u8f85\u52a9\u540c\u884c\u8bc4\u5ba1\u662f\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u9886\u57df\u8bc4\u5ba1\u5371\u673a\u7684\u5173\u952e\uff0c\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u534f\u4f5c\u5de5\u5177\uff0c\u800c\u975e\u66ff\u4ee3\u4eba\u7c7b\u5224\u65ad\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u9886\u57df\u8bba\u6587\u63d0\u4ea4\u91cf\u6fc0\u589e\uff0c\u5bfc\u81f4\u8bc4\u5ba1\u8d28\u91cf\u4e0b\u964d\u3001\u8bc4\u5ba1\u75b2\u52b3\uff0c\u4e9f\u9700AI\u8f85\u52a9\u63d0\u5347\u8bc4\u5ba1\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u6784\u5efaAI\u589e\u5f3a\u7684\u8bc4\u5ba1\u751f\u6001\u7cfb\u7edf\uff0c\u5229\u7528LLMs\u8f85\u52a9\u4e8b\u5b9e\u6838\u67e5\u3001\u8bc4\u5ba1\u6307\u5bfc\u3001\u4f5c\u8005\u6539\u8fdb\u548c\u51b3\u7b56\u652f\u6301\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u7ed3\u6784\u5316\u8bc4\u5ba1\u6570\u636e\u652f\u6301\u3002", "result": "\u8bba\u6587\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u7814\u7a76\u8bae\u7a0b\u548c\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u547c\u5401\u673a\u5668\u5b66\u4e60\u793e\u533a\u79ef\u6781\u5f00\u53d1AI\u8f85\u52a9\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u4ee5\u4fdd\u969c\u79d1\u5b66\u9a8c\u8bc1\u7684\u5b8c\u6574\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184", "abs": "https://arxiv.org/abs/2506.08184", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "title": "Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u4fe1\u606f\u68c0\u7d22\u4e0e\u751f\u6210\u80fd\u529b\u7684\u5173\u8054\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u5e72\u6270\u663e\u8457\u5f71\u54cd\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u63d0\u793a\u6a21\u578b\u5b58\u5728\u5de5\u4f5c\u8bb0\u5fc6\u74f6\u9888\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u56e0\u4e0a\u4e0b\u6587\u5e72\u6270\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u4e3b\u52a8\u5e72\u6270\uff08PI\uff09\u8303\u5f0f\u3002", "method": "\u63d0\u51faPI-LLM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u987a\u5e8f\u6d41\u5f0f\u4f20\u8f93\u8bed\u4e49\u76f8\u5173\u7684\u952e\u503c\u66f4\u65b0\u5e76\u4ec5\u67e5\u8be2\u6700\u7ec8\u503c\uff0c\u6d4b\u8bd5LLMs\u7684\u68c0\u7d22\u80fd\u529b\u3002", "result": "LLMs\u7684\u68c0\u7d22\u51c6\u786e\u6027\u968f\u5e72\u6270\u7d2f\u79ef\u5448\u5bf9\u6570\u7ebf\u6027\u4e0b\u964d\u81f3\u96f6\uff0c\u63d0\u793a\u5de5\u7a0b\u7f13\u89e3\u6548\u679c\u6709\u9650\u3002", "conclusion": "LLMs\u5728\u5e72\u6270\u5206\u79bb\u548c\u4fe1\u606f\u7075\u6d3b\u5904\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u9700\u6539\u8fdb\u6a21\u578b\u6291\u5236\u65e0\u5173\u5185\u5bb9\u7684\u80fd\u529b\u3002"}}
{"id": "2506.08163", "pdf": "https://arxiv.org/pdf/2506.08163", "abs": "https://arxiv.org/abs/2506.08163", "authors": ["Harshvardhan Takawale", "Nirupam Roy"], "title": "Spectral Domain Neural Reconstruction for Passband FMCW Radars", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.23313", "summary": "We present SpINRv2, a neural framework for high-fidelity volumetric\nreconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.\nExtending our prior work (SpINR), this version introduces enhancements that\nallow accurate learning under high start frequencies-where phase aliasing and\nsub-bin ambiguity become prominent. Our core contribution is a fully\ndifferentiable frequency-domain forward model that captures the complex radar\nresponse using closed-form synthesis, paired with an implicit neural\nrepresentation (INR) for continuous volumetric scene modeling. Unlike\ntime-domain baselines, SpINRv2 directly supervises the complex frequency\nspectrum, preserving spectral fidelity while drastically reducing computational\noverhead. Additionally, we introduce sparsity and smoothness regularization to\ndisambiguate sub-bin ambiguities that arise at fine range resolutions.\nExperimental results show that SpINRv2 significantly outperforms both classical\nand learning-based baselines, especially under high-frequency regimes,\nestablishing a new benchmark for neural radar-based 3D imaging.", "AI": {"tldr": "SpINRv2\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u5ea6\u4f53\u79ef\u91cd\u5efa\uff0c\u901a\u8fc7\u6539\u8fdb\u89e3\u51b3\u4e86\u9ad8\u9891\u4e0b\u7684\u76f8\u4f4d\u6df7\u53e0\u548c\u5b50\u7bb1\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u9ad8\u8d77\u59cb\u9891\u7387\u4e0b\u76f8\u4f4d\u6df7\u53e0\u548c\u5b50\u7bb1\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u4f53\u79ef\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u5b8c\u5168\u53ef\u5fae\u7684\u9891\u7387\u57df\u524d\u5411\u6a21\u578b\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\uff0c\u7ed3\u5408\u7a00\u758f\u6027\u548c\u5e73\u6ed1\u6027\u6b63\u5219\u5316\u3002", "result": "SpINRv2\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9ad8\u9891\u6761\u4ef6\u4e0b\u3002", "conclusion": "SpINRv2\u4e3a\u57fa\u4e8e\u795e\u7ecf\u96f7\u8fbe\u76843D\u6210\u50cf\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.08150", "pdf": "https://arxiv.org/pdf/2506.08150", "abs": "https://arxiv.org/abs/2506.08150", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Di\u00e9guez", "Javier Romero", "Susana Hahn", "Torsten Schaub"], "title": "Compiling Metric Temporal Answer Set Programming", "categories": ["cs.AI", "cs.LO", "I.2.4; I.2.8"], "comment": null, "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to\nallow for expressing quantitative temporal constrains, like durations and\ndeadlines. A central challenge is to maintain scalability when dealing with\nfine-grained timing constraints, which can significantly exacerbate ASP's\ngrounding bottleneck. To address this issue, we leverage extensions of ASP with\ndifference constraints, a simplified form of linear constraints, to handle\ntime-related aspects externally. Our approach effectively decouples metric ASP\nfrom the granularity of time, resulting in a solution that is unaffected by\ntime precision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u5b9a\u91cf\u65f6\u95f4\u7ea6\u675f\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55ASP\u7684\u5dee\u5f02\u7ea6\u675f\u6765\u907f\u514d\u65f6\u95f4\u7c92\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3ASP\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7ea6\u675f\u65f6\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528ASP\u7684\u5dee\u5f02\u7ea6\u675f\u6269\u5c55\uff0c\u5c06\u65f6\u95f4\u76f8\u5173\u90e8\u5206\u5916\u90e8\u5904\u7406\u3002", "result": "\u5b9e\u73b0\u4e86\u4e0d\u53d7\u65f6\u95f4\u7cbe\u5ea6\u5f71\u54cd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ASP\u5728\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2506.08221", "pdf": "https://arxiv.org/pdf/2506.08221", "abs": "https://arxiv.org/abs/2506.08221", "authors": ["Samra Zafar", "Shaheer Minhas", "Syed Ali Hassan Zaidi", "Arfa Naeem", "Zahra Ali"], "title": "\"I Wrote, I Paused, I Rewrote\" Teaching LLMs to Read Between the Lines of Student Writing", "categories": ["cs.CL"], "comment": "7 pages, 6 figures, 2 tables", "summary": "Large language models(LLMs) like Gemini are becoming common tools for\nsupporting student writing. But most of their feedback is based only on the\nfinal essay missing important context about how that text was written. In this\npaper, we explore whether using writing process data, collected through\nkeystroke logging and periodic snapshots, can help LLMs give feedback that\nbetter reflects how learners think and revise while writing. We built a digital\nwriting tool that captures both what students type and how their essays evolve\nover time. Twenty students used this tool to write timed essays, which were\nthen evaluated in two ways: (i) LLM generated feedback using both the final\nessay and the full writing trace, and (ii) After the task, students completed\nsurveys about how useful and relatable they found the feedback. Early results\nshow that learners preferred the process-aware LLM feedback, finding it more in\ntune with their own thinking. We also found that certain types of edits, like\nadding new content or reorganizing paragraphs, aligned closely with higher\nscores in areas like coherence and elaboration. Our findings suggest that\nmaking LLMs more aware of the writing process can lead to feedback that feels\nmore meaningful, personal, and supportive.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5199\u4f5c\u8fc7\u7a0b\u6570\u636e\uff08\u5982\u952e\u76d8\u8bb0\u5f55\u548c\u5b9a\u671f\u5feb\u7167\uff09\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u5b66\u751f\u5199\u4f5c\u53cd\u9988\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5b66\u751f\u66f4\u503e\u5411\u4e8e\u57fa\u4e8e\u8fc7\u7a0b\u7684\u53cd\u9988\u3002", "motivation": "\u73b0\u6709LLM\u53cd\u9988\u4ec5\u57fa\u4e8e\u6700\u7ec8\u6587\u672c\uff0c\u7f3a\u4e4f\u5199\u4f5c\u8fc7\u7a0b\u7684\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u53cd\u6620\u5b66\u751f\u7684\u601d\u8003\u548c\u4fee\u6539\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u5b57\u5199\u4f5c\u5de5\u5177\uff0c\u8bb0\u5f55\u5b66\u751f\u7684\u6253\u5b57\u5185\u5bb9\u548c\u6587\u7ae0\u6f14\u53d8\u8fc7\u7a0b\uff0c\u5e76\u8ba920\u540d\u5b66\u751f\u4f7f\u7528\u8be5\u5de5\u5177\u5b8c\u6210\u9650\u65f6\u5199\u4f5c\uff0c\u968f\u540e\u901a\u8fc7LLM\u751f\u6210\u53cd\u9988\u5e76\u6536\u96c6\u5b66\u751f\u8bc4\u4ef7\u3002", "result": "\u5b66\u751f\u66f4\u504f\u597d\u57fa\u4e8e\u8fc7\u7a0b\u7684LLM\u53cd\u9988\uff0c\u8ba4\u4e3a\u5176\u66f4\u8d34\u8fd1\u81ea\u8eab\u601d\u7ef4\uff1b\u67d0\u4e9b\u7f16\u8f91\u884c\u4e3a\uff08\u5982\u6dfb\u52a0\u5185\u5bb9\u6216\u91cd\u7ec4\u6bb5\u843d\uff09\u4e0e\u66f4\u9ad8\u7684\u8fde\u8d2f\u6027\u548c\u8be6\u8ff0\u5f97\u5206\u76f8\u5173\u3002", "conclusion": "\u589e\u5f3aLLM\u5bf9\u5199\u4f5c\u8fc7\u7a0b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u53ef\u63d0\u4f9b\u66f4\u6709\u610f\u4e49\u3001\u4e2a\u6027\u5316\u548c\u652f\u6301\u6027\u7684\u53cd\u9988\u3002"}}
{"id": "2506.08185", "pdf": "https://arxiv.org/pdf/2506.08185", "abs": "https://arxiv.org/abs/2506.08185", "authors": ["Huixin Zhan", "Jason H. Moore"], "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u6846\u67b6\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7ba1\u9053\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u5916\u79d1\u533b\u751f\u7684\u4e2a\u6027\u5316\u64cd\u4f5c\u98ce\u683c\uff0c\u540c\u65f6\u5173\u6ce8\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5e38\u5ffd\u7565\u5916\u79d1\u533b\u751f\u7684\u4e2a\u6027\u5316\u64cd\u4f5c\u98ce\u683c\uff0c\u800c\u4e2a\u6027\u5316\u5efa\u6a21\u5bf9\u63d0\u5347\u624b\u672fAI\u7cfb\u7edf\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u79bb\u6563\u6269\u6563\u6846\u67b6\u548cVLA\u7ba1\u9053\uff0c\u5c06\u624b\u52bf\u9884\u6d4b\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\uff0c\u7ed3\u5408\u5185\u7aa5\u955c\u89c6\u9891\u3001\u624b\u672f\u610f\u56fe\u8bed\u8a00\u548c\u9690\u79c1\u611f\u77e5\u7684\u5916\u79d1\u533b\u751f\u8eab\u4efd\u5d4c\u5165\u3002", "result": "\u5728JIGSAWS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u624b\u52bf\u5e8f\u5217\u5e76\u5b66\u4e60\u5230\u6bcf\u4f4d\u5916\u79d1\u533b\u751f\u7684\u72ec\u7279\u8fd0\u52a8\u6307\u7eb9\u3002", "conclusion": "\u4e2a\u6027\u5316\u5d4c\u5165\u867d\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e5f\u589e\u52a0\u8eab\u4efd\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u5728\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u98ce\u9669\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2506.08306", "pdf": "https://arxiv.org/pdf/2506.08306", "abs": "https://arxiv.org/abs/2506.08306", "authors": ["Tuan Truong", "Rithwik Sudharsan", "Yibo Yang", "Peter Xiangyuan Ma", "Ruihan Yang", "Stephan Mandt", "Joshua S. Bloom"], "title": "AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data", "categories": ["cs.AI", "astro-ph.IM"], "comment": "ICLR 2025 conference paper. See reviews at\n  https://openreview.net/forum?id=kQCHCkNk7s", "summary": "The site conditions that make astronomical observatories in space and on the\nground so desirable -- cold and dark -- demand a physical remoteness that leads\nto limited data transmission capabilities. Such transmission limitations\ndirectly bottleneck the amount of data acquired and in an era of costly modern\nobservatories, any improvements in lossless data compression has the potential\nscale to billions of dollars worth of additional science that can be\naccomplished on the same instrument. Traditional lossless methods for\ncompressing astrophysical data are manually designed. Neural data compression,\non the other hand, holds the promise of learning compression algorithms\nend-to-end from data and outperforming classical techniques by leveraging the\nunique spatial, temporal, and wavelength structures of astronomical images.\nThis paper introduces AstroCompress: a neural compression challenge for\nastrophysics data, featuring four new datasets (and one legacy dataset) with\n16-bit unsigned integer imaging data in various modes: space-based,\nground-based, multi-wavelength, and time-series imaging. We provide code to\neasily access the data and benchmark seven lossless compression methods (three\nneural and four non-neural, including all practical state-of-the-art\nalgorithms). Our results on lossless compression indicate that lossless neural\ncompression techniques can enhance data collection at observatories, and\nprovide guidance on the adoption of neural compression in scientific\napplications. Though the scope of this paper is restricted to lossless\ncompression, we also comment on the potential exploration of lossy compression\nmethods in future studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAstroCompress\uff0c\u4e00\u79cd\u7528\u4e8e\u5929\u4f53\u7269\u7406\u6570\u636e\u7684\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u63d0\u5347\u6570\u636e\u538b\u7f29\u6548\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5929\u6587\u53f0\u56e0\u73af\u5883\u9650\u5236\u5bfc\u81f4\u6570\u636e\u4f20\u8f93\u80fd\u529b\u53d7\u9650\uff0c\u4f20\u7edf\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\uff0c\u795e\u7ecf\u538b\u7f29\u6280\u672f\u6709\u671b\u901a\u8fc7\u6570\u636e\u5b66\u4e60\u63d0\u5347\u538b\u7f29\u6027\u80fd\u3002", "method": "\u5f15\u5165AstroCompress\u6311\u6218\uff0c\u63d0\u4f9b\u4e94\u4e2a\u6570\u636e\u96c6\uff08\u56db\u4e2a\u65b0\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u65e7\u6570\u636e\u96c6\uff09\uff0c\u5e76\u5bf9\u6bd4\u4e03\u79cd\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\uff08\u4e09\u79cd\u795e\u7ecf\u65b9\u6cd5\u548c\u56db\u79cd\u4f20\u7edf\u65b9\u6cd5\uff09\u3002", "result": "\u795e\u7ecf\u538b\u7f29\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u5929\u6587\u53f0\u6570\u636e\u6536\u96c6\u6548\u7387\uff0c\u4e3a\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u795e\u7ecf\u538b\u7f29\u91c7\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u795e\u7ecf\u538b\u7f29\u6280\u672f\u5728\u5929\u4f53\u7269\u7406\u6570\u636e\u538b\u7f29\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u6709\u635f\u538b\u7f29\u65b9\u6cd5\u3002"}}
{"id": "2506.08234", "pdf": "https://arxiv.org/pdf/2506.08234", "abs": "https://arxiv.org/abs/2506.08234", "authors": ["Yu-Ang Lee", "Guan-Ting Yi", "Mei-Yi Liu", "Jui-Chao Lu", "Guan-Bo Yang", "Yun-Nung Chen"], "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 1 table", "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u590d\u5408AI\u7cfb\u7edf\u4f18\u5316\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u6570\u503c\u548c\u57fa\u4e8e\u8bed\u8a00\u7684\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u590d\u5408AI\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u4f18\u5316\u7ec4\u4ef6\u53ca\u5176\u4ea4\u4e92\u6210\u4e3a\u65b0\u6311\u6218\uff0c\u9700\u8981\u63a2\u7d22\u4f20\u7edf\u65b9\u6cd5\u4e4b\u5916\u7684\u65b0\u9014\u5f84\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u590d\u5408AI\u7cfb\u7edf\u4f18\u5316\u7684\u8fdb\u5c55\uff0c\u5f62\u5f0f\u5316\u4e86\u4f18\u5316\u6982\u5ff5\uff0c\u5e76\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u603b\u7ed3\u4e86\u6570\u503c\u548c\u8bed\u8a00\u53cd\u9988\u6280\u672f\u5728\u4f18\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u653e\u7814\u7a76\u6311\u6218\u3002", "conclusion": "\u590d\u5408AI\u7cfb\u7edf\u4f18\u5316\u662f\u4e00\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u8bed\u8a00\u53cd\u9988\u7b49\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.08189", "pdf": "https://arxiv.org/pdf/2506.08189", "abs": "https://arxiv.org/abs/2506.08189", "authors": ["Amartya Dutta", "Kazi Sajeed Mehrab", "Medha Sawhney", "Abhilash Neog", "Mridul Khurana", "Sepideh Fatemi", "Aanish Pradhan", "M. Maruf", "Ismini Lourentzou", "Arka Daw", "Anuj Karpatne"], "title": "Open World Scene Graph Generation using Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in CVPR 2025 Workshop (CVinW)", "summary": "Scene-Graph Generation (SGG) seeks to recognize objects in an image and\ndistill their salient pairwise relationships. Most methods depend on\ndataset-specific supervision to learn the variety of interactions, restricting\ntheir usefulness in open-world settings, involving novel objects and/or\nrelations. Even methods that leverage large Vision Language Models (VLMs)\ntypically require benchmark-specific fine-tuning. We introduce Open-World SGG,\na training-free, efficient, model-agnostic framework that taps directly into\nthe pretrained knowledge of VLMs to produce scene graphs with zero additional\nlearning. Casting SGG as a zero-shot structured-reasoning problem, our method\ncombines multimodal prompting, embedding alignment, and a lightweight\npair-refinement strategy, enabling inference over unseen object vocabularies\nand relation sets. To assess this setting, we formalize an Open-World\nevaluation protocol that measures performance when no SGG-specific data have\nbeen observed either in terms of objects and relations. Experiments on Visual\nGenome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate\nthe capacity of pretrained VLMs to perform relational understanding without\ntask-level training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u4e16\u754c\u573a\u666f\u56fe\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u76d1\u7763\u6216\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u63d0\u793a\u3001\u5d4c\u5165\u5bf9\u9f50\u548c\u8f7b\u91cf\u7ea7\u5bf9\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u7ed3\u6784\u5316\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u9700\u4efb\u52a1\u7ea7\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u5173\u7cfb\u7406\u89e3\u7684\u80fd\u529b\u3002", "conclusion": "\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u56fe\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.08321", "pdf": "https://arxiv.org/pdf/2506.08321", "abs": "https://arxiv.org/abs/2506.08321", "authors": ["Manooshree Patel", "Rayna Bhattacharyya", "Thomas Lu", "Arnav Mehta", "Niels Voss", "Narges Norouzi", "Gireeja Ranade"], "title": "LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs", "categories": ["cs.AI", "cs.HC", "cs.LO"], "comment": null, "summary": "We present LeanTutor, a Large Language Model (LLM)-based tutoring system for\nmath proofs. LeanTutor interacts with the student in natural language, formally\nverifies student-written math proofs in Lean, generates correct next steps, and\nprovides the appropriate instructional guidance. LeanTutor is composed of three\nmodules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and\n(iii) a natural language feedback generator. The first module faithfully\nautoformalizes student proofs into Lean and verifies proof accuracy via\nsuccessful code compilation. If the proof has an error, the incorrect step is\nidentified. The next-step generator module outputs a valid next Lean tactic for\nincorrect proofs via LLM-based candidate generation and proof search. The\nfeedback generator module leverages Lean data to produce a\npedagogically-motivated natural language hint for the student user. To evaluate\nour system, we introduce PeanoBench, a human-written dataset derived from the\nNatural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each\nnatural language proof step is paired with the corresponding logically\nequivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of\ntactics in correct proofs and accurately identifies the incorrect step in 30%\nof incorrect proofs. In generating natural language hints for erroneous proofs,\nLeanTutor outperforms a simple baseline on accuracy and relevance metrics.", "AI": {"tldr": "LeanTutor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3001\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u751f\u6210\u4e0b\u4e00\u6b65\u6307\u5bfc\u6765\u5e2e\u52a9\u5b66\u751f\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u4e0e\u5b66\u751f\u81ea\u7136\u4ea4\u4e92\u3001\u9a8c\u8bc1\u6570\u5b66\u8bc1\u660e\u5e76\u63d0\u4f9b\u6307\u5bfc\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u6570\u5b66\u5b66\u4e60\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u81ea\u52a8\u5f62\u5f0f\u5316/\u8bc1\u660e\u68c0\u67e5\u5668\u3001\u4e0b\u4e00\u6b65\u751f\u6210\u5668\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u751f\u6210\u5668\uff0c\u5206\u522b\u8d1f\u8d23\u9a8c\u8bc1\u8bc1\u660e\u3001\u751f\u6210\u4e0b\u4e00\u6b65\u7b56\u7565\u548c\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u81ea\u52a8\u5f62\u5f0f\u5316\u6a21\u5757\u5728\u6b63\u786e\u8bc1\u660e\u4e2d\u51c6\u786e\u5f62\u5f0f\u5316\u4e8657%\u7684\u7b56\u7565\uff0c\u5728\u9519\u8bef\u8bc1\u660e\u4e2d\u51c6\u786e\u8bc6\u522b\u4e8630%\u7684\u9519\u8bef\u6b65\u9aa4\uff1b\u53cd\u9988\u751f\u6210\u5668\u5728\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u4e0a\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf\u3002", "conclusion": "LeanTutor\u5c55\u793a\u4e86LLM\u5728\u6570\u5b66\u8f85\u5bfc\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u751f\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2506.08235", "pdf": "https://arxiv.org/pdf/2506.08235", "abs": "https://arxiv.org/abs/2506.08235", "authors": ["Shashidhar Reddy Javaji", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Nikhil Muralidhar", "Zining Zhu"], "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 6 figures, Under review", "summary": "Large language models (LLMs) are increasingly being used for complex research\ntasks such as literature review, idea generation, and scientific paper\nanalysis, yet their ability to truly understand and process the intricate\nrelationships within complex research papers, such as the logical links between\nclaims and supporting evidence remains largely unexplored. In this study, we\npresent CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'\ncapabilities in scientific claim-evidence extraction and validation, a task\nthat reflects deeper comprehension of scientific argumentation. We\nsystematically compare three approaches which are inspired by divide and\nconquer approaches, across six diverse LLMs, highlighting model-specific\nstrengths and weaknesses in scientific comprehension. Through evaluation\ninvolving over 300 claim-evidence pairs across multiple research domains, we\nreveal significant limitations in LLMs' ability to process complex scientific\ncontent. Our results demonstrate that closed-source models like GPT-4 and\nClaude consistently outperform open-source counterparts in precision and recall\nacross claim-evidence identification tasks. Furthermore, strategically designed\nthree-pass and one-by-one prompting approaches significantly improve LLMs'\nabilities to accurately link dispersed evidence with claims, although this\ncomes at increased computational cost. CLAIM-BENCH sets a new standard for\nevaluating scientific comprehension in LLMs, offering both a diagnostic tool\nand a path forward for building systems capable of deeper, more reliable\nreasoning across full-length papers.", "AI": {"tldr": "CLAIM-BENCH\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u8bba\u6587\u4e2d\u63d0\u53d6\u548c\u9a8c\u8bc1\u58f0\u660e\u4e0e\u8bc1\u636e\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u5904\u7406\u590d\u6742\u79d1\u5b66\u5185\u5bb9\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u771f\u6b63\u7406\u89e3\u548c\u5904\u7406\u590d\u6742\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u903b\u8f91\u5173\u7cfb\uff0c\u7279\u522b\u662f\u58f0\u660e\u4e0e\u652f\u6301\u8bc1\u636e\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e09\u79cd\u57fa\u4e8e\u5206\u6cbb\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u516d\u79cd\u4e0d\u540c\u7684LLMs\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86300\u591a\u4e2a\u58f0\u660e-\u8bc1\u636e\u5bf9\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4\u548cClaude\uff09\u5728\u58f0\u660e-\u8bc1\u636e\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4e14\u4e09\u904d\u548c\u9010\u4e00\u63d0\u793a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u7684\u51c6\u786e\u6027\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "CLAIM-BENCH\u4e3a\u8bc4\u4f30LLMs\u7684\u79d1\u5b66\u7406\u89e3\u80fd\u529b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u548c\u8def\u5f84\u3002"}}
{"id": "2506.08191", "pdf": "https://arxiv.org/pdf/2506.08191", "abs": "https://arxiv.org/abs/2506.08191", "authors": ["Antoni Nowinowski", "Krzysztof Krawiec"], "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes", "categories": ["cs.CV"], "comment": null, "summary": "This study builds on the architecture of the Disentangler of Visual Priors\n(DVP), a type of autoencoder that learns to interpret scenes by decomposing the\nperceived objects into independent visual aspects of shape, size, orientation,\nand color appearance. These aspects are expressed as latent parameters which\ncontrol a differentiable renderer that performs image reconstruction, so that\nthe model can be trained end-to-end with gradient using reconstruction loss. In\nthis study, we extend the original DVP so that it can handle multiple objects\nin a scene. We also exploit the interpretability of its latent by using the\ndecoder to sample additional training examples and devising alternative\ntraining modes that rely on loss functions defined not only in the image space,\nbut also in the latent space. This significantly facilitates training, which is\notherwise challenging due to the presence of extensive plateaus in the\nimage-space reconstruction loss. To examine the performance of this approach,\nwe propose a new benchmark featuring multiple 2D objects, which subsumes the\npreviously proposed Multi-dSprites dataset while being more parameterizable. We\ncompare the DVP extended in these ways with two baselines (MONet and LIVE) and\ndemonstrate its superiority in terms of reconstruction quality and capacity to\ndecompose overlapping objects. We also analyze the gradients induced by the\nconsidered loss functions, explain how they impact the efficacy of training,\nand discuss the limitations of differentiable rendering in autoencoders and the\nways in which they can be addressed.", "AI": {"tldr": "\u6269\u5c55\u4e86DVP\u67b6\u6784\uff0c\u4f7f\u5176\u80fd\u5904\u7406\u591a\u7269\u4f53\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u635f\u5931\u51fd\u6570\u6539\u8fdb\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u539f\u59cbDVP\u65e0\u6cd5\u5904\u7406\u591a\u7269\u4f53\u573a\u666f\u53ca\u8bad\u7ec3\u4e2d\u56fe\u50cf\u7a7a\u95f4\u635f\u5931\u51fd\u6570\u5bfc\u81f4\u7684\u8bad\u7ec3\u56f0\u96be\u95ee\u9898\u3002", "method": "\u6269\u5c55DVP\u67b6\u6784\uff0c\u5f15\u5165\u6f5c\u5728\u7a7a\u95f4\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bbe\u8ba1\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u5206\u89e3\u91cd\u53e0\u7269\u4f53\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bMONet\u548cLIVE\u3002", "conclusion": "\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u679c\uff0c\u4f46\u53ef\u5fae\u5206\u6e32\u67d3\u5728\u81ea\u7f16\u7801\u5668\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\u3002"}}
{"id": "2506.08332", "pdf": "https://arxiv.org/pdf/2506.08332", "abs": "https://arxiv.org/abs/2506.08332", "authors": ["Amur Ghose", "Andrew B. Kahng", "Sayak Kundu", "Zhiang Wang"], "title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Machine learning has been widely used to optimize complex engineering\nworkflows across numerous domains. In the context of integrated circuit design,\nmodern flows (e.g., going from a register-transfer level netlist to physical\nlayouts) involve extensive configuration via thousands of parameters, and small\nchanges to these parameters can have large downstream impacts on desired\noutcomes - namely design performance, power, and area. Recent advances in Large\nLanguage Models (LLMs) offer new opportunities for learning and reasoning\nwithin such high-dimensional optimization tasks. In this work, we introduce\nORFS-agent, an LLM-based iterative optimization agent that automates parameter\ntuning in an open-source hardware design flow. ORFS-agent adaptively explores\nparameter configurations, demonstrating clear improvements over standard\nBayesian optimization approaches in terms of resource efficiency and final\ndesign metrics. Our empirical evaluations on two different technology nodes and\na range of circuit benchmarks indicate that ORFS-agent can improve both routed\nwirelength and effective clock period by over 13%, all while using 40% fewer\noptimization iterations. Moreover, by following natural language objectives to\ntrade off certain metrics for others, ORFS-agent demonstrates a flexible and\ninterpretable framework for multi-objective optimization. Crucially, RFS-agent\nis modular and model-agnostic, and can be plugged in to any frontier LLM\nwithout any further fine-tuning.", "AI": {"tldr": "ORFS-agent\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fed\u4ee3\u4f18\u5316\u4ee3\u7406\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u786c\u4ef6\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u53c2\u6570\u8c03\u4f18\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u6d41\u7a0b\u6d89\u53ca\u5927\u91cf\u53c2\u6570\u914d\u7f6e\uff0c\u5fae\u5c0f\u53d8\u5316\u53ef\u80fd\u5bf9\u8bbe\u8ba1\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\u3002", "method": "ORFS-agent\u901a\u8fc7\u81ea\u9002\u5e94\u63a2\u7d22\u53c2\u6570\u914d\u7f6e\uff0c\u7ed3\u5408LLM\u8fdb\u884c\u9ad8\u7ef4\u4f18\u5316\u4efb\u52a1\uff0c\u5b9e\u73b0\u8d44\u6e90\u6548\u7387\u548c\u8bbe\u8ba1\u6307\u6807\u7684\u63d0\u5347\u3002", "result": "\u5728\u4e24\u4e2a\u6280\u672f\u8282\u70b9\u548c\u591a\u4e2a\u7535\u8def\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cORFS-agent\u5c06\u5e03\u7ebf\u957f\u5ea6\u548c\u6709\u6548\u65f6\u949f\u5468\u671f\u6539\u5584\u4e8613%\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c1140%\u7684\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "ORFS-agent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u4e14\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55\u524d\u6cbfLLM\u4e2d\u3002"}}
{"id": "2506.08260", "pdf": "https://arxiv.org/pdf/2506.08260", "abs": "https://arxiv.org/abs/2506.08260", "authors": ["Wanjing Anya Ma", "Michael Flor", "Zuowei Wang"], "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2025), co-located with the ACL 2025", "summary": "Inference making is an essential but complex skill in reading comprehension\n(RC). Some inferences require resolving references across sentences, and some\nrely on using prior knowledge to fill in the detail that is not explicitly\nwritten in the text. Diagnostic RC questions can help educators provide more\neffective and targeted reading instruction and interventions for school-age\nstudents. We introduce a taxonomy of inference types for RC and use it to\nanalyze the distribution of items within a diagnostic RC item bank. Next, we\npresent experiments using GPT-4o to generate bridging-inference RC items for\ngiven reading passages via few-shot prompting, comparing conditions with and\nwithout chain-of-thought prompts. Generated items were evaluated on three\naspects: overall item quality, appropriate inference type, and LLM reasoning,\nachieving high inter-rater agreements above 0.90. Our results show that GPT-4o\nproduced 93.8% good-quality questions suitable for operational use in grade\n3-12 contexts; however, only 42.6% of the generated questions accurately\nmatched the targeted inference type. We conclude that combining automatic item\ngeneration with human judgment offers a promising path toward scalable,\nhigh-quality diagnostic RC assessments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9605\u8bfb\u7406\u89e3\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u7c7b\u578b\u5206\u7c7b\u6cd5\uff0c\u5e76\u5229\u7528GPT-4o\u751f\u6210\u8bca\u65ad\u6027\u9605\u8bfb\u7406\u89e3\u9898\u76ee\uff0c\u9a8c\u8bc1\u4e86\u81ea\u52a8\u751f\u6210\u4e0e\u4eba\u5de5\u5224\u65ad\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u9605\u8bfb\u7406\u89e3\u6559\u5b66\u4e2d\u8bca\u65ad\u6027\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b66\u9f84\u5b66\u751f\u63d0\u4f9b\u66f4\u6709\u9488\u5bf9\u6027\u7684\u9605\u8bfb\u6307\u5bfc\u548c\u5e72\u9884\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u7c7b\u578b\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u8bca\u65ad\u6027\u9605\u8bfb\u7406\u89e3\u9898\u5e93\u4e2d\u7684\u9898\u76ee\u5206\u5e03\uff1b\u4f7f\u7528GPT-4o\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u751f\u6210\u63a8\u7406\u9898\u76ee\uff0c\u6bd4\u8f83\u6709\u65e0\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u6548\u679c\u3002", "result": "GPT-4o\u751f\u6210\u7684\u9898\u76ee93.8%\u8d28\u91cf\u826f\u597d\uff0c\u9002\u54083-12\u5e74\u7ea7\u4f7f\u7528\uff0c\u4f46\u4ec542.6%\u51c6\u786e\u5339\u914d\u76ee\u6807\u63a8\u7406\u7c7b\u578b\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u4e0e\u4eba\u5de5\u5224\u65ad\u7ed3\u5408\u662f\u6269\u5c55\u9ad8\u8d28\u91cf\u8bca\u65ad\u6027\u9605\u8bfb\u7406\u89e3\u8bc4\u4f30\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.08194", "pdf": "https://arxiv.org/pdf/2506.08194", "abs": "https://arxiv.org/abs/2506.08194", "authors": ["Mateusz Michalkiewicz", "Anekha Sokhal", "Tadeusz Michalkiewicz", "Piotr Pawlikowski", "Mahsa Baktashmotlagh", "Varun Jampani", "Guha Balakrishnan"], "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "categories": ["cs.CV", "68T45", "I.5.4; I.2.10; I.3.5"], "comment": "15 pages, 4 figures", "summary": "Monocular 3D reconstruction methods and vision-language models (VLMs)\ndemonstrate impressive results on standard benchmarks, yet their true\nunderstanding of geometric properties remains unclear. We introduce GIQ , a\ncomprehensive benchmark specifically designed to evaluate the geometric\nreasoning capabilities of vision and vision-language foundation models. GIQ\ncomprises synthetic and real-world images of 224 diverse polyhedra - including\nPlatonic, Archimedean, Johnson, and Catalan solids, as well as stellations and\ncompound shapes - covering varying levels of complexity and symmetry. Through\nsystematic experiments involving monocular 3D reconstruction, 3D symmetry\ndetection, mental rotation tests, and zero-shot shape classification tasks, we\nreveal significant shortcomings in current models. State-of-the-art\nreconstruction algorithms trained on extensive 3D datasets struggle to\nreconstruct even basic geometric forms accurately. While foundation models\neffectively detect specific 3D symmetry elements via linear probing, they\nfalter significantly in tasks requiring detailed geometric differentiation,\nsuch as mental rotation. Moreover, advanced vision-language assistants exhibit\nremarkably low accuracy on complex polyhedra, systematically misinterpreting\nbasic properties like face geometry, convexity, and compound structures. GIQ is\npublicly available, providing a structured platform to highlight and address\ncritical gaps in geometric intelligence, facilitating future progress in\nrobust, geometry-aware representation learning.", "AI": {"tldr": "GIQ\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u51e0\u4f55\u7406\u89e3\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7684\u5355\u76ee3D\u91cd\u5efa\u65b9\u6cd5\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u51e0\u4f55\u5c5e\u6027\u7684\u771f\u5b9e\u7406\u89e3\u5c1a\u4e0d\u660e\u786e\u3002", "method": "GIQ\u5305\u542b224\u79cd\u591a\u6837\u591a\u9762\u4f53\u7684\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\uff0c\u901a\u8fc7\u5355\u76ee3D\u91cd\u5efa\u30013D\u5bf9\u79f0\u68c0\u6d4b\u3001\u5fc3\u7406\u65cb\u8f6c\u6d4b\u8bd5\u548c\u96f6\u6837\u672c\u5f62\u72b6\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u57fa\u672c\u51e0\u4f55\u5f62\u72b6\u91cd\u5efa\u3001\u8be6\u7ec6\u51e0\u4f55\u533a\u5206\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u89c6\u89c9\u8bed\u8a00\u52a9\u624b\u5728\u590d\u6742\u591a\u9762\u4f53\u4e0a\u51c6\u786e\u7387\u6781\u4f4e\u3002", "conclusion": "GIQ\u4e3a\u63ed\u793a\u548c\u89e3\u51b3\u51e0\u4f55\u667a\u80fd\u7684\u5173\u952e\u5dee\u8ddd\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u5e73\u53f0\uff0c\u63a8\u52a8\u672a\u6765\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u7684\u8fdb\u5c55\u3002"}}
{"id": "2506.08363", "pdf": "https://arxiv.org/pdf/2506.08363", "abs": "https://arxiv.org/abs/2506.08363", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "title": "FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs", "categories": ["cs.AI"], "comment": null, "summary": "In the architectural design process, floorplan design is often a dynamic and\niterative process. Architects progressively draw various parts of the floorplan\naccording to their ideas and requirements, continuously adjusting and refining\nthroughout the design process. Therefore, the ability to predict a complete\nfloorplan from a partial one holds significant value in the design process.\nSuch prediction can help architects quickly generate preliminary designs,\nimprove design efficiency, and reduce the workload associated with repeated\nmodifications. To address this need, we propose FloorplanMAE, a self-supervised\nlearning framework for restoring incomplete floor plans into complete ones.\nFirst, we developed a floor plan reconstruction dataset, FloorplanNet,\nspecifically trained on architectural floor plans. Secondly, we propose a floor\nplan reconstruction method based on Masked Autoencoders (MAE), which\nreconstructs missing parts by masking sections of the floor plan and training a\nlightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy\nof FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,\nwe validated the model using real sketches from the early stages of\narchitectural design. Experimental results show that the FloorplanMAE model can\ngenerate high-quality complete floor plans from incomplete partial plans. This\nframework provides a scalable solution for floor plan generation, with broad\napplication prospects.", "AI": {"tldr": "FloorplanMAE\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e0d\u5b8c\u6574\u7684\u5e73\u9762\u56fe\u9884\u6d4b\u5b8c\u6574\u5e73\u9762\u56fe\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u5728\u5efa\u7b51\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\uff0c\u5e73\u9762\u56fe\u8bbe\u8ba1\u662f\u52a8\u6001\u8fed\u4ee3\u7684\uff0c\u9884\u6d4b\u5b8c\u6574\u5e73\u9762\u56fe\u53ef\u5e2e\u52a9\u5efa\u7b51\u5e08\u5feb\u901f\u751f\u6210\u521d\u6b65\u8bbe\u8ba1\uff0c\u51cf\u5c11\u91cd\u590d\u4fee\u6539\u7684\u5de5\u4f5c\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eMasked Autoencoders (MAE)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u90e8\u5206\u5e73\u9762\u56fe\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7Vision Transformer (ViT)\u6765\u91cd\u5efa\u7f3a\u5931\u90e8\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFloorplanMAE\u80fd\u4ece\u4e0d\u5b8c\u6574\u5e73\u9762\u56fe\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b8c\u6574\u5e73\u9762\u56fe\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "FloorplanMAE\u4e3a\u5e73\u9762\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2506.08300", "pdf": "https://arxiv.org/pdf/2506.08300", "abs": "https://arxiv.org/abs/2506.08300", "authors": ["Matteo Cargnelutti", "Catherine Brobston", "John Hess", "Jack Cushman", "Kristi Mukk", "Aristana Scourtas", "Kyle Courtney", "Greg Leppert", "Amanda Watson", "Martha Whitehead", "Jonathan Zittrain"], "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.", "AI": {"tldr": "\u8fd9\u7bc7\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86Institutional Books 1.0\uff0c\u4e00\u4e2a\u57fa\u4e8e\u54c8\u4f5b\u56fe\u4e66\u9986\u516c\u5171\u9886\u57df\u4e66\u7c4d\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u9ad8\u8d28\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u62a5\u544a\u5f3a\u8c03\u4e86\u53ef\u6301\u7eed\u7ba1\u7406\u548c\u6e05\u6670\u6765\u6e90\u94fe\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5e0c\u671b\u901a\u8fc7\u63d0\u4f9b\u8fd9\u4e00\u5386\u53f2\u6587\u672c\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e0e\u54c8\u4f5b\u56fe\u4e66\u9986\u5408\u4f5c\uff0c\u63d0\u53d6\u3001\u5206\u6790\u548c\u5904\u7406\u4e86Google Books\u9879\u76ee\u4e2d\u626b\u63cf\u7684\u4e66\u7c4d\uff0c\u751f\u6210\u5305\u542bOCR\u6587\u672c\u548c\u5143\u6570\u636e\u7684\u6587\u6863\u5316\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b983,004\u5377\u516c\u5171\u9886\u57df\u4e66\u7c4d\uff0c\u603b\u8ba1242B tokens\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u8bed\u8a00\u548c\u5386\u53f2\u6587\u672c\u3002", "conclusion": "\u8be5\u9879\u76ee\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u5386\u53f2\u6587\u672c\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u4eba\u7c7b\u548c\u673a\u5668\u8fc7\u6ee4\u3001\u9605\u8bfb\u548c\u4f7f\u7528\u3002"}}
{"id": "2506.08210", "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f7f\u7528\u73b0\u4ee3\u4ec5\u89e3\u7801\u5668LLM\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u591a\u5c42\u5f52\u4e00\u5316\u5e73\u5747\u5d4c\u5165\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u73b0\u4ee3LLM\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u7684\u6f5c\u529b\uff0c\u4ee5\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efa\u6807\u51c6\u5316\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8bad\u7ec327\u4e2a\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u6587\u672c\u5d4c\u5165\u7684\u5f71\u54cd\u3002", "result": "\u591a\u5c42\u5f52\u4e00\u5316\u5e73\u5747\u5d4c\u5165\u663e\u8457\u63d0\u5347\u590d\u6742\u63d0\u793a\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u591a\u6570LLM\u8868\u73b0\u4f18\u4e8eT5\u57fa\u7ebf\u3002", "conclusion": "\u73b0\u4ee3LLM\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u80fd\u6709\u6548\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.08390", "pdf": "https://arxiv.org/pdf/2506.08390", "abs": "https://arxiv.org/abs/2506.08390", "authors": ["Leheng Sheng", "An Zhang", "Zijian Wu", "Weixiang Zhao", "Changshuo Shen", "Yi Zhang", "Xiang Wang", "Tat-Seng Chua"], "title": "On Reasoning Strength Planning in Large Reasoning Models", "categories": ["cs.AI"], "comment": null, "summary": "Recent studies empirically reveal that large reasoning models (LRMs) can\nautomatically allocate more reasoning strengths (i.e., the number of reasoning\ntokens) for harder problems, exhibiting difficulty-awareness for better task\nperformance. While this automatic reasoning strength allocation phenomenon has\nbeen widely observed, its underlying mechanism remains largely unexplored. To\nthis end, we provide explanations for this phenomenon from the perspective of\nmodel activations. We find evidence that LRMs pre-plan the reasoning strengths\nin their activations even before generation, with this reasoning strength\ncausally controlled by the magnitude of a pre-allocated directional vector.\nSpecifically, we show that the number of reasoning tokens is predictable solely\nbased on the question activations using linear probes, indicating that LRMs\nestimate the required reasoning strength in advance. We then uncover that LRMs\nencode this reasoning strength through a pre-allocated directional vector\nembedded in the activations of the model, where the vector's magnitude\nmodulates the reasoning strength. Subtracting this vector can lead to reduced\nreasoning token number and performance, while adding this vector can lead to\nincreased reasoning token number and even improved performance. We further\nreveal that this direction vector consistently yields positive reasoning length\nprediction, and it modifies the logits of end-of-reasoning token </think> to\naffect the reasoning length. Finally, we demonstrate two potential applications\nof our findings: overthinking behavior detection and enabling efficient\nreasoning on simple problems. Our work provides new insights into the internal\nmechanisms of reasoning in LRMs and offers practical tools for controlling\ntheir reasoning behaviors. Our code is available at\nhttps://github.com/AlphaLab-USTC/LRM-plans-CoT.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u80fd\u81ea\u52a8\u4e3a\u66f4\u96be\u7684\u95ee\u9898\u5206\u914d\u66f4\u591a\u63a8\u7406\u5f3a\u5ea6\uff08\u63a8\u7406\u6807\u8bb0\u6570\u91cf\uff09\uff0c\u5176\u673a\u5236\u4e0e\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u9884\u5206\u914d\u65b9\u5411\u5411\u91cf\u76f8\u5173\u3002", "motivation": "\u63a2\u7d22LRMs\u81ea\u52a8\u5206\u914d\u63a8\u7406\u5f3a\u5ea6\u7684\u5185\u90e8\u673a\u5236\uff0c\u4ee5\u7406\u89e3\u5176\u96be\u5ea6\u611f\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u6fc0\u6d3b\u5206\u6790\uff0c\u53d1\u73b0\u9884\u5206\u914d\u65b9\u5411\u5411\u91cf\u63a7\u5236\u63a8\u7406\u5f3a\u5ea6\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u9884\u6d4b\u63a8\u7406\u6807\u8bb0\u6570\u91cf\u3002", "result": "\u65b9\u5411\u5411\u91cf\u7684\u5e45\u5ea6\u8c03\u8282\u63a8\u7406\u5f3a\u5ea6\uff0c\u4fee\u6539\u5176\u53ef\u6539\u53d8\u63a8\u7406\u6807\u8bb0\u6570\u91cf\u548c\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LRMs\u63a8\u7406\u7684\u5185\u90e8\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a7\u5236\u63a8\u7406\u884c\u4e3a\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.08343", "pdf": "https://arxiv.org/pdf/2506.08343", "abs": "https://arxiv.org/abs/2506.08343", "authors": ["Chenlong Wang", "Yuanning Feng", "Dongping Chen", "Zhaoyang Chu", "Ranjay Krishna", "Tianyi Zhou"], "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.", "AI": {"tldr": "NoWait\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u663e\u5f0f\u81ea\u6211\u53cd\u601d\u6807\u8bb0\uff08\u5982\u201cWait\u201d\u548c\u201cHmm\u201d\uff09\u6765\u51cf\u5c11\u63a8\u7406\u4e2d\u7684\u5197\u4f59\u8f93\u51fa\uff0c\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u5e38\u4ea7\u751f\u5197\u4f59\u8f93\u51fa\uff0c\u5f71\u54cd\u6548\u7387\u3002\u7814\u7a76\u63a2\u8ba8\u663e\u5f0f\u81ea\u6211\u53cd\u601d\u662f\u5426\u5fc5\u8981\u3002", "method": "\u63d0\u51faNoWait\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6291\u5236\u663e\u5f0f\u81ea\u6211\u53cd\u601d\u6807\u8bb0\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNoWait\u5c06\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u51cf\u5c1127%-51%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "NoWait\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4fdd\u6301\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08214", "pdf": "https://arxiv.org/pdf/2506.08214", "abs": "https://arxiv.org/abs/2506.08214", "authors": ["Ioannis Iakovidis", "Zahra Kalantari", "Amir Hossein Payberah", "Fernando Jaramillo", "Francisco Pena Escobar"], "title": "Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation", "categories": ["cs.CV"], "comment": "16 pages, 9 figures", "summary": "In recent years the wide availability of high-resolution radar satellite\nimages along with the advancement of computer vision models have enabled the\nremote monitoring of the surface area of wetlands. However, these models\nrequire large amounts of manually annotated satellite images, which are slow\nand expensive to produce. To overcome this problem, self-supervised training\nmethods have been deployed to train models without using annotated data. In\nthis paper we use a combination of deep clustering and negative sampling to\ntrain a model to segment radar satellite images into areas that separate water\nfrom land without the use of any manual annotations. Furthermore, we implement\nan ensemble version of the model to reduce variance and improve performance.\nCompared to a single fully-supervised model using the same architecture, our\nensemble of self-supervised models achieves a 0.02 improvement in the\nIntersection Over Union metric over our test dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u805a\u7c7b\u548c\u8d1f\u91c7\u6837\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f7\u8fbe\u536b\u661f\u56fe\u50cf\u7684\u6c34\u9646\u5206\u5272\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6e7f\u5730\u76d1\u6d4b\u4e2d\u4eba\u5de5\u6807\u6ce8\u536b\u661f\u56fe\u50cf\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u805a\u7c7b\u548c\u8d1f\u91c7\u6837\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u96c6\u6210\u6a21\u578b\u4ee5\u51cf\u5c11\u65b9\u5dee\u3002", "result": "\u81ea\u76d1\u7763\u96c6\u6210\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684IOU\u6307\u6807\u6bd4\u5168\u76d1\u7763\u5355\u6a21\u578b\u63d0\u9ad8\u4e860.02\u3002", "conclusion": "\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.08399", "pdf": "https://arxiv.org/pdf/2506.08399", "abs": "https://arxiv.org/abs/2506.08399", "authors": ["Jiachen Ma", "Zhanhui Zhou", "Chao Yang", "Chaochao Lu"], "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Ensuring safe and appropriate responses from vision-language models (VLMs)\nremains a critical challenge, particularly in high-risk or ambiguous scenarios.\nWe introduce SafeCoT, a lightweight, interpretable framework that leverages\nrule-based chain-of-thought (CoT) supervision to improve refusal behavior in\nVLMs. Unlike prior methods that rely on large-scale safety annotations or\ncomplex modeling, SafeCoT uses minimal supervision to help models reason about\nsafety risks and make context-aware refusals. Experiments across multiple\nbenchmarks show that SafeCoT significantly reduces overrefusal and enhances\ngeneralization, even with limited training data. Our approach offers a scalable\nsolution for aligning VLMs with safety-critical objectives.", "AI": {"tldr": "SafeCoT\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u94fe\u5f0f\u601d\u7ef4\u76d1\u7763\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u62d2\u7edd\u884c\u4e3a\u3002", "motivation": "\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u786e\u4fdd\u5b89\u5168\u4e14\u9002\u5f53\u7684\u54cd\u5e94\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u6216\u6a21\u7cca\u573a\u666f\u4e2d\u3002", "method": "SafeCoT\u5229\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u94fe\u5f0f\u601d\u7ef4\u76d1\u7763\uff0c\u5e2e\u52a9\u6a21\u578b\u63a8\u7406\u5b89\u5168\u98ce\u9669\u5e76\u505a\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u62d2\u7edd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeCoT\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u5ea6\u62d2\u7edd\u5e76\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u3002", "conclusion": "SafeCoT\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5b89\u5168\u5173\u952e\u76ee\u6807\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08349", "pdf": "https://arxiv.org/pdf/2506.08349", "abs": "https://arxiv.org/abs/2506.08349", "authors": ["Yuxuan Zhou", "Xien Liu", "Chenwei Yan", "Chen Ning", "Xiao Zhang", "Boxun Li", "Xiangling Fu", "Shijin Wang", "Guoping Hu", "Yu Wang", "Ji Wu"], "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 11 figures. Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious medical benchmarks, but their capabilities across different cognitive\nlevels remain underexplored. Inspired by Bloom's Taxonomy, we propose a\nmulti-cognitive-level evaluation framework for assessing LLMs in the medical\ndomain in this study. The framework integrates existing medical datasets and\nintroduces tasks targeting three cognitive levels: preliminary knowledge grasp,\ncomprehensive knowledge application, and scenario-based problem solving. Using\nthis framework, we systematically evaluate state-of-the-art general and medical\nLLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.\nOur findings reveal a significant performance decline as cognitive complexity\nincreases across evaluated models, with model size playing a more critical role\nin performance at higher cognitive levels. Our study highlights the need to\nenhance LLMs' medical capabilities at higher cognitive levels and provides\ninsights for developing LLMs suited to real-world medical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBloom\u5206\u7c7b\u6cd5\u7684\u591a\u8ba4\u77e5\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u8ba4\u77e5\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8ba4\u77e5\u5c42\u6b21\u4e0a\u7684\u533b\u5b66\u80fd\u529b\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u591a\u8ba4\u77e5\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u73b0\u6709\u533b\u5b66\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u9488\u5bf9\u4e09\u4e2a\u8ba4\u77e5\u5c42\u6b21\u7684\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u516d\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u8ba4\u77e5\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u6a21\u578b\u89c4\u6a21\u5bf9\u9ad8\u8ba4\u77e5\u5c42\u6b21\u4efb\u52a1\u8868\u73b0\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u8ba4\u77e5\u5c42\u6b21\u7684\u533b\u5b66\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u9002\u5408\u5b9e\u9645\u533b\u5b66\u5e94\u7528\u7684\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2506.08220", "pdf": "https://arxiv.org/pdf/2506.08220", "abs": "https://arxiv.org/abs/2506.08220", "authors": ["Octave Mariotti", "Zhipeng Du", "Yash Bhalgat", "Oisin Mac Aodha", "Hakan Bilen"], "title": "Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence", "categories": ["cs.CV"], "comment": null, "summary": "Semantic correspondence (SC) aims to establish semantically meaningful\nmatches across different instances of an object category. We illustrate how\nrecent supervised SC methods remain limited in their ability to generalize\nbeyond sparsely annotated training keypoints, effectively acting as keypoint\ndetectors. To address this, we propose a novel approach for learning dense\ncorrespondences by lifting 2D keypoints into a canonical 3D space using\nmonocular depth estimation. Our method constructs a continuous canonical\nmanifold that captures object geometry without requiring explicit 3D\nsupervision or camera annotations. Additionally, we introduce SPair-U, an\nextension of SPair-71k with novel keypoint annotations, to better assess\ngeneralization. Experiments not only demonstrate that our model significantly\noutperforms supervised baselines on unseen keypoints, highlighting its\neffectiveness in learning robust correspondences, but that unsupervised\nbaselines outperform supervised counterparts when generalized across different\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5c062D\u5173\u952e\u70b9\u63d0\u5347\u52303D\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b66\u4e60\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u5728\u7a00\u758f\u6807\u6ce8\u5173\u952e\u70b9\u4e0a\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u5728\u7a00\u758f\u6807\u6ce8\u5173\u952e\u70b9\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5c062D\u5173\u952e\u70b9\u63d0\u5347\u52303D\u7a7a\u95f4\uff0c\u6784\u5efa\u8fde\u7eed\u89c4\u8303\u6d41\u5f62\uff0c\u65e0\u9700\u663e\u5f0f3D\u76d1\u7763\u6216\u76f8\u673a\u6807\u6ce8\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u5173\u952e\u70b9\u4e0a\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\uff0c\u4e14\u65e0\u76d1\u7763\u57fa\u7ebf\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u6cdb\u5316\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u9c81\u68d2\u5bf9\u5e94\u5173\u7cfb\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.08401", "pdf": "https://arxiv.org/pdf/2506.08401", "abs": "https://arxiv.org/abs/2506.08401", "authors": ["Runze Li", "Di Jin", "Xiaobao Wang", "Dongxiao He", "Bingdao Feng", "Zhen Wang"], "title": "Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems", "categories": ["cs.AI"], "comment": null, "summary": "Graph recommendation systems have been widely studied due to their ability to\neffectively capture the complex interactions between users and items. However,\nthese systems also exhibit certain vulnerabilities when faced with attacks. The\nprevailing shilling attack methods typically manipulate recommendation results\nby injecting a large number of fake nodes and edges. However, such attack\nstrategies face two primary challenges: low stealth and high destructiveness.\nTo address these challenges, this paper proposes a novel graph backdoor attack\nmethod that aims to enhance the exposure of target items to the target user in\na covert manner, without affecting other unrelated nodes. Specifically, we\ndesign a single-node trigger generator, which can effectively expose multiple\ntarget items to the target user by inserting only one fake user node.\nAdditionally, we introduce constraint conditions between the target nodes and\nirrelevant nodes to mitigate the impact of fake nodes on the recommendation\nsystem's performance. Experimental results show that the exposure of the target\nitems reaches no less than 50% in 99% of the target users, while the impact on\nthe recommendation system's performance is controlled within approximately 5%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u5165\u5355\u4e2a\u865a\u5047\u7528\u6237\u8282\u70b9\uff0c\u9690\u853d\u5730\u63d0\u5347\u76ee\u6807\u7269\u54c1\u5bf9\u76ee\u6807\u7528\u6237\u7684\u66dd\u5149\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u56fe\u63a8\u8350\u7cfb\u7edf\u5728\u9762\u5bf9\u653b\u51fb\u65f6\u5b58\u5728\u4f4e\u9690\u853d\u6027\u548c\u9ad8\u7834\u574f\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9690\u853d\u4e14\u7834\u574f\u6027\u66f4\u5c0f\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5355\u8282\u70b9\u89e6\u53d1\u5668\u751f\u6210\u5668\uff0c\u901a\u8fc7\u63d2\u5165\u4e00\u4e2a\u865a\u5047\u7528\u6237\u8282\u70b9\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u8282\u70b9\u4e0e\u65e0\u5173\u8282\u70b9\u4e4b\u95f4\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u51cf\u5c11\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76ee\u6807\u7269\u54c1\u5bf999%\u7684\u76ee\u6807\u7528\u6237\u66dd\u5149\u7387\u4e0d\u4f4e\u4e8e50%\uff0c\u540c\u65f6\u5bf9\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u63a7\u5236\u5728\u7ea65%\u4ee5\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u76ee\u6807\u7269\u54c1\u66dd\u5149\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u653b\u51fb\u5bf9\u63a8\u8350\u7cfb\u7edf\u7684\u7834\u574f\u6027\u3002"}}
{"id": "2506.08354", "pdf": "https://arxiv.org/pdf/2506.08354", "abs": "https://arxiv.org/abs/2506.08354", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "title": "Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "This position paper argues that the text embedding research community should\nmove beyond surface meaning and embrace implicit semantics as a central\nmodeling goal. Text embedding models have become foundational in modern NLP,\npowering a wide range of applications and drawing increasing research\nattention. Yet, much of this progress remains narrowly focused on surface-level\nsemantics. In contrast, linguistic theory emphasizes that meaning is often\nimplicit, shaped by pragmatics, speaker intent, and sociocultural context.\nCurrent embedding models are typically trained on data that lacks such depth\nand evaluated on benchmarks that reward the capture of surface meaning. As a\nresult, they struggle with tasks requiring interpretive reasoning, speaker\nstance, or social meaning. Our pilot study highlights this gap, showing that\neven state-of-the-art models perform only marginally better than simplistic\nbaselines on implicit semantics tasks. To address this, we call for a paradigm\nshift: embedding research should prioritize more diverse and linguistically\ngrounded training data, design benchmarks that evaluate deeper semantic\nunderstanding, and explicitly frame implicit meaning as a core modeling\nobjective, better aligning embeddings with real-world language complexity.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u6587\u672c\u5d4c\u5165\u7814\u7a76\u5e94\u8d85\u8d8a\u8868\u5c42\u8bed\u4e49\uff0c\u5173\u6ce8\u9690\u5f0f\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8fc7\u4e8e\u5173\u6ce8\u8868\u5c42\u8bed\u4e49\uff0c\u5ffd\u89c6\u4e86\u9690\u5f0f\u8bed\u4e49\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u5728\u9700\u8981\u6df1\u5c42\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u8bd5\u70b9\u7814\u7a76\u5c55\u793a\u73b0\u6709\u6a21\u578b\u5728\u9690\u5f0f\u8bed\u4e49\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u9690\u5f0f\u8bed\u4e49\u4efb\u52a1\u4e2d\u8868\u73b0\u4ec5\u7565\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf\u3002", "conclusion": "\u547c\u5401\u7814\u7a76\u793e\u533a\u8f6c\u5411\u66f4\u6ce8\u91cd\u9690\u5f0f\u8bed\u4e49\u7684\u5efa\u6a21\u76ee\u6807\uff0c\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2506.08227", "pdf": "https://arxiv.org/pdf/2506.08227", "abs": "https://arxiv.org/abs/2506.08227", "authors": ["Vishaal Udandarao", "Mehdi Cherti", "Shyamgopal Karthik", "Jenia Jitsev", "Samuel Albanie", "Matthias Bethge"], "title": "A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks", "categories": ["cs.CV"], "comment": null, "summary": "We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for\nmeasuring compositional understanding capabilities of vision-language models\n(VLMs). We scrutinize design choices in their construction, including data\nsource (e.g. MS-COCO) and curation procedures (e.g. constructing negative\nimages/captions), uncovering several inherent biases across most benchmarks. We\nfind that blind heuristics (e.g. token-length, log-likelihood under a language\nmodel) perform on par with CLIP models, indicating that these benchmarks do not\neffectively measure compositional understanding. We demonstrate that the\nunderlying factor is a distribution asymmetry between positive and negative\nimages/captions, induced by the benchmark construction procedures. To mitigate\nthese issues, we provide a few key recommendations for constructing more robust\nvision-language compositional understanding benchmarks, that would be less\nprone to such simple attacks.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e8617\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ec4\u5408\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5176\u8bbe\u8ba1\u4e2d\u7684\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u662f\u5426\u771f\u6b63\u8861\u91cf\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5176\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bbe\u8ba1\u9009\u62e9\uff08\u5982\u6570\u636e\u6765\u6e90\u548c\u6784\u5efa\u8fc7\u7a0b\uff09\uff0c\u5e76\u6bd4\u8f83\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0eCLIP\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5206\u5e03\u4e0d\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0eCLIP\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u672a\u80fd\u6709\u6548\u8861\u91cf\u7ec4\u5408\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\u9700\u6539\u8fdb\u4ee5\u51cf\u5c11\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u66f4\u7a33\u5065\u57fa\u51c6\u7684\u5efa\u8bae\u3002"}}
{"id": "2506.08422", "pdf": "https://arxiv.org/pdf/2506.08422", "abs": "https://arxiv.org/abs/2506.08422", "authors": ["Ikkei Itoku", "David Theil", "Evelyn Eichelsdoerfer Uehara", "Sreyoshi Bhaduri", "Junnosuke Kuroda", "Toshi Yumoto", "Alex Gil", "Natalie Perez", "Rajesh Cherukuri", "Naumaan Nayyar"], "title": "Transforming Expert Knowledge into Scalable Ontology via Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Having a unified, coherent taxonomy is essential for effective knowledge\nrepresentation in domain-specific applications as diverse terminologies need to\nbe mapped to underlying concepts. Traditional manual approaches to taxonomy\nalignment rely on expert review of concept pairs, but this becomes\nprohibitively expensive and time-consuming at scale, while subjective\ninterpretations often lead to expert disagreements. Existing automated methods\nfor taxonomy alignment have shown promise but face limitations in handling\nnuanced semantic relationships and maintaining consistency across different\ndomains. These approaches often struggle with context-dependent concept\nmappings and lack transparent reasoning processes. We propose a novel framework\nthat combines large language models (LLMs) with expert calibration and\niterative prompt optimization to automate taxonomy alignment. Our method\nintegrates expert-labeled examples, multi-stage prompt engineering, and human\nvalidation to guide LLMs in generating both taxonomy linkages and supporting\nrationales. In evaluating our framework on a domain-specific mapping task of\nconcept essentiality, we achieved an F1-score of 0.97, substantially exceeding\nthe human benchmark of 0.68. These results demonstrate the effectiveness of our\napproach in scaling taxonomy alignment while maintaining high-quality mappings\nand preserving expert oversight for ambiguous cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4e13\u5bb6\u6821\u51c6\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5206\u7c7b\u6cd5\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u5206\u7c7b\u6cd5\u5bf9\u9f50\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u5904\u7406\u8bed\u4e49\u5173\u7cfb\u548c\u8de8\u9886\u57df\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408LLMs\u3001\u4e13\u5bb6\u6821\u51c6\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\uff0c\u96c6\u6210\u4e13\u5bb6\u6807\u6ce8\u793a\u4f8b\u548c\u591a\u9636\u6bb5\u63d0\u793a\u5de5\u7a0b\uff0c\u751f\u6210\u5206\u7c7b\u6cd5\u94fe\u63a5\u53ca\u652f\u6301\u7406\u7531\u3002", "result": "\u5728\u6982\u5ff5\u91cd\u8981\u6027\u6620\u5c04\u4efb\u52a1\u4e2d\uff0cF1\u5206\u6570\u8fbe\u52300.97\uff0c\u8fdc\u8d85\u4eba\u5de5\u57fa\u51c60.68\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u9ad8\u6548\u6269\u5c55\u5206\u7c7b\u6cd5\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6620\u5c04\uff0c\u5e76\u5728\u6a21\u7cca\u6848\u4f8b\u4e2d\u4fdd\u7559\u4e13\u5bb6\u76d1\u7763\u3002"}}
{"id": "2506.08359", "pdf": "https://arxiv.org/pdf/2506.08359", "abs": "https://arxiv.org/abs/2506.08359", "authors": ["Li-Ming Zhan", "Bo Liu", "Zexin Lu", "Chengqiang Xie", "Jiannong Cao", "Xiao-Ming Wu"], "title": "DEAL: Disentangling Transformer Head Activations for LLM Steering", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Inference-time steering aims to alter the response characteristics of large\nlanguage models (LLMs) without modifying their underlying parameters. A\ncritical step in this process is the identification of internal modules within\nLLMs that are associated with the target behavior. However, current approaches\nto module selection often depend on superficial cues or ad-hoc heuristics,\nwhich can result in suboptimal or unintended outcomes. In this work, we propose\na principled causal-attribution framework for identifying behavior-relevant\nattention heads in transformers. For each head, we train a vector-quantized\nautoencoder (VQ-AE) on its attention activations, partitioning the latent space\ninto behavior-relevant and behavior-irrelevant subspaces, each quantized with a\nshared learnable codebook. We assess the behavioral relevance of each head by\nquantifying the separability of VQ-AE encodings for behavior-aligned versus\nbehavior-violating responses using a binary classification metric. This yields\na behavioral relevance score that reflects each head discriminative capacity\nwith respect to the target behavior, guiding both selection and importance\nweighting. Experiments on seven LLMs from two model families and five\nbehavioral steering datasets demonstrate that our method enables more accurate\ninference-time interventions, achieving superior performance on the\ntruthfulness-steering task. Furthermore, the heads selected by our approach\nexhibit strong zero-shot generalization in cross-domain truthfulness-steering\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5f52\u56e0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522bTransformer\u4e2d\u4e0e\u76ee\u6807\u884c\u4e3a\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668\uff08VQ-AE\uff09\u5212\u5206\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u8bc4\u4f30\u884c\u4e3a\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u7684\u63a8\u7406\u65f6\u5e72\u9884\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u6216\u4e34\u65f6\u542f\u53d1\u5f0f\uff0c\u5bfc\u81f4\u6a21\u5757\u9009\u62e9\u4e0d\u7406\u60f3\u6216\u7ed3\u679c\u504f\u79bb\u76ee\u6807\u3002", "method": "\u8bad\u7ec3VQ-AE\u5bf9\u6ce8\u610f\u529b\u5934\u7684\u6fc0\u6d3b\u8fdb\u884c\u91cf\u5316\uff0c\u5212\u5206\u884c\u4e3a\u76f8\u5173\u4e0e\u65e0\u5173\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e8c\u5143\u5206\u7c7b\u6307\u6807\u8bc4\u4f30\u884c\u4e3a\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e03\u4e2aLLM\u548c\u4e94\u4e2a\u884c\u4e3a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u6027\u5f15\u5bfc\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u9009\u62e9\u884c\u4e3a\u76f8\u5173\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u9886\u57df\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.08257", "pdf": "https://arxiv.org/pdf/2506.08257", "abs": "https://arxiv.org/abs/2506.08257", "authors": ["L. Lao Beyer", "T. Li", "X. Chen", "S. Karaman", "K. He"], "title": "Highly Compressed Tokenizer Can Generate Without Training", "categories": ["cs.CV", "cs.AI"], "comment": "Main manuscript: 9 pages, 7 figures. Appendix: 8 pages, 9 figures. To\n  appear in the Proceedings of the 42nd International Conference on Machine\n  Learning", "summary": "Commonly used image tokenizers produce a 2D grid of spatially arranged\ntokens. In contrast, so-called 1D image tokenizers represent images as highly\ncompressed one-dimensional sequences of as few as 32 discrete tokens. We find\nthat the high degree of compression achieved by a 1D tokenizer with vector\nquantization enables image editing and generative capabilities through\nheuristic manipulation of tokens, demonstrating that even very crude\nmanipulations -- such as copying and replacing tokens between latent\nrepresentations of images -- enable fine-grained image editing by transferring\nappearance and semantic attributes. Motivated by the expressivity of the 1D\ntokenizer's latent space, we construct an image generation pipeline leveraging\ngradient-based test-time optimization of tokens with plug-and-play loss\nfunctions such as reconstruction or CLIP similarity. Our approach is\ndemonstrated for inpainting and text-guided image editing use cases, and can\ngenerate diverse and realistic samples without requiring training of any\ngenerative model.", "AI": {"tldr": "1D\u56fe\u50cf\u6807\u8bb0\u5668\u901a\u8fc7\u9ad8\u5ea6\u538b\u7f29\u7684\u4e00\u7ef4\u5e8f\u5217\u8868\u793a\u56fe\u50cf\uff0c\u652f\u6301\u542f\u53d1\u5f0f\u6807\u8bb0\u64cd\u4f5c\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\uff0c\u65e0\u9700\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u63a2\u7d221D\u56fe\u50cf\u6807\u8bb0\u5668\u5728\u9ad8\u5ea6\u538b\u7f29\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u7cbe\u7ec6\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528\u5411\u91cf\u91cf\u5316\u76841D\u6807\u8bb0\u5668\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u64cd\u4f5c\u548c\u68af\u5ea6\u4f18\u5316\u7684\u6d4b\u8bd5\u65f6\u6807\u8bb0\u8c03\u6574\uff0c\u7ed3\u5408\u91cd\u5efa\u6216CLIP\u76f8\u4f3c\u6027\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u548c\u903c\u771f\u7684\u56fe\u50cf\u751f\u6210\uff0c\u652f\u6301\u4fee\u590d\u548c\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u3002", "conclusion": "1D\u6807\u8bb0\u5668\u7684\u9ad8\u538b\u7f29\u6027\u548c\u8868\u8fbe\u6027\u4e3a\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.08424", "pdf": "https://arxiv.org/pdf/2506.08424", "abs": "https://arxiv.org/abs/2506.08424", "authors": ["Yong Liang Goh", "Zhiguang Cao", "Yining Ma", "Jianan Zhou", "Mohammad Haroon Dupty", "Wee Sun Lee"], "title": "SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy", "categories": ["cs.AI"], "comment": "Accepted in the 42nd International Conference of Machine Learning\n  (ICML)", "summary": "Recent advances toward foundation models for routing problems have shown\ngreat potential of a unified deep model for various VRP variants. However, they\noverlook the complex real-world customer distributions. In this work, we\nadvance the Multi-Task VRP (MTVRP) setting to the more realistic yet\nchallenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce\nSHIELD, a novel model that leverages both sparsity and hierarchy principles.\nBuilding on a deeper decoder architecture, we first incorporate the\nMixture-of-Depths (MoD) technique to enforce sparsity. This improves both\nefficiency and generalization by allowing the model to dynamically select nodes\nto use or skip each decoder layer, providing the needed capacity to adaptively\nallocate computation for learning the task/distribution specific and shared\nrepresentations. We also develop a context-based clustering layer that exploits\nthe presence of hierarchical structures in the problems to produce better local\nrepresentations. These two designs inductively bias the network to identify key\nfeatures that are common across tasks and distributions, leading to\nsignificantly improved generalization on unseen ones. Our empirical results\ndemonstrate the superiority of our approach over existing methods on 9\nreal-world maps with 16 VRP variants each.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSHIELD\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u66f4\u73b0\u5b9e\u4e14\u590d\u6742\u7684\u591a\u4efb\u52a1\u591a\u5206\u5e03\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08MTMDVRP\uff09\uff0c\u901a\u8fc7\u7a00\u758f\u6027\u548c\u5c42\u6b21\u6027\u8bbe\u8ba1\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5ffd\u89c6\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u5ba2\u6237\u5206\u5e03\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u4efb\u52a1\u591a\u5206\u5e03\u573a\u666f\u3002", "method": "\u7ed3\u5408Mixture-of-Depths\u6280\u672f\u5b9e\u73b0\u7a00\u758f\u6027\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u805a\u7c7b\u5c42\u4ee5\u5229\u7528\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u572816\u79cdVRP\u53d8\u4f53\u76849\u4e2a\u771f\u5b9e\u5730\u56fe\u4e0a\uff0cSHIELD\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SHIELD\u901a\u8fc7\u7a00\u758f\u6027\u548c\u5c42\u6b21\u6027\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u548c\u5206\u5e03\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2506.08364", "pdf": "https://arxiv.org/pdf/2506.08364", "abs": "https://arxiv.org/abs/2506.08364", "authors": ["Jash Rajesh Parekh", "Pengcheng Jiang", "Jiawei Han"], "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails.", "AI": {"tldr": "CC-RAG\u901a\u8fc7\u6574\u5408\u96f6\u6837\u672c\u4e09\u5143\u7ec4\u63d0\u53d6\u548c\u4e3b\u9898\u611f\u77e5\u56fe\u94fe\uff0c\u6539\u8fdb\u4e86RAG\u7ba1\u9053\uff0c\u4ee5\u5efa\u6a21\u56e0\u679c\u5173\u7cfb\uff0c\u63d0\u5347LLMs\u5728\u4e13\u4e1a\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6807\u51c6RAG\u7ba1\u9053\u7f3a\u4e4f\u5bf9\u56e0\u679c\u5173\u7cfb\u7684\u7ed3\u6784\u5316\u5efa\u6a21\uff0c\u5bfc\u81f4\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "CC-RAG\u901a\u8fc7\u6784\u5efaDAG\uff08\u6709\u5411\u65e0\u73af\u56fe\uff09\u5e76\u4f7f\u7528\u524d\u5411/\u540e\u5411\u94fe\u8fdb\u884c\u7ed3\u6784\u5316\u591a\u8df3\u63a8\u7406\u3002", "result": "\u5728\u6bd4\u7279\u5e01\u4ef7\u683c\u6ce2\u52a8\u548c\u9ad8\u96ea\u6c0f\u75c5\u4e24\u4e2a\u9886\u57df\u4e2d\uff0cCC-RAG\u5728\u94fe\u76f8\u4f3c\u6027\u3001\u4fe1\u606f\u5bc6\u5ea6\u548c\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u6807\u51c6RAG\u548c\u96f6\u6837\u672cLLMs\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u56e0\u679c\u7ed3\u6784\u4f7fLLMs\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u751f\u6210\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u54cd\u5e94\u3002"}}
{"id": "2506.08279", "pdf": "https://arxiv.org/pdf/2506.08279", "abs": "https://arxiv.org/abs/2506.08279", "authors": ["Aditi Sundararaman", "Amogh Adishesha", "Andrew Jaegle", "Dan Bigioi", "Hyoung-Kyu Song", "Jon Kyl", "Justin Mao", "Kevin Lan", "Mojtaba Komeili", "ShahRukh Athar", "Sheila Babayan", "Stanislau Beliasau", "William Buchwalter"], "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Technical report website: mirage.app/research/seeing-voices, product\n  website: mirage.app", "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links).", "AI": {"tldr": "Mirage\u662f\u4e00\u79cd\u97f3\u9891\u5230\u89c6\u9891\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u97f3\u9891\u8f93\u5165\u751f\u6210\u903c\u771f\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u5c24\u5176\u64c5\u957f\u751f\u6210\u4eba\u7269\u8bb2\u8bdd\u7684\u89c6\u9891\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u97f3\u9891\u4e13\u6ce8\u4e8e\u65e0\u58f0\u56fe\u50cf\u5e8f\u5217\u751f\u6210\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u5e94\u7528\u9886\u57df\uff08\u5982\u91cd\u65b0\u914d\u97f3\uff09\u3002Mirage\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u97f3\u9891\u4e0e\u89c6\u9891\u7684\u548c\u8c10\u96c6\u6210\u3002", "method": "Mirage\u91c7\u7528\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u7edf\u4e00\u8bad\u7ec3\u65b9\u6cd5\uff0c\u652f\u6301\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u6216\u57fa\u4e8e\u73b0\u6709\u6743\u91cd\u8bad\u7ec3\uff0c\u4fdd\u7559\u4e86\u97f3\u9891\u5230\u89c6\u9891\u751f\u6210\u7684\u901a\u7528\u6027\u3002", "result": "Mirage\u751f\u6210\u7684\u89c6\u9891\u5728\u4e3b\u89c2\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u903c\u771f\u5730\u5448\u73b0\u4eba\u7269\u8bb2\u8bdd\u7684\u8868\u73b0\u529b\u3002", "conclusion": "Mirage\u4e3a\u97f3\u9891\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4eba\u7269\u8bb2\u8bdd\u5185\u5bb9\u7684\u751f\u6210\u3002"}}
{"id": "2506.08446", "pdf": "https://arxiv.org/pdf/2506.08446", "abs": "https://arxiv.org/abs/2506.08446", "authors": ["Peng-Yuan Wang", "Tian-Shuo Liu", "Chenyang Wang", "Yi-Di Wang", "Shu Yan", "Cheng-Xing Jia", "Xu-Hui Liu", "Xin-Wei Chen", "Jia-Cheng Xu", "Ziniu Li", "Yang Yu"], "title": "A Survey on Large Language Models for Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical reasoning has long represented one of the most fundamental and\nchallenging frontiers in artificial intelligence research. In recent years,\nlarge language models (LLMs) have achieved significant advances in this area.\nThis survey examines the development of mathematical reasoning abilities in\nLLMs through two high-level cognitive phases: comprehension, where models gain\nmathematical understanding via diverse pretraining strategies, and answer\ngeneration, which has progressed from direct prediction to step-by-step\nChain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical\nreasoning, ranging from training-free prompting to fine-tuning approaches such\nas supervised fine-tuning and reinforcement learning, and discuss recent work\non extended CoT and \"test-time scaling\". Despite notable progress, fundamental\nchallenges remain in terms of capacity, efficiency, and generalization. To\naddress these issues, we highlight promising research directions, including\nadvanced pretraining and knowledge augmentation techniques, formal reasoning\nframeworks, and meta-generalization through principled learning paradigms. This\nsurvey tries to provide some insights for researchers interested in enhancing\nreasoning capabilities of LLMs and for those seeking to apply these techniques\nto other domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u53d1\u5c55\uff0c\u5206\u4e3a\u7406\u89e3\u548c\u7b54\u6848\u751f\u6210\u4e24\u4e2a\u8ba4\u77e5\u9636\u6bb5\uff0c\u5e76\u63a2\u8ba8\u4e86\u63d0\u5347\u65b9\u6cd5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6570\u5b66\u63a8\u7406\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u7684\u57fa\u7840\u6311\u6218\uff0cLLMs\u8fd1\u5e74\u5728\u8be5\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u6709\u5bb9\u91cf\u3001\u6548\u7387\u548c\u6cdb\u5316\u7b49\u95ee\u9898\u9700\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u7406\u89e3\u80fd\u529b\uff0c\u4ece\u76f4\u63a5\u9884\u6d4b\u5230\u5206\u6b65\u63a8\u7406\uff08CoT\uff09\uff0c\u5e76\u91c7\u7528\u8bad\u7ec3\u65e0\u5173\u63d0\u793a\u548c\u5fae\u8c03\u7b49\u65b9\u6cd5\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5c3d\u7ba1LLMs\u5728\u6570\u5b66\u63a8\u7406\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u5bb9\u91cf\u3001\u6548\u7387\u548c\u6cdb\u5316\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u9ad8\u7ea7\u9884\u8bad\u7ec3\u3001\u77e5\u8bc6\u589e\u5f3a\u3001\u5f62\u5f0f\u63a8\u7406\u6846\u67b6\u548c\u5143\u6cdb\u5316\uff0c\u4e3a\u63d0\u5347LLMs\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2506.08371", "pdf": "https://arxiv.org/pdf/2506.08371", "abs": "https://arxiv.org/abs/2506.08371", "authors": ["Zikai Xiao", "Ziyang Wang", "Wen Ma", "Yan Zhang", "Wei Shen", "Yan Wang", "Luqi Gong", "Zuozhu Liu"], "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) support long contexts, they struggle with\nperformance degradation within the context window. Current solutions incur\nprohibitive training costs, leaving statistical behaviors and cost-effective\napproaches underexplored. From the decoding perspective, we identify the\nPosterior Salience Attenuation (PSA) phenomenon, where the salience ratio\ncorrelates with long-text performance degradation. Notably, despite the\nattenuation, gold tokens still occupy high-ranking positions in the decoding\nspace. Motivated by it, we propose the training-free Positional Contrastive\nDecoding (PCD) that contrasts the logits derived from long-aware attention with\nthose from designed local-aware attention, enabling the model to focus on the\ngains introduced by large-scale short-to-long training. Through the analysis of\nlong-term decay simulation, we demonstrate that PCD effectively alleviates\nattention score degradation. Experimental results show that PCD achieves\nstate-of-the-art performance on long-context benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684Positional Contrastive Decoding (PCD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u957f\u6587\u672c\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u751f\u6210\u7684logits\uff0c\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u5b58\u5728\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4e14\u7edf\u8ba1\u884c\u4e3a\u548c\u6210\u672c\u6548\u76ca\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPCD\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u957f\u6587\u672c\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u7684logits\uff0c\u5229\u7528\u5927\u89c4\u6a21\u77ed\u5230\u957f\u8bad\u7ec3\u7684\u4f18\u52bf\u3002", "result": "PCD\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u6ce8\u610f\u529b\u5206\u6570\u9000\u5316\u3002", "conclusion": "PCD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.08297", "pdf": "https://arxiv.org/pdf/2506.08297", "abs": "https://arxiv.org/abs/2506.08297", "authors": ["Nhat Thanh Tran", "Fanghui Xue", "Shuai Zhang", "Jiancheng Lyu", "Yunling Zheng", "Yingyong Qi", "Jack Xin"], "title": "SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, figures 3", "summary": "Attention is the critical component of a transformer. Yet the quadratic\ncomputational complexity of vanilla full attention in the input size and the\ninability of its linear attention variant to focus have been challenges for\ncomputer vision tasks. We provide a mathematical definition of generalized\nattention and formulate both vanilla softmax attention and linear attention\nwithin the general framework. We prove that generalized attention disperses,\nthat is, as the number of keys tends to infinity, the query assigns equal\nweights to all keys. Motivated by the dispersion property and recent\ndevelopment of Mamba form of attention, we design Scalable and Efficient Mamba\nlike Attention (SEMA) which utilizes token localization to avoid dispersion and\nmaintain focusing, complemented by theoretically consistent arithmetic\naveraging to capture global aspect of attention. We support our approach on\nImagenet-1k where classification results show that SEMA is a scalable and\neffective alternative beyond linear attention, outperforming recent vision\nMamba models on increasingly larger scales of images at similar model parameter\nsizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEMA\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u65e0\u6cd5\u805a\u7126\u7684\u95ee\u9898\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u5b58\u5728\u6311\u6218\uff0c\u7ebf\u6027\u6ce8\u610f\u529b\u65e0\u6cd5\u6709\u6548\u805a\u7126\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u8ba1\u7b97\u53c8\u80fd\u4fdd\u6301\u805a\u7126\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5b9a\u4e49\u5e7f\u4e49\u6ce8\u610f\u529b\uff0c\u8bbe\u8ba1SEMA\u673a\u5236\uff0c\u5229\u7528token\u5b9a\u4f4d\u907f\u514d\u5206\u6563\uff0c\u5e76\u901a\u8fc7\u7b97\u672f\u5e73\u5747\u6355\u83b7\u5168\u5c40\u6ce8\u610f\u529b\u3002", "result": "\u5728Imagenet-1k\u4e0a\uff0cSEMA\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u8fd1\u671f\u89c6\u89c9Mamba\u6a21\u578b\u3002", "conclusion": "SEMA\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u56fe\u50cf\u4efb\u52a1\u3002"}}
{"id": "2506.08462", "pdf": "https://arxiv.org/pdf/2506.08462", "abs": "https://arxiv.org/abs/2506.08462", "authors": ["Christos Margadji", "Sebastian W. Pattinson"], "title": "Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing", "categories": ["cs.AI", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Industrial processes must be robust and adaptable, as environments and tasks\nare often unpredictable, while operational errors remain costly and difficult\nto detect. AI-based control systems offer a path forward, yet typically depend\non supervised learning with extensive labelled datasets, which limits their\nability to generalize across variable and data-scarce industrial settings.\nFoundation models could enable broader reasoning and knowledge integration, but\nrarely deliver the quantitative precision demanded by engineering applications.\nHere, we introduceControl and Interpretation of Production via Hybrid Expertise\nand Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming\nto replicate human-like reasoning for industrial control, instantiated in a\ncommercial-grade 3D printer. It integrates a process expert, a regression model\nenabling quantitative characterization of system states required for\nengineering tasks. CIPHER also incorporates retrieval-augmented generation to\naccess external expert knowledge and support physics-informed, chain-of-thought\nreasoning. This hybrid architecture exhibits strong generalization to\nout-of-distribution tasks. It interprets visual or textual inputs from process\nmonitoring, explains its decisions, and autonomously generates precise machine\ninstructions, without requiring explicit annotations. CIPHER thus lays the\nfoundations for autonomous systems that act with precision, reason with\ncontext, and communicate decisions transparently, supporting safe and trusted\ndeployment in industrial settings.", "AI": {"tldr": "CIPHER\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7684\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u63a7\u5236\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u77e5\u8bc6\u548c\u63a8\u7406\u5b9e\u73b0\u4eba\u7c7b\u5f0f\u51b3\u7b56\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u73af\u5883\u3002", "motivation": "\u5de5\u4e1a\u8fc7\u7a0b\u9700\u8981\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f46\u4f20\u7edfAI\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u5de5\u7a0b\u7cbe\u5ea6\u3002", "method": "CIPHER\u6574\u5408\u8fc7\u7a0b\u4e13\u5bb6\u3001\u56de\u5f52\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u652f\u6301\u7269\u7406\u77e5\u8bc6\u9a71\u52a8\u7684\u94fe\u5f0f\u63a8\u7406\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u3002", "result": "\u6a21\u578b\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u89e3\u91ca\u51b3\u7b56\u5e76\u751f\u6210\u7cbe\u786e\u6307\u4ee4\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u3002", "conclusion": "CIPHER\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cbe\u786e\u3001\u900f\u660e\u548c\u53ef\u4fe1\u7684\u5de5\u4e1a\u90e8\u7f72\u57fa\u7840\u3002"}}
{"id": "2506.08373", "pdf": "https://arxiv.org/pdf/2506.08373", "abs": "https://arxiv.org/abs/2506.08373", "authors": ["Kevin Galim", "Ethan Ewer", "Wonjun Kang", "Minjae Lee", "Hyung Il Koo", "Kangwook Lee"], "title": "Draft-based Approximate Inference for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u4f18\u5316\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u5305\u62ecSpecKV\u548cSpecPC\u4e24\u79cd\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8eTransformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u548c\u7ebf\u6027\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u4f18\u5316\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7565\u7684\u9884\u6d4b\uff0c\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8349\u7a3f\u6a21\u578b\u9884\u6d4btoken\u548cKV\u5bf9\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51faSpecKV\uff08\u4f18\u5316KV\u7f13\u5b58\u4e22\u5f03\uff09\u548cSpecPC\uff08\u538b\u7f29\u63d0\u793atoken\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u6027\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u3001\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u7684\u6539\u8fdb\u3002", "conclusion": "\u8349\u7a3f\u6a21\u578b\u9996\u6b21\u7528\u4e8e\u8fd1\u4f3cLLM\u63a8\u7406\u52a0\u901f\uff0c\u6269\u5c55\u4e86\u5176\u7528\u9014\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.08299", "pdf": "https://arxiv.org/pdf/2506.08299", "abs": "https://arxiv.org/abs/2506.08299", "authors": ["Kangning Yang", "Ling Ouyang", "Huiming Sun", "Jie Cai", "Lan Fu", "Jiaming Ding", "Chiu Man Ho", "Zibo Meng"], "title": "OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal", "categories": ["cs.CV"], "comment": null, "summary": "Reflection removal technology plays a crucial role in photography and\ncomputer vision applications. However, existing techniques are hindered by the\nlack of high-quality in-the-wild datasets. In this paper, we propose a novel\nparadigm for collecting reflection datasets from a fresh perspective. Our\napproach is convenient, cost-effective, and scalable, while ensuring that the\ncollected data pairs are of high quality, perfectly aligned, and represent\nnatural and diverse scenarios. Following this paradigm, we collect a\nReal-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which\ncontains 1,000 high-quality transmission-reflection image pairs collected in\nthe wild. Through the analysis of several reflection removal methods and\nbenchmark evaluation experiments on our dataset, we demonstrate its\neffectiveness in improving robustness in challenging real-world environments.\nOur dataset is available at https://github.com/caijie0620/OpenRR-1k.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cd\u5c04\u6570\u636e\u96c6\u6536\u96c6\u8303\u5f0f\uff0c\u5e76\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684OpenRR-1k\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u53cd\u5c04\u53bb\u9664\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u53cd\u5c04\u53bb\u9664\u6280\u672f\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u800c\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4fbf\u6377\u3001\u7ecf\u6d4e\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u6536\u96c6\u8303\u5f0f\uff0c\u786e\u4fdd\u6570\u636e\u5bf9\u9ad8\u8d28\u91cf\u3001\u5b8c\u7f8e\u5bf9\u9f50\u4e14\u591a\u6837\u5316\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86OpenRR-1k\u6570\u636e\u96c6\u3002", "result": "OpenRR-1k\u6570\u636e\u96c6\u5305\u542b1000\u5bf9\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u5bf9\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u63d0\u5347\u53cd\u5c04\u53bb\u9664\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "OpenRR-1k\u6570\u636e\u96c6\u4e3a\u53cd\u5c04\u53bb\u9664\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u573a\u666f\u6570\u636e\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.08486", "pdf": "https://arxiv.org/pdf/2506.08486", "abs": "https://arxiv.org/abs/2506.08486", "authors": ["Rahatara Ferdousi", "M Anwar Hossain"], "title": "RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being", "categories": ["cs.AI", "68T50, 92C50, 68T01 68T50, 92C50, 68T01 68T50, 92C50, 68T01", "I.2.7; J.3; I.5.1"], "comment": "18 pages, 12 figures, IEEE EMBS JBHI", "summary": "The rise of large language models (LLMs) has created new possibilities for\ndigital twins in healthcare. However, the deployment of such systems in\nconsumer health contexts raises significant concerns related to hallucination,\nbias, lack of transparency, and ethical misuse. In response to recommendations\nfrom health authorities such as the World Health Organization (WHO), we propose\nResponsible Health Twin (RHealthTwin), a principled framework for building and\ngoverning AI-powered digital twins for well-being assistance. RHealthTwin\nprocesses multimodal inputs that guide a health-focused LLM to produce safe,\nrelevant, and explainable responses. At the core of RHealthTwin is the\nResponsible Prompt Engine (RPE), which addresses the limitations of traditional\nLLM configuration. Conventionally, users input unstructured prompt and the\nsystem instruction to configure the LLM, which increases the risk of\nhallucination. In contrast, RPE extracts predefined slots dynamically to\nstructure both inputs. This guides the language model to generate responses\nthat are context aware, personalized, fair, reliable, and explainable for\nwell-being assistance. The framework further adapts over time through a\nfeedback loop that updates the prompt structure based on user satisfaction. We\nevaluate RHealthTwin across four consumer health domains including mental\nsupport, symptom triage, nutrition planning, and activity coaching. RPE\nachieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and\nBERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical\ncompliance and instruction-following metrics using LLM-as-judge evaluation,\noutperforming baseline strategies. We envision RHealthTwin as a forward-looking\nfoundation for responsible LLM-based applications in health and well-being.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRHealthTwin\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u3001\u504f\u89c1\u548c\u4f26\u7406\u95ee\u9898\u3002\u901a\u8fc7\u52a8\u6001\u7ed3\u6784\u5316\u8f93\u5165\u548c\u53cd\u9988\u673a\u5236\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u54cd\u5e94\u7684\u5b89\u5168\u6027\u3001\u76f8\u5173\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u3001\u504f\u89c1\u3001\u900f\u660e\u5ea6\u548c\u4f26\u7406\u95ee\u9898\u3002\u4e3a\u54cd\u5e94\u4e16\u754c\u536b\u751f\u7ec4\u7ec7\u7b49\u673a\u6784\u7684\u5efa\u8bae\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d1f\u8d23\u4efb\u7684\u6280\u672f\u6846\u67b6\u3002", "method": "RHealthTwin\u6846\u67b6\u901a\u8fc7Responsible Prompt Engine\uff08RPE\uff09\u52a8\u6001\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\uff0c\u5e76\u7ed3\u5408\u53cd\u9988\u673a\u5236\u4f18\u5316\u63d0\u793a\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u5065\u5eb7\u9886\u57df\uff08\u5fc3\u7406\u652f\u6301\u3001\u75c7\u72b6\u5206\u8bca\u3001\u8425\u517b\u89c4\u5212\u548c\u6d3b\u52a8\u6307\u5bfc\uff09\u4e2d\uff0cRPE\u5728BLEU\u3001ROUGE-L\u548cBERTScore\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f26\u7406\u5408\u89c4\u548c\u6307\u4ee4\u9075\u5faa\u7387\u8d85\u8fc790%\u3002", "conclusion": "RHealthTwin\u4e3a\u8d1f\u8d23\u4efb\u5730\u5e94\u7528LLM\u4e8e\u5065\u5eb7\u548c\u798f\u7949\u9886\u57df\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u57fa\u7840\u3002"}}
{"id": "2506.08375", "pdf": "https://arxiv.org/pdf/2506.08375", "abs": "https://arxiv.org/abs/2506.08375", "authors": ["Tao Zou", "Xinghua Zhang", "Haiyang Yu", "Minzheng Wang", "Fei Huang", "Yongbin Li"], "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "24 pages", "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EIFBENCH\u57fa\u51c6\u548cSegPO\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u9700\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1EIFBENCH\u57fa\u51c6\uff0c\u5305\u542b\u591a\u4efb\u52a1\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u5e76\u63d0\u51faSegPO\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "result": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "EIFBENCH\u548cSegPO\u4e3aLLM\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u65b9\u5411\u3002"}}
{"id": "2506.08324", "pdf": "https://arxiv.org/pdf/2506.08324", "abs": "https://arxiv.org/abs/2506.08324", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15155,\n  arXiv:2504.13045, arXiv:2503.23472", "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more effectively extract\nand fuse spatial context with fine spectral information in hyperspectral image\n(HSI) classification, this paper proposes a novel network architecture called\nSTNet. The core advantage of STNet stems from the dual innovative design of its\nSpatial-Spectral Transformer module: first, the fundamental explicit decoupling\nof spatial and spectral attention ensures targeted capture of key information\nin HSI; second, two functionally distinct gating mechanisms perform intelligent\nregulation at both the fusion level of attention flows (adaptive attention\nfusion gating) and the internal level of feature transformation (GFFN). This\ncharacteristic demonstrates superior feature extraction and fusion capabilities\ncompared to traditional convolutional neural networks, while reducing\noverfitting risks in small-sample and high-noise scenarios. STNet enhances\nmodel representation capability without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTNet\u7684\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u7a7a\u95f4-\u5149\u8c31Transformer\u6a21\u5757\u7684\u521b\u65b0\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u9ad8\u7ef4\u6570\u636e\u3001\u5730\u7269\u7a00\u758f\u5206\u5e03\u548c\u5149\u8c31\u5197\u4f59\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86STNet\uff0c\u5176\u6838\u5fc3\u662f\u7a7a\u95f4-\u5149\u8c31Transformer\u6a21\u5757\uff0c\u901a\u8fc7\u89e3\u8026\u7a7a\u95f4\u548c\u5149\u8c31\u6ce8\u610f\u529b\u53ca\u53cc\u95e8\u63a7\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u3002", "result": "\u5728IN\u3001UP\u548cKSC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5c0f\u6837\u672c\u548c\u9ad8\u566a\u58f0\u573a\u666f\u7684\u8fc7\u62df\u5408\u98ce\u9669\u3002", "conclusion": "STNet\u5728\u4e0d\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\u6216\u5bbd\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u6a21\u578b\u8868\u793a\u80fd\u529b\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08518", "pdf": "https://arxiv.org/pdf/2506.08518", "abs": "https://arxiv.org/abs/2506.08518", "authors": ["Sunny Gupta", "Nikita Jangid", "Shounak Das", "Amit Sethi"], "title": "FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching", "categories": ["cs.AI", "cs.CV", "cs.LG", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows CFAgentic @ ICML'25", "summary": "Domain Generalization (DG) seeks to train models that perform reliably on\nunseen target domains without access to target data during training. While\nrecent progress in smoothing the loss landscape has improved generalization,\nexisting methods often falter under long-tailed class distributions and\nconflicting optimization objectives. We introduce FedTAIL, a federated domain\ngeneralization framework that explicitly addresses these challenges through\nsharpness-guided, gradient-aligned optimization. Our method incorporates a\ngradient coherence regularizer to mitigate conflicts between classification and\nadversarial objectives, leading to more stable convergence. To combat class\nimbalance, we perform class-wise sharpness minimization and propose a\ncurvature-aware dynamic weighting scheme that adaptively emphasizes\nunderrepresented tail classes. Furthermore, we enhance conditional distribution\nalignment by integrating sharpness-aware perturbations into entropy\nregularization, improving robustness under domain shift. FedTAIL unifies\noptimization harmonization, class-aware regularization, and conditional\nalignment into a scalable, federated-compatible framework. Extensive\nevaluations across standard domain generalization benchmarks demonstrate that\nFedTAIL achieves state-of-the-art performance, particularly in the presence of\ndomain shifts and label imbalance, validating its effectiveness in both\ncentralized and federated settings. Code: https://github.com/sunnyinAI/FedTail", "AI": {"tldr": "FedTAIL\u662f\u4e00\u4e2a\u8054\u90a6\u9886\u57df\u6cdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5bf9\u9f50\u4f18\u5316\u548c\u7c7b\u611f\u77e5\u6b63\u5219\u5316\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u548c\u4f18\u5316\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u7c7b\u5206\u5e03\u548c\u51b2\u7a81\u4f18\u5316\u76ee\u6807\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0cFedTAIL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u68af\u5ea6\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3001\u7c7b\u611f\u77e5\u9510\u5ea6\u6700\u5c0f\u5316\u548c\u66f2\u7387\u611f\u77e5\u52a8\u6001\u52a0\u6743\uff0c\u589e\u5f3a\u6761\u4ef6\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u5728\u6807\u51c6\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedTAIL\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FedTAIL\u5728\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\uff0c\u5c24\u5176\u5728\u9886\u57df\u504f\u79fb\u548c\u6807\u7b7e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.08400", "pdf": "https://arxiv.org/pdf/2506.08400", "abs": "https://arxiv.org/abs/2506.08400", "authors": ["Luel Hagos Beyene", "Vivek Verma", "Min Ma", "Jesujoba O. Alabi", "Fabian David Schmidt", "Joyce Nakatumba-Nabende", "David Ifeoluwa Adelani"], "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "working paper", "summary": "Large Language models (LLMs) have demonstrated impressive performance on a\nwide range of tasks, including in multimodal settings such as speech. However,\ntheir evaluation is often limited to English and a few high-resource languages.\nFor low-resource languages, there is no standardized evaluation benchmark. In\nthis paper, we address this gap by introducing mSTEB, a new benchmark to\nevaluate the performance of LLMs on a wide range of tasks covering language\nidentification, text classification, question answering, and translation tasks\non both speech and text modalities. We evaluated the performance of leading\nLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open\nmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in\nperformance between high-resource and low-resource languages, especially for\nlanguages spoken in Africa and Americas/Oceania. Our findings show that more\ninvestment is needed to address their under-representation in LLMs coverage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86mSTEB\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9ad8\u8d44\u6e90\u4e0e\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709LLMs\u8bc4\u4f30\u591a\u96c6\u4e2d\u4e8e\u82f1\u8bed\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "method": "\u5f15\u5165mSTEB\u57fa\u51c6\uff0c\u6db5\u76d6\u8bed\u8a00\u8bc6\u522b\u3001\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u548c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86Gemini 2.0 Flash\u3001GPT-4o (Audio)\u7b49\u6a21\u578b\u3002", "result": "\u9ad8\u8d44\u6e90\u4e0e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5c24\u5176\u662f\u975e\u6d32\u548c\u7f8e\u6d32/\u5927\u6d0b\u6d32\u8bed\u8a00\uff09\u95f4\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u3002", "conclusion": "\u9700\u66f4\u591a\u6295\u8d44\u4ee5\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728LLMs\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2506.08327", "pdf": "https://arxiv.org/pdf/2506.08327", "abs": "https://arxiv.org/abs/2506.08327", "authors": ["Yuto Kase", "Kai Ishibe", "Ryoma Yasuda", "Yudai Washida", "Sakiko Hashimoto"], "title": "Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera", "categories": ["cs.CV"], "comment": "17 pages, 10 figures, 3 tables", "summary": "In racket sports, such as tennis, locating the ball's position at impact is\nimportant in clarifying player and equipment characteristics, thereby aiding in\npersonalized equipment design. High-speed cameras are used to measure the\nimpact location; however, their excessive memory consumption limits prolonged\nscene capture, and manual digitization for position detection is time-consuming\nand prone to human error. These limitations make it difficult to effectively\ncapture the entire playing scene, hindering the ability to analyze the player's\nperformance. We propose a method for locating the tennis ball impact on the\nracket in real time using an event camera. Event cameras efficiently measure\nbrightness changes (called `events') with microsecond accuracy under high-speed\nmotion while using lower memory consumption. These cameras enable users to\ncontinuously monitor their performance over extended periods. Our method\nconsists of three identification steps: time range of swing, timing at impact,\nand contours of ball and racket. Conventional computer vision techniques are\nutilized along with an original event-based processing to detect the timing at\nimpact (PATS: the amount of polarity asymmetry in time symmetry). The results\nof the experiments were within the permissible range for measuring tennis\nplayers' performance. Moreover, the computation time was sufficiently short for\nreal-time applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u5b9e\u65f6\u5b9a\u4f4d\u7f51\u7403\u62cd\u51fb\u7403\u70b9\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u901f\u76f8\u673a\u5185\u5b58\u6d88\u8017\u5927\u548c\u624b\u52a8\u6570\u5b57\u5316\u8017\u65f6\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u7f51\u7403\u7b49\u7403\u62cd\u8fd0\u52a8\u4e2d\uff0c\u51fb\u7403\u70b9\u5b9a\u4f4d\u5bf9\u5206\u6790\u7403\u5458\u8868\u73b0\u548c\u4e2a\u6027\u5316\u88c5\u5907\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5185\u5b58\u6d88\u8017\u5927\u548c\u4eba\u5de5\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u9ad8\u6548\u6355\u6349\u4eae\u5ea6\u53d8\u5316\uff0c\u7ed3\u5408\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u548c\u539f\u521b\u4e8b\u4ef6\u5904\u7406\uff08PATS\uff09\u5206\u4e09\u6b65\u8bc6\u522b\u51fb\u7403\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5728\u6d4b\u91cf\u7f51\u7403\u8fd0\u52a8\u5458\u8868\u73b0\u7684\u5141\u8bb8\u8303\u56f4\u5185\uff0c\u8ba1\u7b97\u65f6\u95f4\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u65f6\u5b9a\u4f4d\u51fb\u7403\u70b9\uff0c\u9002\u7528\u4e8e\u957f\u65f6\u95f4\u76d1\u6d4b\u7403\u5458\u8868\u73b0\u3002"}}
{"id": "2506.08532", "pdf": "https://arxiv.org/pdf/2506.08532", "abs": "https://arxiv.org/abs/2506.08532", "authors": ["Yanwei Gong", "Xiaolin Chang"], "title": "Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness", "categories": ["cs.AI"], "comment": null, "summary": "The rapid growth of the low-altitude economy has driven the widespread\nadoption of unmanned aerial vehicles (UAVs). This growing deployment presents\nnew challenges for UAV trajectory planning in complex urban environments.\nHowever, existing studies often overlook key factors, such as urban airspace\nconstraints and economic efficiency, which are essential in low-altitude\neconomy contexts. Deep reinforcement learning (DRL) is regarded as a promising\nsolution to these issues, while its practical adoption remains limited by low\nlearning efficiency. To overcome this limitation, we propose a novel UAV\ntrajectory planning framework that combines DRL with large language model (LLM)\nreasoning to enable safe, compliant, and economically viable path planning.\nExperimental results demonstrate that our method significantly outperforms\nexisting baselines across multiple metrics, including data collection rate,\ncollision avoidance, successful landing, regulatory compliance, and energy\nefficiency. These results validate the effectiveness of our approach in\naddressing UAV trajectory planning key challenges under constraints of the\nlow-altitude economy networking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u7565\u57ce\u5e02\u7a7a\u57df\u7ea6\u675f\u548c\u7ecf\u6d4e\u6548\u7387\u7b49\u5173\u952e\u56e0\u7d20\u3002", "method": "\u7ed3\u5408DRL\u4e0eLLM\u63a8\u7406\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u5408\u89c4\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6536\u96c6\u7387\u3001\u907f\u969c\u3001\u6210\u529f\u7740\u9646\u3001\u6cd5\u89c4\u5408\u89c4\u6027\u548c\u80fd\u6e90\u6548\u7387\u7b49\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7ea6\u675f\u4e0b\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2506.08403", "pdf": "https://arxiv.org/pdf/2506.08403", "abs": "https://arxiv.org/abs/2506.08403", "authors": ["Weiya Li", "Junjie Chen", "Bei Li", "Boyang Liu", "Zichen Wen", "Nuanqiao Shan", "Xiaoqian Liu", "Anping Liu", "Huajie Liu", "Youyan Wang", "Wujiuge Yin", "Hu Song", "Bing Huang", "Zhiyuan Xia", "Jialiang Chen", "Linfeng Zhang"], "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC", "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTACTIC\u7684\u8ba4\u77e5\u7406\u8bba\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u6f5c\u529b\u5c1a\u672a\u5b8c\u5168\u53d1\u6325\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8ba4\u77e5\u7ffb\u8bd1\u7814\u7a76\u7684\u5173\u952e\u89c1\u89e3\uff0c\u5982\u4eba\u7c7b\u7ffb\u8bd1\u7684\u8ba4\u77e5\u7b56\u7565\u3002", "method": "TACTIC\u6846\u67b6\u5305\u542b\u516d\u4e2a\u529f\u80fd\u4e0d\u540c\u7684\u667a\u80fd\u4f53\uff0c\u5206\u522b\u8d1f\u8d23\u8d77\u8349\u3001\u7cbe\u70bc\u3001\u8bc4\u4f30\u3001\u8bc4\u5206\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u5916\u90e8\u77e5\u8bc6\u6536\u96c6\uff0c\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u7684\u8ba4\u77e5\u6d41\u7a0b\u3002", "result": "\u5728FLORES-200\u548cWMT24\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTACTIC\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8aGPT-4.1\u548cDeepSeek-R1\uff0c\u5e73\u5747\u63d0\u53470.6 XCOMET\u548c1.18 COMETKIWI-23\u3002", "conclusion": "TACTIC\u901a\u8fc7\u8ba4\u77e5\u7406\u8bba\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08351", "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStep AG\u7684\u901a\u7528\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff0c\u7528\u4e8e\u51cf\u5c11\u6587\u672c\u5230\u89c6\u89c9\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u3002", "motivation": "\u5f53\u524d\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u65b9\u6cd5\u9700\u8981\u4e24\u500d\u4e8e\u65e0\u6761\u4ef6\u751f\u6210\u7684\u6b65\u9aa4\uff0c\u5bfc\u81f4\u6210\u672c\u663e\u8457\u589e\u52a0\uff0c\u800c\u73b0\u6709\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u65b9\u6cd5\u7f3a\u4e4f\u5206\u6790\u548c\u5b9e\u8bc1\u652f\u6301\u3002", "method": "\u63d0\u51faStep AG\u7b56\u7565\uff0c\u5c06\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u9650\u5236\u5728\u524d\u51e0\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e73\u5747\u52a0\u901f20%\u81f330%\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u8bbe\u7f6e\u548c\u6a21\u578b\u3002", "conclusion": "Step AG\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u4e14\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.08580", "pdf": "https://arxiv.org/pdf/2506.08580", "abs": "https://arxiv.org/abs/2506.08580", "authors": ["Yang Lv", "Jinlong Lei", "Peng Yi"], "title": "HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Two-stage Colonel Blotto game represents a typical adversarial resource\nallocation problem, in which two opposing agents sequentially allocate\nresources in a network topology across two phases: an initial resource\ndeployment followed by multiple rounds of dynamic reallocation adjustments. The\nsequential dependency between game stages and the complex constraints imposed\nby the graph topology make it difficult for traditional approaches to attain a\nglobally optimal strategy. To address these challenges, we propose a\nhierarchical graph Transformer framework called HGformer. By incorporating an\nenhanced graph Transformer encoder with structural biases and a two-agent\nhierarchical decision model, our approach enables efficient policy generation\nin large-scale adversarial environments. Moreover, we design a layer-by-layer\nfeedback reinforcement learning algorithm that feeds the long-term returns from\nlower-level decisions back into the optimization of the higher-level strategy,\nthus bridging the coordination gap between the two decision-making stages.\nExperimental results demonstrate that, compared to existing hierarchical\ndecision-making or graph neural network methods, HGformer significantly\nimproves resource allocation efficiency and adversarial payoff, achieving\nsuperior overall performance in complex dynamic game scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHGformer\u7684\u5206\u5c42\u56feTransformer\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e24\u9636\u6bb5Colonel Blotto\u6e38\u620f\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u589e\u5f3a\u7684\u56feTransformer\u7f16\u7801\u5668\u548c\u5206\u5c42\u51b3\u7b56\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u5bf9\u6297\u6536\u76ca\u3002", "motivation": "\u4f20\u7edf\u7684\u4e24\u9636\u6bb5Colonel Blotto\u6e38\u620f\u7531\u4e8e\u9636\u6bb5\u95f4\u7684\u987a\u5e8f\u4f9d\u8d56\u6027\u548c\u56fe\u62d3\u6251\u7684\u590d\u6742\u7ea6\u675f\uff0c\u96be\u4ee5\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u7b56\u7565\u3002", "method": "\u63d0\u51faHGformer\u6846\u67b6\uff0c\u7ed3\u5408\u589e\u5f3a\u7684\u56feTransformer\u7f16\u7801\u5668\u548c\u5206\u5c42\u51b3\u7b56\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u5c42\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHGformer\u5728\u590d\u6742\u52a8\u6001\u6e38\u620f\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u5bf9\u6297\u6536\u76ca\u3002", "conclusion": "HGformer\u4e3a\u89e3\u51b3\u590d\u6742\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08410", "pdf": "https://arxiv.org/pdf/2506.08410", "abs": "https://arxiv.org/abs/2506.08410", "authors": ["Ziyang Ma", "Qingyue Yuan", "Zhenglin Wang", "Deyu Zhou"], "title": "Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous research has primarily focused on the cognitive error detection\ncapabilities of Large Language Models (LLMs), often prompting them to analyze\nmistakes in reasoning chains. However, few studies have examined the\nmeta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),\nwhich are crucial for their reliability. While studies on LLM self-evaluation\npresent some measures, such as perplexity, which can reflect the answer\ncorrectness and be viewed as the lens of meta-cognition, they lack step-level\nanalysis and adaptation. This paper studies the evaluation of LLM\nmeta-cognition using the current lenses and how to improve these lenses.\nSpecifically, we propose AutoMeco, an Automated Meta-cognition Evaluation\nframework for benchmarking the existing lenses. Furthermore, a training-free\nMarkovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost\ncurrent meta-cognition lenses. Experimental results on three mathematical\nreasoning datasets and three LLMs show the reasonableness of AutoMeco by\ncomparing it with Best-of-N verification. Moreover, the meta-cognition ability\nof LLMs can be better evaluated using MIRA.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86AutoMeco\u6846\u67b6\u548cMIRA\u7b56\u7565\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8LLM\u7684\u8ba4\u77e5\u9519\u8bef\u68c0\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5143\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u81ea\u6211\u9519\u8bef\u610f\u8bc6\uff09\u7684\u6df1\u5165\u5206\u6790\uff0c\u8fd9\u5bf9LLM\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faAutoMeco\u6846\u67b6\u8bc4\u4f30\u73b0\u6709\u5143\u8ba4\u77e5\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1MIRA\u7b56\u7565\uff08\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9a6c\u5c14\u53ef\u592b\u5185\u5728\u5956\u52b1\u8c03\u6574\u65b9\u6cd5\uff09\u4f18\u5316\u8fd9\u4e9b\u6307\u6807\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAutoMeco\u4e0eBest-of-N\u9a8c\u8bc1\u76f8\u6bd4\u5177\u6709\u5408\u7406\u6027\uff0c\u4e14MIRA\u80fd\u66f4\u51c6\u786e\u8bc4\u4f30LLM\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u3002", "conclusion": "AutoMeco\u548cMIRA\u4e3aLLM\u5143\u8ba4\u77e5\u80fd\u529b\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.08356", "pdf": "https://arxiv.org/pdf/2506.08356", "abs": "https://arxiv.org/abs/2506.08356", "authors": ["Shivang Chopra", "Lingchao Mao", "Gabriela Sanchez-Rodriguez", "Andrew J Feola", "Jing Li", "Zsolt Kira"], "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems.", "AI": {"tldr": "MedMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u533b\u5b66\u6210\u50cf\u6a21\u6001\u7684\u89c6\u89c9-\u8bed\u8a00\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7Mixture-of-Experts\u6a21\u5757\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\u91c7\u7528\u7edf\u4e00\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5b9a\u9700\u6c42\uff0cMedMoE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8eSwin Transformer\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u91d1\u5b57\u5854\uff0c\u901a\u8fc7\u6761\u4ef6MoE\u6a21\u5757\u52a8\u6001\u8def\u7531\u7279\u5f81\u5230\u4e13\u95e8\u8bad\u7ec3\u7684\u4e13\u5bb6\u5206\u652f\uff0c\u6355\u6349\u6a21\u6001\u7279\u5b9a\u7684\u89c6\u89c9\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMedMoE\u5728\u591a\u79cd\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u548c\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "MedMoE\u8bc1\u660e\u4e86\u6a21\u6001\u4e13\u7528\u89c6\u89c9\u8868\u5f81\u5728\u4e34\u5e8a\u89c6\u89c9-\u8bed\u8a00\u7cfb\u7edf\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.08627", "pdf": "https://arxiv.org/pdf/2506.08627", "abs": "https://arxiv.org/abs/2506.08627", "authors": ["Douwe Geurtjens", "Xixi Lu"], "title": "FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings", "categories": ["cs.AI"], "comment": "Conditionally accepted at BPM 2025", "summary": "Conformance checking is a fundamental task of process mining, which\nquantifies the extent to which the observed process executions match a\nnormative process model. The state-of-the-art approaches compute alignments by\nexploring the state space formed by the synchronous product of the process\nmodel and the trace. This often leads to state space explosion, particularly\nwhen the model exhibits a high degree of choice and concurrency. Moreover, as\nalignments inherently impose a sequential structure, they fail to fully\nrepresent the concurrent behavior present in many real-world processes. To\naddress these limitations, this paper proposes a new technique for computing\npartial-order alignments {on the fly using directed Petri net unfoldings, named\nFoldA. We evaluate our technique on 485 synthetic model-log pairs and compare\nit against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6\nbenchmark pairs. The results show that our unfolding alignment, although it\nrequires more computation time, generally reduces the number of queued states\nand provides a more accurate representation of concurrency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePetri\u7f51\u5c55\u5f00\u7684\u5b9e\u65f6\u90e8\u5206\u5e8f\u5bf9\u9f50\u65b9\u6cd5FoldA\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5728\u5e76\u53d1\u884c\u4e3a\u8868\u793a\u548c\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u9009\u62e9\u6027\u548c\u5e76\u53d1\u6027\u6a21\u578b\u65f6\u4f1a\u5bfc\u81f4\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\uff0c\u4e14\u65e0\u6cd5\u5145\u5206\u8868\u793a\u5e76\u53d1\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u5b9a\u5411Petri\u7f51\u5c55\u5f00\u5b9e\u65f6\u8ba1\u7b97\u90e8\u5206\u5e8f\u5bf9\u9f50\u3002", "result": "\u5728485\u5bf9\u5408\u6210\u6a21\u578b-\u65e5\u5fd7\u548c19\u5bf9\u771f\u5b9e/\u57fa\u51c6\u6a21\u578b-\u65e5\u5fd7\u4e0a\u9a8c\u8bc1\uff0cFoldA\u51cf\u5c11\u4e86\u6392\u961f\u72b6\u6001\u6570\u91cf\u5e76\u66f4\u51c6\u786e\u5730\u8868\u793a\u5e76\u53d1\u3002", "conclusion": "FoldA\u867d\u7136\u8ba1\u7b97\u65f6\u95f4\u8f83\u957f\uff0c\u4f46\u5728\u51cf\u5c11\u72b6\u6001\u7a7a\u95f4\u548c\u63d0\u9ad8\u5e76\u53d1\u8868\u793a\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2506.08427", "pdf": "https://arxiv.org/pdf/2506.08427", "abs": "https://arxiv.org/abs/2506.08427", "authors": ["Jiaxiang Liu", "Boxuan Xing", "Chenhao Yuan", "Chenxiang Zhang", "Di Wu", "Xiusheng Huang", "Haida Yu", "Chuhan Lang", "Pengfei Cao", "Jun Zhao", "Kang Liu"], "title": "Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to advance, there is a growing\nurgency to enhance the interpretability of their internal knowledge mechanisms.\nConsequently, many interpretation methods have emerged, aiming to unravel the\nknowledge mechanisms of LLMs from various perspectives. However, current\ninterpretation methods differ in input data formats and interpreting outputs.\nThe tools integrating these methods are only capable of supporting tasks with\nspecific inputs, significantly constraining their practical applications. To\naddress these challenges, we present an open-source Knowledge Mechanisms\nRevealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms\nwithin LLMs systematically. Specifically, we have developed an extensible core\nmodule that can automatically match different input data with interpretation\nmethods and consolidate the interpreting outputs. It enables users to freely\nchoose appropriate interpretation methods based on the inputs, making it easier\nto comprehensively diagnose the model's internal knowledge mechanisms from\nmultiple perspectives. Our code is available at\nhttps://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on\nhttps://youtu.be/NVWZABJ43Bs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKnow-MRI\uff0c\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u90e8\u77e5\u8bc6\u673a\u5236\uff0c\u89e3\u51b3\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u8f93\u5165\u8f93\u51fa\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u8f93\u5165\u6570\u636e\u683c\u5f0f\u548c\u8f93\u51fa\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4e9f\u9700\u7edf\u4e00\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u53ef\u6269\u5c55\u6838\u5fc3\u6a21\u5757\uff0c\u81ea\u52a8\u5339\u914d\u8f93\u5165\u6570\u636e\u4e0e\u89e3\u91ca\u65b9\u6cd5\uff0c\u6574\u5408\u8f93\u51fa\u7ed3\u679c\u3002", "result": "Know-MRI\u652f\u6301\u7528\u6237\u81ea\u7531\u9009\u62e9\u89e3\u91ca\u65b9\u6cd5\uff0c\u591a\u89d2\u5ea6\u8bca\u65ad\u6a21\u578b\u77e5\u8bc6\u673a\u5236\u3002", "conclusion": "Know-MRI\u4e3aLLMs\u77e5\u8bc6\u673a\u5236\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08361", "pdf": "https://arxiv.org/pdf/2506.08361", "abs": "https://arxiv.org/abs/2506.08361", "authors": ["Yanting Mei", "Zhilu Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "title": "Image Demoir\u00e9ing Using Dual Camera Fusion on Mobile Phones", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "When shooting electronic screens, moir\\'e patterns usually appear in captured\nimages, which seriously affects the image quality. Existing image demoir\\'eing\nmethods face great challenges in removing large and heavy moir\\'e. To address\nthe issue, we propose to utilize Dual Camera fusion for Image Demoir\\'eing\n(DCID), \\ie, using the ultra-wide-angle (UW) image to assist the moir\\'e\nremoval of wide-angle (W) image. This is inspired by two motivations: (1) the\ntwo lenses are commonly equipped with modern smartphones, (2) the UW image\ngenerally can provide normal colors and textures when moir\\'e exists in the W\nimage mainly due to their different focal lengths. In particular, we propose an\nefficient DCID method, where a lightweight UW image encoder is integrated into\nan existing demoir\\'eing network and a fast two-stage image alignment manner is\npresent. Moreover, we construct a large-scale real-world dataset with diverse\nmobile phones and monitors, containing about 9,000 samples. Experiments on the\ndataset show our method performs better than state-of-the-art methods. Code and\ndataset are available at https://github.com/Mrduckk/DCID.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53cc\u6444\u50cf\u5934\u878d\u5408\uff08DCID\uff09\u53bb\u9664\u56fe\u50cf\u6469\u5c14\u7eb9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u5e7f\u89d2\uff08UW\uff09\u56fe\u50cf\u8f85\u52a9\u5e7f\u89d2\uff08W\uff09\u56fe\u50cf\u53bb\u6469\u5c14\u7eb9\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u667a\u80fd\u624b\u673a\u901a\u5e38\u914d\u5907\u53cc\u6444\u50cf\u5934\uff0c\u4e14\u8d85\u5e7f\u89d2\u56fe\u50cf\u5728\u5e7f\u89d2\u56fe\u50cf\u51fa\u73b0\u6469\u5c14\u7eb9\u65f6\u80fd\u63d0\u4f9b\u6b63\u5e38\u989c\u8272\u548c\u7eb9\u7406\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7UW\u56fe\u50cf\u7f16\u7801\u5668\u96c6\u6210\u5230\u73b0\u6709\u53bb\u6469\u5c14\u7eb9\u7f51\u7edc\u4e2d\uff0c\u91c7\u7528\u5feb\u901f\u4e24\u9636\u6bb5\u56fe\u50cf\u5bf9\u9f50\u65b9\u5f0f\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b9000\u4e2a\u6837\u672c\u7684\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "DCID\u65b9\u6cd5\u901a\u8fc7\u53cc\u6444\u50cf\u5934\u878d\u5408\u6709\u6548\u53bb\u9664\u6469\u5c14\u7eb9\uff0c\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2506.08630", "pdf": "https://arxiv.org/pdf/2506.08630", "abs": "https://arxiv.org/abs/2506.08630", "authors": ["Laurens Engwegen", "Daan Brinks", "Wendelin B\u00f6hmer"], "title": "Modular Recurrence in Contextual MDPs for Universal Morphology Control", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "A universal controller for any robot morphology would greatly improve\ncomputational and data efficiency. By utilizing contextual information about\nthe properties of individual robots and exploiting their modular structure in\nthe architecture of deep reinforcement learning agents, steps have been made\ntowards multi-robot control. Generalization to new, unseen robots, however,\nremains a challenge. In this paper we hypothesize that the relevant contextual\ninformation is partially observable, but that it can be inferred through\ninteractions for better generalization to contexts that are not seen during\ntraining. To this extent, we implement a modular recurrent architecture and\nevaluate its generalization performance on a large set of MuJoCo robots. The\nresults show a substantial improved performance on robots with unseen dynamics,\nkinematics, and topologies, in four different environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u5faa\u73af\u67b6\u6784\uff0c\u901a\u8fc7\u63a8\u65ad\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u5bf9\u672a\u89c1\u673a\u5668\u4eba\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u901a\u7528\u63a7\u5236\u5668\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u548c\u6570\u636e\u6548\u7387\uff0c\u89e3\u51b3\u591a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u672a\u89c1\u673a\u5668\u4eba\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u5faa\u73af\u67b6\u6784\uff0c\u901a\u8fc7\u4ea4\u4e92\u63a8\u65ad\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5728MuJoCo\u673a\u5668\u4eba\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5728\u672a\u89c1\u52a8\u6001\u3001\u8fd0\u52a8\u5b66\u548c\u62d3\u6251\u7ed3\u6784\u7684\u673a\u5668\u4eba\u4e0a\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u63a7\u5236\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.08430", "pdf": "https://arxiv.org/pdf/2506.08430", "abs": "https://arxiv.org/abs/2506.08430", "authors": ["Ziqi. Liu", "Ziyang. Zhou", "Mingxuan. Hu"], "title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models", "categories": ["cs.CL", "cs.MA"], "comment": "ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "summary": "Large language model (LLM) have become mainstream methods in the field of\nsarcasm detection. However, existing LLM methods face challenges in irony\ndetection, including: 1. single-perspective limitations, 2. insufficient\ncomprehensive understanding, and 3. lack of interpretability. This paper\nintroduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven\nmulti-agent system designed to overcome these issues. CAF-I employs specialized\nagents for Context, Semantics, and Rhetoric, which perform multidimensional\nanalysis and engage in interactive collaborative optimization. A Decision Agent\nthen consolidates these perspectives, with a Refinement Evaluator Agent\nproviding conditional feedback for optimization. Experiments on benchmark\ndatasets establish CAF-I's state-of-the-art zero-shot performance. Achieving\nSOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of\n76.31, a 4.98 absolute improvement over the strongest prior baseline. This\nsuccess is attained by its effective simulation of human-like multi-perspective\nanalysis, enhancing detection accuracy and interpretability.", "AI": {"tldr": "CAF-I\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5206\u6790\u548c\u534f\u4f5c\u4f18\u5316\u89e3\u51b3\u8bbd\u523a\u68c0\u6d4b\u4e2d\u7684\u5355\u89c6\u89d2\u9650\u5236\u3001\u7406\u89e3\u4e0d\u8db3\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u8bbd\u523a\u68c0\u6d4b\u4e2d\u5b58\u5728\u5355\u89c6\u89d2\u9650\u5236\u3001\u7406\u89e3\u4e0d\u8db3\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0cCAF-I\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "CAF-I\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ecContext\u3001Semantics\u548cRhetoric\u667a\u80fd\u4f53\u8fdb\u884c\u591a\u7ef4\u5206\u6790\uff0cDecision Agent\u6574\u5408\u7ed3\u679c\uff0cRefinement Evaluator Agent\u63d0\u4f9b\u53cd\u9988\u4f18\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCAF-I\u7684\u96f6\u6837\u672c\u6027\u80fd\u8fbe\u5230SOTA\uff0c\u5e73\u5747Macro-F1\u4e3a76.31\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u57fa\u7ebf\u63d0\u53474.98\u3002", "conclusion": "CAF-I\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u591a\u89c6\u89d2\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbd\u523a\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.08391", "pdf": "https://arxiv.org/pdf/2506.08391", "abs": "https://arxiv.org/abs/2506.08391", "authors": ["Woohyeon Park", "Woojin Kim", "Jaeik Kim", "Jaeyoung Do"], "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in Vision-Language Models (VLMs), the\nperformance of existing VLMs remains hindered by object hallucination, a\ncritical challenge to achieving accurate visual understanding. To address this\nissue, we propose SECOND: Selective and Contrastive Decoding, a novel approach\nthat enables VLMs to effectively leverage multi-scale visual information with\nan object-centric manner, closely aligning with human visual perception. SECOND\nprogressively selects and integrates multi-scale visual information,\nfacilitating a more precise interpretation of images. By contrasting these\nvisual information iteratively, SECOND significantly reduces perceptual\nhallucinations and outperforms a wide range of benchmarks. Our theoretical\nanalysis and experiments highlight the largely unexplored potential of\nmulti-scale application in VLMs, showing that prioritizing and contrasting\nacross scales outperforms existing methods.", "AI": {"tldr": "SECOND\u662f\u4e00\u79cd\u9009\u62e9\u6027\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u4fe1\u606f\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u89c6\u89c9\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faSECOND\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u548c\u5bf9\u6bd4\u6027\u5730\u6574\u5408\u591a\u5c3a\u5ea6\u89c6\u89c9\u4fe1\u606f\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u89e3\u91ca\u56fe\u50cf\u3002", "result": "SECOND\u663e\u8457\u51cf\u5c11\u4e86\u611f\u77e5\u5e7b\u89c9\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u89c6\u89c9\u4fe1\u606f\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0cSECOND\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.08745", "pdf": "https://arxiv.org/pdf/2506.08745", "abs": "https://arxiv.org/abs/2506.08745", "authors": ["Kongcheng Zhang", "Qi Yao", "Shunyu Liu", "Yingjie Wang", "Baisheng Lai", "Jieping Ye", "Mingli Song", "Dacheng Tao"], "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances of Reinforcement Learning (RL) have highlighted its potential\nin complex reasoning tasks, yet effective training often relies on external\nsupervision, which limits the broader applicability. In this work, we propose a\nnovel self-rewarding reinforcement learning framework to enhance Large Language\nModel (LLM) reasoning by leveraging the consistency of intermediate reasoning\nstates across different reasoning trajectories. Our key insight is that correct\nresponses often exhibit consistent trajectory patterns in terms of model\nlikelihood: their intermediate reasoning states tend to converge toward their\nown final answers (high consistency) with minimal deviation toward other\ncandidates (low volatility). Inspired by this observation, we introduce CoVo,\nan intrinsic reward mechanism that integrates Consistency and Volatility via a\nrobust vector-space aggregation strategy, complemented by a curiosity bonus to\npromote diverse exploration. CoVo enables LLMs to perform RL in a\nself-rewarding manner, offering a scalable pathway for learning to reason\nwithout external supervision. Extensive experiments on diverse reasoning\nbenchmarks show that CoVo achieves performance comparable to or even surpassing\nsupervised RL. Our code is available at https://github.com/sastpg/CoVo.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6CoVo\uff0c\u901a\u8fc7\u5206\u6790\u4e2d\u95f4\u63a8\u7406\u72b6\u6001\u7684\u4e00\u81f4\u6027\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5229\u7528\u4e2d\u95f4\u63a8\u7406\u72b6\u6001\u7684\u4e00\u81f4\u6027\u8bbe\u8ba1CoVo\u673a\u5236\uff0c\u7ed3\u5408\u4e00\u81f4\u6027\u548c\u6ce2\u52a8\u6027\uff0c\u5e76\u901a\u8fc7\u597d\u5947\u5fc3\u5956\u52b1\u4fc3\u8fdb\u63a2\u7d22\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoVo\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "CoVo\u4e3a\u65e0\u76d1\u7763\u63a8\u7406\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433", "abs": "https://arxiv.org/abs/2506.08433", "authors": ["Hern\u00e1n Maina", "Nicol\u00e1s Wolovick", "Luciana Benotti"], "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precisions and data parallelization strategies impacts both training\nspeed (as a proxy to energy and hardware consumption) and model accuracy, with\nthe goal of facilitating domain adaptation in low-resource environments. Our\nfindings are relevant to any setting where energy efficiency, accessibility, or\nlimited hardware availability are key concerns.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u5316\u7b56\u7565\u6765\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u7684\u6210\u672c\uff0c\u4ee5\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u9886\u57df\u9002\u5e94\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5bb9\u6613\u53d7\u4e3b\u6d41\u6587\u5316\u548c\u4ef7\u503c\u89c2\u5f71\u54cd\uff0c\u9886\u57df\u9002\u5e94\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u56e2\u961f\u7684\u5e94\u7528\u3002", "method": "\u8bc4\u4f30\u4e0d\u540c\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u5316\u7b56\u7565\u5bf9\u8bad\u7ec3\u901f\u5ea6\u548c\u6a21\u578b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5173\u6ce8\u80fd\u6e90\u6548\u7387\u3001\u53ef\u8bbf\u95ee\u6027\u6216\u786c\u4ef6\u9650\u5236\u7684\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u66f4\u9ad8\u6548\u5730\u5b9e\u73b0\u9886\u57df\u9002\u5e94\u3002"}}
{"id": "2506.08418", "pdf": "https://arxiv.org/pdf/2506.08418", "abs": "https://arxiv.org/abs/2506.08418", "authors": ["Taiqin Chen", "Zikun Zhou", "Zheng Fang", "Wenzhen Zou", "Kanjun Liu", "Ke Chen", "Yongbing Zhang", "Yaowei Wang"], "title": "RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "The radio map represents the spatial distribution of spectrum resources\nwithin a region, supporting efficient resource allocation and interference\nmitigation. However, it is difficult to construct a dense radio map as a\nlimited number of samples can be measured in practical scenarios. While\nexisting works have used deep learning to estimate dense radio maps from sparse\nsamples, they are hard to integrate with the physical characteristics of the\nradio map. To address this challenge, we cast radio map estimation as the\nsparse signal recovery problem. A physical propagation model is further\nincorporated to decompose the problem into multiple factor optimization\nsub-problems, thereby reducing recovery complexity. Inspired by the existing\ncompressive sensing methods, we propose the Radio Deep Unfolding Network\n(RadioDUN) to unfold the optimization process, achieving adaptive parameter\nadjusting and prior fitting in a learnable manner. To account for the radio\npropagation characteristics, we develop a dynamic reweighting module (DRM) to\nadaptively model the importance of each factor for the radio map. Inspired by\nthe shadowing factor in the physical propagation model, we integrate\nobstacle-related factors to express the obstacle-induced signal stochastic\ndecay. The shadowing loss is further designed to constrain the factor\nprediction and act as a supplementary supervised objective, which enhances the\nperformance of RadioDUN. Extensive experiments have been conducted to\ndemonstrate that the proposed method outperforms the state-of-the-art methods.\nOur code will be made publicly available upon publication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u7684\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u4f20\u64ad\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u52a0\u6743\u6a21\u5757\u548c\u9634\u5f71\u635f\u5931\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7ed3\u5408\u65e0\u7ebf\u7535\u5730\u56fe\u7684\u7269\u7406\u7279\u6027\uff0c\u7a00\u758f\u6837\u672c\u9650\u5236\u4e86\u5bc6\u96c6\u65e0\u7ebf\u7535\u5730\u56fe\u7684\u6784\u5efa\u3002", "method": "\u5c06\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u95ee\u9898\uff0c\u7ed3\u5408\u7269\u7406\u4f20\u64ad\u6a21\u578b\u5206\u89e3\u4e3a\u591a\u56e0\u5b50\u4f18\u5316\u5b50\u95ee\u9898\uff0c\u63d0\u51faRadioDUN\u7f51\u7edc\u548c\u52a8\u6001\u91cd\u52a0\u6743\u6a21\u5757\uff08DRM\uff09\uff0c\u5e76\u8bbe\u8ba1\u9634\u5f71\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\uff0cRadioDUN\u5728\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.08747", "pdf": "https://arxiv.org/pdf/2506.08747", "abs": "https://arxiv.org/abs/2506.08747", "authors": ["Boyang Sun", "Yu Yao", "Xinshuai Dong", "Zongfang Liu", "Tongliang Liu", "Yumou Qiu", "Kun Zhang"], "title": "A Sample Efficient Conditional Independence Test in the Presence of Discretization", "categories": ["cs.AI", "stat.ML"], "comment": null, "summary": "In many real-world scenarios, interested variables are often represented as\ndiscretized values due to measurement limitations. Applying Conditional\nIndependence (CI) tests directly to such discretized data, however, can lead to\nincorrect conclusions. To address this, recent advancements have sought to\ninfer the correct CI relationship between the latent variables through\nbinarizing observed data. However, this process inevitably results in a loss of\ninformation, which degrades the test's performance. Motivated by this, this\npaper introduces a sample-efficient CI test that does not rely on the\nbinarization process. We find that the independence relationships of latent\ncontinuous variables can be established by addressing an over-identifying\nrestriction problem with Generalized Method of Moments (GMM). Based on this\ninsight, we derive an appropriate test statistic and establish its asymptotic\ndistribution correctly reflecting CI by leveraging nodewise regression.\nTheoretical findings and Empirical results across various datasets demonstrate\nthat the superiority and effectiveness of our proposed test. Our code\nimplementation is provided in https://github.com/boyangaaaaa/DCT", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u72ec\u7acb\u6027\u68c0\u9a8c\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5bf9\u79bb\u6563\u5316\u6570\u636e\u7684\u4e8c\u503c\u5316\u5904\u7406\uff0c\u901a\u8fc7\u5e7f\u4e49\u77e9\u65b9\u6cd5\uff08GMM\uff09\u548c\u8282\u70b9\u56de\u5f52\u89e3\u51b3\u4e86\u6f5c\u5728\u8fde\u7eed\u53d8\u91cf\u7684\u72ec\u7acb\u6027\u5173\u7cfb\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6d4b\u91cf\u9650\u5236\uff0c\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53d8\u91cf\u5e38\u88ab\u79bb\u6563\u5316\uff0c\u76f4\u63a5\u5e94\u7528\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u4f1a\u5bfc\u81f4\u9519\u8bef\u7ed3\u8bba\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4e8c\u503c\u5316\u5904\u7406\u63a8\u65ad\u6f5c\u5728\u53d8\u91cf\u7684\u72ec\u7acb\u6027\uff0c\u4f46\u4f1a\u635f\u5931\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5e7f\u4e49\u77e9\u65b9\u6cd5\uff08GMM\uff09\u89e3\u51b3\u8fc7\u5ea6\u8bc6\u522b\u9650\u5236\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u9002\u5f53\u7684\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u5e76\u5229\u7528\u8282\u70b9\u56de\u5f52\u5efa\u7acb\u5176\u6e10\u8fd1\u5206\u5e03\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u907f\u514d\u4e86\u4fe1\u606f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u7684\u6027\u80fd\u3002"}}
{"id": "2506.08436", "pdf": "https://arxiv.org/pdf/2506.08436", "abs": "https://arxiv.org/abs/2506.08436", "authors": ["Jiujun He", "Huazhen Lin"], "title": "Olica: Efficient Structured Pruning of Large Language Models without Retraining", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Most existing structured pruning methods for Large Language Models (LLMs)\nrequire substantial computational and data resources for retraining to\nreestablish the corrupted correlations, making them prohibitively expensive. To\naddress this, we propose a pruning framework for LLMs called Orthogonal\ndecomposition and Linear Calibration (Olica), which eliminates the need for\nretraining. A key observation is that the multi-head attention (MHA) layer\ndepends on two types of matrix products. By treating these matrix products as\nunified entities and applying principal component analysis (PCA), we extract\nthe most important information to compress LLMs without sacrificing accuracy or\ndisrupting their original structure. Consequently, retraining becomes\nunnecessary. A fast decomposition method is devised, reducing the complexity of\nPCA by a factor of the square of the number of attention heads. Additionally,\nto mitigate error accumulation problem caused by pruning the feed-forward\nnetwork (FFN) layer, we introduce a linear calibration method to reconstruct\nthe residual errors of pruned layers using low-rank matrices. By leveraging\nsingular value decomposition (SVD) on the solution of the least-squares\nproblem, these matrices are obtained without requiring retraining. Extensive\nexperiments show that the proposed Olica is efficient in terms of data usage,\nGPU memory, and running time, while delivering superior performance across\nmultiple benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684LLM\u526a\u679d\u6846\u67b6Olica\uff0c\u901a\u8fc7\u6b63\u4ea4\u5206\u89e3\u548c\u7ebf\u6027\u6821\u51c6\u538b\u7f29\u6a21\u578b\uff0c\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\u91cd\u65b0\u8bad\u7ec3\uff0c\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u5229\u7528PCA\u5904\u7406\u591a\u5934\u6ce8\u610f\u529b\u5c42\u7684\u77e9\u9635\u4e58\u79ef\uff0c\u8bbe\u8ba1\u5feb\u901f\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u7ebf\u6027\u6821\u51c6\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOlica\u5728\u6570\u636e\u4f7f\u7528\u3001GPU\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u9ad8\u6548\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u3002", "conclusion": "Olica\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u538b\u7f29LLM\uff0c\u4fdd\u6301\u6027\u80fd\u3002"}}
{"id": "2506.08429", "pdf": "https://arxiv.org/pdf/2506.08429", "abs": "https://arxiv.org/abs/2506.08429", "authors": ["Mingjie Xu", "Andrew Estornell", "Hongzheng Yang", "Yuzhi Zhao", "Zhaowei Zhu", "Qi Xuan", "Jiaheng Wei"], "title": "Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring", "categories": ["cs.CV"], "comment": null, "summary": "The application of visual instruction tuning and other post-training\ntechniques has significantly enhanced the capabilities of Large Language Models\n(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with\nmore comprehensive visual language datasets. However, the effectiveness of VLMs\nis highly dependent on large-scale, high-quality datasets that ensure precise\nrecognition and accurate reasoning. Two key challenges hinder progress: (1)\nnoisy alignments between images and the corresponding text, which leads to\nmisinterpretation, and (2) ambiguous or misleading text, which obscures visual\ncontent. To address these challenges, we propose SCALE (Single modality data\nquality and Cross modality Alignment Evaluation), a novel quality-driven data\nselection pipeline for VLM instruction tuning datasets. Specifically, SCALE\nintegrates a cross-modality assessment framework that first assigns each data\nentry to its appropriate vision-language task, generates general and\ntask-specific captions (covering scenes, objects, style, etc.), and evaluates\nthe alignment, clarity, task rarity, text coherence, and image clarity of each\nentry based on the generated captions. We reveal that: (1) current unimodal\nquality assessment methods evaluate one modality while overlooking the rest,\nwhich can underestimate samples essential for specific tasks and discard the\nlower-quality instances that help build model robustness; and (2) appropriately\ngenerated image captions provide an efficient way to transfer the image-text\nmultimodal task into a unified text modality.", "AI": {"tldr": "SCALE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u8d28\u91cf\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u4e0e\u6587\u672c\u5bf9\u9f50\u566a\u58f0\u548c\u6587\u672c\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u5f53\u524dVLM\u6027\u80fd\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4f46\u5b58\u5728\u56fe\u50cf\u4e0e\u6587\u672c\u5bf9\u9f50\u566a\u58f0\u548c\u6587\u672c\u6a21\u7cca\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "method": "SCALE\u901a\u8fc7\u8de8\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u63cf\u8ff0\u5e76\u8bc4\u4f30\u6570\u636e\u6761\u76ee\u8d28\u91cf\uff0c\u5305\u62ec\u5bf9\u9f50\u6027\u3001\u6e05\u6670\u5ea6\u7b49\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5355\u6a21\u6001\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u4f4e\u4f30\u5173\u952e\u6837\u672c\uff0c\u800c\u751f\u6210\u7684\u4efb\u52a1\u7279\u5b9a\u63cf\u8ff0\u80fd\u6709\u6548\u7edf\u4e00\u591a\u6a21\u6001\u4efb\u52a1\u4e3a\u6587\u672c\u6a21\u6001\u3002", "conclusion": "SCALE\u4e3aVLM\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.08771", "pdf": "https://arxiv.org/pdf/2506.08771", "abs": "https://arxiv.org/abs/2506.08771", "authors": ["Yuni Susanti", "Michael F\u00e4rber"], "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Accepted at KDD 2025 (full research paper)", "summary": "Inferring causal relationships between variable pairs is crucial for\nunderstanding multivariate interactions in complex systems. Knowledge-based\ncausal discovery -- which involves inferring causal relationships by reasoning\nover the metadata of variables (e.g., names or textual context) -- offers a\ncompelling alternative to traditional methods that rely on observational data.\nHowever, existing methods using Large Language Models (LLMs) often produce\nunstable and inconsistent results, compromising their reliability for causal\ninference. To address this, we introduce a novel approach that integrates\nKnowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.\nOur approach identifies informative metapath-based subgraphs within KGs and\nfurther refines the selection of these subgraphs using Learning-to-Rank-based\nmodels. The top-ranked subgraphs are then incorporated into zero-shot prompts,\nimproving the effectiveness of LLMs in inferring the causal relationship.\nExtensive experiments on biomedical and open-domain datasets demonstrate that\nour method outperforms most baselines by up to 44.4 points in F1 scores,\nevaluated across diverse LLMs and KGs. Our code and datasets are available on\nGitHub: https://github.com/susantiyuni/path-to-causality", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u57fa\u4e8e\u77e5\u8bc6\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c2\u6d4b\u6570\u636e\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u57fa\u4e8e\u77e5\u8bc6\u7684\u56e0\u679c\u53d1\u73b0\uff08\u5982\u901a\u8fc7\u53d8\u91cf\u5143\u6570\u636e\u63a8\u7406\uff09\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u7684\u7ed3\u679c\u4e0d\u7a33\u5b9a\u4e14\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u8bc6\u522b\u4fe1\u606f\u4e30\u5bcc\u7684\u5143\u8def\u5f84\u5b50\u56fe\uff0c\u5e76\u5229\u7528\u5b66\u4e60\u6392\u5e8f\u6a21\u578b\u4f18\u5316\u5b50\u56fe\u9009\u62e9\uff0c\u5c06\u6392\u540d\u9760\u524d\u7684\u5b50\u56fe\u878d\u5165\u96f6\u6837\u672c\u63d0\u793a\u4e2d\uff0c\u63d0\u5347LLM\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u751f\u7269\u533b\u5b66\u548c\u5f00\u653e\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u6bd4\u5927\u591a\u6570\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa44.4\u5206\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cdLLM\u548cKGs\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u77e5\u8bc6\u7684\u56e0\u679c\u53d1\u73b0\u7684\u53ef\u9760\u6027\u548c\u6548\u679c\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u591a\u53d8\u91cf\u4ea4\u4e92\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.08477", "pdf": "https://arxiv.org/pdf/2506.08477", "abs": "https://arxiv.org/abs/2506.08477", "authors": ["Fengjun Pan", "Anh Tuan Luu", "Xiaobao Wu"], "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Detecting harmful memes is essential for maintaining the integrity of online\nenvironments. However, current approaches often struggle with resource\nefficiency, flexibility, or explainability, limiting their practical deployment\nin content moderation systems. To address these challenges, we introduce\nU-CoT+, a novel framework for harmful meme detection. Instead of relying solely\non prompting or fine-tuning multimodal models, we first develop a high-fidelity\nmeme-to-text pipeline that converts visual memes into detail-preserving textual\ndescriptions. This design decouples meme interpretation from meme\nclassification, thus avoiding immediate reasoning over complex raw visual\ncontent and enabling resource-efficient harmful meme detection with general\nlarge language models (LLMs). Building on these textual descriptions, we\nfurther incorporate targeted, interpretable human-crafted guidelines to guide\nmodels' reasoning under zero-shot CoT prompting. As such, this framework allows\nfor easy adaptation to different harmfulness detection criteria across\nplatforms, regions, and over time, offering high flexibility and\nexplainability. Extensive experiments on seven benchmark datasets validate the\neffectiveness of our framework, highlighting its potential for explainable and\nlow-resource harmful meme detection using small-scale LLMs. Codes and data are\navailable at: https://anonymous.4open.science/r/HMC-AF2B/README.md.", "AI": {"tldr": "U-CoT+\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\uff0c\u901a\u8fc7\u5c06\u6a21\u56e0\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\u5e76\u7ed3\u5408\u4eba\u7c7b\u6307\u5bfc\u7684\u96f6\u6837\u672cCoT\u63d0\u793a\u3002", "motivation": "\u5f53\u524d\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\u65b9\u6cd5\u5728\u8d44\u6e90\u6548\u7387\u3001\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u4fdd\u771f\u7684\u6a21\u56e0\u5230\u6587\u672c\u7684\u8f6c\u6362\u6d41\u7a0b\uff0c\u7ed3\u5408\u4eba\u7c7b\u6307\u5bfc\u7684\u96f6\u6837\u672cCoT\u63d0\u793a\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5c0f\u89c4\u6a21LLMs\u4e0a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "U-CoT+\u6846\u67b6\u4e3a\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8de8\u5e73\u53f0\u3001\u8de8\u533a\u57df\u548c\u52a8\u6001\u53d8\u5316\u7684\u68c0\u6d4b\u9700\u6c42\u3002"}}
{"id": "2506.08456", "pdf": "https://arxiv.org/pdf/2506.08456", "abs": "https://arxiv.org/abs/2506.08456", "authors": ["June Suk Choi", "Kyungmin Lee", "Sihyun Yu", "Yisol Choi", "Jinwoo Shin", "Kimin Lee"], "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance", "categories": ["cs.CV"], "comment": "Preprint. Under review. Project page available at\n  http://choi403.github.io/ALG", "summary": "Recent text-to-video (T2V) models have demonstrated strong capabilities in\nproducing high-quality, dynamic videos. To improve the visual controllability,\nrecent works have considered fine-tuning pre-trained T2V models to support\nimage-to-video (I2V) generation. However, such adaptation frequently suppresses\nmotion dynamics of generated outputs, resulting in more static videos compared\nto their T2V counterparts. In this work, we analyze this phenomenon and\nidentify that it stems from the premature exposure to high-frequency details in\nthe input image, which biases the sampling process toward a shortcut trajectory\nthat overfits to the static appearance of the reference image. To address this,\nwe propose adaptive low-pass guidance (ALG), a simple fix to the I2V model\nsampling procedure to generate more dynamic videos without compromising\nper-frame image quality. Specifically, ALG adaptively modulates the frequency\ncontent of the conditioning image by applying low-pass filtering at the early\nstage of denoising. Extensive experiments demonstrate that ALG significantly\nimproves the temporal dynamics of generated videos, while preserving image\nfidelity and text alignment. Especially, under VBench-I2V test suite, ALG\nachieves an average improvement of 36% in dynamic degree without a significant\ndrop in video quality or image fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4f4e\u901a\u5f15\u5bfc\uff08ALG\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u751f\u6210\u4e2d\u52a8\u6001\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7684\u52a8\u6001\u6027\u3002", "motivation": "\u73b0\u6709I2V\u751f\u6210\u65b9\u6cd5\u56e0\u8fc7\u65e9\u66b4\u9732\u9ad8\u9891\u7ec6\u8282\u5bfc\u81f4\u89c6\u9891\u52a8\u6001\u6027\u4e0d\u8db3\uff0cALG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5236\u8f93\u5165\u56fe\u50cf\u9891\u7387\u5185\u5bb9\uff0c\u5728\u53bb\u566a\u65e9\u671f\u5e94\u7528\u4f4e\u901a\u6ee4\u6ce2\u3002", "result": "ALG\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u52a8\u6001\u6027\uff08VBench-I2V\u6d4b\u8bd5\u4e2d\u52a8\u6001\u5ea6\u5e73\u5747\u63d0\u534736%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u3002", "conclusion": "ALG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86I2V\u751f\u6210\u7684\u52a8\u6001\u6027\u3002"}}
{"id": "2506.08800", "pdf": "https://arxiv.org/pdf/2506.08800", "abs": "https://arxiv.org/abs/2506.08800", "authors": ["Irene Testini", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Lorenzo Pacchiardi"], "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) are increasingly used as\nassistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u52a9\u624b\u548c\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0\u7814\u7a76\u96c6\u4e2d\u5728\u6709\u9650\u7684\u76ee\u6807\u5bfc\u5411\u6d3b\u52a8\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u7ba1\u7406\u548c\u63a2\u7d22\u6027\u6d3b\u52a8\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u4eba\u673a\u534f\u4f5c\u548c\u4efb\u52a1\u8f6c\u578b\u7684\u5173\u6ce8\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u63ed\u793a\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5982\u5bf9\u6570\u636e\u7ba1\u7406\u548c\u63a2\u7d22\u6027\u6d3b\u52a8\u7684\u5ffd\u89c6\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u5bf9\u4eba\u673a\u534f\u4f5c\u548c\u4efb\u52a1\u8f6c\u578b\u7684\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790LLM\u52a9\u624b\u548c\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u8bc4\u4f30\u73b0\u72b6\uff0c\u603b\u7ed3\u7814\u7a76\u8d8b\u52bf\u548c\u4e0d\u8db3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u96c6\u4e2d\u5728\u76ee\u6807\u5bfc\u5411\u6d3b\u52a8\uff0c\u5ffd\u89c6\u6570\u636e\u7ba1\u7406\u548c\u63a2\u7d22\u6027\u6d3b\u52a8\uff1b\u7f3a\u4e4f\u4e2d\u95f4\u4eba\u673a\u534f\u4f5c\u7684\u63a2\u8ba8\uff1b\u8fc7\u5ea6\u5173\u6ce8\u4eba\u7c7b\u66ff\u4ee3\u800c\u975e\u4efb\u52a1\u8f6c\u578b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u66f4\u5168\u9762\u5730\u8bc4\u4f30LLM\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6570\u636e\u7ba1\u7406\u3001\u63a2\u7d22\u6027\u6d3b\u52a8\u3001\u4eba\u673a\u534f\u4f5c\u548c\u4efb\u52a1\u8f6c\u578b\u3002"}}
{"id": "2506.08479", "pdf": "https://arxiv.org/pdf/2506.08479", "abs": "https://arxiv.org/abs/2506.08479", "authors": ["Chihiro Taguchi", "Seiji Maekawa", "Nikita Bhutani"], "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "26 pages, 16 tables, 5 figures", "summary": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs)\nboth address context limitations of LLMs in open-domain question answering\n(QA). However, optimal external context to retrieve remains an open problem:\nfixing the retrieval size risks either wasting tokens or omitting key evidence.\nExisting adaptive methods like Self-RAG and Self-Route rely on iterative LLM\nprompting and perform well on factoid QA, but struggle with aggregation QA,\nwhere the optimal context size is both unknown and variable. We present\nAdaptive-$k$ retrieval, a simple and effective single-pass method that\nadaptively selects the number of passages based on the distribution of the\nsimilarity scores between the query and the candidate passages. It does not\nrequire model fine-tuning, extra LLM inferences or changes to existing\nretriever-reader pipelines. On both factoid and aggregation QA benchmarks,\nAdaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x\nfewer tokens than full-context input, yet still retrieves 70% of relevant\npassages. It improves accuracy across five LCLMs and two embedding models,\nhighlighting that dynamically adjusting context size leads to more efficient\nand accurate QA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u68c0\u7d22\u65b9\u6cd5Adaptive-$k$\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u6bb5\u843d\u6570\u91cf\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u68c0\u7d22\u91cf\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982Self-RAG\u548cSelf-Route\uff09\u5728\u4e8b\u5b9e\u6027\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u805a\u5408\u95ee\u7b54\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6700\u4f73\u4e0a\u4e0b\u6587\u5927\u5c0f\u672a\u77e5\u4e14\u53ef\u53d8\u3002", "method": "Adaptive-$k$\u57fa\u4e8e\u67e5\u8be2\u4e0e\u5019\u9009\u6bb5\u843d\u76f8\u4f3c\u5ea6\u5206\u6570\u7684\u5206\u5e03\uff0c\u5355\u6b21\u9009\u62e9\u6bb5\u843d\u6570\u91cf\uff0c\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u6216\u989d\u5916\u63a8\u7406\u3002", "result": "\u5728\u4e8b\u5b9e\u6027\u548c\u805a\u5408\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaptive-$k$\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u68c0\u7d22\u57fa\u7ebf\uff0c\u540c\u65f6\u51cf\u5c1190%\u7684\u4ee4\u724c\u4f7f\u7528\uff0c\u4ecd\u4fdd\u755970%\u7684\u76f8\u5173\u6bb5\u843d\u3002", "conclusion": "\u52a8\u6001\u8c03\u6574\u4e0a\u4e0b\u6587\u5927\u5c0f\u80fd\u663e\u8457\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u5d4c\u5165\u6a21\u578b\u3002"}}
{"id": "2506.08470", "pdf": "https://arxiv.org/pdf/2506.08470", "abs": "https://arxiv.org/abs/2506.08470", "authors": ["Siyuan Shen", "Ziheng Wang", "Xingyue Peng", "Suan Xia", "Ruiqian Li", "Shiying Li", "Jingyi Yu"], "title": "MARMOT: Masked Autoencoder for Modeling Transient Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained models have demonstrated impressive success in many modalities\nsuch as language and vision. Recent works facilitate the pretraining paradigm\nin imaging research. Transients are a novel modality, which are captured for an\nobject as photon counts versus arrival times using a precisely time-resolved\nsensor. In particular for non-line-of-sight (NLOS) scenarios, transients of\nhidden objects are measured beyond the sensor's direct line of sight. Using\nNLOS transients, the majority of previous works optimize volume density or\nsurfaces to reconstruct the hidden objects and do not transfer priors learned\nfrom datasets. In this work, we present a masked autoencoder for modeling\ntransient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a\nself-supervised model pretrianed on massive and diverse NLOS transient\ndatasets. Using a Transformer-based encoder-decoder, MARMOT learns features\nfrom partially masked transients via a scanning pattern mask (SPM), where the\nunmasked subset is functionally equivalent to arbitrary sampling, and predicts\nfull measurements. Pretrained on TransVerse-a synthesized transient dataset of\n500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature\ntransfer or decoder finetuning. Comprehensive experiments are carried out in\ncomparisons with state-of-the-art methods. Quantitative and qualitative results\ndemonstrate the efficiency of our MARMOT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MARMOT\uff09\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7528\u4e8e\u975e\u89c6\u8ddd\uff08NLOS\uff09\u77ac\u6001\u6210\u50cf\u4efb\u52a1\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u5b66\u4e60\u7279\u5f81\u5e76\u9884\u6d4b\u5b8c\u6574\u6d4b\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728NLOS\u573a\u666f\u4e2d\u4e3b\u8981\u4f18\u5316\u4f53\u79ef\u5bc6\u5ea6\u6216\u8868\u9762\u91cd\u5efa\uff0c\u7f3a\u4e4f\u4ece\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5148\u9a8c\u77e5\u8bc6\u7684\u80fd\u529b\u3002MARMOT\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u626b\u63cf\u6a21\u5f0f\u63a9\u7801\uff08SPM\uff09\u4ece\u90e8\u5206\u63a9\u7801\u7684\u77ac\u6001\u6570\u636e\u4e2d\u5b66\u4e60\u7279\u5f81\uff0c\u5e76\u9884\u6d4b\u5b8c\u6574\u6d4b\u91cf\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6TransVerse\uff0850\u4e073D\u6a21\u578b\uff09\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cMARMOT\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MARMOT\u4e3aNLOS\u77ac\u6001\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u7279\u5f81\u8fc1\u79fb\u6216\u89e3\u7801\u5668\u5fae\u8c03\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2506.08872", "pdf": "https://arxiv.org/pdf/2506.08872", "abs": "https://arxiv.org/abs/2506.08872", "authors": ["Nataliya Kosmyna", "Eugene Hauptmann", "Ye Tong Yuan", "Jessica Situ", "Xian-Hao Liao", "Ashly Vivian Beresnitzky", "Iris Braunstein", "Pattie Maes"], "title": "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task", "categories": ["cs.AI"], "comment": "206 pages, 92 figures, 4 tables and appendix", "summary": "This study explores the neural and behavioral consequences of LLM-assisted\nessay writing. Participants were divided into three groups: LLM, Search Engine,\nand Brain-only (no tools). Each completed three sessions under the same\ncondition. In a fourth session, LLM users were reassigned to Brain-only group\n(LLM-to-Brain), and Brain-only users were reassigned to LLM condition\n(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18\ncompleting session 4. We used electroencephalography (EEG) to assess cognitive\nload during essay writing, and analyzed essays using NLP, as well as scoring\nessays with the help from human teachers and an AI judge. Across groups, NERs,\nn-gram patterns, and topic ontology showed within-group homogeneity. EEG\nrevealed significant differences in brain connectivity: Brain-only participants\nexhibited the strongest, most distributed networks; Search Engine users showed\nmoderate engagement; and LLM users displayed the weakest connectivity.\nCognitive activity scaled down in relation to external tool use. In session 4,\nLLM-to-Brain participants showed reduced alpha and beta connectivity,\nindicating under-engagement. Brain-to-LLM users exhibited higher memory recall\nand activation of occipito-parietal and prefrontal areas, similar to Search\nEngine users. Self-reported ownership of essays was the lowest in the LLM group\nand the highest in the Brain-only group. LLM users also struggled to accurately\nquote their own work. While LLMs offer immediate convenience, our findings\nhighlight potential cognitive costs. Over four months, LLM users consistently\nunderperformed at neural, linguistic, and behavioral levels. These results\nraise concerns about the long-term educational implications of LLM reliance and\nunderscore the need for deeper inquiry into AI's role in learning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLM\u8f85\u52a9\u5199\u4f5c\u5bf9\u795e\u7ecf\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0LLM\u4f7f\u7528\u8005\u5728\u8ba4\u77e5\u3001\u8bed\u8a00\u548c\u884c\u4e3a\u5c42\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u5f15\u53d1\u5bf9\u6559\u80b2\u4e2d\u957f\u671f\u4f9d\u8d56LLM\u7684\u62c5\u5fe7\u3002", "motivation": "\u63a2\u7d22LLM\u8f85\u52a9\u5199\u4f5c\u7684\u795e\u7ecf\u548c\u884c\u4e3a\u540e\u679c\uff0c\u8bc4\u4f30\u5176\u5bf9\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u53c2\u4e0e\u8005\u5206\u4e3aLLM\u3001\u641c\u7d22\u5f15\u64ce\u548c\u65e0\u5de5\u5177\u7ec4\uff0c\u901a\u8fc7EEG\u548cNLP\u5206\u6790\u8ba4\u77e5\u8d1f\u8377\u548c\u5199\u4f5c\u8d28\u91cf\u3002", "result": "LLM\u4f7f\u7528\u8005\u8111\u8fde\u63a5\u6700\u5f31\uff0c\u8ba4\u77e5\u6d3b\u52a8\u51cf\u5c11\uff0c\u5199\u4f5c\u6240\u6709\u6743\u611f\u6700\u4f4e\uff1b\u65e0\u5de5\u5177\u7ec4\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLM\u867d\u7136\u65b9\u4fbf\uff0c\u4f46\u53ef\u80fd\u5e26\u6765\u8ba4\u77e5\u4ee3\u4ef7\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76AI\u5728\u6559\u80b2\u4e2d\u7684\u89d2\u8272\u3002"}}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480", "abs": "https://arxiv.org/abs/2506.08480", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0c\u5ffd\u7565\u4e86\u8bc4\u4f30\u6846\u67b6\u7684\u5176\u4ed6\u5173\u952e\u7279\u6027\u3002", "method": "\u8bc6\u522b\u53ef\u9760\u8bc4\u4f30\u5e94\u89e3\u51b3\u7684\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5f53\u524d\u4e3b\u6d41\u6846\u67b6\u7684\u4e0d\u8db3\u3002", "result": "\u5f53\u524d\u8bc4\u4f30\u6846\u67b6\u5728\u591a\u6837\u5316\u7684\u6307\u6807\u548c\u6a21\u578b\u4e2d\u672a\u80fd\u5b8c\u5168\u6ee1\u8db3\u53ef\u9760\u6027\u8981\u6c42\u3002", "conclusion": "\u63d0\u51fa\u6539\u8fdb\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u8bc4\u4f30\u7684\u5efa\u8bae\u3002"}}
{"id": "2506.08493", "pdf": "https://arxiv.org/pdf/2506.08493", "abs": "https://arxiv.org/abs/2506.08493", "authors": ["Qilin Yin", "Wei Lu", "Xiangyang Luo", "Xiaochun Cao"], "title": "Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Most research efforts in the multimedia forensics domain have focused on\ndetecting forgery audio-visual content and reached sound achievements. However,\nthese works only consider deepfake detection as a classification task and\nignore the case where partial segments of the video are tampered with. Temporal\nforgery localization (TFL) of small fake audio-visual clips embedded in real\nvideos is still challenging and more in line with realistic application\nscenarios. To resolve this issue, we propose a universal context-aware\ncontrastive learning framework (UniCaCLF) for TFL. Our approach leverages\nsupervised contrastive learning to discover and identify forged instants by\nmeans of anomaly detection, allowing for the precise localization of temporal\nforged segments. To this end, we propose a novel context-aware perception layer\nthat utilizes a heterogeneous activation operation and an adaptive context\nupdater to construct a context-aware contrastive objective, which enhances the\ndiscriminability of forged instant features by contrasting them with genuine\ninstant features in terms of their distances to the global context. An\nefficient context-aware contrastive coding is introduced to further push the\nlimit of instant feature distinguishability between genuine and forged instants\nin a supervised sample-by-sample manner, suppressing the cross-sample influence\nto improve temporal forgery localization performance. Extensive experimental\nresults over five public datasets demonstrate that our proposed UniCaCLF\nsignificantly outperforms the state-of-the-art competing algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08UniCaCLF\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u5a92\u4f53\u53d6\u8bc1\u9886\u57df\u4e2d\u7684\u65f6\u95f4\u4f2a\u9020\u5b9a\u4f4d\uff08TFL\uff09\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f2a\u9020\u97f3\u89c6\u9891\u5185\u5bb9\u7684\u68c0\u6d4b\uff0c\u4f46\u5ffd\u89c6\u4e86\u89c6\u9891\u90e8\u5206\u7247\u6bb5\u88ab\u7be1\u6539\u7684\u60c5\u51b5\uff0c\u800c\u65f6\u95f4\u4f2a\u9020\u5b9a\u4f4d\uff08TFL\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faUniCaCLF\u6846\u67b6\uff0c\u5229\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u8bc6\u522b\u4f2a\u9020\u7247\u6bb5\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u611f\u77e5\u5c42\u548c\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u66f4\u65b0\u5668\uff0c\u589e\u5f3a\u4f2a\u9020\u7247\u6bb5\u7279\u5f81\u7684\u533a\u5206\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniCaCLF\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "UniCaCLF\u4e3a\u65f6\u95f4\u4f2a\u9020\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08898", "pdf": "https://arxiv.org/pdf/2506.08898", "abs": "https://arxiv.org/abs/2506.08898", "authors": ["Mingfeng Fan", "Jianan Zhou", "Yifeng Zhang", "Yaoxin Wu", "Jinbiao Chen", "Guillaume Adrien Sartoretti"], "title": "Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation", "categories": ["cs.AI"], "comment": "22 pages, 6 figures, under review", "summary": "Recent deep reinforcement learning methods have achieved remarkable success\nin solving multi-objective combinatorial optimization problems (MOCOPs) by\ndecomposing them into multiple subproblems, each associated with a specific\nweight vector. However, these methods typically treat all subproblems equally\nand solve them using a single model, hindering the effective exploration of the\nsolution space and thus leading to suboptimal performance. To overcome the\nlimitation, we propose POCCO, a novel plug-and-play framework that enables\nadaptive selection of model structures for subproblems, which are subsequently\noptimized based on preference signals rather than explicit reward values.\nSpecifically, we design a conditional computation block that routes subproblems\nto specialized neural architectures. Moreover, we propose a preference-driven\noptimization algorithm that learns pairwise preferences between winning and\nlosing solutions. We evaluate the efficacy and versatility of POCCO by applying\nit to two state-of-the-art neural methods for MOCOPs. Experimental results\nacross four classic MOCOP benchmarks demonstrate its significant superiority\nand strong generalization.", "AI": {"tldr": "POCCO\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u6a21\u578b\u7ed3\u6784\u548c\u504f\u597d\u9a71\u52a8\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6240\u6709\u5b50\u95ee\u9898\u89c6\u4e3a\u5e73\u7b49\u5e76\u7528\u5355\u4e00\u6a21\u578b\u89e3\u51b3\uff0c\u9650\u5236\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u6761\u4ef6\u8ba1\u7b97\u5757\u5c06\u5b50\u95ee\u9898\u8def\u7531\u5230\u4e13\u7528\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u504f\u597d\u9a71\u52a8\u7684\u4f18\u5316\u7b97\u6cd5\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u95f4\u7684\u504f\u597d\u3002", "result": "\u5728\u56db\u4e2a\u7ecf\u5178MOCOP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPOCCO\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "POCCO\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u548c\u504f\u597d\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u6548\u679c\u3002"}}
{"id": "2506.08487", "pdf": "https://arxiv.org/pdf/2506.08487", "abs": "https://arxiv.org/abs/2506.08487", "authors": ["Sumanth Manduru", "Carlotta Domeniconi"], "title": "Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of Small Language Models (SLMs) for on-device and\nresource-constrained deployments has outpaced our understanding of their\nethical risks. To the best of our knowledge, we present the first large-scale\naudit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an\noverlooked \"middle tier\" between BERT-class encoders and flagship LLMs. Our\nevaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma\n3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we\nanalyze both utility and fairness across ambiguous and disambiguated contexts.\nThis evaluation reveals three key insights. First, competence and fairness need\nnot be antagonistic: Phi models achieve F1 scores exceeding 90 percent while\nexhibiting minimal bias, showing that efficient and ethical NLP is attainable.\nSecond, social bias varies significantly by architecture: Qwen 2.5 models may\nappear fair, but this often reflects vacuous neutrality, random guessing, or\nevasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2\nmodels exhibit stronger stereotypical bias, suggesting overconfidence rather\nthan neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ\nquantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but\nincreases disability-related bias in Phi-4-Mini by over 7 percentage points.\nThese insights provide practical guidance for the responsible deployment of\nSLMs in applications demanding fairness and efficiency, particularly benefiting\nsmall enterprises and resource-constrained environments.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5927\u89c4\u6a21\u5ba1\u8ba1\u4e860.5\u81f350\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\uff0c\u63ed\u793a\u4e86\u5176\u516c\u5e73\u6027\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\u3001\u67b6\u6784\u5bf9\u504f\u89c1\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u91cf\u5316\u538b\u7f29\u7684\u590d\u6742\u6743\u8861\u3002", "motivation": "\u968f\u7740\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u5176\u4f26\u7406\u98ce\u9669\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u516c\u5e73\u6027\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528BBQ\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u96f6\u6837\u672c\u63d0\u793a\u4e0b\u8bc4\u4f30\u4e869\u4e2a\u5f00\u6e90SLMs\uff08\u6765\u81eaQwen 2.5\u3001LLaMA 3.2\u3001Gemma 3\u548cPhi\u7cfb\u5217\uff09\uff0c\u5206\u6790\u5176\u5728\u6a21\u7cca\u548c\u660e\u786e\u4e0a\u4e0b\u6587\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u516c\u5e73\u6027\u3002", "result": "1. \u516c\u5e73\u6027\u4e0e\u6027\u80fd\u53ef\u5171\u5b58\uff08\u5982Phi\u6a21\u578b\uff09\uff1b2. \u67b6\u6784\u663e\u8457\u5f71\u54cd\u504f\u89c1\uff08Qwen 2.5\u8868\u73b0\u865a\u5047\u4e2d\u7acb\uff0cLLaMA 3.2\u5b58\u5728\u523b\u677f\u504f\u89c1\uff09\uff1b3. \u91cf\u5316\u538b\u7f29\u5e26\u6765\u590d\u6742\u6743\u8861\uff08\u59824-bit AWQ\u63d0\u5347\u6027\u80fd\u4f46\u589e\u52a0\u504f\u89c1\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u516c\u5e73\u9ad8\u6548\u7684SLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5c0f\u578b\u4f01\u4e1a\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2506.08512", "pdf": "https://arxiv.org/pdf/2506.08512", "abs": "https://arxiv.org/abs/2506.08512", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Zihao Liu", "Linlin Yang"], "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Temporal Grounding (VTG), which aims to localize video clips\ncorresponding to natural language queries, is a fundamental yet challenging\ntask in video understanding. Existing Transformer-based methods often suffer\nfrom redundant attention and suboptimal multi-modal alignment. To address these\nlimitations, we propose MLVTG, a novel framework that integrates two key\nmodules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba\nblocks as a backbone instead of Transformers to model temporal dependencies and\nextract robust video representations for multi-modal alignment. LLMRefiner\nleverages the specific frozen layer of a pre-trained Large Language Model (LLM)\nto implicitly transfer semantic priors, enhancing multi-modal alignment without\nfine-tuning. This dual alignment strategy, temporal modeling via structured\nstate-space dynamics and semantic purification via textual priors, enables more\nprecise localization. Extensive experiments on QVHighlights, Charades-STA, and\nTVSum demonstrate that MLVTG achieves state-of-the-art performance and\nsignificantly outperforms existing baselines.", "AI": {"tldr": "MLVTG\u6846\u67b6\u901a\u8fc7MambaAligner\u548cLLMRefiner\u6a21\u5757\u89e3\u51b3\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5197\u4f59\u6ce8\u610f\u529b\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u7684\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5b58\u5728\u5197\u4f59\u6ce8\u610f\u529b\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "method": "MLVTG\u7ed3\u5408MambaAligner\uff08\u4f7f\u7528Vision Mamba\u5757\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\uff09\u548cLLMRefiner\uff08\u5229\u7528\u9884\u8bad\u7ec3LLM\u7684\u7279\u5b9a\u5c42\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff09\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4f18\u5316\u591a\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728QVHighlights\u3001Charades-STA\u548cTVSum\u6570\u636e\u96c6\u4e0a\uff0cMLVTG\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MLVTG\u901a\u8fc7\u53cc\u5bf9\u9f50\u7b56\u7565\uff08\u65f6\u95f4\u5efa\u6a21\u548c\u8bed\u4e49\u51c0\u5316\uff09\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2506.08957", "pdf": "https://arxiv.org/pdf/2506.08957", "abs": "https://arxiv.org/abs/2506.08957", "authors": ["Yash Ranjan", "Rahul Sengupta", "Anand Rangarajan", "Sanjay Ranka"], "title": "IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Traffic simulators are widely used to study the operational efficiency of\nroad infrastructure, but their rule-based approach limits their ability to\nmimic real-world driving behavior. Traffic intersections are critical\ncomponents of the road infrastructure, both in terms of safety risk (nearly 28%\nof fatal crashes and 58% of nonfatal crashes happen at intersections) as well\nas the operational efficiency of a road corridor. This raises an important\nquestion: can we create a data-driven simulator that can mimic the macro- and\nmicro-statistics of the driving behavior at a traffic intersection? Deep\nGenerative Modeling-based trajectory prediction models provide a good starting\npoint to model the complex dynamics of vehicles at an intersection. But they\nare not tested in a \"live\" micro-simulation scenario and are not evaluated on\ntraffic engineering-related metrics. In this study, we propose traffic\nengineering-related metrics to evaluate generative trajectory prediction models\nand provide a simulation-in-the-loop pipeline to do so. We also provide a\nmulti-headed self-attention-based trajectory prediction model that incorporates\nthe signal information, which outperforms our previous models on the evaluation\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6539\u8fdb\u4ea4\u53c9\u8def\u53e3\u7684\u9a7e\u9a76\u884c\u4e3a\u6a21\u62df\uff0c\u5e76\u5f15\u5165\u4ea4\u901a\u5de5\u7a0b\u76f8\u5173\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u4ea4\u901a\u6a21\u62df\u5668\u96be\u4ee5\u771f\u5b9e\u6a21\u62df\u9a7e\u9a76\u884c\u4e3a\uff0c\u800c\u4ea4\u53c9\u8def\u53e3\u662f\u4e8b\u6545\u9ad8\u53d1\u533a\uff0c\u4e9f\u9700\u66f4\u7cbe\u786e\u7684\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u5934\u90e8\u81ea\u6ce8\u610f\u529b\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u4fe1\u53f7\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u95ed\u73af\u7ba1\u9053\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u3002", "result": "\u65b0\u6a21\u578b\u5728\u4ea4\u901a\u5de5\u7a0b\u76f8\u5173\u6307\u6807\u4e0a\u4f18\u4e8e\u4e4b\u524d\u6a21\u578b\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u6a21\u62df\u5668\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u4ea4\u53c9\u8def\u53e3\u884c\u4e3a\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2506.08488", "pdf": "https://arxiv.org/pdf/2506.08488", "abs": "https://arxiv.org/abs/2506.08488", "authors": ["Ashutosh Dwivedi", "Siddhant Shivdutt Singh", "Ashutosh Modi"], "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at ACL Findings 2025, 22 pages (9 pages main content + 4\n  pages references + 9 pages appendix)", "summary": "In recent years, researchers have started analyzing the cultural sensitivity\nof LLMs. In this respect, Etiquettes have been an active area of research.\nEtiquettes are region-specific and are an essential part of the culture of a\nregion; hence, it is imperative to make LLMs sensitive to etiquettes. However,\nthere needs to be more resources in evaluating LLMs for their understanding and\nbias with regard to etiquettes. In this resource paper, we introduce EtiCor++,\na corpus of etiquettes worldwide. We introduce different tasks for evaluating\nLLMs for knowledge about etiquettes across various regions. Further, we\nintroduce various metrics for measuring bias in LLMs. Extensive experimentation\nwith LLMs shows inherent bias towards certain regions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86EtiCor++\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5bf9\u5168\u7403\u793c\u4eea\u7684\u7406\u89e3\u548c\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u4efb\u52a1\u548c\u6307\u6807\u3002", "motivation": "\u7814\u7a76LLMs\u7684\u6587\u5316\u654f\u611f\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u793c\u4eea\u7684\u7406\u89e3\u548c\u504f\u89c1\uff0c\u76ee\u524d\u7f3a\u4e4f\u76f8\u5173\u8d44\u6e90\u3002", "method": "\u6784\u5efaEtiCor++\u8bed\u6599\u5e93\uff0c\u8bbe\u8ba1\u8bc4\u4f30\u4efb\u52a1\u548c\u504f\u89c1\u5ea6\u91cf\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5bf9\u67d0\u4e9b\u5730\u533a\u5b58\u5728\u56fa\u6709\u504f\u89c1\u3002", "conclusion": "EtiCor++\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u7684\u6587\u5316\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.08526", "pdf": "https://arxiv.org/pdf/2506.08526", "abs": "https://arxiv.org/abs/2506.08526", "authors": ["Zhongtao Tian", "Wenhao Huang", "Zhidong Chen", "Xiao Wei Sun"], "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Visual localization remains challenging in dynamic environments where\nfluctuating lighting, adverse weather, and moving objects disrupt appearance\ncues. Despite advances in feature representation, current absolute pose\nregression methods struggle to maintain consistency under varying conditions.\nTo address this challenge, we propose a framework that synergistically combines\nmulti-scale feature learning with semantic scene understanding. Our approach\nemploys a hierarchical Transformer with cross-scale attention to fuse geometric\ndetails and contextual cues, preserving spatial precision while adapting to\nenvironmental changes. We improve the performance of this architecture with\nsemantic supervision via neural scene representation during training, guiding\nthe network to learn view-invariant features that encode persistent structural\ninformation while suppressing complex environmental interference. Experiments\non TartanAir demonstrate that our approach outperforms existing pose regression\nmethods in challenging scenarios with dynamic objects, illumination changes,\nand occlusions. Our findings show that integrating multi-scale processing with\nsemantic guidance offers a promising strategy for robust visual localization in\nreal-world dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u8bed\u4e49\u573a\u666f\u7406\u89e3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u5149\u7167\u3001\u5929\u6c14\u548c\u79fb\u52a8\u7269\u4f53\u7b49\u56e0\u7d20\u5e72\u6270\u89c6\u89c9\u5b9a\u4f4d\uff0c\u73b0\u6709\u7edd\u5bf9\u59ff\u6001\u56de\u5f52\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42Transformer\u4e0e\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u878d\u5408\u51e0\u4f55\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u7ed3\u5408\u8bed\u4e49\u76d1\u7763\u8bad\u7ec3\u7f51\u7edc\u3002", "result": "\u5728TartanAir\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u3001\u5149\u7167\u53d8\u5316\u548c\u906e\u6321\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u5904\u7406\u4e0e\u8bed\u4e49\u6307\u5bfc\u7684\u7ed3\u5408\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2506.08963", "pdf": "https://arxiv.org/pdf/2506.08963", "abs": "https://arxiv.org/abs/2506.08963", "authors": ["Yash Ranjan", "Rahul Sengupta", "Anand Rangarajan", "Sanjay Ranka"], "title": "Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics", "categories": ["cs.AI"], "comment": null, "summary": "Traffic Intersections are vital to urban road networks as they regulate the\nmovement of people and goods. However, they are regions of conflicting\ntrajectories and are prone to accidents. Deep Generative models of traffic\ndynamics at signalized intersections can greatly help traffic authorities\nbetter understand the efficiency and safety aspects. At present, models are\nevaluated on computational metrics that primarily look at trajectory\nreconstruction errors. They are not evaluated online in a `live'\nmicrosimulation scenario. Further, these metrics do not adequately consider\ntraffic engineering-specific concerns such as red-light violations, unallowed\nstoppage, etc. In this work, we provide a comprehensive analytics tool to\ntrain, run, and evaluate models with metrics that give better insights into\nmodel performance from a traffic engineering point of view. We train a\nstate-of-the-art multi-vehicle trajectory forecasting model on a large dataset\ncollected by running a calibrated scenario of a real-world urban intersection.\nWe then evaluate the performance of the prediction models, online in a\nmicrosimulator, under unseen traffic conditions. We show that despite using\nideally-behaved trajectories as input, and achieving low trajectory\nreconstruction errors, the generated trajectories show behaviors that break\ntraffic rules. We introduce new metrics to evaluate such undesired behaviors\nand present our results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u4ea4\u901a\u751f\u6210\u6a21\u578b\u7684\u65b0\u5de5\u5177\uff0c\u91cd\u70b9\u5173\u6ce8\u4ea4\u901a\u5de5\u7a0b\u89c6\u89d2\u7684\u6027\u80fd\u6307\u6807\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u89c4\u5219\u9075\u5b88\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4ea4\u901a\u8def\u53e3\u662f\u57ce\u5e02\u8def\u7f51\u7684\u5173\u952e\uff0c\u4f46\u6613\u53d1\u751f\u4e8b\u6545\u3002\u73b0\u6709\u751f\u6210\u6a21\u578b\u4ec5\u5173\u6ce8\u8f68\u8ff9\u91cd\u5efa\u8bef\u5dee\uff0c\u7f3a\u4e4f\u5bf9\u4ea4\u901a\u5de5\u7a0b\u5b9e\u9645\u95ee\u9898\u7684\u8bc4\u4f30\u3002", "method": "\u8bad\u7ec3\u591a\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528\u771f\u5b9e\u8def\u53e3\u6570\u636e\uff0c\u5e76\u5728\u5fae\u6a21\u62df\u5668\u4e2d\u5728\u7ebf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5c3d\u7ba1\u8f93\u5165\u7406\u60f3\u8f68\u8ff9\u4e14\u91cd\u5efa\u8bef\u5dee\u4f4e\uff0c\u751f\u6210\u7684\u8f68\u8ff9\u4ecd\u8fdd\u53cd\u4ea4\u901a\u89c4\u5219\u3002\u65b0\u6307\u6807\u63ed\u793a\u4e86\u8fd9\u4e9b\u4e0d\u826f\u884c\u4e3a\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u9700\u4ece\u4ea4\u901a\u5de5\u7a0b\u89d2\u5ea6\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.08490", "pdf": "https://arxiv.org/pdf/2506.08490", "abs": "https://arxiv.org/abs/2506.08490", "authors": ["Xiao Wei", "Xiaobao Wang", "Ning Zhuang", "Chenyang Wang", "Longbiao Wang", "Jianwu dang"], "title": "Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework", "categories": ["cs.CL"], "comment": "9 pages, 2 figures, 7 tables, IJCAI 2025", "summary": "Intent detection aims to identify user intents from natural language inputs,\nwhere supervised methods rely heavily on labeled in-domain (IND) data and\nstruggle with out-of-domain (OOD) intents, limiting their practical\napplicability. Generalized Intent Discovery (GID) addresses this by leveraging\nunlabeled OOD data to discover new intents without additional annotation.\nHowever, existing methods focus solely on clustering unsupervised data while\nneglecting domain adaptation. Therefore, we propose a consistency-driven\nprototype-prompting framework for GID from the perspective of integrating old\nand new knowledge, which includes a prototype-prompting framework for\ntransferring old knowledge from external sources, and a hierarchical\nconsistency constraint for learning new knowledge from target domains. We\nconducted extensive experiments and the results show that our method\nsignificantly outperforms all baseline methods, achieving state-of-the-art\nresults, which strongly demonstrates the effectiveness and generalization of\nour methods. Our source code is publicly available at\nhttps://github.com/smileix/cpp.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e00\u81f4\u6027\u9a71\u52a8\u7684\u539f\u578b\u63d0\u793a\u6846\u67b6\uff08GID\uff09\uff0c\u7528\u4e8e\u4ece\u65b0\u65e7\u77e5\u8bc6\u6574\u5408\u7684\u89d2\u5ea6\u89e3\u51b3\u5e7f\u4e49\u610f\u56fe\u53d1\u73b0\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u5904\u7406\u57df\u5916\u610f\u56fe\uff0c\u73b0\u6709GID\u65b9\u6cd5\u5ffd\u89c6\u57df\u9002\u5e94\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u539f\u578b\u63d0\u793a\u6846\u67b6\uff08\u8f6c\u79fb\u65e7\u77e5\u8bc6\uff09\u548c\u5206\u5c42\u4e00\u81f4\u6027\u7ea6\u675f\uff08\u5b66\u4e60\u65b0\u77e5\u8bc6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8fbe\u5230SOTA\u6548\u679c\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08529", "pdf": "https://arxiv.org/pdf/2506.08529", "abs": "https://arxiv.org/abs/2506.08529", "authors": ["Xijun Wang", "Xin Li", "Bingchen Li", "Zhibo Chen"], "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\\times$RTX 4090s", "categories": ["cs.CV"], "comment": "Project page: https://kopperx.github.io/projects/liftvsr", "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.", "AI": {"tldr": "LiftVSR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u5185\u5b58\u7f13\u5b58\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u4e86\u957f\u671f\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u957f\u89c6\u9891\u5904\u7406\u9700\u8981\u5927\u91cfGPU\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u65f6\u95f4\u5efa\u6a21\u673a\u5236\uff0c\u5305\u62ec\u52a8\u6001\u65f6\u95f4\u6ce8\u610f\u529b\uff08DTA\uff09\u548c\u6ce8\u610f\u529b\u5185\u5b58\u7f13\u5b58\uff08AMC\uff09\uff0c\u5e76\u7ed3\u5408\u975e\u5bf9\u79f0\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2aVSR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "LiftVSR\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08970", "pdf": "https://arxiv.org/pdf/2506.08970", "abs": "https://arxiv.org/abs/2506.08970", "authors": ["Jiyao Wei", "Saiping Guan", "Da Li", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "A Survey of Link Prediction in N-ary Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph\ndesigned to efficiently represent complex real-world facts. Unlike traditional\nknowledge graphs, where a fact typically involves two entities, NKGs can\ncapture n-ary facts containing more than two entities. Link prediction in NKGs\naims to predict missing elements within these n-ary facts, which is essential\nfor completing NKGs and improving the performance of downstream applications.\nThis task has recently gained significant attention. In this paper, we present\nthe first comprehensive survey of link prediction in NKGs, providing an\noverview of the field, systematically categorizing existing methods, and\nanalyzing their performance and application scenarios. We also outline\npromising directions for future research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86N\u5143\u77e5\u8bc6\u56fe\u8c31\uff08NKGs\uff09\u4e2d\u7684\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\uff0c\u5305\u62ec\u65b9\u6cd5\u5206\u7c7b\u3001\u6027\u80fd\u5206\u6790\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "NKGs\u80fd\u9ad8\u6548\u8868\u793a\u590d\u6742\u4e8b\u5b9e\uff0c\u4f46\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u5c1a\u672a\u6709\u7cfb\u7edf\u7efc\u8ff0\uff0c\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u6027\u80fd\u548c\u5e94\u7528\u573a\u666f\u3002", "result": "\u603b\u7ed3\u4e86NKGs\u94fe\u63a5\u9884\u6d4b\u7684\u73b0\u72b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\uff0c\u4e3aNKGs\u94fe\u63a5\u9884\u6d4b\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.08500", "pdf": "https://arxiv.org/pdf/2506.08500", "abs": "https://arxiv.org/abs/2506.08500", "authors": ["Arie Cattan", "Alon Jacovi", "Ori Ram", "Jonathan Herzig", "Roee Aharoni", "Sasha Goldshtein", "Eran Ofek", "Idan Szpektor", "Avi Caciularu"], "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is a commonly used approach for\nenhancing large language models (LLMs) with relevant and up-to-date\ninformation. However, the retrieved sources can often contain conflicting\ninformation and it remains unclear how models should address such\ndiscrepancies. In this work, we first propose a novel taxonomy of knowledge\nconflict types in RAG, along with the desired model behavior for each type. We\nthen introduce CONFLICTS, a high-quality benchmark with expert annotations of\nconflict types in a realistic RAG setting. CONFLICTS is the first benchmark\nthat enables tracking progress on how models address a wide range of knowledge\nconflicts. We conduct extensive experiments on this benchmark, showing that\nLLMs often struggle to appropriately resolve conflicts between sources. While\nprompting LLMs to explicitly reason about the potential conflict in the\nretrieved documents significantly improves the quality and appropriateness of\ntheir responses, substantial room for improvement in future research remains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RAG\u77e5\u8bc6\u51b2\u7a81\u5206\u7c7b\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86CONFLICTS\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u89e3\u51b3\u51b2\u7a81\u4fe1\u606f\u65f6\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728\u51b2\u7a81\u89e3\u51b3\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u63d0\u793a\u5176\u663e\u5f0f\u63a8\u7406\u53ef\u663e\u8457\u6539\u8fdb\u3002", "motivation": "RAG\u65b9\u6cd5\u4e2d\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u5e38\u5b58\u5728\u51b2\u7a81\uff0c\u4f46LLMs\u5982\u4f55\u5e94\u5bf9\u6b64\u7c7b\u51b2\u7a81\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u548c\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u51b2\u7a81\u5206\u7c7b\u6cd5\uff0c\u5e76\u6784\u5efaCONFLICTS\u57fa\u51c6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30LLMs\u5728\u51b2\u7a81\u89e3\u51b3\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u89e3\u51b3\u51b2\u7a81\u4fe1\u606f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u63d0\u793a\u53ef\u663e\u8457\u63d0\u5347\u5176\u56de\u7b54\u8d28\u91cf\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u6539\u8fdbLLMs\u5728RAG\u4e2d\u5904\u7406\u51b2\u7a81\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2506.08541", "pdf": "https://arxiv.org/pdf/2506.08541", "abs": "https://arxiv.org/abs/2506.08541", "authors": ["Qi Yan", "Brian Zhang", "Yutong Zhang", "Daniel Yang", "Joshua White", "Di Chen", "Jiachao Liu", "Langechuan Liu", "Binnan Zhuang", "Shaoshuai Shi", "Renjie Liao"], "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and\ninformed decision-making in autonomous driving, particularly under dynamic\nreal-world conditions that necessitate multi-modal forecasts. We introduce\nTrajFlow, a novel flow matching-based motion prediction framework that\naddresses the scalability and efficiency challenges of existing generative\ntrajectory prediction methods. Unlike conventional generative approaches that\nemploy i.i.d. sampling and require multiple inference passes to capture diverse\noutcomes, TrajFlow predicts multiple plausible future trajectories in a single\npass, significantly reducing computational overhead while maintaining coherence\nacross predictions. Moreover, we propose a ranking loss based on the\nPlackett-Luce distribution to improve uncertainty estimation of predicted\ntrajectories. Additionally, we design a self-conditioning training technique\nthat reuses the model's own predictions to construct noisy inputs during a\nsecond forward pass, thereby improving generalization and accelerating\ninference. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across\nvarious key metrics, underscoring its effectiveness for safety-critical\nautonomous driving applications. The code and other details are available on\nthe project website https://traj-flow.github.io/.", "AI": {"tldr": "TrajFlow\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u9884\u6d4b\u751f\u6210\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u7684\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u9ad8\u6548\u51c6\u786e\u7684\u8fd0\u52a8\u9884\u6d4b\u5bf9\u5b89\u5168\u548c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u591a\u6a21\u6001\u573a\u666f\u4e0b\u3002\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTrajFlow\u6846\u67b6\uff0c\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u5355\u6b21\u751f\u6210\u591a\u6a21\u6001\u8f68\u8ff9\uff1b\u5f15\u5165\u57fa\u4e8ePlackett-Luce\u5206\u5e03\u7684\u6392\u5e8f\u635f\u5931\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b\u8bbe\u8ba1\u81ea\u6761\u4ef6\u8bad\u7ec3\u6280\u672f\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728Waymo Open Motion Dataset\u4e0a\uff0cTrajFlow\u5728\u591a\u9879\u5173\u952e\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "TrajFlow\u4e3a\u5b89\u5168\u5173\u952e\u7684\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u8fd0\u52a8\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09038", "pdf": "https://arxiv.org/pdf/2506.09038", "abs": "https://arxiv.org/abs/2506.09038", "authors": ["Polina Kirichenko", "Mark Ibrahim", "Kamalika Chaudhuri", "Samuel J. Bell"], "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions", "categories": ["cs.AI"], "comment": null, "summary": "For Large Language Models (LLMs) to be reliably deployed in both everyday and\nhigh-stakes domains, knowing when not to answer is equally critical as\nanswering correctly. Real-world user queries, which can be underspecified,\nill-posed, or fundamentally unanswerable, require LLMs to reason about\nuncertainty and selectively abstain -- i.e., refuse to answer definitively.\nHowever, abstention remains understudied, without a systematic evaluation\nframework for modern LLMs. In this work, we introduce AbstentionBench, a\nlarge-scale benchmark for holistically evaluating abstention across 20 diverse\ndatasets, including questions with unknown answers, underspecification, false\npremises, subjective interpretations, and outdated information. Evaluating 20\nfrontier LLMs reveals abstention is an unsolved problem, and one where scaling\nmodels is of little use. While recent reasoning LLMs have shown impressive\nresults in complex problem solving, surprisingly, we find that reasoning\nfine-tuning degrades abstention (by $24\\%$ on average), even for math and\nscience domains on which reasoning models are explicitly trained. We find that\nwhile a carefully crafted system prompt can boost abstention in practice, it\ndoes not resolve models' fundamental inability to reason about uncertainty. We\nrelease AbstentionBench to foster research into advancing LLM reliability.", "AI": {"tldr": "AbstentionBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u62d2\u7edd\u56de\u7b54\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d620\u4e2a\u6570\u636e\u96c6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u6269\u5c55\u5bf9\u63d0\u5347\u62d2\u7edd\u56de\u7b54\u80fd\u529b\u5e2e\u52a9\u6709\u9650\uff0c\u4e14\u63a8\u7406\u5fae\u8c03\u53cd\u800c\u4f1a\u964d\u4f4e\u8be5\u80fd\u529b\u3002", "motivation": "\u4e3a\u786e\u4fddLLMs\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u9760\u90e8\u7f72\uff0c\u9700\u8981\u7814\u7a76\u5176\u5728\u4e0d\u786e\u5b9a\u6216\u65e0\u6cd5\u56de\u7b54\u65f6\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165AbstentionBench\u57fa\u51c6\uff0c\u8bc4\u4f3020\u4e2a\u524d\u6cbfLLMs\u5728\u591a\u79cd\u95ee\u9898\u7c7b\u578b\uff08\u5982\u672a\u77e5\u7b54\u6848\u3001\u6a21\u7cca\u95ee\u9898\u7b49\uff09\u4e0b\u7684\u62d2\u7edd\u56de\u7b54\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u62d2\u7edd\u56de\u7b54\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u6a21\u578b\u6269\u5c55\u548c\u63a8\u7406\u5fae\u8c03\u5bf9\u5176\u5e2e\u52a9\u6709\u9650\uff0c\u751a\u81f3\u53ef\u80fd\u964d\u4f4e\u8868\u73b0\u3002", "conclusion": "AbstentionBench\u7684\u53d1\u5e03\u65e8\u5728\u63a8\u52a8LLM\u53ef\u9760\u6027\u7814\u7a76\uff0c\u5f53\u524d\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002"}}
{"id": "2506.08504", "pdf": "https://arxiv.org/pdf/2506.08504", "abs": "https://arxiv.org/abs/2506.08504", "authors": ["Divyaksh Shukla", "Ritesh Baviskar", "Dwijesh Gohil", "Aniket Tiwari", "Atul Shree", "Ashutosh Modi"], "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025 (16 pages: 5 pages main content + 3\n  pages references + 8 pages appendix)", "summary": "Discourse parsing is an important task useful for NLU applications such as\nsummarization, machine comprehension, and emotion recognition. The current\ndiscourse parsing datasets based on conversations consists of written English\ndialogues restricted to a single domain. In this resource paper, we introduce\nCoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in\nconversations. The corpus (code-mixed in Hindi and English) has both audio and\ntranscribed text and is annotated with nine discourse relations. We experiment\nwith various SoTA baseline models; the poor performance of SoTA models\nhighlights the challenges of multi-domain code-mixed corpus, pointing towards\nthe need for developing better models for such realistic settings.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CoMuMDR\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u591a\u9886\u57df\u4ee3\u7801\u6df7\u5408\u5bf9\u8bdd\u7684\u7bc7\u7ae0\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u7bc7\u7ae0\u5206\u6790\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\u7684\u82f1\u6587\u5bf9\u8bdd\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u548c\u4ee3\u7801\u6df7\u5408\u7684\u591a\u6837\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u5370\u5730\u8bed\u548c\u82f1\u8bed\u4ee3\u7801\u6df7\u5408\u7684\u591a\u6a21\u6001\u8bed\u6599\u5e93\uff0c\u5e76\u6807\u6ce8\u4e86\u4e5d\u79cd\u7bc7\u7ae0\u5173\u7cfb\u3002", "result": "\u73b0\u6709SoTA\u6a21\u578b\u5728CoMuMDR\u8bed\u6599\u5e93\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5e94\u591a\u9886\u57df\u4ee3\u7801\u6df7\u5408\u573a\u666f\u7684\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u548c\u4ee3\u7801\u6df7\u5408\u7684\u7bc7\u7ae0\u5206\u6790\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u6a21\u578b\u3002"}}
{"id": "2506.08543", "pdf": "https://arxiv.org/pdf/2506.08543", "abs": "https://arxiv.org/abs/2506.08543", "authors": ["Bowei Tian", "Xuntao Lyu", "Meng Liu", "Hongyi Wang", "Ang Li"], "title": "Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2503.22720", "summary": "High-level representations have become a central focus in enhancing AI\ntransparency and control, shifting attention from individual neurons or\ncircuits to structured semantic directions that align with human-interpretable\nconcepts. Motivated by the Linear Representation Hypothesis (LRH), we propose\nthe Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned\ndirections originate in the input space and are selectively amplified with\nincreasing depth. We then introduce the Spectral Principal Path (SPP)\nframework, which formalizes how deep networks progressively distill linear\nrepresentations along a small set of dominant spectral directions. Building on\nthis framework, we further demonstrate the multimodal robustness of these\nrepresentations in Vision-Language Models (VLMs). By bridging theoretical\ninsights with empirical validation, this work advances a structured theory of\nrepresentation formation in deep networks, paving the way for improving AI\nrobustness, fairness, and transparency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8f93\u5165\u7a7a\u95f4\u7ebf\u6027\u5047\u8bbe\uff08ISLH\uff09\uff0c\u8ba4\u4e3a\u6982\u5ff5\u5bf9\u9f50\u65b9\u5411\u6e90\u4e8e\u8f93\u5165\u7a7a\u95f4\u5e76\u901a\u8fc7\u6df1\u5ea6\u7f51\u7edc\u9009\u62e9\u6027\u653e\u5927\uff0c\u5f15\u5165\u8c31\u4e3b\u8def\u5f84\uff08SPP\uff09\u6846\u67b6\uff0c\u9a8c\u8bc1\u5176\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u9c81\u68d2\u6027\u3002", "motivation": "\u57fa\u4e8e\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff08LRH\uff09\uff0c\u63a2\u7d22\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6982\u5ff5\u5bf9\u9f50\u65b9\u5411\u7684\u8d77\u6e90\u4e0e\u5f62\u6210\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8AI\u7684\u900f\u660e\u5ea6\u4e0e\u63a7\u5236\u6027\u3002", "method": "\u63d0\u51faISLH\u5047\u8bbe\uff0c\u5f00\u53d1SPP\u6846\u67b6\u5206\u6790\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7ebf\u6027\u8868\u793a\u7684\u9010\u6b65\u63d0\u70bc\u8fc7\u7a0b\uff0c\u5e76\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "SPP\u6846\u67b6\u63ed\u793a\u4e86\u6df1\u5ea6\u7f51\u7edc\u6cbf\u5c11\u6570\u4e3b\u5bfc\u8c31\u65b9\u5411\u9010\u6b65\u63d0\u70bc\u7ebf\u6027\u8868\u793a\u7684\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6df1\u5ea6\u7f51\u7edc\u8868\u793a\u5f62\u6210\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7406\u8bba\uff0c\u6709\u52a9\u4e8e\u63d0\u5347AI\u7684\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2506.09049", "pdf": "https://arxiv.org/pdf/2506.09049", "abs": "https://arxiv.org/abs/2506.09049", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://faceong.github.io/VIKI-R/", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "AI": {"tldr": "VIKI-Bench\u662f\u9996\u4e2a\u9488\u5bf9\u5177\u8eab\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u5206\u5c42\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4f53\u73b0\u548c\u591a\u89c6\u89d2\u89c6\u89c9\u89c2\u5bdf\u3002VIKI-R\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u534f\u4f5c\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u9a71\u52a8\u7684\u63a8\u7406\u548c\u591a\u6837\u5316\u4f53\u73b0\u7684\u652f\u6301\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faVIKI-Bench\u57fa\u51c6\u6d4b\u8bd5\u548cVIKI-R\u6846\u67b6\uff0c\u540e\u8005\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "VIKI-R\u5728\u6240\u6709\u4efb\u52a1\u5c42\u7ea7\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f02\u6784\u667a\u80fd\u4f53\u7684\u7ec4\u5408\u534f\u4f5c\u6a21\u5f0f\u3002", "conclusion": "VIKI-Bench\u548cVIKI-R\u4e3a\u5177\u8eabAI\u7cfb\u7edf\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u89c6\u89c9\u9a71\u52a8\u534f\u4f5c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u65b9\u6cd5\u3002"}}
{"id": "2506.08552", "pdf": "https://arxiv.org/pdf/2506.08552", "abs": "https://arxiv.org/abs/2506.08552", "authors": ["Xinyuan Wang", "Dongjie Wang", "Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Sixun Dong", "Kunpeng Liu", "Yanjie Fu"], "title": "Efficient Post-Training Refinement of Latent Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning is a key component of language understanding in Large Language\nModels. While Chain-of-Thought prompting enhances performance via explicit\nintermediate steps, it suffers from sufficient token overhead and a fixed\nreasoning trajectory, preventing step-wise refinement. Recent advances in\nlatent reasoning address these limitations by refining internal reasoning\nprocesses directly in the model's latent space, without producing explicit\noutputs. However, a key challenge remains: how to effectively update reasoning\nembeddings during post-training to guide the model toward more accurate\nsolutions. To overcome this challenge, we propose a lightweight post-training\nframework that refines latent reasoning trajectories using two novel\nstrategies: 1) Contrastive reasoning feedback, which compares reasoning\nembeddings against strong and weak baselines to infer effective update\ndirections via embedding enhancement; 2) Residual embedding refinement, which\nstabilizes updates by progressively integrating current and historical\ngradients, enabling fast yet controlled convergence. Extensive experiments and\ncase studies are conducted on five reasoning benchmarks to demonstrate the\neffectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA\nwithout additional training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u63a8\u7406\u53cd\u9988\u548c\u6b8b\u5dee\u5d4c\u5165\u7ec6\u5316\u7b56\u7565\u4f18\u5316\u6f5c\u5728\u63a8\u7406\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\uff08\u5982Chain-of-Thought\uff09\u5b58\u5728\u56fa\u5b9a\u8f68\u8ff9\u548c\u663e\u5f0f\u8f93\u51fa\u5f00\u9500\u95ee\u9898\uff0c\u800c\u6f5c\u5728\u63a8\u7406\u867d\u80fd\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u540e\u8bad\u7ec3\u66f4\u65b0\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a1\uff09\u5bf9\u6bd4\u63a8\u7406\u53cd\u9988\uff0c\u901a\u8fc7\u5f3a/\u5f31\u57fa\u7ebf\u6bd4\u8f83\u4f18\u5316\u5d4c\u5165\u65b9\u5411\uff1b2\uff09\u6b8b\u5dee\u5d4c\u5165\u7ec6\u5316\uff0c\u7ed3\u5408\u5386\u53f2\u68af\u5ea6\u5b9e\u73b0\u7a33\u5b9a\u66f4\u65b0\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0cMathQA\u51c6\u786e\u7387\u63d0\u53475%\u3002", "conclusion": "\u6846\u67b6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002"}}
{"id": "2506.08553", "pdf": "https://arxiv.org/pdf/2506.08553", "abs": "https://arxiv.org/abs/2506.08553", "authors": ["Agnese Taluzzi", "Davide Gesualdi", "Riccardo Santambrogio", "Chiara Plizzari", "Francesca Palermo", "Simone Mentasti", "Matteo Matteucci"], "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge", "categories": ["cs.CV"], "comment": "Technical report for the HD-EPIC VQA Challenge 2025 (1st place)", "summary": "This report presents SceneNet and KnowledgeNet, our approaches developed for\nthe HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with\na multi-modal large language model (MLLM) to capture fine-grained object\ninteractions, spatial relationships, and temporally grounded events. In\nparallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge\nto introduce high-level semantic connections between entities, enabling\nreasoning beyond directly observable visual evidence. Each method demonstrates\ndistinct strengths across the seven categories of the HD-EPIC benchmark, and\ntheir combination within our framework results in an overall accuracy of 44.21%\non the challenge, highlighting its effectiveness for complex egocentric VQA\ntasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SceneNet\u548cKnowledgeNet\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3HD-EPIC VQA Challenge 2025\u4e2d\u7684\u590d\u6742\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u7ed3\u5408\u573a\u666f\u56fe\u548c\u5916\u90e8\u5e38\u8bc6\u77e5\u8bc6\uff0c\u6700\u7ec8\u8fbe\u523044.21%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u4ea4\u4e92\u548c\u5916\u90e8\u5e38\u8bc6\u77e5\u8bc6\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "SceneNet\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u573a\u666f\u56fe\uff0c\u6355\u6349\u5bf9\u8c61\u4ea4\u4e92\u548c\u65f6\u7a7a\u5173\u7cfb\uff1bKnowledgeNet\u5f15\u5165ConceptNet\u7684\u5916\u90e8\u5e38\u8bc6\u77e5\u8bc6\uff0c\u589e\u5f3a\u8bed\u4e49\u8fde\u63a5\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728HD-EPIC\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e03\u4e2a\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ec4\u5408\u6846\u67b6\u7684\u603b\u4f53\u51c6\u786e\u7387\u4e3a44.21%\u3002", "conclusion": "SceneNet\u548cKnowledgeNet\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09050", "pdf": "https://arxiv.org/pdf/2506.09050", "abs": "https://arxiv.org/abs/2506.09050", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "categories": ["cs.AI"], "comment": "36 pages", "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "AI": {"tldr": "ALE-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u57fa\u4e8e\u5206\u6570\u7684\u7b97\u6cd5\u7f16\u7a0b\u7ade\u8d5b\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u786c\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u7b97\u6cd5\u5de5\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4f18\u5316\u95ee\u9898\uff08\u5982\u7269\u6d41\u8def\u7531\u3001\u6392\u73ed\u7b49\uff09\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528AtCoder Heuristic Contests\u7684\u771f\u5b9e\u4efb\u52a1\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u4ee3\u7406\u67b6\u6784\u548c\u6d4b\u8bd5\u53cd\u9988\u3002", "result": "\u524d\u6cbfLLMs\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8de8\u95ee\u9898\u4e00\u81f4\u6027\u548c\u957f\u671f\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u3002", "conclusion": "ALE-Bench\u6709\u52a9\u4e8e\u63a8\u52a8AI\u5728\u7b97\u6cd5\u5de5\u7a0b\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2506.08564", "pdf": "https://arxiv.org/pdf/2506.08564", "abs": "https://arxiv.org/abs/2506.08564", "authors": ["Tuukka T\u00f6r\u00f6", "Antti Suni", "Juraj \u0160imko"], "title": "Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "27 pages, 11 figures (+5 supplementary), submitted to PLOS One", "summary": "Investigating linguistic relationships on a global scale requires analyzing\ndiverse features such as syntax, phonology and prosody, which evolve at varying\nrates influenced by internal diversification, language contact, and\nsociolinguistic factors. Recent advances in machine learning (ML) offer\ncomplementary alternatives to traditional historical and typological\napproaches. Instead of relying on expert labor in analyzing specific linguistic\nfeatures, these new methods enable the exploration of linguistic variation\nthrough embeddings derived directly from speech, opening new avenues for\nlarge-scale, data-driven analyses.\n  This study employs embeddings from the fine-tuned XLS-R self-supervised\nlanguage identification model voxlingua107-xls-r-300m-wav2vec, to analyze\nrelationships between 106 world languages based on speech recordings. Using\nlinear discriminant analysis (LDA), language embeddings are clustered and\ncompared with genealogical, lexical, and geographical distances. The results\ndemonstrate that embedding-based distances align closely with traditional\nmeasures, effectively capturing both global and local typological patterns.\nChallenges in visualizing relationships, particularly with hierarchical\nclustering and network-based methods, highlight the dynamic nature of language\nchange.\n  The findings show potential for scalable analyses of language variation based\non speech embeddings, providing new perspectives on relationships among\nlanguages. By addressing methodological considerations such as corpus size and\nlatent space dimensionality, this approach opens avenues for studying\nlow-resource languages and bridging macro- and micro-level linguistic\nvariation. Future work aims to extend these methods to underrepresented\nlanguages and integrate sociolinguistic variation for a more comprehensive\nunderstanding of linguistic diversity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u8bed\u97f3\u5d4c\u5165\uff0c\u63a2\u7d22106\u79cd\u4e16\u754c\u8bed\u8a00\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u5176\u4e0e\u4f20\u7edf\u8bed\u8a00\u5b66\u65b9\u6cd5\u7ed3\u679c\u4e00\u81f4\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u8a00\u53d8\u5316\u7684\u52a8\u6001\u6027\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u5b66\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u5206\u6790\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\uff0c\u800c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u3001\u6570\u636e\u9a71\u52a8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u63a2\u7d22\u8bed\u8a00\u5173\u7cfb\u3002", "method": "\u4f7f\u7528XLS-R\u81ea\u76d1\u7763\u8bed\u8a00\u8bc6\u522b\u6a21\u578b\u751f\u6210\u8bed\u97f3\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5224\u522b\u5206\u6790\uff08LDA\uff09\u805a\u7c7b\u8bed\u8a00\uff0c\u4e0e\u4f20\u7edf\u8bed\u8a00\u5b66\u8ddd\u79bb\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8bed\u97f3\u5d4c\u5165\u7684\u8ddd\u79bb\u4e0e\u4f20\u7edf\u8bed\u8a00\u5b66\u8ddd\u79bb\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u7684\u8bed\u8a00\u7c7b\u578b\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bed\u8a00\u53d8\u5f02\u7684\u53ef\u6269\u5c55\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u793e\u4f1a\u8bed\u8a00\u5b66\u53d8\u5f02\u7684\u7814\u7a76\u3002"}}
{"id": "2506.08555", "pdf": "https://arxiv.org/pdf/2506.08555", "abs": "https://arxiv.org/abs/2506.08555", "authors": ["Xinyue Niu", "Akira Furui"], "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement", "categories": ["cs.CV", "cs.HC"], "comment": "6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025", "summary": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u6d88\u9664\u6821\u51c6\u9700\u6c42\u7684\u8de8\u4e3b\u9898EMG\u6a21\u5f0f\u8bc6\u522b\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u5206\u652f\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u3002", "motivation": "\u8de8\u4e3b\u9898EMG\u6a21\u5f0f\u8bc6\u522b\u56e0\u4e2a\u4f53\u5dee\u5f02\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6821\u51c6\u6570\u636e\uff0c\u8017\u65f6\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u53cc\u5206\u652f\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06EMG\u7279\u5f81\u89e3\u8026\u4e3a\u6a21\u5f0f\u7279\u5b9a\u548c\u4e3b\u9898\u7279\u5b9a\u7ec4\u4ef6\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u7528\u6237\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u6821\u51c6\u8de8\u4e3b\u9898EMG\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4efb\u52a1\u65e0\u5173\u751f\u7269\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.00160", "pdf": "https://arxiv.org/pdf/2506.00160", "abs": "https://arxiv.org/abs/2506.00160", "authors": ["Qihui Fan", "Enfu Nan", "Wenbo Li", "Lei Lu", "Pu Zhao", "Yanzhi Wang"], "title": "Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The growing popularity of social deduction game systems for both business\napplications and AI research has greatly benefited from the rapid advancements\nin Large Language Models (LLMs), which now demonstrate stronger reasoning and\npersuasion capabilities. Especially with the raise of DeepSeek R1 and V3\nmodels, LLMs should enable a more engaging experience for human players in\nLLM-agent-based social deduction games like Werewolf. Previous works either\nfine-tuning, advanced prompting engineering, or additional experience pool to\nachieve engaging text-format Werewolf game experience. We propose a novel yet\nstraightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)\nmodels designed for enhanced compatibility with various LLM models, and\nimproved user engagement. We argue with ever enhancing LLM reasoning, extra\ncomponents will be unnecessary in the case of Werewolf.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b0\u578b\u72fc\u4eba\u6740\u6e38\u620f\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u7684TTS\u6a21\u578b\u63d0\u5347\u517c\u5bb9\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u8ba4\u4e3a\u968f\u7740LLM\u63a8\u7406\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u989d\u5916\u7ec4\u4ef6\u5c06\u53d8\u5f97\u591a\u4f59\u3002", "motivation": "\u968f\u7740LLM\u5728\u63a8\u7406\u548c\u8bf4\u670d\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\uff0c\u5c24\u5176\u662f\u5728DeepSeek R1\u548cV3\u6a21\u578b\u7684\u63a8\u52a8\u4e0b\uff0c\u63d0\u5347LLM\u4ee3\u7406\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\uff08\u5982\u72fc\u4eba\u6740\uff09\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u76f4\u63a5\u7684\u57fa\u4e8eLLM\u7684\u72fc\u4eba\u6740\u7cfb\u7edf\uff0c\u7ed3\u5408\u4f18\u5316\u7684TTS\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u4e0e\u4e0d\u540cLLM\u6a21\u578b\u7684\u517c\u5bb9\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u4f18\u5316\u7684TTS\u6a21\u578b\u63d0\u5347\u4e86\u517c\u5bb9\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u5916\u90e8\u7ec4\u4ef6\u7684\u4f9d\u8d56\u3002", "conclusion": "\u968f\u7740LLM\u63a8\u7406\u80fd\u529b\u7684\u6301\u7eed\u589e\u5f3a\uff0c\u672a\u6765\u5728\u72fc\u4eba\u6740\u7b49\u6e38\u620f\u4e2d\uff0c\u989d\u5916\u7684\u7ec4\u4ef6\u53ef\u80fd\u4e0d\u518d\u5fc5\u8981\u3002"}}
{"id": "2506.08584", "pdf": "https://arxiv.org/pdf/2506.08584", "abs": "https://arxiv.org/abs/2506.08584", "authors": ["Yahan Li", "Jifan Yao", "John Bosco S. Bunyi", "Adam C. Frank", "Angel Hwang", "Ruishan Liu"], "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly proposed for use in mental\nhealth support, yet their behavior in realistic counseling scenarios remains\nlargely untested. We introduce CounselBench, a large-scale benchmark developed\nwith 100 mental health professionals to evaluate and stress-test LLMs in\nsingle-turn counseling. The first component, CounselBench-EVAL, contains 2,000\nexpert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human\ntherapists to real patient questions. Each response is rated along six\nclinically grounded dimensions, with written rationales and span-level\nannotations. We find that LLMs often outperform online human therapists in\nperceived quality, but experts frequently flag their outputs for safety\nconcerns such as unauthorized medical advice. Follow-up experiments show that\nLLM judges consistently overrate model responses and overlook safety issues\nidentified by human experts. To probe failure modes more directly, we construct\nCounselBench-Adv, an adversarial dataset of 120 expert-authored counseling\nquestions designed to trigger specific model issues. Evaluation across 2,880\nresponses from eight LLMs reveals consistent, model-specific failure patterns.\nTogether, CounselBench establishes a clinically grounded framework for\nbenchmarking and improving LLM behavior in high-stakes mental health settings.", "AI": {"tldr": "CounselBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u538b\u529b\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5355\u8f6e\u5fc3\u7406\u54a8\u8be2\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\uff0c\u5305\u542b\u4e13\u5bb6\u8bc4\u4f30\u548c\u5bf9\u6297\u6027\u6570\u636e\u96c6\u3002\u7814\u7a76\u53d1\u73b0LLM\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4eba\u7c7b\u6cbb\u7597\u5e08\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u586b\u8865\u5176\u5728\u771f\u5b9e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\u4e2d\u884c\u4e3a\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1CounselBench\u57fa\u51c6\uff0c\u5305\u542b\u4e13\u5bb6\u8bc4\u4f30\uff08CounselBench-EVAL\uff09\u548c\u5bf9\u6297\u6027\u6570\u636e\u96c6\uff08CounselBench-Adv\uff09\uff0c\u5bf9LLM\u548c\u4eba\u7c7b\u6cbb\u7597\u5e08\u7684\u8868\u73b0\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u5206\u3002", "result": "LLM\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4eba\u7c7b\u6cbb\u7597\u5e08\uff0c\u4f46\u5e38\u88ab\u4e13\u5bb6\u6807\u8bb0\u5b89\u5168\u9690\u60a3\uff1bLLM\u8bc4\u59d4\u9ad8\u4f30\u6a21\u578b\u8868\u73b0\u4e14\u5ffd\u89c6\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "CounselBench\u4e3a\u9ad8\u98ce\u9669\u7684\u5fc3\u7406\u5065\u5eb7\u573a\u666f\u4e2dLLM\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e34\u5e8a\u57fa\u51c6\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2506.08562", "pdf": "https://arxiv.org/pdf/2506.08562", "abs": "https://arxiv.org/abs/2506.08562", "authors": ["Duc Thanh Pham", "Hong Dang Nguyen", "Nhat Minh Nguyen Quoc", "Linh Ngo Van", "Sang Dinh Viet", "Duc Anh Nguyen"], "title": "Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, object detection models have witnessed notable performance\nimprovements, particularly with transformer-based models. However, new objects\nfrequently appear in the real world, requiring detection models to continually\nlearn without suffering from catastrophic forgetting. Although Incremental\nObject Detection (IOD) has emerged to address this challenge, these existing\nmodels are still not practical due to their limited performance and prolonged\ninference time. In this paper, we introduce a novel framework for IOD, called\nHier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both\nefficiency and competitive performance by leveraging Neural Collapse for\nimbalance dataset and Hierarchical relation of classes' labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHier-DETR\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u91cf\u76ee\u6807\u68c0\u6d4b\uff08IOD\uff09\uff0c\u901a\u8fc7\u5229\u7528\u795e\u7ecf\u5d29\u6e83\u548c\u6807\u7b7e\u5c42\u6b21\u5173\u7cfb\uff0c\u517c\u987e\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u65b0\u7269\u4f53\u4e0d\u65ad\u51fa\u73b0\uff0c\u9700\u8981\u68c0\u6d4b\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u800c\u4e0d\u53d7\u707e\u96be\u6027\u9057\u5fd8\u5f71\u54cd\uff0c\u73b0\u6709IOD\u6a21\u578b\u6027\u80fd\u6709\u9650\u4e14\u63a8\u7406\u65f6\u95f4\u957f\u3002", "method": "\u91c7\u7528Hier-DETR\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u5d29\u6e83\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u5c42\u6b21\u5173\u7cfb\u3002", "result": "\u6846\u67b6\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Hier-DETR\u4e3aIOD\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.04760", "pdf": "https://arxiv.org/pdf/2506.04760", "abs": "https://arxiv.org/abs/2506.04760", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown potential in generating hypothetical\ndocuments for query expansion, thereby enhancing information retrieval\nperformance. However, the efficacy of this method is highly dependent on the\nquality of the generated documents, which often requires complex prompt\nstrategies and the integration of advanced dense retrieval techniques. This can\nbe both costly and computationally intensive. To mitigate these limitations, we\nexplore the use of zero-shot LLM-based query expansion to improve sparse\nretrieval, particularly for learned sparse retrievers. We introduce a novel\nfusion ranking framework, Exp4Fuse, which enhances the performance of sparse\nretrievers through an indirect application of zero-shot LLM-based query\nexpansion. Exp4Fuse operates by simultaneously considering two retrieval\nroutes-one based on the original query and the other on the LLM-augmented\nquery. It then generates two ranked lists using a sparse retriever and fuses\nthem using a modified reciprocal rank fusion method. We conduct extensive\nevaluations of Exp4Fuse against leading LLM-based query expansion methods and\nadvanced retrieval techniques on three MS MARCO-related datasets and seven\nlow-resource datasets. Experimental results reveal that Exp4Fuse not only\nsurpasses existing LLM-based query expansion methods in enhancing sparse\nretrievers but also, when combined with advanced sparse retrievers, achieves\nSOTA results on several benchmarks. This highlights the superior performance\nand effectiveness of Exp4Fuse in improving query expansion for sparse\nretrieval.", "AI": {"tldr": "Exp4Fuse\u662f\u4e00\u79cd\u65b0\u578b\u878d\u5408\u6392\u540d\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u6837\u672cLLM\u67e5\u8be2\u6269\u5c55\u63d0\u5347\u7a00\u758f\u68c0\u7d22\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLM\u751f\u6210\u7684\u6587\u6863\u8d28\u91cf\u5bf9\u67e5\u8be2\u6269\u5c55\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faExp4Fuse\u6846\u67b6\uff0c\u7ed3\u5408\u539f\u59cb\u67e5\u8be2\u548cLLM\u589e\u5f3a\u67e5\u8be2\u7684\u4e24\u6761\u68c0\u7d22\u8def\u5f84\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u4e92\u9006\u6392\u540d\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cExp4Fuse\u8d85\u8d8a\u73b0\u6709LLM\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u7ea7\u7a00\u758f\u68c0\u7d22\u5668\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "Exp4Fuse\u5728\u7a00\u758f\u68c0\u7d22\u67e5\u8be2\u6269\u5c55\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2506.08592", "pdf": "https://arxiv.org/pdf/2506.08592", "abs": "https://arxiv.org/abs/2506.08592", "authors": ["Liyan Xu", "Zhenlin Su", "Mo Yu", "Jiangnan Li", "Fandong Meng", "Jie Zhou"], "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u7f16\u7801\u5668\u5728\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u6216\u4e8b\u4ef6\u8bc6\u522b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e2d\u6587\u8bc4\u4f30\u6570\u636e\u96c6CapRetrieval\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u7f16\u7801\u5668\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u7f16\u7801\u5668\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5339\u914d\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u5bc6\u96c6\u68c0\u7d22\u7684\u6548\u679c\u3002", "method": "\u5f15\u5165CapRetrieval\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u6570\u636e\u751f\u6210\u7b56\u7565\u5fae\u8c03\u7f16\u7801\u5668\u3002", "result": "\u5fae\u8c03\u540e\u7684\u7f16\u7801\u5668\u5728CapRetrieval\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u53d1\u73b0\u4e86\u7c92\u5ea6\u56f0\u5883\u95ee\u9898\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5339\u914d\u4ecd\u9700\u6539\u8fdb\uff0c\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08566", "pdf": "https://arxiv.org/pdf/2506.08566", "abs": "https://arxiv.org/abs/2506.08566", "authors": ["Yibo Cui", "Liang Xie", "Yu Zhao", "Jiawei Sun", "Erwei Yin"], "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate\nenvironments by integrating visual perception and natural language\ninstructions, yet faces significant challenges due to the scarcity of\nfine-grained cross-modal alignment annotations. Existing datasets primarily\nfocus on global instruction-trajectory matching, neglecting\nsub-instruction-level and entity-level alignments critical for accurate\nnavigation action decision-making. To address this limitation, we propose\nFCA-NIG, a generative framework that automatically constructs navigation\ninstructions with dual-level fine-grained cross-modal annotations. In this\nframework, an augmented trajectory is first divided into sub-trajectories,\nwhich are then processed through GLIP-based landmark detection, crafted\ninstruction construction, OFA-Speaker based R2R-like instruction generation,\nand CLIP-powered entity selection, generating sub-instruction-trajectory pairs\nwith entity-landmark annotations. Finally, these sub-pairs are aggregated to\nform a complete instruction-trajectory pair. The framework generates the\nFCA-R2R dataset, the first large-scale augmentation dataset featuring precise\nsub-instruction-sub-trajectory and entity-landmark alignments. Extensive\nexperiments demonstrate that training with FCA-R2R significantly improves the\nperformance of multiple state-of-the-art VLN agents, including SF, EnvDrop,\nRecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances\nagents' state awareness and decision accuracy, while entity-landmark alignment\nfurther boosts navigation performance and generalization. These results\nhighlight the effectiveness of FCA-NIG in generating high-quality, scalable\ntraining data without manual annotation, advancing fine-grained cross-modal\nlearning in complex navigation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFCA-NIG\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u5177\u6709\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u6807\u6ce8\u7684\u5bfc\u822a\u6307\u4ee4\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5b50\u6307\u4ee4\u548c\u5b9e\u4f53\u7ea7\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6570\u636e\u96c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u6807\u6ce8\uff0c\u5f71\u54cd\u5bfc\u822a\u51b3\u7b56\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faFCA-NIG\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u8f68\u8ff9\u5206\u5272\u3001\u5730\u6807\u68c0\u6d4b\u3001\u6307\u4ee4\u751f\u6210\u548c\u5b9e\u4f53\u9009\u62e9\uff0c\u751f\u6210\u5e26\u6807\u6ce8\u7684\u5b50\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u3002", "result": "\u751f\u6210\u7684FCA-R2R\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2aVLN\u4ee3\u7406\u7684\u6027\u80fd\u3002", "conclusion": "FCA-NIG\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u63a8\u52a8\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5b66\u4e60\u3002"}}
{"id": "2506.05695", "pdf": "https://arxiv.org/pdf/2506.05695", "abs": "https://arxiv.org/abs/2506.05695", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge Distillation (KD) compresses large language models (LLMs) by\ntransferring the teacher model's capabilities to a smaller student model,\nreducing inference cost and memory usage while maintaining performance.\nHowever, existing KD methods for LLMs often fail to prevent significant shifts\nin the student model's distribution during training, leading to issues such as\ncatastrophic forgetting, mode collapse, and training-inference mismatch. To\naddress these challenges, we propose a novel, plug-in curriculum learning\nframework inspired by the strength training principle of \"progressive overload\"\n(POCL), which can be seamlessly integrated into existing white-box KD\napproaches with minimal computational overhead. The framework comprises two\ncore components: (1) a difficulty measurer that ranks and partitions training\nsamples from easy to hard, and (2) a training scheduler that incrementally\nintroduces these subsets into the distillation process at fixed intervals while\napplying loss functions with progressively rising temperatures. By starting\nwith the easiest samples and progressively increasing the difficulty, the\napproach enhances both the stability and efficiency of learning. Extensive\nexperiments in instruction-following settings demonstrate that POCL\nconsistently improves the performance of distilled student models across\nvarious white-box KD methods and model families. Our findings highlight the\neffectiveness of sorted training samples in KD for LLMs. More generally, our\nwork demonstrates how to structure training data within the KD process to\nenhance the stability and performance of distilled LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e10\u8fdb\u5f0f\u8fc7\u8f7d\u539f\u5219\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff08POCL\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5f15\u5165\u8bad\u7ec3\u6837\u672c\u63d0\u5347\u5b66\u4e60\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709KD\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5bfc\u81f4\u5b66\u751f\u6a21\u578b\u5206\u5e03\u663e\u8457\u504f\u79fb\uff0c\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u3001\u6a21\u5f0f\u5d29\u6e83\u7b49\u95ee\u9898\uff0c\u9700\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "POCL\u6846\u67b6\u5305\u542b\u96be\u5ea6\u6d4b\u91cf\u5668\u548c\u8bad\u7ec3\u8c03\u5ea6\u5668\uff0c\u5206\u9636\u6bb5\u4ece\u6613\u5230\u96be\u5f15\u5165\u6837\u672c\uff0c\u5e76\u9010\u6b65\u63d0\u9ad8\u635f\u5931\u51fd\u6570\u6e29\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePOCL\u80fd\u663e\u8457\u63d0\u5347\u84b8\u998f\u540e\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cdKD\u65b9\u6cd5\u548c\u6a21\u578b\u5bb6\u65cf\u3002", "conclusion": "POCL\u901a\u8fc7\u7ed3\u6784\u5316\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u4e86KD\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.08593", "pdf": "https://arxiv.org/pdf/2506.08593", "abs": "https://arxiv.org/abs/2506.08593", "authors": ["Shuzhou Yuan", "Ercong Nie", "Mario Tawfelis", "Helmut Schmid", "Hinrich Sch\u00fctze", "Michael F\u00e4rber"], "title": "Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is a socially sensitive and inherently subjective task,\nwith judgments often varying based on personal traits. While prior work has\nexamined how socio-demographic factors influence annotation, the impact of\npersonality traits on Large Language Models (LLMs) remains largely unexplored.\nIn this paper, we present the first comprehensive study on the role of persona\nprompts in hate speech classification, focusing on MBTI-based traits. A human\nannotation survey confirms that MBTI dimensions significantly affect labeling\nbehavior. Extending this to LLMs, we prompt four open-source models with MBTI\npersonas and evaluate their outputs across three hate speech datasets. Our\nanalysis uncovers substantial persona-driven variation, including\ninconsistencies with ground truth, inter-persona disagreement, and logit-level\nbiases. These findings highlight the need to carefully define persona prompts\nin LLM-based annotation workflows, with implications for fairness and alignment\nwith human values.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86MBTI\u4eba\u683c\u7279\u8d28\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u683c\u63d0\u793a\u4f1a\u5bfc\u81f4\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u516c\u5e73\u6027\u548c\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u542f\u793a\u3002", "motivation": "\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u4e14\u53d7\u4e2a\u4eba\u7279\u8d28\u5f71\u54cd\u3002\u6b64\u524d\u7814\u7a76\u591a\u5173\u6ce8\u793e\u4f1a\u4eba\u53e3\u56e0\u7d20\uff0c\u4f46\u4eba\u683c\u7279\u8d28\u5bf9LLMs\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u8c03\u67e5\u9a8c\u8bc1MBTI\u7ef4\u5ea6\u5bf9\u6807\u6ce8\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u56db\u4e2a\u5f00\u6e90LLMs\u4e2d\u5e94\u7528MBTI\u4eba\u683c\u63d0\u793a\uff0c\u8bc4\u4f30\u5176\u5728\u4e09\u4e2a\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u683c\u63d0\u793a\u5bfc\u81f4\u663e\u8457\u5dee\u5f02\uff0c\u5305\u62ec\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u4e0d\u4e00\u81f4\u3001\u4eba\u683c\u95f4\u5206\u6b67\u548c\u903b\u8f91\u5c42\u9762\u7684\u504f\u89c1\u3002", "conclusion": "\u9700\u8c28\u614e\u8bbe\u8ba1\u4eba\u683c\u63d0\u793a\uff0c\u4ee5\u786e\u4fddLLM\u6807\u6ce8\u6d41\u7a0b\u7684\u516c\u5e73\u6027\u548c\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.08591", "pdf": "https://arxiv.org/pdf/2506.08591", "abs": "https://arxiv.org/abs/2506.08591", "authors": ["Chengchao Shen", "Hourun Zhu", "Gongfan Fang", "Jianxin Wang", "Xinchao Wang"], "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Transformer models achieve excellent scaling property, where the performance\nis improved with the increment of model capacity. However, large-scale model\nparameters lead to an unaffordable cost of computing and memory. We analyze\npopular transformer architectures and find that multilayer perceptron (MLP)\nmodules take up the majority of model parameters. To this end, we focus on the\nrecoverability of the compressed models and propose a Diversity-Guided MLP\nReduction (DGMR) method to significantly reduce the parameters of large vision\ntransformers with only negligible performance degradation. Specifically, we\nconduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons\nof MLP hidden layer, while preserving weight diversity for better performance\nrecover during distillation. Compared to the model trained from scratch, our\npruned model only requires 0.06\\% data of LAION-2B (for the training of large\nvision transformers) without labels (ImageNet-1K) to recover the original\nperformance. Experimental results on several state-of-the-art large vision\ntransformers demonstrate that our method achieves a more than 57.0\\% parameter\nand FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),\nour method accomplishes a 71.5\\% parameter and FLOPs reduction without\nperformance degradation. The source code and trained weights are available at\nhttps://github.com/visresearch/DGMR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6837\u6027\u5f15\u5bfc\u7684MLP\u7f29\u51cf\u65b9\u6cd5\uff08DGMR\uff09\uff0c\u663e\u8457\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u53d8\u6362\u5668\u7684\u53c2\u6570\uff0c\u540c\u65f6\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u5927\u578b\u53d8\u6362\u5668\u6a21\u578b\u53c2\u6570\u8fc7\u591a\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u6602\uff0c\u7814\u7a76\u53d1\u73b0MLP\u6a21\u5757\u5360\u7528\u4e86\u5927\u90e8\u5206\u53c2\u6570\u3002", "method": "\u91c7\u7528Gram-Schmidt\u6743\u91cd\u526a\u679d\u7b56\u7565\u6d88\u9664MLP\u9690\u85cf\u5c42\u7684\u5197\u4f59\u795e\u7ecf\u5143\uff0c\u4fdd\u7559\u6743\u91cd\u591a\u6837\u6027\u4ee5\u63d0\u5347\u84b8\u998f\u6027\u80fd\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDGMR\u65b9\u6cd5\u5728\u591a\u4e2a\u5148\u8fdb\u89c6\u89c9\u53d8\u6362\u5668\u4e0a\u5b9e\u73b0\u4e8657%\u4ee5\u4e0a\u7684\u53c2\u6570\u548cFLOPs\u51cf\u5c11\uff0c\u6027\u80fd\u51e0\u4e4e\u65e0\u635f\u3002", "conclusion": "DGMR\u65b9\u6cd5\u9ad8\u6548\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u4e14\u6027\u80fd\u6062\u590d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2506.06363", "pdf": "https://arxiv.org/pdf/2506.06363", "abs": "https://arxiv.org/abs/2506.06363", "authors": ["Thang D. Pham", "Aditya Tanikanti", "Murat Ke\u00e7eli"], "title": "ChemGraph: An Agentic Framework for Computational Chemistry Workflows", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci", "cs.AI", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Atomistic simulations are essential tools in chemistry and materials science,\naccelerating the discovery of novel catalysts, energy storage materials, and\npharmaceuticals. However, running these simulations remains challenging due to\nthe wide range of computational methods, diverse software ecosystems, and the\nneed for expert knowledge and manual effort for the setup, execution, and\nvalidation stages. In this work, we present ChemGraph, an agentic framework\npowered by artificial intelligence and state-of-the-art simulation tools to\nstreamline and automate computational chemistry and materials science\nworkflows. ChemGraph leverages graph neural network-based foundation models for\naccurate yet computationally efficient calculations and large language models\n(LLMs) for natural language understanding, task planning, and scientific\nreasoning to provide an intuitive and interactive interface. Users can perform\ntasks such as molecular structure generation, single-point energy, geometry\noptimization, vibrational analysis, and thermochemistry calculations with\nmethods ranging from tight-binding and machine learning interatomic potentials\nto density functional theory or wave function theory-based methods. We evaluate\nChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs\n(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,\nwhile more complex tasks benefit from using larger models like GPT-4o.\nImportantly, we show that decomposing complex tasks into smaller subtasks\nthrough a multi-agent framework enables smaller LLM models to match or exceed\nGPT-4o's performance in specific scenarios.", "AI": {"tldr": "ChemGraph\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8ba1\u7b97\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7b80\u5316\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u3002", "motivation": "\u539f\u5b50\u6a21\u62df\u5728\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u65b9\u6cd5\u591a\u6837\u3001\u8f6f\u4ef6\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u6267\u884c\u8fd9\u4e9b\u6a21\u62df\u5177\u6709\u6311\u6218\u6027\u3002", "method": "ChemGraph\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u754c\u9762\uff0c\u652f\u6301\u591a\u79cd\u8ba1\u7b97\u4efb\u52a1\u548c\u65b9\u6cd5\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u5c0f\u578bLLM\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u590d\u6742\u4efb\u52a1\u9700\u66f4\u5927\u6a21\u578b\uff1b\u901a\u8fc7\u591a\u4ee3\u7406\u6846\u67b6\u5206\u89e3\u4efb\u52a1\uff0c\u5c0f\u578b\u6a21\u578b\u53ef\u5339\u914d\u6216\u8d85\u8d8aGPT-4o\u3002", "conclusion": "ChemGraph\u901a\u8fc7AI\u548c\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u539f\u5b50\u6a21\u62df\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2506.08625", "pdf": "https://arxiv.org/pdf/2506.08625", "abs": "https://arxiv.org/abs/2506.08625", "authors": ["Minhae Oh", "Jeonghye Kim", "Nakyung Lee", "Donggeon Seo", "Taeuk Kim", "Jungwoo Lee"], "title": "RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Scientific reasoning requires not only long-chain reasoning processes, but\nalso knowledge of domain-specific terminologies and adaptation to updated\nfindings. To deal with these challenges for scientific reasoning, we introduce\nRAISE, a step-by-step retrieval-augmented framework which retrieves logically\nrelevant documents from in-the-wild corpus. RAISE is divided into three steps:\nproblem decomposition, logical query generation, and logical retrieval. We\nobserve that RAISE consistently outperforms other baselines on scientific\nreasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves\ndocuments that are not only similar in terms of the domain knowledge, but also\ndocuments logically more relevant.", "AI": {"tldr": "RAISE\u662f\u4e00\u4e2a\u5206\u6b65\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u79d1\u5b66\u63a8\u7406\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u3001\u903b\u8f91\u67e5\u8be2\u751f\u6210\u548c\u903b\u8f91\u68c0\u7d22\u4e09\u4e2a\u6b65\u9aa4\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u63a8\u7406\u9700\u8981\u957f\u94fe\u63a8\u7406\u3001\u9886\u57df\u672f\u8bed\u77e5\u8bc6\u548c\u9002\u5e94\u65b0\u53d1\u73b0\uff0cRAISE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "RAISE\u5206\u4e3a\u4e09\u6b65\uff1a\u95ee\u9898\u5206\u89e3\u3001\u903b\u8f91\u67e5\u8be2\u751f\u6210\u548c\u903b\u8f91\u68c0\u7d22\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u903b\u8f91\u76f8\u5173\u6587\u6863\u3002", "result": "RAISE\u5728\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u68c0\u7d22\u7684\u6587\u6863\u4e0d\u4ec5\u9886\u57df\u77e5\u8bc6\u76f8\u4f3c\uff0c\u903b\u8f91\u76f8\u5173\u6027\u4e5f\u66f4\u5f3a\u3002", "conclusion": "RAISE\u901a\u8fc7\u903b\u8f91\u76f8\u5173\u6587\u6863\u68c0\u7d22\uff0c\u63d0\u5347\u4e86\u79d1\u5b66\u63a8\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2506.08596", "pdf": "https://arxiv.org/pdf/2506.08596", "abs": "https://arxiv.org/abs/2506.08596", "authors": ["Guyang Zhang", "Waleed Abdulla"], "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transformers have become the architecture of choice for learning long-range\ndependencies, yet their adoption in hyperspectral imaging (HSI) is still\nemerging. We reviewed more than 300 papers published up to 2025 and present the\nfirst end-to-end survey dedicated to Transformer-based HSI classification. The\nstudy categorizes every stage of a typical pipeline-pre-processing, patch or\npixel tokenization, positional encoding, spatial-spectral feature extraction,\nmulti-head self-attention variants, skip connections, and loss design-and\ncontrasts alternative design choices with the unique spatial-spectral\nproperties of HSI. We map the field's progress against persistent obstacles:\nscarce labeled data, extreme spectral dimensionality, computational overhead,\nand limited model explainability. Finally, we outline a research agenda\nprioritizing valuable public data sets, lightweight on-edge models,\nillumination and sensor shifts robustness, and intrinsically interpretable\nattention mechanisms. Our goal is to guide researchers in selecting, combining,\nor extending Transformer components that are truly fit for purpose for\nnext-generation HSI applications.", "AI": {"tldr": "\u672c\u6587\u662f\u5bf9Transformer\u5728HSI\u5206\u7c7b\u4e2d\u7684\u9996\u6b21\u7aef\u5230\u7aef\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86300\u591a\u7bc7\u8bba\u6587\uff0c\u5206\u6790\u4e86\u5404\u9636\u6bb5\u8bbe\u8ba1\u9009\u62e9\u4e0eHSI\u7279\u6027\u7684\u9002\u914d\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "Transformer\u5728HSI\u4e2d\u7684\u5e94\u7528\u5c1a\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u7c7b\u5206\u6790\u4e86HSI\u5206\u7c7b\u6d41\u7a0b\u4e2d\u7684\u5173\u952e\u73af\u8282\uff08\u5982\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u3001\u81ea\u6ce8\u610f\u529b\u53d8\u4f53\u7b49\uff09\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u603b\u7ed3\u4e86HSI\u9886\u57df\u7684\u8fdb\u5c55\u4e0e\u6311\u6218\uff08\u5982\u6570\u636e\u7a00\u7f3a\u3001\u8ba1\u7b97\u5f00\u9500\u5927\u7b49\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u91cd\u70b9\uff08\u5982\u8f7b\u91cf\u5316\u6a21\u578b\u3001\u53ef\u89e3\u91ca\u6027\u7b49\uff09\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u6216\u6269\u5c55Transformer\u7ec4\u4ef6\u7684\u6846\u67b6\uff0c\u4ee5\u63a8\u52a8\u4e0b\u4e00\u4ee3HSI\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.07675", "pdf": "https://arxiv.org/pdf/2506.07675", "abs": "https://arxiv.org/abs/2506.07675", "authors": ["Yuyang Song", "Hanxu Yan", "Jiale Lao", "Yibo Wang", "Yufei Li", "Yuanchun Zhou", "Jianguo Wang", "Mingjie Tang"], "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684SQL\u67e5\u8be2\u91cd\u5199\u7cfb\u7edfQUITE\uff0c\u89e3\u51b3\u4e86\u89c4\u5219\u65b9\u6cd5\u7684\u4e09\u9879\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u5b9e\u65f6\u53cd\u9988\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u89c4\u5219\u65b9\u6cd5\u5728SQL\u67e5\u8be2\u91cd\u5199\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u96be\u4ee5\u53d1\u73b0\u65b0\u89c4\u5219\u3001\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u67e5\u8be2\u6a21\u5f0f\u7b49\u3002\u4eba\u7c7b\u4e13\u5bb6\u80fd\u529b\u66f4\u5f3a\u4f46\u6269\u5c55\u6027\u5dee\uff0c\u800cLLM\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u4e49\u548c\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u63a2\u7d22LLM\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u548c\u5b9e\u65f6\u6570\u636e\u5e93\u53cd\u9988\uff1b\u5f00\u53d1\u4e86\u91cd\u5199\u4e2d\u95f4\u4ef6\u4f18\u5316\u67e5\u8be2\u7b49\u4ef7\u751f\u6210\uff1b\u91c7\u7528\u63d0\u793a\u6ce8\u5165\u6280\u672f\u6539\u8fdb\u6267\u884c\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQUITE\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c11\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u8fbe35.8%\uff0c\u91cd\u5199\u8986\u76d6\u7387\u63d0\u9ad824.1%\uff0c\u80fd\u5904\u7406\u6b64\u524d\u672a\u8986\u76d6\u7684\u67e5\u8be2\u6848\u4f8b\u3002", "conclusion": "QUITE\u901a\u8fc7LLM\u548c\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SQL\u67e5\u8be2\u91cd\u5199\u7684\u6027\u80fd\u548c\u8986\u76d6\u8303\u56f4\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u3002"}}
{"id": "2506.08643", "pdf": "https://arxiv.org/pdf/2506.08643", "abs": "https://arxiv.org/abs/2506.08643", "authors": ["Son The Nguyen", "Theja Tulabandhula"], "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for both open-ended and\nstructured tasks, yet their inference-time behavior is still largely dictated\nby heuristic decoding strategies such as greedy search, sampling, or reranking.\nThese methods provide limited control and do not explicitly optimize for\ntask-specific objectives. We introduce MEMETRON, a task-agnostic framework that\nformulates LLM decoding as a discrete black-box optimization problem. MEMETRON\nleverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the\nresponse space, guided by reward models and contextual operations performed by\nthe LLM itself. This approach enables efficient discovery of high-reward\nresponses without requiring model retraining or gradient access. The framework\nis modular and generalizes across diverse tasks, requiring only a reward\nfunction and lightweight prompt templates. We evaluate our framework on the\ncritical human preference alignment task and demonstrate that it significantly\noutperforms standard decoding and reranking methods, highlighting its potential\nto improve alignment without model retraining.", "AI": {"tldr": "MEMETRON\u662f\u4e00\u4e2a\u4efb\u52a1\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5c06LLM\u89e3\u7801\u89c6\u4e3a\u79bb\u6563\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u9ad8\u6548\u53d1\u73b0\u9ad8\u5956\u52b1\u54cd\u5e94\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u6216\u68af\u5ea6\u8bbf\u95ee\u3002", "motivation": "\u73b0\u6709LLM\u7684\u89e3\u7801\u7b56\u7565\uff08\u5982\u8d2a\u5a6a\u641c\u7d22\u3001\u91c7\u6837\u6216\u91cd\u6392\u5e8f\uff09\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u7684\u663e\u5f0f\u4f18\u5316\uff0c\u9650\u5236\u4e86\u63a7\u5236\u80fd\u529b\u3002", "method": "MEMETRON\u5229\u7528\u6df7\u5408\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08GENETRON\u548cANNETRON\uff09\u5728LLM\u7684\u54cd\u5e94\u7a7a\u95f4\u4e2d\u641c\u7d22\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u548c\u4e0a\u4e0b\u6587\u64cd\u4f5c\u5f15\u5bfc\u3002", "result": "\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0cMEMETRON\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u89e3\u7801\u548c\u91cd\u6392\u5e8f\u65b9\u6cd5\u3002", "conclusion": "MEMETRON\u5c55\u793a\u4e86\u5728\u4e0d\u91cd\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5bf9\u9f50\u80fd\u529b\u7684\u6f5c\u529b\uff0c\u5177\u6709\u6a21\u5757\u5316\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2506.08611", "pdf": "https://arxiv.org/pdf/2506.08611", "abs": "https://arxiv.org/abs/2506.08611", "authors": ["Shiji Zhao", "Chi Chen", "Ranjie Duan", "Xizhe Wang", "Xingxing Wei"], "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2312.05508", "summary": "Adversarial Training (AT) is widely recognized as an effective approach to\nenhance the adversarial robustness of Deep Neural Networks. As a variant of AT,\nAdversarial Robustness Distillation (ARD) has shown outstanding performance in\nenhancing the robustness of small models. However, both AT and ARD face robust\nfairness issue: these models tend to display strong adversarial robustness\nagainst some classes (easy classes) while demonstrating weak adversarial\nrobustness against others (hard classes). This paper explores the underlying\nfactors of this problem and points out the smoothness degree of soft labels for\ndifferent classes significantly impacts the robust fairness from both empirical\nobservation and theoretical analysis. Based on the above exploration, we\npropose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge\nDistillation framework to enhance the adversarial robust fairness.\nSpecifically, ABSLD adaptively reduces the student's error risk gap between\ndifferent classes, which is accomplished by adjusting the class-wise smoothness\ndegree of teacher's soft labels during the training process, and the adjustment\nis managed by assigning varying temperatures to different classes.\nAdditionally, as a label-based approach, ABSLD is highly adaptable and can be\nintegrated with the sample-based methods. Extensive experiments demonstrate\nABSLD outperforms state-of-the-art methods on the comprehensive performance of\nrobustness and fairness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aABSLD\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6559\u5e08\u6a21\u578b\u8f6f\u6807\u7b7e\u7684\u5e73\u6ed1\u5ea6\uff0c\u63d0\u5347\u5bf9\u6297\u6027\u9c81\u68d2\u516c\u5e73\u6027\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\uff08AT\uff09\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u84b8\u998f\uff08ARD\uff09\u5b58\u5728\u9c81\u68d2\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5bf9\u67d0\u4e9b\u7c7b\u522b\u7684\u9c81\u68d2\u6027\u8f83\u5f3a\uff0c\u800c\u5bf9\u5176\u4ed6\u7c7b\u522b\u8f83\u5f31\u3002", "method": "\u63d0\u51faABSLD\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u4e0d\u540c\u7c7b\u522b\u5206\u914d\u4e0d\u540c\u6e29\u5ea6\u53c2\u6570\uff0c\u8c03\u6574\u8f6f\u6807\u7b7e\u7684\u5e73\u6ed1\u5ea6\uff0c\u51cf\u5c11\u5b66\u751f\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u8bef\u5dee\u98ce\u9669\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eABSLD\u5728\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ABSLD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u63d0\u5347\u5bf9\u6297\u6027\u9c81\u68d2\u516c\u5e73\u6027\u3002"}}
{"id": "2506.08018", "pdf": "https://arxiv.org/pdf/2506.08018", "abs": "https://arxiv.org/abs/2506.08018", "authors": ["Fei Li", "Song Liu", "Weiguo Wu", "Shiqiang Nie", "Jinyu Wang"], "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache", "categories": ["cs.LG", "cs.AI", "03B65 ((Primary))", "I.2.7"], "comment": "14 pages, 8 figures, 4 tables", "summary": "The high memory demands of the Key-Value (KV) Cache during the inference of\nLarge Language Models (LLMs) severely restrict their deployment in\nresource-constrained platforms. Quantization can effectively alleviate the\nmemory pressure caused by KV Cache. However, existing methods either rely on\nstatic one-size-fits-all precision allocation or fail to dynamically prioritize\ncritical KV in long-context tasks, forcing memory-accuracy-throughput\ntradeoffs. In this work, we propose a novel mixed-precision quantization method\nfor KV Cache named KVmix. KVmix leverages gradient-based importance analysis to\nevaluate how individual Key and Value projection matrices affect the model\nloss, enabling layer-specific bit-width allocation for mix-precision\nquantization. It dynamically prioritizes higher precision for important layers\nwhile aggressively quantizing less influential ones, achieving a tunable\nbalance between accuracy and efficiency. KVmix also introduces a dynamic\nlong-context optimization strategy that adaptively keeps full-precision KV\npairs for recent pivotal tokens and compresses older ones, achieving\nhigh-quality sequence generation with low memory usage. Additionally, KVmix\nprovides efficient low-bit quantization and CUDA kernels to optimize\ncomputational overhead. On LLMs such as Llama and Mistral, KVmix achieves\nnear-lossless inference performance with extremely low quantization\nconfiguration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x\nmemory compression and a 5.3x speedup in inference throughput.", "AI": {"tldr": "KVmix\u662f\u4e00\u79cd\u9488\u5bf9KV Cache\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u91cd\u8981\u6027\u5206\u6790\u548c\u52a8\u6001\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "LLMs\u63a8\u7406\u65f6KV Cache\u7684\u9ad8\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u52a8\u6001\u4f18\u5316\u7cbe\u5ea6\u5206\u914d\u3002", "method": "KVmix\u5229\u7528\u68af\u5ea6\u91cd\u8981\u6027\u5206\u6790\u5206\u914d\u5c42\u7279\u5b9a\u6bd4\u7279\u5bbd\u5ea6\uff0c\u52a8\u6001\u4f18\u5316\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684KV\u5bf9\u7cbe\u5ea6\u3002", "result": "\u5728Llama\u548cMistral\u4e0a\uff0cKVmix\u5b9e\u73b0\u4e86\u8fd1\u65e0\u635f\u63a8\u7406\u6027\u80fd\uff0c\u5185\u5b58\u538b\u7f294.9\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53475.3\u500d\u3002", "conclusion": "KVmix\u4e3aLLMs\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8c03\u5e73\u8861\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08646", "pdf": "https://arxiv.org/pdf/2506.08646", "abs": "https://arxiv.org/abs/2506.08646", "authors": ["Mingyu Zheng", "Zhifan Feng", "Jia Wang", "Lanrui Wang", "Zheng Lin", "Yang Hao", "Weiping Wang"], "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "27 pages, 19 figures, Findings of ACL 2025", "summary": "Despite the commendable progress of recent LLM-based data synthesis methods,\nthey face two limitations in generating table instruction tuning data. First,\nthey can not thoroughly explore the vast input space of table understanding\ntasks, leading to limited data diversity. Second, they ignore the weaknesses in\ntable understanding ability of the target LLM and blindly pursue the increase\nof data quantity, resulting in suboptimal data efficiency. In this paper, we\nintroduce a progressive and weakness-guided data synthesis framework tailored\nfor table instruction tuning, named TableDreamer, to mitigate the above issues.\nSpecifically, we first synthesize diverse tables and related instructions as\nseed data, and then perform an iterative exploration of the input space under\nthe guidance of the newly identified weakness data, which eventually serve as\nthe final training data for fine-tuning the target LLM. Extensive experiments\non 10 tabular benchmarks demonstrate the effectiveness of the proposed\nframework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%\n(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms\nstate-of-the-art data synthesis baselines which use more training data. The\ncode and data is available at https://github.com/SpursGoZmy/TableDreamer", "AI": {"tldr": "TableDreamer\u662f\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u3001\u5f31\u70b9\u5bfc\u5411\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u5728\u8868\u683c\u6307\u4ee4\u8c03\u4f18\u4e2d\u6570\u636e\u591a\u6837\u6027\u548c\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u8868\u683c\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u65f6\uff0c\u672a\u80fd\u5145\u5206\u63a2\u7d22\u8f93\u5165\u7a7a\u95f4\u4e14\u5ffd\u89c6\u76ee\u6807LLM\u7684\u5f31\u70b9\uff0c\u5bfc\u81f4\u6570\u636e\u591a\u6837\u6027\u548c\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5408\u6210\u591a\u6837\u5316\u7684\u79cd\u5b50\u6570\u636e\uff0c\u5e76\u57fa\u4e8e\u65b0\u8bc6\u522b\u7684\u5f31\u70b9\u6570\u636e\u8fed\u4ee3\u63a2\u7d22\u8f93\u5165\u7a7a\u95f4\uff0c\u6700\u7ec8\u751f\u6210\u7528\u4e8e\u5fae\u8c03\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u572810\u4e2a\u8868\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTableDreamer\u5c06Llama3.1-8B-instruct\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534711.62%\uff0c\u4f18\u4e8e\u4f7f\u7528\u66f4\u591a\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TableDreamer\u901a\u8fc7\u5f31\u70b9\u5bfc\u5411\u7684\u6570\u636e\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u6307\u4ee4\u8c03\u4f18\u7684\u6548\u679c\uff0c\u4e14\u6570\u636e\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2506.08612", "pdf": "https://arxiv.org/pdf/2506.08612", "abs": "https://arxiv.org/abs/2506.08612", "authors": ["Robert-Jan Bruintjes", "Attila Lengyel", "Osman Semih Kayhan", "Davide Zambrano", "Nergis T\u00f6men", "Hadi Jamali-Rad", "Jan van Gemert"], "title": "Data-Efficient Challenges in Visual Inductive Priors: A Retrospective", "categories": ["cs.CV"], "comment": null, "summary": "Deep Learning requires large amounts of data to train models that work well.\nIn data-deficient settings, performance can be degraded. We investigate which\nDeep Learning methods benefit training models in a data-deficient setting, by\norganizing the \"VIPriors: Visual Inductive Priors for Data-Efficient Deep\nLearning\" workshop series, featuring four editions of data-impaired challenges.\nThese challenges address the problem of training deep learning models for\ncomputer vision tasks with limited data. Participants are limited to training\nmodels from scratch using a low number of training samples and are not allowed\nto use any form of transfer learning. We aim to stimulate the development of\nnovel approaches that incorporate prior knowledge to improve the data\nefficiency of deep learning models. Successful challenge entries make use of\nlarge model ensembles that mix Transformers and CNNs, as well as heavy data\naugmentation. Novel prior knowledge-based methods contribute to success in some\nentries.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6570\u636e\u4e0d\u8db3\u60c5\u51b5\u4e0b\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u7ec7\u6570\u636e\u53d7\u9650\u6311\u6218\u8d5b\uff0c\u9f13\u52b1\u5f00\u53d1\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u65f6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u6570\u636e\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u7ec4\u7ec7\u6570\u636e\u53d7\u9650\u6311\u6218\u8d5b\uff0c\u9650\u5236\u53c2\u4e0e\u8005\u4f7f\u7528\u5c11\u91cf\u6570\u636e\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\uff0c\u7981\u6b62\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u6210\u529f\u65b9\u6848\u7ed3\u5408\u4e86Transformer\u4e0eCNN\u7684\u5927\u6a21\u578b\u96c6\u6210\u3001\u5f3a\u6570\u636e\u589e\u5f3a\u53ca\u5148\u9a8c\u77e5\u8bc6\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u548c\u521b\u65b0\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u6570\u636e\u4e0d\u8db3\u65f6\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.08020", "pdf": "https://arxiv.org/pdf/2506.08020", "abs": "https://arxiv.org/abs/2506.08020", "authors": ["Zi-Ying Chen", "Chuan-Xian Ren", "Hong Yan"], "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Partial domain adaptation (PDA) problem requires aligning cross-domain\nsamples while distinguishing the outlier classes for accurate knowledge\ntransfer. The widely used weighting framework tries to address the outlier\nclasses by introducing the reweighed source domain with a similar label\ndistribution to the target domain. However, the empirical modeling of weights\ncan only characterize the sample-wise relations, which leads to insufficient\nexploration of cluster structures, and the weights could be sensitive to the\ninaccurate prediction and cause confusion on the outlier classes. To tackle\nthese issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model\nto simultaneously characterize the sample-wise and class-wise relations in a\nunified transport framework. Specifically, a cooperation mechanism between\nsample-level and class-level transport is introduced, where the sample-level\ntransport provides essential structure information for the class-level\nknowledge transfer, while the class-level transport supplies discriminative\ninformation for the outlier identification. The bi-level transport plan\nprovides guidance for the alignment process. By incorporating the label-aware\ntransport cost, the local transport structure is ensured and a fast computation\nformulation is derived to improve the efficiency. Extensive experiments on\nbenchmark datasets validate the competitiveness of BUOT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdBi-level Unbalanced Optimal Transport (BUOT)\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u57df\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u6837\u672c\u7ea7\u548c\u7c7b\u7ea7\u4f20\u8f93\u7684\u7edf\u4e00\u6846\u67b6\u63d0\u5347\u77e5\u8bc6\u8f6c\u79fb\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u90e8\u5206\u57df\u9002\u5e94\u95ee\u9898\u9700\u8981\u5bf9\u9f50\u8de8\u57df\u6837\u672c\u5e76\u533a\u5206\u5f02\u5e38\u7c7b\uff0c\u73b0\u6709\u52a0\u6743\u6846\u67b6\u4ec5\u80fd\u8868\u5f81\u6837\u672c\u7ea7\u5173\u7cfb\uff0c\u5bf9\u805a\u7c7b\u7ed3\u6784\u63a2\u7d22\u4e0d\u8db3\u4e14\u6613\u53d7\u9884\u6d4b\u4e0d\u51c6\u786e\u5f71\u54cd\u3002", "method": "\u63d0\u51faBUOT\u6a21\u578b\uff0c\u7ed3\u5408\u6837\u672c\u7ea7\u548c\u7c7b\u7ea7\u4f20\u8f93\u7684\u5408\u4f5c\u673a\u5236\uff0c\u5229\u7528\u6807\u7b7e\u611f\u77e5\u4f20\u8f93\u6210\u672c\u786e\u4fdd\u5c40\u90e8\u4f20\u8f93\u7ed3\u6784\uff0c\u5e76\u63a8\u5bfc\u5feb\u901f\u8ba1\u7b97\u5f62\u5f0f\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BUOT\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "BUOT\u901a\u8fc7\u7edf\u4e00\u7684\u53cc\u5c42\u4f20\u8f93\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u90e8\u5206\u57df\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u8f6c\u79fb\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.08647", "pdf": "https://arxiv.org/pdf/2506.08647", "abs": "https://arxiv.org/abs/2506.08647", "authors": ["Oumaima El Khettari", "Solen Quiniou", "Samuel Chaffron"], "title": "Summarization for Generative Relation Extraction in the Microbiome Domain", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We explore a generative relation extraction (RE) pipeline tailored to the\nstudy of interactions in the intestinal microbiome, a complex and low-resource\nbiomedical domain. Our method leverages summarization with large language\nmodels (LLMs) to refine context before extracting relations via\ninstruction-tuned generation. Preliminary results on a dedicated corpus show\nthat summarization improves generative RE performance by reducing noise and\nguiding the model. However, BERT-based RE approaches still outperform\ngenerative models. This ongoing work demonstrates the potential of generative\nmethods to support the study of specialized domains in low-resources setting.", "AI": {"tldr": "\u751f\u6210\u5f0f\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\u5728\u80a0\u9053\u5fae\u751f\u7269\u7ec4\u7814\u7a76\u4e2d\u8868\u73b0\u6f5c\u529b\uff0c\u4f46BERT\u6a21\u578b\u4ecd\u4f18\u4e8e\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u80a0\u9053\u5fae\u751f\u7269\u7ec4\u8fd9\u4e00\u590d\u6742\u4e14\u8d44\u6e90\u532e\u4e4f\u7684\u751f\u7269\u533b\u5b66\u9886\u57df\u4e2d\u7684\u5173\u7cfb\u63d0\u53d6\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6458\u8981\u751f\u6210\u4ee5\u4f18\u5316\u4e0a\u4e0b\u6587\uff0c\u518d\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u751f\u6210\u5173\u7cfb\u3002", "result": "\u6458\u8981\u751f\u6210\u51cf\u5c11\u4e86\u566a\u58f0\u5e76\u5f15\u5bfc\u6a21\u578b\uff0c\u4f46BERT\u65b9\u6cd5\u4ecd\u4f18\u4e8e\u751f\u6210\u6a21\u578b\u3002", "conclusion": "\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u4e13\u4e1a\u9886\u57df\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.08613", "pdf": "https://arxiv.org/pdf/2506.08613", "abs": "https://arxiv.org/abs/2506.08613", "authors": ["Joost van Dalen", "Yuki M. Asano", "Marc Russwurm"], "title": "SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything", "categories": ["cs.CV"], "comment": null, "summary": "This work proposes SAMSelect, an algorithm to obtain a salient three-channel\nvisualization for multispectral images. We develop SAMSelect and show its use\nfor marine scientists visually interpreting floating marine debris in\nSentinel-2 imagery. These debris are notoriously difficult to visualize due to\ntheir compositional heterogeneity in medium-resolution imagery. Out of these\ndifficulties, a visual interpretation of imagery showing marine debris remains\na common practice by domain experts, who select bands and spectral indices on a\ncase-by-case basis informed by common practices and heuristics. SAMSelect\nselects the band or index combination that achieves the best classification\naccuracy on a small annotated dataset through the Segment Anything Model. Its\ncentral assumption is that the three-channel visualization achieves the most\naccurate segmentation results also provide good visual information for\nphoto-interpretation.\n  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine\ndebris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets\nfrom the Plastic Litter Project. This reveals the potential of new previously\nunused band combinations (e.g., a normalized difference index of B8, B2), which\ndemonstrate improved performance compared to literature-based indices. We\ndescribe the algorithm in this paper and provide an open-source code repository\nthat will be helpful for domain scientists doing visual photo interpretation,\nespecially in the marine field.", "AI": {"tldr": "SAMSelect\u662f\u4e00\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u591a\u5149\u8c31\u56fe\u50cf\u4e2d\u9009\u62e9\u6700\u4f73\u7684\u4e09\u901a\u9053\u53ef\u89c6\u5316\u7ec4\u5408\uff0c\u4ee5\u5e2e\u52a9\u6d77\u6d0b\u79d1\u5b66\u5bb6\u66f4\u76f4\u89c2\u5730\u8bc6\u522b\u6d77\u6d0b\u6f02\u6d6e\u7269\u3002", "motivation": "\u6d77\u6d0b\u6f02\u6d6e\u7269\u5728\u4e2d\u7b49\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u96be\u4ee5\u53ef\u89c6\u5316\uff0c\u800c\u4e13\u5bb6\u901a\u5e38\u4f9d\u8d56\u7ecf\u9a8c\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u9009\u62e9\u6ce2\u6bb5\u3002SAMSelect\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u9009\u62e9\u6700\u4f73\u6ce2\u6bb5\u7ec4\u5408\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SAMSelect\u5229\u7528Segment Anything Model\u5728\u5c0f\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u9009\u62e9\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\u7684\u6ce2\u6bb5\u6216\u6307\u6570\u7ec4\u5408\uff0c\u5047\u8bbe\u6700\u4f73\u5206\u5272\u7ed3\u679c\u4e5f\u63d0\u4f9b\u826f\u597d\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728\u52a0\u7eb3\u963f\u514b\u62c9\u548c\u5357\u975e\u5fb7\u73ed\u7684Sentinel-2\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0cSAMSelect\u53d1\u73b0\u4e86\u65b0\u7684\u672a\u4f7f\u7528\u6ce2\u6bb5\u7ec4\u5408\uff08\u5982B8\u548cB2\u7684\u5f52\u4e00\u5316\u5dee\u5f02\u6307\u6570\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u6587\u732e\u4e2d\u7684\u6307\u6570\u3002", "conclusion": "SAMSelect\u4e3a\u6d77\u6d0b\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u5e93\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u89c6\u89c9\u89e3\u8bd1\u6548\u7387\u3002"}}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u597d\u5b66\u4e60\u6846\u67b6MBPO\uff0c\u901a\u8fc7\u751f\u6210\u786c\u8d1f\u6837\u672c\u548c\u5728\u7ebf\u9a8c\u8bc1\u5956\u52b1\uff0c\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4efb\u52a1\u9002\u5e94\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u672a\u6709\u6548\u6291\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u504f\u89c1\uff0c\u4e14\u4f9d\u8d56\u79bb\u7ebf\u6570\u636e\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5206\u5e03\u53d8\u5316\u3002MBPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MBPO\u901a\u8fc7\u5bf9\u6297\u6027\u6270\u52a8\u751f\u6210\u786c\u8d1f\u6837\u672c\uff0c\u6784\u5efa\u79bb\u7ebf\u504f\u597d\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u5c01\u95ed\u4efb\u52a1\u751f\u6210\u5728\u7ebf\u9a8c\u8bc1\u5956\u52b1\uff0c\u7ed3\u5408GRPO\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMBPO\u80fd\u663e\u8457\u63d0\u5347LMMs\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "MBPO\u901a\u8fc7\u6a21\u6001\u5e73\u8861\u504f\u597d\u4f18\u5316\uff0c\u4e3aLMMs\u7684\u4efb\u52a1\u9002\u5e94\u6027\u548c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672", "abs": "https://arxiv.org/abs/2506.08672", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "AI": {"tldr": "RuleReasoner\u662f\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u91c7\u6837\u589e\u5f3a\u5c0f\u63a8\u7406\u6a21\u578b\uff08SRMs\uff09\u89c4\u5219\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u3002", "motivation": "\u89e3\u51b3\u5c0f\u63a8\u7406\u6a21\u578b\u5728\u591a\u6837\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u89c4\u5219\u63a8\u7406\u7684\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5386\u53f2\u5956\u52b1\u8c03\u6574\u4e0d\u540c\u9886\u57df\u7684\u91c7\u6837\u6743\u91cd\u3002", "result": "\u5728ID\u548cOOD\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u6cbfLRMs\uff08ID\u4efb\u52a1\u63d0\u53474.1%\uff0cOOD\u4efb\u52a1\u63d0\u534710.4%\uff09\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "RuleReasoner\u8bc1\u660e\u4e86\u5c0f\u6a21\u578b\u5728\u89c4\u5219\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u7684\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2506.08619", "pdf": "https://arxiv.org/pdf/2506.08619", "abs": "https://arxiv.org/abs/2506.08619", "authors": ["Gon\u00e7alo Dias Pais", "Valter Piedade", "Moitreya Chatterjee", "Marcus Greiff", "Pedro Miraldo"], "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering", "categories": ["cs.CV"], "comment": "Accepted in ECCV 2024", "summary": "Several variants of Neural Radiance Fields (NeRFs) have significantly\nimproved the accuracy of synthesized images and surface reconstruction of 3D\nscenes/objects. In all of these methods, a key characteristic is that none can\ntrain the neural network with every possible input data, specifically, every\npixel and potential 3D point along the projection rays due to scalability\nissues. While vanilla NeRFs uniformly sample both the image pixels and 3D\npoints along the projection rays, some variants focus only on guiding the\nsampling of the 3D points along the projection rays. In this paper, we leverage\nthe implicit surface representation of the foreground scene and model a\nprobability density function in a 3D image projection space to achieve a more\ntargeted sampling of the rays toward regions of interest, resulting in improved\nrendering. Additionally, a new surface reconstruction loss is proposed for\nimproved performance. This new loss fully explores the proposed 3D image\nprojection space model and incorporates near-to-surface and empty space\ncomponents. By integrating our novel sampling strategy and novel loss into\ncurrent state-of-the-art neural implicit surface renderers, we achieve more\naccurate and detailed 3D reconstructions and improved image rendering,\nespecially for the regions of interest in any given scene.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684NeRF\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u8868\u9762\u8868\u793a\u548c3D\u56fe\u50cf\u6295\u5f71\u7a7a\u95f4\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u91c7\u6837\u548c\u6e32\u67d3\uff0c\u540c\u65f6\u5f15\u5165\u65b0\u7684\u8868\u9762\u91cd\u5efa\u635f\u5931\u51fd\u6570\uff0c\u63d0\u53473D\u91cd\u5efa\u548c\u56fe\u50cf\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u53ef\u6269\u5c55\u6027\u95ee\u9898\u65e0\u6cd5\u5bf9\u6240\u6709\u8f93\u5165\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u7cbe\u51c6\u7684\u91c7\u6837\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u4f18\u5316\u6e32\u67d3\u6548\u679c\u3002", "method": "\u5229\u7528\u9690\u5f0f\u8868\u9762\u8868\u793a\u5efa\u6a213D\u56fe\u50cf\u6295\u5f71\u7a7a\u95f4\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff0c\u5b9e\u73b0\u9488\u5bf9\u6027\u91c7\u6837\uff1b\u63d0\u51fa\u65b0\u7684\u8868\u9762\u91cd\u5efa\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u8fd1\u8868\u9762\u548c\u7a7a\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u73b0\u6709\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u6e32\u67d3\u5668\u4e2d\u96c6\u6210\u65b0\u65b9\u6cd5\u540e\uff0c3D\u91cd\u5efa\u548c\u56fe\u50cf\u6e32\u67d3\u7684\u51c6\u786e\u6027\u548c\u7ec6\u8282\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u611f\u5174\u8da3\u533a\u57df\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u91c7\u6837\u548c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86NeRF\u7684\u6e32\u67d3\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u4e3a3D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08023", "pdf": "https://arxiv.org/pdf/2506.08023", "abs": "https://arxiv.org/abs/2506.08023", "authors": ["Qifeng Wu", "Zhengzhe Liu", "Han Zhu", "Yizhou Zhao", "Daisuke Kihara", "Min Xu"], "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": "4 pages for body, 3 pages for appendix, 11 figures. Accepted to CVPR\n  2025 Workshop on Multimodal Foundation Models for Biomedicine: Challenges and\n  Opportunities(MMFM-BIOMED)", "summary": "This paper aims to retrieve proteins with similar structures and semantics\nfrom large-scale protein dataset, facilitating the functional interpretation of\nprotein structures derived by structural determination methods like\ncryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of\nvision-language models (VLMs), we propose a CLIP-style framework for aligning\n3D protein structures with functional annotations using contrastive learning.\nFor model training, we propose a large-scale dataset of approximately 200,000\nprotein-caption pairs with rich functional descriptors. We evaluate our model\nin both in-domain and more challenging cross-database retrieval on Protein Data\nBank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In\nboth cases, our approach demonstrates promising zero-shot retrieval\nperformance, highlighting the potential of multimodal foundation models for\nstructure-function understanding in protein biology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u6846\u67b6\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5927\u89c4\u6a21\u86cb\u767d\u8d28\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u7ed3\u6784\u548c\u8bed\u4e49\u76f8\u4f3c\u7684\u86cb\u767d\u8d28\uff0c\u4ee5\u8f85\u52a9\u529f\u80fd\u6ce8\u91ca\u3002", "motivation": "\u8fd1\u5e74\u6765\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u8fdb\u5c55\u542f\u53d1\u4e86\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f503D\u86cb\u767d\u8d28\u7ed3\u6784\u548c\u529f\u80fd\u6ce8\u91ca\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528CLIP\u98ce\u683c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u7ea620\u4e07\u86cb\u767d\u8d28-\u63cf\u8ff0\u5bf9\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728PDB\u548cEMDB\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u68c0\u7d22\u7684\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u86cb\u767d\u8d28\u7ed3\u6784-\u529f\u80fd\u7406\u89e3\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.08686", "pdf": "https://arxiv.org/pdf/2506.08686", "abs": "https://arxiv.org/abs/2506.08686", "authors": ["Soham Poddar", "Paramita Koley", "Janardan Misra", "Sanjay Podder", "Navveen Balani", "Niloy Ganguly", "Saptarshi Ghosh"], "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to appear at the ACL 2025 findings", "summary": "A significant portion of the energy consumed by Large Language Models (LLMs)\narises from their inference processes; hence developing energy-efficient\nmethods for inference is crucial. While several techniques exist for inference\noptimization, output compression remains relatively unexplored, with only a few\npreliminary efforts addressing this aspect. In this work, we first benchmark 12\ndecoder-only LLMs across 5 datasets, revealing that these models often produce\nresponses that are substantially longer than necessary. We then conduct a\ncomprehensive quality assessment of LLM responses, formally defining six\ninformation categories present in LLM responses. We show that LLMs often tend\nto include redundant or additional information besides the minimal answer. To\naddress this issue of long responses by LLMs, we explore several simple and\nintuitive prompt-engineering strategies. Empirical evaluation shows that\nappropriate prompts targeting length reduction and controlling information\ncontent can achieve significant energy optimization between 25-60\\% by reducing\nthe response length while preserving the quality of LLM responses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u8f93\u51fa\u538b\u7f29\u548c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\uff0c\u4ece\u800c\u663e\u8457\u4f18\u5316\u80fd\u6e90\u6548\u7387\u3002", "motivation": "LLM\u63a8\u7406\u8fc7\u7a0b\u6d88\u8017\u5927\u91cf\u80fd\u6e90\uff0c\u800c\u8f93\u51fa\u538b\u7f29\u9886\u57df\u7814\u7a76\u8f83\u5c11\uff0c\u56e0\u6b64\u63a2\u7d22\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u7684\u65b9\u6cd5\u4ee5\u4f18\u5316\u80fd\u6e90\u6548\u7387\u3002", "method": "\u9996\u5148\u5bf912\u79cd\u89e3\u7801\u5668LLM\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u54cd\u5e94\u957f\u5ea6\uff1b\u7136\u540e\u5b9a\u4e49\u516d\u79cd\u4fe1\u606f\u7c7b\u522b\u8bc4\u4f30\u54cd\u5e94\u8d28\u91cf\uff1b\u6700\u540e\u63a2\u7d22\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u4ee5\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002", "result": "\u9002\u5f53\u7684\u63d0\u793a\u7b56\u7565\u53ef\u51cf\u5c1125-60%\u7684\u80fd\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u63a7\u5236\u54cd\u5e94\u957f\u5ea6\u548c\u4fe1\u606f\u5185\u5bb9\u662f\u4f18\u5316LLM\u80fd\u6e90\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.08629", "pdf": "https://arxiv.org/pdf/2506.08629", "abs": "https://arxiv.org/abs/2506.08629", "authors": ["Feixiang Du", "Shengkun Wu"], "title": "ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 2 figures, 4 tables", "summary": "In the past decade, Convolutional Neural Networks (CNNs) and Transformers\nhave achieved wide applicaiton in semantic segmentation tasks. Although CNNs\nwith Transformer models greatly improve performance, the global context\nmodeling remains inadequate. Recently, Mamba achieved great potential in vision\ntasks, showing its advantages in modeling long-range dependency. In this paper,\nwe propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,\ndubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based\nframework to address their complementary weaknesses. Specifically, We design a\nEnhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to\nimprove the representations ability of feature, We devise a Multi-Scale\nAttention Unit (MSAU) to integrate multi-scale feature aggregation, spatial\naggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion\nModule (FFM) merges diverse level feature, significantly enhancing segmented\naccuracy. Extensive experiments on two representative datasets demonstrate that\nthe proposed model excels in accuracy and efficiency balance, achieving 70.6%\nmIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M\nparameters and 8.27G FLOPs on a single RTX 3090 GPU platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9ad8\u6548CNN-Mamba\u7f51\u7edc\uff08ECMNet\uff09\uff0c\u7528\u4e8e\u8bed\u4e49\u5206\u5272\uff0c\u7ed3\u5408CNN\u548cMamba\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u53cc\u6ce8\u610f\u529b\u5757\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u5355\u5143\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1CNN\u548cTransformer\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u4ecd\u4e0d\u8db3\u3002Mamba\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u7ed3\u5408CNN\u4e0eMamba\u4ee5\u5f25\u8865\u5404\u81ea\u5f31\u70b9\u3002", "method": "\u8bbe\u8ba1\u4e86\u589e\u5f3a\u53cc\u6ce8\u610f\u529b\u5757\uff08EDAB\uff09\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u5355\u5143\uff08MSAU\uff09\uff0c\u5e76\u63d0\u51fa\u4e86Mamba\u589e\u5f3a\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08FFM\uff09\uff0c\u4ee5\u6574\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cCityscapes\u548cCamVid\u6d4b\u8bd5\u96c6\u7684mIoU\u5206\u522b\u8fbe\u523070.6%\u548c73.6%\uff0c\u53c2\u6570\u91cf\u4e3a0.87M\uff0c\u8ba1\u7b97\u91cf\u4e3a8.27G FLOPs\u3002", "conclusion": "ECMNet\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9a8c\u8bc1\u4e86CNN\u4e0eMamba\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.08027", "pdf": "https://arxiv.org/pdf/2506.08027", "abs": "https://arxiv.org/abs/2506.08027", "authors": ["Asit Mishra", "Dusan Stosic", "Simon Layton"], "title": "Recipes for Pre-training LLMs with MXFP8", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Precision scaling - using fewer bits to represent model parameters and\nrelated tensors during pre-training - has emerged as a compelling technique for\nimproving GPU efficiency without sacrificing accuracy. Microscaling (MX)\nformats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling\nthis precision scaling aspect. These formats combine narrow floating-point data\ntypes with per-block scaling factors, offering a fine-grained approach to\nquantizing tensors.\n  Although MX-formats offer the promise of improved numeric stability compared\nto other reduced-precision representations, in practice they must be used\ncarefully in order to successfully converge an LLM on a multi-trillion token\ndataset. In this paper, we show that the rounding mode suggested in OCP\nspecification can lead to divergence when pre-training an LLM. We show an\nimproved rounding mode, which uses round-to-infinity to compute scaling\nfactors, enables successful pre-training in MXFP8 for an 8B model on 15T\ntokens.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u9884\u8bad\u7ec3\u4e2d\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u8868\u793a\uff08\u5982MX\u683c\u5f0f\uff09\u4ee5\u63d0\u9ad8GPU\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u820d\u5165\u6a21\u5f0f\u4ee5\u907f\u514d\u6a21\u578b\u53d1\u6563\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u9884\u8bad\u7ec3\u4e2d\u5229\u7528\u4f4e\u7cbe\u5ea6\u8868\u793a\uff08\u5982MX\u683c\u5f0f\uff09\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u907f\u514d\u56e0\u820d\u5165\u6a21\u5f0f\u5bfc\u81f4\u7684\u6a21\u578b\u53d1\u6563\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u820d\u5165\u6a21\u5f0f\uff08round-to-infinity\uff09\uff0c\u7528\u4e8e\u8ba1\u7b97MXFP8\u683c\u5f0f\u7684\u7f29\u653e\u56e0\u5b50\uff0c\u5e76\u57288B\u6a21\u578b\u548c15T tokens\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6539\u8fdb\u7684\u820d\u5165\u6a21\u5f0f\u6210\u529f\u5b9e\u73b0\u4e86\u5728MXFP8\u683c\u5f0f\u4e0b\u5bf98B\u6a21\u578b\u7684\u9884\u8bad\u7ec3\uff0c\u907f\u514d\u4e86\u53d1\u6563\u95ee\u9898\u3002", "conclusion": "MX\u683c\u5f0f\u7684\u4f4e\u7cbe\u5ea6\u8868\u793a\u5728\u9884\u8bad\u7ec3\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8c28\u614e\u9009\u62e9\u820d\u5165\u6a21\u5f0f\u4ee5\u786e\u4fdd\u6536\u655b\u3002"}}
{"id": "2506.08700", "pdf": "https://arxiv.org/pdf/2506.08700", "abs": "https://arxiv.org/abs/2506.08700", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper.", "AI": {"tldr": "ClimateViz\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u79d1\u5b66\u56fe\u8868\u4e8b\u5b9e\u6838\u67e5\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8fd15\u4e07\u6761\u4e0e\u79d1\u5b66\u56fe\u8868\u5173\u8054\u7684\u58f0\u660e\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u79d1\u5b66\u4e8b\u5b9e\u6838\u67e5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6587\u672c\u548c\u8868\u683c\uff0c\u5ffd\u7565\u4e86\u79d1\u5b66\u56fe\u8868\u7684\u91cd\u8981\u6027\u3002ClimateViz\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u65e8\u5728\u63a8\u52a8\u56fe\u8868\u63a8\u7406\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b49,862\u6761\u58f0\u660e\u548c2,896\u4e2a\u56fe\u8868\u7684ClimateViz\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u652f\u6301\u3001\u53cd\u9a73\u6216\u4fe1\u606f\u4e0d\u8db3\u3002\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u56fe\u8868\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u6a21\u578b\uff08\u5982Gemini 2.5\u548cInternVL 2.5\uff09\u51c6\u786e\u7387\u4ec5\u4e3a76.2%\u81f377.8%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0889.3%\u81f392.7%\uff09\u3002", "conclusion": "ClimateViz\u4e3a\u79d1\u5b66\u56fe\u8868\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u89e3\u91ca\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2506.08632", "pdf": "https://arxiv.org/pdf/2506.08632", "abs": "https://arxiv.org/abs/2506.08632", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning.", "AI": {"tldr": "RoboSwap\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\u7684\u65b0\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u8de8\u4f53\u73b0\u6570\u636e\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u6761\u4ef6\u4e0b\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u5272\u673a\u5668\u4eba\u624b\u81c2\u5e76\u8bad\u7ec3\u975e\u914d\u5bf9GAN\u6a21\u578b\u8fdb\u884c\u7ffb\u8bd1\uff0c\u518d\u7ed3\u5408\u6269\u6563\u6a21\u578b\u589e\u5f3a\u89c6\u9891\u8fde\u8d2f\u6027\u548c\u8fd0\u52a8\u771f\u5b9e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRoboSwap\u5728\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RoboSwap\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8de8\u4f53\u73b0\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08029", "pdf": "https://arxiv.org/pdf/2506.08029", "abs": "https://arxiv.org/abs/2506.08029", "authors": ["Jiayu Li", "Masood Mortazavi", "Ning Yan", "Yihong Ma", "Reza Zafarani"], "title": "Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "A briefer version of this paper was accepted as a Work-in-Progress\n  (WIP) at the Design Automation Conference (DAC) 2024", "summary": "The goal of inverse design in distributed circuits is to generate\nnear-optimal designs that meet a desirable transfer function specification.\nExisting design exploration methods use some combination of strategies\ninvolving artificial grids, differentiable evaluation procedures, and specific\ntemplate topologies. However, real-world design practices often require\nnon-differentiable evaluation procedures, varying topologies, and\nnear-continuous placement spaces. In this paper, we propose DCIDA, a design\nexploration framework that learns a near-optimal design sampling policy for a\ntarget transfer function. DCIDA decides all design factors in a compound\nsingle-step action by sampling from a set of jointly-trained conditional\ndistributions generated by the policy. Utilizing an injective interdependent\n``map\", DCIDA transforms raw sampled design ``actions\" into uniquely equivalent\nphysical representations, enabling the framework to learn the conditional\ndependencies among joint ``raw'' design decisions. Our experiments demonstrate\nDCIDA's Transformer-based policy network achieves significant reductions in\ndesign error compared to state-of-the-art approaches, with significantly better\nfit in cases involving more complex transfer functions.", "AI": {"tldr": "DCIDA\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u7535\u8def\u9006\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u7684\u91c7\u6837\u7b56\u7565\u751f\u6210\u8fd1\u6700\u4f18\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bbe\u8ba1\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u975e\u53ef\u5fae\u8bc4\u4f30\u3001\u591a\u53d8\u62d3\u6251\u548c\u8fde\u7eed\u5e03\u5c40\u7a7a\u95f4\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DCIDA\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u7684\u91c7\u6837\u7b56\u7565\u751f\u6210\u8bbe\u8ba1\u51b3\u7b56\uff0c\u5e76\u5229\u7528\u6ce8\u5165\u5f0f\u6620\u5c04\u5c06\u539f\u59cb\u8bbe\u8ba1\u52a8\u4f5c\u8f6c\u6362\u4e3a\u7269\u7406\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDCIDA\u7684Transformer\u7b56\u7565\u7f51\u7edc\u5728\u590d\u6742\u4f20\u9012\u51fd\u6570\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DCIDA\u4e3a\u5206\u5e03\u5f0f\u7535\u8def\u9006\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08712", "pdf": "https://arxiv.org/pdf/2506.08712", "abs": "https://arxiv.org/abs/2506.08712", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Mark A. Hasegawa-Johnson", "Sungwoong Kim", "Chang D. Yoo"], "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "We introduce ConfPO, a method for preference learning in Large Language\nModels (LLMs) that identifies and optimizes preference-critical tokens based\nsolely on the training policy's confidence, without requiring any auxiliary\nmodels or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as\nDirect Preference Optimization (DPO), which uniformly adjust all token\nprobabilities regardless of their relevance to preference, ConfPO focuses\noptimization on the most impactful tokens. This targeted approach improves\nalignment quality while mitigating overoptimization (i.e., reward hacking) by\nusing the KL divergence budget more efficiently. In contrast to recent\ntoken-level methods that rely on credit-assignment models or AI annotators,\nraising concerns about scalability and reliability, ConfPO is simple,\nlightweight, and model-free. Experimental results on challenging alignment\nbenchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO\nconsistently outperforms uniform DAAs across various LLMs, delivering better\nalignment with zero additional computational overhead.", "AI": {"tldr": "ConfPO\u662f\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u7b56\u7565\u7f6e\u4fe1\u5ea6\u7684\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u5bf9\u504f\u597d\u5f71\u54cd\u6700\u5927\u7684\u5173\u952etoken\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982DPO\uff09\u5bf9\u6240\u6709token\u8fdb\u884c\u5747\u5300\u8c03\u6574\uff0c\u800cConfPO\u65e8\u5728\u901a\u8fc7\u66f4\u9ad8\u6548\u5730\u5229\u7528KL\u6563\u5ea6\u9884\u7b97\uff0c\u63d0\u9ad8\u5bf9\u9f50\u8d28\u91cf\u5e76\u907f\u514d\u8fc7\u4f18\u5316\u3002", "method": "ConfPO\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u7b56\u7565\u7684\u7f6e\u4fe1\u5ea6\u6765\u8bc6\u522b\u548c\u4f18\u5316\u504f\u597d\u5173\u952etoken\uff0c\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u6216\u989d\u5916\u8ba1\u7b97\u3002", "result": "\u5728AlpacaEval 2\u548cArena-Hard\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConfPO\u8868\u73b0\u4f18\u4e8e\u5747\u5300\u8c03\u6574\u65b9\u6cd5\uff0c\u4e14\u65e0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "ConfPO\u662f\u4e00\u79cd\u7b80\u5355\u3001\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2506.08635", "pdf": "https://arxiv.org/pdf/2506.08635", "abs": "https://arxiv.org/abs/2506.08635", "authors": ["Siddhant Ranade", "Gon\u00e7alo Dias Pais", "Ross Tyler Whitaker", "Jacinto C. Nascimento", "Pedro Miraldo", "Srikumar Ramalingam"], "title": "SurfR: Surface Reconstruction with Multi-scale Attention", "categories": ["cs.CV"], "comment": "Accepted in 3DV 2025", "summary": "We propose a fast and accurate surface reconstruction algorithm for\nunorganized point clouds using an implicit representation. Recent learning\nmethods are either single-object representations with small neural models that\nallow for high surface details but require per-object training or generalized\nrepresentations that require larger models and generalize to newer shapes but\nlack details, and inference is slow. We propose a new implicit representation\nfor general 3D shapes that is faster than all the baselines at their optimum\nresolution, with only a marginal loss in performance compared to the\nstate-of-the-art. We achieve the best accuracy-speed trade-off using three key\ncontributions. Many implicit methods extract features from the point cloud to\nclassify whether a query point is inside or outside the object. First, to speed\nup the reconstruction, we show that this feature extraction does not need to\nuse the query point at an early stage (lazy query). Second, we use a parallel\nmulti-scale grid representation to develop robust features for different noise\nlevels and input resolutions. Finally, we show that attention across scales can\nprovide improved reconstruction results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u8868\u793a\u7684\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u65e0\u7ec4\u7ec7\u70b9\u4e91\u8868\u9762\u91cd\u5efa\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u8282\u548c\u901f\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u9010\u5bf9\u8c61\u8bad\u7ec3\u7684\u5c0f\u6a21\u578b\uff08\u7ec6\u8282\u9ad8\u4f46\u6cdb\u5316\u5dee\uff09\uff0c\u8981\u4e48\u662f\u5927\u6a21\u578b\uff08\u6cdb\u5316\u5f3a\u4f46\u7ec6\u8282\u5dee\u4e14\u63a8\u7406\u6162\uff09\uff0c\u9700\u8981\u4e00\u79cd\u517c\u987e\u901f\u5ea6\u548c\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u8d21\u732e\u5b9e\u73b0\uff1a1) \u5ef6\u8fdf\u67e5\u8be2\u70b9\u7279\u5f81\u63d0\u53d6\uff1b2) \u5e76\u884c\u591a\u5c3a\u5ea6\u7f51\u683c\u8868\u793a\uff1b3) \u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u901f\u5ea6\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u4ec5\u7565\u4f4e\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u7cbe\u5ea6-\u901f\u5ea6\u6743\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u901a\u75283D\u5f62\u72b6\u91cd\u5efa\u3002"}}
{"id": "2506.08041", "pdf": "https://arxiv.org/pdf/2506.08041", "abs": "https://arxiv.org/abs/2506.08041", "authors": ["Siddharth Siddharth", "Brainerd Prince", "Amol Harsh", "Shreyas Ramachandran"], "title": "The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted for publication at AIED 2025 in the late-breaking work track", "summary": "This work presents a novel course titled The World of AI designed for\nfirst-year undergraduate engineering students with little to no prior exposure\nto AI. The central problem addressed by this course is that engineering\nstudents often lack foundational knowledge of AI and its broader societal\nimplications at the outset of their academic journeys. We believe the way to\naddress this gap is to design and deliver an interdisciplinary course that can\na) be accessed by first-year undergraduate engineering students across any\ndomain, b) enable them to understand the basic workings of AI systems sans\nmathematics, and c) make them appreciate AI's far-reaching implications on our\nlives. The course was divided into three modules co-delivered by faculty from\nboth engineering and humanities. The planetary module explored AI's dual role\nas both a catalyst for sustainability and a contributor to environmental\nchallenges. The societal impact module focused on AI biases and concerns around\nprivacy and fairness. Lastly, the workplace module highlighted AI-driven job\ndisplacement, emphasizing the importance of adaptation. The novelty of this\ncourse lies in its interdisciplinary curriculum design and pedagogical\napproach, which combines technical instruction with societal discourse. Results\nrevealed that students' comprehension of AI challenges improved across diverse\nmetrics like (a) increased awareness of AI's environmental impact, and (b)\nefficient corrective solutions for AI fairness. Furthermore, it also indicated\nthe evolution in students' perception of AI's transformative impact on our\nlives.", "AI": {"tldr": "\u4e3a\u5927\u4e00\u5de5\u79d1\u751f\u8bbe\u8ba1\u7684\u8de8\u5b66\u79d1AI\u8bfe\u7a0b\uff0c\u6db5\u76d6\u6280\u672f\u4e0e\u793e\u4f1a\u5f71\u54cd\uff0c\u63d0\u5347\u5b66\u751f\u5bf9AI\u7684\u7406\u89e3\u4e0e\u8ba4\u77e5\u3002", "motivation": "\u89e3\u51b3\u5de5\u79d1\u751f\u7f3a\u4e4fAI\u57fa\u7840\u53ca\u5176\u793e\u4f1a\u5f71\u54cd\u8ba4\u77e5\u7684\u95ee\u9898\u3002", "method": "\u8bfe\u7a0b\u5206\u4e3a\u4e09\u4e2a\u6a21\u5757\uff0c\u7531\u5de5\u7a0b\u4e0e\u4eba\u6587\u5b66\u79d1\u6559\u5e08\u8054\u5408\u6388\u8bfe\uff0c\u5185\u5bb9\u6db5\u76d6AI\u7684\u73af\u5883\u3001\u793e\u4f1a\u548c\u804c\u573a\u5f71\u54cd\u3002", "result": "\u5b66\u751fAI\u8ba4\u77e5\u63d0\u5347\uff0c\u5305\u62ec\u73af\u5883\u610f\u8bc6\u589e\u5f3a\u548c\u516c\u5e73\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u63d0\u9ad8\u3002", "conclusion": "\u8de8\u5b66\u79d1\u8bfe\u7a0b\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u5b66\u751f\u5bf9AI\u7684\u7efc\u5408\u7406\u89e3\u3002"}}
{"id": "2506.08713", "pdf": "https://arxiv.org/pdf/2506.08713", "abs": "https://arxiv.org/abs/2506.08713", "authors": ["Fariz Ikhwantri", "Dusica Marijan"], "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u7684\u5408\u89c4\u68c0\u6d4b\u65b9\u6cd5EXCLAIM\uff0c\u7528\u4e8e\u89e3\u51b3\u4fdd\u8bc1\u6848\u4f8b\u4e2d\u7684\u590d\u6742\u6027\u548c\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u4fdd\u8bc1\u6848\u4f8b\u7684\u5408\u89c4\u68c0\u6d4b\u9762\u4e34\u6cd5\u5f8b\u548c\u6280\u672f\u6587\u672c\u590d\u6742\u3001\u6a21\u578b\u89e3\u91ca\u9700\u6c42\u9ad8\u4ee5\u53ca\u6570\u636e\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u8df3\u63a8\u7406\u5c06\u4fdd\u8bc1\u6848\u4f8b\u7684\u58f0\u660e-\u8bba\u636e-\u8bc1\u636e\u7ed3\u6784\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u5408\u89c4\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4fdd\u8bc1\u6848\u4f8b\u3002", "result": "\u901a\u8fc7GDPR\u8981\u6c42\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u4fdd\u8bc1\u6848\u4f8b\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "NLI\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u5408\u89c4\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.08640", "pdf": "https://arxiv.org/pdf/2506.08640", "abs": "https://arxiv.org/abs/2506.08640", "authors": ["Yichong Lu", "Yuzhuo Tian", "Zijin Jiang", "Yikun Zhao", "Yuanbo Yang", "Hao Ouyang", "Haoji Hu", "Huimin Yu", "Yujun Shen", "Yiyi Liao"], "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned", "categories": ["cs.CV"], "comment": "Project Page: https://xdimlab.github.io/Orientation_Matters", "summary": "Humans intuitively perceive object shape and orientation from a single image,\nguided by strong priors about canonical poses. However, existing 3D generative\nmodels often produce misaligned results due to inconsistent training data,\nlimiting their usability in downstream tasks. To address this gap, we introduce\nthe task of orientation-aligned 3D object generation: producing 3D objects from\nsingle images with consistent orientations across categories. To facilitate\nthis, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D\nmodels spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two\nrepresentative 3D generative models based on multi-view diffusion and 3D\nvariational autoencoder frameworks to produce aligned objects that generalize\nwell to unseen objects across various categories. Experimental results\ndemonstrate the superiority of our method over post-hoc alignment approaches.\nFurthermore, we showcase downstream applications enabled by our aligned object\ngeneration, including zero-shot object orientation estimation via\nanalysis-by-synthesis and efficient arrow-based object rotation manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5355\u56fe\u50cf\u76843D\u7269\u4f53\u751f\u6210\u4efb\u52a1\uff0c\u786e\u4fdd\u751f\u6210\u7269\u4f53\u5728\u7c7b\u522b\u95f4\u5177\u6709\u4e00\u81f4\u7684\u671d\u5411\uff0c\u5e76\u6784\u5efa\u4e86Objaverse-OA\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5fae\u8c03\u4e24\u79cd3D\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u65b9\u6cd5\u4f18\u4e8e\u540e\u5904\u7406\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u4e00\u81f4\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u671d\u5411\u4e0d\u7edf\u4e00\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u53ef\u7528\u6027\u3002", "method": "\u6784\u5efaObjaverse-OA\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u57fa\u4e8e\u591a\u89c6\u56fe\u6269\u6563\u548c3D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u671d\u5411\u4e00\u81f4\u76843D\u7269\u4f53\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u540e\u5904\u7406\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u671d\u5411\u4f30\u8ba1\u548c\u9ad8\u6548\u7269\u4f53\u65cb\u8f6c\u64cd\u4f5c\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e863D\u751f\u6210\u6a21\u578b\u671d\u5411\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u751f\u6210\u7ed3\u679c\u3002"}}
{"id": "2506.08045", "pdf": "https://arxiv.org/pdf/2506.08045", "abs": "https://arxiv.org/abs/2506.08045", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "categories": ["cs.RO", "cs.AI"], "comment": "40 pages, 6 Figures", "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86Agentic UAVs\uff08\u81ea\u4e3b\u65e0\u4eba\u673a\uff09\u7684\u6280\u672f\u67b6\u6784\u3001\u5e94\u7528\u9886\u57df\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5f3a\u8c03\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u80fd\u529b\u548c\u793e\u4f1a\u4ef7\u503c\u3002", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u6709\u9650\uff0c\u800cAgentic UAVs\u901a\u8fc7\u6574\u5408\u611f\u77e5\u3001\u51b3\u7b56\u3001\u8bb0\u5fc6\u548c\u534f\u4f5c\u89c4\u5212\uff0c\u5c55\u73b0\u4e86\u66f4\u9ad8\u7684\u667a\u80fd\u5316\u548c\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790Agentic UAVs\u7684\u67b6\u6784\u7ec4\u4ef6\u548c\u5173\u952e\u6280\u672f\uff0c\u5e76\u4e0e\u4f20\u7edf\u65e0\u4eba\u673a\u8fdb\u884c\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u5176\u5728AI\u4ee3\u7406\u3001\u5b66\u4e60\u548c\u4efb\u52a1\u7075\u6d3b\u6027\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86Agentic UAVs\u5728\u519c\u4e1a\u3001\u707e\u5bb3\u54cd\u5e94\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u4e03\u5927\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u786c\u4ef6\u521b\u65b0\u3001\u5b66\u4e60\u67b6\u6784\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u4e3aAgentic UAVs\u7684\u672a\u6765\u53d1\u5c55\u3001\u90e8\u7f72\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u4e86\u81ea\u8fdb\u5316\u7a7a\u4e2d\u751f\u6001\u7cfb\u7edf\u548c\u53ef\u6301\u7eed\u90e8\u7f72\u7684\u8def\u5f84\u3002"}}
{"id": "2506.08717", "pdf": "https://arxiv.org/pdf/2506.08717", "abs": "https://arxiv.org/abs/2506.08717", "authors": ["Mehedi Hasan Bijoy", "Dejan Porjazovski", "Tam\u00e1s Gr\u00f3sz", "Mikko Kurimo"], "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025 conference", "summary": "Speech Emotion Recognition (SER) is crucial for improving human-computer\ninteraction. Despite strides in monolingual SER, extending them to build a\nmultilingual system remains challenging. Our goal is to train a single model\ncapable of multilingual SER by distilling knowledge from multiple teacher\nmodels. To address this, we introduce a novel language-aware multi-teacher\nknowledge distillation method to advance SER in English, Finnish, and French.\nIt leverages Wav2Vec2.0 as the foundation of monolingual teacher models and\nthen distills their knowledge into a single multilingual student model. The\nstudent model demonstrates state-of-the-art performance, with a weighted recall\nof 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish\ndataset, surpassing fine-tuning and knowledge distillation baselines. Our\nmethod excels in improving recall for sad and neutral emotions, although it\nstill faces challenges in recognizing anger and happiness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u82f1\u8bed\u3001\u82ac\u5170\u8bed\u548c\u6cd5\u8bed\u7684\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5355\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u6784\u5efa\u591a\u8bed\u8a00\u7cfb\u7edf\u4ecd\u5177\u6311\u6218\u6027\u3002\u76ee\u6807\u662f\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u611f\u77e5\u7684\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u4ee5Wav2Vec2.0\u4e3a\u57fa\u7840\u6784\u5efa\u5355\u8bed\u8a00\u6559\u5e08\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u77e5\u8bc6\u84b8\u998f\u5230\u591a\u8bed\u8a00\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5b66\u751f\u6a21\u578b\u5728\u82f1\u8bed\u6570\u636e\u96c6\u4e0a\u52a0\u6743\u53ec\u56de\u7387\u8fbe72.9\uff0c\u82ac\u5170\u8bed\u6570\u636e\u96c6\u4e0a\u672a\u52a0\u6743\u53ec\u56de\u7387\u8fbe63.4\uff0c\u4f18\u4e8e\u5fae\u8c03\u548c\u77e5\u8bc6\u84b8\u998f\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u60b2\u4f24\u548c\u4e2d\u6027\u60c5\u7eea\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6124\u6012\u548c\u5feb\u4e50\u60c5\u7eea\u8bc6\u522b\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2506.08649", "pdf": "https://arxiv.org/pdf/2506.08649", "abs": "https://arxiv.org/abs/2506.08649", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Youwei Lu"], "title": "Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization", "categories": ["cs.CV"], "comment": null, "summary": "Video memorability refers to the ability of videos to be recalled after\nviewing, playing a crucial role in creating content that remains memorable.\nExisting models typically focus on extracting multimodal features to predict\nvideo memorability scores but often fail to fully utilize motion cues. The\nrepresentation of motion features is compromised during the fine-tuning phase\nof the motion feature extractor due to a lack of labeled data. In this paper,\nwe introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal\nvideo memorability prediction model designed to enhance the representation of\nmotion features. We tackle the challenge of improving motion feature\nrepresentation by leveraging text description similarities across videos to\nestablish positive and negative motion sample sets for a given target. This\nenhancement allows the model to learn similar feature representations for\nsemantically related motion content, resulting in more accurate memorability\npredictions. Our model achieves state-of-the-art performance on two video\nmemorability prediction datasets. Moreover, the potential applications of video\nmemorability prediction have been underexplored. To address this gap, we\npresent Memorability Weighted Correction for Video Summarization (MWCVS), using\nvideo memorability prediction to reduce subjectivity in video summarization\nlabels. Experimental results on two video summarization datasets demonstrate\nthe effectiveness of MWCVS, showcasing the promising applications of video\nmemorability prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u89c6\u9891\u8bb0\u5fc6\u6027\u9884\u6d4b\u6a21\u578bTMCCL\uff0c\u901a\u8fc7\u6587\u672c-\u8fd0\u52a8\u8de8\u6a21\u6001\u5bf9\u6bd4\u635f\u5931\u589e\u5f3a\u8fd0\u52a8\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5728\u89c6\u9891\u6458\u8981\u4e2d\u5e94\u7528\u8bb0\u5fc6\u6027\u52a0\u6743\u6821\u6b63MWCVS\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u89c6\u9891\u8bb0\u5fc6\u6027\u65f6\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd0\u52a8\u7ebf\u7d22\uff0c\u4e14\u8fd0\u52a8\u7279\u5f81\u8868\u793a\u5728\u5fae\u8c03\u9636\u6bb5\u56e0\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u800c\u53d7\u635f\u3002\u672c\u6587\u65e8\u5728\u6539\u8fdb\u8fd0\u52a8\u7279\u5f81\u8868\u793a\u5e76\u63a2\u7d22\u89c6\u9891\u8bb0\u5fc6\u6027\u9884\u6d4b\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165TMCCL\u6a21\u578b\uff0c\u5229\u7528\u6587\u672c\u63cf\u8ff0\u76f8\u4f3c\u6027\u6784\u5efa\u6b63\u8d1f\u8fd0\u52a8\u6837\u672c\u96c6\uff0c\u589e\u5f3a\u8fd0\u52a8\u7279\u5f81\u8868\u793a\uff1b\u63d0\u51faMWCVS\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u9891\u8bb0\u5fc6\u6027\u9884\u6d4b\u51cf\u5c11\u89c6\u9891\u6458\u8981\u6807\u6ce8\u7684\u4e3b\u89c2\u6027\u3002", "result": "TMCCL\u5728\u4e24\u4e2a\u89c6\u9891\u8bb0\u5fc6\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff1bMWCVS\u5728\u4e24\u4e2a\u89c6\u9891\u6458\u8981\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "TMCCL\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8bb0\u5fc6\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0cMWCVS\u5c55\u793a\u4e86\u89c6\u9891\u8bb0\u5fc6\u6027\u9884\u6d4b\u5728\u89c6\u9891\u6458\u8981\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.08047", "pdf": "https://arxiv.org/pdf/2506.08047", "abs": "https://arxiv.org/abs/2506.08047", "authors": ["A. G. R. Sandeepa", "Sanka Mohottala"], "title": "Evaluation of Machine Learning Models in Student Academic Performance Prediction", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "Paper Accepted for IEEE ICARC Conference (2025). 6 pages, 5 figures", "summary": "This research investigates the use of machine learning methods to forecast\nstudents' academic performance in a school setting. Students' data with\nbehavioral, academic, and demographic details were used in implementations with\nstandard classical machine learning models including multi-layer perceptron\nclassifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across\nall implementations. Under 10-fold cross validation, MLPC obtained 79.58%\naverage accuracy for test set while for train set, it was 99.65%. MLP's better\nperformance over other machine learning models strongly suggest the potential\nuse of neural networks as data-efficient models. Feature selection approach\nplayed a crucial role in improving the performance and multiple evaluation\napproaches were used in order to compare with existing literature. Explainable\nmachine learning methods were utilized to demystify the black box models and to\nvalidate the feature selection approach.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9884\u6d4b\u5b66\u751f\u5b66\u4e1a\u8868\u73b0\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\uff08MLPC\uff09\u53d6\u5f97\u4e86\u6700\u9ad886.46%\u7684\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u3002", "motivation": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5b66\u751f\u5b66\u4e1a\u8868\u73b0\uff0c\u4ee5\u5e2e\u52a9\u6559\u80b2\u673a\u6784\u4f18\u5316\u6559\u5b66\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u5b66\u751f\u884c\u4e3a\u3001\u5b66\u672f\u548c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\uff0c\u7ed3\u5408\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\uff08MLPC\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u7279\u5f81\u9009\u62e9\u548c\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "MLPC\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6700\u9ad8\u51c6\u786e\u7387\u4e3a86.46%\uff0c10\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5e73\u5747\u51c6\u786e\u7387\u4e3a79.58%\uff0c\u8bad\u7ec3\u96c6\u51c6\u786e\u7387\u4e3a99.65%\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\uff08\u5982MLPC\uff09\u5728\u6570\u636e\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u5f81\u9009\u62e9\u548c\u53ef\u89e3\u91ca\u65b9\u6cd5\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.08726", "pdf": "https://arxiv.org/pdf/2506.08726", "abs": "https://arxiv.org/abs/2506.08726", "authors": ["Nelvin Tan", "Zian Seng", "Liang Zhang", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Improved LLM Agents for Financial Document Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Large language models (LLMs) have shown impressive capabilities on numerous\nnatural language processing tasks. However, LLMs still struggle with numerical\nquestion answering for financial documents that include tabular and textual\ndata. Recent works have showed the effectiveness of critic agents (i.e.,\nself-correction) for this task given oracle labels. Building upon this\nframework, this paper examines the effectiveness of the traditional critic\nagent when oracle labels are not available, and show, through experiments, that\nthis critic agent's performance deteriorates in this scenario. With this in\nmind, we present an improved critic agent, along with the calculator agent\nwhich outperforms the previous state-of-the-art approach (program-of-thought)\nand is safer. Furthermore, we investigate how our agents interact with each\nother, and how this interaction affects their performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91d1\u878d\u6587\u6863\u6570\u503c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f20\u7edf\u6279\u8bc4\u4ee3\u7406\u5728\u65e0\u6807\u7b7e\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u6279\u8bc4\u4ee3\u7406\u548c\u8ba1\u7b97\u5668\u4ee3\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLMs\u5728\u91d1\u878d\u6587\u6863\u7684\u6570\u503c\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6807\u7b7e\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u6279\u8bc4\u4ee3\u7406\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u6279\u8bc4\u4ee3\u7406\u548c\u8ba1\u7b97\u5668\u4ee3\u7406\uff0c\u5e76\u7814\u7a76\u5b83\u4eec\u7684\u4ea4\u4e92\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6539\u8fdb\u7684\u4ee3\u7406\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7a0b\u5e8f\u601d\u7ef4\uff09\uff0c\u4e14\u66f4\u5b89\u5168\u3002", "conclusion": "\u6539\u8fdb\u7684\u6279\u8bc4\u4ee3\u7406\u548c\u8ba1\u7b97\u5668\u4ee3\u7406\u5728\u65e0\u6807\u7b7e\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4ea4\u4e92\u7814\u7a76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.08650", "pdf": "https://arxiv.org/pdf/2506.08650", "abs": "https://arxiv.org/abs/2506.08650", "authors": ["Peter Gr\u00f6nquist", "Stepan Tulyakov", "Dengxin Dai"], "title": "Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Achieving consistent color reproduction across multiple cameras is essential\nfor seamless image fusion and Image Processing Pipeline (ISP) compatibility in\nmodern devices, but it is a challenging task due to variations in sensors and\noptics. Existing raw-to-raw conversion methods face limitations such as poor\nadaptability to changing illumination, high computational costs, or impractical\nrequirements such as simultaneous camera operation and overlapping\nfields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,\nphysically-informed approach that simulates raw images under specified\nillumination to estimate transformations between devices. The NPM effectively\nadapts to varying illumination conditions, can be initialized with physical\nmeasurements, and supports training with or without paired data. Experiments on\npublic datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent\nstate-of-the-art methods, providing robust chromatic consistency across\ndifferent sensors and optical systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u7269\u7406\u4fe1\u606f\u5316\u7684\u65b9\u6cd5\uff08NPM\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u539f\u59cb\u56fe\u50cf\uff0c\u4ee5\u4f30\u8ba1\u8bbe\u5907\u95f4\u7684\u8f6c\u6362\uff0c\u89e3\u51b3\u4e86\u591a\u76f8\u673a\u989c\u8272\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u591a\u76f8\u673a\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u4f20\u611f\u5668\u548c\u5149\u5b66\u5668\u4ef6\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u989c\u8272\u4e00\u81f4\u6027\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u3001\u8ba1\u7b97\u6210\u672c\u6216\u5b9e\u7528\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528Neural Physical Model\uff08NPM\uff09\uff0c\u901a\u8fc7\u6a21\u62df\u6307\u5b9a\u5149\u7167\u4e0b\u7684\u539f\u59cb\u56fe\u50cf\u6765\u4f30\u8ba1\u8bbe\u5907\u95f4\u7684\u8f6c\u6362\uff0c\u652f\u6301\u7269\u7406\u6d4b\u91cf\u521d\u59cb\u5316\u548c\u6709\u65e0\u914d\u5bf9\u6570\u636e\u7684\u8bad\u7ec3\u3002", "result": "\u5728NUS\u548cBeyondRGB\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cNPM\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u4f20\u611f\u5668\u548c\u5149\u5b66\u7cfb\u7edf\u7684\u7a33\u5065\u989c\u8272\u4e00\u81f4\u6027\u3002", "conclusion": "NPM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u76f8\u673a\u7cfb\u7edf\u7684\u989c\u8272\u4e00\u81f4\u6027\u9700\u6c42\u3002"}}
{"id": "2506.08738", "pdf": "https://arxiv.org/pdf/2506.08738", "abs": "https://arxiv.org/abs/2506.08738", "authors": ["Dror Kris Markus", "Fabrizio Gilardi", "Daria Stetsenko"], "title": "Societal AI Research Has Become Less Interdisciplinary", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday\nlife, calls to align AI development with ethical and societal values have\nintensified. Interdisciplinary collaboration is often championed as a key\npathway for fostering such engagement. Yet it remains unclear whether\ninterdisciplinary research teams are actually leading this shift in practice.\nThis study analyzes over 100,000 AI-related papers published on ArXiv between\n2014 and 2024 to examine how ethical values and societal concerns are\nintegrated into technical AI research. We develop a classifier to identify\nsocietal content and measure the extent to which research papers express these\nconsiderations. We find a striking shift: while interdisciplinary teams remain\nmore likely to produce societally-oriented research, computer science-only\nteams now account for a growing share of the field's overall societal output.\nThese teams are increasingly integrating societal concerns into their papers\nand tackling a wide range of domains - from fairness and safety to healthcare\nand misinformation. These findings challenge common assumptions about the\ndrivers of societal AI and raise important questions. First, what are the\nimplications for emerging understandings of AI safety and governance if most\nsocietally-oriented research is being undertaken by exclusively technical\nteams? Second, for scholars in the social sciences and humanities: in a\ntechnical field increasingly responsive to societal demands, what distinctive\nperspectives can we still offer to help shape the future of AI?", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e862014-2024\u5e74ArXiv\u4e0a\u768410\u4e07\u7bc7AI\u8bba\u6587\uff0c\u53d1\u73b0\u8de8\u5b66\u79d1\u56e2\u961f\u867d\u66f4\u503e\u5411\u793e\u4f1a\u5bfc\u5411\u7814\u7a76\uff0c\u4f46\u7eaf\u8ba1\u7b97\u673a\u79d1\u5b66\u56e2\u961f\u5728\u793e\u4f1a\u8bae\u9898\u7814\u7a76\u4e2d\u7684\u5360\u6bd4\u663e\u8457\u589e\u52a0\u3002", "motivation": "\u63a2\u8ba8AI\u53d1\u5c55\u5982\u4f55\u4e0e\u4f26\u7406\u548c\u793e\u4f1a\u4ef7\u503c\u5bf9\u9f50\uff0c\u4ee5\u53ca\u8de8\u5b66\u79d1\u5408\u4f5c\u662f\u5426\u5728\u5b9e\u8df5\u4e2d\u63a8\u52a8\u8fd9\u4e00\u8d8b\u52bf\u3002", "method": "\u5f00\u53d1\u5206\u7c7b\u5668\u8bc6\u522b\u8bba\u6587\u4e2d\u7684\u793e\u4f1a\u5185\u5bb9\uff0c\u5206\u6790\u8de8\u5b66\u79d1\u4e0e\u7eaf\u6280\u672f\u56e2\u961f\u5bf9\u793e\u4f1a\u8bae\u9898\u7684\u5173\u6ce8\u53d8\u5316\u3002", "result": "\u7eaf\u8ba1\u7b97\u673a\u79d1\u5b66\u56e2\u961f\u8d8a\u6765\u8d8a\u591a\u5730\u6574\u5408\u793e\u4f1a\u8bae\u9898\uff0c\u6d89\u53ca\u516c\u5e73\u3001\u5b89\u5168\u3001\u533b\u7597\u7b49\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u8de8\u5b66\u79d1\u56e2\u961f\u4e3b\u5bfc\u793e\u4f1a\u5bfc\u5411\u7814\u7a76\u7684\u5047\u8bbe\uff0c\u5e76\u5f15\u53d1\u5bf9AI\u5b89\u5168\u3001\u6cbb\u7406\u53ca\u793e\u4f1a\u79d1\u5b66\u72ec\u7279\u8d21\u732e\u7684\u601d\u8003\u3002"}}
{"id": "2506.08666", "pdf": "https://arxiv.org/pdf/2506.08666", "abs": "https://arxiv.org/abs/2506.08666", "authors": ["Wenzhuo Liu", "Fei Zhu", "Haiyang Guo", "Longhui Wei", "Cheng-Lin Liu"], "title": "LLaVA-c: Continual Improved Visual Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal models like LLaVA-1.5 achieve state-of-the-art visual\nunderstanding through visual instruction tuning on multitask datasets, enabling\nstrong instruction-following and multimodal performance. However, multitask\nlearning faces challenges such as task balancing, requiring careful adjustment\nof data proportions, and expansion costs, where new tasks risk catastrophic\nforgetting and need costly retraining. Continual learning provides a promising\nalternative to acquiring new knowledge incrementally while preserving existing\ncapabilities. However, current methods prioritize task-specific performance,\nneglecting base model degradation from overfitting to specific instructions,\nwhich undermines general capabilities. In this work, we propose a simple but\neffective method with two modifications on LLaVA-1.5: spectral-aware\nconsolidation for improved task balance and unsupervised inquiry regularization\nto prevent base model degradation. We evaluate both general and task-specific\nperformance across continual pretraining and fine-tuning. Experiments\ndemonstrate that LLaVA-c consistently enhances standard benchmark performance\nand preserves general capabilities. For the first time, we show that\ntask-by-task continual learning can achieve results that match or surpass\nmultitask joint learning. The code will be publicly released.", "AI": {"tldr": "LLaVA-c\u901a\u8fc7\u6539\u8fdb\u4efb\u52a1\u5e73\u8861\u548c\u9632\u6b62\u57fa\u7840\u6a21\u578b\u9000\u5316\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u591a\u4efb\u52a1\u8054\u5408\u5b66\u4e60\u7684\u6548\u679c\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u5b58\u5728\u4efb\u52a1\u5e73\u8861\u548c\u6269\u5c55\u6210\u672c\u95ee\u9898\uff0c\u6301\u7eed\u5b66\u4e60\u867d\u80fd\u589e\u91cf\u83b7\u53d6\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u57fa\u7840\u6a21\u578b\u9000\u5316\u3002", "method": "\u5728LLaVA-1.5\u57fa\u7840\u4e0a\u5f15\u5165\u9891\u8c31\u611f\u77e5\u5de9\u56fa\u548c\u65e0\u76d1\u7763\u67e5\u8be2\u6b63\u5219\u5316\u3002", "result": "LLaVA-c\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u63d0\u5347\u57fa\u51c6\u6027\u80fd\u5e76\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u4efb\u52a1\u7ea7\u6301\u7eed\u5b66\u4e60\u53ef\u5ab2\u7f8e\u591a\u4efb\u52a1\u8054\u5408\u5b66\u4e60\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.08049", "pdf": "https://arxiv.org/pdf/2506.08049", "abs": "https://arxiv.org/abs/2506.08049", "authors": ["Tengfei Lyu", "Weijia Zhang", "Hao Liu"], "title": "Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions\nfrom several weeks to months in advance, presents significant challenges due to\nthe chaotic dynamics of atmospheric systems and complex interactions across\nmultiple scales. Current approaches often fail to explicitly model underlying\nphysical processes and teleconnections that are crucial at S2S timescales. We\nintroduce TelePiT, a novel deep learning architecture that enhances global S2S\nforecasting through integrated multi-scale physics and teleconnection\nawareness. Our approach consists of three key components: (1) Spherical\nHarmonic Embedding, which accurately encodes global atmospheric variables onto\nspherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which\nexplicitly captures atmospheric physical processes across multiple learnable\nfrequency bands; (3) Teleconnection-Aware Transformer, which models critical\nglobal climate interactions through tactfully injecting teleconnection patterns\ninto the self-attention. Extensive experiments demonstrate that TelePiT\nsignificantly outperforms state-of-the-art data-driven baselines and\noperational numerical weather prediction systems, with remarkable improvements\nfor atmospheric variables including a 57.7% reduction in RMSE for 2-meter\ntemperature compared to previous best models.", "AI": {"tldr": "TelePiT\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5c3a\u5ea6\u7269\u7406\u548c\u9065\u76f8\u5173\u610f\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u7403S2S\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "S2S\u9884\u6d4b\u56e0\u5927\u6c14\u7cfb\u7edf\u7684\u6df7\u6c8c\u52a8\u529b\u5b66\u548c\u591a\u5c3a\u5ea6\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u660e\u786e\u5efa\u6a21\u5173\u952e\u7269\u7406\u8fc7\u7a0b\u548c\u9065\u76f8\u5173\u3002", "method": "TelePiT\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7403\u8c10\u5d4c\u5165\u3001\u591a\u5c3a\u5ea6\u7269\u7406\u4fe1\u606f\u795e\u7ecfODE\u548c\u9065\u76f8\u5173\u611f\u77e5Transformer\u3002", "result": "TelePiT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u548c\u6570\u503c\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\uff0c\u59822\u7c73\u6e29\u5ea6RMSE\u964d\u4f4e57.7%\u3002", "conclusion": "TelePiT\u901a\u8fc7\u6574\u5408\u7269\u7406\u548c\u9065\u76f8\u5173\u5efa\u6a21\uff0c\u4e3aS2S\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08746", "pdf": "https://arxiv.org/pdf/2506.08746", "abs": "https://arxiv.org/abs/2506.08746", "authors": ["Muhammad Anwar", "Mishca de Costa", "Issam Hammad", "Daniel Lau"], "title": "Towards Secure and Private Language Models for Nuclear Power Plants", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper introduces a domain-specific Large Language Model for nuclear\napplications, built from the publicly accessible Essential CANDU textbook.\nDrawing on a compact Transformer-based architecture, the model is trained on a\nsingle GPU to protect the sensitive data inherent in nuclear operations.\nDespite relying on a relatively small dataset, it shows encouraging signs of\ncapturing specialized nuclear vocabulary, though the generated text sometimes\nlacks syntactic coherence. By focusing exclusively on nuclear content, this\napproach demonstrates the feasibility of in-house LLM solutions that align with\nrigorous cybersecurity and data confidentiality standards. Early successes in\ntext generation underscore the model's utility for specialized tasks, while\nalso revealing the need for richer corpora, more sophisticated preprocessing,\nand instruction fine-tuning to enhance domain accuracy. Future directions\ninclude extending the dataset to cover diverse nuclear subtopics, refining\ntokenization to reduce noise, and systematically evaluating the model's\nreadiness for real-world applications in nuclear domain.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u6838\u5e94\u7528\u9886\u57df\u7684\u4e13\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e\u516c\u5f00\u7684Essential CANDU\u6559\u6750\u6784\u5efa\uff0c\u91c7\u7528\u7d27\u51d1\u7684Transformer\u67b6\u6784\uff0c\u5355GPU\u8bad\u7ec3\u4ee5\u4fdd\u62a4\u654f\u611f\u6570\u636e\u3002\u5c3d\u7ba1\u6570\u636e\u96c6\u8f83\u5c0f\uff0c\u6a21\u578b\u80fd\u6355\u6349\u4e13\u4e1a\u6838\u8bcd\u6c47\uff0c\u4f46\u751f\u6210\u6587\u672c\u6709\u65f6\u7f3a\u4e4f\u53e5\u6cd5\u8fde\u8d2f\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7b26\u5408\u4e25\u683c\u7f51\u7edc\u5b89\u5168\u548c\u6570\u636e\u4fdd\u5bc6\u6807\u51c6\u7684\u5185\u90e8LLM\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u6838\u9886\u57df\u5185\u5bb9\u3002", "method": "\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u4f7f\u7528\u5355GPU\u8bad\u7ec3\uff0c\u6570\u636e\u6765\u6e90\u4e8eEssential CANDU\u6559\u6750\u3002", "result": "\u6a21\u578b\u80fd\u6355\u6349\u4e13\u4e1a\u6838\u8bcd\u6c47\uff0c\u4f46\u751f\u6210\u6587\u672c\u6709\u65f6\u4e0d\u8fde\u8d2f\uff0c\u5c55\u793a\u4e86\u5728\u6838\u9886\u57df\u4efb\u52a1\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672a\u6765\u9700\u6269\u5c55\u6570\u636e\u96c6\u3001\u4f18\u5316\u9884\u5904\u7406\u548c\u6307\u4ee4\u5fae\u8c03\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u6838\u9886\u57df\u7684\u51c6\u786e\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u5b9e\u9645\u5e94\u7528\u51c6\u5907\u5ea6\u3002"}}
{"id": "2506.08678", "pdf": "https://arxiv.org/pdf/2506.08678", "abs": "https://arxiv.org/abs/2506.08678", "authors": ["Juan Yeo", "Soonwoo Cha", "Jiwoo Song", "Hyunbin Jin", "Taesup Kim"], "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models such as CLIP have recently propelled open-vocabulary\ndense prediction tasks by enabling recognition of a broad range of visual\nconcepts. However, CLIP still struggles with fine-grained, region-level\nunderstanding, hindering its effectiveness on these dense prediction tasks. We\nidentify two pivotal factors required to address this limitation: semantic\ncoherence and fine-grained vision-language alignment. Current adaptation\nmethods often improve fine-grained alignment at the expense of semantic\ncoherence, and often rely on extra modules or supervised fine-tuning. To\novercome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel\napproach that simultaneously enhances semantic coherence and fine-grained\nalignment by leveraging own knowledge of a model across all representation\nlevels. Unlike prior methods, ATAS uses only unlabeled images and an internal\nself-distillation process to refine representations of CLIP vision encoders,\npreserving local semantic consistency while sharpening local detail\nrecognition. On open-vocabulary object detection and semantic segmentation\nbenchmarks, ATAS achieves substantial performance gains, outperforming baseline\nCLIP models. These results validate the effectiveness of our approach and\nunderscore the importance of jointly maintaining semantic coherence and\nfine-grained alignment for advanced open-vocabulary dense prediction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aATAS\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u533a\u57df\u7ea7\u7406\u89e3\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "CLIP\u5728\u7ec6\u7c92\u5ea6\u548c\u533a\u57df\u7ea7\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u5176\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002\u5f53\u524d\u65b9\u6cd5\u5f80\u5f80\u727a\u7272\u8bed\u4e49\u4e00\u81f4\u6027\u6765\u63d0\u5347\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "method": "\u63d0\u51faAny-to-Any Self-Distillation (ATAS)\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u8fc7\u7a0b\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u77e5\u8bc6\uff0c\u5728\u65e0\u6807\u7b7e\u56fe\u50cf\u4e0a\u4f18\u5316CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8868\u793a\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0cATAS\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfCLIP\u6a21\u578b\u3002", "conclusion": "ATAS\u901a\u8fc7\u540c\u65f6\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08054", "pdf": "https://arxiv.org/pdf/2506.08054", "abs": "https://arxiv.org/abs/2506.08054", "authors": ["Yiming Wang", "Hao Peng", "Senzhang Wang", "Haohua Du", "Chunyang Liu", "Jia Wu", "Guanlin Wu"], "title": "STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 5 figures, 3 tables. Extended version of paper accepted at\n  IJCAI 2025", "summary": "Traffic data imputation is fundamentally important to support various\napplications in intelligent transportation systems such as traffic flow\nprediction. However, existing time-to-space sequential methods often fail to\neffectively extract features in block-wise missing data scenarios. Meanwhile,\nthe static graph structure for spatial feature propagation significantly\nconstrains the models flexibility in handling the distribution shift issue for\nthe nonstationary traffic data. To address these issues, this paper proposes a\nSpatioTemporal Attention Mixture of experts network named STAMImputer for\ntraffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)\nframework to capture latent spatio-temporal features and their influence\nweights, effectively imputing block missing. A novel Low-rank guided Sampling\nGraph ATtention (LrSGAT) mechanism is designed to dynamically balance the local\nand global correlations across road networks. The sampled attention vectors are\nutilized to generate dynamic graphs that capture real-time spatial\ncorrelations. Extensive experiments are conducted on four traffic datasets for\nevaluation. The result shows STAMImputer achieves significantly performance\nimprovement compared with existing SOTA approaches. Our codes are available at\nhttps://github.com/RingBDStack/STAMImupter.", "AI": {"tldr": "STAMImputer\u662f\u4e00\u79cd\u7528\u4e8e\u4ea4\u901a\u6570\u636e\u586b\u8865\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u6df7\u5408\u4e13\u5bb6\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u5757\u7f3a\u5931\u6570\u636e\u573a\u666f\u548c\u9759\u6001\u56fe\u7ed3\u6784\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5757\u7f3a\u5931\u6570\u636e\u573a\u666f\u4e2d\u63d0\u53d6\u7279\u5f81\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u9759\u6001\u56fe\u7ed3\u6784\u9650\u5236\u4e86\u5904\u7406\u975e\u5e73\u7a33\u4ea4\u901a\u6570\u636e\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff08MoE\uff09\u6355\u83b7\u65f6\u7a7a\u7279\u5f81\u53ca\u5176\u6743\u91cd\uff0c\u8bbe\u8ba1\u4f4e\u79e9\u5f15\u5bfc\u91c7\u6837\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff08LrSGAT\uff09\u52a8\u6001\u5e73\u8861\u8def\u7f51\u76f8\u5173\u6027\u3002", "result": "\u5728\u56db\u4e2a\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTAMImputer\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STAMImputer\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u6570\u636e\u586b\u8865\u4e2d\u7684\u5757\u7f3a\u5931\u548c\u52a8\u6001\u7a7a\u95f4\u76f8\u5173\u6027\u6311\u6218\u3002"}}
{"id": "2506.08750", "pdf": "https://arxiv.org/pdf/2506.08750", "abs": "https://arxiv.org/abs/2506.08750", "authors": ["Muhammad Anwar", "Daniel Lau", "Mishca de Costa", "Issam Hammad"], "title": "Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data", "categories": ["cs.CL"], "comment": null, "summary": "The nuclear industry possesses a wealth of valuable information locked away\nin unstructured text data. This data, however, is not readily usable for\nadvanced Large Language Model (LLM) applications that require clean, structured\nquestion-answer pairs for tasks like model training, fine-tuning, and\nevaluation. This paper explores how synthetic data generation can bridge this\ngap, enabling the development of robust LLMs for the nuclear domain. We discuss\nthe challenges of data scarcity and privacy concerns inherent in the nuclear\nindustry and how synthetic data provides a solution by transforming existing\ntext data into usable Q&A pairs. This approach leverages LLMs to analyze text,\nextract key information, generate relevant questions, and evaluate the quality\nof the resulting synthetic dataset. By unlocking the potential of LLMs in the\nnuclear industry, synthetic data can pave the way for improved information\nretrieval, enhanced knowledge sharing, and more informed decision-making in\nthis critical sector.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u89e3\u51b3\u6838\u5de5\u4e1a\u4e2d\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u7684\u53ef\u7528\u6027\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u3002", "motivation": "\u6838\u5de5\u4e1a\u4e2d\u5b58\u5728\u5927\u91cf\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u9700\u8981\u7ed3\u6784\u5316\u95ee\u7b54\u5bf9\u7684LLM\u4efb\u52a1\u3002", "method": "\u5229\u7528LLM\u5206\u6790\u6587\u672c\u3001\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u751f\u6210\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u5408\u6210\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002", "result": "\u5408\u6210\u6570\u636e\u80fd\u591f\u586b\u8865\u6838\u5de5\u4e1a\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\u7684\u7a7a\u767d\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4e3a\u6838\u5de5\u4e1a\u4e2d\u7684\u4fe1\u606f\u68c0\u7d22\u3001\u77e5\u8bc6\u5171\u4eab\u548c\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.08690", "pdf": "https://arxiv.org/pdf/2506.08690", "abs": "https://arxiv.org/abs/2506.08690", "authors": ["Hugo Porta", "Emanuele Dalsasso", "Jessica L. McCarty", "Devis Tuia"], "title": "CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities", "categories": ["cs.CV"], "comment": "34 pages, 11 figures", "summary": "Canada experienced in 2023 one of the most severe wildfire seasons in recent\nhistory, causing damage across ecosystems, destroying communities, and emitting\nlarge quantities of CO2. This extreme wildfire season is symptomatic of a\nclimate-change-induced increase in the length and severity of the fire season\nthat affects the boreal ecosystem. Therefore, it is critical to empower\nwildfire management in boreal communities with better mitigation solutions.\nWildfire probability maps represent an important tool for understanding the\nlikelihood of wildfire occurrence and the potential severity of future\nwildfires. The massive increase in the availability of Earth observation data\nhas enabled the development of deep learning-based wildfire forecasting models,\naiming at providing precise wildfire probability maps at different spatial and\ntemporal scales. A main limitation of such methods is their reliance on\ncoarse-resolution environmental drivers and satellite products, leading to\nwildfire occurrence prediction of reduced resolution, typically around $\\sim\n0.1${\\deg}. This paper presents a benchmark dataset: CanadaFireSat, and\nbaseline methods for high-resolution: 100 m wildfire forecasting across Canada,\nleveraging multi-modal data from high-resolution multi-spectral satellite\nimages (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and\nenvironmental factors (ERA5 reanalysis data). Our experiments consider two\nmajor deep learning architectures. We observe that using multi-modal temporal\ninputs outperforms single-modal temporal inputs across all metrics, achieving a\npeak performance of 60.3% in F1 score for the 2023 wildfire season, a season\nnever seen during model training. This demonstrates the potential of\nmulti-modal deep learning models for wildfire forecasting at high-resolution\nand continental scale.", "AI": {"tldr": "\u52a0\u62ff\u59272023\u5e74\u91ce\u706b\u5b63\u8282\u5f02\u5e38\u4e25\u91cd\uff0c\u51f8\u663e\u6c14\u5019\u53d8\u5316\u5bf9\u706b\u707e\u5b63\u8282\u7684\u5f71\u54cd\u3002\u7814\u7a76\u63d0\u51fa\u9ad8\u5206\u8fa8\u7387\u91ce\u706b\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u91ce\u706b\u5b63\u8282\u5ef6\u957f\u548c\u52a0\u5267\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u91ce\u706b\u7ba1\u7406\u5de5\u5177\u3002\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\u6a21\u578b\u53ef\u5e2e\u52a9\u7406\u89e3\u91ce\u706b\u53d1\u751f\u6982\u7387\u548c\u6f5c\u5728\u4e25\u91cd\u6027\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff08Sentinel-2\u3001MODIS\u3001ERA5\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5f00\u53d1\u9ad8\u5206\u8fa8\u7387\uff08100\u7c73\uff09\u91ce\u706b\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u591a\u6a21\u6001\u8f93\u5165\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001\u8f93\u5165\uff0c\u57282023\u5e74\u91ce\u706b\u5b63\u8282\u7684F1\u5206\u6570\u8fbe\u523060.3%\u3002", "conclusion": "\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u548c\u5927\u9646\u5c3a\u5ea6\u91ce\u706b\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.08059", "pdf": "https://arxiv.org/pdf/2506.08059", "abs": "https://arxiv.org/abs/2506.08059", "authors": ["Huong Van Le", "Weibin Ren", "Junhong Kim", "Yukyung Yun", "Young Bin Park", "Young Jun Kim", "Bok Kyung Han", "Inho Choi", "Jong IL Park", "Hwi-Yeol Yun", "Jae-Mun Choi"], "title": "CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "49 pages, 11 figures", "summary": "Caco-2 permeability serves as a critical in vitro indicator for predicting\nthe oral absorption of drug candidates during early-stage drug discovery. To\nenhance the accuracy and efficiency of computational predictions, we\nsystematically investigated the impact of eight molecular feature\nrepresentation types including 2D/3D descriptors, structural fingerprints, and\ndeep learning-based embeddings combined with automated machine learning\ntechniques to predict Caco-2 permeability. Using two datasets of differing\nscale and diversity (TDC benchmark and curated OCHEM data), we assessed model\nperformance across representations and identified PaDEL, Mordred, and RDKit\ndescriptors as particularly effective for Caco-2 prediction. Notably, the\nAutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,\nfor both PaDEL and Mordred representations, the incorporation of 3D descriptors\nresulted in a 15.73% reduction in MAE compared to using 2D features alone, as\nconfirmed by feature importance analysis. These findings highlight the\neffectiveness of AutoML approaches in ADMET modeling and offer practical\nguidance for feature selection in data-limited prediction tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u591a\u79cd\u5206\u5b50\u7279\u5f81\u8868\u793a\u548cAutoML\u6280\u672f\u63d0\u9ad8Caco-2\u6e17\u900f\u6027\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53d1\u73b03D\u63cf\u8ff0\u7b26\u7ed3\u5408AutoML\u6a21\u578b\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u63d0\u5347\u65e9\u671f\u836f\u7269\u53d1\u73b0\u4e2dCaco-2\u6e17\u900f\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u516b\u79cd\u5206\u5b50\u7279\u5f81\u8868\u793a\u7c7b\u578b\uff08\u59822D/3D\u63cf\u8ff0\u7b26\u3001\u6307\u7eb9\u7b49\uff09\u7ed3\u5408AutoML\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u8868\u73b0\u3002", "result": "PaDEL\u3001Mordred\u548cRDKit\u63cf\u8ff0\u7b26\u6548\u679c\u663e\u8457\uff0cAutoML\u6a21\u578bCaliciBoost\u8868\u73b0\u6700\u4f73\uff0c3D\u63cf\u8ff0\u7b26\u4f7fMAE\u964d\u4f4e15.73%\u3002", "conclusion": "AutoML\u65b9\u6cd5\u5728ADMET\u5efa\u6a21\u4e2d\u6709\u6548\uff0c\u4e3a\u6570\u636e\u6709\u9650\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u7279\u5f81\u9009\u62e9\u6307\u5bfc\u3002"}}
{"id": "2506.08753", "pdf": "https://arxiv.org/pdf/2506.08753", "abs": "https://arxiv.org/abs/2506.08753", "authors": ["Pradyoth Hegde", "Santosh Kesiraju", "Jan \u0160vec", "\u0160imon Sedl\u00e1\u010dek", "Bolaji Yusuf", "Old\u0159ich Plchot", "Deepak K T", "Jan \u010cernock\u00fd"], "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Interspeech 2025", "summary": "This study explores the application of in-context learning (ICL) to the\ndialogue state tracking (DST) problem and investigates the factors that\ninfluence its effectiveness. We use a sentence embedding based k-nearest\nneighbour method to retrieve the suitable demonstrations for ICL. The selected\ndemonstrations, along with the test samples, are structured within a template\nas input to the LLM. We then conduct a systematic study to analyse the impact\nof factors related to demonstration selection and prompt context on DST\nperformance. This work is conducted using the MultiWoZ2.4 dataset and focuses\nprimarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and\nLlama3.2-3B-Instruct models. Our findings provide several useful insights on\nin-context learning abilities of LLMs for dialogue state tracking.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\uff08DST\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5f71\u54cd\u5176\u6548\u679c\u7684\u56e0\u7d20\uff0c\u5e76\u57fa\u4e8e\u68c0\u7d22\u65b9\u6cd5\u548c\u6a21\u677f\u8f93\u5165\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "motivation": "\u63a2\u7d22ICL\u5728DST\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u7814\u7a76\u5f71\u54cd\u5176\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u53e5\u5b50\u5d4c\u5165\u7684k\u8fd1\u90bb\u65b9\u6cd5\u68c0\u7d22\u9002\u5408\u7684\u6f14\u793a\u6837\u672c\uff0c\u7ed3\u5408\u6a21\u677f\u8f93\u5165\u5230LLM\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728MultiWoZ2.4\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u591a\u4e2a\u6a21\u578b\uff08\u5982OLMo-7B-instruct\u7b49\uff09\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u5728DST\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5728DST\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53d1\u73b0\u3002"}}
{"id": "2506.08691", "pdf": "https://arxiv.org/pdf/2506.08691", "abs": "https://arxiv.org/abs/2506.08691", "authors": ["Congzhi Zhang", "Jiawei Peng", "Zhenglin Wang", "Yilong Lai", "Haowen Sun", "Heng Chang", "Fei Ma", "Weijiang Yu"], "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism", "categories": ["cs.CV"], "comment": "Accepted by ACL 2025 main", "summary": "Large Vision-Language Models (LVLMs) have shown exceptional performance in\nmultimodal tasks, but their effectiveness in complex visual reasoning is still\nconstrained, especially when employing Chain-of-Thought prompting techniques.\nIn this paper, we propose VReST, a novel training-free approach that enhances\nReasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.\nVReST meticulously traverses the reasoning landscape by establishing a search\ntree, where each node encapsulates a reasoning step, and each path delineates a\ncomprehensive reasoning sequence. Our innovative multimodal Self-Reward\nmechanism assesses the quality of reasoning steps by integrating the utility of\nsub-questions, answer correctness, and the relevance of vision-language clues,\nall without the need for additional models. VReST surpasses current prompting\nmethods and secures state-of-the-art performance across three multimodal\nmathematical reasoning benchmarks. Furthermore, it substantiates the efficacy\nof test-time scaling laws in multimodal tasks, offering a promising direction\nfor future research.", "AI": {"tldr": "VReST\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u81ea\u5956\u52b1\u673a\u5236\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4e2d\u4ecd\u6709\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u6280\u672f\u4e0b\u3002", "method": "VReST\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u63a8\u7406\u6811\uff0c\u7ed3\u5408\u81ea\u5956\u52b1\u673a\u5236\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\u8d28\u91cf\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u3002", "result": "VReST\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "VReST\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u5b9a\u5f8b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.08060", "pdf": "https://arxiv.org/pdf/2506.08060", "abs": "https://arxiv.org/abs/2506.08060", "authors": ["Asankhaya Sharma"], "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log\n\\frac{m}{\\delta} \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l\n\\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ suffice to approximate\nfine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$\nis the vocabulary size and $\\delta$ is the failure probability. For linear\nclassification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon}\n\\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log\n\\frac{1}{\\delta} \\right)$ are sufficient, where $d$ is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u6280\u672f\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u53ef\u4ee5\u8fd1\u4f3c\u76d1\u7763\u5fae\u8c03\u7684\u80fd\u529b\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u5728\u7406\u60f3\u548c\u5b9e\u9645\u573a\u666f\u4e0b\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u89c4\u6a21\u7684\u6570\u5b66\u754c\u9650\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7814\u7a76\u5982\u4f55\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b49\u6280\u672f\u8fd1\u4f3c\u5176\u80fd\u529b\uff0c\u4ee5\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u7406\u60f3\u5047\u8bbe\uff08\u65e0\u9650\u8ba1\u7b97\u8d44\u6e90\u548c\u5b8c\u6574\u6570\u636e\u96c6\uff09\uff0c\u8bc1\u660e\u57fa\u7840Transformer\u6a21\u578b\u53ef\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fd1\u4f3c\u5fae\u8c03\u80fd\u529b\uff0c\u5e76\u6269\u5c55\u5230\u6709\u9650\u4e0a\u4e0b\u6587\u548c\u90e8\u5206\u6570\u636e\u96c6\u7684\u5b9e\u9645\u60c5\u51b5\u3002", "result": "\u5bf9\u4e8e\u6587\u672c\u751f\u6210\u548c\u7ebf\u6027\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed9\u51fa\u4e86\u6570\u636e\u96c6\u89c4\u6a21\u7684\u6570\u5b66\u754c\u9650\uff0c\u8bc1\u660e\u4e86\u7406\u8bba\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5b9e\u9645\u6280\u672f\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u53ef\u8fde\u63a5\u7406\u8bba\u4e0e\u5e94\u7528\u3002"}}
{"id": "2506.08757", "pdf": "https://arxiv.org/pdf/2506.08757", "abs": "https://arxiv.org/abs/2506.08757", "authors": ["Mishca de Costa", "Muhammad Anwar", "Dave Mercier", "Mark Randall", "Issam Hammad"], "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL", "categories": ["cs.CL", "cs.LG"], "comment": "44th Annual CNS Conference and the 49th Annual CNS/CNA Student\n  Conference, Westin Harbour Castle Hotel, Toronto, ON, Canada, June 8-11, 2025", "summary": "Retrieving operational data from nuclear power plants requires exceptional\naccuracy and transparency due to the criticality of the decisions it supports.\nTraditionally, natural language to SQL (NL-to-SQL) approaches have been\nexplored for querying such data. While NL-to-SQL promises ease of use, it poses\nsignificant risks: end-users cannot easily validate generated SQL queries, and\nlegacy nuclear plant databases -- often complex and poorly structured --\ncomplicate query generation due to decades of incremental modifications. These\nchallenges increase the likelihood of inaccuracies and reduce trust in the\napproach. In this work, we propose an alternative paradigm: leveraging\nfunction-calling large language models (LLMs) to address these challenges.\nInstead of directly generating SQL queries, we define a set of pre-approved,\npurpose-specific functions representing common use cases. Queries are processed\nby invoking these functions, which encapsulate validated SQL logic. This hybrid\napproach mitigates the risks associated with direct NL-to-SQL translations by\nensuring that SQL queries are reviewed and optimized by experts before\ndeployment. While this strategy introduces the upfront cost of developing and\nmaintaining the function library, we demonstrate how NL-to-SQL tools can assist\nin the initial generation of function code, allowing experts to focus on\nvalidation rather than creation. Our study includes a performance comparison\nbetween direct NL-to-SQL generation and the proposed function-based approach,\nhighlighting improvements in accuracy and maintainability. This work\nunderscores the importance of balancing user accessibility with operational\nsafety and provides a novel, actionable framework for robust data retrieval in\ncritical systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u8c03\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u66ff\u4ee3\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u8f6cSQL\uff08NL-to-SQL\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6838\u7535\u5382\u6570\u636e\u67e5\u8be2\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u6838\u7535\u5382\u64cd\u4f5c\u6570\u636e\u67e5\u8be2\u9700\u8981\u9ad8\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4f20\u7edfNL-to-SQL\u65b9\u6cd5\u5b58\u5728\u9a8c\u8bc1\u56f0\u96be\u548c\u590d\u6742\u6570\u636e\u5e93\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4fe1\u4efb\u5ea6\u964d\u4f4e\u3002", "method": "\u91c7\u7528\u9884\u5b9a\u4e49\u3001\u4e13\u5bb6\u5ba1\u6838\u7684\u529f\u80fd\u5e93\uff0c\u901a\u8fc7LLM\u8c03\u7528\u8fd9\u4e9b\u529f\u80fd\u800c\u975e\u76f4\u63a5\u751f\u6210SQL\uff0c\u786e\u4fdd\u67e5\u8be2\u903b\u8f91\u7684\u53ef\u9760\u6027\u548c\u4f18\u5316\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u529f\u80fd\u5e93\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u4e0a\u4f18\u4e8e\u76f4\u63a5NL-to-SQL\u751f\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u5e73\u8861\u4e86\u7528\u6237\u6613\u7528\u6027\u548c\u64cd\u4f5c\u5b89\u5168\u6027\uff0c\u4e3a\u5173\u952e\u7cfb\u7edf\u6570\u636e\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08694", "pdf": "https://arxiv.org/pdf/2506.08694", "abs": "https://arxiv.org/abs/2506.08694", "authors": ["Mohammadreza Salehi", "Shashanka Venkataramanan", "Ioana Simion", "Efstratios Gavves", "Cees G. M. Snoek", "Yuki M Asano"], "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning", "categories": ["cs.CV"], "comment": "preprint", "summary": "Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u5bc6\u96c6\u70b9\u8f68\u8ff9\u5b66\u4e60\u65f6\u7a7a\u4e00\u81f4\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u589e\u5f3a\uff0c\u65e0\u6cd5\u5904\u7406\u7269\u4f53\u53d8\u5f62\u3001\u906e\u6321\u548c\u76f8\u673a\u8fd0\u52a8\uff0c\u5bfc\u81f4\u65f6\u95f4\u4e0a\u7684\u7279\u5f81\u5b66\u4e60\u4e0d\u4e00\u81f4\u3002", "method": "\u5229\u7528\u73b0\u6709\u70b9\u8ddf\u8e2a\u5668\u63d0\u53d6\u957f\u7a0b\u8fd0\u52a8\u8f68\u8ff9\uff0c\u901a\u8fc7\u52a8\u91cf\u7f16\u7801\u5668\u7684\u6700\u4f18\u4f20\u8f93\u673a\u5236\u4f18\u5316\u7279\u5f81\u805a\u7c7b\uff0c\u5e76\u6cbf\u8f68\u8ff9\u4f20\u64ad\u805a\u7c7b\u5206\u914d\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516d\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u96c6\u53ca\u56db\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u4e861%\u81f36%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fd0\u52a8\u4f5c\u4e3a\u9690\u5f0f\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u7684\u7279\u5f81\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2506.08062", "pdf": "https://arxiv.org/pdf/2506.08062", "abs": "https://arxiv.org/abs/2506.08062", "authors": ["Woosung Kim", "Jinho Lee", "Jongmin Lee", "Byung-Jun Lee"], "title": "FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Multi-objective Reinforcement Learning", "summary": "Multi-objective reinforcement learning (MORL) aims to optimize policies in\nthe presence of conflicting objectives, where linear scalarization is commonly\nused to reduce vector-valued returns into scalar signals. While effective for\ncertain preferences, this approach cannot capture fairness-oriented goals such\nas Nash social welfare or max-min fairness, which require nonlinear and\nnon-additive trade-offs. Although several online algorithms have been proposed\nfor specific fairness objectives, a unified approach for optimizing nonlinear\nwelfare criteria in the offline setting-where learning must proceed from a\nfixed dataset-remains unexplored. In this work, we present FairDICE, the first\noffline MORL framework that directly optimizes nonlinear welfare objective.\nFairDICE leverages distribution correction estimation to jointly account for\nwelfare maximization and distributional regularization, enabling stable and\nsample-efficient learning without requiring explicit preference weights or\nexhaustive weight search. Across multiple offline benchmarks, FairDICE\ndemonstrates strong fairness-aware performance compared to existing baselines.", "AI": {"tldr": "FairDICE\u662f\u9996\u4e2a\u79bb\u7ebf\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316\u975e\u7ebf\u6027\u798f\u5229\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u516c\u5e73\u5bfc\u5411\u76ee\u6807\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u6807\u91cf\u5316\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u975e\u7ebf\u6027\u3001\u975e\u52a0\u6027\u7684\u516c\u5e73\u76ee\u6807\uff08\u5982Nash\u793e\u4f1a\u798f\u5229\u6216\u6700\u5927\u6700\u5c0f\u516c\u5e73\uff09\uff0c\u79bb\u7ebf\u73af\u5883\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u4f18\u5316\u65b9\u6cd5\u3002", "method": "FairDICE\u5229\u7528\u5206\u5e03\u6821\u6b63\u4f30\u8ba1\uff0c\u8054\u5408\u4f18\u5316\u798f\u5229\u6700\u5927\u5316\u548c\u5206\u5e03\u6b63\u5219\u5316\uff0c\u65e0\u9700\u663e\u5f0f\u504f\u597d\u6743\u91cd\u6216\u7a77\u4e3e\u641c\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFairDICE\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u516c\u5e73\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "FairDICE\u4e3a\u79bb\u7ebfMORL\u4e2d\u7684\u975e\u7ebf\u6027\u798f\u5229\u4f18\u5316\u63d0\u4f9b\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08768", "pdf": "https://arxiv.org/pdf/2506.08768", "abs": "https://arxiv.org/abs/2506.08768", "authors": ["Ahmed Hasanaath", "Aisha Alansari", "Ahmed Ashraf", "Chafik Salmane", "Hamzah Luqman", "Saad Ezzini"], "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in reasoning\nabilities and general natural language processing (NLP) tasks, yet their\nperformance on Arabic data, characterized by rich morphology, diverse dialects,\nand complex script, remains underexplored. This paper presents a comprehensive\nbenchmarking study of multiple reasoning-focused LLMs, with a special emphasis\non the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP\ntasks. We experiment with various strategies, including zero-shot, few-shot,\nand fine-tuning. This allows us to systematically evaluate performance on\ndatasets covering a range of applications to examine their capacity for\nlinguistic reasoning under different levels of complexity. Our experiments\nreveal several key findings. First, carefully selecting just three in-context\nexamples delivers an average uplift of over 13 F1 points on classification\ntasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection\nfrom 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures\noutperform a strong GPT o4-mini baseline by an average of 12 F1 points on\ncomplex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning\nyields up to an additional 8 points in F1 and BLEU compared to equivalent\nincreases in model scale. The code is available at\nhttps://anonymous.4open.science/r/AraReasoner41299", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8DeepSeek\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u79cd\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\uff09\u8bc4\u4f30\u5176\u572815\u4e2a\u963f\u62c9\u4f2f\u8bedNLP\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u56e0\u5176\u4e30\u5bcc\u7684\u5f62\u6001\u3001\u591a\u6837\u7684\u65b9\u8a00\u548c\u590d\u6742\u7684\u4e66\u5199\u7cfb\u7edf\uff0cLLMs\u5728\u5176\u4e0a\u7684\u8868\u73b0\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u963f\u62c9\u4f2f\u8bed\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8DeepSeek\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u7cbe\u90093\u4e2a\u4e0a\u4e0b\u6587\u793a\u4f8b\u53ef\u663e\u8457\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\uff1b2\uff09DeepSeek\u6a21\u578b\u5728\u96f6\u6837\u672c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8eGPT o4-mini\uff1b3\uff09LoRA\u5fae\u8c03\u6bd4\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u66f4\u6709\u6548\u3002", "conclusion": "DeepSeek\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5c11\u6837\u672c\u548c\u5fae\u8c03\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2506.08699", "pdf": "https://arxiv.org/pdf/2506.08699", "abs": "https://arxiv.org/abs/2506.08699", "authors": ["Frederik Hagelskjaer"], "title": "ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 4 tables", "summary": "This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose\nestimation network for colorless point clouds. The pose estimation is\ncalculated from center and top points of the object, predicted by the neural\nnetwork. The network is trained on synthetic data, and tested on a benchmark\ndataset, where it demonstrates state-of-the-art performance and outperforms all\ncolorless methods. The network is able to run inference in only 250\nmilliseconds making it usable in many scenarios. Project page with code at\narrowpose.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u68c0\u6d4b\u548c5\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u7f51\u7edc\uff0c\u9002\u7528\u4e8e\u65e0\u8272\u70b9\u4e91\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u65e0\u8272\u70b9\u4e91\u4e2d\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7269\u4f53\u7684\u4e2d\u5fc3\u548c\u9876\u90e8\u70b9\u6765\u8ba1\u7b97\u59ff\u6001\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u63a8\u7406\u65f6\u95f4\u4ec5250\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2506.08066", "pdf": "https://arxiv.org/pdf/2506.08066", "abs": "https://arxiv.org/abs/2506.08066", "authors": ["Alexander Stepikin", "Evgenia Romanenkova", "Alexey Zaytsev"], "title": "WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Change Point Detection (CPD) aims to identify moments of abrupt distribution\nshifts in data streams. Real-world high-dimensional CPD remains challenging due\nto data pattern complexity and violation of common assumptions. Resorting to\nstandalone deep neural networks, the current state-of-the-art detectors have\nyet to achieve perfect quality. Concurrently, ensembling provides more robust\nsolutions, boosting the performance. In this paper, we investigate ensembles of\ndeep change point detectors and realize that standard prediction aggregation\ntechniques, e.g., averaging, are suboptimal and fail to account for problem\npeculiarities. Alternatively, we introduce WWAggr -- a novel task-specific\nmethod of ensemble aggregation based on the Wasserstein distance. Our procedure\nis versatile, working effectively with various ensembles of deep CPD models.\nMoreover, unlike existing solutions, we practically lift a long-standing\nproblem of the decision threshold selection for CPD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u65b0\u578b\u96c6\u6210\u805a\u5408\u65b9\u6cd5WWAggr\uff0c\u7528\u4e8e\u63d0\u5347\u9ad8\u7ef4\u6570\u636e\u6d41\u4e2d\u7684\u53d8\u5316\u70b9\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53d8\u5316\u70b9\u68c0\u6d4b\u5668\u5728\u590d\u6742\u6570\u636e\u6a21\u5f0f\u548c\u5e38\u89c1\u5047\u8bbe\u4e0d\u6210\u7acb\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6807\u51c6\u96c6\u6210\u805a\u5408\u65b9\u6cd5\uff08\u5982\u5e73\u5747\uff09\u65e0\u6cd5\u5145\u5206\u5229\u7528\u95ee\u9898\u7279\u6027\u3002", "method": "\u5f15\u5165WWAggr\u65b9\u6cd5\uff0c\u5229\u7528Wasserstein\u8ddd\u79bb\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u96c6\u6210\u805a\u5408\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6df1\u5ea6\u53d8\u5316\u70b9\u68c0\u6d4b\u6a21\u578b\u7684\u96c6\u6210\u3002", "result": "WWAggr\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u53d8\u5316\u70b9\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u51b3\u7b56\u9608\u503c\u9009\u62e9\u7684\u957f\u671f\u95ee\u9898\u3002", "conclusion": "WWAggr\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u96c6\u6210\u805a\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d8\u5316\u70b9\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.08827", "pdf": "https://arxiv.org/pdf/2506.08827", "abs": "https://arxiv.org/abs/2506.08827", "authors": ["Francisco Vargas", "Alejandro Gonz\u00e1lez Coene", "Gaston Escalante", "Exequiel Lob\u00f3n", "Manuel Pulido"], "title": "The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extraction of information about traffic accidents from legal documents is\ncrucial for quantifying insurance company costs. Extracting entities such as\npercentages of physical and/or psychological disability and the involved\ncompensation amounts is a challenging process, even for experts, due to the\nsubtle arguments and reasoning in the court decision. A two-step procedure is\nproposed: first, segmenting the document identifying the most relevant\nsegments, and then extracting the entities. For text segmentation, two\nmethodologies are compared: a classic method based on regular expressions and a\nsecond approach that divides the document into blocks of n-tokens, which are\nthen vectorized using multilingual models for semantic searches\n(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models\n(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to\nthe selected segments for entity extraction. For the LLaMA models, fine-tuning\nis performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a\nsignificant number of hallucinations in extractions which are an important\ncontention point for named entity extraction. This work shows that these\nhallucinations are substantially reduced after finetuning the model. The\nperformance of the methodology based on segment vectorization and subsequent\nuse of LLMs significantly surpasses the classic method which achieves an\naccuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning\nachieves the highest accuracy 79.4%, surpassing its base version 61.7%.\nNotably, the base LLaMA-3 8B model already performs comparably to the finetuned\nLLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model\ndevelopment. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6cd5\u5f8b\u6587\u6863\u4e2d\u63d0\u53d6\u4ea4\u901a\u4e8b\u6545\u4fe1\u606f\u7684\u4e24\u6b65\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5206\u5272\u548c\u5b9e\u4f53\u63d0\u53d6\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u57fa\u4e8e\u8bed\u4e49\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5fae\u8c03\u540eLLaMA-2 70B\u548cGPT-4 Turbo\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u4ece\u6cd5\u5f8b\u6587\u6863\u4e2d\u63d0\u53d6\u4ea4\u901a\u4e8b\u6545\u4fe1\u606f\u5bf9\u4fdd\u9669\u516c\u53f8\u6210\u672c\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6cd5\u9662\u5224\u51b3\u4e2d\u7684\u590d\u6742\u8bba\u8bc1\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u5148\u901a\u8fc7\u6b63\u5219\u8868\u8fbe\u5f0f\u6216\u8bed\u4e49\u641c\u7d22\u5206\u5272\u6587\u6863\uff0c\u518d\u7528LLMs\uff08\u5982LLaMA-2\u3001LLaMA-3\u3001GPT-4 Turbo\uff09\u63d0\u53d6\u5b9e\u4f53\uff0c\u5e76\u5bf9\u90e8\u5206\u6a21\u578b\u8fdb\u884cLoRA\u5fae\u8c03\u3002", "result": "\u57fa\u4e8e\u8bed\u4e49\u641c\u7d22\u548cLLMs\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0839.5%\u51c6\u786e\u7387\uff09\u3002\u5fae\u8c03\u540e\u7684LLaMA-2 70B\u8fbe\u523079.4%\uff0cLLaMA-3 8B\u57fa\u7840\u7248\u4e3a76.6%\uff0cGPT-4 Turbo\u6700\u9ad8\uff0886.1%\uff09\u3002", "conclusion": "\u5fae\u8c03\u548c\u5148\u8fdb\u6a21\u578b\uff08\u5982LLaMA-3\u548cGPT-4 Turbo\uff09\u663e\u8457\u63d0\u5347\u5b9e\u4f53\u63d0\u53d6\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5feb\u901f\u53d1\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.08704", "pdf": "https://arxiv.org/pdf/2506.08704", "abs": "https://arxiv.org/abs/2506.08704", "authors": ["Xiaohan Zhang", "Sitong Wang", "Yushen Yan", "Yi Yang", "Mingda Xu", "Qi Liu"], "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering", "categories": ["cs.CV"], "comment": null, "summary": "High-quality novel view synthesis for large-scale scenes presents a\nchallenging dilemma in 3D computer vision. Existing methods typically partition\nlarge scenes into multiple regions, reconstruct a 3D representation using\nGaussian splatting for each region, and eventually merge them for novel view\nrendering. They can accurately render specific scenes, yet they do not\ngeneralize effectively for two reasons: (1) rigid spatial partition techniques\nstruggle with arbitrary camera trajectories, and (2) the merging of regions\nresults in Gaussian overlap to distort texture details. To address these\nchallenges, we propose TraGraph-GS, leveraging a trajectory graph to enable\nhigh-precision rendering for arbitrarily large-scale scenes. We present a\nspatial partitioning method for large-scale scenes based on graphs, which\nincorporates a regularization constraint to enhance the rendering of textures\nand distant objects, as well as a progressive rendering strategy to mitigate\nartifacts caused by Gaussian overlap. Experimental results demonstrate its\nsuperior performance both on four aerial and four ground datasets and highlight\nits remarkable efficiency: our method achieves an average improvement of 1.86\ndB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to\nstate-of-the-art approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTraGraph-GS\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u56fe\u5b9e\u73b0\u5927\u89c4\u6a21\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u76f8\u673a\u8f68\u8ff9\u548c\u7eb9\u7406\u7ec6\u8282\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u573a\u666f\u65f6\u5b58\u5728\u76f8\u673a\u8f68\u8ff9\u9002\u5e94\u6027\u548c\u7eb9\u7406\u7ec6\u8282\u5931\u771f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u573a\u666f\u5206\u533a\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b63\u5219\u5316\u7ea6\u675f\u548c\u6e10\u8fdb\u6e32\u67d3\u7b56\u7565\uff0c\u51cf\u5c11\u9ad8\u65af\u91cd\u53e0\u5bfc\u81f4\u7684\u5931\u771f\u3002", "result": "\u5728\u56db\u4e2a\u7a7a\u4e2d\u548c\u56db\u4e2a\u5730\u9762\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u5e73\u5747\u63d0\u53471.86 dB\uff08\u7a7a\u4e2d\uff09\u548c1.62 dB\uff08\u5730\u9762\uff09\u3002", "conclusion": "TraGraph-GS\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u573a\u666f\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2506.08070", "pdf": "https://arxiv.org/pdf/2506.08070", "abs": "https://arxiv.org/abs/2506.08070", "authors": ["Ziheng Qin", "Hailun Xu", "Wei Chee Yew", "Qi Jia", "Yang Luo", "Kanchan Sarkar", "Danhui Guan", "Kai Wang", "Yang You"], "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution", "categories": ["cs.LG", "cs.AI"], "comment": "V1", "summary": "Machine learning relies heavily on data, yet the continuous growth of\nreal-world data poses challenges for efficient dataset construction and\ntraining. A fundamental yet unsolved question is: given our current model and\ndata, does a new data (sample/batch) need annotation/learning? Conventional\napproaches retain all available data, leading to non-optimal data and training\nefficiency. Active learning aims to reduce data redundancy by selecting a\nsubset of samples to annotate, while it increases pipeline complexity and\nintroduces bias. In this work, we propose Info-Coevolution, a novel framework\nthat efficiently enables models and data to coevolve through online selective\nannotation with no bias. Leveraging task-specific models (and open-source\nmodels), it selectively annotates and integrates online and web data to improve\ndatasets efficiently. For real-world datasets like ImageNet-1K,\nInfo-Coevolution reduces annotation and training costs by 32\\% without\nperformance loss. It is able to automatically give the saving ratio without\ntuning the ratio. It can further reduce the annotation ratio to 50\\% with\nsemi-supervised learning. We also explore retrieval-based dataset enhancement\nusing unlabeled open-source data. Code is available at\nhttps://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.", "AI": {"tldr": "\u63d0\u51faInfo-Coevolution\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u9009\u62e9\u6027\u6807\u6ce8\u51cf\u5c11\u6570\u636e\u5197\u4f59\uff0c\u964d\u4f4e\u6807\u6ce8\u548c\u8bad\u7ec3\u6210\u672c32%\uff0c\u4e14\u65e0\u9700\u8c03\u53c2\u3002\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u53ef\u8fdb\u4e00\u6b65\u964d\u4f4e\u6807\u6ce8\u6bd4\u4f8b\u81f350%\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u6570\u636e\u589e\u957f\u5e26\u6765\u7684\u6807\u6ce8\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u5197\u4f59\u548c\u504f\u5dee\u3002", "method": "\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u9009\u62e9\u6027\u6807\u6ce8\u5e76\u6574\u5408\u5728\u7ebf\u548c\u7f51\u7edc\u6570\u636e\uff0c\u5b9e\u73b0\u6a21\u578b\u4e0e\u6570\u636e\u7684\u534f\u540c\u8fdb\u5316\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0c\u6807\u6ce8\u548c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e32%\uff0c\u6027\u80fd\u65e0\u635f\u5931\uff1b\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u53ef\u8fdb\u4e00\u6b65\u964d\u4f4e\u6807\u6ce8\u6bd4\u4f8b\u81f350%\u3002", "conclusion": "Info-Coevolution\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u504f\u7684\u6570\u636e\u6807\u6ce8\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u96c6\u6784\u5efa\u6548\u7387\u3002"}}
{"id": "2506.08836", "pdf": "https://arxiv.org/pdf/2506.08836", "abs": "https://arxiv.org/abs/2506.08836", "authors": ["Flavio D'Intino", "Hans-Peter Hutter"], "title": "Advancing STT for Low-Resource Real-World Speech", "categories": ["cs.CL", "cs.HC"], "comment": "Conference: HCI International 2025, 20 pages, 4 figures", "summary": "Swiss German is a low-resource language represented by diverse dialects that\ndiffer significantly from Standard German and from each other, lacking a\nstandardized written form. As a result, transcribing Swiss German involves\ntranslating into Standard German. Existing datasets have been collected in\ncontrolled environments, yielding effective speech-to-text (STT) models, but\nthese models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour\nannotated speech corpus featuring real-world long-audio recordings from 39\nSwiss German radio and TV stations. It captures spontaneous speech across all\nmajor Swiss dialects recorded in various realistic environments and overcomes\nthe limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,\nachieving notable enhancements over previous zero-shot performance metrics.\nImprovements in word error rate (WER) ranged from 19% to 33%, while BLEU scores\nincreased between 8% and 40%. The best fine-tuned model, large-v3, achieved a\nWER of 17.1% and a BLEU score of 74.8. This advancement is crucial for\ndeveloping effective and robust STT systems for Swiss German and other\nlow-resource languages in real-world contexts.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SRB-300\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u745e\u58eb\u5fb7\u8bed\u7684\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\uff0c\u901a\u8fc7\u5fae\u8c03Whisper\u6a21\u578b\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\u5e76\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u745e\u58eb\u5fb7\u8bed\u7f3a\u4e4f\u6807\u51c6\u5316\u4e66\u5199\u5f62\u5f0f\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5728\u81ea\u7136\u5bf9\u8bdd\u8bed\u97f3\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u7684\u6570\u636e\u96c6\u6765\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u4f7f\u7528SRB-300\u6570\u636e\u96c6\uff08300\u5c0f\u65f6\u7684\u771f\u5b9e\u5e7f\u64ad\u5f55\u97f3\uff09\u5fae\u8c03OpenAI Whisper\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\uff0c\u8bcd\u9519\u8bef\u7387\u964d\u4f4e19%-33%\uff0cBLEU\u5206\u6570\u63d0\u9ad88%-40%\uff0c\u6700\u4f73\u6a21\u578bWER\u4e3a17.1%\uff0cBLEU\u4e3a74.8%\u3002", "conclusion": "SRB-300\u6570\u636e\u96c6\u548c\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u745e\u58eb\u5fb7\u8bed\u8bed\u97f3\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.08710", "pdf": "https://arxiv.org/pdf/2506.08710", "abs": "https://arxiv.org/abs/2506.08710", "authors": ["Mengjiao Ma", "Qi Ma", "Yue Li", "Jiahuan Cheng", "Runyi Yang", "Bin Ren", "Nikola Popovic", "Mingqiang Wei", "Nicu Sebe", "Luc Van Gool", "Theo Gevers", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, codes, data and benchmark will be released", "summary": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient\nencoding of scene geometry, appearance, and semantics. Moreover, grounding\nlanguage in 3D scenes has proven to be an effective strategy for 3D scene\nunderstanding. Current Language Gaussian Splatting line of work fall into three\nmain groups: (i) per-scene optimization-based, (ii) per-scene\noptimization-free, and (iii) generalizable approach. However, most of them are\nevaluated only on rendered 2D views of a handful of scenes and viewpoints close\nto the training views, limiting ability and insight into holistic 3D\nunderstanding. To address this gap, we propose the first large-scale benchmark\nthat systematically assesses these three groups of methods directly in 3D\nspace, evaluating on 1060 scenes across three indoor datasets and one outdoor\ndataset. Benchmark results demonstrate a clear advantage of the generalizable\nparadigm, particularly in relaxing the scene-specific limitation, enabling fast\nfeed-forward inference on novel scenes, and achieving superior segmentation\nperformance. We further introduce GaussianWorld-49K a carefully curated 3DGS\ndataset comprising around 49K diverse indoor and outdoor scenes obtained from\nmultiple sources, with which we demonstrate the generalizable approach could\nharness strong data priors. Our codes, benchmark, and datasets will be made\npublic to accelerate research in generalizable 3DGS scene understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f303D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u57283D\u7a7a\u95f4\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b49K\u573a\u666f\u7684\u6570\u636e\u96c6GaussianWorld-49K\u3002", "motivation": "\u5f53\u524d3DGS\u65b9\u6cd5\u4e3b\u8981\u57282D\u89c6\u56fe\u4e0a\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf93D\u7a7a\u95f4\u6574\u4f53\u7406\u89e3\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e09\u7c7b3DGS\u65b9\u6cd5\uff08\u57fa\u4e8e\u573a\u666f\u4f18\u5316\u3001\u65e0\u573a\u666f\u4f18\u5316\u548c\u901a\u7528\u65b9\u6cd5\uff09\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6\u3002", "result": "\u901a\u7528\u65b9\u6cd5\u5728\u653e\u677e\u573a\u666f\u9650\u5236\u3001\u5feb\u901f\u63a8\u7406\u548c\u5206\u5272\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u901a\u7528\u65b9\u6cd5\u5229\u7528\u6570\u636e\u5148\u9a8c\u80fd\u529b\u5f3a\uff0c\u7814\u7a76\u5c06\u516c\u5f00\u4ee5\u52a0\u901f3DGS\u573a\u666f\u7406\u89e3\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.08073", "pdf": "https://arxiv.org/pdf/2506.08073", "abs": "https://arxiv.org/abs/2506.08073", "authors": ["Yu Liu", "Utkarsh Pratiush", "Kamyar Barakati", "Hiroshi Funakubo", "Ching-Che Lin", "Jaegyu Kim", "Lane W. Martin", "Sergei V. Kalinin"], "title": "Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.AI", "cs.LG"], "comment": null, "summary": "Ferroelectric polarization switching underpins the functional performance of\na wide range of materials and devices, yet its dependence on complex local\nmicrostructural features renders systematic exploration by manual or grid-based\nspectroscopic measurements impractical. Here, we introduce a multi-objective\nkernel-learning workflow that infers the microstructural rules governing\nswitching behavior directly from high-resolution imaging data. Applied to\nautomated piezoresponse force microscopy (PFM) experiments, our framework\nefficiently identifies the key relationships between domain-wall configurations\nand local switching kinetics, revealing how specific wall geometries and defect\ndistributions modulate polarization reversal. Post-experiment analysis projects\nabstract reward functions, such as switching ease and domain symmetry, onto\nphysically interpretable descriptors including domain configuration and\nproximity to boundaries. This enables not only high-throughput active learning,\nbut also mechanistic insight into the microstructural control of switching\nphenomena. While demonstrated for ferroelectric domain switching, our approach\nprovides a powerful, generalizable tool for navigating complex,\nnon-differentiable design spaces, from structure-property correlations in\nmolecular discovery to combinatorial optimization across diverse imaging\nmodalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u6838\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u6570\u636e\u63a8\u65ad\u5fae\u7ed3\u6784\u89c4\u5219\uff0c\u63ed\u793a\u4e86\u94c1\u7535\u7574\u58c1\u6784\u578b\u4e0e\u5c40\u90e8\u5f00\u5173\u52a8\u529b\u5b66\u7684\u5173\u7cfb\u3002", "motivation": "\u94c1\u7535\u6781\u5316\u5f00\u5173\u884c\u4e3a\u7684\u590d\u6742\u6027\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u5fae\u7ed3\u6784\u4e0e\u5f00\u5173\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u6838\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u538b\u7535\u529b\u663e\u5fae\u955c\uff08PFM\uff09\u5b9e\u9a8c\uff0c\u5206\u6790\u7574\u58c1\u6784\u578b\u4e0e\u5c40\u90e8\u5f00\u5173\u52a8\u529b\u5b66\u7684\u5173\u8054\u3002", "result": "\u63ed\u793a\u4e86\u7279\u5b9a\u7574\u58c1\u51e0\u4f55\u5f62\u72b6\u548c\u7f3a\u9677\u5206\u5e03\u5bf9\u6781\u5316\u53cd\u8f6c\u7684\u8c03\u63a7\u4f5c\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u7684\u63cf\u8ff0\u7b26\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u94c1\u7535\u7574\u5f00\u5173\u7814\u7a76\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u590d\u6742\u8bbe\u8ba1\u7a7a\u95f4\u7684\u63a2\u7d22\u3002"}}
{"id": "2506.08885", "pdf": "https://arxiv.org/pdf/2506.08885", "abs": "https://arxiv.org/abs/2506.08885", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86LLM\u9632\u5fa1\u4e2d\u7684\u51e0\u4f55\u76f2\u70b9\uff0c\u63d0\u51faALKALI\u5bf9\u6297\u57fa\u51c6\u548cGRACE\u9632\u5fa1\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dLLM\u9632\u5fa1\u65e0\u6cd5\u5e94\u5bf9\u5feb\u901f\u589e\u957f\u7684\u5bf9\u6297\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u6f5c\u5728\u4f2a\u88c5\u653b\u51fb\u3002", "method": "\u63d0\u51faALKALI\u5bf9\u6297\u57fa\u51c6\u548cGRACE\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u8868\u793a\u589e\u5f3a\u548c\u6f5c\u5728\u7a7a\u95f4\u6b63\u5219\u5316\u9632\u5fa1\u653b\u51fb\u3002", "result": "GRACE\u6846\u67b6\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e39%\uff0c\u5e76\u5f15\u5165AVQI\u6307\u6807\u91cf\u5316\u6f5c\u5728\u5bf9\u9f50\u5931\u8d25\u3002", "conclusion": "\u6f5c\u5728\u4f2a\u88c5\u662fLLM\u7684\u7ed3\u6784\u6027\u76f2\u70b9\uff0cGRACE\u548cAVQI\u4e3a\u9632\u5fa1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08729", "pdf": "https://arxiv.org/pdf/2506.08729", "abs": "https://arxiv.org/abs/2506.08729", "authors": ["Dieuwertje Alblas", "Patryk Rygiel", "Julian Suk", "Kaj O. Kappe", "Marieke Hofman", "Christoph Brune", "Kak Khee Yeung", "Jelmer M. Wolterink"], "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the\nabdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current\nclinical guidelines recommend elective surgical repair when the maximum AAA\ndiameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet\nthese criteria are periodically monitored, with surveillance intervals based on\nthe maximum AAA diameter. However, this diameter does not take into account the\ncomplex relation between the 3D AAA shape and its growth, making standardized\nintervals potentially unfit. Personalized AAA growth predictions could improve\nmonitoring strategies. We propose to use an SE(3)-symmetric transformer model\nto predict AAA growth directly on the vascular model surface enriched with\nlocal, multi-physical features. In contrast to other works which have\nparameterized the AAA shape, this representation preserves the vascular\nsurface's anatomical structure and geometric fidelity. We train our model using\na longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24\nAAA patients at irregularly sampled intervals. After training, our model\npredicts AAA growth to the next scan moment with a median diameter error of\n1.18 mm. We further demonstrate our model's utility to identify whether a\npatient will become eligible for elective repair within two years (acc = 0.93).\nFinally, we evaluate our model's generalization on an external validation set\nconsisting of 25 CTAs from 7 AAA patients from a different hospital. Our\nresults show that local directional AAA growth prediction from the vascular\nsurface is feasible and may contribute to personalized surveillance strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSE(3)-\u5bf9\u79f0\u53d8\u6362\u5668\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u8179\u4e3b\u52a8\u8109\u7624\uff08AAA\uff09\u7684\u751f\u957f\uff0c\u901a\u8fc7\u4fdd\u7559\u8840\u7ba1\u8868\u9762\u7684\u89e3\u5256\u7ed3\u6784\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u76d1\u6d4b\u7b56\u7565\u3002", "motivation": "\u5f53\u524dAAA\u76d1\u6d4b\u57fa\u4e8e\u6700\u5927\u76f4\u5f84\uff0c\u5ffd\u7565\u4e863D\u5f62\u72b6\u4e0e\u751f\u957f\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5bfc\u81f4\u6807\u51c6\u5316\u76d1\u6d4b\u95f4\u9694\u53ef\u80fd\u4e0d\u9002\u7528\u3002\u4e2a\u6027\u5316\u751f\u957f\u9884\u6d4b\u53ef\u4f18\u5316\u76d1\u6d4b\u7b56\u7565\u3002", "method": "\u4f7f\u7528SE(3)-\u5bf9\u79f0\u53d8\u6362\u5668\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u8840\u7ba1\u6a21\u578b\u8868\u9762\u9884\u6d4bAAA\u751f\u957f\uff0c\u7ed3\u5408\u5c40\u90e8\u591a\u7269\u7406\u7279\u5f81\u3002\u8bad\u7ec3\u6570\u636e\u4e3a24\u540d\u60a3\u8005\u7684113\u6b21CTA\u626b\u63cf\u3002", "result": "\u6a21\u578b\u9884\u6d4bAAA\u751f\u957f\u7684\u4e2d\u4f4d\u76f4\u5f84\u8bef\u5dee\u4e3a1.18 mm\uff0c\u5e76\u80fd\u4ee50.93\u7684\u51c6\u786e\u7387\u9884\u6d4b\u60a3\u8005\u662f\u5426\u5728\u4e24\u5e74\u5185\u9700\u624b\u672f\u4fee\u590d\u3002\u5916\u90e8\u9a8c\u8bc1\u96c6\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u57fa\u4e8e\u8840\u7ba1\u8868\u9762\u7684\u5c40\u90e8\u5b9a\u5411AAA\u751f\u957f\u9884\u6d4b\u53ef\u884c\uff0c\u6709\u52a9\u4e8e\u4e2a\u6027\u5316\u76d1\u6d4b\u7b56\u7565\u3002"}}
{"id": "2506.08074", "pdf": "https://arxiv.org/pdf/2506.08074", "abs": "https://arxiv.org/abs/2506.08074", "authors": ["Abdellah Ghassel", "Ian Robinson", "Gabriel Tanase", "Hal Cooper", "Bryan Thompson", "Zhen Han", "Vassilis N. Ioannidis", "Soji Adeshina", "Huzefa Rangwala"], "title": "Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "KDD '25", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models in\nexternal evidence, yet it still falters when answers must be pieced together\nacross semantically distant documents. We close this gap with the Hierarchical\nLexical Graph (HLG), a three-tier index that (i) traces every atomic\nproposition to its source, (ii) clusters propositions into latent topics, and\n(iii) links entities and relations to expose cross-document paths. On top of\nHLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,\nwhich performs fine-grained entity-aware beam search over propositions for\nhigh-precision factoid questions, and TopicGraphRAG, which selects coarse\ntopics before expanding along entity links to supply broad yet relevant context\nfor exploratory queries. Additionally, existing benchmarks lack the complexity\nrequired to rigorously evaluate multi-hop summarization systems, often focusing\non single-document queries or limited datasets. To address this, we introduce a\nsynthetic dataset generation pipeline that curates realistic, multi-document\nquestion-answer pairs, enabling robust evaluation of multi-hop retrieval\nsystems. Extensive experiments across five datasets demonstrate that our\nmethods outperform naive chunk-based RAG achieving an average relative\nimprovement of 23.1% in retrieval recall and correctness. Open-source Python\nlibrary is available at https://github.com/awslabs/graphrag-toolkit.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHLG\u7684\u4e09\u5c42\u7d22\u5f15\u7ed3\u6784\uff0c\u7ed3\u5408\u4e24\u79cd\u68c0\u7d22\u65b9\u6cd5\uff08StatementGraphRAG\u548cTopicGraphRAG\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6587\u6863\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRAG\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u8de8\u6587\u6863\u62fc\u63a5\u7b54\u6848\u7684\u590d\u6742\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86HLG\u7d22\u5f15\u7ed3\u6784\uff0c\u5305\u542b\u539f\u5b50\u547d\u9898\u8ffd\u8e2a\u3001\u6f5c\u5728\u4e3b\u9898\u805a\u7c7b\u548c\u8de8\u6587\u6863\u5b9e\u4f53\u94fe\u63a5\uff1b\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfRAG\u5e73\u5747\u63d0\u5347\u4e8623.1%\u7684\u68c0\u7d22\u53ec\u56de\u7387\u548c\u6b63\u786e\u6027\u3002", "conclusion": "HLG\u548c\u914d\u5957\u68c0\u7d22\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6587\u6863\u8bed\u4e49\u8ddd\u79bb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u5305\u63a8\u5e7f\u4f7f\u7528\u3002"}}
{"id": "2506.08897", "pdf": "https://arxiv.org/pdf/2506.08897", "abs": "https://arxiv.org/abs/2506.08897", "authors": ["Hiba Khey", "Amine Lakhder", "Salma Rouichi", "Imane El Ghabi", "Kamal Hejjaoui", "Younes En-nahli", "Fahd Kalloubi", "Moez Amri"], "title": "PlantBert: An Open Source Language Model for Plant Science", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of transformer-based language models has catalyzed\nbreakthroughs in biomedical and clinical natural language processing; however,\nplant science remains markedly underserved by such domain-adapted tools. In\nthis work, we present PlantBert, a high-performance, open-source language model\nspecifically tailored for extracting structured knowledge from plant\nstress-response literature. Built upon the DeBERTa architecture-known for its\ndisentangled attention and robust contextual encoding-PlantBert is fine-tuned\non a meticulously curated corpus of expert-annotated abstracts, with a primary\nfocus on lentil (Lens culinaris) responses to diverse abiotic and biotic\nstressors. Our methodology combines transformer-based modeling with\nrule-enhanced linguistic post-processing and ontology-grounded entity\nnormalization, enabling PlantBert to capture biologically meaningful\nrelationships with precision and semantic fidelity. The underlying corpus is\nannotated using a hierarchical schema aligned with the Crop Ontology,\nencompassing molecular, physiological, biochemical, and agronomic dimensions of\nplant adaptation. PlantBert exhibits strong generalization capabilities across\nentity types and demonstrates the feasibility of robust domain adaptation in\nlow-resource scientific fields. By providing a scalable and reproducible\nframework for high-resolution entity recognition, PlantBert bridges a critical\ngap in agricultural NLP and paves the way for intelligent, data-driven systems\nin plant genomics, phenomics, and agronomic knowledge discovery. Our model is\npublicly released to promote transparency and accelerate cross-disciplinary\ninnovation in computational plant science.", "AI": {"tldr": "PlantBert\u662f\u4e00\u4e2a\u57fa\u4e8eDeBERTa\u67b6\u6784\u7684\u9ad8\u6027\u80fd\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u690d\u7269\u80c1\u8feb\u54cd\u5e94\u6587\u732e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u77e5\u8bc6\u3002", "motivation": "\u690d\u7269\u79d1\u5b66\u9886\u57df\u7f3a\u4e4f\u9488\u5bf9\u6027\u7684\u8bed\u8a00\u6a21\u578b\u5de5\u5177\uff0cPlantBert\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u690d\u7269\u57fa\u56e0\u7ec4\u5b66\u3001\u8868\u578b\u7ec4\u5b66\u548c\u519c\u5b66\u77e5\u8bc6\u53d1\u73b0\u3002", "method": "\u57fa\u4e8eDeBERTa\u67b6\u6784\uff0c\u7ed3\u5408\u89c4\u5219\u589e\u5f3a\u7684\u8bed\u8a00\u540e\u5904\u7406\u548c\u57fa\u4e8e\u672c\u4f53\u7684\u5b9e\u4f53\u89c4\u8303\u5316\uff0c\u4f7f\u7528\u4e13\u5bb6\u6807\u6ce8\u7684\u6458\u8981\u8fdb\u884c\u5fae\u8c03\u3002", "result": "PlantBert\u5728\u5b9e\u4f53\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u79d1\u5b66\u9886\u57df\u7684\u7a33\u5065\u9886\u57df\u9002\u5e94\u3002", "conclusion": "PlantBert\u4e3a\u519c\u4e1aNLP\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u5236\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u690d\u7269\u79d1\u5b66\u7684\u8de8\u5b66\u79d1\u521b\u65b0\u3002"}}
{"id": "2506.08735", "pdf": "https://arxiv.org/pdf/2506.08735", "abs": "https://arxiv.org/abs/2506.08735", "authors": ["Yuhang Wang", "Jun Li", "Zhijian Wu", "Jianhua Xu"], "title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba.", "AI": {"tldr": "InceptionNeXt\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u5c40\u90e8\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\u3002InceptionMamba\u901a\u8fc7\u6b63\u4ea4\u5e26\u5377\u79ef\u548cMamba\u6a21\u5757\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "InceptionNeXt\u5728\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faInceptionMamba\u67b6\u6784\uff0c\u91c7\u7528\u6b63\u4ea4\u5e26\u5377\u79ef\u66ff\u4ee3\u4f20\u7edf\u4e00\u7ef4\u6761\u5e26\u5377\u79ef\uff0c\u5e76\u5f15\u5165\u74f6\u9888Mamba\u6a21\u5757\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "result": "\u5728\u5206\u7c7b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cInceptionMamba\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u8d8a\u3002", "conclusion": "InceptionMamba\u901a\u8fc7\u6539\u8fdb\u7a7a\u95f4\u5efa\u6a21\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08113", "pdf": "https://arxiv.org/pdf/2506.08113", "abs": "https://arxiv.org/abs/2506.08113", "authors": ["Timoth\u00e9e Hornek Amir Sartipi", "Igor Tchappi", "Gilbert Fridgen"], "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting", "categories": ["cs.LG", "cs.AI", "q-fin.ST"], "comment": null, "summary": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5728\u7535\u529b\u4ef7\u683c\u9884\u6d4b\uff08EPF\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0TSFMs\u8868\u73b0\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u53cc\u5b63\u8282\u6027MSTL\u6a21\u578b\u8868\u73b0\u6700\u7a33\u5b9a\u3002", "motivation": "\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u5bf9\u7535\u529b\u73b0\u8d27\u5e02\u573a\u4ea4\u6613\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u5f0fAI\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728EPF\u4e2d\u7684\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u75282024\u5e74\u5fb7\u56fd\u3001\u6cd5\u56fd\u3001\u8377\u5170\u3001\u5965\u5730\u5229\u548c\u6bd4\u5229\u65f6\u7684\u65e5\u524d\u62cd\u5356\u7535\u4ef7\u6570\u636e\uff0c\u5bf9\u6bd4\u4e86\u591a\u79cdTSFMs\u4e0e\u4f20\u7edf\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "Chronos-Bolt\u548cTime-MoE\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u53cc\u5b63\u8282\u6027MSTL\u6a21\u578b\u5728\u5404\u56fd\u548c\u8bc4\u4f30\u6307\u6807\u4e2d\u8868\u73b0\u6700\u7a33\u5b9a\u3002", "conclusion": "TSFMs\u5728EPF\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46MSTL\u6a21\u578b\u4ecd\u662f\u6700\u4f18\u9009\u62e9\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316TSFMs\u3002"}}
{"id": "2506.08899", "pdf": "https://arxiv.org/pdf/2506.08899", "abs": "https://arxiv.org/abs/2506.08899", "authors": ["Elias Horner", "Cristinel Mateis", "Guido Governatori", "Agata Ciabattoni"], "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LO"], "comment": null, "summary": "We present a novel approach to the automated semantic analysis of legal texts\nusing large language models (LLMs), targeting their transformation into formal\nrepresentations in Defeasible Deontic Logic (DDL). We propose a structured\npipeline that segments complex normative language into atomic snippets,\nextracts deontic rules, and evaluates them for syntactic and semantic\ncoherence. Our methodology is evaluated across various LLM configurations,\nincluding prompt engineering strategies, fine-tuned models, and multi-stage\npipelines, focusing on legal norms from the Australian Telecommunications\nConsumer Protections Code. Empirical results demonstrate promising alignment\nbetween machine-generated and expert-crafted formalizations, showing that LLMs\n- particularly when prompted effectively - can significantly contribute to\nscalable legal informatics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5206\u6790\u6cd5\u5f8b\u6587\u672c\u8bed\u4e49\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u5e9f\u6b62\u4e49\u52a1\u903b\u8f91\uff08DDL\uff09\u7684\u5f62\u5f0f\u8868\u793a\u3002", "motivation": "\u76ee\u6807\u662f\u89e3\u51b3\u6cd5\u5f8b\u6587\u672c\u7684\u590d\u6742\u6027\u548c\u89c4\u8303\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u9ad8\u6cd5\u5f8b\u4fe1\u606f\u5b66\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6d41\u7a0b\uff0c\u5c06\u590d\u6742\u6cd5\u5f8b\u8bed\u8a00\u5206\u89e3\u4e3a\u539f\u5b50\u7247\u6bb5\uff0c\u63d0\u53d6\u4e49\u52a1\u89c4\u5219\uff0c\u5e76\u8bc4\u4f30\u5176\u8bed\u6cd5\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u914d\u7f6e\uff0c\u5305\u62ec\u63d0\u793a\u5de5\u7a0b\u3001\u5fae\u8c03\u6a21\u578b\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u673a\u5668\u751f\u6210\u7684DDL\u5f62\u5f0f\u5316\u4e0e\u4e13\u5bb6\u624b\u5de5\u5f62\u5f0f\u5316\u5177\u6709\u826f\u597d\u4e00\u81f4\u6027\uff0c\u6709\u6548\u63d0\u793a\u7684LLMs\u80fd\u663e\u8457\u63d0\u5347\u6cd5\u5f8b\u4fe1\u606f\u5b66\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "LLMs\u5728\u6cd5\u5f8b\u6587\u672c\u8bed\u4e49\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6709\u6548\u63d0\u793a\u4e0b\uff0c\u80fd\u591f\u4e3a\u6cd5\u5f8b\u4fe1\u606f\u5b66\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08772", "pdf": "https://arxiv.org/pdf/2506.08772", "abs": "https://arxiv.org/abs/2506.08772", "authors": ["Jiayi Song", "Kaiyu Li", "Xiangyong Cao", "Deyu Meng"], "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nWe propose that Vision Foundation Models (VFMs), pre-trained on vast and\ndiverse datasets, possess robust generalization capabilities that can\neffectively bridge this distribution gap and provide strong semantic priors for\nSSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and\nFusion), a novel framework that leverages the powerful semantic knowledge\nembedded in VFMs to guide semi-supervised learning in remote sensing.\nSpecifically, RS-MTDF employs multiple frozen VFMs (\\textit{e.g.}, DINOv2 and\nCLIP) as expert teachers, utilizing feature-level distillation to align student\nfeatures with their robust representations. To further enhance discriminative\npower, the distilled knowledge is seamlessly fused into the student decoder.\nExtensive experiments on three challenging remote sensing datasets (ISPRS\nPotsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves\nstate-of-the-art performance. Notably, our method outperforms existing\napproaches across various label ratios on LoveDA and secures the highest IoU in\nthe majority of semantic categories. These results underscore the efficacy of\nmulti-teacher VFM guidance in significantly enhancing both generalization and\nsemantic understanding for remote sensing segmentation. Ablation studies\nfurther validate the contribution of each proposed module.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6RS-MTDF\uff0c\u901a\u8fc7\u591a\u6559\u5e08\u84b8\u998f\u4e0e\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u534a\u76d1\u7763\u5b66\u4e60\uff08SSS\uff09\u867d\u80fd\u7f13\u89e3\u6570\u636e\u4f9d\u8d56\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u6807\u8bb0\u4e0e\u672a\u6807\u8bb0\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u800c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "RS-MTDF\u5229\u7528\u9884\u8bad\u7ec3\u7684VFMs\uff08\u5982DINOv2\u548cCLIP\uff09\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u7279\u5f81\u7ea7\u84b8\u998f\u5c06\u5b66\u751f\u6a21\u578b\u7279\u5f81\u4e0e\u6559\u5e08\u6a21\u578b\u5bf9\u9f50\uff0c\u5e76\u5728\u89e3\u7801\u5668\u4e2d\u878d\u5408\u77e5\u8bc6\u4ee5\u589e\u5f3a\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728ISPRS Potsdam\u3001LoveDA\u548cDeepGlobe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRS-MTDF\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728LoveDA\u4e0a\u4e0d\u540c\u6807\u6ce8\u6bd4\u4f8b\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u5728\u591a\u6570\u8bed\u4e49\u7c7b\u522b\u4e2d\u53d6\u5f97\u6700\u9ad8IoU\u3002", "conclusion": "\u591a\u6559\u5e08VFM\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u5206\u5272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.08907", "pdf": "https://arxiv.org/pdf/2506.08907", "abs": "https://arxiv.org/abs/2506.08907", "authors": ["Antonios Dimakis", "John Pavlopoulos", "Antonios Anastasopoulos"], "title": "Dialect Normalization using Large Language Models and Morphological Rules", "categories": ["cs.CL", "I.2.7"], "comment": "19 pages, 18 figures, to be published in the Findings of the\n  Association for Computational Linguistics 2025", "summary": "Natural language understanding systems struggle with low-resource languages,\nincluding many dialects of high-resource ones. Dialect-to-standard\nnormalization attempts to tackle this issue by transforming dialectal text so\nthat it can be used by standard-language tools downstream. In this study, we\ntackle this task by introducing a new normalization method that combines\nrule-based linguistically informed transformations and large language models\n(LLMs) with targeted few-shot prompting, without requiring any parallel data.\nWe implement our method for Greek dialects and apply it on a dataset of\nregional proverbs, evaluating the outputs using human annotators. We then use\nthis dataset to conduct downstream experiments, finding that previous results\nregarding these proverbs relied solely on superficial linguistic information,\nincluding orthographic artifacts, while new observations can still be made\nthrough the remaining semantics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c4\u5219\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u65b9\u8a00\u6807\u51c6\u5316\uff0c\u65e0\u9700\u5e73\u884c\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5305\u62ec\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u65b9\u8a00\uff09\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u4e2d\u7684\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u8a00\u5b66\u8f6c\u6362\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u5728\u5e0c\u814a\u65b9\u8a00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5148\u524d\u7814\u7a76\u4ec5\u4f9d\u8d56\u8868\u5c42\u8bed\u8a00\u4fe1\u606f\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u6355\u6349\u66f4\u6df1\u5c42\u6b21\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u65b9\u8a00\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08777", "pdf": "https://arxiv.org/pdf/2506.08777", "abs": "https://arxiv.org/abs/2506.08777", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a\ncornerstone for many 3D vision tasks, enabling effective learning from\nlarge-scale unannotated data. At the scene level, existing SSL methods often\nincorporate volume rendering into the pre-training framework, using RGB-D\nimages as reconstruction signals to facilitate cross-modal learning. This\nstrategy promotes alignment between 2D and 3D modalities and enables the model\nto benefit from rich visual cues in the RGB-D inputs. However, these approaches\nare limited by their reliance on implicit scene representations and high memory\ndemands. Furthermore, since their reconstruction objectives are applied only in\n2D space, they often fail to capture underlying 3D geometric structures. To\naddress these challenges, we propose Gaussian2Scene, a novel scene-level SSL\nframework that leverages the efficiency and explicit nature of 3D Gaussian\nSplatting (3DGS) for pre-training. The use of 3DGS not only alleviates the\ncomputational burden associated with volume rendering but also supports direct\n3D scene reconstruction, thereby enhancing the geometric understanding of the\nbackbone network. Our approach follows a progressive two-stage training\nstrategy. In the first stage, a dual-branch masked autoencoder learns both 2D\nand 3D scene representations. In the second stage, we initialize training with\nreconstructed point clouds and further supervise learning using the geometric\nlocations of Gaussian primitives and rendered RGB images. This process\nreinforces both geometric and cross-modal learning. We demonstrate the\neffectiveness of Gaussian2Scene across several downstream 3D object detection\ntasks, showing consistent improvements over existing pre-training methods.", "AI": {"tldr": "Gaussian2Scene\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u70b9\u4e91\u9884\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5b58\u9700\u6c42\u548c3D\u51e0\u4f55\u7ed3\u6784\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u573a\u666f\u8868\u793a\u548c\u9ad8\u5185\u5b58\u9700\u6c42\uff0c\u4e14\u91cd\u5efa\u76ee\u6807\u4ec5\u5e94\u7528\u4e8e2D\u7a7a\u95f4\uff0c\u96be\u4ee5\u6355\u63493D\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u91c7\u75283DGS\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u53cc\u5206\u652f\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5b66\u4e602D\u548c3D\u573a\u666f\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u91cd\u5efa\u70b9\u4e91\u548c\u9ad8\u65af\u57fa\u5143\u7684\u51e0\u4f55\u4f4d\u7f6e\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e383D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "Gaussian2Scene\u901a\u8fc73DGS\u63d0\u9ad8\u4e86\u51e0\u4f55\u7406\u89e3\u80fd\u529b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4e3a3D\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2506.08139", "pdf": "https://arxiv.org/pdf/2506.08139", "abs": "https://arxiv.org/abs/2506.08139", "authors": ["Aviad Susman", "Mayte Su\u00e1rez-Fari\u00f1as", "Joseph T Colonel"], "title": "Nearness of Neighbors Attention for Regression in Supervised Finetuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "It is common in supervised machine learning to combine the feature extraction\ncapabilities of neural networks with the predictive power of traditional\nalgorithms, such as k-nearest neighbors (k-NN) or support vector machines. This\nprocedure involves performing supervised fine-tuning (SFT) on a\ndomain-appropriate feature extractor, followed by training a traditional\npredictor on the resulting SFT embeddings. When used in this manner,\ntraditional predictors often deliver increased performance over the SFT model\nitself, despite the fine-tuned feature extractor yielding embeddings\nspecifically optimized for prediction by the neural network's final dense\nlayer. This suggests that directly incorporating traditional algorithms into\nSFT as prediction layers may further improve performance. However, many\ntraditional algorithms have not been implemented as neural network layers due\nto their non-differentiable nature and their unique optimization requirements.\nAs a step towards solving this problem, we introduce the Nearness of Neighbors\nAttention (NONA) regression layer. NONA uses the mechanics of neural network\nattention and a novel learned attention-masking scheme to yield a\ndifferentiable proxy of the k-NN regression algorithm. Results on multiple\nunstructured datasets show improved performance over both dense layer\nprediction and k-NN on SFT embeddings for regression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNONA\u7684\u53ef\u5fae\u5206k-NN\u56de\u5f52\u5c42\uff0c\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u6ce8\u610f\u529b\u548c\u5b66\u4e60\u6ce8\u610f\u529b\u63a9\u7801\u673a\u5236\uff0c\u63d0\u5347\u4e86\u56de\u5f52\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7b97\u6cd5\uff08\u5982k-NN\uff09\u5728\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u540e\u8868\u73b0\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\u672c\u8eab\uff0c\u4f46\u56e0\u5176\u4e0d\u53ef\u5fae\u5206\u6027\u96be\u4ee5\u76f4\u63a5\u6574\u5408\u5230SFT\u4e2d\u3002", "method": "\u5f15\u5165NONA\u56de\u5f52\u5c42\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6ce8\u610f\u529b\u548c\u5b66\u4e60\u6ce8\u610f\u529b\u63a9\u7801\u673a\u5236\u6a21\u62dfk-NN\u56de\u5f52\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u975e\u7ed3\u6784\u5316\u6570\u636e\u96c6\u4e0a\uff0cNONA\u7684\u6027\u80fd\u4f18\u4e8e\u5bc6\u96c6\u5c42\u9884\u6d4b\u548c\u57fa\u4e8eSFT\u5d4c\u5165\u7684k-NN\u56de\u5f52\u3002", "conclusion": "NONA\u4e3a\u5c06\u4f20\u7edf\u7b97\u6cd5\u6574\u5408\u5230SFT\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u5f52\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.08920", "pdf": "https://arxiv.org/pdf/2506.08920", "abs": "https://arxiv.org/abs/2506.08920", "authors": ["Zeyu Leo Liu", "Greg Durrett", "Eunsol Choi"], "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Knowledge editing techniques for large language models (LLMs) can inject\nknowledge that is later reproducible verbatim, but they fall short on\npropagating that knowledge: models cannot answer questions that require\nreasoning with the injected knowledge. We present a hypernetwork-based approach\nfor knowledge propagation, named PropMEND, where we meta-learn how to modify\ngradients of a language modeling loss to encourage injected information to\npropagate. Our approach extends the meta-objective of MEND [29] so that\ngradient updates on knowledge are transformed to enable answering multi-hop\nquestions involving that knowledge. We show improved performance on the\nRippleEdit dataset, showing almost 2x accuracy on challenging multi-hop\nquestions whose answers are not explicitly stated in the injected fact. We\nfurther introduce a new dataset, Controlled RippleEdit, to evaluate the\ngeneralization of our hypernetwork, testing knowledge propagation along\nrelations and entities unseen during hypernetwork training. PropMEND still\noutperforms existing approaches in unseen entity-relation pairs, yet the\nperformance gap decreases substantially, suggesting future work in propagating\nknowledge to a wide range of relations.", "AI": {"tldr": "PropMEND\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u77e5\u8bc6\u4f20\u64ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u4fee\u6539\u8bed\u8a00\u6a21\u578b\u635f\u5931\u7684\u68af\u5ea6\uff0c\u4ee5\u4fc3\u8fdb\u6ce8\u5165\u77e5\u8bc6\u7684\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8df3\u95ee\u9898\u7684\u56de\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u65e0\u6cd5\u6709\u6548\u4f20\u64ad\u6ce8\u5165\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u56de\u7b54\u9700\u8981\u63a8\u7406\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8d85\u7f51\u7edc\u65b9\u6cd5\uff0c\u6269\u5c55MEND\u7684\u5143\u76ee\u6807\uff0c\u901a\u8fc7\u4fee\u6539\u68af\u5ea6\u66f4\u65b0\u5b9e\u73b0\u77e5\u8bc6\u4f20\u64ad\u3002", "result": "\u5728RippleEdit\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u8df3\u95ee\u9898\u51c6\u786e\u7387\u63d0\u5347\u8fd12\u500d\uff1b\u5728\u65b0\u6570\u636e\u96c6Controlled RippleEdit\u4e2d\uff0c\u672a\u89c1\u5b9e\u4f53\u5173\u7cfb\u5bf9\u4e0a\u7684\u6027\u80fd\u4ecd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PropMEND\u5728\u77e5\u8bc6\u4f20\u64ad\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u672a\u89c1\u5173\u7cfb\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u77e5\u8bc6\u4f20\u64ad\u8303\u56f4\u3002"}}
{"id": "2506.08780", "pdf": "https://arxiv.org/pdf/2506.08780", "abs": "https://arxiv.org/abs/2506.08780", "authors": ["Isaac Corley", "Lakshay Sharma", "Ruth Crasto"], "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L.", "AI": {"tldr": "Landsat-Bench\u662f\u4e00\u5957\u57fa\u4e8eLandsat\u5f71\u50cf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08GFM\uff09\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793aSSL4EO-L\u9884\u8bad\u7ec3\u7684GFM\u4f18\u4e8eImageNet\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u9488\u5bf9Landsat\u6570\u636e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9650\u5236\u4e86\u57fa\u4e8eLandsat\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7EuroSAT-L\u3001BigEarthNet-L\u548cLC100-L\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5e38\u89c1\u67b6\u6784\u548cSSL4EO-L\u9884\u8bad\u7ec3\u7684Landsat\u57fa\u7840\u6a21\u578b\u3002", "result": "SSL4EO-L\u9884\u8bad\u7ec3\u7684GFM\u5728EuroSAT-L\u548cBigEarthNet-L\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86+4% OA\u548c+5.1% mAP\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Landsat-Bench\u4e3aLandsat\u6570\u636e\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u8bc1\u660eSSL4EO-L\u9884\u8bad\u7ec3\u7684GFM\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eImageNet\u3002"}}
{"id": "2506.08935", "pdf": "https://arxiv.org/pdf/2506.08935", "abs": "https://arxiv.org/abs/2506.08935", "authors": ["Andrew Shin"], "title": "Can A Gamer Train A Mathematical Reasoning Model?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5355\u5f20\u666e\u901a\u6e38\u620fGPU\u4e0a\u8bad\u7ec3\u9ad8\u6027\u80fd\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u9ad8\u6602\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u5728\u5355\u5f20RTX 3080 Ti\uff0816GB\u5185\u5b58\uff09\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a1.5B\u53c2\u6570\u7684\u6570\u5b66\u63a8\u7406\u6a21\u578b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u8d44\u6e90\u9700\u6c42\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u6027\u80fd\u6570\u5b66\u63a8\u7406\u6a21\u578b\u65e0\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.08784", "pdf": "https://arxiv.org/pdf/2506.08784", "abs": "https://arxiv.org/abs/2506.08784", "authors": ["Jongyub Seok", "Chanjin Kang"], "title": "HomographyAD: Deep Anomaly Detection Using Self Homography Learning", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection (AD) is a task that distinguishes normal and abnormal data,\nwhich is important for applying automation technologies of the manufacturing\nfacilities. For MVTec dataset that is a representative AD dataset for\nindustrial environment, many recent works have shown remarkable performances.\nHowever, the existing anomaly detection works have a limitation of showing good\nperformance for fully-aligned datasets only, unlike real-world industrial\nenvironments. To solve this limitation, we propose HomographyAD, a novel deep\nanomaly detection methodology based on the ImageNet-pretrained network, which\nis specially designed for actual industrial dataset. Specifically, we first\nsuggest input foreground alignment using the deep homography estimation method.\nIn addition, we fine-tune the model by self homography learning to learn\nadditional shape information from normal samples. Finally, we conduct anomaly\ndetection based on the measure of how far the feature of test sample is from\nthe distribution of the extracted normal features. By applying our proposed\nmethod to various existing AD approaches, we show performance enhancement\nthrough extensive experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eImageNet\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5HomographyAD\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u5b8c\u5168\u5bf9\u9f50\u7684\u6570\u636e\u96c6\uff0c\u800c\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6570\u636e\u901a\u5e38\u672a\u5bf9\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHomographyAD\u65b9\u6cd5\uff0c\u5305\u62ec\u8f93\u5165\u524d\u666f\u5bf9\u9f50\u3001\u81ea\u540c\u6784\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7279\u5f81\u8ddd\u79bb\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "HomographyAD\u4e3a\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08149", "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCALL\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0fAI\u548c\u4e16\u754c\u6a21\u578b\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u901a\u4fe1\u5171\u4eab\u4fe1\u606f\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u590d\u6742\u9ad8\u7ef4\u73af\u5883\u4e2d\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u4fe1\u606f\u5171\u4eab\u867d\u5e38\u7528\u4f46\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faCALL\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u72b6\u6001\u548c\u610f\u56fe\u7f16\u7801\u4e3a\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u4fe1\u5171\u4eab\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u5176\u6cdb\u5316\u80fd\u529b\u6539\u8fdb\u9884\u6d4b\u548c\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u5728CARLA\u5e73\u53f0\u4e0a\u8fdb\u884c\uff0c\u5c55\u793a\u4e86CALL\u5728\u5c40\u90e8\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CALL\u901a\u8fc7\u4fe1\u606f\u5171\u4eab\u548c\u4e16\u754c\u6a21\u578b\u7684\u6709\u6548\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86MARL\u7684\u6027\u80fd\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2506.08938", "pdf": "https://arxiv.org/pdf/2506.08938", "abs": "https://arxiv.org/abs/2506.08938", "authors": ["Qinggang Zhang", "Zhishang Xiang", "Yilin Xiao", "Le Wang", "Junhui Li", "Xinrun Wang", "Jinsong Su"], "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Qinggang Zhang and Zhishang Xiang contributed equally to this work.\n  Corresponding author: Jinsong Su", "summary": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https:// github.com/DeepLearnXMU/Faithful-RAG", "AI": {"tldr": "FaithfulRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u51b2\u7a81\u65f6\u751f\u6210\u7ed3\u679c\u4e0d\u5fe0\u5b9e\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5236\u6291\u5236\u6a21\u578b\u77e5\u8bc6\u5b9e\u73b0\u5fe0\u5b9e\u6027\uff0c\u4f46\u635f\u5bb3\u4e86\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7ed3\u6784\u3002", "method": "FaithfulRAG\u5728\u4e8b\u5b9e\u5c42\u9762\u8bc6\u522b\u51b2\u7a81\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u81ea\u601d\u8003\u8fc7\u7a0b\uff0c\u8ba9\u6a21\u578b\u5728\u751f\u6210\u524d\u63a8\u7406\u5e76\u6574\u5408\u51b2\u7a81\u4e8b\u5b9e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFaithfulRAG\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FaithfulRAG\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u77e5\u8bc6\u51b2\u7a81\uff0c\u63d0\u5347\u4e86\u751f\u6210\u7ed3\u679c\u7684\u5fe0\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u77e5\u8bc6\u7ed3\u6784\u3002"}}
{"id": "2506.08793", "pdf": "https://arxiv.org/pdf/2506.08793", "abs": "https://arxiv.org/abs/2506.08793", "authors": ["Zhuoran Zheng"], "title": "A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory", "categories": ["cs.CV", "eess.IV"], "comment": "report", "summary": "This paper presents a novel partial differential equation (PDE) framework for\nsingle-image dehazing. By integrating the atmospheric scattering model with\nnonlocal regularization and dark channel prior, we propose the improved PDE: \\[\n-\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A) \\]\nwhere $D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$ is the edge-preserving\ndiffusion coefficient, $G(u)$ is the Gaussian convolution operator, and\n$\\lambda(t)$ is the adaptive regularization parameter based on transmission map\n$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\\Omega)$\nusing Lax-Milgram theorem, and implement an efficient fixed-point iteration\nscheme accelerated by PyTorch GPU computation. The experimental results\ndemonstrate that this method is a promising deghazing solution that can be\ngeneralized to the deep model paradigm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u5355\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u6c14\u6563\u5c04\u6a21\u578b\u3001\u975e\u5c40\u90e8\u6b63\u5219\u5316\u548c\u6697\u901a\u9053\u5148\u9a8c\uff0c\u6539\u8fdb\u4e86PDE\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5f31\u89e3\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u56fe\u50cf\u53bb\u96fe\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u7684PDE\u6846\u67b6\u63d0\u5347\u53bb\u96fe\u6548\u679c\u3002", "method": "\u7ed3\u5408\u5927\u6c14\u6563\u5c04\u6a21\u578b\u3001\u975e\u5c40\u90e8\u6b63\u5219\u5316\u548c\u6697\u901a\u9053\u5148\u9a8c\uff0c\u63d0\u51fa\u6539\u8fdb\u7684PDE\uff0c\u5e76\u91c7\u7528\u56fa\u5b9a\u70b9\u8fed\u4ee3\u52a0\u901f\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u53bb\u96fe\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5355\u56fe\u50cf\u53bb\u96fe\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u901a\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.08153", "pdf": "https://arxiv.org/pdf/2506.08153", "abs": "https://arxiv.org/abs/2506.08153", "authors": ["Renato Cordeiro Ferreira"], "title": "A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.8; I.2.0"], "comment": "4 pages, 3 figures (2 diagrams, 1 table), to be published in CAIN\n  2025", "summary": "How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper showcases the first step for\ncreating the metrics-based architectural model: an extension of a reference\narchitecture that can describe MLES to collect their metrics.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6807\u7684\u67b6\u6784\u6a21\u578b\u6765\u7ba1\u7406ML\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u652f\u6301\u67b6\u6784\u51b3\u7b56\u3002", "motivation": "\u63a2\u8ba8\u590d\u6742\u6027\u5bf9ML\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u7ba1\u7406\u590d\u6742\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u53c2\u8003\u67b6\u6784\u4ee5\u63cf\u8ff0ML\u7cfb\u7edf\u5e76\u6536\u96c6\u6307\u6807\u3002", "result": "\u5c55\u793a\u4e86\u521b\u5efa\u6307\u6807\u6a21\u578b\u7684\u7b2c\u4e00\u6b65\u3002", "conclusion": "\u4e3aML\u7cfb\u7edf\u7684\u521d\u59cb\u5316\u548c\u589e\u957f\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.08952", "pdf": "https://arxiv.org/pdf/2506.08952", "abs": "https://arxiv.org/abs/2506.08952", "authors": ["Clara Lachenmaier", "Judith Sieker", "Sina Zarrie\u00df"], "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint accepted at ACL Main Conference 2025", "summary": "Communication among humans relies on conversational grounding, allowing\ninterlocutors to reach mutual understanding even when they do not have perfect\nknowledge and must resolve discrepancies in each other's beliefs. This paper\ninvestigates how large language models (LLMs) manage common ground in cases\nwhere they (don't) possess knowledge, focusing on facts in the political domain\nwhere the risk of misinformation and grounding failure is high. We examine the\nability of LLMs to answer direct knowledge questions and loaded questions that\npresuppose misinformation. We evaluate whether loaded questions lead LLMs to\nengage in active grounding and correct false user beliefs, in connection to\ntheir level of knowledge and their political bias. Our findings highlight\nsignificant challenges in LLMs' ability to engage in grounding and reject false\nuser beliefs, raising concerns about their role in mitigating misinformation in\npolitical discourse.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u653f\u6cbb\u9886\u57df\u5982\u4f55\u5904\u7406\u5171\u540c\u57fa\u7840\uff08common ground\uff09\uff0c\u7279\u522b\u662f\u5728\uff08\u4e0d\uff09\u5177\u5907\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc4\u4f30\u5176\u56de\u7b54\u76f4\u63a5\u77e5\u8bc6\u95ee\u9898\u548c\u9884\u8bbe\u9519\u8bef\u4fe1\u606f\u7684\u8bf1\u5bfc\u6027\u95ee\u9898\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u653f\u6cbb\u9886\u57df\u4e2d\u5982\u4f55\u7ba1\u7406\u5171\u540c\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u9519\u8bef\u4fe1\u606f\u548c\u9ad8\u98ce\u9669\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u7ea0\u6b63\u7528\u6237\u9519\u8bef\u4fe1\u5ff5\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u5bf9\u76f4\u63a5\u77e5\u8bc6\u95ee\u9898\u548c\u8bf1\u5bfc\u6027\u95ee\u9898\u7684\u56de\u7b54\uff0c\u8bc4\u4f30\u5176\u662f\u5426\u80fd\u591f\u4e3b\u52a8\u7ea0\u6b63\u7528\u6237\u7684\u9519\u8bef\u4fe1\u5ff5\uff0c\u5e76\u7ed3\u5408\u5176\u77e5\u8bc6\u6c34\u5e73\u548c\u653f\u6cbb\u504f\u89c1\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u5171\u540c\u57fa\u7840\u548c\u7ea0\u6b63\u9519\u8bef\u4fe1\u5ff5\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5f15\u53d1\u5bf9\u5176\u5728\u653f\u6cbb\u8bdd\u8bed\u4e2d\u51cf\u5c11\u9519\u8bef\u4fe1\u606f\u4f5c\u7528\u7684\u62c5\u5fe7\u3002", "conclusion": "LLMs\u5728\u653f\u6cbb\u9886\u57df\u7684\u5171\u540c\u57fa\u7840\u7ba1\u7406\u548c\u9519\u8bef\u4fe1\u606f\u7ea0\u6b63\u80fd\u529b\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5e94\u5bf9\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2506.08796", "pdf": "https://arxiv.org/pdf/2506.08796", "abs": "https://arxiv.org/abs/2506.08796", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "AI": {"tldr": "Discretized-RF\u901a\u8fc7\u5c06\u76f4\u7ebf\u8def\u5f84\u79bb\u6563\u5316\u4e3a\u53ef\u53d8\u901f\u5ea6\u5b50\u8def\u5f84\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfRectified Flow\u7684\u591a\u6837\u6027\u548c\u591a\u5c3a\u5ea6\u566a\u58f0\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRectified Flow\u7684\u76f4\u7ebf\u8def\u5f84\u9650\u5236\u4e86\u91c7\u6837\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u4e14\u591a\u5c3a\u5ea6\u566a\u58f0\u5efa\u6a21\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDiscretized-RF\uff0c\u5c06\u8def\u5f84\u79bb\u6563\u5316\u4e3a\u52a8\u91cf\u573a\u5b50\u8def\u5f84\uff0c\u5e76\u5728\u901f\u5ea6\u4e0a\u5f15\u5165\u566a\u58f0\u4ee5\u6539\u53d8\u65b9\u5411\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\uff0c\u5e76\u6301\u7eed\u4ea7\u751f\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u7ed3\u679c\u3002", "conclusion": "Discretized-RF\u901a\u8fc7\u52a8\u91cf\u573a\u548c\u901f\u5ea6\u566a\u58f0\u7684\u5f15\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u591a\u6837\u6027\u548c\u566a\u58f0\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2506.08167", "pdf": "https://arxiv.org/pdf/2506.08167", "abs": "https://arxiv.org/abs/2506.08167", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "comment": null, "summary": "Federated Learning (FL) often suffers from severe performance degradation\nwhen faced with non-IID data, largely due to local classifier bias. Traditional\nremedies such as global model regularization or layer freezing either incur\nhigh computational costs or struggle to adapt to feature shifts. In this work,\nwe propose UniVarFL, a novel FL framework that emulates IID-like training\ndynamics directly at the client level, eliminating the need for global model\ndependency. UniVarFL leverages two complementary regularization strategies\nduring local training: Classifier Variance Regularization, which aligns\nclass-wise probability distributions with those expected under IID conditions,\neffectively mitigating local classifier bias; and Hyperspherical Uniformity\nRegularization, which encourages a uniform distribution of feature\nrepresentations across the hypersphere, thereby enhancing the model's ability\nto generalize under diverse data distributions. Extensive experiments on\nmultiple benchmark datasets demonstrate that UniVarFL outperforms existing\nmethods in accuracy, highlighting its potential as a highly scalable and\nefficient solution for real-world FL deployments, especially in\nresource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL", "AI": {"tldr": "UniVarFL\u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565\u89e3\u51b3\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u5168\u5c40\u6a21\u578b\u3002", "motivation": "\u975eIID\u6570\u636e\u5bfc\u81f4\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u4e0b\u964d\uff0c\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u6216\u9002\u5e94\u6027\u5dee\u3002", "method": "UniVarFL\u91c7\u7528\u5206\u7c7b\u5668\u65b9\u5dee\u6b63\u5219\u5316\u548c\u8d85\u7403\u9762\u5747\u5300\u6b63\u5219\u5316\uff0c\u6a21\u62dfIID\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cUniVarFL\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniVarFL\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.08966", "pdf": "https://arxiv.org/pdf/2506.08966", "abs": "https://arxiv.org/abs/2506.08966", "authors": ["Marek Kadl\u010d\u00edk", "Michal \u0160tef\u00e1nik", "Timothee Mickus", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers", "categories": ["cs.CL", "cs.LG", "cs.NE"], "comment": null, "summary": "Pretrained language models (LMs) are prone to arithmetic errors. Existing\nwork showed limited success in probing numeric values from models'\nrepresentations, indicating that these errors can be attributed to the inherent\nunreliability of distributionally learned embeddings in representing exact\nquantities. However, we observe that previous probing methods are inadequate\nfor the emergent structure of learned number embeddings with sinusoidal\npatterns.\n  In response, we propose a novel probing technique that decodes numeric values\nfrom input embeddings with near-perfect accuracy across a range of open-source\nLMs. This proves that after the sole pre-training, LMs represent numbers with\nremarkable precision. Finally, we find that the embeddings' preciseness judged\nby our probe's accuracy explains a large portion of LM's errors in elementary\narithmetic, and show that aligning the embeddings with the pattern discovered\nby our probe can mitigate these errors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a2\u6d4b\u6280\u672f\uff0c\u80fd\u591f\u4ece\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u4e2d\u9ad8\u7cbe\u5ea6\u89e3\u7801\u6570\u503c\uff0c\u8bc1\u660e\u6a21\u578b\u786e\u5b9e\u80fd\u7cbe\u786e\u8868\u793a\u6570\u5b57\uff0c\u5e76\u53d1\u73b0\u8fd9\u79cd\u7cbe\u786e\u6027\u4e0e\u7b97\u672f\u9519\u8bef\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u6d4b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6570\u503c\u8868\u793a\u65f6\u6548\u679c\u6709\u9650\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u5206\u5e03\u5b66\u4e60\u5d4c\u5165\u5728\u8868\u793a\u7cbe\u786e\u6570\u91cf\u65f6\u7684\u56fa\u6709\u4e0d\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a2\u6d4b\u6280\u672f\uff0c\u80fd\u591f\u89e3\u7801\u8f93\u5165\u5d4c\u5165\u4e2d\u7684\u6570\u503c\uff0c\u5e76\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u591a\u79cd\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u6570\u503c\u89e3\u7801\u7cbe\u5ea6\uff0c\u8bc1\u660e\u6a21\u578b\u80fd\u7cbe\u786e\u8868\u793a\u6570\u5b57\u3002\u6b64\u5916\uff0c\u5d4c\u5165\u7684\u7cbe\u786e\u6027\u4e0e\u7b97\u672f\u9519\u8bef\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u5d4c\u5165\u4ee5\u7b26\u5408\u63a2\u6d4b\u53d1\u73b0\u7684\u6a21\u5f0f\uff0c\u53ef\u4ee5\u51cf\u8f7b\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u7840\u7b97\u672f\u4e2d\u7684\u9519\u8bef\u3002"}}
{"id": "2506.08797", "pdf": "https://arxiv.org/pdf/2506.08797", "abs": "https://arxiv.org/abs/2506.08797", "authors": ["Ziyao Huang", "Zixiang Zhou", "Juan Cao", "Yifeng Ma", "Yi Chen", "Zejing Rao", "Zhiyong Xu", "Hongmei Wang", "Qin Lin", "Yuan Zhou", "Qinglin Lu", "Fan Tang"], "title": "HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation", "categories": ["cs.CV"], "comment": null, "summary": "To address key limitations in human-object interaction (HOI) video generation\n-- specifically the reliance on curated motion data, limited generalization to\nnovel objects/scenarios, and restricted accessibility -- we introduce\nHunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.\nHunyuanVideo-HOMA enhances controllability and reduces dependency on precise\ninputs through sparse, decoupled motion guidance. It encodes appearance and\nmotion signals into the dual input space of a multimodal diffusion transformer\n(MMDiT), fusing them within a shared context space to synthesize temporally\nconsistent and physically plausible interactions. To optimize training, we\nintegrate a parameter-space HOI adapter initialized from pretrained MMDiT\nweights, preserving prior knowledge while enabling efficient adaptation, and a\nfacial cross-attention adapter for anatomically accurate audio-driven lip\nsynchronization. Extensive experiments confirm state-of-the-art performance in\ninteraction naturalness and generalization under weak supervision. Finally,\nHunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and\ninteractive object manipulation, supported by a user-friendly demo interface.\nThe project page is at https://anonymous.4open.science/w/homa-page-0FBE/.", "AI": {"tldr": "HunyuanVideo-HOMA\u662f\u4e00\u4e2a\u5f31\u6761\u4ef6\u591a\u6a21\u6001\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3HOI\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5982\u4f9d\u8d56\u7cbe\u9009\u8fd0\u52a8\u6570\u636e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3HOI\u89c6\u9891\u751f\u6210\u4e2d\u5bf9\u7cbe\u9009\u8fd0\u52a8\u6570\u636e\u7684\u4f9d\u8d56\u3001\u5bf9\u65b0\u5bf9\u8c61/\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u53ef\u8bbf\u95ee\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u89e3\u8026\u7684\u8fd0\u52a8\u5f15\u5bfc\u589e\u5f3a\u53ef\u63a7\u6027\uff0c\u5c06\u5916\u89c2\u548c\u8fd0\u52a8\u4fe1\u53f7\u7f16\u7801\u5230\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MMDiT\uff09\u7684\u53cc\u8f93\u5165\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5728\u5171\u4eab\u4e0a\u4e0b\u6587\u7a7a\u95f4\u4e2d\u878d\u5408\u5b83\u4eec\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHunyuanVideo-HOMA\u5728\u5f31\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u4ea4\u4e92\u81ea\u7136\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6587\u672c\u6761\u4ef6\u751f\u6210\u548c\u4ea4\u4e92\u5f0f\u5bf9\u8c61\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u591a\u529f\u80fd\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u6237\u53cb\u597d\u7684\u6f14\u793a\u754c\u9762\u3002"}}
{"id": "2506.08171", "pdf": "https://arxiv.org/pdf/2506.08171", "abs": "https://arxiv.org/abs/2506.08171", "authors": ["Daniel Koh", "Yannic Noller", "Corina S. Pasareanu", "Adrians Skapars", "Youcheng Sun"], "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been successfully applied to a variety of\ncoding tasks, including code generation, completion, and repair. However, more\ncomplex symbolic reasoning tasks remain largely unexplored by LLMs. This paper\ninvestigates the capacity of LLMs to reason about worst-case executions in\nprograms through symbolic constraints analysis, aiming to connect LLMs and\nsymbolic reasoning approaches. Specifically, we define and address the problem\nof worst-case symbolic constraints analysis as a measure to assess the\ncomprehension of LLMs. We evaluate the performance of existing LLMs on this\nnovel task and further improve their capabilities through symbolic\nreasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)\nconstraint solving and supported by a specially designed dataset of symbolic\nconstraints. Experimental results show that our solver-aligned model,\nWARP-1.0-3B, consistently surpasses size-matched and even much larger\nbaselines, demonstrating that a 3B LLM can recover the very constraints that\npin down an algorithm's worst-case behaviour through reinforcement learning\nmethods. These findings suggest that LLMs are capable of engaging in deeper\nsymbolic reasoning, supporting a closer integration between neural\nnetwork-based learning and formal methods for rigorous program analysis.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7a0b\u5e8f\u6700\u574f\u60c5\u51b5\u6267\u884c\u4e2d\u7684\u7b26\u53f7\u7ea6\u675f\u5206\u6790\u80fd\u529b\uff0c\u901a\u8fc7SMT\u7ea6\u675f\u6c42\u89e3\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u590d\u6742\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u4e86\u6700\u574f\u60c5\u51b5\u7b26\u53f7\u7ea6\u675f\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7SMT\u7ea6\u675f\u6c42\u89e3\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u7b26\u53f7\u7ea6\u675f\u6570\u636e\u96c6\u5bf9LLMs\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3B\u89c4\u6a21\u7684WARP-1.0-3B\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "LLMs\u5177\u5907\u6df1\u5165\u7b26\u53f7\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u652f\u6301\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e0e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u7d27\u5bc6\u7ed3\u5408\u3002"}}
{"id": "2506.08972", "pdf": "https://arxiv.org/pdf/2506.08972", "abs": "https://arxiv.org/abs/2506.08972", "authors": ["Yuan Guo", "Tingjia Miao", "Zheng Wu", "Pengzhou Cheng", "Ming Zhou", "Zhuosheng Zhang"], "title": "Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System", "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents powered by multimodal large language models have been\ndeveloped to facilitate task execution on mobile devices. However, prior work\nhas predominantly focused on atomic tasks -- such as shot-chain execution tasks\nand single-screen grounding tasks -- while overlooking the generalization to\ncompositional tasks, which are indispensable for real-world applications. This\nwork introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile\nagents on three categories of compositional operations: Simple Concatenation,\nContext Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in\n20 fully controllable local utility app environments, as well as 30 online\nChinese and English service apps. It comprises 100 interactive task templates\nwith an average optimal step count of 14.05. Experimental results across a\nrange of mobile agents with agentic workflow or agent-as-a-model show that\nUI-NEXUS presents significant challenges. Specifically, existing agents\ngenerally struggle to balance performance and efficiency, exhibiting\nrepresentative failure modes such as under-execution, over-execution, and\nattention drift, causing visible atomic-to-compositional generalization gap.\nInspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient\nscheduling system to tackle compositional mobile tasks. AGENT-NEXUS\nextrapolates the abilities of existing mobile agents by dynamically decomposing\nlong-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS\nachieves 24% to 40% task success rate improvement for existing mobile agents on\ncompositional operation tasks within the UI-NEXUS benchmark without\nsignificantly sacrificing inference overhead. The demo video, dataset, and code\nare available on the project page at https://ui-nexus.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UI-NEXUS\u57fa\u51c6\u6d4b\u8bd5\u548cAGENT-NEXUS\u8c03\u5ea6\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u79fb\u52a8\u4ee3\u7406\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u539f\u5b50\u4efb\u52a1\uff0c\u800c\u5ffd\u7565\u4e86\u7ec4\u5408\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165UI-NEXUS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u7c7b\u7ec4\u5408\u64cd\u4f5c\uff0c\u5e76\u63d0\u51faAGENT-NEXUS\u8c03\u5ea6\u7cfb\u7edf\u52a8\u6001\u5206\u89e3\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u4ee3\u7406\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0cAGENT-NEXUS\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "AGENT-NEXUS\u6709\u6548\u63d0\u5347\u4e86\u79fb\u52a8\u4ee3\u7406\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08809", "pdf": "https://arxiv.org/pdf/2506.08809", "abs": "https://arxiv.org/abs/2506.08809", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "AI": {"tldr": "HiSin\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u6b63\u5f26\u56fe\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u8fa8\u7387\u5f15\u5bfc\u7684\u6e10\u8fdb\u63a8\u7406\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u6b63\u5f26\u56fe\u4fee\u590d\u5bf9CT\u91cd\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u56e0\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u96be\u4ee5\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u3002", "method": "HiSin\u91c7\u7528\u5206\u8fa8\u7387\u5f15\u5bfc\u7684\u6e10\u8fdb\u63a8\u7406\uff0c\u5148\u5728\u4f4e\u5206\u8fa8\u7387\u63d0\u53d6\u5168\u5c40\u7ed3\u6784\uff0c\u518d\u5728\u9ad8\u5206\u8fa8\u7387\u5904\u7406\u5c0f\u8865\u4e01\uff0c\u5e76\u7ed3\u5408\u9891\u7387\u611f\u77e5\u8865\u4e01\u8df3\u8fc7\u548c\u7ed3\u6784\u81ea\u9002\u5e94\u6b65\u9aa4\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiSin\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1131.25%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1118.15%\uff0c\u4e14\u4fee\u590d\u7cbe\u5ea6\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "HiSin\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u9ad8\u5206\u8fa8\u7387\u6b63\u5f26\u56fe\u4fee\u590d\u65b9\u6cd5\u3002"}}
{"id": "2506.08173", "pdf": "https://arxiv.org/pdf/2506.08173", "abs": "https://arxiv.org/abs/2506.08173", "authors": ["Nguyen Phu Vinh", "Anh Chung Hoang", "Chris Ngo", "Truong-Son Hy"], "title": "Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong capabilities in code\ngeneration and comprehension, yet their application to complex software\nengineering tasks often suffers from low precision and limited\ninterpretability. We present Repeton, a fully open-source framework that\nleverages LLMs for precise and automated code manipulation in real-world Git\nrepositories. Rather than generating holistic fixes, Repeton operates through a\nstructured patch-and-test pipeline: it iteratively diagnoses issues, proposes\ncode changes, and validates each patch through automated testing. This stepwise\nprocess is guided by lightweight heuristics and development tools, avoiding\nreliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite\nbenchmark, our method shows good performance compared to RAG-based methods in\nboth patch validity and interpretability. By decomposing software engineering\ntasks into modular, verifiable stages, Repeton provides a practical path toward\nscalable and transparent autonomous debugging.", "AI": {"tldr": "Repeton\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5229\u7528LLMs\u901a\u8fc7\u7ed3\u6784\u5316\u8865\u4e01\u548c\u6d4b\u8bd5\u6d41\u7a0b\u5b9e\u73b0\u7cbe\u786e\u7684\u4ee3\u7801\u64cd\u4f5c\uff0c\u63d0\u5347\u4ee3\u7801\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "LLMs\u5728\u4ee3\u7801\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7cbe\u5ea6\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "Repeton\u91c7\u7528\u5206\u6b65\u8865\u4e01\u548c\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u5f00\u53d1\u5de5\u5177\uff0c\u907f\u514d\u4f9d\u8d56\u5d4c\u5165\u68c0\u7d22\u7cfb\u7edf\u3002", "result": "\u5728SWE-bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRepeton\u5728\u8865\u4e01\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8eRAG\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5757\u5316\u548c\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u5206\u89e3\uff0cRepeton\u4e3a\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u81ea\u4e3b\u8c03\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.08981", "pdf": "https://arxiv.org/pdf/2506.08981", "abs": "https://arxiv.org/abs/2506.08981", "authors": ["Satu Hopponen", "Tomi Kinnunen", "Alexandre Nikolaev", "Rosa Gonz\u00e1lez Hautam\u00e4ki", "Lauri Tavi", "Einar Meister"], "title": "FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents", "categories": ["cs.CL"], "comment": "Accepted in Interspeech 2025", "summary": "We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of\nElectromagnetic Articulography) corpus. It consists of 18 bilingual speakers,\nwho produced speech in their native language (L1), second language (L2), and\nimitated L2 (fake foreign accent). The new corpus enables research into\nlanguage variability from phonetic and technological points of view.\nAccordingly, we include two preliminary case studies to demonstrate both\nperspectives. The first case study explores the impact of L2 and imitated L2 on\nthe performance of an automatic speaker verification system, while the second\nillustrates the articulatory patterns of one speaker in L1, L2, and a fake\naccent.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86FROST-EMA\u8bed\u6599\u5e93\uff0c\u5305\u542b18\u540d\u53cc\u8bed\u8005\u7684\u6bcd\u8bed\u3001\u7b2c\u4e8c\u8bed\u8a00\u53ca\u6a21\u4eff\u7b2c\u4e8c\u8bed\u8a00\u7684\u8bed\u97f3\u6570\u636e\uff0c\u652f\u6301\u8bed\u97f3\u548c\u6280\u672f\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u53d8\u5f02\u6027\u5728\u8bed\u97f3\u548c\u6280\u672f\u5c42\u9762\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efaFROST-EMA\u8bed\u6599\u5e93\uff0c\u5e76\u8fdb\u884c\u4e24\u9879\u521d\u6b65\u6848\u4f8b\u7814\u7a76\uff1a\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8868\u73b0\u548c\u53d1\u97f3\u6a21\u5f0f\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86\u7b2c\u4e8c\u8bed\u8a00\u548c\u6a21\u4eff\u7b2c\u4e8c\u8bed\u8a00\u5bf9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u53d1\u97f3\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "conclusion": "FROST-EMA\u8bed\u6599\u5e93\u4e3a\u8bed\u8a00\u53d8\u5f02\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.08817", "pdf": "https://arxiv.org/pdf/2506.08817", "abs": "https://arxiv.org/abs/2506.08817", "authors": ["Shuyi Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Pengwei Wang", "Zhongyuan Wang", "Hongxuan Ma", "Shanghang Zhang"], "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ .", "AI": {"tldr": "Video-CoT\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u65b9\u6cd5\u63d0\u5347\u89c6\u9891\u5185\u5bb9\u7406\u89e3\u7684\u65f6\u7a7a\u7ec6\u8282\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u89c6\u9891\u5206\u6790\u7684\u65f6\u7a7a\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5f15\u5165Video-CoT\u6570\u636e\u96c6\uff0c\u5305\u542b192,000\u4e2a\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u95ee\u7b54\u5bf9\u548c23,000\u4e2a\u9ad8\u8d28\u91cfCoT\u6807\u6ce8\u6837\u672c\uff0c\u5e76\u63d0\u4f9b\u57fa\u51c6\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u65f6\u7a7a\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "Video-CoT\u4e3a\u591a\u5a92\u4f53\u7406\u89e3\u548c\u667a\u80fd\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u5206\u6790\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.08986", "pdf": "https://arxiv.org/pdf/2506.08986", "abs": "https://arxiv.org/abs/2506.08986", "authors": ["Yuejiao Wang", "Xianmin Gong", "Xixin Wu", "Patrick Wong", "Hoi-lam Helene Fung", "Man Wai Mak", "Helen Meng"], "title": "Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder", "categories": ["cs.CL"], "comment": "5 pages,3 figures, accepted by ISCSLP 2024", "summary": "Early detection is crucial for timely intervention aimed at preventing and\nslowing the progression of neurocognitive disorder (NCD), a common and\nsignificant health problem among the aging population. Recent evidence has\nsuggested that language-related functional magnetic resonance imaging (fMRI)\nmay be a promising approach for detecting cognitive decline and early NCD. In\nthis paper, we proposed a novel, naturalistic language-related fMRI task for\nthis purpose. We examined the effectiveness of this task among 97 non-demented\nChinese older adults from Hong Kong. The results showed that machine-learning\nclassification models based on fMRI features extracted from the task and\ndemographics (age, gender, and education year) achieved an average area under\nthe curve of 0.86 when classifying participants' cognitive status (labeled as\nNORMAL vs DECLINE based on their scores on a standard neurcognitive test).\nFeature localization revealed that the fMRI features most frequently selected\nby the data-driven approach came primarily from brain regions associated with\nlanguage processing, such as the superior temporal gyrus, middle temporal\ngyrus, and right cerebellum. The study demonstrated the potential of the\nnaturalistic language-related fMRI task for early detection of aging-related\ncognitive decline and NCD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684fMRI\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u8001\u5e74\u4eba\u8ba4\u77e5\u8870\u9000\u548c\u795e\u7ecf\u8ba4\u77e5\u969c\u788d\uff08NCD\uff09\uff0c\u5e76\u572897\u540d\u975e\u75f4\u5446\u4e2d\u56fd\u8001\u5e74\u4eba\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u5bf9\u9884\u9632\u548c\u5ef6\u7f13\u795e\u7ecf\u8ba4\u77e5\u969c\u788d\uff08NCD\uff09\u7684\u8fdb\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u8bed\u8a00\u76f8\u5173\u7684fMRI\u53ef\u80fd\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684fMRI\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\uff08\u5e74\u9f84\u3001\u6027\u522b\u3001\u6559\u80b2\u5e74\u9650\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u7684AUC\u8fbe\u52300.86\uff0c\u7279\u5f81\u5b9a\u4f4d\u663e\u793a\u5173\u952e\u8111\u533a\u4e0e\u8bed\u8a00\u5904\u7406\u76f8\u5173\uff08\u5982\u989e\u4e0a\u56de\u3001\u989e\u4e2d\u56de\u3001\u53f3\u5c0f\u8111\uff09\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684fMRI\u5728\u65e9\u671f\u68c0\u6d4b\u8ba4\u77e5\u8870\u9000\u548cNCD\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.08835", "pdf": "https://arxiv.org/pdf/2506.08835", "abs": "https://arxiv.org/abs/2506.08835", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Sta\u0144czak", "Aishwarya Agrawal"], "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies.", "AI": {"tldr": "\u7814\u7a76\u91cf\u5316\u4e86\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u6587\u5316\u8868\u73b0\u4e0a\u7684\u4e0d\u8db3\uff0c\u53d1\u73b0\u5176\u672a\u80fd\u6ee1\u8db3\u663e\u6027\u548c\u9690\u6027\u6587\u5316\u671f\u671b\uff0c\u5e76\u63d0\u51fa\u65b0\u57fa\u51c6CulturalFrames\u3002", "motivation": "T2I\u6a21\u578b\u5728\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u4e2d\u7684\u666e\u53ca\u5f15\u53d1\u4e86\u5bf9\u6587\u5316\u8868\u73b0\u51c6\u786e\u6027\u7684\u62c5\u5fe7\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u5176\u6587\u5316\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u5f15\u5165CulturalFrames\u57fa\u51c6\uff0c\u6db5\u76d610\u4e2a\u56fd\u5bb6\u30015\u4e2a\u793e\u4f1a\u6587\u5316\u9886\u57df\uff0c\u901a\u8fc7983\u4e2a\u63d0\u793a\u30013637\u5f20\u56fe\u50cf\u548c10k+\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "T2I\u6a21\u578b\u5e73\u574744%\u672a\u6ee1\u8db3\u6587\u5316\u671f\u671b\uff0c\u663e\u6027\u671f\u671b\u5931\u8d25\u738768%\uff0c\u9690\u6027\u671f\u671b49%\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u5dee\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86T2I\u6a21\u578b\u7684\u6587\u5316\u8868\u73b0\u7f3a\u9677\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u6587\u5316\u610f\u8bc6\u7684\u6a21\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.08999", "pdf": "https://arxiv.org/pdf/2506.08999", "abs": "https://arxiv.org/abs/2506.08999", "authors": ["Theo Zhang", "Madurya Suresh", "Anne S. Warlaumont", "Kasia Hitczenko", "Alejandrina Cristia", "Margaret Cychosz"], "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in Interspeech 2025. 5 pages, 2 figures. For\n  associated Github repository, see\n  https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM", "summary": "Speech technology systems struggle with many downstream tasks for child\nspeech due to small training corpora and the difficulties that child speech\npose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer\nmodels to address a fundamental classification task: identifying child\nvocalizations. Unlike previous corpora, our dataset captures maximally\necologically-valid child vocalizations across an unprecedented sample,\ncomprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,\nPapua New Guinea, Solomon Islands, and France. The dataset contains 242,004\nlabeled vocalizations, magnitudes larger than previous work. Models were\ntrained to distinguish between cry, laughter, mature (consonant+vowel), and\nimmature speech (just consonant or vowel). Models trained on the dataset\noutperform state-of-the-art models trained on previous datasets, achieved\nclassification accuracy comparable to humans, and were robust across rural and\nurban settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpeechMaturity\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u513f\u7ae5\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u548c\u591a\u5730\u533a\u6837\u672c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u513f\u7ae5\u8bed\u97f3\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u5904\u7406\uff0c\u5bfc\u81f4\u8bed\u97f3\u6280\u672f\u7cfb\u7edf\u5728\u513f\u7ae5\u8bed\u97f3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528SpeechMaturity\u6570\u636e\u96c6\u8bad\u7ec3\u6700\u5148\u8fdb\u7684Transformer\u6a21\u578b\uff0c\u5206\u7c7b\u513f\u7ae5\u53d1\u58f0\uff08\u5982\u54ed\u58f0\u3001\u7b11\u58f0\u3001\u6210\u719f\u548c\u4e0d\u6210\u719f\u8bed\u97f3\uff09\u3002", "result": "\u6a21\u578b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u5728\u57ce\u4e61\u73af\u5883\u4e2d\u5747\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "SpeechMaturity\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u513f\u7ae5\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08849", "pdf": "https://arxiv.org/pdf/2506.08849", "abs": "https://arxiv.org/abs/2506.08849", "authors": ["Jingguo Qu", "Xinyang Han", "Tonghuan Xiao", "Jia Ai", "Juan Wu", "Tong Zhao", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Ying\u0131nst"], "title": "Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Medical ultrasonography is an essential imaging technique for examining\nsuperficial organs and tissues, including lymph nodes, breast, and thyroid. It\nemploys high-frequency ultrasound waves to generate detailed images of the\ninternal structures of the human body. However, manually contouring regions of\ninterest in these images is a labor-intensive task that demands expertise and\noften results in inconsistent interpretations among individuals.\nVision-language foundation models, which have excelled in various computer\nvision applications, present new opportunities for enhancing ultrasound image\nanalysis. Yet, their performance is hindered by the significant differences\nbetween natural and medical imaging domains. This research seeks to overcome\nthese challenges by developing domain adaptation methods for vision-language\nfoundation models. In this study, we explore the fine-tuning pipeline for\nvision-language foundation models by utilizing large language model as text\nrefiner with special-designed adaptation strategies and task-driven heads. Our\napproach has been extensively evaluated on six ultrasound datasets and two\ntasks: segmentation and classification. The experimental results show that our\nmethod can effectively improve the performance of vision-language foundation\nmodels for ultrasound image analysis, and outperform the existing\nstate-of-the-art vision-language and pure foundation models. The source code of\nthis study is available at\n\\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u7ba1\u9053\u548c\u4efb\u52a1\u9a71\u52a8\u5934\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u624b\u52a8\u6807\u6ce8\u8d39\u65f6\u4e14\u4e0d\u4e00\u81f4\uff0c\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u8868\u73b0\u53d7\u9650\uff0c\u9700\u514b\u670d\u9886\u57df\u5dee\u5f02\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6587\u672c\u7ec6\u5316\u5668\uff0c\u7ed3\u5408\u7279\u6b8a\u8bbe\u8ba1\u7684\u9002\u5e94\u7b56\u7565\u548c\u4efb\u52a1\u9a71\u52a8\u5934\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u516d\u4e2a\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u548c\u7eaf\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09003", "pdf": "https://arxiv.org/pdf/2506.09003", "abs": "https://arxiv.org/abs/2506.09003", "authors": ["Lei Zhang", "Jiaxi Yang", "Min Yang", "Jian Yang", "Mouxiang Chen", "Jiajun Zhang", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner", "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).", "AI": {"tldr": "SWE-Flow\u662f\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u81ea\u52a8\u63a8\u65ad\u589e\u91cf\u5f00\u53d1\u6b65\u9aa4\uff0c\u751f\u6210\u7ed3\u6784\u5316\u5f00\u53d1\u8ba1\u5212\uff0c\u5e76\u521b\u5efa\u4e86SWE-Flow-Eval\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u6570\u636e\u4f9d\u8d56\u4eba\u5de5\u63d0\u4ea4\u7684\u95ee\u9898\uff0c\u800cSWE-Flow\u76f4\u63a5\u4ece\u5355\u5143\u6d4b\u8bd5\u4e2d\u63a8\u65ad\u5f00\u53d1\u6b65\u9aa4\uff0c\u66f4\u9ad8\u6548\u5730\u6355\u6349\u9700\u6c42\u3002", "method": "\u6784\u5efa\u8fd0\u884c\u65f6\u4f9d\u8d56\u56fe\uff08RDG\uff09\u4ee5\u7cbe\u786e\u6355\u83b7\u51fd\u6570\u4ea4\u4e92\uff0c\u751f\u6210\u5206\u6b65\u5f00\u53d1\u8ba1\u5212\uff0c\u5305\u62ec\u90e8\u5206\u4ee3\u7801\u5e93\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u4ee3\u7801\u4fee\u6539\u3002", "result": "\u4eceGitHub\u9879\u76ee\u4e2d\u751f\u6210\u4e8616,061\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u548c2,020\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eTDD\u7684\u7f16\u7801\u6027\u80fd\u3002", "conclusion": "SWE-Flow\u4e3aTDD\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.08854", "pdf": "https://arxiv.org/pdf/2506.08854", "abs": "https://arxiv.org/abs/2506.08854", "authors": ["Junzhuo Liu", "Markus Eckstein", "Zhixiang Wang", "Friedrich Feuerhake", "Dorit Merhof"], "title": "Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 7 figures", "summary": "Spatial transcriptomics is a technology that captures gene expression levels\nat different spatial locations, widely used in tumor microenvironment analysis\nand molecular profiling of histopathology, providing valuable insights into\nresolving gene expression and clinical diagnosis of cancer. Due to the high\ncost of data acquisition, large-scale spatial transcriptomics data remain\nchallenging to obtain. In this study, we develop a contrastive learning-based\ndeep learning method to predict spatially resolved gene expression from\nwhole-slide images. Evaluation across six different disease datasets\ndemonstrates that, compared to existing studies, our method improves Pearson\nCorrelation Coefficient (PCC) in the prediction of highly expressed genes,\nhighly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%\nrespectively. Further analysis indicates that our method preserves gene-gene\ncorrelations and applies to datasets with limited samples. Additionally, our\nmethod exhibits potential in cancer tissue localization based on biomarker\nexpression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5168\u5207\u7247\u56fe\u50cf\u9884\u6d4b\u7a7a\u95f4\u5206\u8fa8\u57fa\u56e0\u8868\u8fbe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96be\u4ee5\u83b7\u5f97\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u4ece\u73b0\u6709\u56fe\u50cf\u6570\u636e\u4e2d\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u5168\u5207\u7247\u56fe\u50cf\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\uff0c\u5e76\u5728\u516d\u4e2a\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u4e0e\u73b0\u6709\u7814\u7a76\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u8868\u8fbe\u57fa\u56e0\u3001\u9ad8\u53d8\u57fa\u56e0\u548c\u6807\u8bb0\u57fa\u56e0\u7684\u9884\u6d4b\u4e2d\uff0c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5206\u522b\u63d0\u9ad8\u4e866.27%\u30016.11%\u548c11.26%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4fdd\u7559\u4e86\u57fa\u56e0\u95f4\u76f8\u5173\u6027\uff0c\u8fd8\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u6570\u636e\u96c6\uff0c\u5e76\u5728\u57fa\u4e8e\u751f\u7269\u6807\u5fd7\u7269\u8868\u8fbe\u7684\u764c\u75c7\u7ec4\u7ec7\u5b9a\u4f4d\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\u3002"}}
{"id": "2506.08228", "pdf": "https://arxiv.org/pdf/2506.08228", "abs": "https://arxiv.org/abs/2506.08228", "authors": ["Mustafa Baniodeh", "Kratarth Goel", "Scott Ettinger", "Carlos Fuertes", "Ari Seff", "Tim Shen", "Cole Gulino", "Chenjie Yang", "Ghassen Jerfel", "Dokook Choe", "Rui Wang", "Vinutha Kallem", "Sergio Casas", "Rami Al-Rfou", "Benjamin Sapp", "Dragomir Anguelov"], "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "We study the empirical scaling laws of a family of encoder-decoder\nautoregressive transformer models on the task of joint motion forecasting and\nplanning in the autonomous driving domain. Using a 500 thousand hours driving\ndataset, we demonstrate that, similar to language modeling, model performance\nimproves as a power-law function of the total compute budget, and we observe a\nstrong correlation between model training loss and model evaluation metrics.\nMost interestingly, closed-loop metrics also improve with scaling, which has\nimportant implications for the suitability of open-loop metrics for model\ndevelopment and hill climbing. We also study the optimal scaling of the number\nof transformer parameters and the training data size for a training\ncompute-optimal model. We find that as the training compute budget grows,\noptimal scaling requires increasing the model size 1.5x as fast as the dataset\nsize. We also study inference-time compute scaling, where we observe that\nsampling and clustering the output of smaller models makes them competitive\nwith larger models, up to a crossover point beyond which a larger models\nbecomes more inference-compute efficient. Overall, our experimental results\ndemonstrate that optimizing the training and inference-time scaling properties\nof motion forecasting and planning models is a key lever for improving their\nperformance to address a wide variety of driving scenarios. Finally, we briefly\nstudy the utility of training on general logged driving data of other agents to\nimprove the performance of the ego-agent, an important research area to address\nthe scarcity of robotics data for large capacity models training.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u81ea\u56de\u5f52\u53d8\u6362\u5668\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u968f\u8ba1\u7b97\u9884\u7b97\u589e\u957f\u7684\u5e42\u5f8b\u5173\u7cfb\uff0c\u53d1\u73b0\u95ed\u73af\u6307\u6807\u4e5f\u968f\u89c4\u6a21\u63d0\u5347\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u53c2\u6570\u4e0e\u6570\u636e\u89c4\u6a21\u7684\u4f18\u5316\u6bd4\u4f8b\u3002", "motivation": "\u63a2\u7d22\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u6a21\u578b\u7684\u6027\u80fd\u5982\u4f55\u968f\u8ba1\u7b97\u8d44\u6e90\u589e\u52a0\u800c\u63d0\u5347\uff0c\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "method": "\u4f7f\u752850\u4e07\u5c0f\u65f6\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u5206\u6790\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u9884\u7b97\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u6a21\u578b\u53c2\u6570\u548c\u6570\u636e\u89c4\u6a21\u7684\u4f18\u5316\u6bd4\u4f8b\uff0c\u5e76\u63a2\u8ba8\u63a8\u7406\u65f6\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6a21\u578b\u6027\u80fd\u968f\u8ba1\u7b97\u9884\u7b97\u5e42\u5f8b\u589e\u957f\uff0c\u95ed\u73af\u6307\u6807\u63d0\u5347\uff1b\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u9700\u6bd4\u6570\u636e\u89c4\u6a21\u589e\u957f\u5feb1.5\u500d\uff1b\u5c0f\u6a21\u578b\u901a\u8fc7\u91c7\u6837\u548c\u805a\u7c7b\u5728\u63a8\u7406\u65f6\u53ef\u4e0e\u66f4\u5927\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u7684\u89c4\u6a21\u7279\u6027\u662f\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\uff0c\u540c\u65f6\u5229\u7528\u5176\u4ed6\u4ee3\u7406\u7684\u9a7e\u9a76\u6570\u636e\u53ef\u7f13\u89e3\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2506.09009", "pdf": "https://arxiv.org/pdf/2506.09009", "abs": "https://arxiv.org/abs/2506.09009", "authors": ["Hakyung Sung", "Gyu-Ho Shin", "Chanyoung Lee", "You Kyung Sung", "Boo Kyung Jung"], "title": "UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags", "categories": ["cs.CL"], "comment": null, "summary": "The present study extends recent work on Universal Dependencies annotations\nfor second-language (L2) Korean by introducing a semi-automated framework that\nidentifies morphosyntactic constructions from XPOS sequences and aligns those\nconstructions with corresponding UPOS categories. We also broaden the existing\nL2-Korean corpus by annotating 2,998 new sentences from argumentative essays.\nTo evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean\nmorphosyntactic analysis models on datasets both with and without these\nalignments, using two NLP toolkits. Our results indicate that the aligned\ndataset not only improves consistency across annotation layers but also\nenhances morphosyntactic tagging and dependency-parsing accuracy, particularly\nin cases of limited annotated data.", "AI": {"tldr": "\u7814\u7a76\u6269\u5c55\u4e86L2\u97e9\u8bed\u7684\u901a\u7528\u4f9d\u5b58\u6807\u6ce8\u5de5\u4f5c\uff0c\u63d0\u51fa\u534a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7XPOS\u5e8f\u5217\u8bc6\u522b\u5f62\u6001\u53e5\u6cd5\u7ed3\u6784\u5e76\u4e0eUPOS\u5bf9\u9f50\uff0c\u6269\u5145\u4e86\u8bed\u6599\u5e93\u3002\u5b9e\u9a8c\u8868\u660e\u5bf9\u9f50\u6570\u636e\u63d0\u5347\u4e86\u6807\u6ce8\u4e00\u81f4\u6027\u548c\u5206\u6790\u51c6\u786e\u6027\u3002", "motivation": "\u6269\u5c55L2\u97e9\u8bed\u7684\u6807\u6ce8\u5de5\u4f5c\uff0c\u89e3\u51b3\u5f62\u6001\u53e5\u6cd5\u7ed3\u6784\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u6807\u6ce8\u548c\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u534a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7XPOS\u5e8f\u5217\u8bc6\u522b\u5f62\u6001\u53e5\u6cd5\u7ed3\u6784\u5e76\u4e0eUPOS\u5bf9\u9f50\uff1b\u6269\u5145\u8bed\u6599\u5e93\uff1b\u4f7f\u7528\u4e24\u79cdNLP\u5de5\u5177\u5305\u8bc4\u4f30\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u5bf9\u9f50\u6570\u636e\u96c6\u63d0\u5347\u4e86\u6807\u6ce8\u4e00\u81f4\u6027\u548c\u5f62\u6001\u53e5\u6cd5\u6807\u6ce8\u3001\u4f9d\u5b58\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "XPOS-UPOS\u5bf9\u9f50\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86L2\u97e9\u8bed\u6807\u6ce8\u548c\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.08862", "pdf": "https://arxiv.org/pdf/2506.08862", "abs": "https://arxiv.org/abs/2506.08862", "authors": ["Zike Wu", "Qi Yan", "Xuanyu Yi", "Lele Wang", "Renjie Liao"], "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.", "AI": {"tldr": "StreamSplat\u662f\u4e00\u4e2a\u5b9e\u65f6\u5904\u7406\u672a\u6821\u51c6\u89c6\u9891\u6d41\u5e76\u751f\u6210\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u3001\u52a8\u6001\u5efa\u6a21\u548c\u957f\u671f\u7a33\u5b9a\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u672a\u6821\u51c6\u8f93\u5165\u3001\u5b9e\u65f6\u52a8\u6001\u5efa\u6a21\u548c\u957f\u671f\u7a33\u5b9a\u6027\uff0cStreamSplat\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9759\u6001\u7f16\u7801\u5668\u4e2d\u7684\u6982\u7387\u91c7\u6837\u673a\u5236\u548c\u52a8\u6001\u89e3\u7801\u5668\u4e2d\u7684\u53cc\u5411\u53d8\u5f62\u573a\uff0c\u5b9e\u73b0\u9ad8\u6548\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u5728\u9759\u6001\u548c\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStreamSplat\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u52a8\u6001\u5efa\u6a21\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u4efb\u610f\u957f\u5ea6\u89c6\u9891\u6d41\u7684\u5728\u7ebf\u91cd\u5efa\u3002", "conclusion": "StreamSplat\u662f\u9996\u4e2a\u5b8c\u5168\u524d\u9988\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u672a\u6821\u51c6\u89c6\u9891\u6d41\u5e76\u751f\u6210\u52a8\u60013D\u573a\u666f\u8868\u793a\u3002"}}
{"id": "2506.08231", "pdf": "https://arxiv.org/pdf/2506.08231", "abs": "https://arxiv.org/abs/2506.08231", "authors": ["Melissa Estevez", "Nisha Singh", "Lauren Dyson", "Blythe Adamson", "Qianyu Yuan", "Megan W. Hildner", "Erin Fidyk", "Olive Mbah", "Farhad Khan", "Kathi Seidl-Rathkopf", "Aaron B. Cohen"], "title": "Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": "18 pages, 3 tables, 1 figure", "summary": "Large language models (LLMs) are increasingly used to extract clinical data\nfrom electronic health records (EHRs), offering significant improvements in\nscalability and efficiency for real-world data (RWD) curation in oncology.\nHowever, the adoption of LLMs introduces new challenges in ensuring the\nreliability, accuracy, and fairness of extracted data, which are essential for\nresearch, regulatory, and clinical applications. Existing quality assurance\nframeworks for RWD and artificial intelligence do not fully address the unique\nerror modes and complexities associated with LLM-extracted data. In this paper,\nwe propose a comprehensive framework for evaluating the quality of clinical\ndata extracted by LLMs. The framework integrates variable-level performance\nbenchmarking against expert human abstraction, automated verification checks\nfor internal consistency and plausibility, and replication analyses comparing\nLLM-extracted data to human-abstracted datasets or external standards. This\nmultidimensional approach enables the identification of variables most in need\nof improvement, systematic detection of latent errors, and confirmation of\ndataset fitness-for-purpose in real-world research. Additionally, the framework\nsupports bias assessment by stratifying metrics across demographic subgroups.\nBy providing a rigorous and transparent method for assessing LLM-extracted RWD,\nthis framework advances industry standards and supports the trustworthy use of\nAI-powered evidence generation in oncology research and practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30LLM\u63d0\u53d6\u4e34\u5e8a\u6570\u636e\u8d28\u91cf\u7684\u7efc\u5408\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u8d28\u91cf\u4fdd\u8bc1\u6846\u67b6\u672a\u8986\u76d6\u7684\u95ee\u9898\u3002", "motivation": "LLM\u5728\u4e34\u5e8a\u6570\u636e\u63d0\u53d6\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u786e\u4fdd\u5176\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u53d8\u91cf\u7ea7\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3001\u81ea\u52a8\u5316\u9a8c\u8bc1\u68c0\u67e5\u548c\u590d\u5236\u5206\u6790\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u8bc6\u522b\u9700\u6539\u8fdb\u7684\u53d8\u91cf\u3001\u7cfb\u7edf\u68c0\u6d4b\u6f5c\u5728\u9519\u8bef\u5e76\u786e\u8ba4\u6570\u636e\u96c6\u9002\u7528\u6027\u3002", "conclusion": "\u6846\u67b6\u4e3aLLM\u63d0\u53d6\u7684RWD\u63d0\u4f9b\u4e25\u683c\u900f\u660e\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a8\u52a8\u884c\u4e1a\u6807\u51c6\u3002"}}
{"id": "2506.09014", "pdf": "https://arxiv.org/pdf/2506.09014", "abs": "https://arxiv.org/abs/2506.09014", "authors": ["Jianing Qi", "Xi Ye", "Hao Tang", "Zhigang Zhu", "Eunsol Choi"], "title": "Learning to Reason Across Parallel Samples for LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Scaling test-time compute brings substantial performance gains for large\nlanguage models (LLMs). By sampling multiple answers and heuristically\naggregate their answers (e.g., either through majority voting or using\nverifiers to rank the answers), one can achieve consistent performance gains in\nmath domains. In this paper, we propose a new way to leverage such multiple\nsample set. We train a compact LLM, called Sample Set Aggregator (SSA), that\ntakes a concatenated sequence of multiple samples and output the final answer,\noptimizing it for the answer accuracy with reinforcement learning. Experiments\non multiple reasoning datasets show that SSA outperforms other test-time\nscaling methods such as reward model-based re-ranking. Our approach also shows\na promising generalization ability, across sample set sizes, base model\nfamilies and scales, and tasks. By separating LLMs to generate answers and LLMs\nto analyze and aggregate sampled answers, our approach can work with the\noutputs from premier black box models easily and efficiently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u7d27\u51d1\u7684LLM\uff08SSA\uff09\u6765\u805a\u5408\u591a\u4e2a\u6837\u672c\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u91c7\u6837\u591a\u4e2a\u7b54\u6848\u5e76\u542f\u53d1\u5f0f\u805a\u5408\uff08\u5982\u591a\u6570\u6295\u7968\u6216\u9a8c\u8bc1\u5668\u6392\u540d\uff09\u53ef\u4ee5\u63d0\u5347LLM\u5728\u6570\u5b66\u9886\u57df\u7684\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u7d27\u51d1\u7684LLM\uff08SSA\uff09\uff0c\u8f93\u5165\u4e3a\u591a\u4e2a\u6837\u672c\u7684\u62fc\u63a5\u5e8f\u5217\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5176\u8f93\u51fa\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0cSSA\u4f18\u4e8e\u5176\u4ed6\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u7684\u91cd\u65b0\u6392\u5e8f\uff09\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u6837\u672c\u96c6\u5927\u5c0f\u3001\u57fa\u7840\u6a21\u578b\u548c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SSA\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u751f\u6210\u7b54\u6848\u7684LLM\u548c\u5206\u6790\u805a\u5408\u6837\u672c\u7684LLM\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u4e0e\u9ed1\u76d2\u6a21\u578b\u8f93\u51fa\u7ed3\u5408\uff0c\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08887", "pdf": "https://arxiv.org/pdf/2506.08887", "abs": "https://arxiv.org/abs/2506.08887", "authors": ["Leqi Shen", "Guoqiang Gong", "Tianxiang Hao", "Tao He", "Yifeng Zhang", "Pengzhang Liu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.", "AI": {"tldr": "DiscoVLA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3CLIP\u6a21\u578b\u5728\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u4e2d\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u5bf9\u9f50\u5dee\u5f02\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u548c\u5bf9\u9f50\u84b8\u998f\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u5dee\u5f02\uff0c\u5ffd\u7565\u4e86\u8bed\u8a00\u548c\u5bf9\u9f50\u5dee\u5f02\uff0c\u5bfc\u81f4CLIP\u5728\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faImage-Video Features Fusion\u548cImage-to-Video Alignment Distillation\uff0c\u5206\u522b\u89e3\u51b3\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u5bf9\u9f50\u5dee\u5f02\u3002", "result": "\u5728MSRVTT\u6570\u636e\u96c6\u4e0a\uff0cDiscoVLA\u4ee5CLIP\uff08ViT-B/16\uff09\u4e3a\u57fa\u7840\uff0cR@1\u8fbe\u523050.5%\uff0c\u4f18\u4e8e\u4e4b\u524d\u65b9\u6cd51.5%\u3002", "conclusion": "DiscoVLA\u901a\u8fc7\u540c\u65f6\u89e3\u51b3\u4e09\u79cd\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09021", "pdf": "https://arxiv.org/pdf/2506.09021", "abs": "https://arxiv.org/abs/2506.09021", "authors": ["Hakyung Sung", "Karla Csuros", "Min-Chang Sung"], "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features", "categories": ["cs.CL"], "comment": null, "summary": "This study examines the lexical and syntactic interventions of human and LLM\nproofreading aimed at improving overall intelligibility in identical second\nlanguage writings, and evaluates the consistency of outcomes across three LLMs\n(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and\nLLM proofreading enhance bigram lexical features, which may contribute to\nbetter coherence and contextual connectedness between adjacent words. However,\nLLM proofreading exhibits a more generative approach, extensively reworking\nvocabulary and sentence structures, such as employing more diverse and\nsophisticated vocabulary and incorporating a greater number of adjective\nmodifiers in noun phrases. The proofreading outcomes are highly consistent in\nmajor lexical and syntactic features across the three models.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e0e\u4e09\u79cdLLM\uff08ChatGPT-4o\u3001Llama3.1-8b\u3001Deepseek-r1-8b\uff09\u5728\u4e8c\u8bed\u5199\u4f5c\u6821\u5bf9\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e24\u8005\u5747\u80fd\u63d0\u5347\u8bcd\u6c47\u8fde\u8d2f\u6027\uff0c\u4f46LLM\u66f4\u503e\u5411\u4e8e\u751f\u6210\u6027\u4fee\u6539\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u4e0eLLM\u5728\u4e8c\u8bed\u5199\u4f5c\u6821\u5bf9\u4e2d\u7684\u5e72\u9884\u6548\u679c\u53ca\u4e00\u81f4\u6027\u3002", "method": "\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u4e09\u79cdLLM\u5bf9\u76f8\u540c\u4e8c\u8bed\u5199\u4f5c\u7684\u8bcd\u6c47\u548c\u53e5\u6cd5\u4fee\u6539\u3002", "result": "\u4e24\u8005\u5747\u63d0\u5347\u8bcd\u6c47\u8fde\u8d2f\u6027\uff0cLLM\u4fee\u6539\u66f4\u751f\u6210\u5316\uff0c\u4e14\u4e09\u79cd\u6a21\u578b\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "LLM\u6821\u5bf9\u5728\u8bcd\u6c47\u548c\u53e5\u6cd5\u4fee\u6539\u4e0a\u8868\u73b0\u4e00\u81f4\u4e14\u751f\u6210\u6027\u5f3a\uff0c\u53ef\u80fd\u4f18\u4e8e\u4eba\u7c7b\u6821\u5bf9\u3002"}}
{"id": "2506.08894", "pdf": "https://arxiv.org/pdf/2506.08894", "abs": "https://arxiv.org/abs/2506.08894", "authors": ["Yunzhi Zhang", "Carson Murtuza-Lanier", "Zizhang Li", "Yilun Du", "Jiajun Wu"], "title": "Product of Experts for Visual Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://product-of-experts.github.io/", "summary": "Modern neural models capture rich priors and have complementary knowledge\nover shared data domains, e.g., images and videos. Integrating diverse\nknowledge from multiple sources -- including visual generative models, visual\nlanguage models, and sources with human-crafted knowledge such as graphics\nengines and physics simulators -- remains under-explored. We propose a Product\nof Experts (PoE) framework that performs inference-time knowledge composition\nfrom heterogeneous models. This training-free approach samples from the product\ndistribution across experts via Annealed Importance Sampling (AIS). Our\nframework shows practical benefits in image and video synthesis tasks, yielding\nbetter controllability than monolithic methods and additionally providing\nflexible user interfaces for specifying visual generation goals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\u6574\u5408\u5f02\u6784\u6a21\u578b\u7684\u4e92\u8865\u77e5\u8bc6\uff0c\u63d0\u5347\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u7684\u53ef\u63a7\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6765\u6e90\uff08\u5982\u89c6\u89c9\u751f\u6210\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u5f62\u5f15\u64ce\u548c\u7269\u7406\u6a21\u62df\u5668\uff09\u7684\u4e92\u8865\u77e5\u8bc6\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9000\u706b\u91cd\u8981\u6027\u91c7\u6837\uff08AIS\uff09\u4ece\u5f02\u6784\u6a21\u578b\u7684\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u63a7\u6027\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u7528\u6237\u754c\u9762\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f02\u6784\u77e5\u8bc6\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2506.09033", "pdf": "https://arxiv.org/pdf/2506.09033", "abs": "https://arxiv.org/abs/2506.09033", "authors": ["Haozhen Zhang", "Tao Feng", "Jiaxuan You"], "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/ulab-uiuc/Router-R1", "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.", "AI": {"tldr": "Router-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u805a\u5408\u591a\u4e2aLLM\u7684\u80fd\u529b\uff0c\u4f18\u5316\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709LLM\u8def\u7531\u5668\u901a\u5e38\u4ec5\u652f\u6301\u5355\u8f6e\u4e00\u5bf9\u4e00\u6620\u5c04\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u4e2aLLM\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "Router-R1\u5c06\u8def\u7531\u548c\u805a\u5408\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u52a8\u6001\u8c03\u7528\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89c4\u5219\u5956\u52b1\u6307\u5bfc\u5b66\u4e60\u3002", "result": "\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRouter-R1\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u7684\u4f18\u5316\u3002", "conclusion": "Router-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u591aLLM\u7684\u52a8\u6001\u8def\u7531\u4e0e\u805a\u5408\uff0c\u4e3a\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.08896", "pdf": "https://arxiv.org/pdf/2506.08896", "abs": "https://arxiv.org/abs/2506.08896", "authors": ["Negin Ghamsarian", "Raphael Sznitman", "Klaus Schoeffmann", "Jens Kowal"], "title": "WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "To meet the growing demand for systematic surgical training, wetlab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wetlab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wetlab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wetlab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse https://www.synapse.org/Synapse:syn66401174/files.", "AI": {"tldr": "WetCat\u662f\u4e00\u4e2a\u4e13\u4e3a\u81ea\u52a8\u5316\u6280\u80fd\u8bc4\u4f30\u8bbe\u8ba1\u7684\u6e7f\u5b9e\u9a8c\u5ba4\u767d\u5185\u969c\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8d44\u6e90\u5728\u5168\u9762\u6280\u80fd\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u6e7f\u5b9e\u9a8c\u5ba4\u57f9\u8bad\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\uff0c\u6548\u7387\u4f4e\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u8bb0\u5f55\u548c\u8be6\u7ec6\u7684\u9636\u6bb5\u6ce8\u91ca\u3001\u8bed\u4e49\u5206\u5272\uff0c\u6784\u5efaWetCat\u6570\u636e\u96c6\uff0c\u652f\u6301\u5173\u952e\u624b\u672f\u9636\u6bb5\u7684\u6280\u80fd\u8bc4\u4f30\u3002", "result": "WetCat\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u53ef\u89e3\u91ca\u7684AI\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u624b\u672f\u6559\u80b2\u3002", "conclusion": "WetCat\u4e3a\u773c\u79d1\u57f9\u8bad\u4e2d\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u5206\u6790\u548c\u6280\u80fd\u8bc4\u4f30\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2506.08244", "pdf": "https://arxiv.org/pdf/2506.08244", "abs": "https://arxiv.org/abs/2506.08244", "authors": ["Riccardo Ali", "Pietro Li\u00f2", "Jamie Vicary"], "title": "Parameter-free approximate equivariance for tasks with finite group symmetry", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Equivariant neural networks incorporate symmetries through group actions,\nembedding them as an inductive bias to improve performance on a wide variety of\ntasks. However, existing equivariant methods can be computationally intensive,\nwith high parameter counts, and are often tied to a specific architecture. We\npropose a simple zero-parameter approach that imposes approximate equivariance\nfor a finite group in the latent representation, as an additional term in the\nloss function. We conduct experiments which allow the network to learn a group\nrepresentation on the latent space, and show in every case it prefers to learn\nthe regular representation. Fixing this action on the latent space, this yields\na simple method to impose approximate equivariance as an additional loss\npenalty. We benchmark our approach on three datasets and compare it against\nseveral existing equivariant methods, showing that in many cases it achieves\nsimilar or better performance for a fraction of the parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96f6\u53c2\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u635f\u5931\u51fd\u6570\u4e2d\u6dfb\u52a0\u8fd1\u4f3c\u7b49\u53d8\u6027\u9879\uff0c\u7b80\u5316\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u73b0\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7b49\u53d8\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u53c2\u6570\u591a\uff0c\u9700\u9488\u5bf9\u7279\u5b9a\u67b6\u6784\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u5728\u6f5c\u5728\u8868\u793a\u4e2d\u4e3a\u6709\u9650\u7fa4\u6dfb\u52a0\u8fd1\u4f3c\u7b49\u53d8\u6027\u635f\u5931\u9879\uff0c\u7f51\u7edc\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\u7684\u7fa4\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "\u63d0\u51fa\u7684\u96f6\u53c2\u6570\u65b9\u6cd5\u7b80\u5316\u4e86\u7b49\u53d8\u7f51\u7edc\u7684\u5b9e\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2506.09047", "pdf": "https://arxiv.org/pdf/2506.09047", "abs": "https://arxiv.org/abs/2506.09047", "authors": ["Yaniv Nikankin", "Dana Arad", "Yossi Gandelsman", "Yonatan Belinkov"], "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs", "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Vision-Language models (VLMs) show impressive abilities to answer questions\non visual inputs (e.g., counting objects in an image), yet demonstrate higher\naccuracies when performing an analogous task on text (e.g., counting words in a\ntext). We investigate this accuracy gap by identifying and comparing the\n\\textit{circuits} - the task-specific computational sub-graphs - in different\nmodalities. We show that while circuits are largely disjoint between\nmodalities, they implement relatively similar functionalities: the differences\nlie primarily in processing modality-specific data positions (an image or a\ntext sequence). Zooming in on the image data representations, we observe they\nbecome aligned with the higher-performing analogous textual representations\nonly towards later layers, too late in processing to effectively influence\nsubsequent positions. To overcome this, we patch the representations of visual\ndata tokens from later layers back into earlier layers. In experiments with\nmultiple tasks and models, this simple intervention closes a third of the\nperformance gap between the modalities, on average. Our analysis sheds light on\nthe multi-modal performance gap in VLMs and suggests a training-free approach\nfor reducing it.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u89c6\u89c9\u4efb\u52a1\uff0c\u539f\u56e0\u662f\u4e24\u79cd\u6a21\u6001\u7684\u7535\u8def\uff08\u8ba1\u7b97\u5b50\u56fe\uff09\u529f\u80fd\u76f8\u4f3c\u4f46\u5904\u7406\u65b9\u5f0f\u4e0d\u540c\u3002\u901a\u8fc7\u5c06\u89c6\u89c9\u6570\u636e\u7684\u540e\u671f\u5c42\u8868\u793a\u4fee\u8865\u5230\u65e9\u671f\u5c42\uff0c\u53ef\u4ee5\u7f29\u5c0f\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u548c\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u5dee\u5f02\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u89c6\u89c9\u548c\u6587\u672c\u4efb\u52a1\u7684\u7535\u8def\u529f\u80fd\uff0c\u5206\u6790\u6570\u636e\u8868\u793a\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u5c06\u540e\u671f\u5c42\u89c6\u89c9\u8868\u793a\u4fee\u8865\u5230\u65e9\u671f\u5c42\u7684\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u7f29\u5c0f\u4e86\u4e24\u79cd\u6a21\u6001\u95f4\u6027\u80fd\u5dee\u8ddd\u7684\u4e09\u5206\u4e4b\u4e00\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6027\u80fd\u5dee\u8ddd\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2506.08900", "pdf": "https://arxiv.org/pdf/2506.08900", "abs": "https://arxiv.org/abs/2506.08900", "authors": ["Jos\u00e9 Morano", "Botond Fazekas", "Emese S\u00fckei", "Ronald Fecso", "Taha Emre", "Markus Gumpinger", "Georg Faustmann", "Marzieh Oghbaie", "Ursula Schmidt-Erfurth", "Hrvoje Bogunovi\u0107"], "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790OCT\u548cSLO\u56fe\u50cf\uff0c\u5e76\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u5728\u773c\u79d1\u56fe\u50cf\u5206\u6790\u4e2d\u9700\u8981\u5927\u91cf\u6807\u6ce8\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u57fa\u7840\u6a21\u578b\u867d\u6709\u6f5c\u529b\u4f46\u7f3a\u4e4f\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faMIRAGE\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684OCT/SLO\u5206\u7c7b\u548c\u5206\u5272\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "MIRAGE\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u901a\u7528\u548c\u4e13\u7528\u57fa\u7840\u6a21\u578b\u53ca\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "MIRAGE\u9002\u5408\u4f5c\u4e3a\u5f00\u53d1\u7a33\u5065\u773c\u79d1AI\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u6a21\u578b\u548c\u8bc4\u4f30\u57fa\u51c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08255", "pdf": "https://arxiv.org/pdf/2506.08255", "abs": "https://arxiv.org/abs/2506.08255", "authors": ["Patryk Krukowski", "\u0141ukasz Gorczyca", "Piotr Helm", "Kamil Ksi\u0105\u017cek", "Przemys\u0142aw Spurek"], "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Traditional deep neural networks suffer from several limitations, including\ncatastrophic forgetting. When models are adapted to new datasets, they tend to\nquickly forget previously learned knowledge. Another significant issue is the\nlack of robustness to even small perturbations in the input data. In practice,\nwe can often easily perform adversarial attacks and change the network's\npredictions, adding minimal noise to the input. Dedicated architectures and\ntraining procedures can solve each of the above problems separately.\nUnfortunately, currently, no model can simultaneously address both catastrophic\nforgetting and vulnerability to adversarial attacks. We introduce SHIELD\n(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel\napproach that integrates a hypernetwork-based continual learning approach with\ninterval arithmetic. SHIELD use the hypernetwork to transfer trainable task\nembedding vectors into the weights of a target model dedicated to specific\ndata. This paradigm allows for the dynamic generation of separate networks for\neach subtask, while the hypernetwork aggregates and analyzes information across\nall tasks. The target model takes in the input a data sample with a defined\ninterval range, and by creating a hypercube, produces a prediction for the\ngiven range. Therefore, such target models provide strict guarantees against\nall possible attacks for data samples within the interval range. Our approach\nenhances security without sacrificing network adaptability, addressing the\noverlooked challenge of safety in continual learning.", "AI": {"tldr": "SHIELD\u662f\u4e00\u79cd\u7ed3\u5408\u8d85\u7f51\u7edc\u548c\u533a\u95f4\u7b97\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u548c\u5bf9\u6297\u653b\u51fb\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u95ee\u9898\uff0c\u76ee\u524d\u5c1a\u65e0\u6a21\u578b\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u8005\u3002", "method": "SHIELD\u901a\u8fc7\u8d85\u7f51\u7edc\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u7684\u52a8\u6001\u5b50\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u533a\u95f4\u7b97\u672f\u63d0\u4f9b\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "SHIELD\u5728\u589e\u5f3a\u5b89\u5168\u6027\u7684\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "SHIELD\u4e3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08906", "pdf": "https://arxiv.org/pdf/2506.08906", "abs": "https://arxiv.org/abs/2506.08906", "authors": ["Peilin Yu", "Yuwei Wu", "Zhi Gao", "Xiaomeng Fan", "Shuo Yang", "Yunde Jia"], "title": "Hyperbolic Dual Feature Augmentation for Open-Environment", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2207.03824,\n  arXiv:2304.11855 by other authors", "summary": "Feature augmentation generates novel samples in the feature space, providing\nan effective way to enhance the generalization ability of learning algorithms\nwith hyperbolic geometry. Most hyperbolic feature augmentation is confined to\nclosed-environment, assuming the number of classes is fixed (\\emph{i.e.}, seen\nclasses) and generating features only for these classes. In this paper, we\npropose a hyperbolic dual feature augmentation method for open-environment,\nwhich augments features for both seen and unseen classes in the hyperbolic\nspace. To obtain a more precise approximation of the real data distribution for\nefficient training, (1) we adopt a neural ordinary differential equation\nmodule, enhanced by meta-learning, estimating the feature distributions of both\nseen and unseen classes; (2) we then introduce a regularizer to preserve the\nlatent hierarchical structures of data in the hyperbolic space; (3) we also\nderive an upper bound for the hyperbolic dual augmentation loss, allowing us to\ntrain a hyperbolic model using infinite augmentations for seen and unseen\nclasses. Extensive experiments on five open-environment tasks:\nclass-incremental learning, few-shot open-set recognition, few-shot learning,\nzero-shot learning, and general image classification, demonstrate that our\nmethod effectively enhances the performance of hyperbolic algorithms in\nopen-environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u66f2\u53cc\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u73af\u5883\u4e2d\u7684\u7279\u5f81\u589e\u5f3a\uff0c\u63d0\u5347\u53cc\u66f2\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53cc\u66f2\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\u5c40\u9650\u4e8e\u5c01\u95ed\u73af\u5883\uff0c\u65e0\u6cd5\u5904\u7406\u5f00\u653e\u73af\u5883\u4e2d\u7684\u672a\u89c1\u7c7b\u522b\u3002", "method": "\u91c7\u7528\u795e\u7ecfODE\u6a21\u5757\u548c\u5143\u5b66\u4e60\u4f30\u8ba1\u7279\u5f81\u5206\u5e03\uff0c\u5f15\u5165\u6b63\u5219\u5316\u4fdd\u6301\u5c42\u6b21\u7ed3\u6784\uff0c\u63a8\u5bfc\u635f\u5931\u4e0a\u754c\u4ee5\u652f\u6301\u65e0\u9650\u589e\u5f3a\u3002", "result": "\u5728\u4e94\u9879\u5f00\u653e\u73af\u5883\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u53cc\u66f2\u7b97\u6cd5\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u53cc\u66f2\u7b97\u6cd5\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.08908", "pdf": "https://arxiv.org/pdf/2506.08908", "abs": "https://arxiv.org/abs/2506.08908", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSkipVAR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7\u5197\u4f59\u6b65\u9aa4\u548c\u66ff\u6362\u65e0\u6761\u4ef6\u5206\u652f\uff0c\u663e\u8457\u63d0\u5347VAR\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0VAR\u6a21\u578b\u7684\u9ad8\u9891\u7ec4\u4ef6\u6216\u540e\u671f\u6b65\u9aa4\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f46\u8ba1\u7b97\u5197\u4f59\u5c1a\u672a\u6df1\u5165\u5206\u6790\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u8df3\u8fc7\u6b65\u9aa4\u7b56\u7565\u548c\u65e0\u6761\u4ef6\u5206\u652f\u66ff\u6362\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u9891\u7387\u4fe1\u606f\u52a8\u6001\u9009\u62e9\u52a0\u901f\u7b56\u7565\u3002", "result": "SkipVAR\u5728GenEval\u57fa\u51c6\u4e0a\u5b9e\u73b01.81\u500d\u52a0\u901f\u548c2.62\u500d\u901f\u5ea6\u63d0\u5347\uff0c\u5e73\u5747SSIM\u8fbe0.88\u3002", "conclusion": "\u9891\u7387\u611f\u77e5\u7684\u81ea\u9002\u5e94\u52a0\u901f\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2506.08125", "pdf": "https://arxiv.org/pdf/2506.08125", "abs": "https://arxiv.org/abs/2506.08125", "authors": ["Hanbing Liu", "Lang Cao", "Yuanyi Ren", "Mengyu Zhou", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities,\nyet they often suffer from inefficiencies due to unnecessarily verbose or\nredundant outputs. While many works have explored reinforcement learning (RL)\nto enhance reasoning abilities, most primarily focus on improving accuracy,\nwith limited attention to reasoning efficiency. Some existing approaches\nintroduce direct length-based rewards to encourage brevity, but this often\nleads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL\nframework that advances length-based reward design to boost efficient\nreasoning. Bingo incorporates two key mechanisms: a significance-aware length\nreward, which gradually guides the model to reduce only insignificant tokens,\nand a dynamic length reward, which initially encourages elaborate reasoning for\nhard questions but decays over time to improve overall efficiency. Experiments\nacross multiple reasoning benchmarks show that Bingo improves both accuracy and\nefficiency. It outperforms the vanilla reward and several other length-based\nreward baselines in RL, achieving a favorable trade-off between accuracy and\nefficiency. These results underscore the potential of training LLMs explicitly\nfor efficient reasoning.", "AI": {"tldr": "Bingo\u662f\u4e00\u4e2aRL\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u957f\u5ea6\u5956\u52b1\u8bbe\u8ba1\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u5347\u63a8\u7406\u6548\u7387\u65f6\u5f80\u5f80\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\uff0cBingo\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Bingo\u5f15\u5165\u663e\u8457\u6027\u611f\u77e5\u957f\u5ea6\u5956\u52b1\u548c\u52a8\u6001\u957f\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u9010\u6b65\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBingo\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "conclusion": "Bingo\u8bc1\u660e\u4e86\u901a\u8fc7\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2506.08915", "pdf": "https://arxiv.org/pdf/2506.08915", "abs": "https://arxiv.org/abs/2506.08915", "authors": ["Ananthu Aniraj", "Cassio F. Dantas", "Dino Ienco", "Diego Marcos"], "title": "Inherently Faithful Attention Maps for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u4e8c\u8fdb\u5236\u6ce8\u610f\u529b\u63a9\u7801\u786e\u4fdd\u53ea\u6709\u5173\u6ce8\u7684\u56fe\u50cf\u533a\u57df\u5f71\u54cd\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u4e0a\u4e0b\u6587\u53ef\u80fd\u5f3a\u70c8\u5f71\u54cd\u7269\u4f53\u611f\u77e5\uff0c\u5c24\u5176\u662f\u5728\u7269\u4f53\u51fa\u73b0\u5728\u5206\u5e03\u5916\u80cc\u666f\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u504f\u8868\u793a\u3002\u540c\u65f6\uff0c\u8bb8\u591a\u56fe\u50cf\u7ea7\u4efb\u52a1\u9700\u8981\u8bc6\u522b\u76f8\u5173\u533a\u57df\uff0c\u901a\u5e38\u9700\u8981\u4e0a\u4e0b\u6587\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5904\u7406\u5b8c\u6574\u56fe\u50cf\u4ee5\u53d1\u73b0\u7269\u4f53\u90e8\u5206\u548c\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u8f93\u5165\u6ce8\u610f\u529b\u63a9\u7801\u9650\u5236\u5176\u611f\u53d7\u91ce\u5230\u8fd9\u4e9b\u533a\u57df\uff0c\u5b9e\u73b0\u805a\u7126\u5206\u6790\u5e76\u8fc7\u6ee4\u6f5c\u5728\u865a\u5047\u4fe1\u606f\u3002\u4e24\u9636\u6bb5\u8054\u5408\u8bad\u7ec3\uff0c\u7b2c\u4e8c\u9636\u6bb5\u53ef\u4f18\u5316\u7b2c\u4e00\u9636\u6bb5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u865a\u5047\u76f8\u5173\u6027\u548c\u5206\u5e03\u5916\u80cc\u666f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.08266", "pdf": "https://arxiv.org/pdf/2506.08266", "abs": "https://arxiv.org/abs/2506.08266", "authors": ["Yaswanth Chittepu", "Blossom Metevier", "Will Schwarzer", "Austin Hoag", "Scott Niekum", "Philip S. Thomas"], "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "comment": "20 pages, 6 figures, 4 tables, Second Reinforcement Learning\n  Conference (RLC 2025)", "summary": "Existing approaches to language model alignment often treat safety as a\ntradeoff against helpfulness, which can lead to unacceptable responses in\nsensitive domains. To ensure reliable performance in such settings, we propose\nHigh-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a\nmethod that provides high-confidence safety guarantees while maximizing\nhelpfulness. Similar to previous methods, HC-RLHF explicitly decouples human\npreferences into helpfulness and harmlessness (safety), which are learned by\ntraining a reward model and a cost model, respectively. It then employs a\ntwo-step process to find safe solutions. In the first step, it optimizes the\nreward function under an intentionally pessimistic version of the cost\nconstraint. In the second step, the trained model undergoes a safety test to\nverify whether its performance stays within an upper-confidence bound of the\nactual cost constraint. We provide a theoretical analysis of HC-RLHF, including\nproof that it will not return an unsafe solution with a probability greater\nthan a user-specified threshold. For our empirical analysis, we apply HC-RLHF\nto align three different language models (Qwen2-1.5B, Qwen2.5-3B, and\nLLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF\nproduces safe models with high probability and can improve harmlessness and\nhelpfulness compared to previous methods.", "AI": {"tldr": "HC-RLHF\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u7f6e\u4fe1\u5ea6\u5b89\u5168\u4fdd\u8bc1\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u5e73\u8861\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u9886\u57df\u7684\u4e0d\u5f53\u54cd\u5e94\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "HC-RLHF\u901a\u8fc7\u5206\u79bb\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u548c\u6210\u672c\u6a21\u578b\uff0c\u5206\u4e24\u6b65\u4f18\u5316\uff1a\u5148\u60b2\u89c2\u7ea6\u675f\u4e0b\u4f18\u5316\u5956\u52b1\uff0c\u518d\u901a\u8fc7\u5b89\u5168\u6d4b\u8bd5\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHC-RLHF\u80fd\u9ad8\u6982\u7387\u751f\u6210\u5b89\u5168\u6a21\u578b\uff0c\u5e76\u5728\u65e0\u5bb3\u6027\u548c\u6709\u7528\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HC-RLHF\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6548\u679c\u517c\u5177\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2506.08140", "pdf": "https://arxiv.org/pdf/2506.08140", "abs": "https://arxiv.org/abs/2506.08140", "authors": ["Yifei Li", "Hanane Nour Moussa", "Ziru Chen", "Shijie Chen", "Botao Yu", "Mingyi Xue", "Benjamin Burns", "Tzu-Yao Chiu", "Vishal Dey", "Zitong Lu", "Chen Wei", "Qianheng Zhang", "Tianyu Zhang", "Song Gao", "Xuhui Huang", "Xia Ning", "Nesreen K. Ahmed", "Ali Payani", "Huan Sun"], "title": "AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Despite long-standing efforts in accelerating scientific discovery with AI,\nbuilding AI co-scientists remains challenging due to limited high-quality data\nfor training and evaluation. To tackle this data scarcity issue, we present\nAutoSDT, an automatic pipeline that collects high-quality coding tasks in\nreal-world data-driven discovery workflows. AutoSDT leverages the coding\ncapabilities and parametric knowledge of LLMs to search for diverse sources,\nselect ecologically valid tasks, and synthesize accurate task instructions and\ncode solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404\ncoding tasks for data-driven discovery that covers four scientific disciplines\nand 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the\nonly automatically collected and the largest open dataset for data-driven\nscientific discovery. Expert feedback on a subset of 256 tasks shows the\neffectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,\nand 92.2% of the synthesized programs are functionally correct. Trained on\nAutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show\nsubstantial improvement on two challenging data-driven discovery benchmarks,\nScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches\nthe same level of performance as GPT-4o on ScienceAgentBench with a success\nrate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it\nlifts the hypothesis matching score to 8.1, bringing a 17.4% relative\nimprovement and closing the gap between open-weight models and GPT-4o.", "AI": {"tldr": "AutoSDT\u662f\u4e00\u4e2a\u81ea\u52a8\u6536\u96c6\u9ad8\u8d28\u91cf\u7f16\u7801\u4efb\u52a1\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86AutoSDT-5K\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u53d1\u73b0\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4ee5\u52a0\u901fAI\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528LLMs\u7684\u7f16\u7801\u80fd\u529b\u548c\u53c2\u6570\u77e5\u8bc6\uff0c\u81ea\u52a8\u641c\u7d22\u591a\u6837\u6765\u6e90\u3001\u9009\u62e9\u751f\u6001\u6709\u6548\u7684\u4efb\u52a1\uff0c\u5e76\u5408\u6210\u4efb\u52a1\u6307\u4ee4\u548c\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6784\u5efa\u4e86AutoSDT-5K\u6570\u636e\u96c6\uff0c\u5305\u542b5,404\u4e2a\u4efb\u52a1\uff0c\u4e13\u5bb6\u9a8c\u8bc1\u663e\u793a93%\u7684\u4efb\u52a1\u6709\u6548\uff0c92.2%\u7684\u4ee3\u7801\u6b63\u786e\u3002\u8bad\u7ec3\u540e\u7684AutoSDT-Coder\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AutoSDT\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u4e0eGPT-4o\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.08927", "pdf": "https://arxiv.org/pdf/2506.08927", "abs": "https://arxiv.org/abs/2506.08927", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts.", "AI": {"tldr": "\u63a2\u7d22\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u76d1\u7763\u7684\u641c\u7d22\u673a\u5236\uff0c\u901a\u8fc7MCTS\u7b97\u6cd5\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u8bf1\u5bfc\u957f\u63a8\u7406\u94fe\u3002", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u641c\u7d22\u673a\u5236\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u73b0\u6709\u975e\u63a8\u7406\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u6fc0\u53d1\u5176\u9690\u85cf\u77e5\u8bc6\u5e76\u751f\u6210\u957f\u63a8\u7406\u94fe\u3002", "method": "\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u5b50\u95ee\u9898-\u5b50\u7b54\u6848\u5bf9\u6765\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\uff0cMMMU-PRO\u603b\u4f53\u63d0\u53472%\uff0c\u5176\u4e2d\u6587\u79d1\u9886\u57df\u663e\u8457\u63d0\u53479%\u3002", "conclusion": "\u901a\u8fc7\u641c\u7d22\u673a\u5236\u53ef\u4ee5\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u6210\u529f\u8bf1\u5bfc\u957f\u63a8\u7406\u94fe\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u76d1\u7763\u3002"}}
{"id": "2506.08267", "pdf": "https://arxiv.org/pdf/2506.08267", "abs": "https://arxiv.org/abs/2506.08267", "authors": ["Mansooreh Montazerin", "Majd Al Aawar", "Antonio Ortega", "Ajitesh Srivastava"], "title": "Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Symbolic regression (SR) aims to discover closed-form mathematical\nexpressions that accurately describe data, offering interpretability and\nanalytical insight beyond standard black-box models. Existing SR methods often\nrely on population-based search or autoregressive modeling, which struggle with\nscalability and symbolic consistency. We introduce LIES (Logarithm, Identity,\nExponential, Sine), a fixed neural network architecture with interpretable\nprimitive activations that are optimized to model symbolic expressions. We\ndevelop a framework to extract compact formulae from LIES networks by training\nwith an appropriate oversampling strategy and a tailored loss function to\npromote sparsity and to prevent gradient instability. After training, it\napplies additional pruning strategies to further simplify the learned\nexpressions into compact formulae. Our experiments on SR benchmarks show that\nthe LIES framework consistently produces sparse and accurate symbolic formulae\noutperforming all baselines. We also demonstrate the importance of each design\ncomponent through ablation studies.", "AI": {"tldr": "LIES\u6846\u67b6\u901a\u8fc7\u56fa\u5b9a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u7b26\u53f7\u56de\u5f52\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u7b26\u53f7\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cLIES\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528LIES\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u8fc7\u91c7\u6837\u7b56\u7565\u548c\u5b9a\u5236\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u540e\u901a\u8fc7\u526a\u679d\u7b80\u5316\u8868\u8fbe\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLIES\u751f\u6210\u7684\u7b26\u53f7\u516c\u5f0f\u7a00\u758f\u4e14\u51c6\u786e\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LIES\u6846\u67b6\u4e3a\u7b26\u53f7\u56de\u5f52\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5404\u8bbe\u8ba1\u7ec4\u4ef6\u5747\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2506.08188", "pdf": "https://arxiv.org/pdf/2506.08188", "abs": "https://arxiv.org/abs/2506.08188", "authors": ["Wenlong Meng", "Shuguo Fan", "Chengkun Wei", "Min Chen", "Yuwei Li", "Yuanchao Zhang", "Zhikun Zhang", "Wenzhi Chen"], "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors", "categories": ["cs.CR", "cs.CL"], "comment": "Accepted to USENIX Security'25", "summary": "In this paper, we introduce GradEscape, the first gradient-based evader\ndesigned to attack AI-generated text (AIGT) detectors. GradEscape overcomes the\nundifferentiable computation problem, caused by the discrete nature of text, by\nintroducing a novel approach to construct weighted embeddings for the detector\ninput. It then updates the evader model parameters using feedback from victim\ndetectors, achieving high attack success with minimal text modification. To\naddress the issue of tokenizer mismatch between the evader and the detector, we\nintroduce a warm-started evader method, enabling GradEscape to adapt to\ndetectors across any language model architecture. Moreover, we employ novel\ntokenizer inference and model extraction techniques, facilitating effective\nevasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language\nmodels, benchmarking it against four state-of-the-art AIGT evaders.\nExperimental results demonstrate that GradEscape outperforms existing evaders\nin various scenarios, including with an 11B paraphrase model, while utilizing\nonly 139M parameters. We have successfully applied GradEscape to two real-world\ncommercial AIGT detectors. Our analysis reveals that the primary vulnerability\nstems from disparity in text expression styles within the training data. We\nalso propose a potential defense strategy to mitigate the threat of AIGT\nevaders. We open-source our GradEscape for developing more robust AIGT\ndetectors.", "AI": {"tldr": "GradEscape\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u9488\u5bf9AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u52a0\u6743\u5d4c\u5165\u548c\u6a21\u578b\u53c2\u6570\u66f4\u65b0\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u79bb\u6563\u6027\u5bfc\u81f4\u7684\u4e0d\u53ef\u5fae\u5206\u95ee\u9898\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u68c0\u6d4b\u5668\u3002", "method": "\u5f15\u5165\u52a0\u6743\u5d4c\u5165\u3001\u6696\u542f\u52a8\u65b9\u6cd5\u3001\u5206\u8bcd\u5668\u63a8\u65ad\u548c\u6a21\u578b\u63d0\u53d6\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5546\u4e1a\u68c0\u6d4b\u5668\u3002", "conclusion": "\u63ed\u793a\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u6587\u672c\u8868\u8fbe\u98ce\u683c\u7684\u5dee\u5f02\u662f\u4e3b\u8981\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2506.08933", "pdf": "https://arxiv.org/pdf/2506.08933", "abs": "https://arxiv.org/abs/2506.08933", "authors": ["Wendong Bu", "Yang Wu", "Qifan Yu", "Minghe Gao", "Bingchen Miao", "Zhenkui Zhang", "Kaihang Pan", "Yunfei Li", "Mengze Li", "Wei Ji", "Juncheng Li", "Siliang Tang", "Yueting Zhuang"], "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025 (Oral)", "summary": "As multimodal large language models (MLLMs) advance, MLLM-based virtual\nagents have demonstrated remarkable performance. However, existing benchmarks\nface significant limitations, including uncontrollable task complexity,\nextensive manual annotation with limited scenarios, and a lack of\nmultidimensional evaluation. In response to these challenges, we introduce\nOmniBench, a self-generating, cross-platform, graph-based benchmark with an\nautomated pipeline for synthesizing tasks of controllable complexity through\nsubtask composition. To evaluate the diverse capabilities of virtual agents on\nthe graph, we further present OmniEval, a multidimensional evaluation framework\nthat includes subtask-level evaluation, graph-based metrics, and comprehensive\ntests across 10 capabilities. Our synthesized dataset contains 36k\ngraph-structured tasks across 20 scenarios, achieving a 91\\% human acceptance\nrate. Training on our graph-structured data shows that it can more efficiently\nguide agents compared to manually annotated data. We conduct multidimensional\nevaluations for various open-source and closed-source models, revealing their\nperformance across various capabilities and paving the way for future\nadvancements. Our project is available at https://omni-bench.github.io/.", "AI": {"tldr": "OmniBench\u662f\u4e00\u4e2a\u81ea\u751f\u6210\u3001\u8de8\u5e73\u53f0\u7684\u56fe\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u865a\u62df\u4ee3\u7406\u7684\u591a\u6837\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u4e0d\u53ef\u63a7\u3001\u4eba\u5de5\u6807\u6ce8\u6709\u9650\u4e14\u573a\u666f\u5355\u4e00\u3001\u7f3a\u4e4f\u591a\u7ef4\u8bc4\u4f30\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faOmniBench\uff08\u57fa\u4e8e\u56fe\u7684\u57fa\u51c6\uff09\u548cOmniEval\uff08\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff09\uff0c\u901a\u8fc7\u5b50\u4efb\u52a1\u7ec4\u5408\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u572810\u79cd\u80fd\u529b\u4e0a\u8fdb\u884c\u5168\u9762\u6d4b\u8bd5\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u96c6\u5305\u542b36k\u56fe\u7ed3\u6784\u4efb\u52a1\uff0c\u8986\u76d620\u4e2a\u573a\u666f\uff0c\u4eba\u7c7b\u63a5\u53d7\u7387\u8fbe91%\u3002\u56fe\u7ed3\u6784\u6570\u636e\u6bd4\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u66f4\u9ad8\u6548\u3002", "conclusion": "OmniBench\u548cOmniEval\u4e3a\u865a\u62df\u4ee3\u7406\u7684\u591a\u7ef4\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.08277", "pdf": "https://arxiv.org/pdf/2506.08277", "abs": "https://arxiv.org/abs/2506.08277", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "39 pages, 22 figures", "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos].", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6307\u4ee4\u8c03\u4f18\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u548c\u97f3\u9891\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u975e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u4e14\u5176\u5c42\u6b21\u7ed3\u6784\u4e0e\u5927\u8111\u6d3b\u52a8\u5bf9\u9f50\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u6307\u4ee4\u8c03\u4f18MLLMs\u5728\u591a\u6a21\u6001\u523a\u6fc0\u4e0b\u5927\u8111\u5bf9\u9f50\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u752813\u79cd\u89c6\u9891\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u7684MLLMs\uff0c\u6d4b\u91cf\u5176\u8868\u5f81\u5bf9\u81ea\u7136\u7535\u5f71\u89c2\u770b\u65f6\u795e\u7ecf\u6d3b\u52a8\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u6307\u4ee4\u8c03\u4f18MLLMs\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b15%\uff0c\u5728\u97f3\u9891\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b20%\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u6307\u4ee4\u663e\u8457\u63d0\u5347MLLMs\u4e0e\u5927\u8111\u6d3b\u52a8\u7684\u5bf9\u9f50\uff0c\u4e3a\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08949", "pdf": "https://arxiv.org/pdf/2506.08949", "abs": "https://arxiv.org/abs/2506.08949", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSSS\u65b9\u6cd5\uff0c\u5229\u7528SAM-2\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u589e\u5f3a\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u901a\u8fc7DFE\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u589e\u5f3a\u4f18\u5316\u7279\u5f81\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u4fe1\u606f\u7206\u70b8\u65f6\u4ee3\uff0c\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u57fa\u4e8eSAM-2\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u63d0\u51faSSS\u65b9\u6cd5\uff0c\u7ed3\u5408DFE\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u589e\u5f3a\u6280\u672f\u4f18\u5316\u7279\u5f81\uff0c\u5e76\u5f00\u53d1PCSW\u63d0\u793a\u751f\u6210\u5668\u3002", "result": "\u5728ACDC\u548cBHSD\u6570\u636e\u96c6\u4e0a\uff0cSSS\u7684\u5e73\u5747Dice\u5206\u6570\u4e3a53.15\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u53473.65\u3002", "conclusion": "SSS\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u548cSAM-2\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08249", "pdf": "https://arxiv.org/pdf/2506.08249", "abs": "https://arxiv.org/abs/2506.08249", "authors": ["Ken Gu", "Zhihan Zhang", "Kate Lin", "Yuwei Zhang", "Akshay Paruchuri", "Hong Yu", "Mehran Kazemi", "Kumar Ayush", "A. Ali Heydari", "Maxwell A. Xu", "Girish Narayanswamy", "Yun Liu", "Ming-Zher Poh", "Yuzhe Yang", "Mark Malhotra", "Shwetak Patel", "Hamid Palangi", "Xuhai Xu", "Daniel McDuff", "Tim Althoff", "Xin Liu"], "title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Language models (LMs) are increasingly being deployed to perform autonomous\ndata analyses. However, their data awareness -- the ability to recognize,\nreason over, and appropriately handle data artifacts such as missing values,\noutliers, and logical inconsistencies -- remains underexplored. These artifacts\nare especially common in real-world tabular data and, if mishandled, can\nsignificantly compromise the validity of analytical conclusions. To address\nthis gap, we present RADAR, a benchmark for systematically evaluating\ndata-aware reasoning on tabular data. We develop a framework to simulate data\nartifacts via programmatic perturbations to enable targeted evaluation of model\nbehavior. RADAR comprises 2980 table query pairs, grounded in real-world data\nspanning 9 domains and 5 data artifact types. In addition to evaluating\nartifact handling, RADAR systematically varies table size to study how\nreasoning performance holds when increasing table size. Our evaluation reveals\nthat, despite decent performance on tables without data artifacts, frontier\nmodels degrade significantly when data artifacts are introduced, exposing\ncritical gaps in their capacity for robust, data-aware analysis. Designed to be\nflexible and extensible, RADAR supports diverse perturbation types and\ncontrollable table sizes, offering a valuable resource for advancing tabular\nreasoning.", "AI": {"tldr": "RADAR\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u4e2d\u5904\u7406\u6570\u636e\u5f02\u5e38\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\u5728\u6570\u636e\u5f02\u5e38\u5b58\u5728\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u6570\u636e\u5206\u6790\u4e2d\u7684\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u5bf9\u6570\u636e\u5f02\u5e38\uff08\u5982\u7f3a\u5931\u503c\u3001\u5f02\u5e38\u503c\u7b49\uff09\u7684\u8bc6\u522b\u548c\u5904\u7406\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u4e9b\u5f02\u5e38\u53ef\u80fd\u4e25\u91cd\u5f71\u54cd\u5206\u6790\u7ed3\u8bba\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u6270\u52a8\u6a21\u62df\u6570\u636e\u5f02\u5e38\uff0c\u6784\u5efa\u4e86\u5305\u542b2980\u4e2a\u8868\u683c\u67e5\u8be2\u5bf9\u7684RADAR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d69\u4e2a\u9886\u57df\u548c5\u79cd\u6570\u636e\u5f02\u5e38\u7c7b\u578b\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8868\u683c\u5927\u5c0f\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u65e0\u6570\u636e\u5f02\u5e38\u65f6\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u6570\u636e\u5f02\u5e38\u5b58\u5728\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u66b4\u9732\u4e86\u5176\u5728\u7a33\u5065\u6570\u636e\u5206\u6790\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "conclusion": "RADAR\u4e3a\u63d0\u5347\u8868\u683c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u8d44\u6e90\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u6270\u52a8\u7c7b\u578b\u548c\u53ef\u63a7\u7684\u8868\u683c\u5927\u5c0f\u3002"}}
{"id": "2506.08953", "pdf": "https://arxiv.org/pdf/2506.08953", "abs": "https://arxiv.org/abs/2506.08953", "authors": ["Anirudh Nanduri", "Siyuan Huang", "Rama Chellappa"], "title": "Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nwide range of biometric tasks, including face and body recognition. In this\nwork, we adapt a ViT model pretrained on visible (VIS) imagery to the\nchallenging problem of cross-spectral body recognition, which involves matching\nimages captured in the visible and infrared (IR) domains. Recent ViT\narchitectures have explored incorporating additional embeddings beyond\ntraditional positional embeddings. Building on this idea, we integrate Side\nInformation Embedding (SIE) and examine the impact of encoding domain and\ncamera information to enhance cross-spectral matching. Surprisingly, our\nresults show that encoding only camera information - without explicitly\nincorporating domain information - achieves state-of-the-art performance on the\nLLCM dataset. While occlusion handling has been extensively studied in\nvisible-spectrum person re-identification (Re-ID), occlusions in\nvisible-infrared (VI) Re-ID remain largely underexplored - primarily because\nexisting VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly\nfeature full-body, unoccluded images. To address this gap, we analyze the\nimpact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain\nFace (IJB-MDF) dataset, which provides a diverse set of visible and infrared\nimages captured at various distances, enabling cross-range, cross-spectral\nevaluations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u7684ViT\u6a21\u578b\u5e94\u7528\u4e8e\u8de8\u5149\u8c31\u4eba\u4f53\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5f15\u5165Side Information Embedding\uff08SIE\uff09\u63d0\u5347\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u4ec5\u7f16\u7801\u76f8\u673a\u4fe1\u606f\u5373\u53ef\u5728LLCM\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6548\u679c\uff0c\u540c\u65f6\u5206\u6790\u4e86\u906e\u6321\u95ee\u9898\u5728\u53ef\u89c1-\u7ea2\u5916Re-ID\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u8de8\u5149\u8c31\u4eba\u4f53\u8bc6\u522b\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u53ef\u89c1\u5149\u4e0e\u7ea2\u5916\u56fe\u50cf\u5339\u914d\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u906e\u6321\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684ViT\u6a21\u578b\uff0c\u5f15\u5165SIE\u7f16\u7801\u57df\u548c\u76f8\u673a\u4fe1\u606f\uff0c\u5e76\u5728IJB-MDF\u6570\u636e\u96c6\u4e0a\u5206\u6790\u906e\u6321\u5f71\u54cd\u3002", "result": "\u4ec5\u7f16\u7801\u76f8\u673a\u4fe1\u606f\u5373\u53ef\u5728LLCM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u906e\u6321\u5206\u6790\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "conclusion": "SIE\u4e2d\u7684\u76f8\u673a\u4fe1\u606f\u5bf9\u8de8\u5149\u8c31\u5339\u914d\u81f3\u5173\u91cd\u8981\uff0c\u906e\u6321\u95ee\u9898\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347VI-ReID\u6027\u80fd\u3002"}}
{"id": "2506.08955", "pdf": "https://arxiv.org/pdf/2506.08955", "abs": "https://arxiv.org/abs/2506.08955", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "title": "Segment Concealed Objects with Incomplete Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "IEEE TPAMI", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5SEE\uff0c\u7528\u4e8e\u4e0d\u5b8c\u5168\u76d1\u7763\u7684\u9690\u853d\u76ee\u6807\u5206\u5272\uff08ISCOS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408SAM\u751f\u6210\u4f2a\u6807\u7b7e\u548c\u6df7\u5408\u7c92\u5ea6\u7279\u5f81\u5206\u7ec4\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4e0d\u5b8c\u5168\u76d1\u7763\u548c\u9690\u853d\u76ee\u6807\u4e0e\u80cc\u666f\u76f8\u4f3c\u6027\u7684\u6311\u6218\u3002", "motivation": "\u9690\u853d\u76ee\u6807\u5206\u5272\u4efb\u52a1\u56e0\u4e0d\u5b8c\u5168\u6807\u6ce8\u6570\u636e\u548c\u76ee\u6807\u4e0e\u80cc\u666f\u7684\u76f8\u4f3c\u6027\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51faSEE\u6846\u67b6\uff0c\u5229\u7528SAM\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u4f2a\u6807\u7b7e\u751f\u6210\u3001\u5b58\u50a8\u548c\u76d1\u7763\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\uff1b\u8bbe\u8ba1\u6df7\u5408\u7c92\u5ea6\u7279\u5f81\u5206\u7ec4\u6a21\u5757\u4ee5\u63d0\u5347\u5206\u5272\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSEE\u5728\u591a\u4e2aISCOS\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6848\u63d0\u5347\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "SEE\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u7279\u5f81\u5206\u7ec4\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86ISCOS\u7684\u6311\u6218\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08309", "pdf": "https://arxiv.org/pdf/2506.08309", "abs": "https://arxiv.org/abs/2506.08309", "authors": ["Katherine Tieu", "Dongqi Fu", "Zihao Li", "Ross Maciejewski", "Jingrui He"], "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025. 28 pages, 1 figures, 22 tables", "summary": "Accurate predictions rely on the expressiveness power of graph deep learning\nframeworks like graph neural networks and graph transformers, where a\npositional encoding mechanism has become much more indispensable in recent\nstate-of-the-art works to record the canonical position information. However,\nthe current positional encoding is limited in three aspects: (1) most\npositional encoding methods use pre-defined, and fixed functions, which are\ninadequate to adapt to the complex attributed graphs; (2) a few pioneering\nworks proposed the learnable positional encoding but are still limited to the\nstructural information, not considering the real-world time-evolving\ntopological and feature information; (3) most positional encoding methods are\nequipped with transformers' attention mechanism to fully leverage their\ncapabilities, where the dense or relational attention is often unaffordable on\nlarge-scale structured data. Hence, we aim to develop Learnable\nSpatial-Temporal Positional Encoding in an effective and efficient manner and\npropose a simple temporal link prediction model named L-STEP. Briefly, for\nL-STEP, we (1) prove the proposed positional learning scheme can preserve the\ngraph property from the spatial-temporal spectral viewpoint, (2) verify that\nMLPs can fully exploit the expressiveness and reach transformers' performance\non that encoding, (3) change different initial positional encoding inputs to\nshow robustness, (4) analyze the theoretical complexity and obtain less\nempirical running time than SOTA, and (5) demonstrate its temporal link\nprediction out-performance on 13 classic datasets and with 10 algorithms in\nboth transductive and inductive settings using 3 different sampling strategies.\nAlso, \\name\\ obtains the leading performance in the newest large-scale TGB\nbenchmark. Our code is available at https://github.com/kthrn22/L-STEP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u65f6\u7a7a\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5L-STEP\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u5728\u9002\u5e94\u6027\u3001\u52a8\u6001\u6027\u548c\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5728\u590d\u6742\u5c5e\u6027\u56fe\u3001\u52a8\u6001\u62d3\u6251\u548c\u7279\u5f81\u4fe1\u606f\u4ee5\u53ca\u5927\u89c4\u6a21\u6570\u636e\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faL-STEP\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u65f6\u7a7a\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\uff0c\u7ed3\u5408MLP\u7684\u8868\u793a\u80fd\u529b\uff0c\u66ff\u4ee3\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "result": "L-STEP\u572813\u4e2a\u7ecf\u5178\u6570\u636e\u96c6\u548c10\u79cd\u7b97\u6cd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728TGB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9886\u5148\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "L-STEP\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u65f6\u7a7a\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.08956", "pdf": "https://arxiv.org/pdf/2506.08956", "abs": "https://arxiv.org/abs/2506.08956", "authors": ["DaeEun Yoon", "Semin Kim", "SangWook Yoo", "Jongha Lee"], "title": "Data Augmentation For Small Object using Fast AutoAugment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted and published in the USB Proceedings of the 20th\n  International Conference on Modeling Decisions for Artificial Intelligence\n  (MDAI 2023), Ume{\\aa}, Sweden, June 19--22, 2023, ISBN 978-91-527-7293-5,\n  pp.\\ 12--21", "summary": "In recent years, there has been tremendous progress in object detection\nperformance. However, despite these advances, the detection performance for\nsmall objects is significantly inferior to that of large objects. Detecting\nsmall objects is one of the most challenging and important problems in computer\nvision. To improve the detection performance for small objects, we propose an\noptimal data augmentation method using Fast AutoAugment. Through our proposed\nmethod, we can quickly find optimal augmentation policies that can overcome\ndegradation when detecting small objects, and we achieve a 20% performance\nimprovement on the DOTA dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFast AutoAugment\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u8fdc\u4f4e\u4e8e\u5927\u76ee\u6807\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u4f7f\u7528Fast AutoAugment\u5feb\u901f\u627e\u5230\u6700\u4f18\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5728DOTA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8620%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2506.08311", "pdf": "https://arxiv.org/pdf/2506.08311", "abs": "https://arxiv.org/abs/2506.08311", "authors": ["Ira Ceka", "Saurabh Pujar", "Shyam Ramji", "Luca Buratti", "Gail Kaiser", "Baishakhi Ray"], "title": "Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "With the advent of large language models (LLMs), software engineering agents\n(SWE agents) have emerged as a powerful paradigm for automating a range of\nsoftware tasks -- from code generation and repair to test case synthesis. These\nagents operate autonomously by interpreting user input and responding to\nenvironmental feedback. While various agent architectures have demonstrated\nstrong empirical performance, the internal decision-making worfklows that drive\ntheir behavior remain poorly understood. Deeper insight into these workflows\nhold promise for improving both agent reliability and efficiency. In this work,\nwe present the first systematic study of SWE agent behavior through the lens of\nexecution traces. Our contributions are as follows: (1) we propose the first\ntaxonomy of decision-making pathways across five representative agents; (2)\nusing this taxonomy, we identify three core components essential to agent\nsuccess -- bug localization, patch generation, and reproduction test generation\n-- and study each in depth; (3) we study the impact of test generation on\nsuccessful patch production; and analyze strategies that can lead to successful\ntest generation; (4) we further conduct the first large-scale code clone\nanalysis comparing agent-generated and developer-written patches and provide a\nqualitative study revealing structural and stylistic differences in patch\ncontent. Together, these findings offer novel insights into agent design and\nopen avenues for building agents that are both more effective and more aligned\nwith human development practices.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u901a\u8fc7\u6267\u884c\u8f68\u8ff9\u7cfb\u7edf\u7814\u7a76\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\uff08SWE\u4ee3\u7406\uff09\u7684\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u51b3\u7b56\u8def\u5f84\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u4ee3\u7406\u6210\u529f\u7684\u5173\u952e\u7ec4\u4ef6\u53ca\u5176\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\uff0cSWE\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u5185\u90e8\u51b3\u7b56\u673a\u5236\u5c1a\u4e0d\u6e05\u6670\uff0c\u6df1\u5165\u7814\u7a76\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6267\u884c\u8f68\u8ff9\u5206\u6790\uff0c\u63d0\u51faSWE\u4ee3\u7406\u51b3\u7b56\u8def\u5f84\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6df1\u5165\u7814\u7a76\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9519\u8bef\u5b9a\u4f4d\u3001\u8865\u4e01\u751f\u6210\u548c\u6d4b\u8bd5\u751f\u6210\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6d4b\u8bd5\u751f\u6210\u5bf9\u8865\u4e01\u6210\u529f\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u4ee3\u7406\u751f\u6210\u8865\u4e01\u4e0e\u5f00\u53d1\u8005\u7f16\u5199\u8865\u4e01\u5728\u7ed3\u6784\u548c\u98ce\u683c\u4e0a\u7684\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u5f00\u53d1\u5b9e\u8df5\u7684\u4ee3\u7406\u3002"}}
{"id": "2506.08292", "pdf": "https://arxiv.org/pdf/2506.08292", "abs": "https://arxiv.org/abs/2506.08292", "authors": ["Xie Yi", "Zhanke Zhou", "Chentao Cao", "Qiyu Niu", "Tongliang Liu", "Bo Han"], "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Multi-agent frameworks can substantially boost the reasoning power of large\nlanguage models (LLMs), but they typically incur heavy computational costs and\nlack convergence guarantees. To overcome these challenges, we recast multi-LLM\ncoordination as an incomplete-information game and seek a Bayesian Nash\nequilibrium (BNE), in which each agent optimally responds to its probabilistic\nbeliefs about the strategies of others. We introduce Efficient Coordination via\nNash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that\nmarries distributed reasoning with centralized final output. Under ECON, each\nLLM independently selects responses that maximize its expected reward,\nconditioned on its beliefs about co-agents, without requiring costly\ninter-agent exchanges. We mathematically prove that ECON attains a markedly\ntighter regret bound than non-equilibrium multi-agent schemes. Empirically,\nECON outperforms existing multi-LLM approaches by 11.2% on average across six\nbenchmarks spanning complex reasoning and planning tasks. Further experiments\ndemonstrate ECON's ability to flexibly incorporate additional models,\nconfirming its scalability and paving the way toward larger, more powerful\nmulti-LLM ensembles. The code is publicly available at:\nhttps://github.com/tmlr-group/ECON.", "AI": {"tldr": "ECON\u901a\u8fc7\u8d1d\u53f6\u65af\u7eb3\u4ec0\u5747\u8861\u4f18\u5316\u591aLLM\u534f\u4f5c\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u5c06\u591aLLM\u534f\u4f5c\u5efa\u6a21\u4e3a\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\uff0c\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5206\u5e03\u5f0f\u63a8\u7406\u4e0e\u96c6\u4e2d\u8f93\u51fa\u3002", "result": "ECON\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u534711.2%\uff0c\u5e76\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ECON\u4e3a\u66f4\u5f3a\u5927\u7684\u591aLLM\u534f\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08964", "pdf": "https://arxiv.org/pdf/2506.08964", "abs": "https://arxiv.org/abs/2506.08964", "authors": ["Jinwoo Kim", "Sangmin Han", "Jinho Jeong", "Jiwoo Choi", "Dongyoung Kim", "Seon Joo Kim"], "title": "ORIDa: Object-centric Real-world Image Composition Dataset", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Object compositing, the task of placing and harmonizing objects in images of\ndiverse visual scenes, has become an important task in computer vision with the\nrise of generative models. However, existing datasets lack the diversity and\nscale required to comprehensively explore real-world scenarios. We introduce\nORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,\nreal-captured dataset containing over 30,000 images featuring 200 unique\nobjects, each of which is presented across varied positions and scenes. ORIDa\nhas two types of data: factual-counterfactual sets and factual-only scenes. The\nfactual-counterfactual sets consist of four factual images showing an object in\ndifferent positions within a scene and a single counterfactual (or background)\nimage of the scene without the object, resulting in five images per scene. The\nfactual-only scenes include a single image containing an object in a specific\ncontext, expanding the variety of environments. To our knowledge, ORIDa is the\nfirst publicly available dataset with its scale and complexity for real-world\nimage composition. Extensive analysis and experiments highlight the value of\nORIDa as a resource for advancing further research in object compositing.", "AI": {"tldr": "ORIDa\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b30,000\u591a\u5f20\u56fe\u50cf\u548c200\u4e2a\u72ec\u7279\u5bf9\u8c61\uff0c\u652f\u6301\u5bf9\u8c61\u5408\u6210\u4efb\u52a1\u7684\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u89c4\u6a21\uff0c\u65e0\u6cd5\u5168\u9762\u63a2\u7d22\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u5408\u6210\u95ee\u9898\u3002", "method": "ORIDa\u63d0\u4f9b\u4e24\u79cd\u6570\u636e\u7c7b\u578b\uff1a\u4e8b\u5b9e-\u53cd\u4e8b\u5b9e\u96c6\uff08\u6bcf\u7ec45\u5f20\u56fe\u50cf\uff09\u548c\u4e8b\u5b9e\u573a\u666f\uff08\u5355\u5f20\u56fe\u50cf\uff09\uff0c\u6db5\u76d6\u591a\u6837\u73af\u5883\u548c\u5bf9\u8c61\u4f4d\u7f6e\u3002", "result": "ORIDa\u662f\u9996\u4e2a\u516c\u5f00\u7684\u5177\u6709\u5982\u6b64\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5408\u6210\u6570\u636e\u96c6\u3002", "conclusion": "ORIDa\u4e3a\u5bf9\u8c61\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2506.08320", "pdf": "https://arxiv.org/pdf/2506.08320", "abs": "https://arxiv.org/abs/2506.08320", "authors": ["Vivek Vaidya", "Aditya Patwardhan", "Ashish Kundu"], "title": "How Good LLM-Generated Password Policies Are?", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 2 Tables, 9 figures, 3 Algorithms", "summary": "Generative AI technologies, particularly Large Language Models (LLMs), are\nrapidly being adopted across industry, academia, and government sectors, owing\nto their remarkable capabilities in natural language processing. However,\ndespite their strengths, the inconsistency and unpredictability of LLM outputs\npresent substantial challenges, especially in security-critical domains such as\naccess control. One critical issue that emerges prominently is the consistency\nof LLM-generated responses, which is paramount for ensuring secure and reliable\noperations.\n  In this paper, we study the application of LLMs within the context of\nCybersecurity Access Control Systems. Specifically, we investigate the\nconsistency and accuracy of LLM-generated password policies, translating\nnatural language prompts into executable pwquality.conf configuration files.\nOur experimental methodology adopts two distinct approaches: firstly, we\nutilize pre-trained LLMs to generate configuration files purely from natural\nlanguage prompts without additional guidance. Secondly, we provide these models\nwith official pwquality.conf documentation to serve as an informative baseline.\nWe systematically assess the soundness, accuracy, and consistency of these\nAI-generated configurations. Our findings underscore significant challenges in\nthe current generation of LLMs and contribute valuable insights into refining\nthe deployment of LLMs in Access Control Systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\uff08\u7279\u522b\u662fLLMs\uff09\u5728\u7f51\u7edc\u5b89\u5168\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86LLM\u751f\u6210\u5bc6\u7801\u7b56\u7565\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u8bbf\u95ee\u63a7\u5236\uff09\u4e2d\u5e26\u6765\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u4ec5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u914d\u7f6e\u6587\u4ef6\uff1b2\uff09\u63d0\u4f9b\u5b98\u65b9\u6587\u6863\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u7cfb\u7edf\u8bc4\u4f30\u751f\u6210\u914d\u7f6e\u7684\u5408\u7406\u6027\u3001\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLMs\u5728\u751f\u6210\u914d\u7f6e\u65f6\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316LLMs\u5728\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2506.08295", "pdf": "https://arxiv.org/pdf/2506.08295", "abs": "https://arxiv.org/abs/2506.08295", "authors": ["Zhanke Zhou", "Xiao Feng", "Zhaocheng Zhu", "Jiangchao Yao", "Sanmi Koyejo", "Bo Han"], "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "While existing benchmarks probe the reasoning abilities of large language\nmodels (LLMs) across diverse domains, they predominantly assess passive\nreasoning, providing models with all the information needed to reach a\nsolution. By contrast, active reasoning-where an LLM must interact with\nexternal systems to acquire missing evidence or data-has received little\nsystematic attention. To address this shortfall, we present AR-Bench, a novel\nbenchmark designed explicitly to evaluate an LLM's active reasoning skills.\nAR-Bench comprises three task families-detective cases, situation puzzles, and\nguessing numbers-that together simulate real-world, agentic scenarios and\nmeasure performance across commonsense, logical, and symbolic reasoning\nchallenges. Empirical evaluation on AR-Bench demonstrates that contemporary\nLLMs exhibit pronounced difficulties with active reasoning: they frequently\nfail to acquire or leverage the information needed to solve tasks. This gap\nhighlights a stark divergence between their passive and active reasoning\nabilities. Moreover, ablation studies indicate that even advanced strategies,\nsuch as tree-based searching or post-training approaches, yield only modest\ngains and fall short of the levels required for real-world deployment.\nCollectively, these findings highlight the critical need to advance methodology\nfor active reasoning, e.g., incorporating interactive learning, real-time\nfeedback loops, and environment-aware objectives for training. The benchmark is\npublicly available at: https://github.com/tmlr-group/AR-Bench.", "AI": {"tldr": "AR-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u88ab\u52a8\u63a8\u7406\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30LLMs\u7684\u88ab\u52a8\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e3b\u52a8\u63a8\u7406\uff08\u9700\u8981\u4e0e\u5916\u90e8\u7cfb\u7edf\u4ea4\u4e92\u83b7\u53d6\u4fe1\u606f\uff09\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86AR-Bench\uff0c\u5305\u542b\u4e09\u7c7b\u4efb\u52a1\uff08\u4fa6\u63a2\u6848\u4f8b\u3001\u60c5\u5883\u8c1c\u9898\u548c\u731c\u6570\u5b57\uff09\uff0c\u6a21\u62df\u73b0\u5b9e\u573a\u666f\uff0c\u6d4b\u8bd5\u5e38\u8bc6\u3001\u903b\u8f91\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5f53\u4ee3LLMs\u5728\u4e3b\u52a8\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u83b7\u53d6\u6216\u5229\u7528\u5fc5\u8981\u4fe1\u606f\uff0c\u4e14\u73b0\u6709\u6539\u8fdb\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\uff08\u5982\u4ea4\u4e92\u5b66\u4e60\u3001\u5b9e\u65f6\u53cd\u9988\uff09\u63d0\u5347\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\uff0cAR-Bench\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.08968", "pdf": "https://arxiv.org/pdf/2506.08968", "abs": "https://arxiv.org/abs/2506.08968", "authors": ["Amirreza Rouhi", "Solmaz Arezoomandan", "Knut Peterson", "Joseph T. Woods", "David K. Han"], "title": "ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Object detection models typically rely on predefined categories, limiting\ntheir ability to identify novel objects in open-world scenarios. To overcome\nthis constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,\na training-free, self-refining framework for open-world object labeling. ADAM\nleverages large language models (LLMs) to generate candidate labels for unknown\nobjects based on contextual information from known entities within a scene.\nThese labels are paired with visual embeddings from CLIP to construct an\nEmbedding-Label Repository (ELR) that enables inference without category\nsupervision. For a newly encountered unknown object, ADAM retrieves visually\nsimilar instances from the ELR and applies frequency-based voting and\ncross-modal re-ranking to assign a robust label. To further enhance\nconsistency, we introduce a self-refinement loop that re-evaluates repository\nlabels using visual cohesion analysis and k-nearest-neighbor-based majority\nre-labeling. Experimental results on the COCO and PASCAL datasets demonstrate\nthat ADAM effectively annotates novel categories using only visual and\ncontextual signals, without requiring any fine-tuning or retraining.", "AI": {"tldr": "ADAM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u53d1\u73b0\u4e0e\u6807\u6ce8\u6a21\u578b\uff0c\u5229\u7528LLM\u548cCLIP\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u6807\u6ce8\u65b0\u7269\u4f53\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\u3001\u65e0\u6cd5\u8bc6\u522b\u5f00\u653e\u4e16\u754c\u4e2d\u65b0\u7269\u4f53\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408LLM\u751f\u6210\u5019\u9009\u6807\u7b7e\u548cCLIP\u89c6\u89c9\u5d4c\u5165\u6784\u5efaELR\uff0c\u901a\u8fc7\u9891\u7387\u6295\u7968\u548c\u8de8\u6a21\u6001\u91cd\u6392\u5e8f\u6807\u6ce8\u65b0\u7269\u4f53\uff0c\u5e76\u5f15\u5165\u81ea\u4f18\u5316\u5faa\u73af\u63d0\u5347\u4e00\u81f4\u6027\u3002", "result": "\u5728COCO\u548cPASCAL\u6570\u636e\u96c6\u4e0a\uff0cADAM\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6709\u6548\u6807\u6ce8\u65b0\u7c7b\u522b\u3002", "conclusion": "ADAM\u4e3a\u5f00\u653e\u4e16\u754c\u76ee\u6807\u6807\u6ce8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08326", "pdf": "https://arxiv.org/pdf/2506.08326", "abs": "https://arxiv.org/abs/2506.08326", "authors": ["Xingbo Fu", "Zehong Wang", "Zihan Chen", "Jiazheng Li", "Yaochen Zhu", "Zhenyu Lei", "Cong Shen", "Yanfang Ye", "Chuxu Zhang", "Jundong Li"], "title": "Graph Prompting for Graph Learning Models: Recent Advances and Future Directions", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by KDD 2025 Tutorial/Survey Track", "summary": "Graph learning models have demonstrated great prowess in learning expressive\nrepresentations from large-scale graph data in a wide variety of real-world\nscenarios. As a prevalent strategy for training powerful graph learning models,\nthe \"pre-training, adaptation\" scheme first pre-trains graph learning models on\nunlabeled graph data in a self-supervised manner and then adapts them to\nspecific downstream tasks. During the adaptation phase, graph prompting emerges\nas a promising approach that learns trainable prompts while keeping the\npre-trained graph learning models unchanged. In this paper, we present a\nsystematic review of recent advancements in graph prompting. First, we\nintroduce representative graph pre-training methods that serve as the\nfoundation step of graph prompting. Next, we review mainstream techniques in\ngraph prompting and elaborate on how they design learnable prompts for graph\nprompting. Furthermore, we summarize the real-world applications of graph\nprompting from different domains. Finally, we discuss several open challenges\nin existing studies with promising future directions in this field.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u56fe\u63d0\u793a\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u56fe\u9884\u8bad\u7ec3\u65b9\u6cd5\u3001\u4e3b\u6d41\u56fe\u63d0\u793a\u6280\u672f\u53ca\u5176\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u56fe\u5b66\u4e60\u6a21\u578b\u5728\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56fe\u63d0\u793a\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u56fe\u6a21\u578b\uff0c\u968f\u540e\u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u63d0\u793a\uff08prompts\uff09\u4ee5\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u53d8\u3002", "result": "\u603b\u7ed3\u4e86\u56fe\u63d0\u793a\u5b66\u4e60\u7684\u4e3b\u6d41\u6280\u672f\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u56fe\u63d0\u793a\u5b66\u4e60\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u4ecd\u9762\u4e34\u5f00\u653e\u6311\u6218\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2506.08979", "pdf": "https://arxiv.org/pdf/2506.08979", "abs": "https://arxiv.org/abs/2506.08979", "authors": ["Longyu Yang", "Ping Hu", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu"], "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "LiDAR segmentation has emerged as an important task to enrich multimedia\nexperiences and analysis. Range-view-based methods have gained popularity due\nto their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdbLiDAR\u5206\u5272\u7684\u521d\u59cb\u5904\u7406\u6a21\u5757\uff0c\u63d0\u5347\u5176\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8303\u56f4\u89c6\u56fe\u7684LiDAR\u5206\u5272\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u6846\u67b6\uff0c\u5206\u522b\u5904\u7406\u51e0\u4f55\u5c5e\u6027\u548c\u53cd\u5c04\u5f3a\u5ea6\uff0c\u5e76\u5f15\u5165GAS\u548cRDC\u6a21\u5757\u6291\u5236\u566a\u58f0\u548c\u6821\u6b63\u5931\u771f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u9645\u73af\u5883\u4e2d\u7684LiDAR\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08336", "pdf": "https://arxiv.org/pdf/2506.08336", "abs": "https://arxiv.org/abs/2506.08336", "authors": ["Li Changjiang", "Liang Jiacheng", "Cao Bochuan", "Chen Jinghui", "Wang Ting"], "title": "Your Agent Can Defend Itself against Backdoor Attacks", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite their growing adoption across domains, large language model\n(LLM)-powered agents face significant security risks from backdoor attacks\nduring training and fine-tuning. These compromised agents can subsequently be\nmanipulated to execute malicious operations when presented with specific\ntriggers in their inputs or environments. To address this pressing risk, we\npresent ReAgent, a novel defense against a range of backdoor attacks on\nLLM-based agents. Intuitively, backdoor attacks often result in inconsistencies\namong the user's instruction, the agent's planning, and its execution. Drawing\non this insight, ReAgent employs a two-level approach to detect potential\nbackdoors. At the execution level, ReAgent verifies consistency between the\nagent's thoughts and actions; at the planning level, ReAgent leverages the\nagent's capability to reconstruct the instruction based on its thought\ntrajectory, checking for consistency between the reconstructed instruction and\nthe user's instruction. Extensive evaluation demonstrates ReAgent's\neffectiveness against various backdoor attacks across tasks. For instance,\nReAgent reduces the attack success rate by up to 90\\% in database operation\ntasks, outperforming existing defenses by large margins. This work reveals the\npotential of utilizing compromised agents themselves to mitigate backdoor\nrisks.", "AI": {"tldr": "ReAgent\u662f\u4e00\u79cd\u9488\u5bf9LLM\u4ee3\u7406\u540e\u95e8\u653b\u51fb\u7684\u65b0\u578b\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7528\u6237\u6307\u4ee4\u3001\u4ee3\u7406\u89c4\u5212\u548c\u6267\u884c\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u8bad\u7ec3\u548c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u9762\u4e34\u540e\u95e8\u653b\u51fb\u7684\u5b89\u5168\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u6076\u610f\u64cd\u4f5c\u3002", "method": "ReAgent\u91c7\u7528\u4e24\u7ea7\u65b9\u6cd5\uff1a\u6267\u884c\u5c42\u9a8c\u8bc1\u4ee3\u7406\u601d\u7ef4\u4e0e\u884c\u52a8\u7684\u4e00\u81f4\u6027\uff1b\u89c4\u5212\u5c42\u901a\u8fc7\u4ee3\u7406\u91cd\u5efa\u6307\u4ee4\u68c0\u67e5\u4e0e\u7528\u6237\u6307\u4ee4\u7684\u4e00\u81f4\u6027\u3002", "result": "ReAgent\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6709\u6548\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u5982\u5728\u6570\u636e\u5e93\u64cd\u4f5c\u4efb\u52a1\u4e2d\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e90%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u88ab\u653b\u51fb\u4ee3\u7406\u81ea\u8eab\u80fd\u529b\u53ef\u6709\u6548\u7f13\u89e3\u540e\u95e8\u98ce\u9669\u3002"}}
{"id": "2506.08379", "pdf": "https://arxiv.org/pdf/2506.08379", "abs": "https://arxiv.org/abs/2506.08379", "authors": ["Yurun Yuan", "Tengyang Xie"], "title": "Reinforce LLM Reasoning through Multi-Agent Reflection", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "International Conference on Machine Learning (ICML), 2025", "summary": "Leveraging more test-time computation has proven to be an effective way to\nboost the reasoning capabilities of large language models (LLMs). Among various\nmethods, the verify-and-improve paradigm stands out for enabling dynamic\nsolution exploration and feedback incorporation. However, existing approaches\noften suffer from restricted feedback spaces and lack of coordinated training\nof different parties, leading to suboptimal performance. To address this, we\nmodel this multi-turn refinement process as a Markov Decision Process and\nintroduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement\nlearning algorithm that trains an actor-critic LLM system to iteratively refine\nanswers via direct preference learning on self-generated data. Theoretically,\nDPSDP can match the performance of any policy within the training distribution.\nEmpirically, we instantiate DPSDP with various base models and show\nimprovements on both in- and out-of-distribution benchmarks. For example, on\nbenchmark MATH 500, majority voting over five refinement steps increases\nfirst-turn accuracy from 58.2% to 63.2% with Ministral-based models. An\nablation study further confirms the benefits of multi-agent collaboration and\nout-of-distribution generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDPSDP\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u7cfb\u7edf\uff0c\u52a8\u6001\u4f18\u5316\u7b54\u6848\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53cd\u9988\u7a7a\u95f4\u53d7\u9650\u4e14\u7f3a\u4e4f\u534f\u8c03\u8bad\u7ec3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u5c06\u591a\u8f6e\u4f18\u5316\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f15\u5165DPSDP\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u5b66\u4e60\u8fed\u4ee3\u4f18\u5316\u7b54\u6848\u3002", "result": "\u5728MATH 500\u57fa\u51c6\u4e0a\uff0c\u51c6\u786e\u7387\u4ece58.2%\u63d0\u5347\u81f363.2%\u3002", "conclusion": "DPSDP\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u5206\u5e03\u5916\u6cdb\u5316\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.08990", "pdf": "https://arxiv.org/pdf/2506.08990", "abs": "https://arxiv.org/abs/2506.08990", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Dongyun Liang", "Jing Qin", "Liansheng Wang"], "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "TMI 2025", "summary": "Medical vision-language alignment through cross-modal contrastive learning\nshows promising performance in image-text matching tasks, such as retrieval and\nzero-shot classification. However, conventional cross-modal contrastive\nlearning (CLIP-based) methods suffer from suboptimal visual representation\ncapabilities, which also limits their effectiveness in vision-language\nalignment. In contrast, although the models pretrained via multimodal masked\nmodeling struggle with direct cross-modal matching, they excel in visual\nrepresentation. To address this contradiction, we propose ALTA (ALign Through\nAdapting), an efficient medical vision-language alignment method that utilizes\nonly about 8% of the trainable parameters and less than 1/5 of the\ncomputational consumption required for masked record modeling. ALTA achieves\nsuperior performance in vision-language matching tasks like retrieval and\nzero-shot classification by adapting the pretrained vision model from masked\nrecord modeling. Additionally, we integrate temporal-multiview radiograph\ninputs to enhance the information consistency between radiographs and their\ncorresponding descriptions in reports, further improving the vision-language\nalignment. Experimental evaluations show that ALTA outperforms the\nbest-performing counterpart by over 4% absolute points in text-to-image\naccuracy and approximately 6% absolute points in image-to-text retrieval\naccuracy. The adaptation of vision-language models during efficient alignment\nalso promotes better vision and language understanding. Code is publicly\navailable at https://github.com/DopamineLcy/ALTA.", "AI": {"tldr": "ALTA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u89c9\u8868\u793a\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u591a\u6a21\u6001\u63a9\u7801\u5efa\u6a21\u6a21\u578b\u5728\u89c6\u89c9\u8868\u793a\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u96be\u4ee5\u76f4\u63a5\u5339\u914d\u8de8\u6a21\u6001\u3002ALTA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "ALTA\u901a\u8fc7\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\uff08\u6765\u81ea\u63a9\u7801\u8bb0\u5f55\u5efa\u6a21\uff09\uff0c\u7ed3\u5408\u65f6\u95f4\u591a\u89c6\u89d2\u653e\u5c04\u5f71\u50cf\u8f93\u5165\uff0c\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6548\u679c\u3002", "result": "ALTA\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u5206\u522b\u6bd4\u6700\u4f73\u5bf9\u6bd4\u65b9\u6cd5\u9ad8\u51fa4%\u548c6%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u3002", "conclusion": "ALTA\u4e0d\u4ec5\u9ad8\u6548\uff0c\u8fd8\u63d0\u5347\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08344", "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Ne\u015fet \u00dcnver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Ta\u015fk\u0131n Pad\u0131r"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRe4MPC\u7684\u591a\u6a21\u578b\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "method": "\u901a\u8fc7DRL\u6846\u67b6\u5b66\u4e60\u53cd\u5e94\u6027\u51b3\u7b56\u7b56\u7565\uff0c\u52a8\u6001\u9009\u62e9NMPC\u95ee\u9898\u7684\u6a21\u578b\u3001\u6210\u672c\u548c\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRe4MPC\u6bd4\u4f20\u7edfNMPC\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u4e14\u672b\u7aef\u6267\u884c\u5668\u76ee\u6807\u8fbe\u6210\u7387\u66f4\u9ad8\u3002", "conclusion": "Re4MPC\u4e3a\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08388", "pdf": "https://arxiv.org/pdf/2506.08388", "abs": "https://arxiv.org/abs/2506.08388", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "title": "Reinforcement Learning Teachers of Test Time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Reinforcement-Learned Teachers (RLTs)\uff0c\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\uff0c\u4ee5\u4f18\u5316\u4e0b\u6e38\u84b8\u998f\u6548\u679c\uff0c\u907f\u514d\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u578b\u521d\u59cb\u5316\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u4e14\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u8981\u7528\u9014\u662f\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u7528\u4e8e\u84b8\u998f\u65b0\u5b66\u751f\u6a21\u578b\uff0c\u800c\u975e\u76f4\u63a5\u90e8\u7f72\u3002", "method": "RLTs\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\u7684\u63d0\u793a\uff0c\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\uff08\u57fa\u4e8e\u5b66\u751f\u6a21\u578b\u5bf9\u89e3\u91ca\u7684\u7406\u89e3\uff09\u4f18\u5316\u6559\u5e08\u6a21\u578b\u3002", "result": "7B\u89c4\u6a21\u7684RLT\u5728\u7ade\u8d5b\u548c\u7814\u7a76\u751f\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u84b8\u998f\u548c\u51b7\u542f\u52a8\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u5b66\u751f\u6a21\u578b\u548c\u96f6\u6837\u672c\u4efb\u52a1\u3002", "conclusion": "RLTs\u6846\u67b6\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u7684\u6548\u7387\u4e0e\u53ef\u91cd\u7528\u6027\uff0c\u4e3a\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08991", "pdf": "https://arxiv.org/pdf/2506.08991", "abs": "https://arxiv.org/abs/2506.08991", "authors": ["Anudeep Das", "Gurjot Singh", "Prach Chantasantitam", "N. Asokan"], "title": "Do Concept Replacement Techniques Really Erase Unacceptable Concepts?", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Generative models, particularly diffusion-based text-to-image (T2I) models,\nhave demonstrated astounding success. However, aligning them to avoid\ngenerating content with unacceptable concepts (e.g., offensive or copyrighted\ncontent, or celebrity likenesses) remains a significant challenge. Concept\nreplacement techniques (CRTs) aim to address this challenge, often by trying to\n\"erase\" unacceptable concepts from models. Recently, model providers have\nstarted offering image editing services which accept an image and a text prompt\nas input, to produce an image altered as specified by the prompt. These are\nknown as image-to-image (I2I) models. In this paper, we first use an I2I model\nto empirically demonstrate that today's state-of-the-art CRTs do not in fact\nerase unacceptable concepts. Existing CRTs are thus likely to be ineffective in\nemerging I2I scenarios, despite their proven ability to remove unwanted\nconcepts in T2I pipelines, highlighting the need to understand this discrepancy\nbetween T2I and I2I settings. Next, we argue that a good CRT, while replacing\nunacceptable concepts, should preserve other concepts specified in the inputs\nto generative models. We call this fidelity. Prior work on CRTs have neglected\nfidelity in the case of unacceptable concepts. Finally, we propose the use of\ntargeted image-editing techniques to achieve both effectiveness and fidelity.\nWe present such a technique, AntiMirror, and demonstrate its viability.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u6982\u5ff5\u66ff\u6362\u6280\u672f\uff08CRTs\uff09\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5230\u56fe\u50cf\uff08I2I\uff09\u573a\u666f\u4e2d\u7684\u5931\u6548\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5AntiMirror\u4ee5\u63d0\u5347\u6548\u679c\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u5728\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u907f\u514d\u751f\u6210\u4e0d\u53ef\u63a5\u53d7\u5185\u5bb9\uff08\u5982\u4fb5\u6743\u6216\u5192\u72af\u6027\u5185\u5bb9\uff09\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709CRTs\u5728T2I\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u65b0\u5174\u7684I2I\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u5dee\u5f02\u3002", "method": "\u9996\u5148\u901a\u8fc7I2I\u6a21\u578b\u5b9e\u8bc1\u73b0\u6709CRTs\u7684\u5931\u6548\uff0c\u63d0\u51faCRTs\u5e94\u5177\u5907\u4fdd\u771f\u5ea6\uff08\u4fdd\u7559\u8f93\u5165\u4e2d\u7684\u5176\u4ed6\u6982\u5ff5\uff09\uff0c\u5e76\u5f00\u53d1\u4e86AntiMirror\u6280\u672f\u4ee5\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709CRTs\u5728I2I\u4e2d\u65e0\u6cd5\u771f\u6b63\u5220\u9664\u4e0d\u53ef\u63a5\u53d7\u6982\u5ff5\uff0c\u800cAntiMirror\u5728\u66ff\u6362\u4e0d\u53ef\u63a5\u53d7\u5185\u5bb9\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u4ed6\u6982\u5ff5\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "CRTs\u5728I2I\u573a\u666f\u4e2d\u9700\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u517c\u987e\u6548\u679c\u548c\u4fdd\u771f\u5ea6\uff0cAntiMirror\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.08346", "pdf": "https://arxiv.org/pdf/2506.08346", "abs": "https://arxiv.org/abs/2506.08346", "authors": ["Wenhan Yao", "Fen Xiao", "Xiarun Chen", "Jia Liu", "YongQiang He", "Weiping Wen"], "title": "SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted by IJCNN 2025", "summary": "Deep speech classification tasks, including keyword spotting and speaker\nverification, are vital in speech-based human-computer interaction. Recently,\nthe security of these technologies has been revealed to be susceptible to\nbackdoor attacks. Specifically, attackers use noisy disruption triggers and\nspeech element triggers to produce poisoned speech samples that train models to\nbecome vulnerable. However, these methods typically create only a limited\nnumber of backdoors due to the inherent constraints of the trigger function. In\nthis paper, we propose that speech backdoor attacks can strategically focus on\nspeech elements such as timbre and emotion, leveraging the Speech Large\nLanguage Model (SLLM) to generate diverse triggers. Increasing the number of\ntriggers may disproportionately elevate the poisoning rate, resulting in higher\nattack costs and a lower success rate per trigger. We introduce the Multiple\nGradient Descent Algorithm (MGDA) as a mitigation strategy to address this\nchallenge. The proposed attack is called the Speech Prompt Backdoor Attack\n(SPBA). Building on this foundation, we conducted attack experiments on two\nspeech classification tasks, demonstrating that SPBA shows significant trigger\neffectiveness and achieves exceptional performance in attack metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08SLLM\uff09\u7684\u8bed\u97f3\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff08SPBA\uff09\uff0c\u901a\u8fc7\u5229\u7528\u97f3\u8272\u548c\u60c5\u611f\u7b49\u8bed\u97f3\u5143\u7d20\u751f\u6210\u591a\u6837\u5316\u89e6\u53d1\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u56e0\u89e6\u53d1\u5668\u529f\u80fd\u9650\u5236\uff0c\u53ea\u80fd\u751f\u6210\u6709\u9650\u6570\u91cf\u7684\u540e\u95e8\uff0c\u653b\u51fb\u6548\u679c\u53d7\u9650\u3002", "method": "\u5229\u7528SLLM\u751f\u6210\u591a\u6837\u5316\u89e6\u53d1\u5668\uff0c\u5e76\u5f15\u5165\u591a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff08MGDA\uff09\u4f18\u5316\u653b\u51fb\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPBA\u5728\u4e24\u79cd\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u89e6\u53d1\u5668\u6709\u6548\u6027\uff0c\u653b\u51fb\u6307\u6807\u4f18\u5f02\u3002", "conclusion": "SPBA\u901a\u8fc7\u591a\u6837\u5316\u89e6\u53d1\u5668\u548c\u9ad8\u6548\u7387\u653b\u51fb\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u540e\u95e8\u653b\u51fb\u7684\u6548\u679c\u3002"}}
{"id": "2506.08997", "pdf": "https://arxiv.org/pdf/2506.08997", "abs": "https://arxiv.org/abs/2506.08997", "authors": ["Fabian Immel", "Jan-Hendrik Pauls", "Richard Fehler", "Frank Bieder", "Jonas Merkert", "Christoph Stiller"], "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Autonomous vehicles rely on detailed and accurate environmental information\nto operate safely. High definition (HD) maps offer a promising solution, but\ntheir high maintenance cost poses a significant barrier to scalable deployment.\nThis challenge is addressed by online HD map construction methods, which\ngenerate local HD maps from live sensor data. However, these methods are\ninherently limited by the short perception range of onboard sensors. To\novercome this limitation and improve general performance, recent approaches\nhave explored the use of standard definition (SD) maps as prior, which are\nsignificantly easier to maintain. We propose SDTagNet, the first online HD map\nconstruction method that fully utilizes the information of widely available SD\nmaps, like OpenStreetMap, to enhance far range detection accuracy. Our approach\nintroduces two key innovations. First, in contrast to previous work, we\nincorporate not only polyline SD map data with manually selected classes, but\nadditional semantic information in the form of textual annotations. In this\nway, we enrich SD vector map tokens with NLP-derived features, eliminating the\ndependency on predefined specifications or exhaustive class taxonomies. Second,\nwe introduce a point-level SD map encoder together with orthogonal element\nidentifiers to uniformly integrate all types of map elements. Experiments on\nArgoverse 2 and nuScenes show that this boosts map perception performance by up\nto +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP\n(+20%) w.r.t. previous approaches that already use SD map priors. Code is\navailable at https://github.com/immel-f/SDTagNet", "AI": {"tldr": "SDTagNet\u662f\u4e00\u79cd\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u51c6\u5730\u56fe\uff08SD\uff09\u4fe1\u606f\u63d0\u5347\u8fdc\u8ddd\u79bb\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u7ed3\u5408\u8bed\u4e49\u6587\u672c\u548c\u70b9\u7ea7\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9ad8\u7cbe\u5730\u56fe\u7ef4\u62a4\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u5728\u7ebf\u6784\u5efa\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u611f\u77e5\u8303\u56f4\uff0cSD\u5730\u56fe\u4f5c\u4e3a\u5148\u9a8c\u4fe1\u606f\u53ef\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7ed3\u5408SD\u5730\u56fe\u7684\u6298\u7ebf\u6570\u636e\u548c\u8bed\u4e49\u6587\u672c\uff0c\u5f15\u5165\u70b9\u7ea7\u7f16\u7801\u5668\u548c\u6b63\u4ea4\u5143\u7d20\u6807\u8bc6\u7b26\uff0c\u7edf\u4e00\u6574\u5408\u5404\u7c7b\u5730\u56fe\u5143\u7d20\u3002", "result": "\u5728Argoverse 2\u548cnuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u6700\u9ad8\u8fbe+5.9 mAP\uff08+45%\uff09\u3002", "conclusion": "SDTagNet\u901a\u8fc7\u5145\u5206\u5229\u7528SD\u5730\u56fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.08572", "pdf": "https://arxiv.org/pdf/2506.08572", "abs": "https://arxiv.org/abs/2506.08572", "authors": ["Waiss Azizian", "Michael Kirchhof", "Eugene Ndiaye", "Louis Bethune", "Michal Klein", "Pierre Ablin", "Marco Cuturi"], "title": "The Geometries of Truth Are Orthogonal Across Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive generalization\ncapabilities across various tasks, but their claim to practical relevance is\nstill mired by concerns on their reliability. Recent works have proposed\nexamining the activations produced by an LLM at inference time to assess\nwhether its answer to a question is correct. Some works claim that a \"geometry\nof truth\" can be learned from examples, in the sense that the activations that\ngenerate correct answers can be distinguished from those leading to mistakes\nwith a linear classifier. In this work, we underline a limitation of these\napproaches: we observe that these \"geometries of truth\" are intrinsically\ntask-dependent and fail to transfer across tasks. More precisely, we show that\nlinear classifiers trained across distinct tasks share little similarity and,\nwhen trained with sparsity-enforcing regularizers, have almost disjoint\nsupports. We show that more sophisticated approaches (e.g., using mixtures of\nprobes and tasks) fail to overcome this limitation, likely because activation\nvectors commonly used to classify answers form clearly separated clusters when\nexamined across tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u771f\u7406\u51e0\u4f55\u201d\u65b9\u6cd5\u5728\u4efb\u52a1\u95f4\u7f3a\u4e4f\u53ef\u8fc1\u79fb\u6027\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u5c3d\u7ba1LLMs\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u53ef\u9760\u6027\u4ecd\u53d7\u8d28\u7591\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6fc0\u6d3b\u72b6\u6001\u5224\u65ad\u7b54\u6848\u6b63\u786e\u6027\uff0c\u4f46\u672c\u7814\u7a76\u6307\u51fa\u5176\u4efb\u52a1\u4f9d\u8d56\u6027\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c1d\u8bd5\u4f7f\u7528\u7a00\u758f\u6b63\u5219\u5316\u548c\u6df7\u5408\u63a2\u9488\u7b49\u65b9\u6cd5\u6539\u8fdb\uff0c\u4f46\u6548\u679c\u6709\u9650\u3002", "result": "\u53d1\u73b0\u201c\u771f\u7406\u51e0\u4f55\u201d\u65b9\u6cd5\u5728\u4efb\u52a1\u95f4\u65e0\u6cd5\u8fc1\u79fb\uff0c\u5206\u7c7b\u5668\u652f\u6301\u96c6\u51e0\u4e4e\u4e0d\u91cd\u53e0\uff0c\u6fc0\u6d3b\u5411\u91cf\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5f62\u6210\u660e\u663e\u5206\u79bb\u7684\u7c07\u3002", "conclusion": "\u73b0\u6709\u65b9\u6cd5\u5728\u4efb\u52a1\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u901a\u7528\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2506.09022", "pdf": "https://arxiv.org/pdf/2506.09022", "abs": "https://arxiv.org/abs/2506.09022", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "title": "Do MIL Models Transfer?", "categories": ["cs.CV"], "comment": "ICML 2025 (Spotlight). 20 pages, 8 figures", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u4e0d\u540c\u5668\u5b98\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5b9e\u73b0\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2dMIL\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u5f31\u76d1\u7763\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63a2\u7d22\u5176\u8fc1\u79fb\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f3011\u4e2a\u9884\u8bad\u7ec3MIL\u6a21\u578b\u572821\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7528\u4e8e\u5f62\u6001\u5b66\u548c\u5206\u5b50\u4e9a\u578b\u9884\u6d4b\u3002", "result": "\u9884\u8bad\u7ec3MIL\u6a21\u578b\u5728\u4e0d\u540c\u5668\u5b98\u548c\u76ee\u6807\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "MIL\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u75c5\u7406\u5b66\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8d44\u6e90\u3002"}}
{"id": "2506.08633", "pdf": "https://arxiv.org/pdf/2506.08633", "abs": "https://arxiv.org/abs/2506.08633", "authors": ["\u0160imon Sedl\u00e1\u010dek", "Bolaji Yusuf", "J\u00e1n \u0160vec", "Pradyoth Hegde", "Santosh Kesiraju", "Old\u0159ich Plchot", "Jan \u010cernock\u00fd"], "title": "Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "In this work, we approach spoken Dialogue State Tracking (DST) by bridging\nthe representation spaces of speech encoders and LLMs via a small connector\nmodule, with a focus on fully open-sourced and open-data components\n(WavLM-large, OLMo). We focus on ablating different aspects of such systems\nincluding full/LoRA adapter fine-tuning, the effect of agent turns in the\ndialogue history, as well as fuzzy matching-based output post-processing, which\ngreatly improves performance of our systems on named entities in the dialogue\nslot values. We conduct our experiments on the SpokenWOZ dataset, and\nadditionally utilize the Speech-Aware MultiWOZ dataset to augment our training\ndata. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned\nmodels achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our\nsystem with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%\nJGA on SpokenWOZ test.", "AI": {"tldr": "\u901a\u8fc7\u8fde\u63a5\u8bed\u97f3\u7f16\u7801\u5668\u548cLLM\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5f00\u6e90\u548c\u5f00\u653e\u6570\u636e\u7684\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5e76\u5728SpokenWOZ\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u95ee\u9898\uff0c\u540c\u65f6\u5f3a\u8c03\u5b8c\u5168\u5f00\u6e90\u548c\u5f00\u653e\u6570\u636e\u7684\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528WavLM-large\u548cOLMo\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5c0f\u578b\u8fde\u63a5\u6a21\u5757\u6865\u63a5\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u3001\u5bf9\u8bdd\u5386\u53f2\u4e2d\u4ee3\u7406\u8f6e\u6b21\u7684\u5f71\u54cd\u4ee5\u53ca\u6a21\u7cca\u5339\u914d\u540e\u5904\u7406\u3002", "result": "\u5728SpokenWOZ\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523034.66% JGA\uff0c\u4f7f\u7528Gemma-2-9B-instruct\u8fdb\u4e00\u6b65\u63d0\u5347\u81f342.17% JGA\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u97f3\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u793a\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09024", "pdf": "https://arxiv.org/pdf/2506.09024", "abs": "https://arxiv.org/abs/2506.09024", "authors": ["Felix Wagner", "Pramit Saha", "Harry Anthony", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "categories": ["cs.CV", "cs.LG", "I.2.11; I.4.9; I.4.9; J.3; I.2.0"], "comment": null, "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard training data\nafter deployment or assume that test samples and training data are centrally\nstored together, an assumption that rarely holds in real-world settings. This\nis because shipping training data with the deployed model is usually impossible\ndue to the size of training databases, as well as proprietary or privacy\nconstraints. We introduce the Isolation Network, an OOD detection framework\nthat quantifies the difficulty of separating a target test sample from the\ntraining data by solving a binary classification task. We then propose\nDecentralized Isolation Networks (DIsoN), which enables the comparison of\ntraining and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code will be available upon\nacceptance at: *****", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIsoN\u7684\u53bb\u4e2d\u5fc3\u5316OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u65e0\u6cd5\u5171\u4eab\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u4ea4\u6362\u6a21\u578b\u53c2\u6570\u5b9e\u73b0\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u6570\u636e\u7684\u6bd4\u8f83\uff0c\u5e76\u5728\u533b\u7597\u5f71\u50cf\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u533b\u7597\u5f71\u50cf\uff09\u90e8\u7f72ML\u6a21\u578b\u65f6\uff0c\u9700\u8981\u68c0\u6d4b\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u8f93\u5165\uff08OOD\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4e22\u5f03\u8bad\u7ec3\u6570\u636e\uff0c\u8981\u4e48\u5047\u8bbe\u6570\u636e\u96c6\u4e2d\u5b58\u50a8\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86Isolation Network\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u4e8c\u5206\u7c7b\u4efb\u52a1\u91cf\u5316\u6d4b\u8bd5\u6837\u672c\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u5206\u79bb\u96be\u5ea6\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3aDIsoN\uff0c\u652f\u6301\u65e0\u6570\u636e\u5171\u4eab\u7684\u6a21\u578b\u53c2\u6570\u4ea4\u6362\u3002", "result": "\u5728\u56db\u4e2a\u533b\u7597\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u768412\u4e2aOOD\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cDIsoN\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "conclusion": "DIsoN\u4e3aML\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u670d\u52a1\u6a21\u5f0f\uff0c\u5373\u8fdc\u7a0b\u3001\u5b89\u5168\u5730\u5229\u7528\u8bad\u7ec3\u6570\u636e\u8fdb\u884cOOD\u68c0\u6d4b\u3002"}}
{"id": "2506.09027", "pdf": "https://arxiv.org/pdf/2506.09027", "abs": "https://arxiv.org/abs/2506.09027", "authors": ["Runqian Wang", "Kaiming He"], "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDispersive Loss\u7684\u7b80\u5355\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u53c2\u6570\u3002", "motivation": "\u6269\u6563\u751f\u6210\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u663e\u5f0f\u6b63\u5219\u5316\uff0c\u4e14\u4e0e\u8868\u793a\u5b66\u4e60\u8fdb\u5c55\u72ec\u7acb\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7Dispersive Loss\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faDispersive Loss\uff0c\u9f13\u52b1\u9690\u7a7a\u95f4\u4e2d\u7684\u5185\u90e8\u8868\u793a\u5206\u6563\uff0c\u7c7b\u4f3c\u4e8e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u4f46\u65e0\u9700\u6b63\u6837\u672c\u5bf9\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cDispersive Loss\u5728\u591a\u79cd\u6a21\u578b\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Dispersive Loss\u6709\u6548\u6539\u8fdb\u4e86\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u6709\u671b\u8fde\u63a5\u751f\u6210\u5efa\u6a21\u4e0e\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2506.08357", "pdf": "https://arxiv.org/pdf/2506.08357", "abs": "https://arxiv.org/abs/2506.08357", "authors": ["Franck Meyer", "Kyunghoon Hur", "Edward Choi"], "title": "MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Main paper (16 pages, 5 figures). Paper submitted for review. Code\n  available at https://github.com/fr-meyer/MD-ViSCo", "summary": "Despite the remarkable progress of deep-learning methods generating a target\nvital sign waveform from a source vital sign waveform, most existing models are\ndesigned exclusively for a specific source-to-target pair. This requires\ndistinct model architectures, optimization procedures, and pre-processing\npipelines, resulting in multiple models that hinder usability in clinical\nsettings. To address this limitation, we propose the Multi-Directional\nVital-Sign Converter (MD-ViSCo), a unified framework capable of generating any\ntarget waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or\narterial blood pressure (ABP) from any single input waveform with a single\nmodel. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin\nTransformer that leverages Adaptive Instance Normalization (AdaIN) to capture\ndistinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct\nmulti-directional waveform generation on two publicly available datasets. Our\nframework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average\nacross all waveform types, lowering Mean absolute error (MAE) by 8.8% and\nimproving Pearson correlation (PC) by 4.9% over two datasets. In addition, the\ngenerated ABP waveforms satisfy the Association for the Advancement of Medical\nInstrumentation (AAMI) criterion and achieve Grade B on the British\nHypertension Society (BHS) standard, outperforming all baselines. By\neliminating the need for developing a distinct model for each task, we believe\nthat this work offers a unified framework that can deal with any kind of vital\nsign waveforms with a single model in healthcare monitoring.", "AI": {"tldr": "MD-ViSCo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u8f93\u5165\u6ce2\u5f62\u751f\u6210\u591a\u79cd\u76ee\u6807\u6ce2\u5f62\uff08\u5982ECG\u3001PPG\u6216ABP\uff09\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u79cd\u6ce2\u5f62\u5bf9\u8bbe\u8ba1\u72ec\u7acb\u6a21\u578b\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u6ce2\u5f62\u5bf9\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u9700\u8981\u591a\u4e2a\u6a21\u578b\u548c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u4fbf\u5229\u6027\u3002", "method": "MD-ViSCo\u7ed3\u5408\u4e861-D U-Net\u548cSwin Transformer\uff0c\u5229\u7528AdaIN\u6355\u6349\u6ce2\u5f62\u98ce\u683c\uff0c\u5b9e\u73b0\u591a\u65b9\u5411\u6ce2\u5f62\u751f\u6210\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cMD-ViSCo\u5e73\u5747\u964d\u4f4e\u4e86MAE 8.8%\uff0c\u63d0\u9ad8\u4e86PC 4.9%\uff0c\u751f\u6210\u7684ABP\u6ce2\u5f62\u6ee1\u8db3AAMI\u548cBHS\u6807\u51c6\u3002", "conclusion": "MD-ViSCo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7528\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u79cd\u751f\u547d\u4f53\u5f81\u6ce2\u5f62\uff0c\u63d0\u5347\u4e86\u533b\u7597\u76d1\u6d4b\u7684\u6548\u7387\u3002"}}
{"id": "2506.08762", "pdf": "https://arxiv.org/pdf/2506.08762", "abs": "https://arxiv.org/abs/2506.08762", "authors": ["Issa Sugiura", "Takashi Ishida", "Taro Makino", "Chieko Tazuke", "Takanori Nakagawa", "Kosuke Nakago", "David Ha"], "title": "EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements", "categories": ["q-fin.ST", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "Financial analysis presents complex challenges that could leverage large\nlanguage model (LLM) capabilities. However, the scarcity of challenging\nfinancial datasets, particularly for Japanese financial data, impedes academic\ninnovation in financial analytics. As LLMs advance, this lack of accessible\nresearch resources increasingly hinders their development and evaluation in\nthis specialized domain. To address this gap, we introduce EDINET-Bench, an\nopen-source Japanese financial benchmark designed to evaluate the performance\nof LLMs on challenging financial tasks including accounting fraud detection,\nearnings forecasting, and industry prediction. EDINET-Bench is constructed by\ndownloading annual reports from the past 10 years from Japan's Electronic\nDisclosure for Investors' NETwork (EDINET) and automatically assigning labels\ncorresponding to each evaluation task. Our experiments reveal that even\nstate-of-the-art LLMs struggle, performing only slightly better than logistic\nregression in binary classification for fraud detection and earnings\nforecasting. These results highlight significant challenges in applying LLMs to\nreal-world financial applications and underscore the need for domain-specific\nadaptation. Our dataset, benchmark construction code, and evaluation code is\npublicly available to facilitate future research in finance with LLMs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86EDINET-Bench\uff0c\u4e00\u4e2a\u5f00\u6e90\u65e5\u672c\u91d1\u878d\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u91d1\u878d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5982\u4f1a\u8ba1\u6b3a\u8bc8\u68c0\u6d4b\u3001\u76c8\u5229\u9884\u6d4b\u548c\u884c\u4e1a\u9884\u6d4b\u3002", "motivation": "\u91d1\u878d\u5206\u6790\u9886\u57df\u7f3a\u4e4f\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u65e5\u8bed\u91d1\u878d\u6570\u636e\uff0c\u963b\u788d\u4e86\u5b66\u672f\u521b\u65b0\u548cLLM\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4ece\u65e5\u672cEDINET\u7cfb\u7edf\u4e0b\u8f7d\u8fc7\u53bb10\u5e74\u7684\u5e74\u62a5\uff0c\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u6807\u7b7e\uff0c\u6784\u5efaEDINET-Bench\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684LLM\u5728\u6b3a\u8bc8\u68c0\u6d4b\u548c\u76c8\u5229\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4ec5\u7565\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u3002", "conclusion": "LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u9886\u57df\u7279\u5b9a\u4f18\u5316\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.09035", "pdf": "https://arxiv.org/pdf/2506.09035", "abs": "https://arxiv.org/abs/2506.09035", "authors": ["Karhan Kayan", "Stamatis Alexandropoulos", "Rishabh Jain", "Yiming Zuo", "Erich Liang", "Jia Deng"], "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Princeton365, a large-scale diverse dataset of 365 videos with\naccurate camera pose. Our dataset bridges the gap between accuracy and data\ndiversity in current SLAM benchmarks by introducing a novel ground truth\ncollection framework that leverages calibration boards and a 360-camera. We\ncollect indoor, outdoor, and object scanning videos with synchronized monocular\nand stereo RGB video outputs as well as IMU. We further propose a new scene\nscale-aware evaluation metric for SLAM based on the the optical flow induced by\nthe camera pose estimation error. In contrast to the current metrics, our new\nmetric allows for comparison between the performance of SLAM methods across\nscenes as opposed to existing metrics such as Average Trajectory Error (ATE),\nallowing researchers to analyze the failure modes of their methods. We also\npropose a challenging Novel View Synthesis benchmark that covers cases not\ncovered by current NVS benchmarks, such as fully non-Lambertian scenes with\n360-degree camera trajectories. Please visit\nhttps://princeton365.cs.princeton.edu for the dataset, code, videos, and\nsubmission.", "AI": {"tldr": "Princeton365\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b365\u4e2a\u5e26\u6709\u7cbe\u786e\u76f8\u673a\u59ff\u6001\u7684\u89c6\u9891\uff0c\u586b\u8865\u4e86\u5f53\u524dSLAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7cbe\u5ea6\u4e0e\u6570\u636e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SLAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7cbe\u5ea6\u4e0e\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6311\u6218\u6027\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u6821\u51c6\u677f\u548c360\u76f8\u673a\u6536\u96c6\u6570\u636e\uff0c\u63d0\u51fa\u57fa\u4e8e\u5149\u6d41\u7684\u573a\u666f\u5c3a\u5ea6\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u65b0\u89c6\u89d2\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u5ba4\u5185\u3001\u5ba4\u5916\u548c\u7269\u4f53\u626b\u63cf\u89c6\u9891\uff0c\u63d0\u4f9b\u5355\u76ee\u3001\u7acb\u4f53RGB\u89c6\u9891\u548cIMU\u6570\u636e\uff0c\u652f\u6301\u8de8\u573a\u666fSLAM\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "Princeton365\u4e3aSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6570\u636e\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5206\u6790\u65b9\u6cd5\u7684\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2506.09040", "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.", "AI": {"tldr": "ASVR\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u56fe\u50cf\u7684\u8bed\u4e49\u8868\u793a\u800c\u975e\u539f\u59cb\u5916\u89c2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4ec5\u5bf9\u6587\u672c\u5e8f\u5217\u8fdb\u884c\u81ea\u56de\u5f52\u76d1\u7763\uff0c\u672a\u5145\u5206\u5229\u7528\u89c6\u89c9\u6a21\u6001\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5904\u7406\u65e0\u6807\u6ce8\u56fe\u50cf\u3001\u9057\u6f0f\u89c6\u89c9\u7ec6\u8282\u53ca\u96be\u4ee5\u8868\u8fbe\u89c6\u89c9\u5185\u5bb9\u3002", "method": "\u5f15\u5165\u81ea\u56de\u5f52\u8bed\u4e49\u89c6\u89c9\u91cd\u5efa\uff08ASVR\uff09\uff0c\u5728\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u6846\u67b6\u4e2d\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u91cd\u5efa\u56fe\u50cf\u7684\u8bed\u4e49\u8868\u793a\u800c\u975e\u539f\u59cb\u5916\u89c2\u3002", "result": "ASVR\u5728\u591a\u79cd\u6570\u636e\u89c4\u6a21\u548cLLM\u9aa8\u5e72\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u6027\u80fd\uff0c\u5982LLaVA-1.5\u572814\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53475%\u3002", "conclusion": "\u81ea\u56de\u5f52\u91cd\u5efa\u8bed\u4e49\u8868\u793a\u80fd\u7a33\u5b9a\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\uff0c\u800c\u91cd\u5efa\u539f\u59cb\u5916\u89c2\u5219\u65e0\u76ca\u751a\u81f3\u6709\u5bb3\u3002ASVR\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2506.09042", "pdf": "https://arxiv.org/pdf/2506.09042", "abs": "https://arxiv.org/abs/2506.09042", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "categories": ["cs.CV"], "comment": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao: Equal contribution.\n  Only the core contributors are listed. The full list of contributors can be\n  found in Appendix A of this paper", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "AI": {"tldr": "Cosmos-Drive-Dreams\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\uff08SDG\uff09\u7ba1\u9053\uff0c\u65e8\u5728\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\uff08AV\uff09\u7cfb\u7edf\u7684\u611f\u77e5\u548c\u9a7e\u9a76\u7b56\u7565\u8bad\u7ec3\u3002", "motivation": "\u7531\u4e8e\u6536\u96c6\u548c\u6807\u6ce8\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7528\u4e8e\u5b89\u5168\u5173\u952e\u7684\u7269\u7406AI\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u5c24\u5176\u662f\u96be\u4ee5\u6355\u6349\u7f55\u89c1\u7684\u8fb9\u7f18\u6848\u4f8b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5229\u7528NVIDIA Cosmos\u4e16\u754c\u57fa\u7840\u6a21\u578b\uff0c\u5f00\u53d1\u4e86Cosmos-Drive\uff0c\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u3001\u591a\u89c6\u89d2\u4e14\u65f6\u7a7a\u4e00\u81f4\u7684\u9a7e\u9a76\u89c6\u9891\u7684\u6a21\u578b\u5957\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684\u6570\u636e\u6709\u52a9\u4e8e\u7f13\u89e3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e863D\u8f66\u9053\u68c0\u6d4b\u30013D\u7269\u4f53\u68c0\u6d4b\u548c\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Cosmos-Drive-Dreams\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u5305\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09045", "pdf": "https://arxiv.org/pdf/2506.09045", "abs": "https://arxiv.org/abs/2506.09045", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "categories": ["cs.CV"], "comment": "Project Page: https://zehong-ma.github.io/MagCache", "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u4e00\u5e45\u5ea6\u89c4\u5f8b\u7684\u81ea\u9002\u5e94\u7f13\u5b58\u7b56\u7565\uff08MagCache\uff09\uff0c\u663e\u8457\u52a0\u901f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u8df3\u8fc7\u65f6\u95f4\u6b65\u6216\u7f13\u5b58\u7279\u5f81\uff0c\u9700\u5927\u91cf\u6821\u51c6\u4e14\u6613\u56e0\u63d0\u793a\u8fc7\u62df\u5408\u5bfc\u81f4\u8f93\u51fa\u4e0d\u4e00\u81f4\u3002", "method": "\u53d1\u73b0\u6b8b\u5dee\u8f93\u51fa\u5e45\u5ea6\u6bd4\u5355\u8c03\u9012\u51cf\u89c4\u5f8b\uff0c\u8bbe\u8ba1MagCache\u81ea\u9002\u5e94\u8df3\u8fc7\u4e0d\u91cd\u8981\u65f6\u95f4\u6b65\uff0c\u4ec5\u9700\u5355\u6837\u672c\u6821\u51c6\u3002", "result": "\u5728Open-Sora\u548cWan 2.1\u4e0a\u5206\u522b\u5b9e\u73b02.1x\u548c2.68x\u52a0\u901f\uff0cLPIPS\u3001SSIM\u548cPSNR\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MagCache\u9ad8\u6548\u4e14\u9c81\u68d2\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.08397", "pdf": "https://arxiv.org/pdf/2506.08397", "abs": "https://arxiv.org/abs/2506.08397", "authors": ["Vamshika Sutar", "Amandeep Singh", "Rohitash Chandra"], "title": "Spatiotemporal deep learning models for detection of rapid intensification in cyclones", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Cyclone rapid intensification is the rapid increase in cyclone wind\nintensity, exceeding a threshold of 30 knots, within 24 hours. Rapid\nintensification is considered an extreme event during a cyclone, and its\noccurrence is relatively rare, contributing to a class imbalance in the\ndataset. A diverse array of factors influences the likelihood of a cyclone\nundergoing rapid intensification, further complicating the task for\nconventional machine learning models. In this paper, we evaluate deep learning,\nensemble learning and data augmentation frameworks to detect cyclone rapid\nintensification based on wind intensity and spatial coordinates. We note that\nconventional data augmentation methods cannot be utilised for generating\nspatiotemporal patterns replicating cyclones that undergo rapid\nintensification. Therefore, our framework employs deep learning models to\ngenerate spatial coordinates and wind intensity that replicate cyclones to\naddress the class imbalance problem of rapid intensification. We also use a\ndeep learning model for the classification module within the data augmentation\nframework to differentiate between rapid and non-rapid intensification events\nduring a cyclone. Our results show that data augmentation improves the results\nfor rapid intensification detection in cyclones, and spatial coordinates play a\ncritical role as input features to the given models. This paves the way for\nresearch in synthetic data generation for spatiotemporal data with extreme\nevents.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u6c14\u65cb\u5feb\u901f\u589e\u5f3a\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u5750\u6807\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6c14\u65cb\u5feb\u901f\u589e\u5f3a\u662f\u4e00\u79cd\u6781\u7aef\u4e14\u7f55\u89c1\u7684\u4e8b\u4ef6\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5904\u7406\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u96c6\u6210\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u751f\u6210\u6a21\u62df\u6c14\u65cb\u7684\u7a7a\u95f4\u5750\u6807\u548c\u98ce\u5f3a\u5ea6\u6570\u636e\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u5feb\u901f\u589e\u5f3a\u4e8b\u4ef6\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u7a7a\u95f4\u5750\u6807\u662f\u5173\u952e\u8f93\u5165\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65f6\u7a7a\u6570\u636e\u4e2d\u6781\u7aef\u4e8b\u4ef6\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08019", "pdf": "https://arxiv.org/pdf/2506.08019", "abs": "https://arxiv.org/abs/2506.08019", "authors": ["Andrew Wells", "Geraldine Henningsen", "Brice Bolane Tchinde Kengne"], "title": "Gridding Forced Displacement using Semi-Supervised Learning", "categories": ["cs.LG", "cs.CV", "cs.CY"], "comment": null, "summary": "We present a semi-supervised approach that disaggregates refugee statistics\nfrom administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan\nAfrican countries. By integrating UNHCR's ProGres registration data with\nsatellite-derived building footprints from Google Open Buildings and location\ncoordinates from OpenStreetMap Populated Places, our label spreading algorithm\ncreates spatially explicit refugee statistics at high granularity.This\nmethodology achieves 92.9% average accuracy in placing over 10 million refugee\nobservations into appropriate grid cells, enabling the identification of\nlocalized displacement patterns previously obscured in broader regional and\nnational statistics. The resulting high-resolution dataset provides a\nfoundation for a deeper understanding of displacement drivers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5c06\u96be\u6c11\u7edf\u8ba1\u6570\u636e\u4ece\u884c\u653f\u8fb9\u754c\u5206\u89e3\u52300.5\u5ea6\u7f51\u683c\u5355\u5143\uff0c\u8986\u76d625\u4e2a\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u56fd\u5bb6\uff0c\u51c6\u786e\u7387\u8fbe92.9%\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u533a\u57df\u548c\u56fd\u5bb6\u7edf\u8ba1\u6570\u636e\u4e2d\u96be\u6c11\u5206\u5e03\u6a21\u5f0f\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u96be\u6c11\u5206\u5e03\u6570\u636e\u3002", "method": "\u7ed3\u5408UNHCR\u7684ProGres\u6ce8\u518c\u6570\u636e\u3001Google Open Buildings\u7684\u5efa\u7b51\u8db3\u8ff9\u548cOpenStreetMap\u7684\u5730\u70b9\u5750\u6807\uff0c\u4f7f\u7528\u6807\u7b7e\u4f20\u64ad\u7b97\u6cd5\u751f\u6210\u9ad8\u7cbe\u5ea6\u96be\u6c11\u7edf\u8ba1\u6570\u636e\u3002", "result": "\u6210\u529f\u5c06\u8d85\u8fc71000\u4e07\u96be\u6c11\u89c2\u6d4b\u6570\u636e\u5206\u914d\u5230\u7f51\u683c\u5355\u5143\uff0c\u8bc6\u522b\u51fa\u5c40\u90e8\u6d41\u79bb\u5931\u6240\u6a21\u5f0f\u3002", "conclusion": "\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e3a\u6df1\u5165\u7814\u7a76\u6d41\u79bb\u5931\u6240\u7684\u9a71\u52a8\u56e0\u7d20\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.08967", "pdf": "https://arxiv.org/pdf/2506.08967", "abs": "https://arxiv.org/abs/2506.08967", "authors": ["Ailin Huang", "Bingxin Li", "Bruce Wang", "Boyong Wu", "Chao Yan", "Chengli Feng", "Heng Wang", "Hongyu Zhou", "Hongyuan Wang", "Jingbei Li", "Jianjian Sun", "Joanna Wang", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Shilei Jiang", "Tian Fei", "Wang You", "Xi Chen", "Xuerui Yang", "Yechang Huang", "Yuxiang Zhang", "Zheng Ge", "Zheng Gong", "Zhewei Huang", "Zixin Zhang", "Bin Wang", "Bo Li", "Buyun Ma", "Changxin Miao", "Changyi Wan", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Hanpeng Hu", "Haonan Jia", "Jiahao Gong", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Junzhe Lin", "Kaixiang Li", "Lei Xia", "Longlong Gu", "Ming Li", "Nie Hao", "Ranchen Ming", "Shaoliang Pang", "Siqi Liu", "Song Yuan", "Tiancheng Cao", "Wen Li", "Wenqing He", "Xu Zhao", "Xuelin Zhang", "Yanbo Yu", "Yinmin Zhong", "Yu Zhou", "Yuanwei Liang", "Yuanwei Lu", "Yuxiang Yang", "Zidong Yang", "Zili Zhang", "Binxing Jiao", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Xinhao Zhang", "Yibo Zhu", "Daxin Jiang", "Shuchang Zhou", "Chen Hu"], "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "12 pages, 3 figures", "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.", "AI": {"tldr": "Step-Audio-AQAA\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684LALM\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u7801\u672c\u97f3\u9891\u5206\u8bcd\u5668\u548c1300\u4ebf\u53c2\u6570LLM\u5b9e\u73b0\u97f3\u9891\u67e5\u8be2-\u97f3\u9891\u56de\u7b54\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LALM\u4f9d\u8d56\u6587\u672c\u8f93\u51fa\uff0c\u65e0\u6cd5\u76f4\u63a5\u751f\u6210\u81ea\u7136\u8bed\u97f3\uff0c\u9650\u5236\u4e86\u97f3\u9891\u4ea4\u4e92\u7684\u6d41\u7545\u6027\u3002", "method": "\u6a21\u578b\u6574\u5408\u53cc\u7801\u672c\u97f3\u9891\u5206\u8bcd\u5668\u3001\u5927\u53c2\u6570LLM\u548c\u795e\u7ecf\u58f0\u7801\u5668\uff0c\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u548cDPO\u4f18\u5316\u3002", "result": "\u5728StepEval-Audio-360\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8bed\u97f3\u63a7\u5236\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7aef\u5230\u7aefLALM\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5e76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5206\u8bcd\u7684\u58f0\u7801\u5668\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2506.08417", "pdf": "https://arxiv.org/pdf/2506.08417", "abs": "https://arxiv.org/abs/2506.08417", "authors": ["Qingmao Yao", "Zhichao Lei", "Tianyuan Chen", "Ziyue Yuan", "Xuefan Chen", "Jianxiang Liu", "Faguo Wu", "Xiao Zhang"], "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "Offline Reinforcement Learning (RL) struggles with distributional shifts,\nleading to the $Q$-value overestimation for out-of-distribution (OOD) actions.\nExisting methods address this issue by imposing constraints; however, they\noften become overly conservative when evaluating OOD regions, which constrains\nthe $Q$-function generalization. This over-constraint issue results in poor\n$Q$-value estimation and hinders policy improvement. In this paper, we\nintroduce a novel approach to achieve better $Q$-value estimation by enhancing\n$Q$-function generalization in OOD regions within Convex Hull and its\nNeighborhood (CHN). Under the safety generalization guarantees of the CHN, we\npropose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by\nsmoothing them with neighboring in-sample $Q$-values. We theoretically show\nthat SBO approximates true $Q$-values for both in-sample and OOD actions within\nthe CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),\nempirically alleviates the over-constraint issue, achieving near-accurate\n$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing\nstate-of-the-art methods in both performance and computational efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5SQOG\uff0c\u901a\u8fc7\u5e73\u6ed1\u8d1d\u5c14\u66fc\u7b97\u5b50\uff08SBO\uff09\u5728\u51f8\u5305\u53ca\u5176\u90bb\u57df\uff08CHN\uff09\u5185\u589e\u5f3aQ\u51fd\u6570\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684Q\u503c\u9ad8\u4f30\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5206\u5e03\u504f\u79fb\u5bfc\u81f4Q\u503c\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u52a8\u4f5c\u7684\u9ad8\u4f30\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u7ea6\u675f\u8fc7\u5f3a\u800c\u9650\u5236\u4e86Q\u51fd\u6570\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u5e73\u6ed1\u8d1d\u5c14\u66fc\u7b97\u5b50\uff08SBO\uff09\uff0c\u5728CHN\u5185\u901a\u8fc7\u5e73\u6ed1OOD Q\u503c\u4e0e\u90bb\u8fd1\u6837\u672cQ\u503c\u66f4\u65b0Q\u503c\u3002", "result": "SQOG\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u51c6\u786e\u7684Q\u503c\u4f30\u8ba1\u3002", "conclusion": "SQOG\u901a\u8fc7SBO\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u5347\u4e86Q\u503c\u4f30\u8ba1\u548c\u7b56\u7565\u6539\u8fdb\u6548\u679c\u3002"}}
{"id": "2506.08426", "pdf": "https://arxiv.org/pdf/2506.08426", "abs": "https://arxiv.org/abs/2506.08426", "authors": ["Zheng Lin", "Zhe Chen", "Xianhao Chen", "Wei Ni", "Yue Gao"], "title": "HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "16 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2403.13101", "summary": "Split federated learning (SFL) has emerged as a promising paradigm to\ndemocratize machine learning (ML) on edge devices by enabling layer-wise model\npartitioning. However, existing SFL approaches suffer significantly from the\nstraggler effect due to the heterogeneous capabilities of edge devices. To\naddress the fundamental challenge, we propose adaptively controlling batch\nsizes (BSs) and model splitting (MS) for edge devices to overcome resource\nheterogeneity. We first derive a tight convergence bound of SFL that quantifies\nthe impact of varied BSs and MS on learning performance. Based on the\nconvergence bound, we propose HASFL, a heterogeneity-aware SFL framework\ncapable of adaptively controlling BS and MS to balance communication-computing\nlatency and training convergence in heterogeneous edge networks. Extensive\nexperiments with various datasets validate the effectiveness of HASFL and\ndemonstrate its superiority over state-of-the-art benchmarks.", "AI": {"tldr": "HASFL\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63a7\u5236\u6279\u91cf\u5927\u5c0f\u548c\u6a21\u578b\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u5206\u88c2\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5f02\u6784\u8bbe\u5907\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u6709\u5206\u88c2\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u56e0\u8fb9\u7f18\u8bbe\u5907\u6027\u80fd\u5f02\u6784\u6027\u5bfc\u81f4\u4e25\u91cd\u7684\u6ede\u540e\u6548\u5e94\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHASFL\u6846\u67b6\uff0c\u57fa\u4e8e\u6536\u655b\u8fb9\u754c\u7406\u8bba\u81ea\u9002\u5e94\u8c03\u6574\u6279\u91cf\u5927\u5c0f\u548c\u6a21\u578b\u5206\u5272\uff0c\u5e73\u8861\u901a\u4fe1\u8ba1\u7b97\u5ef6\u8fdf\u4e0e\u8bad\u7ec3\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1HASFL\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "HASFL\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8bbe\u5907\u73af\u5883\u4e0b\u7684\u5206\u88c2\u8054\u90a6\u5b66\u4e60\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09026", "pdf": "https://arxiv.org/pdf/2506.09026", "abs": "https://arxiv.org/abs/2506.09026", "authors": ["Amrith Setlur", "Matthew Y. R. Yang", "Charlie Snell", "Jeremy Greer", "Ian Wu", "Virginia Smith", "Max Simchowitz", "Aviral Kumar"], "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ae3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LLM\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a2\u7d22\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5e76\u5b9e\u73b0\u6027\u80fd\u5916\u63a8\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5728\u6027\u80fd\u5916\u63a8\uff08\u5373\u8d85\u51fa\u8bad\u7ec3\u65f6\u7684\u6700\u5927token\u9884\u7b97\u65f6\u4ecd\u80fd\u63d0\u5347\u6027\u80fd\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "e3\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a(1) \u94fe\u5f0f\u6280\u80fd\u7ec4\u5408\uff08\u5982\u9a8c\u8bc1\u4e0e\u751f\u6210\uff09\uff1b(2) \u5229\u7528\u9519\u8bef\u8f68\u8ff9\u7684\u8d1f\u68af\u5ea6\u589e\u5f3a\u63a2\u7d22\uff1b(3) \u901a\u8fc7\u8bfe\u7a0b\u8bbe\u8ba1\u5c06\u4efb\u52a1\u96be\u5ea6\u4e0e\u8bad\u7ec3token\u9884\u7b97\u7ed3\u5408\u3002", "result": "e3-1.7B\u6a21\u578b\u5728AIME'25\u548cHMMT'25\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u591f\u5916\u63a8\u52302\u500d\u8bad\u7ec3token\u9884\u7b97\uff0c\u5e76\u63d0\u5347\u4e86pass@1\u548cpass@k\u5206\u6570\u3002", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a2\u7d22\u8bad\u7ec3LLM\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\u5916\u63a8\uff0ce3\u65b9\u6cd5\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08441", "pdf": "https://arxiv.org/pdf/2506.08441", "abs": "https://arxiv.org/abs/2506.08441", "authors": ["Anh N. Nhu", "Sanghyun Son", "Ming Lin"], "title": "Time-Aware World Model for Adaptive Prediction and Control", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Paper accepted to ICML 2025", "summary": "In this work, we introduce the Time-Aware World Model (TAWM), a model-based\napproach that explicitly incorporates temporal dynamics. By conditioning on the\ntime-step size, {\\Delta}t, and training over a diverse range of {\\Delta}t\nvalues -- rather than sampling at a fixed time-step -- TAWM learns both high-\nand low-frequency task dynamics across diverse control problems. Grounded in\nthe information-theoretic insight that the optimal sampling rate depends on a\nsystem's underlying dynamics, this time-aware formulation improves both\nperformance and data efficiency. Empirical evaluations show that TAWM\nconsistently outperforms conventional models across varying observation rates\nin a variety of control tasks, using the same number of training samples and\niterations. Our code can be found online at:\ngithub.com/anh-nn01/Time-Aware-World-Model.", "AI": {"tldr": "TAWM\u662f\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u611f\u77e5\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u6b65\u957f\u63d0\u5347\u4efb\u52a1\u52a8\u6001\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u56fa\u5b9a\u65f6\u95f4\u6b65\u957f\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u9891\u7387\u7684\u4efb\u52a1\u52a8\u6001\uff0cTAWM\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u4f18\u5316\u91c7\u6837\u7387\u3002", "method": "TAWM\u901a\u8fc7\u8bad\u7ec3\u591a\u6837\u5316\u7684\u65f6\u95f4\u6b65\u957f\uff08\u0394t\uff09\u6765\u5b66\u4e60\u9ad8\u4f4e\u9891\u4efb\u52a1\u52a8\u6001\uff0c\u57fa\u4e8e\u4fe1\u606f\u8bba\u4f18\u5316\u91c7\u6837\u7387\u3002", "result": "TAWM\u5728\u591a\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u5747\u6709\u63d0\u5347\u3002", "conclusion": "TAWM\u7684\u65f6\u95f4\u611f\u77e5\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2506.08043", "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u6a21\u62df\u8f6f\u7ec4\u7ec7\u53d8\u5f62\uff0c\u7ed3\u5408Kelvinlet\u5148\u9a8c\u548cFEM\u6a21\u62df\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u5feb\u901f\u51c6\u786e\u7684\u8f6f\u7ec4\u7ec7\u53d8\u5f62\u6a21\u62df\u5bf9\u624b\u672f\u673a\u5668\u4eba\u548c\u533b\u5b66\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06Kelvinlet\u5148\u9a8c\u96c6\u6210\u5230\u795e\u7ecf\u6a21\u62df\u5668\u4e2d\uff0c\u5229\u7528FEM\u6a21\u62df\u8fdb\u884c\u6b8b\u5dee\u5b66\u4e60\u548c\u6b63\u5219\u5316\uff0c\u652f\u6301\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8f6f\u7ec4\u7ec7\u54cd\u5e94\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u67b6\u6784\u4e2d\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u624b\u672f\u6a21\u62df\u3002", "conclusion": "Kelvinlet\u589e\u5f3a\u5b66\u4e60\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u624b\u672f\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u3001\u7269\u7406\u611f\u77e5\u8f6f\u7ec4\u7ec7\u6a21\u62df\u3002"}}
{"id": "2506.08459", "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections.", "AI": {"tldr": "\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6f5c\u5728\u6545\u969c\u6848\u4f8b\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u96c6\uff0c\u9002\u7528\u4e8e\u4ea4\u901a\u8def\u53e3\u7684\u5b89\u5168\u9a8c\u8bc1\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u98ce\u9669\u9ad8\u3001\u6210\u672c\u9ad8\uff0c\u4e14\u6f5c\u5728\u6545\u969c\u7f55\u89c1\u4e14\u591a\u6837\u3002", "method": "\u8bad\u7ec3\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u6839\u636e\u521d\u59cb\u4ea4\u901a\u72b6\u6001\u751f\u6210\u6f5c\u5728\u6545\u969c\u6848\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u751f\u6210\u771f\u5b9e\u6545\u969c\u6837\u672c\u5e76\u6355\u6349\u591a\u79cd\u6f5c\u5728\u6545\u969c\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u96c6\u6216\u9ad8\u6027\u80fd\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4ea4\u901a\u8def\u53e3\u7684\u5b89\u5168\u9a8c\u8bc1\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08064", "pdf": "https://arxiv.org/pdf/2506.08064", "abs": "https://arxiv.org/abs/2506.08064", "authors": ["Livio Tenze", "Enrique Canessa"], "title": "A Real-time 3D Desktop Display", "categories": ["cs.GR", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0", "AI": {"tldr": "altiro3D\u5e93\u7684\u6269\u5c55\u7248\u672c\uff0c\u652f\u6301\u4ece2D\u56fe\u50cf\u6216\u89c6\u9891\u6d41\u5b9e\u65f6\u751f\u62103D\u5149\u573a\uff0c\u5229\u7528AI\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u65b0\u589e\u591a\u5e73\u53f0GUI\u3002", "motivation": "\u4e3a\u4e86\u5c062D\u56fe\u50cf\u6216\u89c6\u9891\u6d41\u5b9e\u65f6\u8f6c\u6362\u4e3a3D\u5149\u573a\uff0c\u63d0\u4f9b\u66f4\u771f\u5b9e\u76843D\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528MiDaS CNN\u4ece\u5355\u5f202D\u56fe\u50cf\u63d0\u53d6\u6df1\u5ea6\u56fe\uff0c\u7ed3\u5408AI\u6280\u672f\u4f18\u5316\u6027\u80fd\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u6e90\uff08\u56fe\u50cf\u3001\u89c6\u9891\u3001\u684c\u9762\u533a\u57df\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f63D\u5149\u573a\u5408\u6210\uff0c\u652f\u6301\u591a\u5e73\u53f0GUI\uff0c\u53ef\u76f4\u63a5\u8f93\u51fa\u81f3\u5149\u573a3D\u8bbe\u5907\u3002", "conclusion": "\u6269\u5c55\u540e\u7684altiro3D\u5e93\u529f\u80fd\u66f4\u5f3a\u5927\uff0c\u9002\u7528\u4e8e\u591a\u79cd3D\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.08460", "pdf": "https://arxiv.org/pdf/2506.08460", "abs": "https://arxiv.org/abs/2506.08460", "authors": ["Yihong Guo", "Yu Yang", "Pan Xu", "Anqi Liu"], "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "We study the off-dynamics offline reinforcement learning problem, where the\ngoal is to learn a policy from offline datasets collected from source and\ntarget domains with mismatched transition. Existing off-dynamics offline RL\nmethods typically either filter source transitions that resemble those of the\ntarget domain or apply reward augmentation to source data, both constrained by\nthe limited transitions available from the target domain. As a result, the\nlearned policy is unable to explore target domain beyond the offline datasets.\nWe propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that\naddresses this limitation by enabling exploration of the target domain via\nlearned dynamics. MOBODY generates new synthetic transitions in the target\ndomain through model rollouts, which are used as data augmentation during\noffline policy learning. Unlike existing model-based methods that learn\ndynamics from a single domain, MOBODY tackles the challenge of mismatched\ndynamics by leveraging both source and target datasets. Directly merging these\ndatasets can bias the learned model toward source dynamics. Instead, MOBODY\nlearns target dynamics by discovering a shared latent representation of states\nand transitions across domains through representation learning. To stabilize\ntraining, MOBODY incorporates a behavior cloning loss that regularizes the\npolicy. Specifically, we introduce a Q-weighted behavior cloning loss that\nregularizes the policy toward actions with high target-domain Q-values, rather\nthan uniformly imitating all actions in the dataset. These Q-values are learned\nfrom an enhanced target dataset composed of offline target data, augmented\nsource data, and rollout data from the learned target dynamics. We evaluate\nMOBODY on MuJoCo benchmarks and show that it significantly outperforms\nstate-of-the-art baselines, with especially pronounced improvements in\nchallenging scenarios.", "AI": {"tldr": "MOBODY\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u7684\u5408\u6210\u8f6c\u6362\u6570\u636e\u6765\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u52a8\u6001\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u52a8\u6001\u4e0d\u5339\u914d\u65f6\u8868\u73b0\u53d7\u9650\uff0c\u65e0\u6cd5\u63a2\u7d22\u76ee\u6807\u57df\u8d85\u51fa\u79bb\u7ebf\u6570\u636e\u96c6\u7684\u8303\u56f4\u3002", "method": "MOBODY\u901a\u8fc7\u8868\u793a\u5b66\u4e60\u53d1\u73b0\u8de8\u57df\u7684\u5171\u4eab\u6f5c\u5728\u72b6\u6001\u548c\u8f6c\u6362\u8868\u793a\uff0c\u751f\u6210\u76ee\u6807\u57df\u7684\u5408\u6210\u6570\u636e\uff0c\u5e76\u5f15\u5165Q\u52a0\u6743\u7684\u884c\u4e3a\u514b\u9686\u635f\u5931\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOBODY\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MOBODY\u901a\u8fc7\u6a21\u578b\u751f\u6210\u548c\u8868\u793a\u5b66\u4e60\u89e3\u51b3\u4e86\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08463", "pdf": "https://arxiv.org/pdf/2506.08463", "abs": "https://arxiv.org/abs/2506.08463", "authors": ["Zhishuai Liu", "Yu Yang", "Ruhan Wang", "Pan Xu", "Dongruo Zhou"], "title": "How to Provably Improve Return Conditioned Supervised Learning?", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "25 pages, 4 figures, 12 tables", "summary": "In sequential decision-making problems, Return-Conditioned Supervised\nLearning (RCSL) has gained increasing recognition for its simplicity and\nstability in modern decision-making tasks. Unlike traditional offline\nreinforcement learning (RL) algorithms, RCSL frames policy learning as a\nsupervised learning problem by taking both the state and return as input. This\napproach eliminates the instability often associated with temporal difference\n(TD) learning in offline RL. However, RCSL has been criticized for lacking the\nstitching property, meaning its performance is inherently limited by the\nquality of the policy used to generate the offline dataset. To address this\nlimitation, we propose a principled and simple framework called Reinforced\nRCSL. The key innovation of our framework is the introduction of a concept we\ncall the in-distribution optimal return-to-go. This mechanism leverages our\npolicy to identify the best achievable in-dataset future return based on the\ncurrent state, avoiding the need for complex return augmentation techniques.\nOur theoretical analysis demonstrates that Reinforced RCSL can consistently\noutperform the standard RCSL approach. Empirical results further validate our\nclaims, showing significant performance improvements across a range of\nbenchmarks.", "AI": {"tldr": "Reinforced RCSL\u901a\u8fc7\u5f15\u5165\u201cin-distribution optimal return-to-go\u201d\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRCSL\u7f3a\u4e4f\u62fc\u63a5\u80fd\u529b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRCSL\u56e0\u4f9d\u8d56\u79bb\u7ebf\u6570\u636e\u96c6\u7684\u8d28\u91cf\u800c\u6027\u80fd\u53d7\u9650\uff0c\u7f3a\u4e4f\u62fc\u63a5\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faReinforced RCSL\u6846\u67b6\uff0c\u5229\u7528\u7b56\u7565\u8bc6\u522b\u5f53\u524d\u72b6\u6001\u4e0b\u53ef\u8fbe\u5230\u7684\u6700\u4f73\u672a\u6765\u56de\u62a5\uff0c\u907f\u514d\u590d\u6742\u7684\u56de\u62a5\u589e\u5f3a\u6280\u672f\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u8868\u660e\uff0cReinforced RCSL\u4f18\u4e8e\u6807\u51c6RCSL\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Reinforced RCSL\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86RCSL\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.08183", "pdf": "https://arxiv.org/pdf/2506.08183", "abs": "https://arxiv.org/abs/2506.08183", "authors": ["Isha Puri", "David Cox"], "title": "A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Research in neuroscience and vision science relies heavily on careful\nmeasurements of animal subject's gaze direction. Rodents are the most widely\nstudied animal subjects for such research because of their economic advantage\nand hardiness. Recently, video based eye trackers that use image processing\ntechniques have become a popular option for gaze tracking because they are easy\nto use and are completely noninvasive. Although significant progress has been\nmade in improving the accuracy and robustness of eye tracking algorithms,\nunfortunately, almost all of the techniques have focused on human eyes, which\ndoes not account for the unique characteristics of the rodent eye images, e.g.,\nvariability in eye parameters, abundance of surrounding hair, and their small\nsize. To overcome these unique challenges, this work presents a flexible,\nrobust, and highly accurate model for pupil and corneal reflection\nidentification in rodent gaze determination that can be incrementally trained\nto account for variability in eye parameters encountered in the field. To the\nbest of our knowledge, this is the first paper that demonstrates a highly\naccurate and practical biomedical image segmentation based convolutional neural\nnetwork architecture for pupil and corneal reflection identification in eye\nimages. This new method, in conjunction with our automated infrared videobased\neye recording system, offers the state of the art technology in eye tracking\nfor neuroscience and vision science research for rodents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u9c81\u68d2\u4e14\u9ad8\u7cbe\u5ea6\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u556e\u9f7f\u7c7b\u52a8\u7269\u77b3\u5b54\u548c\u89d2\u819c\u53cd\u5c04\u8bc6\u522b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6280\u672f\u672a\u8003\u8651\u556e\u9f7f\u7c7b\u52a8\u7269\u773c\u90e8\u72ec\u7279\u7279\u5f81\u7684\u95ee\u9898\u3002", "motivation": "\u556e\u9f7f\u7c7b\u52a8\u7269\u662f\u795e\u7ecf\u79d1\u5b66\u548c\u89c6\u89c9\u79d1\u5b66\u7814\u7a76\u7684\u5e38\u7528\u5bf9\u8c61\uff0c\u4f46\u73b0\u6709\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u4eba\u7c7b\u773c\u775b\uff0c\u672a\u8003\u8651\u556e\u9f7f\u7c7b\u52a8\u7269\u773c\u90e8\u7279\u5f81\uff08\u5982\u53c2\u6570\u53d8\u5f02\u3001\u6bdb\u53d1\u591a\u3001\u5c3a\u5bf8\u5c0f\uff09\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u67b6\u6784\uff0c\u53ef\u589e\u91cf\u8bad\u7ec3\u4ee5\u9002\u5e94\u773c\u90e8\u53c2\u6570\u53d8\u5f02\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u7ea2\u5916\u89c6\u9891\u773c\u52a8\u8bb0\u5f55\u7cfb\u7edf\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u556e\u9f7f\u7c7b\u52a8\u7269\u77b3\u5b54\u548c\u89d2\u819c\u53cd\u5c04\u7684\u51c6\u786e\u8bc6\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u81ea\u52a8\u8bb0\u5f55\u7cfb\u7edf\uff0c\u4e3a\u556e\u9f7f\u7c7b\u52a8\u7269\u773c\u52a8\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u5148\u8fdb\u6280\u672f\uff0c\u63a8\u52a8\u4e86\u795e\u7ecf\u79d1\u5b66\u548c\u89c6\u89c9\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2506.08258", "pdf": "https://arxiv.org/pdf/2506.08258", "abs": "https://arxiv.org/abs/2506.08258", "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Toby Collins", "Daniel A. Hashimoto", "Silvana Perretta", "Bernard Dallemagne", "Jacques Marescaux", "EAES Working Group", "Nicolas Padoy", "Pietro Mascagni"], "title": "Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era", "categories": ["cs.CY", "cs.CV"], "comment": "11 pages, 3 figures", "summary": "Artificial Intelligence (AI) is transforming medicine, with generative AI\nmodels like ChatGPT reshaping perceptions of its potential. This study examines\nsurgeons' awareness, expectations, and involvement with AI in surgery through\ncomparative surveys conducted in 2021 and 2024. Two cross-sectional surveys\nwere distributed globally in 2021 and 2024, the first before an IRCAD webinar\nand the second during the annual EAES meeting. The surveys assessed\ndemographics, AI awareness, expectations, involvement, and ethics (2024 only).\nThe surveys collected a total of 671 responses from 98 countries, 522 in 2021\nand 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in\n2024, while course attendance increased from 12.9% to 23%. Despite this,\nfamiliarity with foundational AI concepts remained limited. Expectations for\nAI's role shifted in 2024, with hospital management gaining relevance. Ethical\nconcerns gained prominence, with 87.2% of 2024 participants emphasizing\naccountability and transparency. Infrastructure limitations remained the\nprimary obstacle to implementation. Interdisciplinary collaboration and\nstructured training were identified as critical for successful AI adoption.\nOptimism about AI's transformative potential remained high, with 79.9% of\nrespondents believing AI would positively impact surgery and 96.6% willing to\nintegrate AI into their clinical practice. Surgeons' perceptions of AI are\nevolving, driven by the rise of generative AI and advancements in surgical data\nscience. While enthusiasm for integration is strong, knowledge gaps and\ninfrastructural challenges persist. Addressing these through education, ethical\nframeworks, and infrastructure development is essential.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc72021\u5e74\u548c2024\u5e74\u7684\u5168\u7403\u8c03\u67e5\uff0c\u5206\u6790\u4e86\u5916\u79d1\u533b\u751f\u5bf9AI\u7684\u8ba4\u77e5\u3001\u671f\u671b\u548c\u53c2\u4e0e\u60c5\u51b5\uff0c\u53d1\u73b0AI\u8bfe\u7a0b\u610f\u8bc6\u548c\u53c2\u4e0e\u5ea6\u63d0\u5347\uff0c\u4f46\u57fa\u7840\u77e5\u8bc6\u4ecd\u6709\u9650\uff0c\u4f26\u7406\u548c\u57fa\u7840\u8bbe\u65bd\u662f\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u5916\u79d1\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u53ca\u5916\u79d1\u533b\u751f\u5bf9\u5176\u7684\u8ba4\u77e5\u53d8\u5316\uff0c\u63a8\u52a8AI\u5728\u533b\u5b66\u4e2d\u7684\u6709\u6548\u6574\u5408\u3002", "method": "\u901a\u8fc72021\u5e74\u548c2024\u5e74\u7684\u4e24\u6b21\u5168\u7403\u6a2a\u65ad\u9762\u8c03\u67e5\uff0c\u8bc4\u4f30\u5916\u79d1\u533b\u751f\u5bf9AI\u7684\u8ba4\u77e5\u3001\u671f\u671b\u3001\u53c2\u4e0e\u53ca\u4f26\u7406\u95ee\u9898\u3002", "result": "AI\u8bfe\u7a0b\u610f\u8bc6\u548c\u53c2\u4e0e\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u4f46\u57fa\u7840\u77e5\u8bc6\u4e0d\u8db3\uff1b\u4f26\u7406\u95ee\u9898\u53d7\u5173\u6ce8\uff0c\u57fa\u7840\u8bbe\u65bd\u662f\u4e3b\u8981\u969c\u788d\uff1b\u591a\u6570\u5916\u79d1\u533b\u751f\u5bf9AI\u6301\u4e50\u89c2\u6001\u5ea6\u3002", "conclusion": "\u5c3d\u7ba1\u5916\u79d1\u533b\u751f\u5bf9AI\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u4ecd\u9700\u901a\u8fc7\u6559\u80b2\u3001\u4f26\u7406\u6846\u67b6\u548c\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u89e3\u51b3\u77e5\u8bc6\u5dee\u8ddd\u548c\u5b9e\u65bd\u969c\u788d\u3002"}}
{"id": "2506.08280", "pdf": "https://arxiv.org/pdf/2506.08280", "abs": "https://arxiv.org/abs/2506.08280", "authors": ["Daniel H. Pak", "Shubh Thaker", "Kyle Baylous", "Xiaoran Zhang", "Danny Bluestein", "James S. Duncan"], "title": "Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-quality volumetric meshing from medical images is a key bottleneck for\nphysics-based simulations in personalized medicine. For volumetric meshing of\ncomplex medical structures, recent studies have often utilized deep learning\n(DL)-based template deformation approaches to enable fast test-time generation\nwith high spatial accuracy. However, these approaches still exhibit\nlimitations, such as limited flexibility at high-curvature areas and\nunrealistic inter-part distances. In this study, we introduce a simple yet\neffective snap-and-tune strategy that sequentially applies DL and test-time\noptimization, which combines fast initial shape fitting with more detailed\nsample-specific mesh corrections. Our method provides significant improvements\nin both spatial accuracy and mesh quality, while being fully automated and\nrequiring no additional training labels. Finally, we demonstrate the\nversatility and usefulness of our newly generated meshes via solid mechanics\nsimulations in two different software platforms. Our code is available at\nhttps://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u201csnap-and-tune\u201d\u7b56\u7565\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf\u4f53\u79ef\u7f51\u683c\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u66f2\u7387\u533a\u57df\u548c\u90e8\u4ef6\u95f4\u8ddd\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u677f\u53d8\u5f62\u65b9\u6cd5\u5728\u590d\u6742\u533b\u5b66\u7ed3\u6784\u4f53\u79ef\u7f51\u683c\u751f\u6210\u4e2d\u5b58\u5728\u7075\u6d3b\u6027\u4e0d\u8db3\u548c\u90e8\u4ef6\u95f4\u8ddd\u4e0d\u771f\u5b9e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u201csnap-and-tune\u201d\u7b56\u7565\uff0c\u5148\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5feb\u901f\u62df\u5408\u521d\u59cb\u5f62\u72b6\uff0c\u518d\u901a\u8fc7\u6d4b\u8bd5\u65f6\u4f18\u5316\u8fdb\u884c\u6837\u672c\u7279\u5f02\u6027\u7f51\u683c\u4fee\u6b63\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u95f4\u7cbe\u5ea6\u548c\u7f51\u683c\u8d28\u91cf\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6807\u7b7e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u56fa\u4f53\u529b\u5b66\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08334", "pdf": "https://arxiv.org/pdf/2506.08334", "abs": "https://arxiv.org/abs/2506.08334", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "categories": ["cs.GR", "cs.CV"], "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u52a8\u6001RGBD\u89c6\u9891\u4e2d\u91cd\u5efa\u5173\u8282\u7269\u4f53\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5173\u8282\u7269\u4f53\u91cd\u5efa\u5728AI\u548c\u673a\u5668\u4eba\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u7cbe\u5fc3\u91c7\u96c6\u7684\u6570\u636e\uff0c\u96be\u4ee5\u5b9e\u73b0\u89c4\u6a21\u5316\u63a8\u5e7f\u3002", "method": "\u91c7\u7528\u7531\u7c97\u5230\u7ec6\u7684\u6846\u67b6\uff0c\u4ece\u52a8\u6001RGBD\u89c6\u9891\u4e2d\u63a8\u65ad\u5173\u8282\u53c2\u6570\u5e76\u5206\u5272\u53ef\u79fb\u52a8\u90e8\u4ef6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u52a8\u6001\u89c6\u9891\u4e2d\u9ad8\u6548\u91cd\u5efa\u5173\u8282\u7269\u4f53\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08350", "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Ak\u015fit"], "title": "Complex-Valued Holographic Radiance Fields", "categories": ["cs.GR", "cs.CV", "cs.ET"], "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u590d\u503c\u9ad8\u65af\u57fa\u5143\u76843D\u5168\u606f\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u57283D\u8868\u793a\u4e2d\u5b8c\u6574\u5efa\u6a21\u5149\u7684\u632f\u5e45\u548c\u76f8\u4f4d\u7279\u6027\uff0c\u4ee5\u652f\u6301\u7269\u7406\u771f\u5b9e\u7684\u6e32\u67d3\uff0c\u5c24\u5176\u662f\u5168\u606f\u663e\u793a\u3002", "method": "\u901a\u8fc7RGBD\u591a\u89c6\u89d2\u56fe\u50cf\u76f4\u63a5\u4f18\u5316\u590d\u503c\u9ad8\u65af\u57fa\u5143\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u5f3a\u5ea6\u7684\u4e2d\u95f4\u8868\u793a\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u534730\u500d\u81f31\u4e07\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51e0\u4f55\u5bf9\u9f50\u3001\u7269\u7406\u771f\u5b9e\u76843D\u5168\u606f\u573a\u666f\u8868\u793a\u63d0\u4f9b\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08353", "pdf": "https://arxiv.org/pdf/2506.08353", "abs": "https://arxiv.org/abs/2506.08353", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "An Adaptive Method Stabilizing Activations for Enhanced Generalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce AdaAct, a novel optimization algorithm that adjusts learning\nrates according to activation variance. Our method enhances the stability of\nneuron outputs by incorporating neuron-wise adaptivity during the training\nprocess, which subsequently leads to better generalization -- a complementary\napproach to conventional activation regularization methods. Experimental\nresults demonstrate AdaAct's competitive performance across standard image\nclassification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing\nit with other state-of-the-art methods. Importantly, AdaAct effectively bridges\nthe gap between the convergence speed of Adam and the strong generalization\ncapabilities of SGD, all while maintaining competitive execution times. Code is\navailable at https://github.com/hseung88/adaact.", "AI": {"tldr": "AdaAct\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u6fc0\u6d3b\u65b9\u5dee\u8c03\u6574\u5b66\u4e60\u7387\uff0c\u63d0\u5347\u795e\u7ecf\u5143\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\uff0c\u8fdb\u800c\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6fc0\u6d3b\u6b63\u5219\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0cAdaAct\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u9002\u5e94\u6027\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "AdaAct\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u4ee5\u9002\u5e94\u6fc0\u6d3b\u65b9\u5dee\uff0c\u7ed3\u5408\u4e86Adam\u7684\u5feb\u901f\u6536\u655b\u548cSGD\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728CIFAR\u548cImageNet\u7b49\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaAct\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6267\u884c\u65f6\u95f4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "AdaAct\u6709\u6548\u5e73\u8861\u4e86\u6536\u655b\u901f\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002"}}
{"id": "2506.08505", "pdf": "https://arxiv.org/pdf/2506.08505", "abs": "https://arxiv.org/abs/2506.08505", "authors": ["Shahaf Bassan", "Yizhak Yisrael Elboher", "Tobias Ladner", "Matthias Althoff", "Guy Katz"], "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "To appear in ICML 2025", "summary": "Despite significant advancements in post-hoc explainability techniques for\nneural networks, many current methods rely on heuristics and do not provide\nformally provable guarantees over the explanations provided. Recent work has\nshown that it is possible to obtain explanations with formal guarantees by\nidentifying subsets of input features that are sufficient to determine that\npredictions remain unchanged using neural network verification techniques.\nDespite the appeal of these explanations, their computation faces significant\nscalability challenges. In this work, we address this gap by proposing a novel\nabstraction-refinement technique for efficiently computing provably sufficient\nexplanations of neural network predictions. Our method abstracts the original\nlarge neural network by constructing a substantially reduced network, where a\nsufficient explanation of the reduced network is also provably sufficient for\nthe original network, hence significantly speeding up the verification process.\nIf the explanation is in sufficient on the reduced network, we iteratively\nrefine the network size by gradually increasing it until convergence. Our\nexperiments demonstrate that our approach enhances the efficiency of obtaining\nprovably sufficient explanations for neural network predictions while\nadditionally providing a fine-grained interpretation of the network's\npredictions across different abstraction levels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62bd\u8c61-\u7ec6\u5316\u6280\u672f\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u53ef\u8bc1\u660e\u5145\u5206\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u4e14\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u800c\u57fa\u4e8e\u9a8c\u8bc1\u6280\u672f\u7684\u53ef\u8bc1\u660e\u89e3\u91ca\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7b80\u5316\u7f51\u7edc\u62bd\u8c61\u539f\u59cb\u7f51\u7edc\uff0c\u8fed\u4ee3\u7ec6\u5316\u7f51\u7edc\u89c4\u6a21\u76f4\u81f3\u6536\u655b\uff0c\u4ee5\u52a0\u901f\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u83b7\u53d6\u53ef\u8bc1\u660e\u5145\u5206\u89e3\u91ca\u7684\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u7f51\u7edc\u9884\u6d4b\u7684\u7ec6\u7c92\u5ea6\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u8bc1\u660e\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u652f\u6301\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u3002"}}
{"id": "2506.08435", "pdf": "https://arxiv.org/pdf/2506.08435", "abs": "https://arxiv.org/abs/2506.08435", "authors": ["Mingyuan Fan", "Fuyi Wang", "Cen Chen", "Jianying Zhou"], "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted to Usenix Security 2025", "summary": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc1\u660e\uff0c\u5373\u4f7f\u5728\u73b0\u5b9e\u7684\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u68af\u5ea6\u6cc4\u6f0f\u653b\u51fb\uff08GLAs\uff09\u4ecd\u80fd\u6709\u6548\u91cd\u6784\u5ba2\u6237\u7aef\u6570\u636e\uff0c\u5e76\u63d0\u51faFedLeak\u65b9\u6cd5\u4ee5\u89e3\u51b3\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u63a2\u8ba8\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u68af\u5ea6\u6cc4\u6f0f\u653b\u51fb\u7684\u5b9e\u9645\u98ce\u9669\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u653b\u51fb\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u6709\u6548\u6027\u7684\u5173\u952e\u7a7a\u767d\u3002", "method": "\u63d0\u51faFedLeak\u65b9\u6cd5\uff0c\u5305\u542b\u90e8\u5206\u68af\u5ea6\u5339\u914d\u548c\u68af\u5ea6\u6b63\u5219\u5316\u4e24\u9879\u65b0\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u6587\u732e\u548c\u884c\u4e1a\u5b9e\u8df5\u7684\u5b9e\u9645\u8bc4\u4f30\u534f\u8bae\u3002", "result": "FedLeak\u5728\u73b0\u5b9eFL\u73af\u5883\u4e2d\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6570\u636e\u91cd\u6784\uff0c\u63ed\u793a\u4e86FL\u7cfb\u7edf\u7684\u91cd\u5927\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86FL\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u663e\u8457\u9690\u79c1\u98ce\u9669\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2506.08507", "pdf": "https://arxiv.org/pdf/2506.08507", "abs": "https://arxiv.org/abs/2506.08507", "authors": ["Kuo Yang", "Xingjie Yang", "Linhui Yu", "Qing Xu", "Yan Fang", "Xu Wang", "Zhengyang Zhou", "Yang Wang"], "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently\nemerged as a powerful paradigm for tackling complex real-world tasks. However,\nexisting Mas construction methods typically rely on manually crafted\ninteraction mechanisms or heuristic rules, introducing human biases and\nconstraining the autonomous ability. Even with recent advances in adaptive Mas\nconstruction, existing systems largely remain within the paradigm of\nsemi-autonomous patterns. In this work, we propose MasHost, a Reinforcement\nLearning (RL)-based framework for autonomous and query-adaptive Mas design. By\nformulating Mas construction as a graph search problem, our proposed MasHost\njointly samples agent roles and their interactions through a unified\nprobabilistic sampling mechanism. Beyond the accuracy and efficiency objectives\npursued in prior works, we introduce component rationality as an additional and\nnovel design principle in Mas. To achieve this multi-objective optimization, we\npropose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy\nthat collaboratively integrates group-relative advantages and action-wise\nrewards. To our knowledge, our proposed MasHost is the first RL-driven\nframework for autonomous Mas graph construction. Extensive experiments on six\nbenchmarks demonstrate that MasHost consistently outperforms most competitive\nbaselines, validating its effectiveness, efficiency, and structure rationality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08Mas\uff09\u8bbe\u8ba1\u6846\u67b6MasHost\uff0c\u901a\u8fc7\u56fe\u641c\u7d22\u95ee\u9898\u5efa\u6a21\u548c\u5206\u5c42\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08HRPO\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u89d2\u8272\u548c\u4ea4\u4e92\u91c7\u6837\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u4ea4\u4e92\u673a\u5236\u6216\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u6027\u5e76\u5f15\u5165\u4eba\u4e3a\u504f\u89c1\u3002", "method": "\u5c06Mas\u6784\u5efa\u5efa\u6a21\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u6982\u7387\u91c7\u6837\u673a\u5236\u8054\u5408\u91c7\u6837\u667a\u80fd\u4f53\u89d2\u8272\u548c\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u7ec4\u4ef6\u5408\u7406\u6027\u4f5c\u4e3a\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMasHost\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u7ed3\u6784\u5408\u7406\u6027\u3002", "conclusion": "MasHost\u662f\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3bMas\u56fe\u6784\u5efa\u6846\u67b6\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08443", "pdf": "https://arxiv.org/pdf/2506.08443", "abs": "https://arxiv.org/abs/2506.08443", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition.", "AI": {"tldr": "SakugaFlow\u662f\u4e00\u4e2a\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u65b0\u624b\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\uff0c\u652f\u6301\u975e\u7ebf\u6027\u4fee\u6539\u548c\u591a\u7248\u672c\u5206\u652f\uff0c\u5c06\u9ed1\u76d2\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u5b66\u4e60\u5de5\u5177\u3002", "motivation": "\u5f53\u524dAI\u7ed8\u56fe\u5de5\u5177\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u827a\u672f\u5bb6\u7684\u5206\u6b65\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u652f\u6301\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\uff0c\u652f\u6301\u975e\u7ebf\u6027\u4fee\u6539\u548c\u591a\u7248\u672c\u5206\u652f\u3002", "result": "SakugaFlow\u901a\u8fc7\u5c55\u793a\u4e2d\u95f4\u8f93\u51fa\u548c\u5d4c\u5165\u6559\u5b66\u5bf9\u8bdd\uff0c\u652f\u6301\u521b\u610f\u63a2\u7d22\u548c\u6280\u80fd\u5b66\u4e60\u3002", "conclusion": "SakugaFlow\u6210\u529f\u5c06\u9ed1\u76d2\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u652f\u6301\u5b66\u4e60\u548c\u521b\u4f5c\u7684\u5de5\u5177\u3002"}}
{"id": "2506.08524", "pdf": "https://arxiv.org/pdf/2506.08524", "abs": "https://arxiv.org/abs/2506.08524", "authors": ["Weiguo Wang", "Andy Nie", "Wenrui Zhou", "Yi Kai", "Chengchen Hu"], "title": "Teaching Physical Awareness to LLMs through Sounds", "categories": ["cs.SD", "cs.AI", "cs.MM", "cs.RO", "eess.AS"], "comment": "ICML 2025", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in text and\nmultimodal processing, yet they fundamentally lack physical\nawareness--understanding of real-world physical phenomena. In this work, we\npresent ACORN, a framework that teaches LLMs physical awareness through sound,\nfocusing on fundamental physical phenomena like the Doppler effect, multipath\neffect, and spatial relationships. To overcome data scarcity, ACORN introduce a\nphysics-based simulator combining real-world sound sources with controlled\nphysical channels to generate diverse training data. Using this simulator, we\nbuild AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an\naudio encoder that processes both magnitude and phase information. By\nconnecting our audio encoder to state-of-the-art LLMs, we demonstrate\nreasonable results in both simulated and real-world tasks, such as\nline-of-sight detection, Doppler effect estimation, and Direction-of-Arrival\nestimation, paving the way for enabling LLMs to understand physical world.", "AI": {"tldr": "ACORN\u6846\u67b6\u901a\u8fc7\u58f0\u97f3\u6559\u6388LLMs\u7269\u7406\u611f\u77e5\uff0c\u5229\u7528\u7269\u7406\u6a21\u62df\u5668\u751f\u6210\u6570\u636e\uff0c\u6784\u5efaAQA-PHY\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "LLMs\u7f3a\u4e4f\u5bf9\u73b0\u5b9e\u7269\u7406\u73b0\u8c61\u7684\u7406\u89e3\uff0cACORN\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u7269\u7406\u6a21\u62df\u5668\u751f\u6210\u6570\u636e\uff0c\u6784\u5efaAQA-PHY\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u97f3\u9891\u7f16\u7801\u5668\u5904\u7406\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u3002", "result": "\u5728\u89c6\u7ebf\u68c0\u6d4b\u3001\u591a\u666e\u52d2\u6548\u5e94\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u5408\u7406\u7ed3\u679c\u3002", "conclusion": "ACORN\u4e3aLLMs\u7406\u89e3\u7269\u7406\u4e16\u754c\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2506.08533", "pdf": "https://arxiv.org/pdf/2506.08533", "abs": "https://arxiv.org/abs/2506.08533", "authors": ["Nihal Acharya Adde", "Alexandra Gianzina", "Hanno Gottschalk", "Andreas Ebert"], "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "Published at ESANN 2025 Conference", "summary": "This paper introduces Evolutionary Multi-Objective Network Architecture\nSearch (EMNAS) for the first time to optimize neural network architectures in\nlarge-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses\ngenetic algorithms to automate network design, tailored to enhance rewards and\nreduce model size without compromising performance. Additionally,\nparallelization techniques are employed to accelerate the search, and\nteacher-student methodologies are implemented to ensure scalable optimization.\nThis research underscores the potential of transfer learning as a robust\nframework for optimizing performance across iterative learning processes by\neffectively leveraging knowledge from earlier generations to enhance learning\nefficiency and stability in subsequent generations. Experimental results\ndemonstrate that tailored EMNAS outperforms manually designed models, achieving\nhigher rewards with fewer parameters. The findings of these strategies\ncontribute positively to EMNAS for RL in autonomous driving, advancing the\nfield toward better-performing networks suitable for real-world scenarios.", "AI": {"tldr": "EMNAS\u9996\u6b21\u7528\u4e8e\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4f18\u5316\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u81ea\u52a8\u8bbe\u8ba1\u7f51\u7edc\uff0c\u63d0\u5347\u5956\u52b1\u5e76\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u65f6\u91c7\u7528\u5e76\u884c\u5316\u548c\u5e08\u751f\u65b9\u6cd5\u52a0\u901f\u641c\u7d22\u4e0e\u4f18\u5316\u3002\u5b9e\u9a8c\u8868\u660e\uff0cEMNAS\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u6a21\u578b\u3002", "motivation": "\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u81ea\u52a8\u5316\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5e76\u884c\u5316\u548c\u5e08\u751f\u65b9\u6cd5\u52a0\u901f\u641c\u7d22\u4e0e\u4f18\u5316\u3002", "result": "EMNAS\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u5956\u52b1\u548c\u66f4\u5c11\u53c2\u6570\u3002", "conclusion": "EMNAS\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7f51\u7edc\u4f18\u5316\u6846\u67b6\uff0c\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2506.08520", "pdf": "https://arxiv.org/pdf/2506.08520", "abs": "https://arxiv.org/abs/2506.08520", "authors": ["Srinivasan Kidambi", "Pravin Nair"], "title": "Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 1 pseudo-code, 3 figure panels, 2 plot panels, 7 tables, 24\n  references", "summary": "Multi-head self-attention (MHSA) has become a core component in modern\ncomputer vision models. However, its quadratic complexity with respect to input\nlength poses a significant computational bottleneck in real-time and resource\nconstrained environments. We propose PnP-Nystra, a Nystr\\\"om based linear\napproximation of self-attention, developed as a plug-and-play (PnP) module that\ncan be integrated into the pre-trained image and video restoration models\nwithout retraining. As a drop-in replacement for MHSA, PnP-Nystra enables\nefficient acceleration in various window-based transformer architectures,\nincluding SwinIR, Uformer, and RVRT. Our experiments across diverse image and\nvideo restoration tasks, including denoising, deblurring, and super-resolution,\ndemonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU\nand a 2-5x speed-up on CPU inference. Despite these significant gains, the\nmethod incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To\nthe best of our knowledge, we are the first to demonstrate a linear attention\nfunctioning as a training-free substitute for MHSA in restoration models.", "AI": {"tldr": "PnP-Nystra\u662f\u4e00\u79cd\u57fa\u4e8eNystr\u00f6m\u7684\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u52a0\u901f\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08MHSA\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u4e8c\u6b21\u590d\u6742\u5ea6\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\uff0c\u5c24\u5176\u5728\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51faPnP-Nystra\uff0c\u4e00\u79cd\u57fa\u4e8eNystr\u00f6m\u7684\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u53ef\u76f4\u63a5\u66ff\u6362MHSA\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u4fee\u590d\u4efb\u52a1\u4e2d\uff0cPnP-Nystra\u5b9e\u73b0\u4e862-4\u500d\uff08GPU\uff09\u548c2-5\u500d\uff08CPU\uff09\u7684\u52a0\u901f\uff0cPSNR\u635f\u5931\u6700\u5927\u4ec51.5 dB\u3002", "conclusion": "PnP-Nystra\u662f\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3aMHSA\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u635f\u5931\u8f83\u5c0f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08534", "pdf": "https://arxiv.org/pdf/2506.08534", "abs": "https://arxiv.org/abs/2506.08534", "authors": ["Donglian Li", "Hui Guo", "Minglang Chen", "Huizhen Chen", "Jialing Chen", "Bocheng Liang", "Pengchen Liang", "Ying Tan"], "title": "DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of anatomical structures in the apical four-chamber\n(A4C) view of fetal echocardiography is essential for early diagnosis and\nprenatal evaluation of congenital heart disease (CHD). However, precise\nsegmentation remains challenging due to ultrasound artifacts, speckle noise,\nanatomical variability, and boundary ambiguity across different gestational\nstages. To reduce the workload of sonographers and enhance segmentation\naccuracy, we propose DCD, an advanced deep learning-based model for automatic\nsegmentation of key anatomical structures in the fetal A4C view. Our model\nincorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,\nenabling superior multi-scale feature extraction, and a Convolutional Block\nAttention Module (CBAM) to enhance adaptive feature representation. By\neffectively capturing both local and global contextual information, DCD\nachieves precise and robust segmentation, contributing to improved prenatal\ncardiac assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578bDCD\uff0c\u7528\u4e8e\u80ce\u513f\u5fc3\u810f\u56db\u8154\u5fc3\u5207\u9762\u5173\u952e\u7ed3\u6784\u7684\u81ea\u52a8\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u80ce\u513f\u5fc3\u810f\u56db\u8154\u5fc3\u5207\u9762\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u5148\u5929\u6027\u5fc3\u810f\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8d85\u58f0\u56fe\u50cf\u7684\u566a\u58f0\u548c\u53d8\u5f02\u6027\u589e\u52a0\u4e86\u5206\u5272\u96be\u5ea6\u3002", "method": "DCD\u6a21\u578b\u7ed3\u5408\u4e86Dense ASPP\u6a21\u5757\u548cCBAM\u6a21\u5757\uff0c\u7528\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u7279\u5f81\u8868\u793a\u3002", "result": "DCD\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u5206\u5272\uff0c\u63d0\u5347\u4e86\u4ea7\u524d\u5fc3\u810f\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "conclusion": "DCD\u6a21\u578b\u4e3a\u80ce\u513f\u5fc3\u810f\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u51cf\u8f7b\u8d85\u58f0\u533b\u5e08\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002"}}
{"id": "2506.08618", "pdf": "https://arxiv.org/pdf/2506.08618", "abs": "https://arxiv.org/abs/2506.08618", "authors": ["Xianquan Yan", "Hakan Akg\u00fcn", "Kenji Kawaguchi", "N. Duane Loh", "Ching Hua Lee"], "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset", "categories": ["cs.LG", "cond-mat.mes-hall", "cond-mat.other", "cs.AI", "cs.CV"], "comment": "39 pages, 13 figures, 3 tables. Code & pipeline:\n  [https://github.com/sarinstein-yan/Poly2Graph] Dataset:\n  [https://github.com/sarinstein-yan/HSG-12M] Dataset released under CC BY 4.0", "summary": "Existing graph benchmarks assume non-spatial, simple edges, collapsing\nphysically distinct paths into a single link. We introduce HSG-12M, the first\nlarge-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a\nmetric space where multiple geometrically distinct trajectories between two\nnodes are retained as separate edges. HSG-12M contains 11.6 million static and\n5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401\ncharacteristic-polynomial classes, derived from 177 TB of spectral potential\ndata. Each graph encodes the full geometry of a 1-D crystal's energy spectrum\non the complex plane, producing diverse, physics-grounded topologies that\ntranscend conventional node-coordinate datasets. To enable future extensions,\nwe release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that\nmaps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with\npopular GNNs expose new challenges in learning from multi-edge geometry at\nscale. Beyond its practical utility, we show that spectral graphs serve as\nuniversal topological fingerprints of polynomials, vectors, and matrices,\nforging a new algebra-to-graph link. HSG-12M lays the groundwork for\ngeometry-aware graph learning and new opportunities of data-driven scientific\ndiscovery in condensed matter physics and beyond.", "AI": {"tldr": "HSG-12M\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u7a7a\u95f4\u591a\u91cd\u56fe\u6570\u636e\u96c6\uff0c\u4fdd\u7559\u4e86\u8282\u70b9\u95f4\u51e0\u4f55\u4e0d\u540c\u7684\u8def\u5f84\uff0c\u4e3a\u51e0\u4f55\u611f\u77e5\u56fe\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u56fe\u57fa\u51c6\u5047\u8bbe\u8fb9\u662f\u975e\u7a7a\u95f4\u4e14\u7b80\u5355\u7684\uff0c\u5ffd\u7565\u4e86\u7269\u7406\u4e0a\u4e0d\u540c\u7684\u8def\u5f84\uff0cHSG-12M\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7Poly2Graph\u7ba1\u9053\u5c061-D\u6676\u4f53\u54c8\u5bc6\u987f\u91cf\u6620\u5c04\u4e3a\u8c31\u56fe\uff0c\u751f\u6210\u9759\u6001\u548c\u52a8\u6001\u7684\u7a7a\u95f4\u591a\u91cd\u56fe\u3002", "result": "HSG-12M\u5305\u542b1160\u4e07\u9759\u6001\u548c510\u4e07\u52a8\u6001\u8c31\u56fe\uff0c\u5c55\u793a\u4e86\u591a\u8fb9\u51e0\u4f55\u5b66\u4e60\u7684\u65b0\u6311\u6218\u3002", "conclusion": "\u8c31\u56fe\u4f5c\u4e3a\u591a\u9879\u5f0f\u3001\u5411\u91cf\u548c\u77e9\u9635\u7684\u901a\u7528\u62d3\u6251\u6307\u7eb9\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2506.08623", "pdf": "https://arxiv.org/pdf/2506.08623", "abs": "https://arxiv.org/abs/2506.08623", "authors": ["Rinat Prochii", "Elizaveta Dakhova", "Pavel Birulin", "Maxim Sharaev"], "title": "Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification", "categories": ["eess.IV", "cs.CV"], "comment": "16 pages, 2 figures, 3 tables", "summary": "Accurate classification of second-trimester fetal ultrasound images remains\nchallenging due to low image quality, high intra-class variability, and\nsignificant class imbalance. In this work, we introduce a simple yet powerful,\nbiologically inspired deep learning ensemble framework that-unlike prior\nstudies focused on only a handful of anatomical targets-simultaneously\ndistinguishes 16 fetal structures. Drawing on the hierarchical, modular\norganization of biological vision systems, our model stacks two complementary\nbranches (a \"shallow\" path for coarse, low-resolution cues and a \"detailed\"\npath for fine, high-resolution features), concatenating their outputs for final\nprediction. To our knowledge, no existing method has addressed such a large\nnumber of classes with a comparably lightweight architecture. We trained and\nevaluated on 5,298 routinely acquired clinical images (annotated by three\nexperts and reconciled via Dawid-Skene), reflecting real-world noise and\nvariability rather than a \"cleaned\" dataset. Despite this complexity, our\nensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies\n90% of organs with accuracy > 0.75 and 75% of organs with accuracy >\n0.85-performance competitive with more elaborate models applied to far fewer\ncategories. These results demonstrate that biologically inspired modular\nstacking can yield robust, scalable fetal anatomy recognition in challenging\nclinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u6df1\u5ea6\u5b66\u4e60\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u5206\u7c7b16\u79cd\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b\u4e2d\u56e0\u56fe\u50cf\u8d28\u91cf\u4f4e\u3001\u7c7b\u5185\u5dee\u5f02\u5927\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\uff08\u6d45\u8def\u5f84\u548c\u8be6\u7ec6\u8def\u5f84\uff09\u7684\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408EfficientNet-B0\u548cEfficientNet-B6\u6a21\u578b\uff0c\u4f7f\u7528LDAM-Focal\u635f\u5931\u51fd\u6570\u3002", "result": "\u57285,298\u5f20\u4e34\u5e8a\u56fe\u50cf\u4e0a\u6d4b\u8bd5\uff0c90%\u7684\u5668\u5b98\u5206\u7c7b\u51c6\u786e\u7387>0.75\uff0c75%\u7684\u5668\u5b98>0.85\u3002", "conclusion": "\u751f\u7269\u542f\u53d1\u7684\u6a21\u5757\u5316\u67b6\u6784\u5728\u590d\u6742\u4e34\u5e8a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.08563", "pdf": "https://arxiv.org/pdf/2506.08563", "abs": "https://arxiv.org/abs/2506.08563", "authors": ["Siyuan Yang", "Cheng Song", "Zhilu Lai", "Wenjia Wang"], "title": "KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks", "categories": ["cs.CE", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "comment": "Accepted to IJCAI 2025", "summary": "Differential equations are involved in modeling many engineering problems.\nMany efforts have been devoted to solving differential equations. Due to the\nflexibility of neural networks, Physics Informed Neural Networks (PINNs) have\nrecently been proposed to solve complex differential equations and have\ndemonstrated superior performance in many applications. While the L2 loss\nfunction is usually a default choice in PINNs, it has been shown that the\ncorresponding numerical solution is incorrect and unstable for some complex\nequations. In this work, we propose a new PINNs framework named Kernel Packet\naccelerated PINNs (KP-PINNs), which gives a new expression of the loss function\nusing the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel\nPacket (KP) method to accelerate the computation. Theoretical results show that\nKP-PINNs can be stable across various differential equations. Numerical\nexperiments illustrate that KP-PINNs can solve differential equations\neffectively and efficiently. This framework provides a promising direction for\nimproving the stability and accuracy of PINNs-based solvers in scientific\ncomputing.", "AI": {"tldr": "KP-PINNs\u6846\u67b6\u901a\u8fc7RKHS\u8303\u6570\u548cKP\u65b9\u6cd5\u6539\u8fdbPINNs\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edfPINNs\u4f7f\u7528L2\u635f\u5931\u51fd\u6570\u5728\u590d\u6742\u65b9\u7a0b\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faKP-PINNs\u6846\u67b6\uff0c\u5229\u7528RKHS\u8303\u6570\u91cd\u65b0\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u5e76\u7528KP\u65b9\u6cd5\u52a0\u901f\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u8bc1\u660eKP-PINNs\u7a33\u5b9a\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u5176\u9ad8\u6548\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u3002", "conclusion": "KP-PINNs\u4e3a\u63d0\u5347PINNs\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08634", "pdf": "https://arxiv.org/pdf/2506.08634", "abs": "https://arxiv.org/abs/2506.08634", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills.", "AI": {"tldr": "MOSAIC-F\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u53cd\u9988\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\u3001\u89c2\u5bdf\u3001\u4f20\u611f\u5668\u3001\u4eba\u5de5\u667a\u80fd\u548c\u534f\u4f5c\u8bc4\u4f30\uff0c\u4e3a\u5b66\u751f\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u4e2a\u6027\u5316\u548c\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002", "method": "\u6846\u67b6\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a\u6807\u51c6\u5316\u8bc4\u4f30\u3001\u591a\u6a21\u6001\u6570\u636e\u6536\u96c6\u3001AI\u751f\u6210\u53cd\u9988\u3001\u5b66\u751f\u81ea\u6211\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316\u3002", "result": "\u5728\u63d0\u5347\u53e3\u5934\u6f14\u8bb2\u6280\u80fd\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "MOSAIC-F\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u4e0e\u6570\u636e\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u53cd\u9988\u3002"}}
{"id": "2506.08569", "pdf": "https://arxiv.org/pdf/2506.08569", "abs": "https://arxiv.org/abs/2506.08569", "authors": ["Erwan Plantec", "Gautier Hamon", "Mayalen Etcheverry", "Bert Wang-Chak Chan", "Pierre-Yves Oudeyer", "Cl\u00e9ment Moulin-Frier"], "title": "Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata", "categories": ["nlin.CG", "cs.AI"], "comment": "This manuscript has been accepted for publication in the Artificial\n  Life journal (https://direct.mit.edu/artl)", "summary": "Central to the artificial life endeavour is the creation of artificial\nsystems spontaneously generating properties found in the living world such as\nautopoiesis, self-replication, evolution and open-endedness. While numerous\nmodels and paradigms have been proposed, cellular automata (CA) have taken a\nvery important place in the field notably as they enable the study of\nphenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia\nhave been showed to produce life-like patterns reminiscent, on an aesthetic and\nontological point of view, of biological organisms we call creatures. We\npropose in this paper Flow-Lenia, a mass conservative extension of Lenia. We\npresent experiments demonstrating its effectiveness in generating\nspatially-localized patters (SLPs) with complex behaviors and show that the\nupdate rule parameters can be optimized to generate complex creatures showing\nbehaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed\nthe parameters of the model, defining the properties of the emerging patterns,\nwithin its own dynamics thus allowing for multispecies simulations. By using\nthe evolutionary activity framework as well as other metrics, we shed light on\nthe emergent evolutionary dynamics taking place in this system.", "AI": {"tldr": "Flow-Lenia\u662fLenia\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u590d\u6742\u884c\u4e3a\u7684\u7a7a\u95f4\u5c40\u90e8\u5316\u6a21\u5f0f\uff0c\u5e76\u652f\u6301\u591a\u7269\u79cd\u6a21\u62df\u3002", "motivation": "\u7814\u7a76\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u4e2d\u81ea\u53d1\u4ea7\u751f\u7684\u751f\u547d\u7279\u6027\uff0c\u5982\u81ea\u590d\u5236\u548c\u81ea\u7ec4\u7ec7\u3002", "method": "\u63d0\u51faFlow-Lenia\uff0c\u4e00\u79cd\u8d28\u91cf\u5b88\u6052\u7684Lenia\u6269\u5c55\uff0c\u901a\u8fc7\u4f18\u5316\u53c2\u6570\u751f\u6210\u590d\u6742\u751f\u7269\u884c\u4e3a\u3002", "result": "\u6210\u529f\u751f\u6210\u590d\u6742\u751f\u7269\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u591a\u7269\u79cd\u6a21\u62df\u548c\u8fdb\u5316\u52a8\u6001\u3002", "conclusion": "Flow-Lenia\u4e3a\u4eba\u5de5\u751f\u547d\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u7684\u8fdb\u5316\u52a8\u6001\u3002"}}
{"id": "2506.08641", "pdf": "https://arxiv.org/pdf/2506.08641", "abs": "https://arxiv.org/abs/2506.08641", "authors": ["Simon Roschmann", "Quentin Bouniot", "Vasilii Feofanov", "Ievgen Redko", "Zeynep Akata"], "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Time series classification is a fundamental task in healthcare and industry,\nyet the development of time series foundation models (TSFMs) remains limited by\nthe scarcity of publicly available time series datasets. In this work, we\npropose Time Vision Transformer (TiViT), a framework that converts time series\ninto images to leverage the representational power of frozen Vision\nTransformers (ViTs) pretrained on large-scale image datasets. First, we\ntheoretically motivate our approach by analyzing the 2D patching of ViTs for\ntime series, showing that it can increase the number of label-relevant tokens\nand reduce the sample complexity. Second, we empirically demonstrate that TiViT\nachieves state-of-the-art performance on standard time series classification\nbenchmarks by utilizing the hidden representations of large OpenCLIP models. We\nexplore the structure of TiViT representations and find that intermediate\nlayers with high intrinsic dimension are the most effective for time series\nclassification. Finally, we assess the alignment between TiViT and TSFM\nrepresentation spaces and identify a strong complementarity, with further\nperformance gains achieved by combining their features. Our findings reveal yet\nanother direction for reusing vision representations in a non-visual domain.", "AI": {"tldr": "TiViT\u6846\u67b6\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684Vision Transformers\uff08ViTs\uff09\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u6027\u80fd\uff0c\u8fbe\u5230SOTA\u7ed3\u679c\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u53d1\u5c55\u53d7\u9650\uff0c\u56e0\u516c\u5f00\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9700\u63a2\u7d22\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTiViT\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3ViTs\u7684\u8868\u793a\u80fd\u529b\uff0c\u5206\u6790\u51762D\u5206\u5757\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u9002\u7528\u6027\u3002", "result": "TiViT\u5728\u6807\u51c6\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e2d\u95f4\u5c42\u9ad8\u7ef4\u8868\u793a\u6700\u6709\u6548\uff0c\u4e0eTSFM\u8868\u793a\u7a7a\u95f4\u4e92\u8865\u3002", "conclusion": "TiViT\u5c55\u793a\u4e86\u89c6\u89c9\u8868\u793a\u5728\u975e\u89c6\u89c9\u9886\u57df\u7684\u590d\u7528\u6f5c\u529b\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08570", "pdf": "https://arxiv.org/pdf/2506.08570", "abs": "https://arxiv.org/abs/2506.08570", "authors": ["Or Tal", "Felix Kreuk", "Yossi Adi"], "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM", "AI": {"tldr": "\u8bba\u6587\u5bf9\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u7684\u4e24\u79cd\u5e38\u89c1\u5efa\u6a21\u8303\u5f0f\uff08\u81ea\u56de\u5f52\u89e3\u7801\u548c\u6761\u4ef6\u6d41\u5339\u914d\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u7684\u591a\u6837\u6027\u4f7f\u5f97\u516c\u5e73\u8bc4\u4f30\u548c\u8bbe\u8ba1\u9009\u62e9\u7684\u5f71\u54cd\u96be\u4ee5\u786e\u5b9a\uff0c\u672c\u6587\u4e13\u6ce8\u4e8e\u5efa\u6a21\u8303\u5f0f\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u76f8\u540c\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u914d\u7f6e\u548c\u7c7b\u4f3c\u67b6\u6784\uff0c\u5bf9\u6bd4\u81ea\u56de\u5f52\u89e3\u7801\u548c\u6761\u4ef6\u6d41\u5339\u914d\u4e24\u79cd\u8303\u5f0f\u3002", "result": "\u8bc4\u4f30\u4e86\u751f\u6210\u8d28\u91cf\u3001\u63a8\u7406\u914d\u7f6e\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u6761\u4ef6\u9075\u5faa\u80fd\u529b\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5404\u81ea\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2506.08677", "pdf": "https://arxiv.org/pdf/2506.08677", "abs": "https://arxiv.org/abs/2506.08677", "authors": ["Milica \u0160kipina", "Nikola Jovi\u0161i\u0107", "Nicola Dall'Asen", "Vanja \u0160venda", "Anil Osman Tur", "Slobodan Ili\u0107", "Elisa Ricci", "Dubravko \u0106ulibrk"], "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "categories": ["eess.IV", "cs.CV"], "comment": "21 pages, 14 figures, 7 tables", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAMBO\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e73\u817aX\u5149\u7247\u751f\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u8bad\u7ec3AI\u7cfb\u7edf\u6240\u9700\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u4e73\u817aX\u5149\u7247\u662f\u4e73\u817a\u764c\u68c0\u6d4b\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u8bad\u7ec3AI\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u6570\u636e\uff0c\u800c\u9690\u79c1\u548c\u4f26\u7406\u9650\u5236\u4f7f\u5f97\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u56f0\u96be\u3002", "method": "MAMBO\u91c7\u7528\u57fa\u4e8e\u5757\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\uff083840x3840\u50cf\u7d20\uff09\u7684\u4e73\u817aX\u5149\u7247\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAMBO\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u4e73\u817aX\u5149\u7247\uff0c\u5e76\u53ef\u7528\u4e8e\u5206\u7c7b\u6a21\u578b\u8bad\u7ec3\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "MAMBO\u5728\u56fe\u50cf\u751f\u6210\u3001\u8d85\u5206\u8fa8\u7387\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u6709\u671b\u63d0\u5347\u4e73\u817aX\u5149\u7247\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u65e9\u671f\u75c5\u53d8\u68c0\u6d4b\u3002"}}
{"id": "2506.08577", "pdf": "https://arxiv.org/pdf/2506.08577", "abs": "https://arxiv.org/abs/2506.08577", "authors": ["Nicholas A. Pearson", "Francesca Cairoli", "Luca Bortolussi", "Davide Russo", "Francesca Zanello"], "title": "Diffusion-based Time Series Forecasting for Sewerage Systems", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted for presentation at the 13th Urban Drainage Modelling\n  Conference, Innsbruck (Austria), September 2025", "summary": "We introduce a novel deep learning approach that harnesses the power of\ngenerative artificial intelligence to enhance the accuracy of contextual\nforecasting in sewerage systems. By developing a diffusion-based model that\nprocesses multivariate time series data, our system excels at capturing complex\ncorrelations across diverse environmental signals, enabling robust predictions\neven during extreme weather events. To strengthen the model's reliability, we\nfurther calibrate its predictions with a conformal inference technique,\ntailored for probabilistic time series data, ensuring that the resulting\nprediction intervals are statistically reliable and cover the true target\nvalues with a desired confidence level. Our empirical tests on real sewerage\nsystem data confirm the model's exceptional capability to deliver reliable\ncontextual predictions, maintaining accuracy even under severe weather\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u6c61\u6c34\u7cfb\u7edf\u4e0a\u4e0b\u6587\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u4fdd\u5f62\u63a8\u7406\u6280\u672f\uff0c\u786e\u4fdd\u9884\u6d4b\u7ed3\u679c\u7684\u7edf\u8ba1\u53ef\u9760\u6027\u3002", "motivation": "\u901a\u8fc7\u6355\u6349\u590d\u6742\u7684\u73af\u5883\u4fe1\u53f7\u76f8\u5173\u6027\uff0c\u63d0\u5347\u6c61\u6c34\u7cfb\u7edf\u5728\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u5904\u7406\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4fdd\u5f62\u63a8\u7406\u6280\u672f\u6821\u51c6\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5b9e\u8bc1\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u771f\u5b9e\u6c61\u6c34\u7cfb\u7edf\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u6c61\u6c34\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u7edf\u8ba1\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2506.08708", "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving.", "AI": {"tldr": "PhyBlock\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7269\u7406\u7406\u89e3\u548c\u89c4\u5212\u80fd\u529b\u7684\u6e10\u8fdb\u5f0f\u57fa\u51c6\uff0c\u901a\u8fc73D\u79ef\u6728\u7ec4\u88c5\u4efb\u52a1\u548c\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u7269\u7406\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u7406\u89e3\u7269\u7406\u73b0\u8c61\u548c3D\u73af\u5883\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PhyBench\u5305\u542b2600\u4e2a\u4efb\u52a1\uff08400\u4e2a\u7ec4\u88c5\u4efb\u52a1\u548c2200\u4e2aVQA\u4efb\u52a1\uff09\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u90e8\u5206\u5b8c\u6210\u3001\u6545\u969c\u8bca\u65ad\u548c\u89c4\u5212\u9c81\u68d2\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "result": "21\u4e2a\u5148\u8fdbVLMs\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u5b83\u4eec\u5728\u9ad8\u7ea7\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u5c24\u5176\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u8868\u73b0\u4e0b\u964d\u3002", "conclusion": "PhyBlock\u4e3a\u63d0\u5347VLMs\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u4f9d\u8d56\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2506.08594", "pdf": "https://arxiv.org/pdf/2506.08594", "abs": "https://arxiv.org/abs/2506.08594", "authors": ["Yixuan Ma", "Chang Liu", "Weikang Li", "Shun-Yao Zhang", "L. -M. Duan", "Yukai Wu", "Dong-Ling Deng"], "title": "Solving excited states for long-range interacting trapped ions with neural networks", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI", "cs.LG"], "comment": null, "summary": "The computation of excited states in strongly interacting quantum many-body\nsystems is of fundamental importance. Yet, it is notoriously challenging due to\nthe exponential scaling of the Hilbert space dimension with the system size.\nHere, we introduce a neural network-based algorithm that can simultaneously\noutput multiple low-lying excited states of a quantum many-body spin system in\nan accurate and efficient fashion. This algorithm, dubbed the neural quantum\nexcited-state (NQES) algorithm, requires no explicit orthogonalization of the\nstates and is generally applicable to higher dimensions. We demonstrate,\nthrough concrete examples including the Haldane-Shastry model with all-to-all\ninteractions, that the NQES algorithm is capable of efficiently computing\nmultiple excited states and their related observable expectations. In addition,\nwe apply the NQES algorithm to two classes of long-range interacting\ntrapped-ion systems in a two-dimensional Wigner crystal. For non-decaying\nall-to-all interactions with alternating signs, our computed low-lying excited\nstates bear spatial correlation patterns similar to those of the ground states,\nwhich closely match recent experimental observations that the\nquasi-adiabatically prepared state accurately reproduces analytical\nground-state correlations. For a system of up to 300 ions with power-law\ndecaying antiferromagnetic interactions, we successfully uncover its gap\nscaling and correlation features. Our results establish a scalable and\nefficient algorithm for computing excited states of interacting quantum\nmany-body systems, which holds potential applications ranging from benchmarking\nquantum devices to photoisomerization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7b97\u6cd5\uff08NQES\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u5f3a\u76f8\u4e92\u4f5c\u7528\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u4f4e\u6fc0\u53d1\u6001\uff0c\u65e0\u9700\u663e\u5f0f\u6b63\u4ea4\u5316\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u3002", "motivation": "\u5f3a\u76f8\u4e92\u4f5c\u7528\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u6fc0\u53d1\u6001\u8ba1\u7b97\u56e0\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ef4\u5ea6\u7684\u6307\u6570\u589e\u957f\u800c\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86NQES\u7b97\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u540c\u65f6\u8f93\u51fa\u591a\u4e2a\u4f4e\u6fc0\u53d1\u6001\uff0c\u5e76\u5e94\u7528\u4e8eHaldane-Shastry\u6a21\u578b\u548c\u4e8c\u7ef4Wigner\u6676\u4f53\u4e2d\u7684\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u7cfb\u7edf\u3002", "result": "NQES\u7b97\u6cd5\u80fd\u9ad8\u6548\u8ba1\u7b97\u591a\u4e2a\u6fc0\u53d1\u6001\u53ca\u5176\u76f8\u5173\u53ef\u89c2\u6d4b\u91cf\uff0c\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u89c2\u6d4b\u4e00\u81f4\uff0c\u5e76\u6210\u529f\u63ed\u793a\u4e86300\u79bb\u5b50\u7cfb\u7edf\u7684\u80fd\u9699\u6807\u5ea6\u548c\u5173\u8054\u7279\u5f81\u3002", "conclusion": "NQES\u7b97\u6cd5\u4e3a\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u6fc0\u53d1\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u4ece\u91cf\u5b50\u8bbe\u5907\u57fa\u51c6\u6d4b\u8bd5\u5230\u5149\u5f02\u6784\u5316\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2506.08716", "pdf": "https://arxiv.org/pdf/2506.08716", "abs": "https://arxiv.org/abs/2506.08716", "authors": ["Maximilian Tschuchnig", "Lukas Lamminger", "Philipp Steininger", "Michael Gadermayr"], "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment", "categories": ["eess.IV", "cs.CV"], "comment": "Data is open source. Code will be provided on acceptance. Paper\n  currently under review", "summary": "Cone-Beam Computed Tomography (CBCT) is widely used for real-time\nintraoperative imaging due to its low radiation dose and high acquisition\nspeed. However, despite its high resolution, CBCT suffers from significant\nartifacts and thereby lower visual quality, compared to conventional Computed\nTomography (CT). A recent approach to mitigate these artifacts is synthetic CT\n(sCT) generation, translating CBCT volumes into the CT domain. In this work, we\nenhance sCT generation through multimodal learning, integrating intraoperative\nCBCT with preoperative CT. Beyond validation on two real-world datasets, we use\na versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT\nquality affect sCT quality. The results demonstrate that multimodal sCT\nconsistently outperform unimodal baselines, with the most significant gains\nobserved in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate\nthat these findings are highly reproducible in real-world clinical datasets.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u5347CBCT\u5230CT\u7684\u5408\u6210\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4f4e\u8d28\u91cfCBCT-CT\u5bf9\u9f50\u60c5\u51b5\u4e0b\u7684\u663e\u8457\u6548\u679c\u3002", "motivation": "CBCT\u867d\u5feb\u901f\u4f4e\u8f90\u5c04\uff0c\u4f46\u5b58\u5728\u4f2a\u5f71\u95ee\u9898\uff0c\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\uff0c\u9700\u901a\u8fc7\u5408\u6210CT\uff08sCT\uff09\u63d0\u5347\u3002", "method": "\u7ed3\u5408\u672f\u4e2dCBCT\u4e0e\u672f\u524dCT\u8fdb\u884c\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u5206\u6790\u5bf9\u9f50\u548c\u8d28\u91cf\u5bf9sCT\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6a21\u6001sCT\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u4f4e\u8d28\u91cfCBCT-CT\u5bf9\u9f50\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u65b9\u6cd5\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e2d\u5177\u6709\u9ad8\u5ea6\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2506.08761", "pdf": "https://arxiv.org/pdf/2506.08761", "abs": "https://arxiv.org/abs/2506.08761", "authors": ["Matthias Beckmann", "Robert Beinert", "Jonas Bresch"], "title": "Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification", "categories": ["math.NA", "cs.CV", "cs.IT", "cs.NA", "math.IT"], "comment": null, "summary": "The Radon cumulative distribution transform (R-CDT), is an easy-to-compute\nfeature extractor that facilitates image classification tasks especially in the\nsmall data regime. It is closely related to the sliced Wasserstein distance and\nprovably guaranties the linear separability of image classes that emerge from\ntranslations or scalings. In many real-world applications, like the recognition\nof watermarks in filigranology, however, the data is subject to general affine\ntransformations originating from the measurement process. To overcome this\nissue, we recently introduced the so-called max-normalized R-CDT that only\nrequires elementary operations and guaranties the separability under arbitrary\naffine transformations. The aim of this paper is to continue our study of the\nmax-normalized R-CDT especially with respect to its robustness against\nnon-affine image deformations. Our sensitivity analysis shows that its\nseparability properties are stable provided the Wasserstein-infinity distance\nbetween the samples can be controlled. Since the Wasserstein-infinity distance\nonly allows small local image deformations, we moreover introduce a\nmean-normalized version of the R-CDT. In this case, robustness relates to the\nWasserstein-2 distance and also covers image deformations caused by impulsive\nnoise for instance. Our theoretical results are supported by numerical\nexperiments showing the effectiveness of our novel feature extractors as well\nas their robustness against local non-affine deformations and impulsive noise.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86max-normalized R-CDT\u548cmean-normalized R-CDT\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u975e\u4eff\u5c04\u53d8\u5f62\u548c\u8109\u51b2\u566a\u58f0\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u56fe\u50cf\u56e0\u6d4b\u91cf\u8fc7\u7a0b\u53d7\u5230\u4e00\u822c\u4eff\u5c04\u53d8\u6362\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u7814\u7a76R-CDT\u5728\u975e\u4eff\u5c04\u53d8\u5f62\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51famax-normalized R-CDT\u548cmean-normalized R-CDT\uff0c\u901a\u8fc7\u63a7\u5236Wasserstein\u8ddd\u79bb\u5206\u6790\u5176\u7a33\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u7279\u5f81\u63d0\u53d6\u5668\u5bf9\u5c40\u90e8\u975e\u4eff\u5c04\u53d8\u5f62\u548c\u8109\u51b2\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "max-normalized\u548cmean-normalized R-CDT\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.08602", "pdf": "https://arxiv.org/pdf/2506.08602", "abs": "https://arxiv.org/abs/2506.08602", "authors": ["Tingzhi Li", "Xuefeng Liu"], "title": "WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) are increasingly deployed in graph-related\napplications, making ownership verification critical to protect their\nintellectual property against model theft. Fingerprinting and black-box\nwatermarking are two main methods. However, the former relies on determining\nmodel similarity, which is computationally expensive and prone to ownership\ncollisions after model post-processing such as model pruning or fine-tuning.\nThe latter embeds backdoors, exposing watermarked models to the risk of\nbackdoor attacks. Moreover, both methods enable ownership verification but do\nnot convey additional information. As a result, each distributed model requires\na unique trigger graph, and all trigger graphs must be used to query the\nsuspect model during verification. Multiple queries increase the financial cost\nand the risk of detection.\n  To address these challenges, this paper proposes WGLE, a novel black-box\nwatermarking paradigm for GNNs that enables embedding the multi-bit string as\nthe ownership information without using backdoors. WGLE builds on a key insight\nwe term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the\ndifference between the feature distance and the prediction distance of two\nconnected nodes. By predefining positive or negative LDDE values for multiple\nselected edges, WGLE embeds the watermark encoding the intended information\nwithout introducing incorrect mappings that compromise the primary task. WGLE\nis evaluated on six public datasets and six mainstream GNN architectures along\nwith state-of-the-art methods. The results show that WGLE achieves 100%\nownership verification accuracy, an average fidelity degradation of 0.85%,\ncomparable robustness against potential attacks, and low embedding overhead.\nThe code is available in the repository.", "AI": {"tldr": "WGLE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GNN\u9ed1\u76d2\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7LDDE\u5d4c\u5165\u591a\u6bd4\u7279\u5b57\u7b26\u4e32\u4f5c\u4e3a\u6240\u6709\u6743\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u540e\u95e8\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u4f4e\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u73b0\u6709GNN\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\uff08\u6307\u7eb9\u548c\u9ed1\u76d2\u6c34\u5370\uff09\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u540e\u95e8\u653b\u51fb\u98ce\u9669\u6216\u65e0\u6cd5\u4f20\u9012\u989d\u5916\u4fe1\u606f\u7684\u95ee\u9898\uff0cWGLE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "WGLE\u57fa\u4e8eLDDE\uff08\u5c42\u95f4\u8ddd\u79bb\u5dee\u5f02\uff09\u6280\u672f\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u8fb9\u4e0a\u7684\u6b63\u8d1fLDDE\u503c\u5d4c\u5165\u6c34\u5370\uff0c\u907f\u514d\u4e86\u4e3b\u4efb\u52a1\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u516d\u79cdGNN\u67b6\u6784\u4e0a\uff0cWGLE\u5b9e\u73b0\u4e86100%\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u5e73\u5747\u6027\u80fd\u635f\u5931\u4ec50.85%\uff0c\u4e14\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u5f3a\u3002", "conclusion": "WGLE\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684GNN\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.09023", "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eViT\u6a21\u578b\u7684\u6750\u6599\u9009\u62e9\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5149\u7167\u548c\u53cd\u5c04\u53d8\u5316\uff0c\u652f\u6301\u7eb9\u7406\u548c\u5b50\u7eb9\u7406\u4e24\u7ea7\u9009\u62e9\u3002", "motivation": "\u56fe\u50cf\u7f16\u8f91\u4e2d\u6750\u6599\u9009\u62e9\u662f\u7b2c\u4e00\u6b65\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u5149\u7167\u548c\u53cd\u5c04\u53d8\u5316\u4e0d\u591f\u9c81\u68d2\u3002", "method": "\u5229\u7528ViT\u6a21\u578b\u7279\u5f81\uff0c\u63d0\u51fa\u591a\u5206\u8fa8\u7387\u5904\u7406\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u5305\u542b80\u4e07\u5f20\u5408\u6210\u56fe\u50cf\u7684DuMaS\u6570\u636e\u96c6\u3002", "result": "\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u7cbe\u7ec6\u548c\u7a33\u5b9a\u7684\u9009\u62e9\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6750\u6599\u9009\u62e9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u4e0b\u6e38\u7f16\u8f91\u4efb\u52a1\u3002"}}
{"id": "2506.08604", "pdf": "https://arxiv.org/pdf/2506.08604", "abs": "https://arxiv.org/abs/2506.08604", "authors": ["Giacomo Baldan", "Qiang Liu", "Alberto Guardone", "Nils Thuerey"], "title": "Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Generative machine learning methods, such as diffusion models and flow\nmatching, have shown great potential in modeling complex system behaviors and\nbuilding efficient surrogate models. However, these methods typically learn the\nunderlying physics implicitly from data. We propose Physics-Based Flow Matching\n(PBFM), a novel generative framework that explicitly embeds physical\nconstraints, both PDE residuals and algebraic relations, into the flow matching\nobjective. We also introduce temporal unrolling at training time that improves\nthe accuracy of the final, noise-free sample prediction. Our method jointly\nminimizes the flow matching loss and the physics-based residual loss without\nrequiring hyperparameter tuning of their relative weights. Additionally, we\nanalyze the role of the minimum noise level, $\\sigma_{\\min}$, in the context of\nphysical constraints and evaluate a stochastic sampling strategy that helps to\nreduce physical residuals. Through extensive benchmarks on three representative\nPDE problems, we show that our approach yields up to an $8\\times$ more accurate\nphysical residuals compared to FM, while clearly outperforming existing\nalgorithms in terms of distributional accuracy. PBFM thus provides a principled\nand efficient framework for surrogate modeling, uncertainty quantification, and\naccelerated simulation in physics and engineering applications.", "AI": {"tldr": "PBFM\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5d4c\u5165\u7269\u7406\u7ea6\u675f\uff08PDE\u6b8b\u5dee\u548c\u4ee3\u6570\u5173\u7cfb\uff09\u5230\u6d41\u5339\u914d\u76ee\u6807\u4e2d\uff0c\u63d0\u9ad8\u4e86\u7269\u7406\u6b8b\u5dee\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\uff09\u901a\u5e38\u4ece\u6570\u636e\u4e2d\u9690\u5f0f\u5b66\u4e60\u7269\u7406\u89c4\u5f8b\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u7ea6\u675f\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u63d0\u51faPBFM\u6846\u67b6\uff0c\u7ed3\u5408\u6d41\u5339\u914d\u635f\u5931\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6b8b\u5dee\u635f\u5931\uff0c\u65e0\u9700\u8c03\u6574\u8d85\u53c2\u6570\u6743\u91cd\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u5c55\u5f00\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027PDE\u95ee\u9898\u4e0a\uff0cPBFM\u7684\u7269\u7406\u6b8b\u5dee\u6bd4\u4f20\u7edf\u6d41\u5339\u914d\u65b9\u6cd5\u7cbe\u786e8\u500d\uff0c\u4e14\u5728\u5206\u5e03\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "PBFM\u4e3a\u7269\u7406\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u4ee3\u7406\u5efa\u6a21\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u52a0\u901f\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u5b8c\u5907\u7684\u6846\u67b6\u3002"}}
{"id": "2506.08652", "pdf": "https://arxiv.org/pdf/2506.08652", "abs": "https://arxiv.org/abs/2506.08652", "authors": ["Mahesh Godavarti"], "title": "JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset", "categories": ["cs.LG", "cs.AI", "20-XX, 08A02", "F.4.1; I.2"], "comment": null, "summary": "Transformers have demonstrated remarkable success in sequence modeling, yet\neffectively incorporating positional information remains a challenging and\nactive area of research. In this paper, we introduce JoFormer, a journey-based\nTransformer architecture grounded in a recently proposed non-commutative\nalgebra for composing transformations across positions. JoFormer represents\nrelative positions through learnable directional transforms that are\nsequentially composed along the input, thereby extending and generalizing\nexisting approaches based on relative position representations. We derive the\nJoFormer attention mechanism from first principles and show that it subsumes\nstandard methods such as rotary transformations as special cases. To evaluate\nits effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny\nShakespeare character-level language modeling task. Our results demonstrate\nthat\n  JoFormer consistently achieves lower perplexity and faster convergence,\nhighlighting the advantages of its more expressive, journey-based treatment of\nposition. Notably, the per-token JoFormer is still a primitive, conceptual\nvariant with layer-independent angles, yet it already demonstrates strong\nperformance-underscoring its promise as a proof of concept for more expressive\narchitectures. We conclude by discussing how JoFormer offers a principled\napproach to integrating positional structure into Transformer architectures.\nThe code used in this work is available at\nhttps://github.com/mahesh-godavarti/joformer.", "AI": {"tldr": "JoFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u65c5\u7a0b\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5b9a\u5411\u53d8\u6362\u8868\u793a\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u4f18\u4e8eRoFormer\uff0c\u5728Tiny Shakespeare\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3Transformer\u4e2d\u6709\u6548\u6574\u5408\u4f4d\u7f6e\u4fe1\u606f\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u975e\u4ea4\u6362\u4ee3\u6570\u7684JoFormer\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5b9a\u5411\u53d8\u6362\u8868\u793a\u76f8\u5bf9\u4f4d\u7f6e\u3002", "result": "JoFormer\u5728Tiny Shakespeare\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u4f4e\u56f0\u60d1\u5ea6\u548c\u66f4\u5feb\u6536\u655b\u3002", "conclusion": "JoFormer\u4e3aTransformer\u4e2d\u4f4d\u7f6e\u4fe1\u606f\u6574\u5408\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.08660", "pdf": "https://arxiv.org/pdf/2506.08660", "abs": "https://arxiv.org/abs/2506.08660", "authors": ["Jinkwan Jang", "Hyungjin Park", "Jinmyeong Choi", "Taesup Kim"], "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world time series data are inherently multivariate, often exhibiting\ncomplex inter-channel dependencies. Each channel is typically sampled at its\nown period and is prone to missing values due to various practical and\noperational constraints. These characteristics pose fundamental challenges\nrelated to channel dependency, sampling asynchrony, and missingness, all of\nwhich must be addressed to enable robust and reliable forecasting in practical\nsettings. However, most existing architectures are built on oversimplified\nassumptions, such as identical sampling periods across channels and fully\nobserved inputs at test time, which often do not hold in real-world scenarios.\nTo bridge this gap, we propose ChannelTokenFormer, a Transformer-based\nforecasting model with a flexible architecture designed to explicitly capture\ncross-channel interactions, accommodate channel-wise asynchronous sampling, and\neffectively handle missing values. Extensive experiments on three benchmark\ndatasets modified to reflect practical settings, along with one real-world\nindustrial dataset, demonstrate the superior robustness and accuracy of\nChannelTokenFormer under challenging real-world conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578bChannelTokenFormer\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u901a\u9053\u4f9d\u8d56\u3001\u5f02\u6b65\u91c7\u6837\u548c\u7f3a\u5931\u503c\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u901a\u5e38\u5b58\u5728\u901a\u9053\u4f9d\u8d56\u3001\u5f02\u6b65\u91c7\u6837\u548c\u7f3a\u5931\u503c\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u57fa\u4e8e\u8fc7\u4e8e\u7b80\u5316\u7684\u5047\u8bbe\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86ChannelTokenFormer\u6a21\u578b\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u67b6\u6784\u8bbe\u8ba1\u663e\u5f0f\u6355\u6349\u8de8\u901a\u9053\u4ea4\u4e92\uff0c\u9002\u5e94\u901a\u9053\u5f02\u6b65\u91c7\u6837\uff0c\u5e76\u6709\u6548\u5904\u7406\u7f3a\u5931\u503c\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChannelTokenFormer\u5728\u590d\u6742\u73b0\u5b9e\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "ChannelTokenFormer\u4e3a\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2506.08662", "pdf": "https://arxiv.org/pdf/2506.08662", "abs": "https://arxiv.org/abs/2506.08662", "authors": ["Florian Borzechowski", "Michael Sch\u00e4fer", "Heiko Schwarz", "Jonathan Pfaff", "Detlev Marpe", "Thomas Wiegand"], "title": "Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "Accepted at ICIP2024, the IEEE International Conference on Image\n  Processing", "summary": "The continuous improvements on image compression with variational\nautoencoders have lead to learned codecs competitive with conventional\napproaches in terms of rate-distortion efficiency. Nonetheless, taking the\nquantization into account during the training process remains a problem, since\nit produces zero derivatives almost everywhere and needs to be replaced with a\ndifferentiable approximation which allows end-to-end optimization. Though there\nare different methods for approximating the quantization, none of them model\nthe quantization noise correctly and thus, result in suboptimal networks.\nHence, we propose an additional finetuning training step: After conventional\nend-to-end training, parts of the network are retrained on quantized latents\nobtained at the inference stage. For entropy-constraint quantizers like\nTrellis-Coded Quantization, the impact of the quantizer is particularly\ndifficult to approximate by rounding or adding noise as the quantized latents\nare interdependently chosen through a trellis search based on both the entropy\nmodel and a distortion measure. We show that retraining on correctly quantized\ndata consistently yields additional coding gain for both uniform scalar and\nespecially for entropy-constraint quantization, without increasing inference\ncomplexity. For the Kodak test set, we obtain average savings between 1% and\n2%, and for the TecNick test set up to 2.2% in terms of Bj{\\o}ntegaard-Delta\nbitrate.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53d8\u5206\u81ea\u7f16\u7801\u5668\u56fe\u50cf\u538b\u7f29\u7684\u989d\u5916\u5fae\u8c03\u8bad\u7ec3\u6b65\u9aa4\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u5bf9\u91cf\u5316\u6f5c\u5728\u8868\u793a\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u7f16\u7801\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u91cf\u5316\u7684\u8fd1\u4f3c\u5904\u7406\u4e0d\u51c6\u786e\uff0c\u5bfc\u81f4\u7f51\u7edc\u6027\u80fd\u6b21\u4f18\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5728\u4f20\u7edf\u7aef\u5230\u7aef\u8bad\u7ec3\u540e\uff0c\u5bf9\u7f51\u7edc\u90e8\u5206\u8fdb\u884c\u57fa\u4e8e\u91cf\u5316\u6f5c\u5728\u8868\u793a\u7684\u5fae\u8c03\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5747\u5300\u6807\u91cf\u91cf\u5316\u548c\u71b5\u7ea6\u675f\u91cf\u5316\u4e2d\u5747\u80fd\u5e26\u6765\u989d\u5916\u7f16\u7801\u589e\u76ca\uff0cKodak\u548cTecNick\u6d4b\u8bd5\u96c6\u7684\u6bd4\u7279\u7387\u5e73\u5747\u8282\u77011%-2.2%\u3002", "conclusion": "\u901a\u8fc7\u6b63\u786e\u91cf\u5316\u6570\u636e\u7684\u5fae\u8c03\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u538b\u7f29\u6027\u80fd\u3002"}}
{"id": "2506.08669", "pdf": "https://arxiv.org/pdf/2506.08669", "abs": "https://arxiv.org/abs/2506.08669", "authors": ["Dongge Han", "Menglin Xia", "Daniel Madrigal Diaz", "Samuel Kessler", "Ankur Mallick", "Xuchao Zhang", "Mirian Del Carmen Hipolito Garcia", "Jin Xu", "Victor R\u00fchle", "Saravan Rajmohan"], "title": "Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search", "categories": ["cs.LG", "cs.AI"], "comment": "TTODLer-FM Workshop@ICML'25 (Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models)", "summary": "Small language models (SLMs) offer promising and efficient alternatives to\nlarge language models (LLMs). However, SLMs' limited capacity restricts their\nreasoning capabilities and makes them sensitive to prompt variations. To\naddress these challenges, we propose a novel framework that enhances SLM\nreasoning capabilities through LLM generated blueprints. The blueprints provide\nstructured, high-level reasoning guides that help SLMs systematically tackle\nrelated problems. Furthermore, our framework integrates a prompt template\nsearch mechanism to mitigate the SLMs' sensitivity to prompt variations. Our\nframework demonstrates improved SLM performance across various tasks, including\nmath (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves\nthe reasoning capabilities of SLMs without increasing model size or requiring\nadditional training, offering a lightweight and deployment-friendly solution\nfor on-device or resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7LLM\u751f\u6210\u7684\u84dd\u56fe\u589e\u5f3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u63d0\u793a\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "motivation": "SLM\u56e0\u5bb9\u91cf\u6709\u9650\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u4e14\u5bf9\u63d0\u793a\u53d8\u5316\u654f\u611f\uff0c\u9700\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u7ed3\u6784\u5316\u84dd\u56fe\u6307\u5bfcSLM\u63a8\u7406\uff0c\u5e76\u96c6\u6210\u63d0\u793a\u6a21\u677f\u641c\u7d22\u673a\u5236\u3002", "result": "\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347SLM\u6027\u80fd\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6216\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7684SLM\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2506.08698", "pdf": "https://arxiv.org/pdf/2506.08698", "abs": "https://arxiv.org/abs/2506.08698", "authors": ["Boyu Xie", "Tangtang Xie"], "title": "Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 2 figures", "summary": "With the development of smart grids, High-Dimensional and Incomplete (HDI)\nPower Load Monitoring (PLM) data challenges the performance of Power Load\nForecasting (PLF) models. In this paper, we propose a potential\ncharacterization model VAE-LF based on Variational Autoencoder (VAE) for\nefficiently representing and complementing PLM missing data. VAE-LF learns a\nlow-dimensional latent representation of the data using an Encoder-Decoder\nstructure by splitting the HDI PLM data into vectors and feeding them\nsequentially into the VAE-LF model, and generates the complementary data.\nExperiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark\nmodels in both 5% and 10% sparsity test cases, with significantly lower RMSE\nand MAE, and especially outperforms on low sparsity ratio data. The method\nprovides an efficient data-completion solution for electric load management in\nsmart grids.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684VAE-LF\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u8868\u793a\u548c\u8865\u5168\u9ad8\u7ef4\u4e0d\u5b8c\u6574\uff08HDI\uff09\u7535\u529b\u8d1f\u8377\u76d1\u6d4b\uff08PLM\uff09\u6570\u636e\uff0c\u63d0\u5347\u7535\u529b\u8d1f\u8377\u9884\u6d4b\uff08PLF\uff09\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u4e2d\u9ad8\u7ef4\u4e0d\u5b8c\u6574\u7684PLM\u6570\u636e\u5bf9PLF\u6a21\u578b\u6027\u80fd\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u8865\u5168\u65b9\u6cd5\u3002", "method": "VAE-LF\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u5b66\u4e60\u6570\u636e\u7684\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u5c06HDI PLM\u6570\u636e\u5206\u5757\u8f93\u5165\u6a21\u578b\u5e76\u751f\u6210\u8865\u5168\u6570\u636e\u3002", "result": "\u5728UK-DALE\u6570\u636e\u96c6\u4e0a\uff0cVAE-LF\u57285%\u548c10%\u7a00\u758f\u5ea6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0cRMSE\u548cMAE\u663e\u8457\u964d\u4f4e\uff0c\u5c24\u5176\u5728\u4f4e\u7a00\u758f\u5ea6\u6570\u636e\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VAE-LF\u4e3a\u667a\u80fd\u7535\u7f51\u4e2d\u7684\u7535\u529b\u8d1f\u8377\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u8865\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08727", "pdf": "https://arxiv.org/pdf/2506.08727", "abs": "https://arxiv.org/abs/2506.08727", "authors": ["Samarth Sikand", "Rohit Mehra", "Priyavanshi Pathania", "Nikhil Bamby", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.SE"], "comment": "5 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "summary": "While Generative AI stands to be one of the fastest adopted technologies\never, studies have made evident that the usage of Large Language Models (LLMs)\nputs significant burden on energy grids and our environment. It may prove a\nhindrance to the Sustainability goals of any organization. A crucial step in\nany Sustainability strategy is monitoring or estimating the energy consumption\nof various components. While there exist multiple tools for monitoring energy\nconsumption, there is a dearth of tools/frameworks for estimating the\nconsumption or carbon emissions. Current drawbacks of both monitoring and\nestimation tools include high input data points, intrusive nature, high error\nmargin, etc. We posit that leveraging emerging LLM benchmarks and related data\npoints can help overcome aforementioned challenges while balancing accuracy of\nthe emission estimations. To that extent, we discuss the challenges of current\napproaches and present our evolving framework, R-ICE, which estimates prompt\nlevel inference carbon emissions by leveraging existing state-of-the-art(SOTA)\nbenchmark. This direction provides a more practical and non-intrusive way to\nenable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our\npromising validation results suggest that benchmark-based modelling holds great\npotential for inference emission estimation and warrants further exploration\nfrom the scientific community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528LLM\u57fa\u51c6\u6570\u636e\u4f30\u7b97\u63a8\u7406\u78b3\u6392\u653e\u7684\u6846\u67b6R-ICE\uff0c\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u7684\u9ad8\u8bef\u5dee\u548c\u4fb5\u5165\u6027\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u9ad8\u80fd\u8017\u53ef\u80fd\u963b\u788d\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u73b0\u6709\u5de5\u5177\u5728\u4f30\u7b97\u78b3\u6392\u653e\u65f6\u5b58\u5728\u6570\u636e\u9700\u6c42\u9ad8\u3001\u8bef\u5dee\u5927\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faR-ICE\u6846\u67b6\uff0c\u5229\u7528SOTA\u57fa\u51c6\u6570\u636e\u4f30\u7b97\u63d0\u793a\u7ea7\u63a8\u7406\u78b3\u6392\u653e\uff0c\u5b9e\u73b0\u975e\u4fb5\u5165\u6027\u4f30\u7b97\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u57fa\u51c6\u7684\u5efa\u6a21\u5728\u78b3\u6392\u653e\u4f30\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "R-ICE\u4e3a\u52a8\u6001LLM\u8def\u7531\u548c\u78b3\u6838\u7b97\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.08737", "pdf": "https://arxiv.org/pdf/2506.08737", "abs": "https://arxiv.org/abs/2506.08737", "authors": ["Haozhe Ma", "Guoji Fu", "Zhengding Luo", "Jiele Wu", "Tze-Yun Leong"], "title": "Exploration by Random Reward Perturbation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Random Reward Perturbation (RRP), a novel exploration strategy\nfor reinforcement learning (RL). Our theoretical analyses demonstrate that\nadding zero-mean noise to environmental rewards effectively enhances policy\ndiversity during training, thereby expanding the range of exploration. RRP is\nfully compatible with the action-perturbation-based exploration strategies,\nsuch as $\\epsilon$-greedy, stochastic policies, and entropy regularization,\nproviding additive improvements to exploration effects. It is general,\nlightweight, and can be integrated into existing RL algorithms with minimal\nimplementation effort and negligible computational overhead. RRP establishes a\ntheoretical connection between reward shaping and noise-driven exploration,\nhighlighting their complementary potential. Experiments show that RRP\nsignificantly boosts the performance of Proximal Policy Optimization and Soft\nActor-Critic, achieving higher sample efficiency and escaping local optima\nacross various tasks, under both sparse and dense reward scenarios.", "AI": {"tldr": "RRP\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u5411\u73af\u5883\u5956\u52b1\u6dfb\u52a0\u96f6\u5747\u503c\u566a\u58f0\u589e\u5f3a\u7b56\u7565\u591a\u6837\u6027\uff0c\u63d0\u5347\u63a2\u7d22\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982\u52a8\u4f5c\u6270\u52a8\u53ef\u80fd\u4e0d\u591f\u9ad8\u6548\u3002RRP\u65e8\u5728\u901a\u8fc7\u5956\u52b1\u6270\u52a8\u8865\u5145\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u3002", "method": "RRP\u5728\u73af\u5883\u5956\u52b1\u4e2d\u6dfb\u52a0\u96f6\u5747\u503c\u566a\u58f0\uff0c\u4e0e\u52a8\u4f5c\u6270\u52a8\u65b9\u6cd5\uff08\u5982\u03b5-greedy\u3001\u968f\u673a\u7b56\u7565\uff09\u517c\u5bb9\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRRP\u663e\u8457\u63d0\u5347\u4e86PPO\u548cSAC\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u5e2e\u52a9\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u3002", "conclusion": "RRP\u662f\u4e00\u79cd\u901a\u7528\u3001\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u5956\u52b1\u6270\u52a8\u4e0e\u566a\u58f0\u9a71\u52a8\u63a2\u7d22\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2506.08743", "pdf": "https://arxiv.org/pdf/2506.08743", "abs": "https://arxiv.org/abs/2506.08743", "authors": ["Michael F\u00e4rber", "David Lamprecht", "Yuni Susanti"], "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.DB", "cs.LG"], "comment": "Accepted at DASFAA 2025", "summary": "Graph Neural Networks (GNNs) have substantially advanced the field of\nrecommender systems. However, despite the creation of more than a thousand\nknowledge graphs (KGs) under the W3C standard RDF, their rich semantic\ninformation has not yet been fully leveraged in GNN-based recommender systems.\nTo address this gap, we propose a comprehensive integration of RDF KGs with\nGNNs that utilizes both the topological information from RDF object properties\nand the content information from RDF datatype properties. Our main focus is an\nin-depth evaluation of various GNNs, analyzing how different semantic feature\ninitializations and types of graph structure heterogeneity influence their\nperformance in recommendation tasks. Through experiments across multiple\nrecommendation scenarios involving multi-million-node RDF graphs, we\ndemonstrate that harnessing the semantic richness of RDF KGs significantly\nimproves recommender systems and lays the groundwork for GNN-based recommender\nsystems for the Linked Open Data cloud. The code and data are available on our\nGitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06RDF\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5168\u9762\u6574\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u5927\u91cfRDF\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f46\u5176\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u5728\u57fa\u4e8eGNN\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u5c1a\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u901a\u8fc7\u7ed3\u5408RDF\u5bf9\u8c61\u5c5e\u6027\u7684\u62d3\u6251\u4fe1\u606f\u548c\u6570\u636e\u7c7b\u578b\u5c5e\u6027\u7684\u5185\u5bb9\u4fe1\u606f\uff0c\u8bc4\u4f30\u4e0d\u540cGNN\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528RDF KGs\u7684\u8bed\u4e49\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8eGNN\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u5f00\u653e\u6570\u636e\u4e91\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.08756", "pdf": "https://arxiv.org/pdf/2506.08756", "abs": "https://arxiv.org/abs/2506.08756", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u7684\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u9700\u8981\u9002\u5e94\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u800c\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\u5728\u672a\u77e5\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u7269\u7406\u5efa\u6a21\u3001\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u5143\u5b66\u4e60\uff0c\u5c06\u7269\u7406\u7b26\u53f7\u63a8\u7406\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u8fd9\u79cd\u6df7\u5408\u67b6\u6784\u6709\u671b\u4f7f\u673a\u5668\u4eba\u8d85\u8d8a\u8bad\u7ec3\u6570\u636e\u6cdb\u5316\uff0c\u5904\u7406\u65b0\u60c5\u51b5\uff0c\u5e76\u6301\u7eed\u6269\u5c55\u77e5\u8bc6\u3002", "conclusion": "\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u662f\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u5e76\u63d0\u4f9b\u4e86\u7814\u7a76\u8def\u7ebf\u56fe\u4ee5\u52a0\u901f\u5176\u53d1\u5c55\u3002"}}
{"id": "2506.08774", "pdf": "https://arxiv.org/pdf/2506.08774", "abs": "https://arxiv.org/abs/2506.08774", "authors": ["Fan Xu", "Luis A. Leiva"], "title": "Multimodal Representation Alignment for Cross-modal Information Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Different machine learning models can represent the same underlying concept\nin different ways. This variability is particularly valuable for in-the-wild\nmultimodal retrieval, where the objective is to identify the corresponding\nrepresentation in one modality given another modality as input. This challenge\ncan be effectively framed as a feature alignment problem. For example, given a\nsentence encoded by a language model, retrieve the most semantically aligned\nimage based on features produced by an image encoder, or vice versa. In this\nwork, we first investigate the geometric relationships between visual and\ntextual embeddings derived from both vision-language models and combined\nunimodal models. We then align these representations using four standard\nsimilarity metrics as well as two learned ones, implemented via neural\nnetworks. Our findings indicate that the Wasserstein distance can serve as an\ninformative measure of the modality gap, while cosine similarity consistently\noutperforms alternative metrics in feature alignment tasks. Furthermore, we\nobserve that conventional architectures such as multilayer perceptrons are\ninsufficient for capturing the complex interactions between image and text\nrepresentations. Our study offers novel insights and practical considerations\nfor researchers working in multimodal information retrieval, particularly in\nreal-world, cross-modal applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u53d1\u73b0Wasserstein\u8ddd\u79bb\u548c\u4f59\u5f26\u76f8\u4f3c\u6027\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u6307\u51fa\u4f20\u7edf\u67b6\u6784\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u4e0d\u540c\u6a21\u578b\u8868\u793a\u540c\u4e00\u6982\u5ff5\u65f6\u7684\u53d8\u5f02\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u89e3\u51b3\u8de8\u6a21\u6001\u68c0\u7d22\u95ee\u9898\u3002", "method": "\u5206\u6790\u89c6\u89c9\u548c\u6587\u672c\u5d4c\u5165\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u4f7f\u7528\u56db\u79cd\u6807\u51c6\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u4e24\u79cd\u5b66\u4e60\u5ea6\u91cf\u8fdb\u884c\u7279\u5f81\u5bf9\u9f50\uff0c\u5e76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "Wasserstein\u8ddd\u79bb\u53ef\u4f5c\u4e3a\u6a21\u6001\u5dee\u8ddd\u7684\u5ea6\u91cf\uff0c\u4f59\u5f26\u76f8\u4f3c\u6027\u5728\u7279\u5f81\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f20\u7edf\u67b6\u6784\u65e0\u6cd5\u6355\u6349\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u8de8\u6a21\u6001\u5e94\u7528\u3002"}}
{"id": "2506.08785", "pdf": "https://arxiv.org/pdf/2506.08785", "abs": "https://arxiv.org/abs/2506.08785", "authors": ["Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration", "categories": ["cs.AR", "cs.AI", "cs.CC", "eess.IV"], "comment": null, "summary": "The increasing complexity of AI models requires flexible hardware capable of\nsupporting diverse precision formats, particularly for energy-constrained edge\nplatforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC\nengine that performs efficient multiply-accumulate operations using a unified\ndata-path for 4/8/16-bit fixed-point, floating point, and posit formats. The\narchitecture incorporates a layer adaptive precision strategy to align\ncomputational accuracy with workload sensitivity, optimizing both performance\nand energy usage. PARV-CE integrates quantization-aware execution with a\nreconfigurable SIMD pipeline, enabling high-throughput processing with minimal\noverhead through hardware-software co-design. The results demonstrate up to 2x\nimprovement in PDP and 3x reduction in resource usage compared to SoTA designs,\nwhile retaining accuracy within 1.8% FP32 baseline. The architecture supports\nboth on-device training and inference across a range of workloads, including\nDNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE\nincorporated POLARON as a scalable and energy-efficient solution for\nprecision-adaptive AI acceleration at edge.", "AI": {"tldr": "PARV-CE\u662f\u4e00\u79cd\u652f\u6301\u591a\u7cbe\u5ea6\u683c\u5f0f\u7684SIMD MAC\u5f15\u64ce\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u6027\u80fd\u548c\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18AI\u52a0\u901f\u3002", "motivation": "AI\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u9700\u8981\u7075\u6d3b\u652f\u6301\u591a\u79cd\u7cbe\u5ea6\u683c\u5f0f\u7684\u786c\u4ef6\uff0c\u5c24\u5176\u662f\u80fd\u8017\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u6570\u636e\u8def\u5f84\u652f\u63014/8/16\u4f4d\u5b9a\u70b9\u3001\u6d6e\u70b9\u548cposit\u683c\u5f0f\uff0c\u7ed3\u5408\u5c42\u81ea\u9002\u5e94\u7cbe\u5ea6\u7b56\u7565\u548c\u53ef\u91cd\u6784SIMD\u6d41\u6c34\u7ebf\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0cPDP\u63d0\u53472\u500d\uff0c\u8d44\u6e90\u4f7f\u7528\u51cf\u5c113\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u57281.8%\u4ee5\u5185\u3002", "conclusion": "PARV-CE\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u8fb9\u7f18AI\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08790", "pdf": "https://arxiv.org/pdf/2506.08790", "abs": "https://arxiv.org/abs/2506.08790", "authors": ["Samarth Sikand", "Rohit Mehra", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Do Generative AI Tools Ensure Green Code? An Investigative Study", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "4 pages. To be published in the proceedings of 2nd International\n  Workshop on Responsible AI Engineering (RAIE '24), co-located with ICSE '24,\n  Lisbon, Portugal", "summary": "Software sustainability is emerging as a primary concern, aiming to optimize\nresource utilization, minimize environmental impact, and promote a greener,\nmore resilient digital ecosystem. The sustainability or \"greenness\" of software\nis typically determined by the adoption of sustainable coding practices. With a\nmaturing ecosystem around generative AI, many software developers now rely on\nthese tools to generate code using natural language prompts. Despite their\npotential advantages, there is a significant lack of studies on the\nsustainability aspects of AI-generated code. Specifically, how environmentally\nfriendly is the AI-generated code based upon its adoption of sustainable coding\npractices? In this paper, we present the results of an early investigation into\nthe sustainability aspects of AI-generated code across three popular generative\nAI tools - ChatGPT, BARD, and Copilot. The results highlight the default\nnon-green behavior of tools for generating code, across multiple rules and\nscenarios. It underscores the need for further in-depth investigations and\neffective remediation strategies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u4ee3\u7801\u7684\u53ef\u6301\u7eed\u6027\uff0c\u53d1\u73b0\u4e3b\u6d41\u5de5\u5177\uff08\u5982ChatGPT\u3001BARD\u3001Copilot\uff09\u751f\u6210\u7684\u4ee3\u7801\u666e\u904d\u4e0d\u7b26\u5408\u7eff\u8272\u7f16\u7801\u5b9e\u8df5\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u666e\u53ca\uff0c\u5176\u4ee3\u7801\u751f\u6210\u5bf9\u73af\u5883\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u79cd\u6d41\u884c\u751f\u6210\u5f0fAI\u5de5\u5177\uff08ChatGPT\u3001BARD\u3001Copilot\uff09\u751f\u6210\u7684\u4ee3\u7801\uff0c\u8bc4\u4f30\u5176\u662f\u5426\u7b26\u5408\u53ef\u6301\u7eed\u7f16\u7801\u5b9e\u8df5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u5de5\u5177\u751f\u6210\u7684\u4ee3\u7801\u666e\u904d\u4e0d\u7b26\u5408\u7eff\u8272\u7f16\u7801\u6807\u51c6\uff0c\u5b58\u5728\u73af\u5883\u4e0d\u53cb\u597d\u7684\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u9700\u8fdb\u4e00\u6b65\u6df1\u5165\u7814\u7a76\u5e76\u5236\u5b9a\u6709\u6548\u7b56\u7565\uff0c\u4ee5\u63d0\u5347AI\u751f\u6210\u4ee3\u7801\u7684\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2506.08795", "pdf": "https://arxiv.org/pdf/2506.08795", "abs": "https://arxiv.org/abs/2506.08795", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u50cf\u5934\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u5168\u81ea\u4e3b\u5047\u624b\u63a7\u5236\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u6293\u53d6\u548c\u91ca\u653e\u4e0d\u540c\u5f62\u72b6\u7684\u7269\u4f53\uff0c\u51cf\u5c11\u7528\u6237\u7684\u5fc3\u7406\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u808c\u7535\u4fe1\u53f7\u63a7\u5236\u5047\u624b\u9700\u8981\u7528\u6237\u4e3b\u52a8\u751f\u6210\u4fe1\u53f7\uff0c\u5bf9\u7528\u6237\u8eab\u5fc3\u8d1f\u62c5\u5927\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5168\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u7b80\u5316\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u6444\u50cf\u5934\u91c7\u96c6\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u5047\u624b\u63a7\u5236\u6a21\u578b\uff0c\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u5b9e\u73b0\u81ea\u4e3b\u6293\u53d6\u548c\u91ca\u653e\u3002", "result": "\u4ec5\u7528\u5c11\u91cf\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u6210\u529f\u63a8\u5e7f\u5230\u4e0d\u540c\u7528\u6237\u548c\u672a\u89c1\u8fc7\u7684\u7269\u4f53\uff0c\u6293\u53d6\u6210\u529f\u7387\u8f83\u9ad8\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u622a\u80a2\u8005\u63d0\u4f9b\u4e86\u6613\u7528\u7684\u5047\u624b\u63a7\u5236\u63a5\u53e3\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f7f\u7528\u65f6\u7684\u5fc3\u7406\u8d1f\u62c5\u3002"}}
{"id": "2506.08822", "pdf": "https://arxiv.org/pdf/2506.08822", "abs": "https://arxiv.org/abs/2506.08822", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available.", "AI": {"tldr": "FreqPolicy\u901a\u8fc7\u9891\u7387\u4e00\u81f4\u6027\u7ea6\u675f\u63d0\u5347\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6548\u7387\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u4e00\u6b65\u52a8\u4f5c\u751f\u6210\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u56e0\u80fd\u5efa\u6a21\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u800c\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u591a\u6b65\u91c7\u6837\u7684\u9ad8\u63a8\u7406\u6210\u672c\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u501f\u9274\u56fe\u50cf\u751f\u6210\u7684\u52a0\u901f\u6280\u672f\uff0c\u4f46\u5ffd\u89c6\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51faFreqPolicy\uff0c\u901a\u8fc7\u9891\u7387\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4f7f\u57fa\u4e8e\u6d41\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6709\u6548\u6355\u6349\u65f6\u95f4\u7ed3\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u6b65\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u57283\u4e2a\u4eff\u771f\u57fa\u51c6\u768453\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u96c6\u6210\u5230VLA\u6a21\u578b\u540e\u572840\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u52a0\u901f\u4e14\u6027\u80fd\u65e0\u635f\uff0c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u63a8\u7406\u9891\u7387\u8fbe93.5Hz\u3002", "conclusion": "FreqPolicy\u901a\u8fc7\u9891\u7387\u4e00\u81f4\u6027\u7ea6\u675f\u663e\u8457\u63d0\u5347\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7cfb\u7edf\u3002"}}
{"id": "2506.08860", "pdf": "https://arxiv.org/pdf/2506.08860", "abs": "https://arxiv.org/abs/2506.08860", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "On The Impact of Merge Request Deviations on Code Review Practices", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Code review is a key practice in software engineering, ensuring quality and\ncollaboration. However, industrial Merge Request (MR) workflows often deviate\nfrom standardized review processes, with many MRs serving non-review purposes\n(e.g., drafts, rebases, or dependency updates). We term these cases deviations\nand hypothesize that ignoring them biases analytics and undermines ML models\nfor review analysis.\n  We identify seven deviation categories, occurring in 37.02% of MRs, and\npropose a few-shot learning detection method (91% accuracy). By excluding\ndeviations, ML models predicting review completion time improve performance in\n53.33% of cases (up to 2.25x) and exhibit significant shifts in feature\nimportance (47% overall, 60% top-*k*).\n  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven\ndetection approach, and (3) empirical evidence of their impact on ML-based\nreview analytics. This work aids practitioners in optimizing review efforts and\nensuring reliable insights.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u504f\u5dee\u884c\u4e3a\uff08\u5982\u8349\u7a3f\u3001\u4f9d\u8d56\u66f4\u65b0\u7b49\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u6392\u9664\u8fd9\u4e9b\u504f\u5dee\u80fd\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u4e2d\u7684\u5408\u5e76\u8bf7\u6c42\uff08MR\uff09\u6d41\u7a0b\u5e38\u504f\u79bb\u6807\u51c6\u5ba1\u67e5\u6d41\u7a0b\uff0c\u8fd9\u4e9b\u504f\u5dee\u53ef\u80fd\u5f71\u54cd\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "method": "\u8bc6\u522b\u4e86\u4e03\u7c7b\u504f\u5dee\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\uff08\u51c6\u786e\u738791%\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6392\u9664\u504f\u5dee\u5bf9ML\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "37.02%\u7684MR\u5b58\u5728\u504f\u5dee\uff0c\u6392\u9664\u540eML\u6a21\u578b\u6027\u80fd\u63d0\u534753.33%\uff0c\u7279\u5f81\u91cd\u8981\u6027\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u4ee3\u7801\u5ba1\u67e5\u63d0\u4f9b\u4e86\u5206\u7c7b\u3001\u68c0\u6d4b\u65b9\u6cd5\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5206\u6790\u53ef\u9760\u6027\u3002"}}
{"id": "2506.08889", "pdf": "https://arxiv.org/pdf/2506.08889", "abs": "https://arxiv.org/abs/2506.08889", "authors": ["Yizhao Gao", "Shuming Guo", "Shijie Cao", "Yuqing Xia", "Yu Cheng", "Lei Wang", "Lingxiao Ma", "Yutao Sun", "Tianzhu Ye", "Li Dong", "Hayden Kwok-Hay So", "Yu Hua", "Ting Cao", "Fan Yang", "Mao Yang"], "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.", "AI": {"tldr": "SeerAttention-R\u662f\u4e00\u4e2a\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u4e13\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u957f\u89e3\u7801\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u95e8\u63a7\u673a\u5236\u5b66\u4e60\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u5e76\u4f18\u5316\u89e3\u7801\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u89e3\u7801\u63a8\u7406\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u7a00\u758f\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u6269\u5c55\u81eaSeerAttention\uff0c\u79fb\u9664\u67e5\u8be2\u6c60\u5316\u4ee5\u9002\u5e94\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u95e8\u63a7\u673a\u5236\uff0c\u65e0\u9700\u4fee\u6539\u539f\u59cb\u6a21\u578b\u53c2\u6570\u3002", "result": "\u57280.4B tokens\u8bad\u7ec3\u4e0b\uff0c4K token\u9884\u7b97\u5185\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u63a8\u7406\u51c6\u786e\u7387\uff0c\u4f18\u5316\u89e3\u7801\u5185\u6838\u5b9e\u73b09\u500d\u52a0\u901f\u3002", "conclusion": "SeerAttention-R\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u957f\u89e3\u7801\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2506.08902", "pdf": "https://arxiv.org/pdf/2506.08902", "abs": "https://arxiv.org/abs/2506.08902", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "title": "Intention-Conditioned Flow Occupancy Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6982\u7387\u6a21\u578bInFOM\uff0c\u7528\u4e8e\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u957f\u671f\u672a\u6765\u7684\u72b6\u6001\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u53d8\u91cf\u6355\u6349\u7528\u6237\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u4ecd\u9762\u4e34\u957f\u671f\u52a8\u4f5c\u4f9d\u8d56\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u72b6\u6001\u5206\u5e03\u548c\u7528\u6237\u610f\u56fe\uff0c\u63d0\u5347RL\u7684\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u6d41\u5339\u914d\u6784\u5efa\u6982\u7387\u6a21\u578b\uff0c\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5206\u5e03\uff08\u5360\u7528\u5ea6\u91cf\uff09\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u53d8\u91cf\u6355\u6349\u7528\u6237\u610f\u56fe\uff0c\u652f\u6301\u5e7f\u4e49\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u572836\u4e2a\u72b6\u6001\u57fa\u51c6\u548c4\u4e2a\u56fe\u50cf\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cInFOM\u7684\u4e2d\u4f4d\u56de\u62a5\u63d0\u53471.8\u500d\uff0c\u6210\u529f\u7387\u63d0\u9ad836%\u3002", "conclusion": "InFOM\u901a\u8fc7\u5efa\u6a21\u72b6\u6001\u5206\u5e03\u548c\u7528\u6237\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u9884\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u4e3aRL\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08917", "pdf": "https://arxiv.org/pdf/2506.08917", "abs": "https://arxiv.org/abs/2506.08917", "authors": ["Sascha M\u00fccke", "Raoul Heese", "Thore Gerlach", "David Biesner", "Loong Kuan Lee", "Nico Piatkowski"], "title": "Quantum Adiabatic Generation of Human-Like Passwords", "categories": ["quant-ph", "cs.AI"], "comment": "9 pages, 4 figures", "summary": "Generative Artificial Intelligence (GenAI) for Natural Language Processing\n(NLP) is the predominant AI technology to date. An important perspective for\nQuantum Computing (QC) is the question whether QC has the potential to reduce\nthe vast resource requirements for training and operating GenAI models. While\nlarge-scale generative NLP tasks are currently out of reach for practical\nquantum computers, the generation of short semantic structures such as\npasswords is not. Generating passwords that mimic real user behavior has many\napplications, for example to test an authentication system against realistic\nthreat models. Classical password generation via deep learning have recently\nbeen investigated with significant progress in their ability to generate novel,\nrealistic password candidates. In the present work we investigate the utility\nof adiabatic quantum computers for this task. More precisely, we study\ndifferent encodings of token strings and propose novel approaches based on the\nQuadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum\nIndependent Set (UD-MIS) problems. Our approach allows us to estimate the token\ndistribution from data and adiabatically prepare a quantum state from which we\neventually sample the generated passwords via measurements. Our results show\nthat relatively small samples of 128 passwords, generated on the QuEra Aquila\n256-qubit neutral atom quantum computer, contain human-like passwords such as\n\"Tunas200992\" or \"teedem28iglove\".", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u751f\u6210\u77ed\u8bed\u4e49\u7ed3\u6784\uff08\u5982\u5bc6\u7801\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u57fa\u4e8eQUBO\u548cUD-MIS\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u751f\u6210\u4eba\u7c7b\u7c7b\u4f3c\u5bc6\u7801\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u662f\u5426\u80fd\u51cf\u5c11\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u77ed\u8bed\u4e49\u7ed3\u6784\uff08\u5982\u5bc6\u7801\uff09\u65b9\u9762\u3002", "method": "\u91c7\u7528QUBO\u548cUD-MIS\u95ee\u9898\u7f16\u7801\uff0c\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u673a\u4f30\u8ba1\u4ee4\u724c\u5206\u5e03\u5e76\u751f\u6210\u5bc6\u7801\u3002", "result": "\u5728256\u91cf\u5b50\u4f4d\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6210\u529f\u751f\u6210128\u4e2a\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5bc6\u7801\u6837\u672c\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u751f\u6210\u77ed\u8bed\u4e49\u7ed3\u6784\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5927\u89c4\u6a21\u4efb\u52a1\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.08961", "pdf": "https://arxiv.org/pdf/2506.08961", "abs": "https://arxiv.org/abs/2506.08961", "authors": ["Chenxu Wang", "Huaping Liu"], "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have\nbeen widely studied in various threat models; however, few consider\nenvironmental state perturbations, which are natural in embodied scenarios. To\nimprove the robustness of DRL agents, we formulate the problem of environmental\nstate perturbation, introducing a preliminary non-targeted attack method as a\ncalibration adversary, and then propose a defense framework, named Boosted\nAdversarial Training (BAT), which first tunes the agents via supervised\nlearning to avoid catastrophic failure and subsequently adversarially trains\nthe agent with reinforcement learning. Extensive experimental results\nsubstantiate the vulnerability of mainstream agents under environmental state\nperturbations and the effectiveness of our proposed attack. The defense results\ndemonstrate that while existing robust reinforcement learning algorithms may\nnot be suitable, our BAT framework can significantly enhance the robustness of\nagents against environmental state perturbations across various situations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e2d\u73af\u5883\u72b6\u6001\u6270\u52a8\u7684\u5bf9\u6297\u653b\u51fb\u4e0e\u9632\u5fa1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBAT\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u73af\u5883\u72b6\u6001\u6270\u52a8\uff0c\u800c\u8fd9\u7c7b\u6270\u52a8\u5728\u5177\u4f53\u573a\u666f\u4e2d\u5f88\u5e38\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347DRL\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3002", "method": "\u9996\u5148\u63d0\u51fa\u4e00\u79cd\u975e\u76ee\u6807\u653b\u51fb\u65b9\u6cd5\u4f5c\u4e3a\u6821\u51c6\u5bf9\u624b\uff0c\u968f\u540e\u63d0\u51faBAT\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5bf9\u6297\u8bad\u7ec3\u6765\u589e\u5f3a\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e3b\u6d41\u4ee3\u7406\u5728\u73af\u5883\u72b6\u6001\u6270\u52a8\u4e0b\u8106\u5f31\uff0c\u800cBAT\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "BAT\u6846\u67b6\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u6709\u6548\u589e\u5f3a\u4ee3\u7406\u5bf9\u73af\u5883\u72b6\u6001\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u800c\u73b0\u6709\u7b97\u6cd5\u53ef\u80fd\u4e0d\u9002\u7528\u3002"}}
{"id": "2506.08962", "pdf": "https://arxiv.org/pdf/2506.08962", "abs": "https://arxiv.org/abs/2506.08962", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to 2025 Frontiers in Education (FIE) Conference", "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u667a\u80fd\u5bfc\u5e08\uff0c\u7528\u4e8e\u672c\u79d1\u7535\u8def\u5206\u6790\u8bfe\u7a0b\u7684\u4f5c\u4e1a\u8bc4\u4f30\u4e0e\u53cd\u9988\uff0c\u5c55\u793a\u4e86\u5176\u8bbe\u8ba1\u3001\u90e8\u7f72\u53ca\u521d\u6b65\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u63d0\u4f9b\u4e2a\u6027\u5316\u4f5c\u4e1a\u53cd\u9988\uff0c\u5e2e\u52a9\u6559\u5e08\u5b9e\u65f6\u4e86\u89e3\u5b66\u751f\u5b66\u4e60\u96be\u70b9\uff0c\u4f18\u5316\u6559\u5b66\u3002", "method": "\u8bbe\u8ba1\u4e86\u652f\u6301\u5f00\u653e\u5f0f\u95ee\u7b54\u548c\u4f5c\u4e1a\u53cd\u9988\u751f\u6210\u7684\u667a\u80fd\u5bfc\u5e08\uff0c\u90e8\u7f72\u4e8e\u5fae\u8f6fAzure\u5e73\u53f0\uff0c\u5e76\u6536\u96c6\u5b66\u751f\u4e92\u52a8\u6570\u636e\u3002", "result": "90.9%\u7684\u5b66\u751f\u5bf9\u5bfc\u5e08\u8868\u793a\u6ee1\u610f\uff0c\u521d\u6b65\u6570\u636e\u5206\u6790\u5e2e\u52a9\u6559\u5e08\u8bc6\u522b\u9ad8\u9891\u95ee\u9898\u3002", "conclusion": "\u667a\u80fd\u5bfc\u5e08\u6548\u679c\u663e\u8457\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f3\u66f4\u591a\u5de5\u7a0b\u5b66\u79d1\uff0c\u5e76\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u7b49\u6280\u672f\u3002"}}
{"id": "2506.08965", "pdf": "https://arxiv.org/pdf/2506.08965", "abs": "https://arxiv.org/abs/2506.08965", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The ability to train high-performing reward models with few-shot data is\ncritical for enhancing the efficiency and scalability of Reinforcement Learning\nfrom Human Feedback (RLHF). We propose a data augmentation and expansion\nframework that enables generative reward models trained on small datasets to\nachieve comparable performance to those trained on large-scale datasets.\nTraditional methods to train a generative reward model, such as Direct\nPreference Optimization (DPO), are constrained by inefficiencies in sample\npairing and limited data diversity. This work introduces preference refinement,\nwhich employs Chain-of-Thought (CoT) sampling to uncover diverse and\nhigh-quality preference relationships. It also incorporates a perplexity-based\nscoring mechanism to assign nuanced preference levels and utilizes Multi-level\nDirect Preference Optimization (M-DPO) to enable the model to capture\nfiner-grained preference differences between samples. Experimental results\ndemonstrate that the proposed method significantly enhances data efficiency and\nmodel performance, enabling reward models trained in a few-shot setting to\nachieve results on par with those trained on large-scale datasets. This study\nunderscores the potential of data-efficient strategies in advancing reward\nmodel optimization, offering a robust solution for low-resource RLHF\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u548c\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u7ec6\u5316\u548c\u591a\u7ea7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08M-DPO\uff09\uff0c\u4f7f\u5c0f\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u751f\u6210\u5956\u52b1\u6a21\u578b\u8fbe\u5230\u4e0e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u76f4\u63a5\u504f\u597d\u4f18\u5316DPO\uff09\u5728\u6837\u672c\u914d\u5bf9\u548c\u6570\u636e\u591a\u6837\u6027\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u751f\u6210\u5956\u52b1\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u504f\u597d\u7ec6\u5316\uff0c\u5229\u7528Chain-of-Thought\u91c7\u6837\u63ed\u793a\u591a\u6837\u5316\u7684\u504f\u597d\u5173\u7cfb\uff0c\u7ed3\u5408\u56f0\u60d1\u5ea6\u8bc4\u5206\u673a\u5236\u548c\u591a\u7ea7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08M-DPO\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5c0f\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u5ab2\u7f8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u6570\u636e\u9ad8\u6548\u7b56\u7565\u5728\u5956\u52b1\u6a21\u578b\u4f18\u5316\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4e3a\u4f4e\u8d44\u6e90RLHF\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08977", "pdf": "https://arxiv.org/pdf/2506.08977", "abs": "https://arxiv.org/abs/2506.08977", "authors": ["Victoria Hankemeier", "Malte Schilling"], "title": "Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IJCNN25, Code: https://github.com/vicky-hnk/time-flex", "summary": "Developments in Deep Learning have significantly improved time series\nforecasting by enabling more accurate modeling of complex temporal dependencies\ninherent in sequential data. The effectiveness of such models is often\ndemonstrated on limited sets of specific real-world data. Although this allows\nfor comparative analysis, it still does not demonstrate how specific data\ncharacteristics align with the architectural strengths of individual models.\nOur research aims at uncovering clear connections between time series\ncharacteristics and particular models. We introduce a novel dataset generated\nusing Gaussian Processes, specifically designed to display distinct, known\ncharacteristics for targeted evaluations of model adaptability to them.\nFurthermore, we present TimeFlex, a new model that incorporates a modular\narchitecture tailored to handle diverse temporal dynamics, including trends and\nperiodic patterns. This model is compared to current state-of-the-art models,\noffering a deeper understanding of how models perform under varied time series\nconditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6570\u636e\u96c6\u548c\u6a21\u578bTimeFlex\uff0c\u65e8\u5728\u63ed\u793a\u65f6\u95f4\u5e8f\u5217\u7279\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u660e\u786e\u8054\u7cfb\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u7279\u6027\u4e0e\u6a21\u578b\u67b6\u6784\u5339\u914d\u6027\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u751f\u6210\u5177\u6709\u660e\u786e\u7279\u6027\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u6a21\u5757\u5316\u6a21\u578bTimeFlex\u4ee5\u5904\u7406\u591a\u6837\u65f6\u95f4\u52a8\u6001\u3002", "result": "TimeFlex\u4e0e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u5bf9\u6bd4\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65f6\u95f4\u5e8f\u5217\u7279\u6027\u4e0e\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.08978", "pdf": "https://arxiv.org/pdf/2506.08978", "abs": "https://arxiv.org/abs/2506.08978", "authors": ["Anna Langedijk", "Jaap Jumelet", "Willem Zuidema"], "title": "Propositional Logic for Probing Generalization in Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The extent to which neural networks are able to acquire and represent\nsymbolic rules remains a key topic of research and debate. Much current work\nfocuses on the impressive capabilities of large language models, as well as\ntheir often ill-understood failures on a wide range of reasoning tasks. In this\npaper, in contrast, we investigate the generalization behavior of three key\nneural architectures (Transformers, Graph Convolution Networks and LSTMs) in a\ncontrolled task rooted in propositional logic. The task requires models to\ngenerate satisfying assignments for logical formulas, making it a structured\nand interpretable setting for studying compositionality. We introduce a\nbalanced extension of an existing dataset to eliminate superficial patterns and\nenable testing on unseen operator combinations. Using this dataset, we evaluate\nthe ability of the three architectures to generalize beyond the training\ndistribution. While all models perform well in-distribution, we find that\ngeneralization to unseen patterns, particularly those involving negation,\nremains a significant challenge. Transformers fail to apply negation\ncompositionally, unless structural biases are introduced. Our findings\nhighlight persistent limitations in the ability of standard architectures to\nlearn systematic representations of logical operators, suggesting the need for\nstronger inductive biases to support robust rule-based reasoning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u547d\u9898\u903b\u8f91\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5bf9\u672a\u89c1\u8fc7\u6a21\u5f0f\uff08\u5c24\u5176\u662f\u5426\u5b9a\u64cd\u4f5c\uff09\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u66f4\u5f3a\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u5728\u7b26\u53f7\u89c4\u5219\u5b66\u4e60\u548c\u8868\u793a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u547d\u9898\u903b\u8f91\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528Transformer\u3001\u56fe\u5377\u79ef\u7f51\u7edc\u548cLSTM\u4e09\u79cd\u67b6\u6784\uff0c\u5728\u547d\u9898\u903b\u8f91\u4efb\u52a1\u4e2d\u751f\u6210\u6ee1\u8db3\u903b\u8f91\u516c\u5f0f\u7684\u8d4b\u503c\uff0c\u5e76\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u6d88\u9664\u8868\u9762\u6a21\u5f0f\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u8bad\u7ec3\u5206\u5e03\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u672a\u89c1\u8fc7\u6a21\u5f0f\uff08\u5c24\u5176\u662f\u5426\u5b9a\u64cd\u4f5c\uff09\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "conclusion": "\u6807\u51c6\u67b6\u6784\u5728\u903b\u8f91\u64cd\u4f5c\u7684\u7cfb\u7edf\u8868\u793a\u5b66\u4e60\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u66f4\u5f3a\u7684\u5f52\u7eb3\u504f\u7f6e\u652f\u6301\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\u3002"}}
{"id": "2506.09018", "pdf": "https://arxiv.org/pdf/2506.09018", "abs": "https://arxiv.org/abs/2506.09018", "authors": ["Marton Havasi", "Brian Karrer", "Itai Gat", "Ricky T. Q. Chen"], "title": "Edit Flows: Flow Matching with Edit Operations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Autoregressive generative models naturally generate variable-length\nsequences, while non-autoregressive models struggle, often imposing rigid,\ntoken-wise structures. We propose Edit Flows, a non-autoregressive model that\novercomes these limitations by defining a discrete flow over sequences through\nedit operations-insertions, deletions, and substitutions. By modeling these\noperations within a Continuous-time Markov Chain over the sequence space, Edit\nFlows enable flexible, position-relative generation that aligns more closely\nwith the structure of sequence data. Our training method leverages an expanded\nstate space with auxiliary variables, making the learning process efficient and\ntractable. Empirical results show that Edit Flows outperforms both\nautoregressive and mask models on image captioning and significantly\noutperforms the mask construction in text and code generation.", "AI": {"tldr": "Edit Flows\u662f\u4e00\u79cd\u975e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u7f16\u8f91\u64cd\u4f5c\uff08\u63d2\u5165\u3001\u5220\u9664\u548c\u66ff\u6362\uff09\u5728\u5e8f\u5217\u7a7a\u95f4\u4e0a\u5b9a\u4e49\u79bb\u6563\u6d41\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u975e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u81ea\u7136\u751f\u6210\u53d8\u957f\u5e8f\u5217\uff0c\u800c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u901a\u5e38\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u6807\u8bb0\u7ed3\u6784\u3002Edit Flows\u65e8\u5728\u901a\u8fc7\u7f16\u8f91\u64cd\u4f5c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u5e8f\u5217\u751f\u6210\u3002", "method": "\u5229\u7528\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u5728\u5e8f\u5217\u7a7a\u95f4\u4e0a\u5efa\u6a21\u7f16\u8f91\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u548c\u8f85\u52a9\u53d8\u91cf\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEdit Flows\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e0a\u4f18\u4e8e\u81ea\u56de\u5f52\u548c\u63a9\u7801\u6a21\u578b\uff0c\u5728\u6587\u672c\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u63a9\u7801\u6784\u9020\u65b9\u6cd5\u3002", "conclusion": "Edit Flows\u901a\u8fc7\u7f16\u8f91\u64cd\u4f5c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u5e8f\u5217\u751f\u6210\uff0c\u4e3a\u975e\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.09034", "pdf": "https://arxiv.org/pdf/2506.09034", "abs": "https://arxiv.org/abs/2506.09034", "authors": ["Sizhe Dang", "Yangyang Guo", "Yanjun Zhao", "Haishan Ye", "Xiaodong Zheng", "Guang Dai", "Ivor Tsang"], "title": "FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:\nthe backward pass of first-order optimizers like Adam increases memory usage to\nmore than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order\n(ZO) optimizers avoid this cost by estimating gradients only from forward\npasses, yet existing methods like MeZO usually require many more steps to\nconverge. Can this trade-off between speed and memory in ZO be fundamentally\nimproved? Normalized-SGD demonstrates strong empirical performance with greater\nmemory efficiency than Adam. In light of this, we introduce FZOO, a Fast\nZeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward\npasses needed for convergence by employing batched one-sided estimates that\nadapt step sizes based on the standard deviation of batch losses. It also\naccelerates per-batch computation through the use of Rademacher random vector\nperturbations coupled with CUDA's parallel processing. Extensive experiments on\ndiverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,\nacross 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms\nMeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For\nRoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy\nand an 18 times reduction in forward passes compared to MeZO, achieving\nconvergence speeds comparable to Adam. We also provide theoretical analysis\nproving FZOO's formal equivalence to a normalized-SGD update rule and its\nconvergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling\neven larger memory savings. Overall, our results make single-GPU, high-speed,\nfull-parameter fine-tuning practical and point toward future work on\nmemory-efficient pre-training.", "AI": {"tldr": "FZOO\u662f\u4e00\u79cd\u5feb\u901f\u96f6\u9636\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u524d\u5411\u4f20\u9012\u6b21\u6570\u548c\u5e76\u884c\u5904\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u6027\u80fd\u4f18\u4e8eMeZO\uff0c\u63a5\u8fd1Adam\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2dGPU\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u5316\u901f\u5ea6\u3002", "method": "\u91c7\u7528\u6279\u91cf\u5355\u8fb9\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u6b65\u957f\u8c03\u6574\uff0c\u7ed3\u5408CUDA\u5e76\u884c\u5904\u7406\uff0c\u51cf\u5c11\u524d\u5411\u4f20\u9012\u6b21\u6570\u3002", "result": "FZOO\u572811\u4e2a\u4efb\u52a1\u4e2d\u5e73\u5747\u6bd4MeZO\u51c6\u786e\u7387\u63d0\u9ad83%\uff0c\u524d\u5411\u4f20\u9012\u51cf\u5c113\u500d\uff1bRoBERTa-large\u4e0a\u51c6\u786e\u7387\u63d0\u53475.6%\uff0c\u524d\u5411\u4f20\u9012\u51cf\u5c1118\u500d\u3002", "conclusion": "FZOO\u5b9e\u73b0\u4e86\u5355GPU\u9ad8\u6548\u5fae\u8c03\uff0c\u4e3a\u5185\u5b58\u9ad8\u6548\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09046", "pdf": "https://arxiv.org/pdf/2506.09046", "abs": "https://arxiv.org/abs/2506.09046", "authors": ["Xiaowen Ma", "Chenyang Lin", "Yao Zhang", "Volker Tresp", "Yunpu Ma"], "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework.", "AI": {"tldr": "Agentic Neural Network (ANN) \u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u4f3c\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u72b6\u7ed3\u6784\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5904\u7406\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u590d\u6742\u4efb\u52a1\u7684\u52a8\u6001\u5904\u7406\u80fd\u529b\u3002ANN \u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7684\u542f\u53d1\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "method": "ANN \u91c7\u7528\u4e24\u5c42\u4f18\u5316\u7b56\u7565\uff1a\u524d\u5411\u9636\u6bb5\u52a8\u6001\u5206\u89e3\u4efb\u52a1\u5e76\u6784\u5efa\u534f\u4f5c\u56e2\u961f\uff1b\u53cd\u5411\u9636\u6bb5\u901a\u8fc7\u53cd\u9988\u4f18\u5316\u5168\u5c40\u548c\u5c40\u90e8\u534f\u4f5c\uff0c\u4f7f\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cANN \u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ANN \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u6570\u636e\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u4e86 LLM \u7684\u534f\u4f5c\u80fd\u529b\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u7075\u6d3b\u6027\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u3002"}}
